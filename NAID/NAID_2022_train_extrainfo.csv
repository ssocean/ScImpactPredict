id,title,cites,TNCSI,TNCSI_SP,abstract,OA,is_practical,new_task,new_dataset,SOTA,is_broad,RQM,SMP,ARQ,Ref_num
a8dbce3c-884a-4b31-82f4-7708b4229bad,Finger Multimodal Feature Fusion and Recognition Based on Channel Spatial Attention,2,0.0772666,0.193988,"Due to the instability and limitations of unimodal biometric systems,
multimodal systems have attracted more and more attention from researchers.
However, how to exploit the independent and complementary information between
different modalities remains a key and challenging problem. In this paper, we
propose a multimodal biometric fusion recognition algorithm based on
fingerprints and finger veins (Fingerprint Finger Veins-Channel Spatial
Attention Fusion Module, FPV-CSAFM). Specifically, for each pair of fingerprint
and finger vein images, we first propose a simple and effective Convolutional
Neural Network (CNN) to extract features. Then, we build a multimodal feature
fusion module (Channel Spatial Attention Fusion Module, CSAFM) to fully fuse
the complementary information between fingerprints and finger veins. Different
from existing fusion strategies, our fusion method can dynamically adjust the
fusion weights according to the importance of different modalities in channel
and spatial dimensions, so as to better combine the information between
different modalities and improve the overall recognition performance. To
evaluate the performance of our method, we conduct a series of experiments on
multiple public datasets. Experimental results show that the proposed FPV-CSAFM
achieves excellent recognition performance on three multimodal datasets based
on fingerprints and finger veins.",0,1,0,0,0,0,0.526409,9.0,0.788825,55
78d7c1e6-6982-4775-a8b5-18dc5966d6b2,Enhanced Teaching-Learning-based Optimization for 3D Path Planning of Multicopter UAVs,1,0.0243332,0.121216,"This paper introduces a new path planning algorithm for unmanned aerial
vehicles (UAVs) based on the teaching-learning-based optimization (TLBO)
technique. We first define an objective function that incorporates requirements
on the path length and constraints on the movement and safe operation of UAVs
to convert the path planning into an optimization problem. The optimization
algorithm named Multi-subject TLBO is then proposed to minimize the formulated
objective function. The algorithm is developed based on TLBO but enhanced with
new operations including mutation, elite selection and multi-subject training
to improve the solution quality and speed up the convergence rate. Comparison
with state-of-the-art algorithms and experiments with real UAVs have been
conducted to evaluate the performance of the proposed algorithm. The results
confirm its validity and effectiveness in generating optimal, collision-free
and flyable paths for UAVs in complex operating environments.",0,1,1,0,1,0,0.19237,15.0,0.78981,19
5efb802f-8c69-41a0-8798-1468cc15c790,Quantifying syntax similarity with a polynomial representation of dependency trees,3,0.0228132,0.533744,"We introduce a graph polynomial that distinguishes tree structures to
represent dependency grammar and a measure based on the polynomial
representation to quantify syntax similarity. The polynomial encodes accurate
and comprehensive information about the dependency structure and dependency
relations of words in a sentence. We apply the polynomial-based methods to
analyze sentences in the Parallel Universal Dependencies treebanks.
Specifically, we compare the syntax of sentences and their translations in
different languages, and we perform a syntactic typology study of available
languages in the Parallel Universal Dependencies treebanks. We also demonstrate
and discuss the potential of the methods in measuring syntax diversity of
corpora.",1,0,0,0,0,0,0.0313273,10.0,0.494317,36
7d426427-2cb8-4534-b890-04ba8e7dcfe6,Dialect-robust Evaluation of Generated Text,11,0.209817,0.353535,"Evaluation metrics that are not robust to dialect variation make it
impossible to tell how well systems perform for many groups of users, and can
even penalize systems for producing text in lower-resource dialects. However,
currently, there exists no way to quantify how metrics respond to change in the
dialect of a generated utterance. We thus formalize dialect robustness and
dialect awareness as goals for NLG evaluation metrics. We introduce a suite of
methods and corresponding statistical tests one can use to assess metrics in
light of the two goals. Applying the suite to current state-of-the-art metrics,
we demonstrate that they are not dialect-robust and that semantic perturbations
frequently lead to smaller decreases in a metric than the introduction of
dialect features. As a first step to overcome this limitation, we propose a
training schema, NANO, which introduces regional and language information to
the pretraining process of a metric. We demonstrate that NANO provides a
size-efficient way for models to improve the dialect robustness while
simultaneously improving their performance on the standard metric benchmark.",0,0,0,0,0,0,0.633226,8.0,0.799196,54
372dfa17-cd78-41e5-83e9-f8344b0f72ac,PatchRot: A Self-Supervised Technique for Training Vision Transformers,1,0.00631359,0.0402278,"Vision transformers require a huge amount of labeled data to outperform
convolutional neural networks. However, labeling a huge dataset is a very
expensive process. Self-supervised learning techniques alleviate this problem
by learning features similar to supervised learning in an unsupervised way. In
this paper, we propose a self-supervised technique PatchRot that is crafted for
vision transformers. PatchRot rotates images and image patches and trains the
network to predict the rotation angles. The network learns to extract both
global and local features from an image. Our extensive experiments on different
datasets showcase PatchRot training learns rich features which outperform
supervised learning and compared baseline.",0,1,0,0,0,0,0.628913,10.0,0.838184,22
e63c57dd-ce4e-4fa2-82e2-5cbc350eef44,Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer,37,0.669745,0.948658,"Text spotting end-to-end methods have recently gained attention in the
literature due to the benefits of jointly optimizing the text detection and
recognition components. Existing methods usually have a distinct separation
between the detection and recognition branches, requiring exact annotations for
the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach
for text spotting and the first text spotting framework which may be trained
with both fully- and weakly-supervised settings. By learning a single latent
representation per word detection, and using a novel loss function based on the
Hungarian loss, our method alleviates the need for expensive localization
annotations. Trained with only text transcription annotations on real data, our
weakly-supervised method achieves competitive performance with previous
state-of-the-art fully-supervised methods. When trained in a fully-supervised
manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.",0,1,0,0,1,0,0.942135,9.0,0.937528,45
41e347f9-ec19-426f-afab-a640cdc6b647,IR-GAN: Image Manipulation with Linguistic Instruction by Increment Reasoning,17,0.213123,0.226538,"Conditional image generation is an active research topic including text2image
and image translation.
  Recently image manipulation with linguistic instruction brings new challenges
of multimodal conditional generation.
  However, traditional conditional image generation models mainly focus on
generating high-quality and visually realistic images, and lack resolving the
partial consistency between image and instruction.
  To address this issue, we propose an Increment Reasoning Generative
Adversarial Network (IR-GAN), which aims to reason the consistency between
visual increment in images and semantic increment in instructions.
  First, we introduce the word-level and instruction-level instruction encoders
to learn user's intention from history-correlated instructions as semantic
increment.
  Second, we embed the representation of semantic increment into that of source
image for generating target image, where source image plays the role of
referring auxiliary.
  Finally, we propose a reasoning discriminator to measure the consistency
between visual increment and semantic increment, which purifies user's
intention and guarantees the good logic of generated target image.
  Extensive experiments and visualization conducted on two datasets show the
effectiveness of IR-GAN.",0,1,0,0,1,0,0.840774,6.0,0.833155,36
f0756b58-0d0c-4ad3-b802-c854e3376b7b,End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking,18,0.171316,0.197419,"Machine learning systems perform well on pattern matching tasks, but their
ability to perform algorithmic or logical reasoning is not well understood. One
important reasoning capability is algorithmic extrapolation, in which models
trained only on small/simple reasoning problems can synthesize complex
strategies for large/complex problems at test time. Algorithmic extrapolation
can be achieved through recurrent systems, which can be iterated many times to
solve difficult reasoning problems. We observe that this approach fails to
scale to highly complex problems because behavior degenerates when many
iterations are applied -- an issue we refer to as ""overthinking."" We propose a
recall architecture that keeps an explicit copy of the problem instance in
memory so that it cannot be forgotten. We also employ a progressive training
routine that prevents the model from learning behaviors that are specific to
iteration number and instead pushes it to learn behaviors that can be repeated
indefinitely. These innovations prevent the overthinking problem, and enable
recurrent systems to solve extremely hard extrapolation tasks.",1,0,0,0,0,0,0.129581,12.0,0.701308,25
a3d3e6f2-10ef-4b15-aa64-144256263e64,PoserNet: Refining Relative Camera Poses Exploiting Object Detections,3,0.0221712,0.27247,"The estimation of the camera poses associated with a set of images commonly
relies on feature matches between the images. In contrast, we are the first to
address this challenge by using objectness regions to guide the pose estimation
problem rather than explicit semantic object detections. We propose Pose
Refiner Network (PoserNet) a light-weight Graph Neural Network to refine the
approximate pair-wise relative camera poses. PoserNet exploits associations
between the objectness regions - concisely expressed as bounding boxes - across
multiple views to globally refine sparsely connected view graphs. We evaluate
on the 7-Scenes dataset across varied sizes of graphs and show how this process
can be beneficial to optimisation-based Motion Averaging algorithms improving
the median error on the rotation by 62 degrees with respect to the initial
estimates obtained based on bounding boxes. Code and data are available at
https://github.com/IIT-PAVIS/PoserNet.",0,1,0,0,0,0,0.153231,8.0,0.574595,38
009887a4-ec84-4371-9edf-33e1dffb45fe,Soft Diffusion: Score Matching for General Corruptions,71,0.366919,0.963586,"We define a broader family of corruption processes that generalizes
previously known diffusion models. To reverse these general diffusions, we
propose a new objective called Soft Score Matching that provably learns the
score function for any linear corruption process and yields state of the art
results for CelebA. Soft Score Matching incorporates the degradation process in
the network. Our new loss trains the model to predict a clean image,
\textit{that after corruption}, matches the diffused observation. We show that
our objective learns the gradient of the likelihood under suitable regularity
conditions for a family of corruption processes. We further develop a
principled way to select the corruption levels for general diffusion processes
and a novel sampling method that we call Momentum Sampler. We show
experimentally that our framework works for general linear corruption
processes, such as Gaussian blur and masking. We achieve state-of-the-art FID
score $1.85$ on CelebA-64, outperforming all previous linear diffusion models.
We also show significant computational benefits compared to vanilla denoising
diffusion.",0,0,1,0,1,0,0.929103,2.0,0.681906,57
32f310f2-06a8-41a6-8c4c-bbf5c83bd4f3,Cross-Spectral Neural Radiance Fields,15,0.621734,0.689794,"We propose X-NeRF, a novel method to learn a Cross-Spectral scene
representation given images captured from cameras with different light spectrum
sensitivity, based on the Neural Radiance Fields formulation. X-NeRF optimizes
camera poses across spectra during training and exploits Normalized
Cross-Device Coordinates (NXDC) to render images of different modalities from
arbitrary viewpoints, which are aligned and at the same resolution. Experiments
on 16 forward-facing scenes, featuring color, multi-spectral and infrared
images, confirm the effectiveness of X-NeRF at modeling Cross-Spectral scene
representations.",1,1,1,1,0,0,0.98697,4.0,0.964639,67
75c3e5e2-39a1-4f8a-bf59-d26209416ce3,Models and Benchmarks for Representation Learning of Partially Observed Subgraphs,1,0.00114899,0.0256015,"Subgraphs are rich substructures in graphs, and their nodes and edges can be
partially observed in real-world tasks. Under partial observation, existing
node- or subgraph-level message-passing produces suboptimal representations. In
this paper, we formulate a novel task of learning representations of partially
observed subgraphs. To solve this problem, we propose Partial Subgraph InfoMax
(PSI) framework and generalize existing InfoMax models, including DGI,
InfoGraph, MVGRL, and GraphCL, into our framework. These models maximize the
mutual information between the partial subgraph's summary and various
substructures from nodes to full subgraphs. In addition, we suggest a novel
two-stage model with $k$-hop PSI, which reconstructs the representation of the
full subgraph and improves its expressiveness from different local-global
structures. Under training and evaluation protocols designed for this problem,
we conduct experiments on three real-world datasets and demonstrate that PSI
models outperform baselines.",1,0,1,0,0,0,0.189262,7.0,0.547002,66
f0a41b77-0bce-452f-adec-f3b6643c4c7b,All You Need Is Supervised Learning: From Imitation Learning to Meta-RL With Upside Down RL,6,0.030708,0.169402,"Upside down reinforcement learning (UDRL) flips the conventional use of the
return in the objective function in RL upside down, by taking returns as input
and predicting actions. UDRL is based purely on supervised learning, and
bypasses some prominent issues in RL: bootstrapping, off-policy corrections,
and discount factors. While previous work with UDRL demonstrated it in a
traditional online RL setting, here we show that this single algorithm can also
work in the imitation learning and offline RL settings, be extended to the
goal-conditioned RL setting, and even the meta-RL setting. With a general agent
architecture, a single UDRL agent can learn across all paradigms.",0,0,0,0,0,1,0.481484,6.0,0.661694,38
53f916f7-8690-4005-b88f-a105e7cfc965,CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical Expression Recognition,15,0.576755,0.986571,"The Transformer-based encoder-decoder architecture has recently made
significant advances in recognizing handwritten mathematical expressions.
However, the transformer model still suffers from the lack of coverage problem,
making its expression recognition rate (ExpRate) inferior to its RNN
counterpart. Coverage information, which records the alignment information of
the past steps, has proven effective in the RNN models. In this paper, we
propose CoMER, a model that adopts the coverage information in the transformer
decoder. Specifically, we propose a novel Attention Refinement Module (ARM) to
refine the attention weights with past alignment information without hurting
its parallelism. Furthermore, we take coverage information to the extreme by
proposing self-coverage and cross-coverage, which utilize the past alignment
information from the current and previous layers. Experiments show that CoMER
improves the ExpRate by 0.61%/2.09%/1.59% compared to the current
state-of-the-art model, and reaches 59.33%/59.81%/62.97% on the CROHME
2014/2016/2019 test sets.",1,1,0,0,1,0,0.879513,12.0,0.928349,36
42acb600-27bb-4570-9305-aef68597bb22,MultiEarth 2022 Deforestation Challenge -- ForestGump,3,0.00448083,0.0955867,"The estimation of deforestation in the Amazon Forest is challenge task
because of the vast size of the area and the difficulty of direct human access.
However, it is a crucial problem in that deforestation results in serious
environmental problems such as global climate change, reduced biodiversity,
etc. In order to effectively solve the problems, satellite imagery would be a
good alternative to estimate the deforestation of the Amazon. With a
combination of optical images and Synthetic aperture radar (SAR) images,
observation of such a massive area regardless of weather conditions become
possible. In this paper, we present an accurate deforestation estimation method
with conventional UNet and comprehensive data processing. The diverse channels
of Sentinel-1, Sentinel-2 and Landsat 8 are carefully selected and utilized to
train deep neural networks. With the proposed method, deforestation status for
novel queries are successfully estimated with high accuracy.",0,1,0,0,0,0,0.168309,4.0,0.174835,13
4705f903-0f25-46f6-bd4e-35d80f8421df,Data Augmentation with Paraphrase Generation and Entity Extraction for Multimodal Dialogue System,13,0.0506349,0.412649,"Contextually aware intelligent agents are often required to understand the
users and their surroundings in real-time. Our goal is to build Artificial
Intelligence (AI) systems that can assist children in their learning process.
Within such complex frameworks, Spoken Dialogue Systems (SDS) are crucial
building blocks to handle efficient task-oriented communication with children
in game-based learning settings. We are working towards a multimodal dialogue
system for younger kids learning basic math concepts. Our focus is on improving
the Natural Language Understanding (NLU) module of the task-oriented SDS
pipeline with limited datasets. This work explores the potential benefits of
data augmentation with paraphrase generation for the NLU models trained on
small task-specific datasets. We also investigate the effects of extracting
entities for conceivably further data expansion. We have shown that
paraphrasing with model-in-the-loop (MITL) strategies using small seed data is
a promising approach yielding improved performance results for the Intent
Recognition task.",0,1,0,0,0,0,0.0309087,10.0,0.49295,82
6ba07fcf-611a-4551-bd77-0d092e5aabdf,Mix and Localize: Localizing Sound Sources in Mixtures,30,0.353669,0.753976,"We present a method for simultaneously localizing multiple sound sources
within a visual scene. This task requires a model to both group a sound mixture
into individual sources, and to associate them with a visual signal. Our method
jointly solves both tasks at once, using a formulation inspired by the
contrastive random walk of Jabri et al. We create a graph in which images and
separated sounds correspond to nodes, and train a random walker to transition
between nodes from different modalities with high return probability. The
transition probabilities for this walk are determined by an audio-visual
similarity metric that is learned by our model. We show through experiments
with musical instruments and human speech that our model can successfully
localize multiple sounds, outperforming other self-supervised methods. Project
site: https://hxixixh.github.io/mix-and-localize",1,0,0,0,0,0,0.734364,9.0,0.852494,54
ab20f26d-5562-4487-8771-999d9aaa01b3,SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks,27,0.423004,0.996267,"We present SkexGen, a novel autoregressive generative model for
computer-aided design (CAD) construction sequences containing
sketch-and-extrude modeling operations. Our model utilizes distinct Transformer
architectures to encode topological, geometric, and extrusion variations of
construction sequences into disentangled codebooks. Autoregressive Transformer
decoders generate CAD construction sequences sharing certain properties
specified by the codebook vectors. Extensive experiments demonstrate that our
disentangled codebook representation generates diverse and high-quality CAD
models, enhances user control, and enables efficient exploration of the design
space. The code is available at https://samxuxiang.github.io/skexgen.",1,1,0,0,0,0,0.667754,7.0,0.78394,42
211b27bb-a6f4-4866-aac3-5535894b6251,WR-ONE2SET: Towards Well-Calibrated Keyphrase Generation,8,0.201308,0.0943829,"Keyphrase generation aims to automatically generate short phrases summarizing
an input document. The recently emerged ONE2SET paradigm (Ye et al., 2021)
generates keyphrases as a set and has achieved competitive performance.
Nevertheless, we observe serious calibration errors outputted by ONE2SET,
especially in the over-estimation of $\varnothing$ token (means ""no
corresponding keyphrase""). In this paper, we deeply analyze this limitation and
identify two main reasons behind: 1) the parallel generation has to introduce
excessive $\varnothing$ as padding tokens into training instances; and 2) the
training mechanism assigning target to each slot is unstable and further
aggravates the $\varnothing$ token over-estimation. To make the model
well-calibrated, we propose WR-ONE2SET which extends ONE2SET with an adaptive
instance-level cost Weighting strategy and a target Re-assignment mechanism.
The former dynamically penalizes the over-estimated slots for different
instances thus smoothing the uneven training distribution. The latter refines
the original inappropriate assignment and reduces the supervisory signals of
over-estimated slots. Experimental results on commonly-used datasets
demonstrate the effectiveness and generality of our proposed paradigm.",1,1,0,0,0,0,0.448071,12.0,0.822521,27
f1cc7615-2d0b-4358-9dc4-c88c704b8659,LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension,6,0.0536176,0.614527,"The application of Natural Language Processing (NLP) to specialized domains,
such as the law, has recently received a surge of interest. As many legal
services rely on processing and analyzing large collections of documents,
automating such tasks with NLP tools emerges as a key challenge. Many popular
language models, such as BERT or RoBERTa, are general-purpose models, which
have limitations on processing specialized legal terminology and syntax. In
addition, legal documents may contain specialized vocabulary from other
domains, such as medical terminology in personal injury text. Here, we propose
LegalRelectra, a legal-domain language model that is trained on mixed-domain
legal and medical corpora. We show that our model improves over general-domain
and single-domain medical and legal language models when processing
mixed-domain (personal injury) text. Our training architecture implements the
Electra framework, but utilizes Reformer instead of BERT for its generator and
discriminator. We show that this improves the model's performance on processing
long passages and results in better long-range text comprehension.",0,1,0,0,0,0,0.658128,7.0,0.780188,27
70e10440-5312-4e68-916c-3d1e840c9836,VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners,24,0.177501,0.634628,"We explore an efficient approach to establish a foundational video-text
model. We present VideoCoCa that maximally reuses a pretrained image-text
contrastive captioner (CoCa) model and adapt it to video-text tasks with
minimal extra training. While previous works adapt image-text models with
various cross-frame fusion modules, we find that the generative attentional
pooling and contrastive attentional pooling layers in CoCa are instantly
adaptable to flattened frame embeddings, yielding state-of-the-art results on
zero-shot video classification and zero-shot text-to-video retrieval.
Furthermore, we explore lightweight finetuning on top of VideoCoCa, and achieve
strong results on video question-answering and video captioning.",0,1,0,0,1,0,0.899798,4.0,0.80593,76
2d505e44-a54c-4b2e-8fb5-3daa01ae8740,Multi-level Consistency Learning for Semi-supervised Domain Adaptation,21,0.181329,0.586545,"Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from
a fully labeled source domain to a scarcely labeled target domain. In this
paper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA.
Specifically, our MCL regularizes the consistency of different views of target
domain samples at three levels: (i) at inter-domain level, we robustly and
accurately align the source and target domains using a prototype-based optimal
transport method that utilizes the pros and cons of different views of target
samples; (ii) at intra-domain level, we facilitate the learning of both
discriminative and compact target feature representations by proposing a novel
class-wise contrastive clustering loss; (iii) at sample level, we follow
standard practice and improve the prediction accuracy by conducting a
consistency-based self-training. Empirically, we verified the effectiveness of
our MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet,
and Office-Home datasets, and the experimental results demonstrate that our MCL
framework achieves the state-of-the-art performance.",1,1,0,0,1,0,0.749747,5.0,0.743293,29
3220037e-f413-4571-9c73-eb82e62fbc75,Coarse-to-Fine Sparse Sequential Recommendation,12,0.176661,0.44761,"Sequential recommendation aims to model dynamic user behavior from historical
interactions. Self-attentive methods have proven effective at capturing
short-term dynamics and long-term preferences. Despite their success, these
approaches still struggle to model sparse data, on which they struggle to learn
high-quality item representations. We propose to model user dynamics from
shopping intents and interacted items simultaneously. The learned intents are
coarse-grained and work as prior knowledge for item recommendation. To this
end, we present a coarse-to-fine self-attention framework, namely CaFe, which
explicitly learns coarse-grained and fine-grained sequential dynamics.
Specifically, CaFe first learns intents from coarse-grained sequences which are
dense and hence provide high-quality user intent representations. Then, CaFe
fuses intent representations into item encoder outputs to obtain improved item
representations. Finally, we infer recommended items based on representations
of items and corresponding intents. Experiments on sparse datasets show that
CaFe outperforms state-of-the-art self-attentive recommenders by 44.03% NDCG@5
on average.",0,1,0,0,1,0,0.824855,8.0,0.86821,27
04e67efb-b515-46cd-b589-73c4f399f698,InstaFormer: Instance-Aware Image-to-Image Translation with Transformer,31,0.471842,0.885141,"We present a novel Transformer-based network architecture for instance-aware
image-to-image translation, dubbed InstaFormer, to effectively integrate
global- and instance-level information. By considering extracted content
features from an image as tokens, our networks discover global consensus of
content features by considering context information through a self-attention
module in Transformers. By augmenting such tokens with an instance-level
feature extracted from the content feature with respect to bounding box
information, our framework is capable of learning an interaction between object
instances and the global image, thus boosting the instance-awareness. We
replace layer normalization (LayerNorm) in standard Transformers with adaptive
instance normalization (AdaIN) to enable a multi-modal translation with style
codes. In addition, to improve the instance-awareness and translation quality
at object regions, we present an instance-level content contrastive loss
defined between input and translated image. We conduct experiments to
demonstrate the effectiveness of our InstaFormer over the latest methods and
provide extensive ablation studies.",0,0,0,0,0,0,0.972248,6.0,0.944528,71
9d58227a-491f-4fb1-8a1d-0bb8bf566535,Training Robots without Robots: Deep Imitation Learning for Master-to-Robot Policy Transfer,9,0.193299,0.750612,"Deep imitation learning is promising for robot manipulation because it only
requires demonstration samples. In this study, deep imitation learning is
applied to tasks that require force feedback. However, existing demonstration
methods have deficiencies; bilateral teleoperation requires a complex control
scheme and is expensive, and kinesthetic teaching suffers from visual
distractions from human intervention. This research proposes a new
master-to-robot (M2R) policy transfer system that does not require robots for
teaching force feedback-based manipulation tasks. The human directly
demonstrates a task using a controller. This controller resembles the kinematic
parameters of the robot arm and uses the same end-effector with force/torque
(F/T) sensors to measure the force feedback. Using this controller, the
operator can feel force feedback without a bilateral system. The proposed
method can overcome domain gaps between the master and robot using gaze-based
imitation learning and a simple calibration method. Furthermore, a Transformer
is applied to infer policy from F/T sensory input. The proposed system was
evaluated on a bottle-cap-opening task that requires force feedback.",0,1,1,0,0,0,0.332519,10.0,0.748483,30
7c65cb6b-6c11-4773-9031-16a13e1d2492,Hero-Gang Neural Model For Named Entity Recognition,7,0.0426462,0.667511,"Named entity recognition (NER) is a fundamental and important task in NLP,
aiming at identifying named entities (NEs) from free text. Recently, since the
multi-head attention mechanism applied in the Transformer model can effectively
capture longer contextual information, Transformer-based models have become the
mainstream methods and have achieved significant performance in this task.
Unfortunately, although these models can capture effective global context
information, they are still limited in the local feature and position
information extraction, which is critical in NER. In this paper, to address
this limitation, we propose a novel Hero-Gang Neural structure (HGN), including
the Hero and Gang module, to leverage both global and local information to
promote NER. Specifically, the Hero module is composed of a Transformer-based
encoder to maintain the advantage of the self-attention mechanism, and the Gang
module utilizes a multi-window recurrent module to extract local features and
position information under the guidance of the Hero module. Afterward, the
proposed multi-window attention effectively combines global information and
multiple local features for predicting entity labels. Experimental results on
several benchmark datasets demonstrate the effectiveness of our proposed model.",1,1,0,0,0,1,0.429204,7.0,0.687433,47
44949f02-3d42-447d-90b9-c7fc5dc28693,Learning Disentangled Representations of Negation and Uncertainty,12,0.199181,0.2153,"Negation and uncertainty modeling are long-standing tasks in natural language
processing. Linguistic theory postulates that expressions of negation and
uncertainty are semantically independent from each other and the content they
modify. However, previous works on representation learning do not explicitly
model this independence. We therefore attempt to disentangle the
representations of negation, uncertainty, and content using a Variational
Autoencoder. We find that simply supervising the latent representations results
in good disentanglement, but auxiliary objectives based on adversarial learning
and mutual information minimization can provide additional disentanglement
gains.",1,0,0,0,0,0,0.317969,9.0,0.714445,63
39bf0807-55f6-4fe4-b30f-1fa3604f6a50,"Theories of ""Gender"" in NLP Bias Research",40,0.423344,0.859482,"The rise of concern around Natural Language Processing (NLP) technologies
containing and perpetuating social biases has led to a rich and rapidly growing
area of research. Gender bias is one of the central biases being analyzed, but
to date there is no comprehensive analysis of how ""gender"" is theorized in the
field. We survey nearly 200 articles concerning gender bias in NLP to discover
how the field conceptualizes gender both explicitly (e.g. through definitions
of terms) and implicitly (e.g. through how gender is operationalized in
practice). In order to get a better idea of emerging trajectories of thought,
we split these articles into two sections by time.
  We find that the majority of the articles do not make their theorization of
gender explicit, even if they clearly define ""bias."" Almost none use a model of
gender that is intersectional or inclusive of nonbinary genders; and many
conflate sex characteristics, social gender, and linguistic gender in ways that
disregard the existence and experience of trans, nonbinary, and intersex
people. There is an increase between the two time-sections in statements
acknowledging that gender is a complicated reality, however, very few articles
manage to put this acknowledgment into practice. In addition to analyzing these
findings, we provide specific recommendations to facilitate interdisciplinary
work, and to incorporate theory and methodology from Gender Studies. Our hope
is that this will produce more inclusive gender bias research in NLP.",0,0,0,0,0,0,0.224445,6.0,0.503472,209
6ddfb2b4-9b7d-44f5-bcb7-7930e150948e,Prompting Is Programming: A Query Language for Large Language Models,42,0.106152,0.741358,"Large language models have demonstrated outstanding performance on a wide
range of tasks such as question answering and code generation. On a high level,
given an input, a language model can be used to automatically complete the
sequence in a statistically-likely way. Based on this, users prompt these
models with language instructions or examples, to implement a variety of
downstream tasks. Advanced prompting methods can even imply interaction between
the language model, a user, and external tools such as calculators. However, to
obtain state-of-the-art performance or adapt language models for specific
tasks, complex task- and model-specific programs have to be implemented, which
may still require ad-hoc interaction.
  Based on this, we present the novel idea of Language Model Programming (LMP).
LMP generalizes language model prompting from pure text prompts to an intuitive
combination of text prompting and scripting. Additionally, LMP allows
constraints to be specified over the language model output. This enables easy
adaption to many tasks while abstracting language model internals and providing
high-level semantics.
  To enable LMP, we implement LMQL(short for Language Model Query Language),
which leverages the constraints and control flow from an LMP prompt to generate
an efficient inference procedure that minimizes the number of expensive calls
to the underlying language model.
  We show that LMQL can capture a wide range of state-of-the-art prompting
methods in an intuitive way, especially facilitating interactive flows that are
challenging to implement with existing high-level APIs. Our evaluation shows
that we retain or increase the accuracy on several downstream tasks, while also
significantly reducing the required amount of computation or cost in the case
of pay-to-use APIs (26-85% cost savings).",1,0,0,0,0,0,0.9231,2.0,0.666309,41
5f3001e5-589e-426d-ad12-0d8d85e0293f,"""Diversity and Uncertainty in Moderation"" are the Key to Data Selection for Multilingual Few-shot Transfer",3,0.00390629,0.121309,"Few-shot transfer often shows substantial gain over zero-shot
transfer~\cite{lauscher2020zero}, which is a practically useful trade-off
between fully supervised and unsupervised learning approaches for multilingual
pretrained model-based systems. This paper explores various strategies for
selecting data for annotation that can result in a better few-shot transfer.
The proposed approaches rely on multiple measures such as data entropy using
$n$-gram language model, predictive entropy, and gradient embedding. We propose
a loss embedding method for sequence labeling tasks, which induces diversity
and uncertainty sampling similar to gradient embedding. The proposed data
selection strategies are evaluated and compared for POS tagging, NER, and NLI
tasks for up to 20 languages. Our experiments show that the gradient and loss
embedding-based strategies consistently outperform random data selection
baselines, with gains varying with the initial performance of the zero-shot
transfer. Furthermore, the proposed method shows similar trends in improvement
even when the model is fine-tuned using a lower proportion of the original
task-specific labeled training data for zero-shot transfer.",0,1,0,0,0,0,0.157051,7.0,0.517655,38
f3ea8a8b-59d8-454d-ae04-9cd8680817af,BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning,10,0.0408966,0.700646,"Current pre-trained language models rely on large datasets for achieving
state-of-the-art performance. However, past research has shown that not all
examples in a dataset are equally important during training. In fact, it is
sometimes possible to prune a considerable fraction of the training set while
maintaining the test performance. Established on standard vision benchmarks,
two gradient-based scoring metrics for finding important examples are GraNd and
its estimated version, EL2N. In this work, we employ these two metrics for the
first time in NLP. We demonstrate that these metrics need to be computed after
at least one epoch of fine-tuning and they are not reliable in early steps.
Furthermore, we show that by pruning a small portion of the examples with the
highest GraNd/EL2N scores, we can not only preserve the test accuracy, but also
surpass it. This paper details adjustments and implementation choices which
enable GraNd and EL2N to be applied to NLP.",0,1,0,0,0,0,0.471766,7.0,0.705926,17
b854bdac-427e-4d55-a647-b0e496d29f73,Forest and Water Bodies Segmentation Through Satellite Images Using U-Net,6,0.168519,0.378105,"Global environment monitoring is a task that requires additional attention in
the contemporary rapid climate change environment. This includes monitoring the
rate of deforestation and areas affected by flooding. Satellite imaging has
greatly helped monitor the earth, and deep learning techniques have helped to
automate this monitoring process. This paper proposes a solution for observing
the area covered by the forest and water. To achieve this task UNet model has
been proposed, which is an image segmentation model. The model achieved a
validation accuracy of 82.55% and 82.92% for the segmentation of areas covered
by forest and water, respectively.",0,1,0,0,0,0,0.339622,5.0,0.50219,14
45a71270-56cd-4526-93fe-4e35d884683a,Lexical Knowledge Internalization for Neural Dialog Generation,1,0.00855588,0.190817,"We propose knowledge internalization (KI), which aims to complement the
lexical knowledge into neural dialog models. Instead of further conditioning
the knowledge-grounded dialog (KGD) models on externally retrieved knowledge,
we seek to integrate knowledge about each input token internally into the
model's parameters. To tackle the challenge due to the large scale of lexical
knowledge, we adopt the contrastive learning approach and create an effective
token-level lexical knowledge retriever that requires only weak supervision
mined from Wikipedia. We demonstrate the effectiveness and general
applicability of our approach on various datasets and diversified model
structures.",1,0,0,0,0,1,0.700434,8.0,0.822174,55
8eb75bac-7b61-4090-9e00-560e6141b7c6,TriBYOL: Triplet BYOL for Self-Supervised Representation Learning,14,0.0868369,0.396943,"This paper proposes a novel self-supervised learning method for learning
better representations with small batch sizes. Many self-supervised learning
methods based on certain forms of the siamese network have emerged and received
significant attention. However, these methods need to use large batch sizes to
learn good representations and require heavy computational resources. We
present a new triplet network combined with a triple-view loss to improve the
performance of self-supervised representation learning with small batch sizes.
Experimental results show that our method can drastically outperform
state-of-the-art self-supervised learning methods on several datasets in
small-batch cases. Our method provides a feasible solution for self-supervised
learning with real-world high-resolution images that uses small batch sizes.",0,1,0,0,1,0,0.816508,7.0,0.845513,31
59d271e4-44fb-4d27-8ad8-364209db6cba,Perturbation Learning Based Anomaly Detection,8,0.0494304,0.316468,"This paper presents a simple yet effective method for anomaly detection. The
main idea is to learn small perturbations to perturb normal data and learn a
classifier to classify the normal data and the perturbed data into two
different classes. The perturbator and classifier are jointly learned using
deep neural networks. Importantly, the perturbations should be as small as
possible but the classifier is still able to recognize the perturbed data from
unperturbed data. Therefore, the perturbed data are regarded as abnormal data
and the classifier provides a decision boundary between the normal data and
abnormal data, although the training data do not include any abnormal data.
Compared with the state-of-the-art of anomaly detection, our method does not
require any assumption about the shape (e.g. hypersphere) of the decision
boundary and has fewer hyper-parameters to determine. Empirical studies on
benchmark datasets verify the effectiveness and superiority of our method.",0,1,0,0,1,1,0.486832,8.0,0.748228,48
b22d3975-0a25-4f44-a97c-15031844dacc,Representative Image Feature Extraction via Contrastive Learning Pretraining for Chest X-ray Report Generation,8,0.0189815,0.226424,"Medical report generation is a challenging task since it is time-consuming
and requires expertise from experienced radiologists. The goal of medical
report generation is to accurately capture and describe the image findings.
Previous works pretrain their visual encoding neural networks with large
datasets in different domains, which cannot learn general visual representation
in the specific medical domain. In this work, we propose a medical report
generation framework that uses a contrastive learning approach to pretrain the
visual encoder and requires no additional meta information. In addition, we
adopt lung segmentation as an augmentation method in the contrastive learning
framework. This segmentation guides the network to focus on encoding the visual
feature within the lung region. Experimental results show that the proposed
framework improves the performance and the quality of the generated medical
reports both quantitatively and qualitatively.",0,1,0,0,0,0,0.122891,9.0,0.595439,25
6cf056c0-65d7-4003-9c08-974ed5a369b8,Content and Style Aware Generation of Text-line Images for Handwriting Recognition,20,0.269176,0.790069,"Handwritten Text Recognition has achieved an impressive performance in public
benchmarks. However, due to the high inter- and intra-class variability between
handwriting styles, such recognizers need to be trained using huge volumes of
manually labeled training data. To alleviate this labor-consuming problem,
synthetic data produced with TrueType fonts has been often used in the training
loop to gain volume and augment the handwriting style variability. However,
there is a significant style bias between synthetic and real data which hinders
the improvement of recognition performance. To deal with such limitations, we
propose a generative method for handwritten text-line images, which is
conditioned on both visual appearance and textual content. Our method is able
to produce long text-line samples with diverse handwriting styles. Once
properly trained, our method can also be adapted to new target data by only
accessing unlabeled text-line images to mimic handwritten styles and produce
images with any textual content. Extensive experiments have been done on making
use of the generated samples to boost Handwritten Text Recognition performance.
Both qualitative and quantitative results demonstrate that the proposed
approach outperforms the current state of the art.",1,1,0,1,1,0,0.511574,9.0,0.784142,50
1be27df6-6c17-4b7f-856c-3ff375f48ff6,Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation,19,0.51322,0.804774,"We present a simple yet effective self-supervised framework for audio-visual
representation learning, to localize the sound source in videos. To understand
what enables to learn useful representations, we systematically investigate the
effects of data augmentations, and reveal that (1) composition of data
augmentations plays a critical role, i.e. explicitly encouraging the
audio-visual representations to be invariant to various transformations~({\em
transformation invariance}); (2) enforcing geometric consistency substantially
improves the quality of learned representations, i.e. the detected sound source
should follow the same transformation applied on input video frames~({\em
transformation equivariance}). Extensive experiments demonstrate that our model
significantly outperforms previous methods on two sound localization
benchmarks, namely, Flickr-SoundNet and VGG-Sound. Additionally, we also
evaluate audio retrieval and cross-modal retrieval tasks. In both cases, our
self-supervised models demonstrate superior retrieval performances, even
competitive with the supervised approach in audio retrieval. This reveals the
proposed framework learns strong multi-modal representations that are
beneficial to sound localisation and generalization to further applications.
\textit{All codes will be available}.",1,1,0,0,1,0,0.960721,6.0,0.927538,55
623a1f26-ff48-4dc0-b7e6-08c7a99ca387,Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction,9,0.107615,0.582419,"We present Claim-Dissector: a novel latent variable model for fact-checking
and analysis, which given a claim and a set of retrieved evidences jointly
learns to identify: (i) the relevant evidences to the given claim, (ii) the
veracity of the claim. We propose to disentangle the per-evidence relevance
probability and its contribution to the final veracity probability in an
interpretable way -- the final veracity probability is proportional to a linear
ensemble of per-evidence relevance probabilities. In this way, the individual
contributions of evidences towards the final predicted probability can be
identified. In per-evidence relevance probability, our model can further
distinguish whether each relevant evidence is supporting (S) or refuting (R)
the claim. This allows to quantify how much the S/R probability contributes to
the final verdict or to detect disagreeing evidence.
  Despite its interpretable nature, our system achieves results competitive
with state-of-the-art on the FEVER dataset, as compared to typical two-stage
system pipelines, while using significantly fewer parameters. It also sets new
state-of-the-art on FAVIQ and RealFC datasets. Furthermore, our analysis shows
that our model can learn fine-grained relevance cues while using coarse-grained
supervision, and we demonstrate it in 2 ways. (i) We show that our model can
achieve competitive sentence recall while using only paragraph-level relevance
supervision. (ii) Traversing towards the finest granularity of relevance, we
show that our model is capable of identifying relevance at the token level. To
do this, we present a new benchmark TLR-FEVER focusing on token-level
interpretability -- humans annotate tokens in relevant evidences they
considered essential when making their judgment. Then we measure how similar
are these annotations to the tokens our model is focusing on.",0,1,0,0,1,0,0.491731,7.0,0.7143,67
7ac59ec9-8e3f-4616-96e8-9f07901e8a80,Topic-Aware Response Generation in Task-Oriented Dialogue with Unstructured Knowledge Access,2,0.0365877,0.362449,"To alleviate the problem of structured databases' limited coverage, recent
task-oriented dialogue systems incorporate external unstructured knowledge to
guide the generation of system responses. However, these usually use word or
sentence level similarities to detect the relevant knowledge context, which
only partially capture the topical level relevance. In this paper, we examine
how to better integrate topical information in knowledge grounded task-oriented
dialogue and propose ``Topic-Aware Response Generation'' (TARG), an end-to-end
response generation model. TARG incorporates multiple topic-aware attention
mechanisms to derive the importance weighting scheme over dialogue utterances
and external knowledge sources towards a better understanding of the dialogue
history. Experimental results indicate that TARG achieves state-of-the-art
performance in knowledge selection and response generation, outperforming
previous state-of-the-art by 3.2, 3.6, and 4.2 points in EM, F1 and BLEU-4
respectively on Doc2Dial, and performing comparably with previous work on
DSTC9; both being knowledge-grounded task-oriented dialogue datasets.",1,1,0,0,1,0,0.450802,7.0,0.696938,54
8e03f137-8f36-4118-92d7-74fc508c301b,Explanations as Programs in Probabilistic Logic Programming,1,0.0214916,0.208357,"The generation of comprehensible explanations is an essential feature of
modern artificial intelligence systems. In this work, we consider probabilistic
logic programming, an extension of logic programming which can be useful to
model domains with relational structure and uncertainty. Essentially, a program
specifies a probability distribution over possible worlds (i.e., sets of
facts). The notion of explanation is typically associated with that of a world,
so that one often looks for the most probable world as well as for the worlds
where the query is true. Unfortunately, such explanations exhibit no causal
structure. In particular, the chain of inferences required for a specific
prediction (represented by a query) is not shown. In this paper, we propose a
novel approach where explanations are represented as programs that are
generated from a given query by a number of unfolding-like transformations.
Here, the chain of inferences that proves a given query is made explicit.
Furthermore, the generated explanations are minimal (i.e., contain no
irrelevant information) and can be parameterized w.r.t. a specification of
visible predicates, so that the user may hide uninteresting details from
explanations.",1,0,0,0,0,0,0.0263196,23.0,0.772453,36
f90b3717-3654-4bfc-910a-7363b8e98200,Cross-Modal ASR Post-Processing System for Error Correction and Utterance Rejection,5,0.131992,0.420659,"Although modern automatic speech recognition (ASR) systems can achieve high
performance, they may produce errors that weaken readers' experience and do
harm to downstream tasks. To improve the accuracy and reliability of ASR
hypotheses, we propose a cross-modal post-processing system for speech
recognizers, which 1) fuses acoustic features and textual features from
different modalities, 2) joints a confidence estimator and an error corrector
in multi-task learning fashion and 3) unifies error correction and utterance
rejection modules. Compared with single-modal or single-task models, our
proposed system is proved to be more effective and efficient. Experiment result
shows that our post-processing system leads to more than 10% relative reduction
of character error rate (CER) for both single-speaker and multi-speaker speech
on our industrial ASR system, with about 1.7ms latency for each token, which
ensures that extra latency introduced by post-processing is acceptable in
streaming speech recognition.",0,1,0,0,0,1,0.46409,5.0,0.583724,34
439f565f-5545-4fad-bed2-2867340375b9,Neural Additive Models for Nowcasting,3,0.0446522,0.145453,"Deep neural networks (DNNs) are one of the most highlighted methods in
machine learning. However, as DNNs are black-box models, they lack explanatory
power for their predictions. Recently, neural additive models (NAMs) have been
proposed to provide this power while maintaining high prediction performance.
In this paper, we propose a novel NAM approach for multivariate nowcasting (NC)
problems, which comprise an important focus area of machine learning. For the
multivariate time-series data used in NC problems, explanations should be
considered for every input value to the variables at distinguishable time
steps. By employing generalized additive models, the proposed NAM-NC
successfully explains each input value's importance for multiple variables and
time steps. Experimental results involving a toy example and two real-world
datasets show that the NAM-NC predicts multivariate time-series data as
accurately as state-of-the-art neural networks, while also providing the
explanatory importance of each input value. We also examine parameter-sharing
networks using NAM-NC to decrease their complexity, and NAM-MC's hard-tied
feature net extracted explanations with good performance.",0,0,0,0,1,0,0.860089,10.0,0.906695,41
87733d67-b369-4330-9ba5-9c08003e2e69,Fast Population-Based Reinforcement Learning on a Single Machine,6,0.0503528,0.273628,"Training populations of agents has demonstrated great promise in
Reinforcement Learning for stabilizing training, improving exploration and
asymptotic performance, and generating a diverse set of solutions. However,
population-based training is often not considered by practitioners as it is
perceived to be either prohibitively slow (when implemented sequentially), or
computationally expensive (if agents are trained in parallel on independent
accelerators). In this work, we compare implementations and revisit previous
studies to show that the judicious use of compilation and vectorization allows
population-based training to be performed on a single machine with one
accelerator with minimal overhead compared to training a single agent. We also
show that, when provided with a few accelerators, our protocols extend to large
population sizes for applications such as hyperparameter tuning. We hope that
this work and the public release of our code will encourage practitioners to
use population-based learning more frequently for their research and
applications.",1,1,0,0,0,1,0.723518,7.0,0.805971,33
42532919-0e90-4b27-8326-16aa653e5751,Thutmose Tagger: Single-pass neural model for Inverse Text Normalization,2,0.0266736,0.0240769,"Inverse text normalization (ITN) is an essential post-processing step in
automatic speech recognition (ASR). It converts numbers, dates, abbreviations,
and other semiotic classes from the spoken form generated by ASR to their
written forms. One can consider ITN as a Machine Translation task and use
neural sequence-to-sequence models to solve it. Unfortunately, such neural
models are prone to hallucinations that could lead to unacceptable errors. To
mitigate this issue, we propose a single-pass token classifier model that
regards ITN as a tagging task. The model assigns a replacement fragment to
every input token or marks it for deletion or copying without changes. We
present a dataset preparation method based on the granular alignment of ITN
examples. The proposed model is less prone to hallucination errors. The model
is trained on the Google Text Normalization dataset and achieves
state-of-the-art sentence accuracy on both English and Russian test sets.
One-to-one correspondence between tags and input words improves the
interpretability of the model's predictions, simplifies debugging, and allows
for post-processing corrections. The model is simpler than sequence-to-sequence
models and easier to optimize in production settings. The model and the code to
prepare the dataset is published as part of NeMo project.",1,1,0,0,1,0,0.426311,7.0,0.68614,19
3b699b22-ad4a-4b50-b48f-18c410000dc8,Learning to Classify Open Intent via Soft Labeling and Manifold Mixup,8,0.0988229,0.332792,"Open intent classification is a practical yet challenging task in dialogue
systems. Its objective is to accurately classify samples of known intents while
at the same time detecting those of open (unknown) intents. Existing methods
usually use outlier detection algorithms combined with K-class classifier to
detect open intents, where K represents the class number of known intents.
Different from them, in this paper, we consider another way without using
outlier detection algorithms. Specifically, we directly train a (K+1)-class
classifier for open intent classification, where the (K+1)-th class represents
open intents. To address the challenge that training a (K+1)-class classifier
with training samples of only K classes, we propose a deep model based on Soft
Labeling and Manifold Mixup (SLMM). In our method, soft labeling is used to
reshape the label distribution of the known intent samples, aiming at reducing
model's overconfident on known intents. Manifold mixup is used to generate
pseudo samples for open intents, aiming at well optimizing the decision
boundary of open intents. Experiments on four benchmark datasets demonstrate
that our method outperforms previous methods and achieves state-of-the-art
performance. All the code and data of this work can be obtained at
https://github.com/zifengcheng/SLMM.",1,1,0,0,1,0,0.659602,6.0,0.744223,40
0e07b8bb-57e5-4deb-9072-6cf613dc2caa,Danish Airs and Grounds: A Dataset for Aerial-to-Street-Level Place Recognition and Localization,6,0.426437,0.736403,"Place recognition and visual localization are particularly challenging in
wide baseline configurations. In this paper, we contribute with the
\emph{Danish Airs and Grounds} (DAG) dataset, a large collection of
street-level and aerial images targeting such cases. Its main challenge lies in
the extreme viewing-angle difference between query and reference images with
consequent changes in illumination and perspective. The dataset is larger and
more diverse than current publicly available data, including more than 50 km of
road in urban, suburban and rural areas. All images are associated with
accurate 6-DoF metadata that allows the benchmarking of visual localization
methods.
  We also propose a map-to-image re-localization pipeline, that first estimates
a dense 3D reconstruction from the aerial images and then matches query
street-level images to street-level renderings of the 3D model. The dataset can
be downloaded at: https://frederikwarburg.github.io/DAG",1,1,1,1,0,0,0.950426,8.0,0.936326,50
c6ff622c-532e-44ce-b535-cd6e46671149,LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging,28,0.308295,0.433805,"We present LINGUIST, a method for generating annotated data for Intent
Classification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a
5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a
flexible instruction prompt. In a 10-shot novel intent setting for the SNIPS
dataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation and
Example Extrapolation) by a wide margin, showing absolute improvement for the
target intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. In
the zero-shot cross-lingual setting of the mATIS++ dataset, LINGUIST
out-performs a strong baseline of Machine Translation with Slot Alignment by
+4.14 points absolute on ST F1 Score across 6 languages, while matching
performance on IC. Finally, we verify our results on an internal large-scale
multilingual dataset for conversational agent IC+ST and show significant
improvements over a baseline which uses Back-Translation, Paraphrasing and Slot
Catalog Resampling. To our knowledge, we are the first to demonstrate
instruction fine-tuning of a large-scale seq2seq model to control the outputs
of multilingual intent- and slot-labeled data generation.",0,1,0,0,1,0,0.670496,6.0,0.749179,55
44a569aa-c721-4347-89e7-cbbaa998df5b,Pushing the limits of fairness impossibility: Who's the fairest of them all?,6,0.134591,0.36658,"The impossibility theorem of fairness is a foundational result in the
algorithmic fairness literature. It states that outside of special cases, one
cannot exactly and simultaneously satisfy all three common and intuitive
definitions of fairness - demographic parity, equalized odds, and predictive
rate parity. This result has driven most works to focus on solutions for one or
two of the metrics. Rather than follow suit, in this paper we present a
framework that pushes the limits of the impossibility theorem in order to
satisfy all three metrics to the best extent possible. We develop an
integer-programming based approach that can yield a certifiably optimal
post-processing method for simultaneously satisfying multiple fairness criteria
under small violations. We show experiments demonstrating that our
post-processor can improve fairness across the different definitions
simultaneously with minimal model performance reduction. We also discuss
applications of our framework for model selection and fairness explainability,
thereby attempting to answer the question: who's the fairest of them all?",1,0,0,0,0,0,0.437805,13.0,0.83375,45
f7714a45-667f-48c5-b683-55e30cb8f7b3,Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation,57,0.674846,0.898629,"Optimizing behaviors for dexterous manipulation has been a longstanding
challenge in robotics, with a variety of methods from model-based control to
model-free reinforcement learning having been previously explored in
literature. Perhaps one of the most powerful techniques to learn complex
manipulation strategies is imitation learning. However, collecting and learning
from demonstrations in dexterous manipulation is quite challenging. The
complex, high-dimensional action-space involved with multi-finger control often
leads to poor sample efficiency of learning-based methods. In this work, we
propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning
framework for dexterous manipulation. DIME only requires a single RGB camera to
observe a human operator and teleoperate our robotic hand. Once demonstrations
are collected, DIME employs standard imitation learning methods to train
dexterous manipulation policies. On both simulation and real robot benchmarks
we demonstrate that DIME can be used to solve complex, in-hand manipulation
tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro
hand. Our framework along with pre-collected demonstrations is publicly
available at https://nyu-robot-learning.github.io/dime.",1,1,0,0,0,0,0.741906,10.0,0.869394,61
9b342689-0a9c-434d-864f-826a28a317d8,Training language models to follow instructions with human feedback,6131,1.0,1.0,"Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent.",0,1,0,0,0,1,0.566304,5.0,0.642146,83
34dfd8c2-c760-4ddd-a8a5-70e49370699d,Mapping Husserlian phenomenology onto active inference,7,0.0237299,0.612881,"Phenomenology is the rigorous descriptive study of conscious experience.
Recent attempts to formalize Husserlian phenomenology provide us with a
mathematical model of perception as a function of prior knowledge and
expectation. In this paper, we re-examine elements of Husserlian phenomenology
through the lens of active inference. In doing so, we aim to advance the
project of computational phenomenology, as recently outlined by proponents of
active inference. We propose that key aspects of Husserl's descriptions of
consciousness can be mapped onto aspects of the generative models associated
with the active inference approach. We first briefly review active inference.
We then discuss Husserl's phenomenology, with a focus on time consciousness.
Finally, we present our mapping from Husserlian phenomenology to active
inference.",0,0,0,0,0,0,0.0303495,9.0,0.434551,53
1851d127-31f3-4a1d-9323-3f6c90d81077,"""I think this is the most disruptive technology"": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data",129,0.425382,0.999993,"Large language models have recently attracted significant attention due to
their impressive performance on a variety of tasks. ChatGPT developed by OpenAI
is one such implementation of a large, pre-trained language model that has
gained immense popularity among early adopters, where certain users go to the
extent of characterizing it as a disruptive technology in many domains.
Understanding such early adopters' sentiments is important because it can
provide insights into the potential success or failure of the technology, as
well as its strengths and weaknesses. In this paper, we conduct a mixed-method
study using 10,732 tweets from early ChatGPT users. We first use topic
modelling to identify the main topics and then perform an in-depth qualitative
sentiment analysis of each topic. Our results show that the majority of the
early adopters have expressed overwhelmingly positive sentiments related to
topics such as Disruptions to software development, Entertainment and
exercising creativity. Only a limited percentage of users expressed concerns
about issues such as the potential for misuse of ChatGPT, especially regarding
topics such as Impact on educational aspects. We discuss these findings by
providing specific examples for each topic and then detail implications related
to addressing these concerns for both researchers and users.",0,1,0,0,0,0,0.0029209,14.0,0.468297,34
29e63dd7-976f-4c57-8977-1fd23b2c1c18,Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts,41,0.263432,0.455361,"Previous work has shown that there exists a scaling law between the size of
Language Models (LMs) and their zero-shot performance on different downstream
NLP tasks. In this work, we show that this phenomenon does not hold when
evaluating large LMs on tasks with negated prompts, but instead shows an
inverse scaling law. We evaluate 9 different tasks with negated prompts on (1)
pretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further
pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with
few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all
LM types perform worse on negated prompts as they scale and show a huge
performance gap between the human performance when comparing the average score
on both original and negated prompts. By highlighting a critical limitation of
existing LMs and methods, we urge the community to develop new approaches of
developing LMs that actually follow the given instructions. We provide the code
and the datasets to explore negated prompts at
https://github.com/joeljang/negated-prompts-for-llms",0,0,0,0,0,0,0.985058,3.0,0.942166,26
33cffb09-a51a-4892-94a3-3e2a2e45f169,Multi-task Active Learning for Pre-trained Transformer-based Models,13,0.224132,0.338477,"Multi-task learning, in which several tasks are jointly learned by a single
model, allows NLP models to share information from multiple annotations and may
facilitate better predictions when the tasks are inter-related. This technique,
however, requires annotating the same text with multiple annotation schemes
which may be costly and laborious. Active learning (AL) has been demonstrated
to optimize annotation processes by iteratively selecting unlabeled examples
whose annotation is most valuable for the NLP model. Yet, multi-task active
learning (MT-AL) has not been applied to state-of-the-art pre-trained
Transformer-based NLP models. This paper aims to close this gap. We explore
various multi-task selection criteria in three realistic multi-task scenarios,
reflecting different relations between the participating tasks, and demonstrate
the effectiveness of multi-task compared to single-task selection. Our results
suggest that MT-AL can be effectively used in order to minimize annotation
efforts for multi-task NLP models.",1,1,0,0,0,0,0.514995,9.0,0.785227,68
73c8b1d4-5dd3-4a31-864f-8b9739cf49b6,Dual Mechanism Priming Effects in Hindi Word Order,1,0.0451644,0.0607341,"Word order choices during sentence production can be primed by preceding
sentences. In this work, we test the DUAL MECHANISM hypothesis that priming is
driven by multiple different sources. Using a Hindi corpus of text productions,
we model lexical priming with an n-gram cache model and we capture more
abstract syntactic priming with an adaptive neural language model. We permute
the preverbal constituents of corpus sentences, and then use a logistic
regression model to predict which sentences actually occurred in the corpus
against artificially generated meaning-equivalent variants. Our results
indicate that lexical priming and lexically-independent syntactic priming
affect complementary sets of verb classes. By showing that different priming
influences are separable from one another, our results support the hypothesis
that multiple different cognitive mechanisms underlie priming.",0,0,0,0,0,0,0.00199736,26.0,0.699063,73
1dd7c115-4890-431b-aea5-d833149b7ec0,Applying a Generic Sequence-to-Sequence Model for Simple and Effective Keyphrase Generation,10,0.201308,0.106235,"In recent years, a number of keyphrase generation (KPG) approaches were
proposed consisting of complex model architectures, dedicated training
paradigms and decoding strategies. In this work, we opt for simplicity and show
how a commonly used seq2seq language model, BART, can be easily adapted to
generate keyphrases from the text in a single batch computation using a simple
training procedure. Empirical results on five benchmarks show that our approach
is as good as the existing state-of-the-art KPG systems, but using a much
simpler and easy to deploy framework.",0,1,0,0,1,0,0.673492,5.0,0.700653,32
77b3ed6f-a428-4310-945b-a6b8c74138b0,External Knowledge Selection with Weighted Negative Sampling in Knowledge-grounded Task-oriented Dialogue Systems,4,0.0144363,0.266293,"Constructing a robust dialogue system on spoken conversations bring more
challenge than written conversation. In this respect, DSTC10-Track2-Task2 is
proposed, which aims to build a task-oriented dialogue (TOD) system
incorporating unstructured external knowledge on a spoken conversation,
extending DSTC9-Track1. This paper introduces our system containing four
advanced methods: data construction, weighted negative sampling, post-training,
and style transfer. We first automatically construct a large training data
because DSTC10-Track2 does not release the official training set. For the
knowledge selection task, we propose weighted negative sampling to train the
model more fine-grained manner. We also employ post-training and style transfer
for the response generation task to generate an appropriate response with a
similar style to the target response. In the experiment, we investigate the
effect of weighted negative sampling, post-training, and style transfer. Our
model ranked 7 out of 16 teams in the objective evaluation and 6 in human
evaluation.",0,1,0,0,0,0,0.635895,7.0,0.771546,33
d10402f5-bf3d-442a-9543-e18fdec364f8,Contrastive Demonstration Tuning for Pre-trained Language Models,9,0.103529,0.0928504,"Pretrained language models can be effectively stimulated by textual prompts
or demonstrations, especially in low-data scenarios. Recent works have focused
on automatically searching discrete or continuous prompts or optimized
verbalizers, yet studies for the demonstration are still limited. Concretely,
the demonstration examples are crucial for an excellent final performance of
prompt-tuning. In this paper, we propose a novel pluggable, extensible, and
efficient approach named contrastive demonstration tuning, which is free of
demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged
into any previous prompt-tuning approaches; (ii) Extended to widespread
classification tasks with a large number of categories. Experimental results on
16 datasets illustrate that our method integrated with previous approaches
LM-BFF and P-tuning can yield better performance. Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.",1,1,0,0,0,0,0.951484,3.0,0.832587,76
9e68eb1d-21f9-44bf-9b8e-2b2ad20e019c,FairGBM: Gradient Boosting with Fairness Constraints,11,0.664994,0.931546,"Tabular data is prevalent in many high-stakes domains, such as financial
services or public policy. Gradient Boosted Decision Trees (GBDT) are popular
in these settings due to their scalability, performance, and low training cost.
While fairness in these domains is a foremost concern, existing in-processing
Fair ML methods are either incompatible with GBDT, or incur in significant
performance losses while taking considerably longer to train. We present
FairGBM, a dual ascent learning framework for training GBDT under fairness
constraints, with little to no impact on predictive performance when compared
to unconstrained GBDT. Since observational fairness metrics are
non-differentiable, we propose smooth convex error rate proxies for common
fairness criteria, enabling gradient-based optimization using a
``proxy-Lagrangian'' formulation. Our implementation shows an order of
magnitude speedup in training time relative to related work, a pivotal aspect
to foster the widespread adoption of FairGBM by real-world practitioners.",1,1,0,0,0,0,0.946714,10.0,0.946628,55
10e346c8-ff5c-42c0-aeb4-4b94d9eabf2d,SKIPP'D: a SKy Images and Photovoltaic Power Generation Dataset for Short-term Solar Forecasting,5,0.106342,0.62498,"Large-scale integration of photovoltaics (PV) into electricity grids is
challenged by the intermittent nature of solar power. Sky-image-based solar
forecasting using deep learning has been recognized as a promising approach to
predicting the short-term fluctuations. However, there are few publicly
available standardized benchmark datasets for image-based solar forecasting,
which limits the comparison of different forecasting models and the exploration
of forecasting methods. To fill these gaps, we introduce SKIPP'D -- a SKy
Images and Photovoltaic Power Generation Dataset. The dataset contains three
years (2017-2019) of quality-controlled down-sampled sky images and PV power
generation data that is ready-to-use for short-term solar forecasting using
deep learning. In addition, to support the flexibility in research, we provide
the high resolution, high frequency sky images and PV power generation data as
well as the concurrent sky video footage. We also include a code base
containing data processing scripts and baseline model implementations for
researchers to reproduce our previous work and accelerate their research in
solar forecasting.",1,1,1,1,0,0,0.299855,6.0,0.559842,28
5d8b0570-dc65-4d11-8c6f-caf7b1391c71,A Close Look into the Calibration of Pre-trained Language Models,22,0.0,0.846799,"Pre-trained language models (PLMs) may fail in giving reliable estimates of
their predictive uncertainty. We take a close look into this problem, aiming to
answer two questions: (1) Do PLMs learn to become calibrated in the training
process? (2) How effective are existing calibration methods? For the first
question, we conduct fine-grained control experiments to study the dynamic
change in PLMs' calibration performance in training. We consider six factors as
control variables, including dataset difficulty, available training samples,
training steps, the number of tunable parameters, model scale, and pretraining.
We observe a consistent change in calibration performance across six factors.
We find that PLMs don't learn to become calibrated in training, evidenced by
the continual increase in confidence, no matter whether the predictions are
correct or not. We highlight that our finding somewhat contradicts two
established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining
improves model calibration. Next, we study the effectiveness of existing
calibration methods in mitigating the overconfidence issue. Besides unlearnable
calibration methods (e.g., label smoothing), we adapt and extend two recently
proposed learnable methods that directly collect data to train models to have
reasonable confidence estimations. Experimental results show that learnable
methods significantly reduce PLMs' confidence in wrong predictions. The code is
available at \url{https://github.com/lifan-yuan/PLMCalibration}.",1,0,0,0,0,0,0.333766,6.0,0.581574,68
a285f49d-2024-4aa1-9b94-91760efcc662,VU-BERT: A Unified framework for Visual Dialog,4,0.0817487,0.0658396,"The visual dialog task attempts to train an agent to answer multi-turn
questions given an image, which requires the deep understanding of interactions
between the image and dialog history. Existing researches tend to employ the
modality-specific modules to model the interactions, which might be troublesome
to use. To fill in this gap, we propose a unified framework for image-text
joint embedding, named VU-BERT, and apply patch projection to obtain vision
embedding firstly in visual dialog tasks to simplify the model. The model is
trained over two tasks: masked language modeling and next utterance retrieval.
These tasks help in learning visual concepts, utterances dependence, and the
relationships between these two modalities. Finally, our VU-BERT achieves
competitive performance (0.7287 NDCG scores) on VisDial v1.0 Datasets.",0,0,0,0,0,0,0.889554,5.0,0.836097,20
6b57ee63-d156-4314-b93c-8fa7e23127b1,SPA-VAE: Similar-Parts-Assignment for Unsupervised 3D Point Cloud Generation,1,0.0392015,0.112796,"This paper addresses the problem of unsupervised parts-aware point cloud
generation with learned parts-based self-similarity. Our SPA-VAE infers a set
of latent canonical candidate shapes for any given object, along with a set of
rigid body transformations for each such candidate shape to one or more
locations within the assembled object. In this way, noisy samples on the
surface of, say, each leg of a table, are effectively combined to estimate a
single leg prototype. When parts-based self-similarity exists in the raw data,
sharing data among parts in this way confers numerous advantages: modeling
accuracy, appropriately self-similar generative outputs, precise in-filling of
occlusions, and model parsimony. SPA-VAE is trained end-to-end using a
variational Bayesian approach which uses the Gumbel-softmax trick for the
shared part assignments, along with various novel losses to provide appropriate
inductive biases. Quantitative and qualitative analyses on ShapeNet demonstrate
the advantage of SPA-VAE.",0,0,0,0,0,0,0.918301,6.0,0.88479,88
a5368b10-d909-4ed3-bfd4-8e6bd2ae9661,Text2LIVE: Text-Driven Layered Image and Video Editing,214,0.945574,0.999996,"We present a method for zero-shot, text-driven appearance manipulation in
natural images and videos. Given an input image or video and a target text
prompt, our goal is to edit the appearance of existing objects (e.g., object's
texture) or augment the scene with visual effects (e.g., smoke, fire) in a
semantically meaningful manner. We train a generator using an internal dataset
of training examples, extracted from a single input (image or video and target
text prompt), while leveraging an external pre-trained CLIP model to establish
our losses. Rather than directly generating the edited output, our key idea is
to generate an edit layer (color+opacity) that is composited over the original
input. This allows us to constrain the generation process and maintain high
fidelity to the original input via novel text-driven losses that are applied
directly to the edit layer. Our method neither relies on a pre-trained
generator nor requires user-provided edit masks. We demonstrate localized,
semantic edits on high-resolution natural images and videos across a variety of
objects and scenes.",0,1,0,0,0,0,0.948341,4.0,0.869199,57
53700508-a61e-43a9-9e9a-13388410176b,"Re2G: Retrieve, Rerank, Generate",41,0.206698,0.85879,"As demonstrated by GPT-3 and T5, transformers grow in capability as parameter
spaces become larger and larger. However, for tasks that require a large amount
of knowledge, non-parametric memory allows models to grow dramatically with a
sub-linear increase in computational cost and GPU memory requirements. Recent
models such as RAG and REALM have introduced retrieval into conditional
generation. These models incorporate neural initial retrieval from a corpus of
passages. We build on this line of research, proposing Re2G, which combines
both neural initial retrieval and reranking into a BART-based
sequence-to-sequence generation. Our reranking approach also permits merging
retrieval results from sources with incomparable scores, enabling an ensemble
of BM25 and neural initial retrieval. To train our system end-to-end, we
introduce a novel variation of knowledge distillation to train the initial
retrieval, reranker, and generation using only ground truth on the target
sequence output. We find large gains in four diverse tasks: zero-shot slot
filling, question answering, fact-checking, and dialog, with relative gains of
9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make
our code available as open source at
https://github.com/IBM/kgi-slot-filling/tree/re2g.",0,1,0,0,1,1,0.792038,6.0,0.806982,55
7292e902-81aa-4b27-ba0e-e295744d1ff7,CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models,5,0.192875,0.461353,"We motivate and introduce CHARD: Clinical Health-Aware Reasoning across
Dimensions, to investigate the capability of text generation models to act as
implicit clinical knowledge bases and generate free-flow textual explanations
about various health-related conditions across several dimensions. We collect
and present an associated dataset, CHARDat, consisting of explanations about 52
health conditions across three clinical dimensions. We conduct extensive
experiments using BART and T5 along with data augmentation, and perform
automatic, human, and qualitative analyses. We show that while our models can
perform decently, CHARD is very challenging with strong potential for further
exploration.",1,1,1,1,0,0,0.80204,4.0,0.7182,45
5a51f404-3bc2-4eb4-8d32-35a2ced7559e,Learning logic programs by combining programs,5,0.0669381,0.62766,"The goal of inductive logic programming is to induce a logic program (a set
of logical rules) that generalises training examples. Inducing programs with
many rules and literals is a major challenge. To tackle this challenge, we
introduce an approach where we learn small non-separable programs and combine
them. We implement our approach in a constraint-driven ILP system. Our approach
can learn optimal and recursive programs and perform predicate invention. Our
experiments on multiple domains, including game playing and program synthesis,
show that our approach can drastically outperform existing approaches in terms
of predictive accuracies and learning times, sometimes reducing learning times
from over an hour to a few seconds.",1,0,0,0,0,0,0.046934,10.0,0.535549,80
3c489e5a-78cb-4017-a005-598ff2a5199f,CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection,9,0.238851,0.568435,"Video anomaly detection (VAD) -- commonly formulated as a multiple-instance
learning problem in a weakly-supervised manner due to its labor-intensive
nature -- is a challenging problem in video surveillance where the frames of
anomaly need to be localized in an untrimmed video. In this paper, we first
propose to utilize the ViT-encoded visual features from CLIP, in contrast with
the conventional C3D or I3D features in the domain, to efficiently extract
discriminative representations in the novel technique. We then model temporal
dependencies and nominate the snippets of interest by leveraging our proposed
Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of
TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA
outperforms the existing state-of-the-art (SOTA) methods by a large margin on
three commonly-used benchmark datasets in the VAD problem (UCF-Crime,
ShanghaiTech Campus, and XD-Violence). Our source code is available at
https://github.com/joos2010kj/CLIP-TSA.",1,1,0,0,1,0,0.777515,6.0,0.799659,105
8d552aed-21f8-4b3b-8104-fbed686c3118,Do Language Models Learn Position-Role Mappings?,2,0.0748644,0.18696,"How is knowledge of position-role mappings in natural language learned? We
explore this question in a computational setting, testing whether a variety of
well-performing pertained language models (BERT, RoBERTa, and DistilBERT)
exhibit knowledge of these mappings, and whether this knowledge persists across
alternations in syntactic, structural, and lexical alternations. In Experiment
1, we show that these neural models do indeed recognize distinctions between
theme and recipient roles in ditransitive constructions, and that these
distinct patterns are shared across construction type. We strengthen this
finding in Experiment 2 by showing that fine-tuning these language models on
novel theme- and recipient-like tokens in one paradigm allows the models to
make correct predictions about their placement in other paradigms, suggesting
that the knowledge of these mappings is shared rather than independently
learned. We do, however, observe some limitations of this generalization when
tasks involve constructions with novel ditransitive verbs, hinting at a degree
of lexical specificity which underlies model performance.",0,0,0,0,0,0,0.809994,10.0,0.88978,20
07043207-6a7e-4449-b4da-5f80c59d69c6,Controlling Bias Exposure for Fair Interpretable Predictions,15,0.344136,0.799448,"Recent work on reducing bias in NLP models usually focuses on protecting or
isolating information related to a sensitive attribute (like gender or race).
However, when sensitive information is semantically entangled with the task
information of the input, e.g., gender information is predictive for a
profession, a fair trade-off between task performance and bias mitigation is
difficult to achieve. Existing approaches perform this trade-off by eliminating
bias information from the latent space, lacking control over how much bias is
necessarily required to be removed. We argue that a favorable debiasing method
should use sensitive information 'fairly', rather than blindly eliminating it
(Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we
provide a novel debiasing algorithm by adjusting the predictive model's belief
to (1) ignore the sensitive information if it is not useful for the task; (2)
use sensitive information minimally as necessary for the prediction (while also
incurring a penalty). Experimental results on two text classification tasks
(influenced by gender) and an open-ended generation task (influenced by race)
indicate that our model achieves a desirable trade-off between debiasing and
task performance along with producing debiased rationales as evidence.",1,0,0,0,0,0,0.850423,8.0,0.879048,40
94db08d1-35be-4b42-bbe4-17370f526064,A Structure-guided Effective and Temporal-lag Connectivity Network for Revealing Brain Disorder Mechanisms,5,0.0276386,0.351935,"Brain network provides important insights for the diagnosis of many brain
disorders, and how to effectively model the brain structure has become one of
the core issues in the domain of brain imaging analysis. Recently, various
computational methods have been proposed to estimate the causal relationship
(i.e., effective connectivity) between brain regions. Compared with traditional
correlation-based methods, effective connectivity can provide the direction of
information flow, which may provide additional information for the diagnosis of
brain diseases. However, existing methods either ignore the fact that there is
a temporal-lag in the information transmission across brain regions, or simply
set the temporal-lag value between all brain regions to a fixed value. To
overcome these issues, we design an effective temporal-lag neural network
(termed ETLN) to simultaneously infer the causal relationships and the
temporal-lag values between brain regions, which can be trained in an
end-to-end manner. In addition, we also introduce three mechanisms to better
guide the modeling of brain networks. The evaluation results on the Alzheimer's
Disease Neuroimaging Initiative (ADNI) database demonstrate the effectiveness
of the proposed method.",0,1,0,0,0,0,0.0092169,13.0,0.516036,42
1bac05e8-76fc-43ce-951d-999f0f78e0d0,MrSARP: A Hierarchical Deep Generative Prior for SAR Image Super-resolution,1,0.00538771,0.098759,"Generative models learned from training using deep learning methods can be
used as priors in inverse under-determined inverse problems, including imaging
from sparse set of measurements. In this paper, we present a novel hierarchical
deep-generative model MrSARP for SAR imagery that can synthesize SAR images of
a target at different resolutions jointly. MrSARP is trained in conjunction
with a critic that scores multi resolution images jointly to decide if they are
realistic images of a target at different resolutions. We show how this deep
generative model can be used to retrieve the high spatial resolution image from
low resolution images of the same target. The cost function of the generator is
modified to improve its capability to retrieve the input parameters for a given
set of resolution images. We evaluate the model's performance using the three
standard error metrics used for evaluating super-resolution performance on
simulated data and compare it to upsampling and sparsity based image sharpening
approaches.",0,0,0,0,0,0,0.0464268,9.0,0.482707,28
5cb857a9-4b21-4831-8c37-c3a4ab99da18,"Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability",21,0.216475,0.745813,"Large-scale pre-training has been proven to be crucial for various computer
vision tasks. However, with the increase of pre-training data amount, model
architecture amount, and the private/inaccessible data, it is not very
efficient or possible to pre-train all the model architectures on large-scale
datasets. In this work, we investigate an alternative strategy for
pre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP),
aiming to efficiently transfer the learned feature representation from existing
pre-trained models to new student models for future downstream tasks. We
observe that existing Knowledge Distillation (KD) methods are unsuitable
towards pre-training since they normally distill the logits that are going to
be discarded when transferred to downstream tasks. To resolve this problem, we
propose a feature-based KD method with non-parametric feature dimension
aligning. Notably, our method performs comparably with supervised pre-training
counterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less
data and 5x less pre-training time. Code is available at
https://github.com/CVMI-Lab/KDEP.",1,1,0,0,0,0,0.913997,8.0,0.911003,78
78aa65eb-b5a1-4ead-9726-32c7971a02ff,"Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models",10,0.192298,0.369857,"The energy requirements of current natural language processing models
continue to grow at a rapid, unsustainable pace. Recent works highlighting this
problem conclude there is an urgent need for methods that reduce the energy
needs of NLP and machine learning more broadly. In this article, we investigate
techniques that can be used to reduce the energy consumption of common NLP
applications. In particular, we focus on techniques to measure energy usage and
different hardware and datacenter-oriented settings that can be tuned to reduce
energy consumption for training and inference for language models. We
characterize the impact of these settings on metrics such as computational
performance and energy consumption through experiments conducted on a high
performance computing system as well as popular cloud computing platforms.
These techniques can lead to significant reduction in energy consumption when
training language models or their use for inference. For example,
power-capping, which limits the maximum power a GPU can consume, can enable a
15\% decrease in energy usage with marginal increase in overall computation
time when training a transformer-based language model.",0,1,0,0,0,0,0.838032,6.0,0.831599,28
9a0b02de-56c2-4daf-af7b-775a8de3ff92,Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation,13,0.0759057,0.310956,"Large Language Models (LLMs) have in recent years demonstrated impressive
prowess in natural language generation. A common practice to improve generation
diversity is to sample multiple outputs from the model. However, there lacks a
simple and robust way of selecting the best output from these stochastic
samples. As a case study framed in the context of question generation, we
propose two prompt-based approaches to selecting high-quality questions from a
set of LLM-generated candidates. Our method works under the constraints of 1) a
black-box (non-modifiable) question generation model and 2) lack of access to
human-annotated references -- both of which are realistic limitations for
real-world deployment of LLMs. With automatic as well as human evaluations, we
empirically demonstrate that our approach can effectively select questions of
higher qualities than greedy generation.",1,1,0,0,0,1,0.655463,6.0,0.742343,51
6b52cd8d-bbec-4cd0-a1db-58e80d6b80bb,Dialog Acts for Task-Driven Embodied Agents,10,0.129425,0.866018,"Embodied agents need to be able to interact in natural language understanding
task descriptions and asking appropriate follow up questions to obtain
necessary information to be effective at successfully accomplishing tasks for a
wide range of users. In this work, we propose a set of dialog acts for
modelling such dialogs and annotate the TEACh dataset that includes over 3,000
situated, task oriented conversations (consisting of 39.5k utterances in total)
with dialog acts. TEACh-DA is one of the first large scale dataset of dialog
act annotations for embodied task completion. Furthermore, we demonstrate the
use of this annotated dataset in training models for tagging the dialog acts of
a given utterance, predicting the dialog act of the next response given a
dialog history, and use the dialog acts to guide agent's non-dialog behaviour.
In particular, our experiments on the TEACh Execution from Dialog History task
where the model predicts the sequence of low level actions to be executed in
the environment for embodied task completion, demonstrate that dialog acts can
improve end task success rate by up to 2 points compared to the system without
dialog acts.",0,1,0,1,0,0,0.266181,7.0,0.602534,32
29c6b6a1-155b-4c8b-983a-7d93197bfb03,BoxGraph: Semantic Place Recognition and Pose Estimation from 3D LiDAR,14,0.564335,0.780225,"This paper is about extremely robust and lightweight localisation using LiDAR
point clouds based on instance segmentation and graph matching. We model 3D
point clouds as fully-connected graphs of semantically identified components
where each vertex corresponds to an object instance and encodes its shape.
Optimal vertex association across graphs allows for full 6-Degree-of-Freedom
(DoF) pose estimation and place recognition by measuring similarity. This
representation is very concise, condensing the size of maps by a factor of 25
against the state-of-the-art, requiring only 3kB to represent a 1.4MB laser
scan. We verify the efficacy of our system on the SemanticKITTI dataset, where
we achieve a new state-of-the-art in place recognition, with an average of
88.4% recall at 100% precision where the next closest competitor follows with
64.9%. We also show accurate metric pose estimation performance - estimating
6-DoF pose with median errors of 10 cm and 0.33 deg.",0,1,0,0,1,0,0.96511,6.0,0.933531,31
e9c3ee9c-07c0-4c7e-888a-0b155cd09a19,Evaluating a Synthetic Image Dataset Generated with Stable Diffusion,8,0.0577492,0.48338,"We generate synthetic images with the ""Stable Diffusion"" image generation
model using the Wordnet taxonomy and the definitions of concepts it contains.
This synthetic image database can be used as training data for data
augmentation in machine learning applications, and it is used to investigate
the capabilities of the Stable Diffusion model.
  Analyses show that Stable Diffusion can produce correct images for a large
number of concepts, but also a large variety of different representations. The
results show differences depending on the test concepts considered and problems
with very specific concepts. These evaluations were performed using a vision
transformer model for image classification.",1,1,0,0,0,0,0.779149,7.0,0.828978,40
b4855bcd-8c25-49e7-a740-d4f63d740254,Learning Probabilities of Causation from Finite Population Data,7,0.231504,0.803088,"This paper deals with the problem of learning the probabilities of causation
of subpopulations given finite population data. The tight bounds of three basic
probabilities of causation, the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN),
were derived by Tian and Pearl. However, obtaining the bounds for each
subpopulation requires experimental and observational distributions of each
subpopulation, which is usually impractical to estimate given finite population
data. We propose a machine learning model that helps to learn the bounds of the
probabilities of causation for subpopulations given finite population data. We
further show by a simulated study that the machine learning model is able to
learn the bounds of PNS for 32768 subpopulations with only knowing roughly 500
of them from the finite population data.",0,0,0,0,0,0,0.543314,8.0,0.768363,21
05f24f64-2cfc-45e7-acd3-554293d88b56,Global Convergence of Localized Policy Iteration in Networked Multi-Agent Reinforcement Learning,13,0.12313,0.847323,"We study a multi-agent reinforcement learning (MARL) problem where the agents
interact over a given network. The goal of the agents is to cooperatively
maximize the average of their entropy-regularized long-term rewards. To
overcome the curse of dimensionality and to reduce communication, we propose a
Localized Policy Iteration (LPI) algorithm that provably learns a
near-globally-optimal policy using only local information. In particular, we
show that, despite restricting each agent's attention to only its $\kappa$-hop
neighborhood, the agents are able to learn a policy with an optimality gap that
decays polynomially in $\kappa$. In addition, we show the finite-sample
convergence of LPI to the global optimal policy, which explicitly captures the
trade-off between optimality and computational complexity in choosing $\kappa$.
Numerical simulations demonstrate the effectiveness of LPI.",0,0,0,0,0,0,0.324638,8.0,0.68192,49
add37541-2262-4fa6-bee8-c65d9f205fae,Exploiting Local and Global Features in Transformer-based Extreme Multi-label Text Classification,1,0.0188688,0.0495805,"Extreme multi-label text classification (XMTC) is the task of tagging each
document with the relevant labels from a very large space of predefined
categories. Recently, large pre-trained Transformer models have made
significant performance improvements in XMTC, which typically use the embedding
of the special CLS token to represent the entire document semantics as a global
feature vector, and match it against candidate labels. However, we argue that
such a global feature vector may not be sufficient to represent different
granularity levels of semantics in the document, and that complementing it with
the local word-level features could bring additional gains. Based on this
insight, we propose an approach that combines both the local and global
features produced by Transformer models to improve the prediction power of the
classifier. Our experiments show that the proposed model either outperforms or
is comparable to the state-of-the-art methods on benchmark datasets.",0,1,0,0,1,0,0.957754,7.0,0.934639,15
80c53587-41f2-4d59-af9e-05712ed08504,HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations,17,0.0952355,0.847999,"Recently, various response generation models for two-party conversations have
achieved impressive improvements, but less effort has been paid to multi-party
conversations (MPCs) which are more practical and complicated. Compared with a
two-party conversation where a dialogue context is a sequence of utterances,
building a response generation model for MPCs is more challenging, since there
exist complicated context structures and the generated responses heavily rely
on both interlocutors (i.e., speaker and addressee) and history utterances. To
address these challenges, we present HeterMPC, a heterogeneous graph-based
neural network for response generation in MPCs which models the semantics of
utterances and interlocutors simultaneously with two types of nodes in a graph.
Besides, we also design six types of meta relations with
node-edge-type-dependent parameters to characterize the heterogeneous
interactions within the graph. Through multi-hop updating, HeterMPC can
adequately utilize the structural knowledge of conversations for response
generation. Experimental results on the Ubuntu Internet Relay Chat (IRC)
channel benchmark show that HeterMPC outperforms various baseline models for
response generation in MPCs.",0,0,1,0,1,0,0.577835,7.0,0.748926,46
70080d58-2ebe-4be5-b770-ee666664cf21,Dual-former: Hybrid Self-attention Transformer for Efficient Image Restoration,9,0.0760775,0.370225,"Recently, image restoration transformers have achieved comparable performance
with previous state-of-the-art CNNs. However, how to efficiently leverage such
architectures remains an open problem. In this work, we present Dual-former
whose critical insight is to combine the powerful global modeling ability of
self-attention modules and the local modeling ability of convolutions in an
overall architecture. With convolution-based Local Feature Extraction modules
equipped in the encoder and the decoder, we only adopt a novel Hybrid
Transformer Block in the latent layer to model the long-distance dependence in
spatial dimensions and handle the uneven distribution between channels. Such a
design eliminates the substantial computational complexity in previous image
restoration transformers and achieves superior performance on multiple image
restoration tasks. Experiments demonstrate that Dual-former achieves a 1.91dB
gain over the state-of-the-art MAXIM method on the Indoor dataset for single
image dehazing while consuming only 4.2% GFLOPs as MAXIM. For single image
deraining, it exceeds the SOTA method by 0.1dB PSNR on the average results of
five datasets with only 21.5% GFLOPs. Dual-former also substantially surpasses
the latest desnowing method on various datasets, with fewer parameters.",0,1,0,0,1,0,0.923041,4.0,0.83308,67
0c8367df-1bd1-4d07-ab6b-b5c2d5e4cc7a,Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization,28,0.212672,0.856856,"Neural abstractive summarization models are prone to generate summaries which
are factually inconsistent with their source documents. Previous work has
introduced the task of recognizing such factual inconsistency as a downstream
application of natural language inference (NLI). However, state-of-the-art NLI
models perform poorly in this context due to their inability to generalize to
the target task. In this work, we show that NLI models can be effective for
this task when the training data is augmented with high-quality task-oriented
examples. We introduce Falsesum, a data generation pipeline leveraging a
controllable text generation model to perturb human-annotated summaries,
introducing varying types of factual inconsistencies. Unlike previously
introduced document-level NLI datasets, our generated dataset contains examples
that are diverse and inconsistent yet plausible. We show that models trained on
a Falsesum-augmented NLI dataset improve the state-of-the-art performance
across four benchmarks for detecting factual inconsistency in summarization.
  The code to obtain the dataset is available online at
https://github.com/joshbambrick/Falsesum",1,1,0,1,1,0,0.787308,6.0,0.804578,52
82b98032-30f3-436b-b572-b1b074c2ac90,In-Hand 3D Object Scanning from an RGB Sequence,12,0.650182,0.968619,"We propose a method for in-hand 3D scanning of an unknown object with a
monocular camera. Our method relies on a neural implicit surface representation
that captures both the geometry and the appearance of the object, however, by
contrast with most NeRF-based methods, we do not assume that the camera-object
relative poses are known. Instead, we simultaneously optimize both the object
shape and the pose trajectory. As direct optimization over all shape and pose
parameters is prone to fail without coarse-level initialization, we propose an
incremental approach that starts by splitting the sequence into carefully
selected overlapping segments within which the optimization is likely to
succeed. We reconstruct the object shape and track its poses independently
within each segment, then merge all the segments before performing a global
optimization. We show that our method is able to reconstruct the shape and
color of both textured and challenging texture-less objects, outperforms
classical methods that rely only on appearance features, and that its
performance is close to recent methods that assume known camera poses.",0,0,0,0,0,0,0.952292,8.0,0.937912,50
8282a19d-b809-4c5d-bc22-13c8c21492cc,CamoFormer: Masked Separable Attention for Camouflaged Object Detection,26,0.082331,0.623977,"How to identify and segment camouflaged objects from the background is
challenging. Inspired by the multi-head self-attention in Transformers, we
present a simple masked separable attention (MSA) for camouflaged object
detection. We first separate the multi-head self-attention into three parts,
which are responsible for distinguishing the camouflaged objects from the
background using different mask strategies. Furthermore, we propose to capture
high-resolution semantic representations progressively based on a simple
top-down decoder with the proposed MSA to attain precise segmentation results.
These structures plus a backbone encoder form a new model, dubbed CamoFormer.
Extensive experiments show that CamoFormer surpasses all existing
state-of-the-art methods on three widely-used camouflaged object detection
benchmarks. There are on average around 5% relative improvements over previous
methods in terms of S-measure and weighted F-measure.",0,1,0,0,1,0,0.671295,4.0,0.624314,75
2bd86f18-4d0f-4db0-9486-d037e9d2373c,Omnigrok: Grokking Beyond Algorithmic Data,36,0.951219,0.53064,"Grokking, the unusual phenomenon for algorithmic datasets where
generalization happens long after overfitting the training data, has remained
elusive. We aim to understand grokking by analyzing the loss landscapes of
neural networks, identifying the mismatch between training and test losses as
the cause for grokking. We refer to this as the ""LU mechanism"" because training
and test losses (against model weight norm) typically resemble ""L"" and ""U"",
respectively. This simple mechanism can nicely explain many aspects of
grokking: data size dependence, weight decay dependence, the emergence of
representations, etc. Guided by the intuitive picture, we are able to induce
grokking on tasks involving images, language and molecules. In the reverse
direction, we are able to eliminate grokking for algorithmic datasets. We
attribute the dramatic nature of grokking for algorithmic datasets to
representation learning.",1,0,0,0,0,1,0.943107,9.0,0.938187,31
49c45e7b-2335-40a4-be1b-643268e9801c,Stop Filtering: Multi-View Attribute-Enhanced Dialogue Learning,2,0.0536694,0.183505,"There is a growing interest in improving the conversational ability of models
by filtering the raw dialogue corpora. Previous filtering strategies usually
rely on a scoring method to assess and discard samples from one perspective,
enabling the model to enhance the corresponding dialogue attributes (e.g.,
consistency) more easily. However, the discarded samples may obtain high scores
in other perspectives and can provide regularization effects on the model
learning, which causes the performance improvement to be sensitive to the
filtering ratio. In this work, we propose a multi-view attribute-enhanced
dialogue learning framework that strengthens the attribute-related features
more robustly and comprehensively. Instead of filtering the raw dataset to
train the model, our framework first pre-trains the model on the raw dataset
and then fine-tunes it through adapters on the selected sub-sets, which also
enhances certain attributes of responses but without suffering from the
problems mentioned above. Considering the variety of the dialogue attribute, we
further design a multi-view enhancement mechanism, including multi-view
selection and inter-view fusion. It groups the high-quality samples from
multiple perspectives, respectively, and enhances different attributes of
responses with the corresponding sample sets and adapters, keeping knowledge
independent and allowing flexible integration. Empirical results and analysis
show that our framework can improve the performance significantly in terms of
enhancing dialogue attributes and fusing view-specific knowledge.",0,0,0,0,0,0,0.910346,8.0,0.908867,65
abd138d0-6a7d-487f-a6e1-c645be613e22,Self-supervised Learning for Unintentional Action Prediction,4,0.0157093,0.142217,"Distinguishing if an action is performed as intended or if an intended action
fails is an important skill that not only humans have, but that is also
important for intelligent systems that operate in human environments.
Recognizing if an action is unintentional or anticipating if an action will
fail, however, is not straightforward due to lack of annotated data. While
videos of unintentional or failed actions can be found in the Internet in
abundance, high annotation costs are a major bottleneck for learning networks
for these tasks. In this work, we thus study the problem of self-supervised
representation learning for unintentional action prediction. While previous
works learn the representation based on a local temporal neighborhood, we show
that the global context of a video is needed to learn a good representation for
the three downstream tasks: unintentional action classification, localization
and anticipation. In the supplementary material, we show that the learned
representation can be used for detecting anomalies in videos as well.",0,0,0,0,1,0,0.213781,6.0,0.494269,41
f60f6280-a9d7-455a-a1d6-dc032e80c5e8,Developmental Negation Processing in Transformer Language Models,3,0.0465514,0.111208,"Reasoning using negation is known to be difficult for transformer-based
language models. While previous studies have used the tools of
psycholinguistics to probe a transformer's ability to reason over negation,
none have focused on the types of negation studied in developmental psychology.
We explore how well transformers can process such categories of negation, by
framing the problem as a natural language inference (NLI) task. We curate a set
of diagnostic questions for our target categories from popular NLI datasets and
evaluate how well a suite of models reason over them. We find that models
perform consistently better only on certain categories, suggesting clear
distinctions in how they are processed.",1,0,0,0,0,0,0.196419,9.0,0.652267,38
b8b38240-55b9-402f-842c-66247e200f17,Building an Efficiency Pipeline: Commutativity and Cumulativeness of Efficiency Operators for Transformers,1,0.0,0.165917,"There exists a wide variety of efficiency methods for natural language
processing (NLP) tasks, such as pruning, distillation, dynamic inference,
quantization, etc. We can consider an efficiency method as an operator applied
on a model. Naturally, we may construct a pipeline of multiple efficiency
methods, i.e., to apply multiple operators on the model sequentially. In this
paper, we study the plausibility of this idea, and more importantly, the
commutativity and cumulativeness of efficiency operators. We make two
interesting observations: (1) Efficiency operators are commutative -- the order
of efficiency methods within the pipeline has little impact on the final
results; (2) Efficiency operators are also cumulative -- the final results of
combining several efficiency methods can be estimated by combining the results
of individual methods. These observations deepen our understanding of
efficiency operators and provide useful guidelines for their real-world
applications.",0,0,0,0,0,0,0.858557,6.0,0.843565,39
0588865b-8b56-4f53-ba6b-5fee1f901672,TweetNLP: Cutting-Edge Natural Language Processing for Social Media,37,0.243635,0.96576,"In this paper we present TweetNLP, an integrated platform for Natural
Language Processing (NLP) in social media. TweetNLP supports a diverse set of
NLP tasks, including generic focus areas such as sentiment analysis and named
entity recognition, as well as social media-specific tasks such as emoji
prediction and offensive language identification. Task-specific systems are
powered by reasonably-sized Transformer-based language models specialized on
social media text (in particular, Twitter) which can be run without the need
for dedicated hardware or cloud services. The main contributions of TweetNLP
are: (1) an integrated Python library for a modern toolkit supporting social
media analysis using our various task-specific models adapted to the social
domain; (2) an interactive online demo for codeless experimentation using our
models; and (3) a tutorial covering a wide variety of typical social media
applications.",1,1,0,0,0,0,0.308139,9.0,0.710211,54
57d24ffe-9217-4a5a-8d69-eda4ca03f0b0,Improving Cross-Modal Retrieval with Set of Diverse Embeddings,11,0.245815,0.498425,"Cross-modal retrieval across image and text modalities is a challenging task
due to its inherent ambiguity: An image often exhibits various situations, and
a caption can be coupled with diverse images. Set-based embedding has been
studied as a solution to this problem. It seeks to encode a sample into a set
of different embedding vectors that capture different semantics of the sample.
In this paper, we present a novel set-based embedding method, which is distinct
from previous work in two aspects. First, we present a new similarity function
called smooth-Chamfer similarity, which is designed to alleviate the side
effects of existing similarity functions for set-based embedding. Second, we
propose a novel set prediction module to produce a set of embedding vectors
that effectively captures diverse semantics of input by the slot attention
mechanism. Our method is evaluated on the COCO and Flickr30K datasets across
different visual backbones, where it outperforms existing methods including
ones that demand substantially larger computation at inference.",0,1,0,0,1,0,0.822536,11.0,0.903463,60
3f533c24-bcdf-471e-89d3-3d79454f8812,DDH-QA: A Dynamic Digital Humans Quality Assessment Database,6,0.180926,0.89228,"In recent years, large amounts of effort have been put into pushing forward
the real-world application of dynamic digital human (DDH). However, most
current quality assessment research focuses on evaluating static 3D models and
usually ignores motion distortions. Therefore, in this paper, we construct a
large-scale dynamic digital human quality assessment (DDH-QA) database with
diverse motion content as well as multiple distortions to comprehensively study
the perceptual quality of DDHs. Both model-based distortion (noise,
compression) and motion-based distortion (binding error, motion unnaturalness)
are taken into consideration. Ten types of common motion are employed to drive
the DDHs and a total of 800 DDHs are generated in the end. Afterward, we render
the video sequences of the distorted DDHs as the evaluation media and carry out
a well-controlled subjective experiment. Then a benchmark experiment is
conducted with the state-of-the-art video quality assessment (VQA) methods and
the experimental results show that existing VQA methods are limited in
assessing the perceptual loss of DDHs.",0,1,0,1,0,0,0.898353,5.0,0.843495,27
340bff8b-89c2-45cf-82e7-72878ab58ca7,TripleRE: Knowledge Graph Embeddings via Tripled Relation Vectors,14,0.125584,0.84798,"Translation-based knowledge graph embedding has been one of the most
important branches for knowledge representation learning since TransE came out.
Although many translation-based approaches have achieved some progress in
recent years, the performance was still unsatisfactory. This paper proposes a
novel knowledge graph embedding method named TripleRE with two versions. The
first version of TripleRE creatively divide the relationship vector into three
parts. The second version takes advantage of the concept of residual and
achieves better performance. In addition, attempts on using NodePiece to encode
entities achieved promising results in reducing the parametric size, and solved
the problems of scalability. Experiments show that our approach achieved
state-of-the-art performance on the large-scale knowledge graph dataset, and
competitive performance on other datasets.",0,1,0,0,1,0,0.488913,13.0,0.845531,34
15b3fbb6-3bc0-464f-b19a-585c0dca1786,A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking,34,0.411893,0.512686,"Large-scale graph training is a notoriously challenging problem for graph
neural networks (GNNs). Due to the nature of evolving graph structures into the
training process, vanilla GNNs usually fail to scale up, limited by the GPU
memory space. Up to now, though numerous scalable GNN architectures have been
proposed, we still lack a comprehensive survey and fair benchmark of this
reservoir to find the rationale for designing scalable GNNs. To this end, we
first systematically formulate the representative methods of large-scale graph
training into several branches and further establish a fair and consistent
benchmark for them by a greedy hyperparameter searching. In addition, regarding
efficiency, we theoretically evaluate the time and space complexity of various
branches and empirically compare them w.r.t GPU memory usage, throughput, and
convergence. Furthermore, We analyze the pros and cons for various branches of
scalable GNNs and then present a new ensembling training manner, named EnGCN,
to address the existing issues. Our code is available at
https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking.",1,0,0,0,0,0,0.784819,7.0,0.831417,67
f1f59ebe-e7d2-4301-b969-01b41c96f7b4,Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks,4,0.0481478,0.165302,"Meta reinforcement learning (meta-RL) aims to learn a policy solving a set of
training tasks simultaneously and quickly adapting to new tasks. It requires
massive amounts of data drawn from training tasks to infer the common structure
shared among tasks. Without heavy reward engineering, the sparse rewards in
long-horizon tasks exacerbate the problem of sample efficiency in meta-RL.
Another challenge in meta-RL is the discrepancy of difficulty level among
tasks, which might cause one easy task dominating learning of the shared policy
and thus preclude policy adaptation to new tasks. This work introduces a novel
objective function to learn an action translator among training tasks. We
theoretically verify that the value of the transferred policy with the action
translator can be close to the value of the source policy and our objective
function (approximately) upper bounds the value difference. We propose to
combine the action translator with context-based meta-RL algorithms for better
data collection and more efficient exploration during meta-training. Our
approach empirically improves the sample efficiency and performance of meta-RL
algorithms on sparse-reward tasks.",0,0,0,0,0,0,0.572637,9.0,0.803132,45
0ab51e42-9af2-4c81-ae32-1f9771c1a248,How Powerful are Spectral Graph Neural Networks,92,0.505178,0.987573,"Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based
on graph signal filters. Some models able to learn arbitrary spectral filters
have emerged recently. However, few works analyze the expressive power of
spectral GNNs. This paper studies spectral GNNs' expressive power
theoretically. We first prove that even spectral GNNs without nonlinearity can
produce arbitrary graph signals and give two conditions for reaching
universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2)
no missing frequency components in node features. We also establish a
connection between the expressive power of spectral GNNs and Graph Isomorphism
(GI) testing, the latter of which is often used to characterize spatial GNNs'
expressive power. Moreover, we study the difference in empirical performance
among different spectral GNNs with the same expressive power from an
optimization perspective, and motivate the use of an orthogonal basis whose
weight function corresponds to the graph signal density in the spectrum.
Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to
its orthogonality and flexibility to adapt to a wide range of weight functions.
JacobiConv deserts nonlinearity while outperforming all baselines on both
synthetic and real-world datasets.",1,0,0,0,0,0,0.645712,7.0,0.77536,47
54aa3a82-694a-4641-bd49-1174ffcc018a,SimVP: Simpler yet Better Video Prediction,90,0.50524,0.99895,"From CNN, RNN, to ViT, we have witnessed remarkable advancements in video
prediction, incorporating auxiliary inputs, elaborate neural architectures, and
sophisticated training strategies. We admire these progresses but are confused
about the necessity: is there a simple method that can perform comparably well?
This paper proposes SimVP, a simple video prediction model that is completely
built upon CNN and trained by MSE loss in an end-to-end fashion. Without
introducing any additional tricks and complicated strategies, we can achieve
state-of-the-art performance on five benchmark datasets. Through extended
experiments, we demonstrate that SimVP has strong generalization and
extensibility on real-world datasets. The significant reduction of training
cost makes it easier to scale to complex scenarios. We believe SimVP can serve
as a solid baseline to stimulate the further development of video prediction.
The code is available at
\href{https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction}{Github}.",0,1,0,0,1,1,0.311046,9.0,0.711473,77
03c00ae7-6ab7-4b44-b88c-a1e66500980a,Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers,1,0.00565513,0.103145,"Text-to-SQL parsers typically struggle with databases unseen during the train
time. Adapting parsers to new databases is a challenging problem due to the
lack of natural language queries in the new schemas. We present ReFill, a
framework for synthesizing high-quality and textually diverse parallel datasets
for adapting a Text-to-SQL parser to a target schema. ReFill learns to
retrieve-and-edit text queries from the existing schemas and transfers them to
the target schema. We show that retrieving diverse existing text, masking their
schema-specific tokens, and refilling with tokens relevant to the target
schema, leads to significantly more diverse text queries than achievable by
standard SQL-to-Text generation methods. Through experiments spanning multiple
databases, we demonstrate that fine-tuning parsers on datasets synthesized
using ReFill consistently outperforms the prior data-augmentation methods.",0,1,0,1,0,0,0.196117,7.0,0.552669,45
483560f4-08ce-479a-b348-0def86431945,Representing Spatial Trajectories as Distributions,3,0.0359211,0.124573,"We introduce a representation learning framework for spatial trajectories. We
represent partial observations of trajectories as probability distributions in
a learned latent space, which characterize the uncertainty about unobserved
parts of the trajectory. Our framework allows us to obtain samples from a
trajectory for any continuous point in time, both interpolating and
extrapolating. Our flexible approach supports directly modifying specific
attributes of a trajectory, such as its pace, as well as combining different
partial observations into single representations. Experiments show our method's
advantage over baselines in prediction tasks.",0,0,0,0,0,1,0.544391,7.0,0.735702,64
53e80dd4-074f-496c-b5e1-1facedd09039,How stable are Transferability Metrics evaluations?,13,0.114371,0.709996,"Transferability metrics is a maturing field with increasing interest, which
aims at providing heuristics for selecting the most suitable source models to
transfer to a given target dataset, without fine-tuning them all. However,
existing works rely on custom experimental setups which differ across papers,
leading to inconsistent conclusions about which transferability metrics work
best. In this paper we conduct a large-scale study by systematically
constructing a broad range of 715k experimental setup variations. We discover
that even small variations to an experimental setup lead to different
conclusions about the superiority of a transferability metric over another.
Then we propose better evaluations by aggregating across many experiments,
enabling to reach more stable conclusions. As a result, we reveal the
superiority of LogME at selecting good source datasets to transfer from in a
semantic segmentation scenario, NLEEP at selecting good source architectures in
an image classification scenario, and GBC at determining which target task
benefits most from a given source model. Yet, no single transferability metric
works best in all scenarios.",0,1,0,0,0,1,0.602056,9.0,0.812083,98
5cdb7db5-8850-4d26-91b5-d54edb289edb,Constrained Optimization with Dynamic Bound-scaling for Effective NLPBackdoor Defense,22,0.362437,0.822469,"We develop a novel optimization method for NLPbackdoor inversion. We leverage
a dynamically reducing temperature coefficient in the softmax function to
provide changing loss landscapes to the optimizer such that the process
gradually focuses on the ground truth trigger, which is denoted as a one-hot
value in a convex hull. Our method also features a temperature rollback
mechanism to step away from local optimals, exploiting the observation that
local optimals can be easily deter-mined in NLP trigger inversion (while not in
general optimization). We evaluate the technique on over 1600 models (with
roughly half of them having injected backdoors) on 3 prevailing NLP tasks, with
4 different backdoor attacks and 7 architectures. Our results show that the
technique is able to effectively and efficiently detect and remove backdoors,
outperforming 4 baseline methods.",0,0,0,0,1,0,0.952534,5.0,0.900992,61
a6f8261d-0443-495e-8fdd-8a4612d5577b,Can No-reference features help in Full-reference image quality estimation?,2,0.00420615,0.0828898,"Development of perceptual image quality assessment (IQA) metrics has been of
significant interest to computer vision community. The aim of these metrics is
to model quality of an image as perceived by humans. Recent works in
Full-reference IQA research perform pixelwise comparison between deep features
corresponding to query and reference images for quality prediction. However,
pixelwise feature comparison may not be meaningful if distortion present in
query image is severe. In this context, we explore utilization of no-reference
features in Full-reference IQA task. Our model consists of both full-reference
and no-reference branches. Full-reference branches use both distorted and
reference images, whereas No-reference branch only uses distorted image. Our
experiments show that use of no-reference features boosts performance of image
quality assessment. Our model achieves higher SRCC and KRCC scores than a
number of state-of-the-art algorithms on KADID-10K and PIPAL datasets.",0,1,0,0,1,0,0.178746,10.0,0.676562,34
957160d6-536a-42e7-8314-a6aafcf06745,SCVRL: Shuffled Contrastive Video Representation Learning,13,0.0468008,0.507478,"We propose SCVRL, a novel contrastive-based framework for self-supervised
learning for videos. Differently from previous contrast learning based methods
that mostly focus on learning visual semantics (e.g., CVRL), SCVRL is capable
of learning both semantic and motion patterns. For that, we reformulate the
popular shuffling pretext task within a modern contrastive learning paradigm.
We show that our transformer-based network has a natural capacity to learn
motion in self-supervised settings and achieves strong performance,
outperforming CVRL on four benchmarks.",0,0,0,0,1,0,0.504946,7.0,0.719756,54
9379c13d-a345-4ce5-9450-7d0016c62406,CenterFormer: Center-based Transformer for 3D Object Detection,81,0.8036,0.850695,"Query-based transformer has shown great potential in constructing long-range
attention in many image-domain tasks, but has rarely been considered in
LiDAR-based 3D object detection due to the overwhelming size of the point cloud
data. In this paper, we propose CenterFormer, a center-based transformer
network for 3D object detection. CenterFormer first uses a center heatmap to
select center candidates on top of a standard voxel-based point cloud encoder.
It then uses the feature of the center candidate as the query embedding in the
transformer. To further aggregate features from multiple frames, we design an
approach to fuse features through cross-attention. Lastly, regression heads are
added to predict the bounding box on the output center feature representation.
Our design reduces the convergence difficulty and computational complexity of
the transformer structure. The results show significant improvements over the
strong baseline of anchor-free object detection networks. CenterFormer achieves
state-of-the-art performance for a single model on the Waymo Open Dataset, with
73.7% mAPH on the validation set and 75.6% mAPH on the test set, significantly
outperforming all previously published CNN and transformer-based methods. Our
code is publicly available at https://github.com/TuSimple/centerformer",1,1,0,0,1,0,0.972323,5.0,0.933585,64
63acf935-f064-4611-909f-fbc18a310e7e,Neural Topic Modeling of Psychotherapy Sessions,13,0.0893006,0.488786,"In this work, we compare different neural topic modeling methods in learning
the topical propensities of different psychiatric conditions from the
psychotherapy session transcripts parsed from speech recordings. We also
incorporate temporal modeling to put this additional interpretability to action
by parsing out topic similarities as a time series in a turn-level resolution.
We believe this topic modeling framework can offer interpretable insights for
the therapist to optimally decide his or her strategy and improve psychotherapy
effectiveness.",1,0,0,0,0,0,0.163077,6.0,0.44412,28
17e229c5-a25f-4552-8baf-9c42f9539ed1,Feature transforms for image data augmentation,10,0.0415743,0.414006,"A problem with Convolutional Neural Networks (CNNs) is that they require
large datasets to obtain adequate robustness; on small datasets, they are prone
to overfitting. Many methods have been proposed to overcome this shortcoming
with CNNs. In cases where additional samples cannot easily be collected, a
common approach is to generate more data points from existing data using an
augmentation technique. In image classification, many augmentation approaches
utilize simple image manipulation algorithms. In this work, we build ensembles
on the data level by adding images generated by combining fourteen augmentation
approaches, three of which are proposed here for the first time. These novel
methods are based on the Fourier Transform (FT), the Radon Transform (RT) and
the Discrete Cosine Transform (DCT). Pretrained ResNet50 networks are finetuned
on training sets that include images derived from each augmentation method.
These networks and several fusions are evaluated and compared across eleven
benchmarks. Results show that building ensembles on the data level by combining
different data augmentation methods produce classifiers that not only compete
competitively against the state-of-the-art but often surpass the best
approaches reported in the literature.",0,1,0,0,1,0,0.0644914,8.0,0.460311,62
a47f75d4-20dc-4a79-9f12-9028aa4677b6,A Multilingual Dataset of COVID-19 Vaccination Attitudes on Twitter,11,0.177697,0.197676,"Vaccine hesitancy is considered as one main cause of the stagnant uptake
ratio of COVID-19 vaccines in Europe and the US where vaccines are sufficiently
supplied. Fast and accurate grasp of public attitudes toward vaccination is
critical to address vaccine hesitancy, and social media platforms have proved
to be an effective source of public opinions. In this paper, we describe the
collection and release of a dataset of tweets related to COVID-19 vaccines.
This dataset consists of the IDs of 2,198,090 tweets collected from Western
Europe, 17,934 of which are annotated with the originators' vaccination
stances. Our annotation will facilitate using and developing data-driven models
to extract vaccination attitudes from social media posts and thus further
confirm the power of social media in public health surveillance. To lay the
groundwork for future research, we not only perform statistical analysis and
visualisation of our dataset, but also evaluate and compare the performance of
established text-based benchmarks in vaccination stance extraction. We
demonstrate one potential use of our data in practice in tracking the temporal
changes of public COVID-19 vaccination attitudes.",0,1,0,1,0,0,0.919866,6.0,0.886071,29
9a517990-d66b-40d5-8981-af53922461aa,CoShNet: A Hybrid Complex Valued Neural Network using Shearlets,1,0.00560512,0.113597,"In a hybrid neural network, the expensive convolutional layers are replaced
by a non-trainable fixed transform with a great reduction in parameters. In
previous works, good results were obtained by replacing the convolutions with
wavelets. However, wavelet based hybrid network inherited wavelet's lack of
vanishing moments along curves and its axis-bias. We propose to use Shearlets
with its robust support for important image features like edges, ridges and
blobs. The resulting network is called Complex Shearlets Network (CoShNet). It
was tested on Fashion-MNIST against ResNet-50 and Resnet-18, obtaining 92.2%
versus 90.7% and 91.8% respectively. The proposed network has 49.9k parameters
versus ResNet-18 with 11.18m and use 52 times fewer FLOPs. Finally, we trained
in under 20 epochs versus 200 epochs required by ResNet and do not need any
hyperparameter tuning nor regularization.
  Code: https://github.com/Ujjawal-K-Panchal/coshnet",1,0,0,0,0,0,0.0101857,14.0,0.557779,53
1b0c5cc8-2532-48ed-9932-626406c98771,NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors,33,0.439908,0.567973,"Though Neural Radiance Field (NeRF) demonstrates compelling novel view
synthesis results, it is still unintuitive to edit a pre-trained NeRF because
the neural network's parameters and the scene geometry/appearance are often not
explicitly associated. In this paper, we introduce the first framework that
enables users to remove unwanted objects or retouch undesired regions in a 3D
scene represented by a pre-trained NeRF without any category-specific data and
training. The user first draws a free-form mask to specify a region containing
unwanted objects over a rendered view from the pre-trained NeRF. Our framework
first transfers the user-provided mask to other rendered views and estimates
guiding color and depth images within these transferred masked regions. Next,
we formulate an optimization problem that jointly inpaints the image content in
all masked regions across multiple views by updating the NeRF model's
parameters. We demonstrate our framework on diverse scenes and show it obtained
visual plausible and structurally consistent results across multiple views
using shorter time and less user manual efforts.",0,1,1,0,0,0,0.98682,4.0,0.963979,46
a0482eb6-22be-4dab-815b-bc1e382fd9d9,ARST: Auto-Regressive Surgical Transformer for Phase Recognition from Laparoscopic Videos,8,0.162357,0.83607,"Phase recognition plays an essential role for surgical workflow analysis in
computer assisted intervention. Transformer, originally proposed for sequential
data modeling in natural language processing, has been successfully applied to
surgical phase recognition. Existing works based on transformer mainly focus on
modeling attention dependency, without introducing auto-regression. In this
work, an Auto-Regressive Surgical Transformer, referred as ARST, is first
proposed for on-line surgical phase recognition from laparoscopic videos,
modeling the inter-phase correlation implicitly by conditional probability
distribution. To reduce inference bias and to enhance phase consistency, we
further develop a consistency constraint inference strategy based on
auto-regression. We conduct comprehensive validations on a well-known public
dataset Cholec80. Experimental results show that our method outperforms the
state-of-the-art methods both quantitatively and qualitatively, and achieves an
inference rate of 66 frames per second (fps).",0,1,0,0,1,0,0.719505,11.0,0.875502,25
634ab71f-07b1-4d26-a622-4a743fa0a591,MOVE: Unsupervised Movable Object Segmentation and Detection,18,0.208038,0.560396,"We introduce MOVE, a novel method to segment objects without any form of
supervision. MOVE exploits the fact that foreground objects can be shifted
locally relative to their initial position and result in realistic
(undistorted) new images. This property allows us to train a segmentation model
on a dataset of images without annotation and to achieve state of the art
(SotA) performance on several evaluation datasets for unsupervised salient
object detection and segmentation. In unsupervised single object discovery,
MOVE gives an average CorLoc improvement of 7.2% over the SotA, and in
unsupervised class-agnostic object detection it gives a relative AP improvement
of 53% on average. Our approach is built on top of self-supervised features
(e.g. from DINO or MAE), an inpainting network (based on the Masked
AutoEncoder) and adversarial training.",0,0,0,0,0,0,0.606149,8.0,0.789988,95
371a912e-9481-40ee-8d9b-6b31f658ce96,Specialized Re-Ranking: A Novel Retrieval-Verification Framework for Cloth Changing Person Re-Identification,8,0.0507771,0.70915,"Cloth changing person re-identification(Re-ID) can work under more
complicated scenarios with higher security than normal Re-ID and biometric
techniques and is therefore extremely valuable in applications. Meanwhile,
higher flexibility in appearance always leads to more similar-looking confusing
images, which is the weakness of the widely used retrieval methods. In this
work, we shed light on how to handle these similar images. Specifically, we
propose a novel retrieval-verification framework. Given an image, the retrieval
module can search for similar images quickly. Our proposed verification network
will then compare the input image and the candidate images by contrasting those
local details and give a similarity score. An innovative ranking strategy is
also introduced to take a good balance between retrieval and verification
results. Comprehensive experiments are conducted to show the effectiveness of
our framework and its capability in improving the state-of-the-art methods
remarkably on both synthetic and realistic datasets.",0,1,0,0,1,0,0.315226,7.0,0.631352,47
864c39c7-b81c-4171-a0d3-b811c0378890,Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models,8,0.0570637,0.150708,"With a growing number of BERTology work analyzing different components of
pre-trained language models, we extend this line of research through an
in-depth analysis of discourse information in pre-trained and fine-tuned
language models. We move beyond prior work along three dimensions: First, we
describe a novel approach to infer discourse structures from arbitrarily long
documents. Second, we propose a new type of analysis to explore where and how
accurately intrinsic discourse is captured in the BERT and BART models.
Finally, we assess how similar the generated structures are to a variety of
baselines as well as their distribution within and between models.",0,0,0,0,0,0,0.458464,6.0,0.650289,65
e780fbdb-c72a-4bf1-a41d-590b488fd1c7,Solving Quantitative Reasoning Problems with Language Models,434,0.998398,0.999958,"Language models have achieved remarkable performance on a wide range of tasks
that require natural language understanding. Nevertheless, state-of-the-art
models have generally struggled with tasks that require quantitative reasoning,
such as solving mathematics, science, and engineering problems at the college
level. To help close this gap, we introduce Minerva, a large language model
pretrained on general natural language data and further trained on technical
content. The model achieves state-of-the-art performance on technical
benchmarks without the use of external tools. We also evaluate our model on
over two hundred undergraduate-level problems in physics, biology, chemistry,
economics, and other sciences that require quantitative reasoning, and find
that the model can correctly answer nearly a third of them.",0,1,0,0,1,0,0.949639,3.0,0.828448,70
0faa3edf-4ae9-4e9d-a26e-3c33ac519993,Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models,17,0.112339,0.342084,"Massively Multilingual Transformer based Language Models have been observed
to be surprisingly effective on zero-shot transfer across languages, though the
performance varies from language to language depending on the pivot language(s)
used for fine-tuning. In this work, we build upon some of the existing
techniques for predicting the zero-shot performance on a task, by modeling it
as a multi-task learning problem. We jointly train predictive models for
different tasks which helps us build more accurate predictors for tasks where
we have test data in very few languages to measure the actual performance of
the model. Our approach also lends us the ability to perform a much more robust
feature selection and identify a common set of features that influence
zero-shot performance across a variety of tasks.",1,0,0,0,0,0,0.510496,8.0,0.756775,54
ab195cc3-c404-4788-be15-a2bfca878503,Neural Face Identification in a 2D Wireframe Projection of a Manifold Object,8,0.0548804,0.72608,"In computer-aided design (CAD) systems, 2D line drawings are commonly used to
illustrate 3D object designs. To reconstruct the 3D models depicted by a single
2D line drawing, an important key is finding the edge loops in the line drawing
which correspond to the actual faces of the 3D object. In this paper, we
approach the classical problem of face identification from a novel data-driven
point of view. We cast it as a sequence generation problem: starting from an
arbitrary edge, we adopt a variant of the popular Transformer model to predict
the edges associated with the same face in a natural order. This allows us to
avoid searching the space of all possible edge loops with various hand-crafted
rules and heuristics as most existing methods do, deal with challenging cases
such as curved surfaces and nested edge loops, and leverage additional cues
such as face types. We further discuss how possibly imperfect predictions can
be used for 3D object reconstruction.",1,1,0,0,0,0,0.000102738,22.0,0.509422,35
8639273d-7012-4b91-a742-4e046a7b46eb,Federated Graph-based Networks with Shared Embedding,1,0.0113222,0.0647393,"Nowadays, user privacy is becoming an issue that cannot be bypassed for
system developers, especially for that of web applications where data can be
easily transferred through internet. Thankfully, federated learning proposes an
innovative method to train models with distributed devices while data are kept
in local storage. However, unlike general neural networks, although graph-based
networks have achieved great success in classification tasks and advanced
recommendation system, its high performance relies on the rich context provided
by a graph structure, which is vulnerable when data attributes are incomplete.
Therefore, the latter becomes a realistic problem when implementing federated
learning for graph-based networks. Knowing that data embedding is a
representation in a different space, we propose our Federated Graph-based
Networks with Shared Embedding (Feras), which uses shared embedding data to
train the network and avoids the direct sharing of original data. A solid
theoretical proof of the convergence of Feras is given in this work.
Experiments on different datasets (PPI, Flickr, Reddit) are conducted to show
the efficiency of Feras for centralized learning. Finally, Feras enables the
training of current graph-based models in the federated learning framework for
privacy concern.",1,0,0,0,0,0,0.594363,9.0,0.80975,41
a9578a4c-f344-482d-9831-1a464f2a944b,AI-Aided Integrated Terrestrial and Non-Terrestrial 6G Solutions for Sustainable Maritime Networking,17,0.464855,0.816944,"The maritime industry is experiencing a technological revolution that affects
shipbuilding, operation of both seagoing and inland vessels, cargo management,
and working practices in harbors. This ongoing transformation is driven by the
ambition to make the ecosystem more sustainable and cost-efficient.
Digitalization and automation help achieve these goals by transforming shipping
and cruising into a much more cost- and energy-efficient, and decarbonized
industry segment. The key enablers in these processes are always-available
connectivity and content delivery services, which can not only aid shipping
companies in improving their operational efficiency and reducing carbon
emissions but also contribute to enhanced crew welfare and passenger
experience. Due to recent advancements in integrating high-capacity and
ultra-reliable terrestrial and non-terrestrial networking technologies,
ubiquitous maritime connectivity is becoming a reality. To cope with the
increased complexity of managing these integrated systems, this article
advocates the use of artificial intelligence and machine learning-based
approaches to meet the service requirements and energy efficiency targets in
various maritime communications scenarios.",0,1,0,0,0,0,0.716564,5.0,0.724457,18
66f76036-5f69-4cad-bf84-124de69b2982,Finding and Listing Front-door Adjustment Sets,5,0.396217,0.748572,"Identifying the effects of new interventions from data is a significant
challenge found across a wide range of the empirical sciences. A well-known
strategy for identifying such effects is Pearl's front-door (FD) criterion
(Pearl, 1995). The definition of the FD criterion is declarative, only allowing
one to decide whether a specific set satisfies the criterion. In this paper, we
present algorithms for finding and enumerating possible sets satisfying the FD
criterion in a given causal diagram. These results are useful in facilitating
the practical applications of the FD criterion for causal effects estimation
and helping scientists to select estimands with desired properties, e.g., based
on cost, feasibility of measurement, or statistical power.",1,0,0,0,0,0,0.930227,10.0,0.936983,46
62cc84e4-45ed-4c90-8a74-c4c51c3ac87e,Towards explainable evaluation of language models on the semantic similarity of visual concepts,2,0.00616646,0.193838,"Recent breakthroughs in NLP research, such as the advent of Transformer
models have indisputably contributed to major advancements in several tasks.
However, few works research robustness and explainability issues of their
evaluation strategies. In this work, we examine the behavior of high-performing
pre-trained language models, focusing on the task of semantic similarity for
visual vocabularies. First, we address the need for explainable evaluation
metrics, necessary for understanding the conceptual quality of retrieved
instances. Our proposed metrics provide valuable insights in local and global
level, showcasing the inabilities of widely used approaches. Secondly,
adversarial interventions on salient query semantics expose vulnerabilities of
opaque metrics and highlight patterns in learned linguistic representations.",0,0,0,0,0,0,0.471382,7.0,0.705764,36
f308d051-ea73-46ac-9467-067ceeb703c2,A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,59,0.091489,0.784893,"Deep reinforcement learning is a promising approach to learning policies in
uncontrolled environments that do not require domain knowledge. Unfortunately,
due to sample inefficiency, deep RL applications have primarily focused on
simulated environments. In this work, we demonstrate that the recent
advancements in machine learning algorithms and libraries combined with a
carefully tuned robot controller lead to learning quadruped locomotion in only
20 minutes in the real world. We evaluate our approach on several indoor and
outdoor terrains which are known to be challenging for classical model-based
controllers. We observe the robot to be able to learn walking gait consistently
on all of these terrains. Finally, we evaluate our design decisions in a
simulated environment.",0,1,0,0,0,0,0.0427325,8.0,0.40744,69
9c5cce4f-54cf-4bf2-bfee-ce9dfad22dfe,PREF: Predictability Regularized Neural Motion Fields,7,0.145638,0.251673,"Knowing the 3D motions in a dynamic scene is essential to many vision
applications. Recent progress is mainly focused on estimating the activity of
some specific elements like humans. In this paper, we leverage a neural motion
field for estimating the motion of all points in a multiview setting. Modeling
the motion from a dynamic scene with multiview data is challenging due to the
ambiguities in points of similar color and points with time-varying color. We
propose to regularize the estimated motion to be predictable. If the motion
from previous frames is known, then the motion in the near future should be
predictable. Therefore, we introduce a predictability regularization by first
conditioning the estimated motion on latent embeddings, then by adopting a
predictor network to enforce predictability on the embeddings. The proposed
framework PREF (Predictability REgularized Fields) achieves on par or better
results than state-of-the-art neural motion field-based dynamic scene
representation methods, while requiring no prior knowledge of the scene.",0,0,0,0,0,0,0.967078,4.0,0.904584,64
95dad649-f4ca-4c8f-917b-9ea6ff5c226a,An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models,20,0.163705,0.320632,"We propose a framework for training non-autoregressive sequence-to-sequence
models for editing tasks, where the original input sequence is iteratively
edited to produce the output. We show that the imitation learning algorithms
designed to train such models for machine translation introduces mismatches
between training and inference that lead to undertraining and poor
generalization in editing scenarios. We address this issue with two
complementary strategies: 1) a roll-in policy that exposes the model to
intermediate training sequences that it is more likely to encounter during
inference, 2) a curriculum that presents easy-to-learn edit operations first,
gradually increasing the difficulty of training samples as the model becomes
competent. We show the efficacy of these strategies on two challenging English
editing tasks: controllable text simplification and abstractive summarization.
Our approach significantly improves output quality on both tasks and controls
output complexity better on the simplification task.",0,1,0,0,0,0,0.443885,7.0,0.693922,56
5c69f65e-ace3-4295-9d0c-5bc2392e669d,Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients,18,0.173056,0.591824,"Deep neural networks for 3D point cloud classification, such as PointNet,
have been demonstrated to be vulnerable to adversarial attacks. Current
adversarial defenders often learn to denoise the (attacked) point clouds by
reconstruction, and then feed them to the classifiers as input. In contrast to
the literature, we propose a family of robust structured declarative
classifiers for point cloud classification, where the internal constrained
optimization mechanism can effectively defend adversarial attacks through
implicit gradients. Such classifiers can be formulated using a bilevel
optimization framework. We further propose an effective and efficient
instantiation of our approach, namely, Lattice Point Classifier (LPC), based on
structured sparse coding in the permutohedral lattice and 2D convolutional
neural networks (CNNs) that is end-to-end trainable. We demonstrate
state-of-the-art robust point cloud classification performance on ModelNet40
and ScanNet under seven different attackers. For instance, we achieve 89.51%
and 83.16% test accuracy on each dataset under the recent JGBA attacker that
outperforms DUP-Net and IF-Defense with PointNet by ~70%. Demo code is
available at https://zhang-vislab.github.io.",1,0,0,0,1,0,0.544036,7.0,0.735561,75
09d6c9cd-366c-4953-bea4-6d2263e30ae8,BayesFormer: Transformer with Uncertainty Estimation,6,0.0660328,0.426247,"Transformer has become ubiquitous due to its dominant performance in various
NLP and image processing tasks. However, it lacks understanding of how to
generate mathematically grounded uncertainty estimates for transformer
architectures. Models equipped with such uncertainty estimates can typically
improve predictive performance, make networks robust, avoid over-fitting and
used as acquisition function in active learning. In this paper, we introduce
BayesFormer, a Transformer model with dropouts designed by Bayesian theory. We
proposed a new theoretical framework to extend the approximate variational
inference-based dropout to Transformer-based architectures. Through extensive
experiments, we validate the proposed architecture in four paradigms and show
improvements across the board: language modeling and classification,
long-sequence understanding, machine translation and acquisition function for
active learning.",0,0,0,0,0,0,0.810283,9.0,0.877635,70
2cb4b024-ad7b-4ebe-b95d-4e8afe61bd00,Learning Deformable Object Manipulation from Expert Demonstrations,16,0.406872,0.572537,"We present a novel Learning from Demonstration (LfD) method, Deformable
Manipulation from Demonstrations (DMfD), to solve deformable manipulation tasks
using states or images as inputs, given expert demonstrations. Our method uses
demonstrations in three different ways, and balances the trade-off between
exploring the environment online and using guidance from experts to explore
high dimensional spaces effectively. We test DMfD on a set of representative
manipulation tasks for a 1-dimensional rope and a 2-dimensional cloth from the
SoftGym suite of tasks, each with state and image observations. Our method
exceeds baseline performance by up to 12.9% for state-based tasks and up to
33.44% on image-based tasks, with comparable or better robustness to
randomness. Additionally, we create two challenging environments for folding a
2D cloth using image-based observations, and set a performance benchmark for
them. We deploy DMfD on a real robot with a minimal loss in normalized
performance during real-world execution compared to simulation (~6%). Source
code is on github.com/uscresl/dmfd",0,1,0,0,1,0,0.822467,8.0,0.867234,40
8cb0264b-c445-4997-a88b-4c3d9acbe798,CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR Segmentation,43,0.129687,0.92621,"3D LiDAR semantic segmentation is fundamental for autonomous driving. Several
Unsupervised Domain Adaptation (UDA) methods for point cloud data have been
recently proposed to improve model generalization for different sensors and
environments. Researchers working on UDA problems in the image domain have
shown that sample mixing can mitigate domain shift. We propose a new approach
of sample mixing for point cloud UDA, namely Compositional Semantic Mix
(CoSMix), the first UDA approach for point cloud segmentation based on sample
mixing. CoSMix consists of a two-branch symmetric network that can process
labelled synthetic data (source) and real-world unlabelled point clouds
(target) concurrently. Each branch operates on one domain by mixing selected
pieces of data from the other one, and by using the semantic information
derived from source labels and target pseudo-labels. We evaluate CoSMix on two
large-scale datasets, showing that it outperforms state-of-the-art methods by a
large margin. Our code is available at
https://github.com/saltoricristiano/cosmix-uda.",1,0,0,0,0,0,0.525715,6.0,0.68291,46
6cb09006-fea7-4399-b48b-5d690617467a,"""What makes a question inquisitive?"" A Study on Type-Controlled Inquisitive Question Generation",9,0.0442258,0.46581,"We propose a type-controlled framework for inquisitive question generation.
We annotate an inquisitive question dataset with question types, train question
type classifiers, and finetune models for type-controlled question generation.
Empirical results demonstrate that we can generate a variety of questions that
adhere to specific types while drawing from the source texts. We also
investigate strategies for selecting a single question from a generated set,
considering both an informative vs.~inquisitive question classifier and a
pairwise ranker trained from a small set of expert annotations. Question
selection using the pairwise ranker yields strong results in automatic and
manual evaluation. Our human evaluation assesses multiple aspects of the
generated questions, finding that the ranker chooses questions with the best
syntax (4.59), semantics (4.37), and inquisitiveness (3.92) on a scale of 1-5,
even rivaling the performance of human-written questions.",0,1,0,1,0,0,0.310803,7.0,0.628902,63
27030c5e-4ded-4ea3-b39c-003d78e672a7,Probing for the Usage of Grammatical Number,37,0.818193,0.913343,"A central quest of probing is to uncover how pre-trained models encode a
linguistic property within their representations. An encoding, however, might
be spurious-i.e., the model might not rely on it when making predictions. In
this paper, we try to find encodings that the model actually uses, introducing
a usage-based probing setup. We first choose a behavioral task which cannot be
solved without using the linguistic property. Then, we attempt to remove the
property by intervening on the model's representations. We contend that, if an
encoding is used by the model, its removal should harm the performance on the
chosen behavioral task. As a case study, we focus on how BERT encodes
grammatical number, and on how it uses this encoding to solve the number
agreement task. Experimentally, we find that BERT relies on a linear encoding
of grammatical number to produce the correct behavioral output. We also find
that BERT uses a separate encoding of grammatical number for nouns and verbs.
Finally, we identify in which layers information about grammatical number is
transferred from a noun to its head verb.",0,0,0,0,0,0,0.968222,5.0,0.925728,62
fd4e1670-5b26-4a08-86f7-ed72943e166f,Learning to Transfer Prompts for Text Generation,28,0.520761,0.253661,"Pretrained language models (PLMs) have made remarkable progress in text
generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in
a data-scarce situation. Therefore, it is non-trivial to develop a general and
lightweight model that can adapt to various text generation tasks based on
PLMs. To fulfill this purpose, the recent prompt-based learning offers a
potential solution. In this paper, we improve this technique and propose a
novel prompt-based method (PTG) for text generation in a transferable setting.
First, PTG learns a set of source prompts for various source generation tasks
and then transfers these prompts as target prompts to perform target generation
tasks. To consider both task- and instance-level information, we design an
adaptive attention mechanism to derive the target prompts. For each data
instance, PTG learns a specific target prompt by attending to highly relevant
source prompts. In extensive experiments, PTG yields competitive or better
results than fine-tuning methods. We release our source prompts as an open
resource, where users can add or reuse them to improve new text generation
tasks for future research. Code and data can be available at
https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.",1,1,0,0,0,1,0.91912,6.0,0.885459,48
6b577ddf-048c-47e6-970d-edc291ea4956,Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach for Rotating Machinery exploiting Augmented Synthetic Data,11,0.0807799,0.557199,"Artificial Intelligence (AI) is one of the approaches that has been proposed
to analyze the collected data (e.g., vibration signals) providing a diagnosis
of the asset's operating condition. It is known that models trained with
labeled data (supervised) achieve excellent results, but two main problems make
their application in production processes difficult: (i) impossibility or long
time to obtain a sample of all operational conditions (since faults seldom
happen) and (ii) high cost of experts to label all acquired data. Another
limitating factor for the applicability of AI approaches in this context is the
lack of interpretability of the models (black-boxes), which reduces the
confidence of the diagnosis and trust/adoption from users. To overcome these
problems, a new generic and interpretable approach for classifying faults in
rotating machinery based on transfer learning from augmented synthetic data to
real rotating machinery is here proposed, namelly FaultD-XAI (Fault Diagnosis
using eXplainable AI). To provide scalability using transfer learning,
synthetic vibration signals are created mimicking the characteristic behavior
of failures in operation. The application of Gradient-weighted Class Activation
Mapping (Grad-CAM) with 1D Convolutional Neural Network (1D CNN) allows the
interpretation of results, supporting the user in decision making and
increasing diagnostic confidence. The proposed approach not only obtained
promising diagnostic performance, but was also able to learn characteristics
used by experts to identify conditions in a source domain and apply them in
another target domain. The experimental results suggest a promising approach on
exploiting transfer learning, synthetic data and explainable artificial
intelligence for fault diagnosis. Lastly, to guarantee reproducibility and
foster research in the field, the developed dataset is made publicly available.",1,1,0,1,0,0,0.138189,8.0,0.560608,71
b7253f8f-e40c-4fb2-a069-d1e4d6d304ca,Patch-wise Contrastive Style Learning for Instagram Filter Removal,6,0.0598767,0.811488,"Image-level corruptions and perturbations degrade the performance of CNNs on
different downstream vision tasks. Social media filters are one of the most
common resources of various corruptions and perturbations for real-world visual
analysis applications. The negative effects of these distractive factors can be
alleviated by recovering the original images with their pure style for the
inference of the downstream vision tasks. Assuming these filters substantially
inject a piece of additional style information to the social media images, we
can formulate the problem of recovering the original versions as a reverse
style transfer problem. We introduce Contrastive Instagram Filter Removal
Network (CIFR), which enhances this idea for Instagram filter removal by
employing a novel multi-layer patch-wise contrastive style learning mechanism.
Experiments show our proposed strategy produces better qualitative and
quantitative results than the previous studies. Moreover, we present the
results of our additional experiments for proposed architecture within
different settings. Finally, we present the inference outputs and quantitative
comparison of filtered and recovered images on localization and segmentation
tasks to encourage the main motivation for this problem.",1,1,0,0,0,0,0.583442,11.0,0.841624,57
0ebc8d90-cfd4-48e4-a98c-e156e0f61093,WeDef: Weakly Supervised Backdoor Defense for Text Classification,7,0.0176176,0.241099,"Existing backdoor defense methods are only effective for limited trigger
types. To defend different trigger types at once, we start from the
class-irrelevant nature of the poisoning process and propose a novel weakly
supervised backdoor defense framework WeDef. Recent advances in weak
supervision make it possible to train a reasonably accurate text classifier
using only a small number of user-provided, class-indicative seed words. Such
seed words shall be considered independent of the triggers. Therefore, a weakly
supervised text classifier trained by only the poisoned documents without their
labels will likely have no backdoor. Inspired by this observation, in WeDef, we
define the reliability of samples based on whether the predictions of the weak
classifier agree with their labels in the poisoned training set. We further
improve the results through a two-phase sanitization: (1) iteratively refine
the weak classifier based on the reliable samples and (2) train a binary poison
classifier by distinguishing the most unreliable samples from the most reliable
samples. Finally, we train the sanitized model on the samples that the poison
classifier predicts as benign. Extensive experiments show that WeDefis
effective against popular trigger-based attacks (e.g., words, sentences, and
paraphrases), outperforming existing defense methods.",0,1,0,0,1,0,0.219948,5.0,0.399565,26
552b2305-3b7e-4c13-b54b-2819204229c9,Investigating Lexical Replacements for Arabic-English Code-Switched Data Augmentation,5,0.0558884,0.349556,"Data sparsity is a main problem hindering the development of code-switching
(CS) NLP systems. In this paper, we investigate data augmentation techniques
for synthesizing dialectal Arabic-English CS text. We perform lexical
replacements using word-aligned parallel corpora where CS points are either
randomly chosen or learnt using a sequence-to-sequence model. We compare these
approaches against dictionary-based replacements. We assess the quality of the
generated sentences through human evaluation and evaluate the effectiveness of
data augmentation on machine translation (MT), automatic speech recognition
(ASR), and speech translation (ST) tasks. Results show that using a predictive
model results in more natural CS sentences compared to the random approach, as
reported in human judgements. In the downstream tasks, despite the random
approach generating more data, both approaches perform equally (outperforming
dictionary-based replacements). Overall, data augmentation achieves 34%
improvement in perplexity, 5.2% relative improvement on WER for ASR task,
+4.0-5.1 BLEU points on MT task, and +2.1-2.2 BLEU points on ST over a baseline
trained on available data without augmentation.",0,1,0,0,0,0,0.118859,7.0,0.474763,71
d21e4321-9eae-4e06-b727-3342c9d43911,Lymphoma segmentation from 3D PET-CT images using a deep evidential network,19,0.045023,0.304672,"An automatic evidential segmentation method based on Dempster-Shafer theory
and deep learning is proposed to segment lymphomas from three-dimensional
Positron Emission Tomography (PET) and Computed Tomography (CT) images. The
architecture is composed of a deep feature-extraction module and an evidential
layer. The feature extraction module uses an encoder-decoder framework to
extract semantic feature vectors from 3D inputs. The evidential layer then uses
prototypes in the feature space to compute a belief function at each voxel
quantifying the uncertainty about the presence or absence of a lymphoma at this
location. Two evidential layers are compared, based on different ways of using
distances to prototypes for computing mass functions. The whole model is
trained end-to-end by minimizing the Dice loss function. The proposed
combination of deep feature extraction and evidential segmentation is shown to
outperform the baseline UNet model as well as three other state-of-the-art
models on a dataset of 173 patients.",0,1,0,0,1,0,0.0394032,8.0,0.397086,62
64cf6061-d718-45be-966f-205f51ad120d,MALM: Mixing Augmented Language Modeling for Zero-Shot Machine Translation,1,0.0162013,0.0361585,"Large pre-trained language models have brought remarkable progress in NLP.
Pre-training and Fine-tuning have given state-of-art performance across tasks
in text processing. Data Augmentation techniques have also helped build
state-of-art models on low or zero resource tasks. Many works in the past have
attempted at learning a single massively-multilingual machine translation model
for zero-shot translation. Although those translation models are producing
correct translations, the main challenge is those models are producing the
wrong languages for zero-shot translation. This work and its results indicate
that prompt conditioned large models do not suffer from off-target language
errors i.e. errors arising due to translation to wrong languages. We
empirically demonstrate the effectiveness of self-supervised pre-training and
data augmentation for zero-shot multi-lingual machine translation.",0,1,0,0,0,0,0.729001,6.0,0.776209,28
2eeade2d-2b54-4a94-884c-ec36fce54aba,PSDoodle: Searching for App Screens via Interactive Sketching,5,0.103214,0.592237,"Keyword-based mobile screen search does not account for screen content and
fails to operate as a universal tool for all levels of users. Visual searching
(e.g., image, sketch) is structured and easy to adopt. Current visual search
approaches count on a complete screen and are therefore slow and tedious.
PSDoodle employs a deep neural network to recognize partial screen element
drawings instantly on a digital drawing interface and shows results in
real-time. PSDoodle is the first tool that utilizes partial sketches and
searches for screens in an interactive iterative way. PSDoodle supports
different drawing styles and retrieves search results that are relevant to the
user's sketch query. A short video demonstration is available online at:
https://youtu.be/3cVLHFm5pY4",1,1,0,0,1,0,0.0267384,26.0,0.799324,20
50802b07-8019-4229-aed3-35438a2c1d72,Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems,15,0.217256,0.518913,"Users interacting with voice assistants today need to phrase their requests
in a very specific manner to elicit an appropriate response. This limits the
user experience, and is partly due to the lack of reasoning capabilities of
dialogue platforms and the hand-crafted rules that require extensive labor. One
possible way to improve user experience and relieve the manual efforts of
designers is to build an end-to-end dialogue system that can do reasoning
itself while perceiving user's utterances. In this work, we propose a novel
method to incorporate the knowledge reasoning capability into dialogue systems
in a more scalable and generalizable manner. Our proposed method allows a
single transformer model to directly walk on a large-scale knowledge graph to
generate responses. To the best of our knowledge, this is the first work to
have transformer models generate responses by reasoning over differentiable
knowledge graphs. We investigate the reasoning abilities of the proposed method
on both task-oriented and domain-specific chit-chat dialogues. Empirical
results show that this method can effectively and efficiently incorporate a
knowledge graph into a dialogue system with fully-interpretable reasoning
paths.",1,0,0,0,0,0,0.735689,8.0,0.834527,50
6bf6cfe3-d9ef-4c07-b46b-fab2ec3b2547,TCTrack: Temporal Contexts for Aerial Tracking,68,0.652621,0.991593,"Temporal contexts among consecutive frames are far from being fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier.",1,1,0,0,1,0,0.916595,7.0,0.900064,81
14fe3563-bca2-4b23-8234-11f6d70652dc,DiffDreamer: Towards Consistent Unsupervised Single-view Scene Extrapolation with Conditional Diffusion Models,16,0.0545382,0.406954,"Scene extrapolation -- the idea of generating novel views by flying into a
given image -- is a promising, yet challenging task. For each predicted frame,
a joint inpainting and 3D refinement problem has to be solved, which is ill
posed and includes a high level of ambiguity. Moreover, training data for
long-range scenes is difficult to obtain and usually lacks sufficient views to
infer accurate camera poses. We introduce DiffDreamer, an unsupervised
framework capable of synthesizing novel views depicting a long camera
trajectory while training solely on internet-collected images of nature scenes.
Utilizing the stochastic nature of the guided denoising steps, we train the
diffusion models to refine projected RGBD images but condition the denoising
steps on multiple past and future frames for inference. We demonstrate that
image-conditioned diffusion models can effectively perform long-range scene
extrapolation while preserving consistency significantly better than prior
GAN-based methods. DiffDreamer is a powerful and efficient solution for scene
extrapolation, producing impressive results despite limited supervision.
Project page: https://primecai.github.io/diffdreamer.",0,0,0,0,0,0,0.673277,5.0,0.700536,109
9426e461-cd82-4e6a-8a8f-556f4c1dafee,Translating Hanja Historical Documents to Contemporary Korean and English,3,0.00742163,0.248707,"The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of
Joseon, the 500-year kingdom preceding the modern nation of Korea. The Annals
were originally written in an archaic Korean writing system, `Hanja', and were
translated into Korean from 1968 to 1993. The resulting translation was however
too literal and contained many archaic Korean words; thus, a new expert
translation effort began in 2012. Since then, the records of only one king have
been completed in a decade. In parallel, expert translators are working on
English translation, also at a slow pace and produced only one king's records
in English so far. Thus, we propose H2KE, a neural machine translation model,
that translates historical documents in Hanja to more easily understandable
Korean and to English. Built on top of multilingual neural machine translation,
H2KE learns to translate a historical document written in Hanja, from both a
full dataset of outdated Korean translation and a small dataset of more
recently translated contemporary Korean and English. We compare our method
against two baselines: a recent model that simultaneously learns to restore and
translate Hanja historical document and a Transformer based model trained only
on newly translated corpora. The experiments reveal that our method
significantly outperforms the baselines in terms of BLEU scores for both
contemporary Korean and English translations. We further conduct extensive
human evaluation which shows that our translation is preferred over the
original expert translations by both experts and non-expert Korean speakers.",1,1,0,0,0,0,0.0237615,12.0,0.555239,23
c9c7e888-cb49-431c-9f19-7434e14802fa,Boundary Smoothing for Named Entity Recognition,42,0.270148,0.903493,"Neural named entity recognition (NER) models may easily encounter the
over-confidence issue, which degrades the performance and calibration. Inspired
by label smoothing and driven by the ambiguity of boundary annotation in NER
engineering, we propose boundary smoothing as a regularization technique for
span-based neural NER models. It re-assigns entity probabilities from annotated
spans to the surrounding ones. Built on a simple but strong baseline, our model
achieves results better than or competitive with previous state-of-the-art
systems on eight well-known NER benchmarks. Further empirical analysis suggests
that boundary smoothing effectively mitigates over-confidence, improves model
calibration, and brings flatter neural minima and more smoothed loss
landscapes.",0,1,0,0,1,1,0.750549,9.0,0.857642,51
49ab3066-ff2f-4fce-a56b-ab5a00c3ce96,Towards Boosting the Open-Domain Chatbot with Human Feedback,10,0.185222,0.426885,"Many open-domain dialogue models pre-trained with social media comments can
generate coherent replies but have difficulties producing engaging responses
when interacting with real users. This phenomenon might mainly result from the
deficiency of annotated human-human conversations and the misalignment with
human preference. In this paper, we propose a novel and efficient approach
Diamante to boost the open-domain chatbot, where two kinds of human feedback
(including explicit demonstration and implicit preference) are collected and
leveraged. By asking annotators to select or amend the model-generated
candidate responses, Diamante efficiently collects the human demonstrated
responses and constructs a Chinese chit-chat dataset. To enhance the alignment
with human preference, Diamante leverages the implicit preference in the data
collection process and introduces the generation-evaluation joint training.
Comprehensive experiments indicate that the Diamante dataset and joint training
paradigm can significantly boost the performance of Chinese pre-trained
dialogue models.",1,1,0,1,1,0,0.867043,5.0,0.818507,43
62e8472d-1de7-4290-b916-72b30ae1cc0d,Unifying Motion Deblurring and Frame Interpolation with Events,45,0.815261,0.98419,"Slow shutter speed and long exposure time of frame-based cameras often cause
visual blur and loss of inter-frame information, degenerating the overall
quality of captured videos. To this end, we present a unified framework of
event-based motion deblurring and frame interpolation for blurry video
enhancement, where the extremely low latency of events is leveraged to
alleviate motion blur and facilitate intermediate frame prediction.
Specifically, the mapping relation between blurry frames and sharp latent
images is first predicted by a learnable double integral network, and a fusion
network is then proposed to refine the coarse results via utilizing the
information from consecutive blurry inputs and the concurrent events. By
exploring the mutual constraints among blurry frames, latent images, and event
streams, we further propose a self-supervised learning framework to enable
network training with real-world blurry videos and events. Extensive
experiments demonstrate that our method compares favorably against the
state-of-the-art approaches and achieves remarkable performance on both
synthetic and real-world datasets.",1,1,0,0,1,0,0.969169,7.0,0.948197,37
0f4f313b-6417-45bc-98b8-843446ca4e65,Multi-Class Segmentation from Aerial Views using Recursive Noise Diffusion,9,0.0996339,0.540464,"Semantic segmentation from aerial views is a crucial task for autonomous
drones, as they rely on precise and accurate segmentation to navigate safely
and efficiently. However, aerial images present unique challenges such as
diverse viewpoints, extreme scale variations, and high scene complexity. In
this paper, we propose an end-to-end multi-class semantic segmentation
diffusion model that addresses these challenges. We introduce recursive
denoising to allow information to propagate through the denoising process, as
well as a hierarchical multi-scale approach that complements the diffusion
process. Our method achieves competitive results on the UAVid dataset and
state-of-the-art performance on the Vaihingen Building segmentation benchmark.
Being the first iteration of this method, it shows great promise for future
improvements.",1,1,0,0,1,0,0.850218,7.0,0.861667,66
4258244a-4dbb-4edd-b5d9-848041eb3a26,PointNeuron: 3D Neuron Reconstruction via Geometry and Topology Learning of Point Clouds,1,0.0393061,0.0403056,"Digital neuron reconstruction from 3D microscopy images is an essential
technique for investigating brain connectomics and neuron morphology. Existing
reconstruction frameworks use convolution-based segmentation networks to
partition the neuron from noisy backgrounds before applying the tracing
algorithm. The tracing results are sensitive to the raw image quality and
segmentation accuracy. In this paper, we propose a novel framework for 3D
neuron reconstruction. Our key idea is to use the geometric representation
power of the point cloud to better explore the intrinsic structural information
of neurons. Our proposed framework adopts one graph convolutional network to
predict the neural skeleton points and another one to produce the connectivity
of these points. We finally generate the target SWC file through the
interpretation of the predicted point coordinates, radius, and connections.
Evaluated on the Janelia-Fly dataset from the BigNeuron project, we show that
our framework achieves competitive neuron reconstruction performance. Our
geometry and topology learning of point clouds could further benefit 3D medical
image analysis, such as cardiac surface reconstruction. Our code is available
at https://github.com/RunkaiZhao/PointNeuron.",1,1,0,0,0,0,0.0610518,13.0,0.663528,57
12b13e01-54c2-4068-bfd3-f75685209d18,Rendering Nighttime Image Via Cascaded Color and Brightness Compensation,1,0.032688,0.0511761,"Image signal processing (ISP) is crucial for camera imaging, and neural
networks (NN) solutions are extensively deployed for daytime scenes. The lack
of sufficient nighttime image dataset and insights on nighttime illumination
characteristics poses a great challenge for high-quality rendering using
existing NN ISPs. To tackle it, we first built a high-resolution nighttime
RAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert
professionals. Meanwhile, to best capture the characteristics of nighttime
illumination light sources, we develop the CBUnet, a two-stage NN ISP to
cascade the compensation of color and brightness attributes. Experiments show
that our method has better visual quality compared to traditional ISP pipeline,
and is ranked at the second place in the NTIRE 2022 Night Photography Rendering
Challenge for two tracks by respective People's and Professional Photographer's
choices. The code and relevant materials are avaiable on our website:
https://njuvision.github.io/CBUnet.",0,1,1,1,1,0,0.106409,12.0,0.683819,40
5cf60820-27ad-45e6-a18a-84121ab8fbdc,Efficient Zero-shot Event Extraction with Context-Definition Alignment,8,0.337388,0.376613,"Event extraction (EE) is the task of identifying interested event mentions
from text. Conventional efforts mainly focus on the supervised setting.
However, these supervised models cannot generalize to event types out of the
pre-defined ontology. To fill this gap, many efforts have been devoted to the
zero-shot EE problem. This paper follows the trend of modeling event-type
semantics but moves one step further. We argue that using the static embedding
of the event type name might not be enough because a single word could be
ambiguous, and we need a sentence to define the type semantics accurately. To
model the definition semantics, we use two separate transformer models to
project the contextualized event mentions and corresponding definitions into
the same embedding space and then minimize their embedding distance via
contrastive learning. On top of that, we also propose a warming phase to help
the model learn the minor difference between similar definitions. We name our
approach Zero-shot Event extraction with Definition (ZED). Experiments on the
MAVEN dataset show that our model significantly outperforms all previous
zero-shot EE methods with fast inference speed due to the disjoint design.
Further experiments also show that ZED can be easily applied to the few-shot
setting when the annotation is available and consistently outperforms baseline
supervised methods.",1,1,0,0,1,0,0.880273,9.0,0.904797,47
27345a4f-463e-4ac7-92c8-54ada880a9b4,MPANet: Multi-Patch Attention For Infrared Small Target object Detection,7,0.269203,0.625713,"Infrared small target detection (ISTD) has attracted widespread attention and
been applied in various fields. Due to the small size of infrared targets and
the noise interference from complex backgrounds, the performance of ISTD using
convolutional neural networks (CNNs) is restricted. Moreover, the constriant
that long-distance dependent features can not be encoded by the vanilla CNNs
also impairs the robustness of capturing targets' shapes and locations in
complex scenarios. To this end, a multi-patch attention network (MPANet) based
on the axial-attention encoder and the multi-scale patch branch (MSPB)
structure is proposed. Specially, an axial-attention-improved encoder
architecture is designed to highlight the effective features of small targets
and suppress background noises. Furthermore, the developed MSPB structure fuses
the coarse-grained and fine-grained features from different semantic scales.
Extensive experiments on the SIRST dataset show the superiority performance and
effectiveness of the proposed MPANet compared to the state-of-the-art methods.",0,1,0,0,0,0,0.989717,5.0,0.982334,19
de863082-ec82-40d6-837b-7d53982fc216,Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models,9,0.271814,0.488645,"Typical deep neural network (DNN) backdoor attacks are based on triggers
embedded in inputs. Existing imperceptible triggers are computationally
expensive or low in attack success. In this paper, we propose a new backdoor
trigger, which is easy to generate, imperceptible, and highly effective. The
new trigger is a uniformly randomly generated three-dimensional (3D) binary
pattern that can be horizontally and/or vertically repeated and mirrored and
superposed onto three-channel images for training a backdoored DNN model.
Dispersed throughout an image, the new trigger produces weak perturbation to
individual pixels, but collectively holds a strong recognizable pattern to
train and activate the backdoor of the DNN. We also analytically reveal that
the trigger is increasingly effective with the improving resolution of the
images. Experiments are conducted using the ResNet-18 and MLP models on the
MNIST, CIFAR-10, and BTSR datasets. In terms of imperceptibility, the new
trigger outperforms existing triggers, such as BadNets, Trojaned NN, and Hidden
Backdoor, by over an order of magnitude. The new trigger achieves an almost
100% attack success rate, only reduces the classification accuracy by less than
0.7%-2.4%, and invalidates the state-of-the-art defense techniques.",0,1,0,0,0,0,0.815944,8.0,0.864597,56
c1d13638-3d79-4ccf-986b-2234a2f6a2e8,Synthesis of Stabilizing Recurrent Equilibrium Network Controllers,10,0.204891,0.517235,"We propose a parameterization of a nonlinear dynamic controller based on the
recurrent equilibrium network, a generalization of the recurrent neural
network. We derive constraints on the parameterization under which the
controller guarantees exponential stability of a partially observed dynamical
system with sector bounded nonlinearities. Finally, we present a method to
synthesize this controller using projected policy gradient methods to maximize
a reward function with arbitrary structure. The projection step involves the
solution of convex optimization problems. We demonstrate the proposed method
with simulated examples of controlling nonlinear plants, including plants
modeled with neural networks.",1,0,0,0,0,0,0.609489,6.0,0.721501,24
c3f8643d-8746-4eac-8a75-8559bc6893bd,HiSMatch: Historical Structure Matching based Temporal Knowledge Graph Reasoning,10,0.163968,0.825023,"A Temporal Knowledge Graph (TKG) is a sequence of KGs with respective
timestamps, which adopts quadruples in the form of (\emph{subject},
\emph{relation}, \emph{object}, \emph{timestamp}) to describe dynamic facts.
TKG reasoning has facilitated many real-world applications via answering such
queries as (\emph{query entity}, \emph{query relation}, \emph{?}, \emph{future
timestamp}) about future. This is actually a matching task between a query and
candidate entities based on their historical structures, which reflect
behavioral trends of the entities at different timestamps. In addition, recent
KGs provide background knowledge of all the entities, which is also helpful for
the matching. Thus, in this paper, we propose the \textbf{Hi}storical
\textbf{S}tructure \textbf{Match}ing (\textbf{HiSMatch}) model. It applies two
structure encoders to capture the semantic information contained in the
historical structures of the query and candidate entities. Besides, it adopts
another encoder to integrate the background knowledge into the model. TKG
reasoning experiments on six benchmark datasets demonstrate the significant
improvement of the proposed HiSMatch model, with up to 5.6\% performance
improvement in MRR, compared to the state-of-the-art baselines.",0,0,0,0,1,0,0.852846,7.0,0.862992,33
e1768579-fd7b-43dc-b3cb-6c75a5d279e3,Holistic Interaction Transformer Network for Action Detection,14,0.101107,0.850023,"Actions are about how we interact with the environment, including other
people, objects, and ourselves. In this paper, we propose a novel multi-modal
Holistic Interaction Transformer Network (HIT) that leverages the largely
ignored, but critical hand and pose information essential to most human
actions. The proposed ""HIT"" network is a comprehensive bi-modal framework that
comprises an RGB stream and a pose stream. Each of them separately models
person, object, and hand interactions. Within each sub-network, an
Intra-Modality Aggregation module (IMA) is introduced that selectively merges
individual interaction units. The resulting features from each modality are
then glued using an Attentive Fusion Mechanism (AFM). Finally, we extract cues
from the temporal context to better classify the occurring actions using cached
memory. Our method significantly outperforms previous approaches on the J-HMDB,
UCF101-24, and MultiSports datasets. We also achieve competitive results on
AVA. The code will be available at https://github.com/joslefaure/HIT.",1,1,0,0,1,0,0.298611,8.0,0.669257,58
88cb942d-aef3-40f4-8c7a-c1d068ee4382,Adaptive Mean-Residue Loss for Robust Facial Age Estimation,5,0.172302,0.565704,"Automated facial age estimation has diverse real-world applications in
multimedia analysis, e.g., video surveillance, and human-computer interaction.
However, due to the randomness and ambiguity of the aging process, age
assessment is challenging. Most research work over the topic regards the task
as one of age regression, classification, and ranking problems, and cannot well
leverage age distribution in representing labels with age ambiguity. In this
work, we propose a simple yet effective loss function for robust facial age
estimation via distribution learning, i.e., adaptive mean-residue loss, in
which, the mean loss penalizes the difference between the estimated age
distribution's mean and the ground-truth age, whereas the residue loss
penalizes the entropy of age probability out of dynamic top-K in the
distribution. Experimental results in the datasets FG-NET and CLAP2016 have
validated the effectiveness of the proposed loss. Our code is available at
https://github.com/jacobzhaoziyuan/AMR-Loss.",1,1,0,0,1,0,0.438302,11.0,0.803662,34
1b5d568a-67bc-4223-b8af-f688a4843460,CONDA: Continual Unsupervised Domain Adaptation Learning in Visual Perception for Self-Driving Cars,4,0.0224504,0.350859,"Although unsupervised domain adaptation methods have achieved remarkable
performance in semantic scene segmentation in visual perception for
self-driving cars, these approaches remain impractical in real-world use cases.
In practice, the segmentation models may encounter new data that have not been
seen yet. Also, the previous data training of segmentation models may be
inaccessible due to privacy problems. Therefore, to address these problems, in
this work, we propose a Continual Unsupervised Domain Adaptation (CONDA)
approach that allows the model to continuously learn and adapt with respect to
the presence of the new data. Moreover, our proposed approach is designed
without the requirement of accessing previous training data. To avoid the
catastrophic forgetting problem and maintain the performance of the
segmentation models, we present a novel Bijective Maximum Likelihood loss to
impose the constraint of predicted segmentation distribution shifts. The
experimental results on the benchmark of continual unsupervised domain
adaptation have shown the advanced performance of the proposed CONDA method.",0,1,0,0,1,0,0.363929,9.0,0.733052,77
565b8466-2fa9-43d2-8db8-f88ee465c100,Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Conservative Natural Policy Gradient Primal-Dual Algorithm,11,0.266027,0.521509,"We consider the problem of constrained Markov decision process (CMDP) in
continuous state-actions spaces where the goal is to maximize the expected
cumulative reward subject to some constraints. We propose a novel Conservative
Natural Policy Gradient Primal-Dual Algorithm (C-NPG-PD) to achieve zero
constraint violation while achieving state of the art convergence results for
the objective value function. For general policy parametrization, we prove
convergence of value function to global optimal upto an approximation error due
to restricted policy class. We even improve the sample complexity of existing
constrained NPG-PD algorithm \cite{Ding2020} from $\mathcal{O}(1/\epsilon^6)$
to $\mathcal{O}(1/\epsilon^4)$. To the best of our knowledge, this is the first
work to establish zero constraint violation with Natural policy gradient style
algorithms for infinite horizon discounted CMDPs. We demonstrate the merits of
proposed algorithm via experimental evaluations.",0,0,1,0,1,0,0.824105,6.0,0.82387,32
88b5145e-3338-4b74-bb01-41ae456c8b03,Instance-Aware Image Completion,1,0.0681483,0.102863,"Image completion is a task that aims to fill in the missing region of a
masked image with plausible contents. However, existing image completion
methods tend to fill in the missing region with the surrounding texture instead
of hallucinating a visual instance that is suitable in accordance with the
context of the scene. In this work, we propose a novel image completion model,
dubbed ImComplete, that hallucinates the missing instance that harmonizes well
with - and thus preserves - the original context. ImComplete first adopts a
transformer architecture that considers the visible instances and the location
of the missing region. Then, ImComplete completes the semantic segmentation
masks within the missing region, providing pixel-level semantic and structural
guidance. Finally, the image synthesis blocks generate photo-realistic content.
We perform a comprehensive evaluation of the results in terms of visual quality
(LPIPS and FID) and contextual preservation scores (CLIPscore and object
detection accuracy) with COCO-panoptic and Visual Genome datasets. Experimental
results show the superiority of ImComplete on various natural images.",0,1,0,0,0,0,0.986473,9.0,0.983321,60
0e10c627-6e77-421e-aa8f-c39fa4be1211,CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models,11,0.578937,0.230224,"Users often ask dialogue systems ambiguous questions that require
clarification. We show that current language models rarely ask users to clarify
ambiguous questions and instead provide incorrect answers. To address this, we
introduce CLAM: a framework for getting language models to selectively ask for
clarification about ambiguous user questions. In particular, we show that we
can prompt language models to detect whether a given question is ambiguous,
generate an appropriate clarifying question to ask the user, and give a final
answer after receiving clarification. We also show that we can simulate users
by providing language models with privileged information. This lets us
automatically evaluate multi-turn clarification dialogues. Finally, CLAM
significantly improves language models' accuracy on mixed ambiguous and
unambiguous questions relative to SotA.",0,1,0,0,1,0,0.937528,5.0,0.882101,26
7bbe9ef3-7a1f-42f8-a794-c22514e3c536,Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations,54,0.777846,0.793808,"We propose an unsupervised method for 3D geometry-aware representation
learning of articulated objects, in which no image-pose pairs or foreground
masks are used for training. Though photorealistic images of articulated
objects can be rendered with explicit pose control through existing 3D neural
representations, these methods require ground truth 3D pose and foreground
masks for training, which are expensive to obtain. We obviate this need by
learning the representations with GAN training. The generator is trained to
produce realistic images of articulated objects from random poses and latent
vectors by adversarial training. To avoid a high computational cost for GAN
training, we propose an efficient neural representation for articulated objects
based on tri-planes and then present a GAN-based framework for its unsupervised
training. Experiments demonstrate the efficiency of our method and show that
GAN-based training enables the learning of controllable 3D representations
without paired supervision.",0,0,0,0,0,0,0.981999,5.0,0.956237,77
91868934-cb28-4bdc-be97-98cae223e341,Spectral Adversarial Training for Robust Graph Neural Network,9,0.116382,0.793988,"Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable
to slight but adversarially designed perturbations, known as adversarial
examples. To address this issue, robust training methods against adversarial
examples have received considerable attention in the literature.
\emph{Adversarial Training (AT)} is a successful approach to learning a robust
model using adversarially perturbed training samples. Existing AT methods on
GNNs typically construct adversarial perturbations in terms of graph structures
or node features. However, they are less effective and fraught with challenges
on graph data due to the discreteness of graph structure and the relationships
between connected examples. In this work, we seek to address these challenges
and propose Spectral Adversarial Training (SAT), a simple yet effective
adversarial training approach for GNNs. SAT first adopts a low-rank
approximation of the graph structure based on spectral decomposition, and then
constructs adversarial perturbations in the spectral domain rather than
directly manipulating the original graph structure. To investigate its
effectiveness, we employ SAT on three widely used GNNs. Experimental results on
four public graph datasets demonstrate that SAT significantly improves the
robustness of GNNs against adversarial attacks without sacrificing
classification accuracy and training efficiency.",1,1,0,0,0,0,0.479143,8.0,0.74541,51
5e3e1e26-be28-4f28-af4f-9e34c058ae00,Depth Estimation with Simplified Transformer,14,0.0799601,0.681898,"Transformer and its variants have shown state-of-the-art results in many
vision tasks recently, ranging from image classification to dense prediction.
Despite of their success, limited work has been reported on improving the model
efficiency for deployment in latency-critical applications, such as autonomous
driving and robotic navigation. In this paper, we aim at improving upon the
existing transformers in vision, and propose a method for self-supervised
monocular Depth Estimation with Simplified Transformer (DEST), which is
efficient and particularly suitable for deployment on GPU-based platforms.
Through strategic design choices, our model leads to significant reduction in
model size, complexity, as well as inference latency, while achieving superior
accuracy as compared to state-of-the-art. We also show that our design
generalize well to other dense prediction task without bells and whistles.",0,1,0,0,1,0,0.830046,6.0,0.827133,33
a684ab87-abda-4a99-b993-5ec1a5e37a11,Diffusion models for missing value imputation in tabular data,21,0.281043,0.991869,"Missing value imputation in machine learning is the task of estimating the
missing values in the dataset accurately using available information. In this
task, several deep generative modeling methods have been proposed and
demonstrated their usefulness, e.g., generative adversarial imputation
networks. Recently, diffusion models have gained popularity because of their
effectiveness in the generative modeling task in images, texts, audio, etc. To
our knowledge, less attention has been paid to the investigation of the
effectiveness of diffusion models for missing value imputation in tabular data.
Based on recent development of diffusion models for time-series data
imputation, we propose a diffusion model approach called ""Conditional
Score-based Diffusion Models for Tabular data"" (TabCSDI). To effectively handle
categorical variables and numerical variables simultaneously, we investigate
three techniques: one-hot encoding, analog bits encoding, and feature
tokenization. Experimental results on benchmark datasets demonstrated the
effectiveness of TabCSDI compared with well-known existing methods, and also
emphasized the importance of the categorical embedding techniques.",0,1,0,0,0,0,0.871683,6.0,0.851664,27
6cbaa189-8cb5-4be0-9e4c-7b8b7b981a59,Weakly-supervised Pre-training for 3D Human Pose Estimation via Perspective Knowledge,10,0.0492042,0.474528,"Modern deep learning-based 3D pose estimation approaches require plenty of 3D
pose annotations. However, existing 3D datasets lack diversity, which limits
the performance of current methods and their generalization ability. Although
existing methods utilize 2D pose annotations to help 3D pose estimation, they
mainly focus on extracting 2D structural constraints from 2D poses, ignoring
the 3D information hidden in the images. In this paper, we propose a novel
method to extract weak 3D information directly from 2D images without 3D pose
supervision. Firstly, we utilize 2D pose annotations and perspective prior
knowledge to generate the relationship of that keypoint is closer or farther
from the camera, called relative depth. We collect a 2D pose dataset (MCPC) and
generate relative depth labels. Based on MCPC, we propose a weakly-supervised
pre-training (WSP) strategy to distinguish the depth relationship between two
points in an image. WSP enables the learning of the relative depth of two
keypoints on lots of in-the-wild images, which is more capable of predicting
depth and generalization ability for 3D human pose estimation. After
fine-tuning on 3D pose datasets, WSP achieves state-of-the-art results on two
widely-used benchmarks.",0,1,0,1,1,0,0.203072,8.0,0.613465,64
59e968e7-7d58-4040-a35c-1ba88efbf63a,An Overview of Distant Supervision for Relation Extraction with a Focus on Denoising and Pre-training Methods,4,0.0300413,0.206919,"Relation Extraction (RE) is a foundational task of natural language
processing. RE seeks to transform raw, unstructured text into structured
knowledge by identifying relational information between entity pairs found in
text. RE has numerous uses, such as knowledge graph completion, text
summarization, question-answering, and search querying. The history of RE
methods can be roughly organized into four phases: pattern-based RE,
statistical-based RE, neural-based RE, and large language model-based RE. This
survey begins with an overview of a few exemplary works in the earlier phases
of RE, highlighting limitations and shortcomings to contextualize progress.
Next, we review popular benchmarks and critically examine metrics used to
assess RE performance. We then discuss distant supervision, a paradigm that has
shaped the development of modern RE methods. Lastly, we review recent RE works
focusing on denoising and pre-training methods.",0,0,0,0,0,0,0.396457,11.0,0.791568,53
97ce9043-e3c7-472b-a66a-476306cdda15,Synthetic Disinformation Attacks on Automated Fact Verification Systems,19,0.404462,0.908585,"Automated fact-checking is a needed technology to curtail the spread of
online misinformation. One current framework for such solutions proposes to
verify claims by retrieving supporting or refuting evidence from related
textual sources. However, the realistic use cases for fact-checkers will
require verifying claims against evidence sources that could be affected by the
same misinformation. Furthermore, the development of modern NLP tools that can
produce coherent, fabricated content would allow malicious actors to
systematically generate adversarial disinformation for fact-checkers.
  In this work, we explore the sensitivity of automated fact-checkers to
synthetic adversarial evidence in two simulated settings: AdversarialAddition,
where we fabricate documents and add them to the evidence repository available
to the fact-checking system, and AdversarialModification, where existing
evidence source documents in the repository are automatically altered. Our
study across multiple models on three benchmarks demonstrates that these
systems suffer significant performance drops against these attacks. Finally, we
discuss the growing threat of modern NLG systems as generators of
disinformation in the context of the challenges they pose to automated
fact-checkers.",0,1,0,0,0,0,0.955518,5.0,0.905208,55
ba4cd1ac-2934-4b44-8e49-d754cccf95a8,Non-Deterministic Approximation Fixpoint Theory and Its Application in Disjunctive Logic Programming,3,0.01256,0.125867,"Approximation fixpoint theory (AFT) is an abstract and general algebraic
framework for studying the semantics of nonmonotonic logics. It provides a
unifying study of the semantics of different formalisms for nonmonotonic
reasoning, such as logic programming, default logic and autoepistemic logic. In
this paper, we extend AFT to dealing with non-deterministic constructs that
allow to handle indefinite information, represented e.g. by disjunctive
formulas. This is done by generalizing the main constructions and corresponding
results of AFT to non-deterministic operators, whose ranges are sets of
elements rather than single elements. The applicability and usefulness of this
generalization is illustrated in the context of disjunctive logic programming.",0,0,0,0,0,0,3.5138e-10,34.0,0.312394,62
5809328a-bd5c-4720-9cf3-a546ca7055eb,Circular Pythagorean fuzzy sets and applications to multi-criteria decision making,7,0.207644,0.644202,"In this paper, we introduce the concept of circular Pythagorean fuzzy set
(value) (C-PFS(V)) as a new generalization of both circular intuitionistic
fuzzy sets (C-IFSs) proposed by Atannassov and Pythagorean fuzzy sets (PFSs)
proposed by Yager. A circular Pythagorean fuzzy set is represented by a circle
that represents the membership degree and the non-membership degree and whose
center consists of non-negative real numbers $\mu$ and $\nu$ with the condition
$\mu^2+\nu^2\leq 1$. A C-PFS models the fuzziness of the uncertain information
more properly thanks to its structure that allows modelling the information
with points of a circle of a certain center and a radius. Therefore, a C-PFS
lets decision makers to evaluate objects in a larger and more flexible region
and thus more sensitive decisions can be made. After defining the concept of
C-PFS we define some fundamental set operations between C-PFSs and propose some
algebraic operations between C-PFVs via general $t$-norms and $t$-conorms. By
utilizing these algebraic operations, we introduce some weighted aggregation
operators to transform input values represented by C-PFVs to a single output
value. Then to determine the degree of similarity between C-PFVs we define a
cosine similarity measure based on radius. Furthermore, we develop a method to
transform a collection of Pythagorean fuzzy values to a PFS. Finally, a method
is given to solve multi-criteria decision making problems in circular
Pythagorean fuzzy environment and the proposed method is practiced to a problem
about selecting the best photovoltaic cell from the literature. We also study
the comparison analysis and time complexity of the proposed method.",0,0,1,1,0,0,0.262733,13.0,0.784806,42
02896e49-bf8f-4c29-8ac0-d33ce82455d7,Towards Robust k-Nearest-Neighbor Machine Translation,10,0.21136,0.713316,"k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research
direction of NMT in recent years. Its main idea is to retrieve useful key-value
pairs from an additional datastore to modify translations without updating the
NMT model. However, the underlying retrieved noisy pairs will dramatically
deteriorate the model performance. In this paper, we conduct a preliminary
study and find that this problem results from not fully exploiting the
prediction of the NMT model. To alleviate the impact of noise, we propose a
confidence-enhanced kNN-MT model with robust training. Concretely, we introduce
the NMT confidence to refine the modeling of two important components of
kNN-MT: kNN distribution and the interpolation weight. Meanwhile we inject two
types of perturbations into the retrieved pairs for robust training.
Experimental results on four benchmark datasets demonstrate that our model not
only achieves significant improvements over current kNN-MT models, but also
exhibits better robustness. Our code is available at
https://github.com/DeepLearnXMU/Robust-knn-mt.",1,1,0,0,0,0,0.313182,8.0,0.676445,30
659ad065-c213-43d8-bb0a-95ec27e59440,Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning,4,0.179751,0.440975,"Federated Learning (FL) is a machine learning technique that addresses the
privacy challenges in terms of access rights of local datasets by enabling the
training of a model across nodes holding their data samples locally. To achieve
decentralized federated learning, blockchain-based FL was proposed as a
distributed FL architecture. In decentralized FL, the chief is eliminated from
the learning process as workers collaborate between each other to train the
global model. Decentralized FL applications need to account for the additional
delay incurred by blockchain-based FL deployments. Particularly in this
setting, to detect targeted/untargeted poisoning attacks, we investigate the
end-to-end learning completion latency of a realistic decentralized FL process
protected against poisoning attacks. We propose a technique which consists in
decoupling the monitoring phase from the detection phase in defenses against
poisoning attacks in a decentralized federated learning deployment that aim at
monitoring the behavior of the workers. We demonstrate that our proposed
blockchain-based monitoring improved network scalability, robustness and time
efficiency. The parallelization of operations results in minimized latency over
the end-to-end communication, computation, and consensus delays incurred during
the FL and blockchain operations.",1,1,0,0,0,0,0.987803,9.0,0.985962,22
d5b94a73-a8ae-4727-ac36-3c5998d901ce,A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank,14,0.263981,0.527381,"We show that the choice of pretraining languages affects downstream
cross-lingual transfer for BERT-based models. We inspect zero-shot performance
in balanced data conditions to mitigate data size confounds, classifying
pretraining languages that improve downstream performance as donors, and
languages that are improved in zero-shot performance as recipients. We develop
a method of quadratic time complexity in the number of languages to estimate
these relations, instead of an exponential exhaustive computation of all
possible combinations. We find that our method is effective on a diverse set of
languages spanning different linguistic features and two downstream tasks. Our
findings can inform developers of large-scale multilingual language models in
choosing better pretraining configurations.",1,0,0,0,0,0,0.796445,6.0,0.80924,29
05b2b505-605b-44a6-9fad-c3ad19482a7b,Parallel Instance Query Network for Named Entity Recognition,34,0.666361,0.533566,"Named entity recognition (NER) is a fundamental task in natural language
processing. Recent works treat named entity recognition as a reading
comprehension task, constructing type-specific queries manually to extract
entities. This paradigm suffers from three issues. First, type-specific queries
can only extract one type of entities per inference, which is inefficient.
Second, the extraction for different types of entities is isolated, ignoring
the dependencies between them. Third, query construction relies on external
knowledge and is difficult to apply to realistic scenarios with hundreds of
entity types. To deal with them, we propose Parallel Instance Query Network
(PIQN), which sets up global and learnable instance queries to extract entities
from a sentence in a parallel manner. Each instance query predicts one entity,
and by feeding all instance queries simultaneously, we can query all entities
in parallel. Instead of being constructed from external knowledge, instance
queries can learn their different query semantics during training. For training
the model, we treat label assignment as a one-to-many Linear Assignment Problem
(LAP) and dynamically assign gold entities to instance queries with minimal
assignment cost. Experiments on both nested and flat NER datasets demonstrate
that our proposed method outperforms previous state-of-the-art models.",1,1,0,0,1,0,0.951232,5.0,0.899209,67
bba72c24-f64f-467a-bc34-68aa8d75b9ed,Great Truths are Always Simple: A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models,11,0.140714,0.249401,"Commonsense reasoning in natural language is a desired ability of artificial
intelligent systems. For solving complex commonsense reasoning tasks, a typical
solution is to enhance pre-trained language models~(PTMs) with a
knowledge-aware graph neural network~(GNN) encoder that models a commonsense
knowledge graph~(CSKG). Despite the effectiveness, these approaches are built
on heavy architectures, and can't clearly explain how external knowledge
resources improve the reasoning capacity of PTMs. Considering this issue, we
conduct a deep empirical analysis, and find that it is indeed relation features
from CSKGs (but not node features) that mainly contribute to the performance
improvement of PTMs. Based on this finding, we design a simple MLP-based
knowledge encoder that utilizes statistical relation paths as features.
Extensive experiments conducted on five benchmarks demonstrate the
effectiveness of our approach, which also largely reduces the parameters for
encoding CSKGs. Our codes and data are publicly available at
https://github.com/RUCAIBox/SAFE.",1,0,0,0,0,0,0.892023,6.0,0.865116,41
9193e6b2-08bb-483d-b6c5-7edfe9b44863,Double Retrieval and Ranking for Accurate Question Answering,6,0.252241,0.632121,"Recent work has shown that an answer verification step introduced in
Transformer-based answer selection models can significantly improve the state
of the art in Question Answering. This step is performed by aggregating the
embeddings of top $k$ answer candidates to support the verification of a target
answer. Although the approach is intuitive and sound still shows two
limitations: (i) the supporting candidates are ranked only according to the
relevancy with the question and not with the answer, and (ii) the support
provided by the other answer candidates is suboptimal as these are retrieved
independently of the target answer. In this paper, we address both drawbacks by
proposing (i) a double reranking model, which, for each target answer, selects
the best support; and (ii) a second neural retrieval stage designed to encode
question and answer pair as the query, which finds more specific verification
information. The results on three well-known datasets for AS2 show consistent
and significant improvement of the state of the art.",0,1,0,0,1,0,0.929103,7.0,0.909116,32
7ac6552f-6317-43ec-a165-d73997090a22,An Outlier Exposure Approach to Improve Visual Anomaly Detection Performance for Mobile Robots,7,0.0521331,0.509035,"We consider the problem of building visual anomaly detection systems for
mobile robots. Standard anomaly detection models are trained using large
datasets composed only of non-anomalous data. However, in robotics
applications, it is often the case that (potentially very few) examples of
anomalies are available. We tackle the problem of exploiting these data to
improve the performance of a Real-NVP anomaly detection model, by minimizing,
jointly with the Real-NVP loss, an auxiliary outlier exposure margin loss. We
perform quantitative experiments on a novel dataset (which we publish as
supplementary material) designed for anomaly detection in an indoor patrolling
scenario. On a disjoint test set, our approach outperforms alternatives and
shows that exposing even a small number of anomalous frames yields significant
performance improvements.",1,1,0,1,0,0,0.186871,13.0,0.754991,42
717dcc39-95a3-4203-8183-49fef1d3b1b9,Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages,32,0.139356,0.975736,"Human languages are full of metaphorical expressions. Metaphors help people
understand the world by connecting new concepts and domains to more familiar
ones. Large pre-trained language models (PLMs) are therefore assumed to encode
metaphorical knowledge useful for NLP systems. In this paper, we investigate
this hypothesis for PLMs, by probing metaphoricity information in their
encodings, and by measuring the cross-lingual and cross-dataset generalization
of this information. We present studies in multiple metaphor detection datasets
and in four languages (i.e., English, Spanish, Russian, and Farsi). Our
extensive experiments suggest that contextual representations in PLMs do encode
metaphorical knowledge, and mostly in their middle layers. The knowledge is
transferable between languages and datasets, especially when the annotation is
consistent across training and testing sets. Our findings give helpful insights
for both cognitive and NLP scientists.",1,0,0,0,0,1,0.251222,6.0,0.525053,53
5f6b0833-6128-4a7d-bdae-8377b58c7064,Lifelong Learning Metrics,13,0.15692,0.214647,"The DARPA Lifelong Learning Machines (L2M) program seeks to yield advances in
artificial intelligence (AI) systems so that they are capable of learning (and
improving) continuously, leveraging data on one task to improve performance on
another, and doing so in a computationally sustainable way. Performers on this
program developed systems capable of performing a diverse range of functions,
including autonomous driving, real-time strategy, and drone simulation. These
systems featured a diverse range of characteristics (e.g., task structure,
lifetime duration), and an immediate challenge faced by the program's testing
and evaluation team was measuring system performance across these different
settings. This document, developed in close collaboration with DARPA and the
program performers, outlines a formalism for constructing and characterizing
the performance of agents performing lifelong learning scenarios.",0,0,0,0,0,0,0.773504,7.0,0.826571,28
cda7e3d9-2b2e-48ae-a202-e67144dfc57c,Tsetlin Machine for Solving Contextual Bandit Problems,5,0.00622169,0.340966,"This paper introduces an interpretable contextual bandit algorithm using
Tsetlin Machines, which solves complex pattern recognition tasks using
propositional logic. The proposed bandit learning algorithm relies on
straightforward bit manipulation, thus simplifying computation and
interpretation. We then present a mechanism for performing Thompson sampling
with Tsetlin Machine, given its non-parametric nature. Our empirical analysis
shows that Tsetlin Machine as a base contextual bandit learner outperforms
other popular base learners on eight out of nine datasets. We further analyze
the interpretability of our learner, investigating how arms are selected based
on propositional expressions that model the context.",0,1,0,0,0,0,0.0288614,7.0,0.265703,37
45bc522d-b067-4d3e-b42e-0e5b64709160,Long-term Control for Dialogue Generation: Methods and Evaluation,7,0.243617,0.530814,"Current approaches for controlling dialogue response generation are primarily
focused on high-level attributes like style, sentiment, or topic. In this work,
we focus on constrained long-term dialogue generation, which involves more
fine-grained control and requires a given set of control words to appear in
generated responses. This setting requires a model to not only consider the
generation of these control words in the immediate context, but also produce
utterances that will encourage the generation of the words at some time in the
(possibly distant) future. We define the problem of constrained long-term
control for dialogue generation, identify gaps in current methods for
evaluation, and propose new metrics that better measure long-term control. We
also propose a retrieval-augmented method that improves performance of
long-term controlled generation via logit modification techniques. We show
through experiments on three task-oriented dialogue datasets that our metrics
better assess dialogue control relative to current alternatives and that our
method outperforms state-of-the-art constrained generation baselines.",1,1,1,0,1,0,0.947813,6.0,0.912226,47
9b9d2764-d22b-4691-809f-4f97400070a8,IDANI: Inference-time Domain Adaptation via Neuron-level Interventions,5,0.0358302,0.459322,"Large pre-trained models are usually fine-tuned on downstream task data, and
tested on unseen data. When the train and test data come from different
domains, the model is likely to struggle, as it is not adapted to the test
domain. We propose a new approach for domain adaptation (DA), using
neuron-level interventions: We modify the representation of each test example
in specific neurons, resulting in a counterfactual example from the source
domain, which the model is more familiar with. The modified example is then fed
back into the model. While most other DA methods are applied during training
time, ours is applied during inference only, making it more efficient and
applicable. Our experiments show that our method improves performance on unseen
domains.",1,0,0,0,0,0,0.0576217,14.0,0.683302,27
335af2ca-3df3-4db1-8a60-f20f3a93ec4c,Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments,6,0.293693,0.167908,"We present a human-in-the-loop evaluation framework for fact-checking novel
misinformation claims and identifying social media messages that support them.
Our approach extracts check-worthy claims, which are aggregated and ranked for
review. Stance classifiers are then used to identify tweets supporting novel
misinformation claims, which are further reviewed to determine whether they
violate relevant policies. To demonstrate the feasibility of our approach, we
develop a baseline system based on modern NLP methods for human-in-the-loop
fact-checking in the domain of COVID-19 treatments. We make our data and
detailed annotation guidelines available to support the evaluation of
human-in-the-loop systems that identify novel misinformation directly from raw
user-generated content.",1,1,0,0,0,0,0.754175,8.0,0.841159,94
08fc8c9f-76a4-42ca-8a3b-cb319234b5bc,Keke AI Competition: Solving puzzle levels in a dynamically changing mechanic space,2,0.0729458,0.24957,"The Keke AI Competition introduces an artificial agent competition for the
game Baba is You - a Sokoban-like puzzle game where players can create rules
that influence the mechanics of the game. Altering a rule can cause temporary
or permanent effects for the rest of the level that could be part of the
solution space. The nature of these dynamic rules and the deterministic aspect
of the game creates a challenge for AI to adapt to a variety of mechanic
combinations in order to solve a level. This paper describes the framework and
evaluation metrics used to rank submitted agents and baseline results from
sample tree search agents.",0,1,0,0,0,0,0.136079,9.0,0.607588,22
3b7dcdbd-42bc-4bd8-a82a-f67f4b43cfd7,Elastic Monte Carlo Tree Search with State Abstraction for Strategy Game Playing,6,0.0401579,0.25627,"Strategy video games challenge AI agents with their combinatorial search
space caused by complex game elements. State abstraction is a popular technique
that reduces the state space complexity. However, current state abstraction
methods for games depend on domain knowledge, making their application to new
games expensive. State abstraction methods that require no domain knowledge are
studied extensively in the planning domain. However, no evidence shows they
scale well with the complexity of strategy games. In this paper, we propose
Elastic MCTS, an algorithm that uses state abstraction to play strategy games.
In Elastic MCTS, the nodes of the tree are clustered dynamically, first grouped
together progressively by state abstraction, and then separated when an
iteration threshold is reached. The elastic changes benefit from efficient
searching brought by state abstraction but avoid the negative influence of
using state abstraction for the whole search. To evaluate our method, we make
use of the general strategy games platform Stratega to generate scenarios of
varying complexity. Results show that Elastic MCTS outperforms MCTS baselines
with a large margin, while reducing the tree size by a factor of $10$. Code can
be found at: https://github.com/egg-west/Stratega",1,1,0,0,0,0,0.000199003,14.0,0.276319,27
5ef2d88c-0a15-49bc-b5fd-4c586714e51a,On the Role of Field of View for Occlusion Removal with Airborne Optical Sectioning,5,0.0526269,0.312833,"Occlusion caused by vegetation is an essential problem for remote sensing
applications in areas, such as search and rescue, wildfire detection, wildlife
observation, surveillance, border control, and others. Airborne Optical
Sectioning (AOS) is an optical, wavelength-independent synthetic aperture
imaging technique that supports computational occlusion removal in real-time.
It can be applied with manned or unmanned aircrafts, such as drones. In this
article, we demonstrate a relationship between forest density and field of view
(FOV) of applied imaging systems. This finding was made with the help of a
simulated procedural forest model which offers the consideration of more
realistic occlusion properties than our previous statistical model. While AOS
has been explored with automatic and autonomous research prototypes in the
past, we present a free AOS integration for DJI systems. It enables bluelight
organizations and others to use and explore AOS with compatible, manually
operated, off-the-shelf drones. The (digitally cropped) default FOV for this
implementation was chosen based on our new finding.",1,1,0,0,0,0,0.00650012,12.0,0.44649,38
14411b03-40c4-4865-bb6c-5d92628ae5bd,How Relevant is Selective Memory Population in Lifelong Language Learning?,5,0.0265316,0.225163,"Lifelong language learning seeks to have models continuously learn multiple
tasks in a sequential order without suffering from catastrophic forgetting.
State-of-the-art approaches rely on sparse experience replay as the primary
approach to prevent forgetting. Experience replay usually adopts sampling
methods for the memory population; however, the effect of the chosen sampling
strategy on model performance has not yet been studied. In this paper, we
investigate how relevant the selective memory population is in the lifelong
learning process of text classification and question-answering tasks. We found
that methods that randomly store a uniform number of samples from the entire
data stream lead to high performances, especially for low memory size, which is
consistent with computer vision studies.",1,0,0,0,0,0,0.415327,7.0,0.681179,35
02a2d64d-d1bc-4269-b689-403c5532b402,Motion Guided Deep Dynamic 3D Garments,14,0.424883,0.597752,"Realistic dynamic garments on animated characters have many AR/VR
applications. While authoring such dynamic garment geometry is still a
challenging task, data-driven simulation provides an attractive alternative,
especially if it can be controlled simply using the motion of the underlying
character. In this work, we focus on motion guided dynamic 3D garments,
especially for loose garments. In a data-driven setup, we first learn a
generative space of plausible garment geometries. Then, we learn a mapping to
this space to capture the motion dependent dynamic deformations, conditioned on
the previous state of the garment as well as its relative position with respect
to the underlying body. Technically, we model garment dynamics, driven using
the input character motion, by predicting per-frame local displacements in a
canonical state of the garment that is enriched with frame-dependent skinning
weights to bring the garment to the global space. We resolve any remaining
per-frame collisions by predicting residual local displacements. The resultant
garment geometry is used as history to enable iterative rollout prediction. We
demonstrate plausible generalization to unseen body shapes and motion inputs,
and show improvements over multiple state-of-the-art alternatives.",0,1,0,0,0,0,0.897972,7.0,0.887977,54
4f4069c2-724e-43bf-bce6-40b575240534,Recovering Sign Bits of DCT Coefficients in Digital Images as an Optimization Problem,5,0.0976615,0.413621,"Recovering unknown, missing, damaged, distorted, or lost information in DCT
coefficients is a common task in multiple applications of digital image
processing, including image compression, selective image encryption, and image
communication. This paper investigates the recovery of sign bits in DCT
coefficients of digital images, by proposing two different approximation
methods to solve a mixed integer linear programming (MILP) problem, which is
NP-hard in general. One method is a relaxation of the MILP problem to a linear
programming (LP) problem, and the other splits the original MILP problem into
some smaller MILP problems and an LP problem. We considered how the proposed
methods can be applied to JPEG-encoded images and conducted extensive
experiments to validate their performances. The experimental results showed
that the proposed methods outperformed other existing methods by a substantial
margin, both according to objective quality metrics and our subjective
evaluation.",1,1,0,0,1,0,9.83267e-06,21.0,0.374322,56
f5e77932-c9c2-4be9-bfbf-a848e023f1ce,Radial Basis Function Networks for Convolutional Neural Networks to Learn Similarity Distance Metric and Improve Interpretability,17,0.0839368,0.638604,"Radial basis function neural networks (RBFs) are prime candidates for pattern
classification and regression and have been used extensively in classical
machine learning applications. However, RBFs have not been integrated into
contemporary deep learning research and computer vision using conventional
convolutional neural networks (CNNs) due to their lack of adaptability with
modern architectures. In this paper, we adapt RBF networks as a classifier on
top of CNNs by modifying the training process and introducing a new activation
function to train modern vision architectures end-to-end for image
classification. The specific architecture of RBFs enables the learning of a
similarity distance metric to compare and find similar and dissimilar images.
Furthermore, we demonstrate that using an RBF classifier on top of any CNN
architecture provides new human-interpretable insights about the
decision-making process of the models. Finally, we successfully apply RBFs to a
range of CNN architectures and evaluate the results on benchmark computer
vision datasets.",0,0,0,0,0,0,0.304633,12.0,0.781507,48
87db8b62-1fba-47d8-b005-9bac6a875660,RDU: A Region-based Approach to Form-style Document Understanding,2,0.00967597,0.105833,"Key Information Extraction (KIE) is aimed at extracting structured
information (e.g. key-value pairs) from form-style documents (e.g. invoices),
which makes an important step towards intelligent document understanding.
Previous approaches generally tackle KIE by sequence tagging, which faces
difficulty to process non-flatten sequences, especially for table-text mixed
documents. These approaches also suffer from the trouble of pre-defining a
fixed set of labels for each type of documents, as well as the label imbalance
issue. In this work, we assume Optical Character Recognition (OCR) has been
applied to input documents, and reformulate the KIE task as a region prediction
problem in the two-dimensional (2D) space given a target field. Following this
new setup, we develop a new KIE model named Region-based Document Understanding
(RDU) that takes as input the text content and corresponding coordinates of a
document, and tries to predict the result by localizing a bounding-box-like
region. Our RDU first applies a layout-aware BERT equipped with a soft layout
attention masking and bias mechanism to incorporate layout information into the
representations. Then, a list of candidate regions is generated from the
representations via a Region Proposal Module inspired by computer vision models
widely applied for object detection. Finally, a Region Categorization Module
and a Region Selection Module are adopted to judge whether a proposed region is
valid and select the one with the largest probability from all proposed regions
respectively. Experiments on four types of form-style documents show that our
proposed method can achieve impressive results. In addition, our RDU model can
be trained with different document types seamlessly, which is especially
helpful over low-resource documents.",0,1,0,0,0,0,0.32431,6.0,0.575687,32
e5e23871-16e5-4d41-95f3-54c0137d6376,DetIE: Multilingual Open Information Extraction Inspired by Object Detection,12,0.0363475,0.45841,"State of the art neural methods for open information extraction (OpenIE)
usually extract triplets (or tuples) iteratively in an autoregressive or
predicate-based manner in order not to produce duplicates. In this work, we
propose a different approach to the problem that can be equally or more
successful. Namely, we present a novel single-pass method for OpenIE inspired
by object detection algorithms from computer vision. We use an order-agnostic
loss based on bipartite matching that forces unique predictions and a
Transformer-based encoder-only architecture for sequence labeling. The proposed
approach is faster and shows superior or similar performance in comparison with
state of the art models on standard benchmarks in terms of both quality metrics
and inference time. Our model sets the new state of the art performance of
67.7% F1 on CaRB evaluated as OIE2016 while being 3.35x faster at inference
than previous state of the art. We also evaluate the multilingual version of
our model in the zero-shot setting for two languages and introduce a strategy
for generating synthetic multilingual data to fine-tune the model for each
specific language. In this setting, we show performance improvement 15% on
multilingual Re-OIE2016, reaching 75% F1 for both Portuguese and Spanish
languages. Code and models are available at
https://github.com/sberbank-ai/DetIE.",1,1,0,0,1,0,0.0229138,11.0,0.511465,44
1f3943b1-a1f2-48ed-a8dd-66034000d1a9,Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field,5,0.272755,0.625551,"Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases
that correctly describe the categories of a given entity mention in a sentence.
Most recent works infer each entity type independently, ignoring the
correlations between types, e.g., when an entity is inferred as a president, it
should also be a politician and a leader. To this end, we use an undirected
graphical model called pairwise conditional random field (PCRF) to formulate
the UFET problem, in which the type variables are not only unarily influenced
by the input but also pairwisely relate to all the other type variables. We use
various modern backbones for entity typing to compute unary potentials, and
derive pairwise potentials from type phrase representations that both capture
prior semantic information and facilitate accelerated inference. We use
mean-field variational inference for efficient type inference on very large
type sets and unfold it as a neural network module to enable end-to-end
training. Experiments on UFET show that the Neural-PCRF consistently
outperforms its backbones with little cost and results in a competitive
performance against cross-encoder based SOTA while being thousands of times
faster. We also find Neural- PCRF effective on a widely used fine-grained
entity typing dataset with a smaller type set. We pack Neural-PCRF as a network
module that can be plugged onto multi-label type classifiers with ease and
release it in https://github.com/modelscope/adaseq/tree/master/examples/NPCRF.",0,1,0,0,1,0,0.804023,10.0,0.887899,47
08f4f576-36c5-44a3-96ee-ff51ea9a4f70,ST-MoE: Designing Stable and Transferable Sparse Expert Models,66,0.789255,0.835356,"Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3).",1,1,0,0,1,0,0.969272,5.0,0.927668,91
7a36e1b2-ab57-45e8-a1f6-844782404a53,ImageArg: A Multi-modal Tweet Dataset for Image Persuasiveness Mining,11,0.470496,0.844932,"The growing interest in developing corpora of persuasive texts has promoted
applications in automated systems, e.g., debating and essay scoring systems;
however, there is little prior work mining image persuasiveness from an
argumentative perspective. To expand persuasiveness mining into a multi-modal
realm, we present a multi-modal dataset, ImageArg, consisting of annotations of
image persuasiveness in tweets. The annotations are based on a persuasion
taxonomy we developed to explore image functionalities and the means of
persuasion. We benchmark image persuasiveness tasks on ImageArg using
widely-used multi-modal learning methods. The experimental results show that
our dataset offers a useful resource for this rich and challenging topic, and
there is ample room for modeling improvement.",1,0,1,1,0,0,0.860416,12.0,0.922344,49
34bd4ce3-8438-4d28-981a-68365cfcee9b,D2A-BSP: Distilled Data Association Belief Space Planning with Performance Guarantees Under Budget Constraints,8,0.236153,0.539801,"Unresolved data association in ambiguous and perceptually aliased
environments leads to multi-modal hypotheses on both the robot's and the
environment state. To avoid catastrophic results, when operating in such
ambiguous environments, it is crucial to reason about data association within
Belief Space Planning (BSP). However, explicitly considering all possible data
associations, the number of hypotheses grows exponentially with the planning
horizon and determining the optimal action sequence quickly becomes
intractable. Moreover, with hard budget constraints where some non-negligible
hypotheses must be pruned, achieving performance guarantees is crucial. In this
work we present a computationally efficient novel approach that utilizes only a
distilled subset of hypotheses to solve BSP problems while reasoning about data
association. Furthermore, to provide performance guarantees, we derive error
bounds with respect to the optimal solution. We then demonstrate our approach
in an extremely aliased environment, where we manage to significantly reduce
computation time without compromising on the quality of the solution.",0,1,0,0,0,0,0.0723045,12.0,0.650082,27
0df138d4-afb8-4d5f-a8df-298f61f1a873,Seeing a Rose in Five Thousand Ways,5,0.0444116,0.111091,"What is a rose, visually? A rose comprises its intrinsics, including the
distribution of geometry, texture, and material specific to its object
category. With knowledge of these intrinsic properties, we may render roses of
different sizes and shapes, in different poses, and under different lighting
conditions. In this work, we build a generative model that learns to capture
such object intrinsics from a single image, such as a photo of a bouquet. Such
an image includes multiple instances of an object type. These instances all
share the same intrinsics, but appear different due to a combination of
variance within these intrinsics and differences in extrinsic factors, such as
pose and illumination. Experiments show that our model successfully learns
object intrinsics (distribution of geometry, texture, and material) for a wide
range of objects, each from a single Internet image. Our method achieves
superior results on multiple downstream tasks, including intrinsic image
decomposition, shape and image generation, view synthesis, and relighting.",0,0,1,0,0,0,0.789004,5.0,0.766525,55
8f4f88db-370f-42b2-b9f7-6d824dc2e664,Hyperbolic Relevance Matching for Neural Keyphrase Extraction,13,0.341306,0.392573,"Keyphrase extraction is a fundamental task in natural language processing and
information retrieval that aims to extract a set of phrases with important
information from a source document. Identifying important keyphrase is the
central component of the keyphrase extraction task, and its main challenge is
how to represent information comprehensively and discriminate importance
accurately. In this paper, to address these issues, we design a new hyperbolic
matching model (HyperMatch) to represent phrases and documents in the same
hyperbolic space and explicitly estimate the phrase-document relevance via the
Poincar\'e distance as the important score of each phrase. Specifically, to
capture the hierarchical syntactic and semantic structure information,
HyperMatch takes advantage of the hidden representations in multiple layers of
RoBERTa and integrates them as the word embeddings via an adaptive mixing
layer. Meanwhile, considering the hierarchical structure hidden in the
document, HyperMatch embeds both phrases and documents in the same hyperbolic
space via a hyperbolic phrase encoder and a hyperbolic document encoder. This
strategy can further enhance the estimation of phrase-document relevance due to
the good properties of hyperbolic space. In this setting, the keyphrase
extraction can be taken as a matching problem and effectively implemented by
minimizing a hyperbolic margin-based triplet loss. Extensive experiments are
conducted on six benchmarks and demonstrate that HyperMatch outperforms the
state-of-the-art baselines.",0,0,0,0,1,0,0.777755,7.0,0.828382,56
b441fab9-6f70-4d13-9418-eabb3467cbeb,Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology,60,0.435572,0.694489,"Tissue phenotyping is a fundamental task in learning objective
characterizations of histopathologic biomarkers within the tumor-immune
microenvironment in cancer pathology. However, whole-slide imaging (WSI) is a
complex computer vision in which: 1) WSIs have enormous image resolutions with
precludes large-scale pixel-level efforts in data curation, and 2) diversity of
morphological phenotypes results in inter- and intra-observer variability in
tissue labeling. To address these limitations, current efforts have proposed
using pretrained image encoders (transfer learning from ImageNet,
self-supervised pretraining) in extracting morphological features from
pathology, but have not been extensively validated. In this work, we conduct a
search for good representations in pathology by training a variety of
self-supervised models with validation on a variety of weakly-supervised and
patch-level tasks. Our key finding is in discovering that Vision Transformers
using DINO-based knowledge distillation are able to learn data-efficient and
interpretable features in histology images wherein the different attention
heads learn distinct morphological phenotypes. We make evaluation code and
pretrained weights publicly-available at:
https://github.com/Richarizardd/Self-Supervised-ViT-Path.",1,1,0,0,0,0,0.74695,5.0,0.741682,46
4dba6289-693a-4c17-82f8-d26666075f90,Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning,1,0.0312165,0.0419788,"By driving models to converge to flat minima, sharpness-aware learning
algorithms (such as SAM) have shown the power to achieve state-of-the-art
performances. However, these algorithms will generally incur one extra
forward-backward propagation at each training iteration, which largely burdens
the computation especially for scalable models. To this end, we propose a
simple yet efficient training scheme, called Randomized Sharpness-Aware
Training (RST). Optimizers in RST would perform a Bernoulli trial at each
iteration to choose randomly from base algorithms (SGD) and sharpness-aware
algorithms (SAM) with a probability arranged by a predefined scheduling
function. Due to the mixture of base algorithms, the overall count of
propagation pairs could be largely reduced. Also, we give theoretical analysis
on the convergence of RST. Then, we empirically study the computation cost and
effect of various types of scheduling functions, and give directions on setting
appropriate scheduling functions. Further, we extend the RST to a general
framework (G-RST), where we can adjust regularization degree on sharpness
freely for any scheduling function. We show that G-RST can outperform SAM in
most cases while saving 50\% extra computation cost.",1,0,0,0,0,1,0.824727,4.0,0.736314,26
16a6b8db-d396-4de3-b169-a3ab023a164d,Predicting Customer Lifetime Value in Free-to-Play Games,13,0.179389,0.457857,"As game companies increasingly embrace a service-oriented business model, the
need for predictive models of players becomes more pressing. Multiple
activities, such as user acquisition, live game operations or game design need
to be supported with information about the choices made by the players and the
choices they could make in the future. This is especially true in the context
of free-to-play games, where the absence of a pay wall and the erratic nature
of the players' playing and spending behavior make predictions about the
revenue and allocation of budget and resources extremely challenging. In this
chapter we will present an overview of customer lifetime value modeling across
different fields, we will introduce the challenges specific to free-to-play
games across different platforms and genres and we will discuss the
state-of-the-art solutions with practical examples and references to existing
implementations.",0,1,0,0,0,0,0.0223446,28.0,0.807167,77
ff9d5096-3f50-4475-b1b3-fbc61bcc546a,Multiple Object Tracking from appearance by hierarchically clustering tracklets,8,0.0180479,0.435813,"Current approaches in Multiple Object Tracking (MOT) rely on the
spatio-temporal coherence between detections combined with object appearance to
match objects from consecutive frames. In this work, we explore MOT using
object appearances as the main source of association between objects in a
video, using spatial and temporal priors as weighting factors. We form initial
tracklets by leveraging on the idea that instances of an object that are close
in time should be similar in appearance, and build the final object tracks by
fusing the tracklets in a hierarchical fashion. We conduct extensive
experiments that show the effectiveness of our method over three different MOT
benchmarks, MOT17, MOT20, and DanceTrack, being competitive in MOT17 and MOT20
and establishing state-of-the-art results in DanceTrack.",1,1,0,0,1,0,0.131261,8.0,0.553691,55
7443a601-e567-4110-8bbe-4029e7a16157,Syntactic Substitutability as Unsupervised Dependency Syntax,1,0.00555897,0.0504406,"Syntax is a latent hierarchical structure which underpins the robust and
compositional nature of human language. In this work, we explore the hypothesis
that syntactic dependencies can be represented in language model attention
distributions and propose a new method to induce these structures
theory-agnostically. Instead of modeling syntactic relations as defined by
annotation schemata, we model a more general property implicit in the
definition of dependency relations, syntactic substitutability. This property
captures the fact that words at either end of a dependency can be substituted
with words from the same category. Substitutions can be used to generate a set
of syntactically invariant sentences whose representations are then used for
parsing. We show that increasing the number of substitutions used improves
parsing accuracy on natural data. On long-distance subject-verb agreement
constructions, our method achieves 79.5% recall compared to 8.9% using a
previous method. Our method also provides improvements when transferred to a
different parsing setup, demonstrating that it generalizes.",0,0,0,0,0,0,0.138838,7.0,0.49856,37
119fddce-afdf-44fe-a056-34f53bda68f7,Semantic Search for Large Scale Clinical Ontologies,1,0.0162113,0.0963721,"Finding concepts in large clinical ontologies can be challenging when queries
use different vocabularies. A search algorithm that overcomes this problem is
useful in applications such as concept normalisation and ontology matching,
where concepts can be referred to in different ways, using different synonyms.
In this paper, we present a deep learning based approach to build a semantic
search system for large clinical ontologies. We propose a Triplet-BERT model
and a method that generates training data directly from the ontologies. The
model is evaluated using five real benchmark data sets and the results show
that our approach achieves high results on both free text to concept and
concept to concept searching tasks, and outperforms all baseline methods.",0,1,0,0,0,0,0.790077,12.0,0.902992,27
8b863fa1-c62b-40ba-9c48-f664823c79b9,Wireless Ad Hoc Federated Learning: A Fully Distributed Cooperative Machine Learning,11,0.0704975,0.334296,"Privacy-sensitive data is stored in autonomous vehicles, smart devices, or
sensor nodes that can move around with making opportunistic contact with each
other. Federation among such nodes was mainly discussed in the context of
federated learning with a centralized mechanism in many works. However, because
of multi-vendor issues, those nodes do not want to rely on a specific server
operated by a third party for this purpose. In this paper, we propose a
wireless ad hoc federated learning (WAFL) -- a fully distributed cooperative
machine learning organized by the nodes physically nearby. WAFL can develop
generalized models from Non-IID datasets stored in distributed nodes locally by
exchanging and aggregating them with each other over opportunistic node-to-node
contacts. In our benchmark-based evaluation with various opportunistic
networks, WAFL has achieved higher accuracy of 94.8-96.3% than the
self-training case of 84.7%. All our evaluation results show that WAFL can
train and converge the model parameters from highly-partitioned Non-IID
datasets over opportunistic networks without any centralized mechanisms.",0,1,1,0,1,0,0.525269,9.0,0.788467,63
4399ce8c-c6fb-42ca-98a1-19c1979199cb,Incremental Online Learning Algorithms Comparison for Gesture and Visual Smart Sensors,3,0.0739634,0.352106,"Tiny machine learning (TinyML) in IoT systems exploits MCUs as edge devices
for data processing. However, traditional TinyML methods can only perform
inference, limited to static environments or classes. Real case scenarios
usually work in dynamic environments, thus drifting the context where the
original neural model is no more suitable. For this reason, pre-trained models
reduce accuracy and reliability during their lifetime because the data recorded
slowly becomes obsolete or new patterns appear. Continual learning strategies
maintain the model up to date, with runtime fine-tuning of the parameters. This
paper compares four state-of-the-art algorithms in two real applications: i)
gesture recognition based on accelerometer data and ii) image classification.
Our results confirm these systems' reliability and the feasibility of deploying
them in tiny-memory MCUs, with a drop in the accuracy of a few percentage
points with respect to the original models for unconstrained computing
platforms.",0,1,0,0,0,0,0.575787,5.0,0.64737,29
6030d998-995f-4872-a637-1212d6b6cb18,First Competitive Ant Colony Scheme for the CARP,31,0.173926,0.458344,"This paper addresses the Capacitated Arc Routing Problem (CARP) using an Ant
Colony Optimization scheme. Ant Colony schemes can compute solutions for medium
scale instances of VRP. The proposed Ant Colony is dedicated to large-scale
instances of CARP with more than 140 nodes and 190 arcs to service. The Ant
Colony scheme is coupled with a local search procedure and provides high
quality solutions. The benchmarks we carried out prove possible to obtain
solutions as profitable as CARPET ones can be obtained using such scheme when a
sufficient number of iterations is devoted to the ants. It competes with the
Genetic Algorithm of Lacomme et al. regarding solution quality but it is more
time consuming on large scale instances. The method has been intensively
benchmarked on the well-known instances of Eglese, DeArmon and the last ones of
Belenguer and Benavent. This research report is a step forward CARP resolution
by Ant Colony proving ant schemes can compete with Taboo search methods and
Genetic Algorithms",0,1,0,0,0,0,0.202463,10.0,0.690434,22
7042bee2-08a8-473b-9bcb-486c69e5c3f0,Excavating RoI Attention for Underwater Object Detection,15,0.674577,0.30269,"Self-attention is one of the most successful designs in deep learning, which
calculates the similarity of different tokens and reconstructs the feature
based on the attention matrix. Originally designed for NLP, self-attention is
also popular in computer vision, and can be categorized into pixel-level
attention and patch-level attention. In object detection, RoI features can be
seen as patches from base feature maps. This paper aims to apply the attention
module to RoI features to improve performance. Instead of employing an original
self-attention module, we choose the external attention module, a modified
self-attention with reduced parameters. With the proposed double head structure
and the Positional Encoding module, our method can achieve promising
performance in object detection. The comprehensive experiments show that it
achieves promising performance, especially in the underwater object detection
dataset. The code will be avaiable in:
https://github.com/zsyasd/Excavating-RoI-Attention-for-Underwater-Object-Detection",1,1,0,0,0,0,0.992191,7.0,0.995722,30
230eaaa7-7695-47d4-8393-24f863d49c1b,Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning,9,0.162985,0.106703,"Commonsense generation aims to generate a realistic sentence describing a
daily scene under the given concepts, which is very challenging, since it
requires models to have relational reasoning and compositional generalization
capabilities. Previous work focuses on retrieving prototype sentences for the
provided concepts to assist generation. They first use a sparse retriever to
retrieve candidate sentences, then re-rank the candidates with a ranker.
However, the candidates returned by their ranker may not be the most relevant
sentences, since the ranker treats all candidates equally without considering
their relevance to the reference sentences of the given concepts. Another
problem is that re-ranking is very expensive, but only using retrievers will
seriously degrade the performance of their generation models. To solve these
problems, we propose the metric distillation rule to distill knowledge from the
metric (e.g., BLEU) to the ranker. We further transfer the critical knowledge
summarized by the distilled ranker to the retriever. In this way, the relevance
scores of candidate sentences predicted by the ranker and retriever will be
more consistent with their quality measured by the metric. Experimental results
on the CommonGen benchmark verify the effectiveness of our proposed method: (1)
Our generation model with the distilled ranker achieves a new state-of-the-art
result. (2) Our generation model with the distilled retriever even surpasses
the previous SOTA.",1,1,0,0,1,0,0.862127,8.0,0.884297,50
0322ef16-3205-46ba-a6e2-9a57ed1ab012,Continual Contrastive Finetuning Improves Low-Resource Relation Extraction,1,0.00759644,0.0903025,"Relation extraction (RE), which has relied on structurally annotated corpora
for model training, has been particularly challenging in low-resource scenarios
and domains. Recent literature has tackled low-resource RE by self-supervised
learning, where the solution involves pretraining the entity pair embedding by
RE-based objective and finetuning on labeled data by classification-based
objective. However, a critical challenge to this approach is the gap in
objectives, which prevents the RE model from fully utilizing the knowledge in
pretrained representations. In this paper, we aim at bridging the gap and
propose to pretrain and finetune the RE model using consistent objectives of
contrastive learning. Since in this kind of representation learning paradigm,
one relation may easily form multiple clusters in the representation space, we
further propose a multi-center contrastive loss that allows one relation to
form multiple clusters to better align with pretraining. Experiments on two
document-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness
of our method. Particularly, when using 1% end-task training data, our method
outperforms PLM-based RE classifier by 10.5% and 6.1% on the two datasets,
respectively.",0,1,0,0,1,0,0.695982,7.0,0.795011,62
c8852bbd-b02e-446d-acf3-a49b08469e7d,Motion-aware Dynamic Graph Neural Network for Video Compressive Sensing,2,0.0278305,0.205459,"Video snapshot compressive imaging (SCI) utilizes a 2D detector to capture
sequential video frames and compresses them into a single measurement. Various
reconstruction methods have been developed to recover the high-speed video
frames from the snapshot measurement. However, most existing reconstruction
methods are incapable of capturing long-range spatial and temporal
dependencies, which are critical for video processing. In this paper, we
propose a flexible and robust approach based on graph neural network (GNN) to
efficiently model non-local interactions between pixels in space as well as
time regardless of the distance. Specifically, we develop a motion-aware
dynamic GNN for better video representation, i.e., represent each pixel as the
aggregation of relative nodes under the guidance of frame-by-frame motions,
which consists of motion-aware dynamic sampling, cross-scale node sampling and
graph aggregation. Extensive results on both simulation and real data
demonstrate both the effectiveness and efficiency of the proposed approach, and
the visualization clearly illustrates the intrinsic dynamic sampling operations
of our proposed model for boosting the video SCI reconstruction results. The
code and models will be released to the public.",0,1,0,0,0,0,0.623496,6.0,0.727853,63
959b8ec7-0270-4d73-9222-168927ad977a,On Pathologies in KL-Regularized Reinforcement Learning from Expert Demonstrations,18,0.325011,0.940226,"KL-regularized reinforcement learning from expert demonstrations has proved
successful in improving the sample efficiency of deep reinforcement learning
algorithms, allowing them to be applied to challenging physical real-world
tasks. However, we show that KL-regularized reinforcement learning with
behavioral reference policies derived from expert demonstrations can suffer
from pathological training dynamics that can lead to slow, unstable, and
suboptimal online learning. We show empirically that the pathology occurs for
commonly chosen behavioral policy classes and demonstrate its impact on sample
efficiency and online policy performance. Finally, we show that the pathology
can be remedied by non-parametric behavioral reference policies and that this
allows KL-regularized reinforcement learning to significantly outperform
state-of-the-art approaches on a variety of challenging locomotion and
dexterous hand manipulation tasks.",0,0,0,0,0,0,0.731583,11.0,0.878597,52
77c42547-ccbb-49ba-ad20-96089e21d882,Transformers are Adaptable Task Planners,15,0.0622736,0.628072,"Every home is different, and every person likes things done in their
particular way. Therefore, home robots of the future need to both reason about
the sequential nature of day-to-day tasks and generalize to user's preferences.
To this end, we propose a Transformer Task Planner(TTP) that learns high-level
actions from demonstrations by leveraging object attribute-based
representations. TTP can be pre-trained on multiple preferences and shows
generalization to unseen preferences using a single demonstration as a prompt
in a simulated dishwasher loading task. Further, we demonstrate real-world dish
rearrangement using TTP with a Franka Panda robotic arm, prompted using a
single human demonstration.",0,1,0,0,0,0,0.423324,5.0,0.558717,74
d26e5546-6d6c-4431-8567-c4a3c0293f0c,KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model for Financial Reports,24,0.159979,0.985052,"We present KPI-BERT, a system which employs novel methods of named entity
recognition (NER) and relation extraction (RE) to extract and link key
performance indicators (KPIs), e.g. ""revenue"" or ""interest expenses"", of
companies from real-world German financial documents. Specifically, we
introduce an end-to-end trainable architecture that is based on Bidirectional
Encoder Representations from Transformers (BERT) combining a recurrent neural
network (RNN) with conditional label masking to sequentially tag entities
before it classifies their relations. Our model also introduces a learnable
RNN-based pooling mechanism and incorporates domain expert knowledge by
explicitly filtering impossible relations. We achieve a substantially higher
prediction performance on a new practical dataset of German financial reports,
outperforming several strong baselines including a competing state-of-the-art
span-based entity tagging approach.",0,1,0,1,1,0,0.300368,6.0,0.560184,33
a1b99188-04bb-4940-bcff-6717cbc570bd,Reasoning with fuzzy and uncertain evidence using epistemic random fuzzy sets: general framework and practical models,16,0.271591,0.550851,"We introduce a general theory of epistemic random fuzzy sets for reasoning
with fuzzy or crisp evidence. This framework generalizes both the
Dempster-Shafer theory of belief functions, and possibility theory. Independent
epistemic random fuzzy sets are combined by the generalized
product-intersection rule, which extends both Dempster's rule for combining
belief functions, and the product conjunctive combination of possibility
distributions. We introduce Gaussian random fuzzy numbers and their
multi-dimensional extensions, Gaussian random fuzzy vectors, as practical
models for quantifying uncertainty about scalar or vector quantities.
Closed-form expressions for the combination, projection and vacuous extension
of Gaussian random fuzzy numbers and vectors are derived.",0,0,0,0,0,1,0.00359306,32.0,0.773863,38
30dc1bd1-c618-4d85-8293-bd38774fe867,Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation,7,0.124896,0.342997,"Image-to-text tasks, such as open-ended image captioning and controllable
image description, have received extensive attention for decades. Here, we
further advance this line of work by presenting Visual Spatial Description
(VSD), a new perspective for image-to-text toward spatial semantics. Given an
image and two objects inside it, VSD aims to produce one description focusing
on the spatial perspective between the two objects. Accordingly, we manually
annotate a dataset to facilitate the investigation of the newly-introduced task
and build several benchmark encoder-decoder models by using VL-BART and VL-T5
as backbones. In addition, we investigate pipeline and joint end-to-end
architectures for incorporating visual spatial relationship classification
(VSRC) information into our model. Finally, we conduct experiments on our
benchmark dataset to evaluate all our models. Results show that our models are
impressive, providing accurate and human-like spatial-oriented text
descriptions. Meanwhile, VSRC has great potential for VSD, and the joint
end-to-end architecture is the better choice for their integration. We make the
dataset and codes public for research purposes.",0,1,1,1,0,0,0.646906,7.0,0.775823,71
3ddcdb75-2477-4c3a-8a41-86c19b80a224,"Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",39,0.601459,0.536982,"While the problem of hallucinations in neural machine translation has long
been recognized, so far the progress on its alleviation is very little. Indeed,
recently it turned out that without artificially encouraging models to
hallucinate, previously existing methods fall short and even the standard
sequence log-probability is more informative. It means that characteristics
internal to the model can give much more information than we expect, and before
using external models and measures, we first need to ask: how far can we go if
we use nothing but the translation model itself ? We propose to use a method
that evaluates the percentage of the source contribution to a generated
translation. Intuitively, hallucinations are translations ""detached"" from the
source, hence they can be identified by low source contribution. This method
improves detection accuracy for the most severe hallucinations by a factor of 2
and is able to alleviate hallucinations at test time on par with the previous
best approach that relies on external models. Next, if we move away from
internal model characteristics and allow external tools, we show that using
sentence similarity from cross-lingual embeddings further improves these
results.",0,1,0,0,0,0,0.822983,5.0,0.787911,39
1742bf57-7625-4c77-9574-61b4bc0565c5,An unambiguous cloudiness index for nonwovens,1,0.00238879,0.0505527,"Cloudiness or formation is a concept routinely used in industry to address
deviations from homogeneity in nonwovens and papers. Measuring a cloudiness
index based on image data is a common task in industrial quality assurance. The
two most popular ways of quantifying cloudiness are based on power spectrum or
correlation function on the one hand or the Laplacian pyramid on the other
hand. Here, we recall the mathematical basis of the first approach
comprehensively, derive a cloudiness index, and demonstrate its practical
estimation. We prove that the Laplacian pyramid as well as other quantities
characterizing cloudiness like the range of interaction and the intensity of
small-angle scattering are very closely related to the power spectrum. Finally,
we show that the power spectrum is easy to be measured image analytically and
carries more information than the alternatives.",0,1,0,0,0,0,3.55271e-15,44.0,0.206944,77
93fbfcb8-30ac-4daa-acfc-13cbbb57f719,Towards Verifiable Federated Learning,10,0.0574343,0.279481,"Federated learning (FL) is an emerging paradigm of collaborative machine
learning that preserves user privacy while building powerful models.
Nevertheless, due to the nature of open participation by self-interested
entities, it needs to guard against potential misbehaviours by legitimate FL
participants. FL verification techniques are promising solutions for this
problem. They have been shown to effectively enhance the reliability of FL
networks and help build trust among participants. Verifiable federated learning
has become an emerging topic of research that has attracted significant
interest from the academia and the industry alike. Currently, there is no
comprehensive survey on the field of verifiable federated learning, which is
interdisciplinary in nature and can be challenging for researchers to enter
into. In this paper, we bridge this gap by reviewing works focusing on
verifiable FL. We propose a novel taxonomy for verifiable FL covering both
centralised and decentralised FL settings, summarise the commonly adopted
performance evaluation approaches, and discuss promising directions towards a
versatile verifiable FL framework.",0,0,0,0,0,0,0.457117,4.0,0.474419,59
6c1a428e-00ef-4dd3-93cb-44e56744b990,Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior,63,0.972578,0.999983,"Learned locomotion policies can rapidly adapt to diverse environments similar
to those experienced during training but lack a mechanism for fast tuning when
they fail in an out-of-distribution test environment. This necessitates a slow
and iterative cycle of reward and environment redesign to achieve good
performance on a new task. As an alternative, we propose learning a single
policy that encodes a structured family of locomotion strategies that solve
training tasks in different ways, resulting in Multiplicity of Behavior (MoB).
Different strategies generalize differently and can be chosen in real-time for
new tasks or environments, bypassing the need for time-consuming retraining. We
release a fast, robust open-source MoB locomotion controller, Walk These Ways,
that can execute diverse gaits with variable footswing, posture, and speed,
unlocking diverse downstream tasks: crouching, hopping, high-speed running,
stair traversal, bracing against shoves, rhythmic dance, and more. Video and
code release: https://gmargo11.github.io/walk-these-ways/",0,1,0,0,0,0,0.965068,4.0,0.900205,29
20569b54-b45f-4c79-ab32-ecbd505bbd37,Race Bias Analysis of Bona Fide Errors in face anti-spoofing,1,0.04555,0.0460576,"The study of bias in Machine Learning is receiving a lot of attention in
recent years, however, few only papers deal explicitly with the problem of race
bias in face anti-spoofing. In this paper, we present a systematic study of
race bias in face anti-spoofing with three key characteristics: the focus is on
analysing potential bias in the bona fide errors, where significant ethical and
legal issues lie; the analysis is not restricted to the final binary outcomes
of the classifier, but also covers the classifier's scalar responses and its
latent space; the threshold determining the operating point of the classifier
is considered a variable. We demonstrate the proposed bias analysis process on
a VQ-VAE based face anti-spoofing algorithm, trained on the Replay Attack and
the Spoof in the Wild (SiW) databases, and analysed for bias on the SiW and
Racial Faces in the Wild (RFW), databases. The results demonstrate that race
bias is not necessarily the result of different mean response values among the
various populations. Instead, it can be better understood as the combined
effect of several possible characteristics of the response distributions:
different means; different variances; bimodal behaviour; existence of outliers.",0,0,0,0,0,0,0.676187,7.0,0.787235,52
68940ea7-9bab-4b92-9a1f-de93e01b2cfe,Context-Consistent Semantic Image Editing with Style-Preserved Modulation,9,0.0619754,0.222757,"Semantic image editing utilizes local semantic label maps to generate the
desired content in the edited region. A recent work borrows SPADE block to
achieve semantic image editing. However, it cannot produce pleasing results due
to style discrepancy between the edited region and surrounding pixels. We
attribute this to the fact that SPADE only uses an image-independent local
semantic layout but ignores the image-specific styles included in the known
pixels. To address this issue, we propose a style-preserved modulation (SPM)
comprising two modulations processes: The first modulation incorporates the
contextual style and semantic layout, and then generates two fused modulation
parameters. The second modulation employs the fused parameters to modulate
feature maps. By using such two modulations, SPM can inject the given semantic
layout while preserving the image-specific context style. Moreover, we design a
progressive architecture for generating the edited content in a coarse-to-fine
manner. The proposed method can obtain context-consistent results and
significantly alleviate the unpleasant boundary between the generated regions
and the known pixels.",1,1,0,0,0,0,0.593736,9.0,0.80956,53
9196294b-f135-4004-9c96-3addb3255e96,SegTAD: Precise Temporal Action Detection via Semantic Segmentation,7,0.106124,0.252955,"Temporal action detection (TAD) is an important yet challenging task in video
analysis. Most existing works draw inspiration from image object detection and
tend to reformulate it as a proposal generation - classification problem.
However, there are two caveats with this paradigm. First, proposals are not
equipped with annotated labels, which have to be empirically compiled, thus the
information in the annotations is not necessarily precisely employed in the
model training process. Second, there are large variations in the temporal
scale of actions, and neglecting this fact may lead to deficient representation
in the video features. To address these issues and precisely model temporal
action detection, we formulate the task of temporal action detection in a novel
perspective of semantic segmentation. Owing to the 1-dimensional property of
TAD, we are able to convert the coarse-grained detection annotations to
fine-grained semantic segmentation annotations for free. We take advantage of
them to provide precise supervision so as to mitigate the impact induced by the
imprecise proposal labels. We propose an end-to-end framework SegTAD composed
of a 1D semantic segmentation network (1D-SSN) and a proposal detection network
(PDN).",0,0,1,0,0,0,0.611196,10.0,0.833365,50
a08574af-9f09-4350-88c3-f5f2857cc46c,NeuPhysics: Editable Neural Geometry and Physics from Monocular Videos,22,0.244365,0.790482,"We present a method for learning 3D geometry and physics parameters of a
dynamic scene from only a monocular RGB video input. To decouple the learning
of underlying scene geometry from dynamic motion, we represent the scene as a
time-invariant signed distance function (SDF) which serves as a reference
frame, along with a time-conditioned deformation field. We further bridge this
neural geometry representation with a differentiable physics simulator by
designing a two-way conversion between the neural field and its corresponding
hexahedral mesh, enabling us to estimate physics parameters from the source
video by minimizing a cycle consistency loss. Our method also allows a user to
interactively edit 3D objects from the source video by modifying the recovered
hexahedral mesh, and propagating the operation back to the neural field
representation. Experiments show that our method achieves superior mesh and
video reconstruction of dynamic scenes compared to competing Neural Field
approaches, and we provide extensive examples which demonstrate its ability to
extract useful 3D representations from videos captured with consumer-grade
cameras.",1,0,0,0,0,0,0.836132,4.0,0.745792,75
56ec508c-8aec-4ea0-82f3-bd7033a85d8e,Data-Driven Deep Supervision for Skin Lesion Classification,5,0.103004,0.454304,"Automatic classification of pigmented, non-pigmented, and depigmented
non-melanocytic skin lesions have garnered lots of attention in recent years.
However, imaging variations in skin texture, lesion shape, depigmentation
contrast, lighting condition, etc. hinder robust feature extraction, affecting
classification accuracy. In this paper, we propose a new deep neural network
that exploits input data for robust feature extraction. Specifically, we
analyze the convolutional network's behavior (field-of-view) to find the
location of deep supervision for improved feature extraction. To achieve this,
first, we perform activation mapping to generate an object mask, highlighting
the input regions most critical for classification output generation. Then the
network layer whose layer-wise effective receptive field matches the
approximated object shape in the object mask is selected as our focus for deep
supervision. Utilizing different types of convolutional feature extractors and
classifiers on three melanoma detection datasets and two vitiligo detection
datasets, we verify the effectiveness of our new method.",0,0,0,0,0,0,0.141615,12.0,0.709275,32
ef12cb19-3f48-4679-bbe5-acd55e04f6a3,What Do Compressed Multilingual Machine Translation Models Forget?,8,0.0314225,0.735116,"Recently, very large pre-trained models achieve state-of-the-art results in
various natural language processing (NLP) tasks, but their size makes it more
challenging to apply them in resource-constrained environments. Compression
techniques allow to drastically reduce the size of the models and therefore
their inference time with negligible impact on top-tier metrics. However, the
general performance averaged across multiple tasks and/or languages may hide a
drastic performance drop on under-represented features, which could result in
the amplification of biases encoded by the models. In this work, we assess the
impact of compression methods on Multilingual Neural Machine Translation models
(MNMT) for various language groups, gender, and semantic biases by extensive
analysis of compressed models on different machine translation benchmarks, i.e.
FLORES-101, MT-Gender, and DiBiMT. We show that the performance of
under-represented languages drops significantly, while the average BLEU metric
only slightly decreases. Interestingly, the removal of noisy memorization with
compression leads to a significant improvement for some medium-resource
languages. Finally, we demonstrate that compression amplifies intrinsic gender
and semantic biases, even in high-resource languages. Code:
https://github.com/alirezamshi/bias-compressedMT",1,1,0,0,0,0,0.528358,4.0,0.526231,62
3ae6b2ef-9baf-4770-a02f-21fba7af3d84,"Understanding, Detecting, and Separating Out-of-Distribution Samples and Adversarial Samples in Text Classification",1,0.00746349,0.0741838,"In this paper, we study the differences and commonalities between
statistically out-of-distribution (OOD) samples and adversarial (Adv) samples,
both of which hurting a text classification model's performance. We conduct
analyses to compare the two types of anomalies (OOD and Adv samples) with the
in-distribution (ID) ones from three aspects: the input features, the hidden
representations in each layer of the model, and the output probability
distributions of the classifier. We find that OOD samples expose their
aberration starting from the first layer, while the abnormalities of Adv
samples do not emerge until the deeper layers of the model. We also illustrate
that the models' output probabilities for Adv samples tend to be more
unconfident. Based on our observations, we propose a simple method to separate
ID, OOD, and Adv samples using the hidden representations and output
probabilities of the model. On multiple combinations of ID, OOD datasets, and
Adv attacks, our proposed method shows exceptional results on distinguishing
ID, OOD, and Adv samples.",0,1,0,0,0,0,0.711361,6.0,0.767959,38
9421cb9d-0fb4-4d4a-8334-a885001a25ce,Classifying Unstructured Clinical Notes via Automatic Weak Supervision,6,0.035845,0.372204,"Healthcare providers usually record detailed notes of the clinical care
delivered to each patient for clinical, research, and billing purposes. Due to
the unstructured nature of these narratives, providers employ dedicated staff
to assign diagnostic codes to patients' diagnoses using the International
Classification of Diseases (ICD) coding system. This manual process is not only
time-consuming but also costly and error-prone. Prior work demonstrated
potential utility of Machine Learning (ML) methodology in automating this
process, but it has relied on large quantities of manually labeled data to
train the models. Additionally, diagnostic coding systems evolve with time,
which makes traditional supervised learning strategies unable to generalize
beyond local applications. In this work, we introduce a general
weakly-supervised text classification framework that learns from class-label
descriptions only, without the need to use any human-labeled documents. It
leverages the linguistic domain knowledge stored within pre-trained language
models and the data programming framework to assign code labels to individual
texts. We demonstrate the efficacy and flexibility of our method by comparing
it to state-of-the-art weak text classifiers across four real-world text
classification datasets, in addition to assigning ICD codes to medical notes in
the publicly available MIMIC-III database.",0,1,0,0,0,0,0.283222,10.0,0.729092,41
73a86ae2-e866-4b9d-9690-e939061d5cf5,Simple and Effective Synthesis of Indoor 3D Scenes,19,0.204042,0.549641,"We study the problem of synthesizing immersive 3D indoor scenes from one or
more images. Our aim is to generate high-resolution images and videos from
novel viewpoints, including viewpoints that extrapolate far beyond the input
images while maintaining 3D consistency. Existing approaches are highly
complex, with many separately trained stages and components. We propose a
simple alternative: an image-to-image GAN that maps directly from reprojections
of incomplete point clouds to full high-resolution RGB-D images. On the
Matterport3D and RealEstate10K datasets, our approach significantly outperforms
prior work when evaluated by humans, as well as on FID scores. Further, we show
that our model is useful for generative data augmentation. A
vision-and-language navigation (VLN) agent trained with trajectories
spatially-perturbed by our model improves success rate by up to 1.5% over a
state of the art baseline on the R2R benchmark. Our code will be made available
to facilitate generative data augmentation and applications to downstream
robotics and embodied AI tasks.",1,1,0,0,1,0,0.784898,7.0,0.831452,83
03575f7b-4e7f-460b-b87f-eda04b0944e1,Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances,7,0.161951,0.2546,"Dialogue State Tracking (DST) is primarily evaluated using Joint Goal
Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue
state exactly matches the prediction. Generally in DST, the dialogue state or
belief state for a given turn contains all the intents shown by the user till
that turn. Due to this cumulative nature of the belief state, it is difficult
to get a correct prediction once a misprediction has occurred. Thus, although
being a useful metric, it can be harsh at times and underestimate the true
potential of a DST model. Moreover, an improvement in JGA can sometimes
decrease the performance of turn-level or non-cumulative belief state
prediction due to inconsistency in annotations. So, using JGA as the only
metric for model selection may not be ideal for all scenarios. In this work, we
discuss various evaluation metrics used for DST along with their shortcomings.
To address the existing issues, we propose a new evaluation metric named
Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike
JGA, it tries to give penalized rewards to mispredictions that are locally
correct i.e. the root cause of the error is an earlier turn. By doing so, FGA
considers the performance of both cumulative and turn-level prediction flexibly
and provides a better insight than the existing metrics. We also show that FGA
is a better discriminator of DST model performance.",1,0,0,0,0,0,0.929676,6.0,0.894478,9
9ac0ceee-ef36-4fba-9ec0-dea4a13c102d,Object Detection in Aerial Images: What Improves the Accuracy?,1,0.0169473,0.0215421,"Object detection is a challenging and popular computer vision problem. The
problem is even more challenging in aerial images due to significant variation
in scale and viewpoint in a diverse set of object categories. Recently, deep
learning-based object detection approaches have been actively explored for the
problem of object detection in aerial images. In this work, we investigate the
impact of Faster R-CNN for aerial object detection and explore numerous
strategies to improve its performance for aerial images. We conduct extensive
experiments on the challenging iSAID dataset. The resulting adapted Faster
R-CNN obtains a significant mAP gain of 4.96% over its vanilla baseline
counterpart on the iSAID validation set, demonstrating the impact of different
strategies investigated in this work.",0,1,0,0,1,0,0.86608,10.0,0.908895,37
38646deb-0418-4581-b461-54aef8592560,Smoothing Entailment Graphs with Language Models,8,0.0707245,0.728046,"The diversity and Zipfian frequency distribution of natural language
predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by
Open Relation Extraction (ORE). EGs are computationally efficient and
explainable models of natural language inference, but as symbolic models, they
fail if a novel premise or hypothesis vertex is missing at test-time. We
present theory and methodology for overcoming such sparsity in symbolic models.
First, we introduce a theory of optimal smoothing of EGs by constructing
transitive chains. We then demonstrate an efficient, open-domain, and
unsupervised smoothing method using an off-the-shelf Language Model to find
approximations of missing premise predicates. This improves recall by 25.1 and
16.3 percentage points on two difficult directional entailment datasets, while
raising average precision and maintaining model explainability. Further, in a
QA task we show that EG smoothing is most useful for answering questions with
lesser supporting text, where missing premise predicates are more costly.
Finally, controlled experiments with WordNet confirm our theory and show that
hypothesis smoothing is difficult, but possible in principle.",0,0,0,0,0,0,0.0847848,10.0,0.596689,45
f5f4e0ac-8273-4033-9898-3321a4a1e453,"Data Smells: Categories, Causes and Consequences, and Detection of Suspicious Data in AI-based Systems",15,0.0648776,0.544129,"High data quality is fundamental for today's AI-based systems. However,
although data quality has been an object of research for decades, there is a
clear lack of research on potential data quality issues (e.g., ambiguous,
extraneous values). These kinds of issues are latent in nature and thus often
not obvious. Nevertheless, they can be associated with an increased risk of
future problems in AI-based systems (e.g., technical debt, data-induced
faults). As a counterpart to code smells in software engineering, we refer to
such issues as Data Smells. This article conceptualizes data smells and
elaborates on their causes, consequences, detection, and use in the context of
AI-based systems. In addition, a catalogue of 36 data smells divided into three
categories (i.e., Believability Smells, Understandability Smells, Consistency
Smells) is presented. Moreover, the article outlines tool support for detecting
data smells and presents the result of an initial smell detection on more than
240 real-world datasets.",0,0,1,0,0,0,0.0553428,6.0,0.254114,59
5be2dd45-bb83-42ab-ba0e-4089ddf38370,Semi-Supervised Disentanglement of Tactile Contact~Geometry from Sliding-Induced Shear,1,0.0379295,0.14502,"The sense of touch is fundamental to human dexterity. When mimicked in
robotic touch, particularly by use of soft optical tactile sensors, it suffers
from distortion due to motion-dependent shear. This complicates tactile tasks
like shape reconstruction and exploration that require information about
contact geometry. In this work, we pursue a semi-supervised approach to remove
shear while preserving contact-only information. We validate our approach by
showing a match between the model-generated unsheared images with their
counterparts from vertically tapping onto the object. The model-generated
unsheared images give faithful reconstruction of contact-geometry otherwise
masked by shear, along with robust estimation of object pose then used for
sliding exploration and full reconstruction of several planar shapes. We show
that our semi-supervised approach achieves comparable performance to its fully
supervised counterpart across all validation tasks with an order of magnitude
less supervision. The semi-supervised method is thus more computational and
labeled sample-efficient. We expect it will have broad applicability to wide
range of complex tactile exploration and manipulation tasks performed via a
shear-sensitive sense of touch.",0,1,0,0,0,0,0.572584,8.0,0.778505,27
3c16d0ce-c16b-4435-8c96-72e8adfb14f7,Generative Pre-Trained Transformers for Biologically Inspired Design,2,0.0327952,0.103733,"Biological systems in nature have evolved for millions of years to adapt and
survive the environment. Many features they developed can be inspirational and
beneficial for solving technical problems in modern industries. This leads to a
novel form of design-by-analogy called bio-inspired design (BID). Although BID
as a design method has been proven beneficial, the gap between biology and
engineering continuously hinders designers from effectively applying the
method. Therefore, we explore the recent advance of artificial intelligence
(AI) for a computational approach to bridge the gap. This paper proposes a
generative design approach based on the pre-trained language model (PLM) to
automatically retrieve and map biological analogy and generate BID in the form
of natural language. The latest generative pre-trained transformer, namely
GPT-3, is used as the base PLM. Three types of design concept generators are
identified and fine-tuned from the PLM according to the looseness of the
problem space representation. Machine evaluators are also fine-tuned to assess
the correlation between the domains within the generated BID concepts. The
approach is then tested via a case study in which the fine-tuned models are
applied to generate and evaluate light-weighted flying car concepts inspired by
nature. The results show our approach can generate BID concepts with good
performance.",0,1,0,0,0,0,0.0961752,14.0,0.721365,49
6c0a1eb5-4a8a-4f5f-a670-c9ce564cfbb6,Revisiting Self-Supervised Contrastive Learning for Facial Expression Recognition,13,0.141319,0.645136,"The success of most advanced facial expression recognition works relies
heavily on large-scale annotated datasets. However, it poses great challenges
in acquiring clean and consistent annotations for facial expression datasets.
On the other hand, self-supervised contrastive learning has gained great
popularity due to its simple yet effective instance discrimination training
strategy, which can potentially circumvent the annotation issue. Nevertheless,
there remain inherent disadvantages of instance-level discrimination, which are
even more challenging when faced with complicated facial representations. In
this paper, we revisit the use of self-supervised contrastive learning and
explore three core strategies to enforce expression-specific representations
and to minimize the interference from other facial attributes, such as identity
and face styling. Experimental results show that our proposed method
outperforms the current state-of-the-art self-supervised learning methods, in
terms of both categorical and dimensional facial expression recognition tasks.",0,1,0,0,0,0,0.711444,6.0,0.767997,47
38f61f36-a807-4d36-921a-fe5ce9a16810,AltUB: Alternating Training Method to Update Base Distribution of Normalizing Flow for Anomaly Detection,5,0.105151,0.277314,"Unsupervised anomaly detection is coming into the spotlight these days in
various practical domains due to the limited amount of anomaly data. One of the
major approaches for it is a normalizing flow which pursues the invertible
transformation of a complex distribution as images into an easy distribution as
N(0, I). In fact, algorithms based on normalizing flow like FastFlow and
CFLOW-AD establish state-of-the-art performance on unsupervised anomaly
detection tasks. Nevertheless, we investigate these algorithms convert normal
images into not N(0, I) as their destination, but an arbitrary normal
distribution. Moreover, their performances are often unstable, which is highly
critical for unsupervised tasks because data for validation are not provided.
To break through these observations, we propose a simple solution AltUB which
introduces alternating training to update the base distribution of normalizing
flow for anomaly detection. AltUB effectively improves the stability of
performance of normalizing flow. Furthermore, our method achieves the new
state-of-the-art performance of the anomaly segmentation task on the MVTec AD
dataset with 98.8% AUROC.",0,1,0,0,1,0,0.944084,4.0,0.862427,26
222ac523-6497-46fa-9661-8f1674dafe80,An Online Approach to Solve the Dynamic Vehicle Routing Problem with Stochastic Trip Requests for Paratransit Services,6,0.117835,0.601083,"Many transit agencies operating paratransit and microtransit services have to
respond to trip requests that arrive in real-time, which entails solving hard
combinatorial and sequential decision-making problems under uncertainty. To
avoid decisions that lead to significant inefficiency in the long term,
vehicles should be allocated to requests by optimizing a non-myopic utility
function or by batching requests together and optimizing a myopic utility
function. While the former approach is typically offline, the latter can be
performed online. We point out two major issues with such approaches when
applied to paratransit services in practice. First, it is difficult to batch
paratransit requests together as they are temporally sparse. Second, the
environment in which transit agencies operate changes dynamically (e.g.,
traffic conditions), causing estimates that are learned offline to become
stale. To address these challenges, we propose a fully online approach to solve
the dynamic vehicle routing problem (DVRP) with time windows and stochastic
trip requests that is robust to changing environmental dynamics by
construction. We focus on scenarios where requests are relatively sparse - our
problem is motivated by applications to paratransit services. We formulate DVRP
as a Markov decision process and use Monte Carlo tree search to evaluate
actions for any given state. Accounting for stochastic requests while
optimizing a non-myopic utility function is computationally challenging;
indeed, the action space for such a problem is intractably large in practice.
To tackle the large action space, we leverage the structure of the problem to
design heuristics that can sample promising actions for the tree search. Our
experiments using real-world data from our partner agency show that the
proposed approach outperforms existing state-of-the-art approaches both in
terms of performance and robustness.",1,1,0,0,1,0,0.318859,14.0,0.816672,37
27d1eb26-55dd-433e-9ff1-db3a1fe3b6f8,LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification,13,0.091804,0.60404,"Weakly supervised text classification methods typically train a deep neural
classifier based on pseudo-labels. The quality of pseudo-labels is crucial to
final performance but they are inevitably noisy due to their heuristic nature,
so selecting the correct ones has a huge potential for performance boost. One
straightforward solution is to select samples based on the softmax probability
scores in the neural classifier corresponding to their pseudo-labels. However,
we show through our experiments that such solutions are ineffective and
unstable due to the erroneously high-confidence predictions from poorly
calibrated models. Recent studies on the memorization effects of deep neural
models suggest that these models first memorize training samples with clean
labels and then those with noisy labels. Inspired by this observation, we
propose a novel pseudo-label selection method LOPS that takes learning order of
samples into consideration. We hypothesize that the learning order reflects the
probability of wrong annotation in terms of ranking, and therefore, propose to
select the samples that are learnt earlier. LOPS can be viewed as a strong
performance-boost plug-in to most of existing weakly-supervised text
classification methods, as confirmed in extensive experiments on four
real-world datasets.",1,1,0,0,0,0,0.399515,8.0,0.714656,46
2fb31164-38c1-4dda-9548-37fbfc77a855,Fast Object Placement Assessment,7,0.0876561,0.815331,"Object placement assessment (OPA) aims to predict the rationality score of a
composite image in terms of the placement (e.g., scale, location) of inserted
foreground object. However, given a pair of scaled foreground and background,
to enumerate all the reasonable locations, existing OPA model needs to place
the foreground at each location on the background and pass the obtained
composite image through the model one at a time, which is very time-consuming.
In this work, we investigate a new task named as fast OPA. Specifically,
provided with a scaled foreground and a background, we only pass them through
the model once and predict the rationality scores for all locations. To
accomplish this task, we propose a pioneering fast OPA model with several
innovations (i.e., foreground dynamic filter, background prior transfer, and
composite feature mimicking) to bridge the performance gap between slow OPA
model and fast OPA model. Extensive experiments on OPA dataset show that our
proposed fast OPA model performs on par with slow OPA model but runs
significantly faster.",0,0,0,0,0,0,0.584963,7.0,0.751719,51
719f7434-5a38-4421-a475-0827e95c7944,MultiCoNER: A Large-scale Multilingual dataset for Complex Named Entity Recognition,62,0.320239,0.999973,"We present MultiCoNER, a large multilingual dataset for Named Entity
Recognition that covers 3 domains (Wiki sentences, questions, and search
queries) across 11 languages, as well as multilingual and code-mixing subsets.
This dataset is designed to represent contemporary challenges in NER, including
low-context scenarios (short and uncased text), syntactically complex entities
like movie titles, and long-tail entity distributions. The 26M token dataset is
compiled from public resources using techniques such as heuristic-based
sentence sampling, template extraction and slotting, and machine translation.
We applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a
state-of-the-art GEMNET model that leverages gazetteers. The baseline achieves
moderate performance (macro-F1=54%), highlighting the difficulty of our data.
GEMNET, which uses gazetteers, improvement significantly (average improvement
of macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained
language models, and we believe that it can help further research in building
robust NER systems. MultiCoNER is publicly available at
https://registry.opendata.aws/multiconer/ and we hope that this resource will
help advance research in various aspects of NER.",1,1,0,1,1,0,0.0308656,11.0,0.538917,17
142bd7e5-0df5-40b0-80eb-230722fb8809,Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings,40,0.713883,0.589872,"Automatic depression detection on Twitter can help individuals privately and
conveniently understand their mental health status in the early stages before
seeing mental health professionals. Most existing black-box-like deep learning
methods for depression detection largely focused on improving classification
performance. However, explaining model decisions is imperative in health
research because decision-making can often be high-stakes and life-and-death.
Reliable automatic diagnosis of mental health problems including depression
should be supported by credible explanations justifying models' predictions. In
this work, we propose a novel explainable model for depression detection on
Twitter. It comprises a novel encoder combining hierarchical attention
mechanisms and feed-forward neural networks. To support psycholinguistic
studies, our model leverages metaphorical concept mappings as input. Thus, it
not only detects depressed individuals, but also identifies features of such
users' tweets and associated metaphor concept mappings.",0,1,0,0,1,0,0.687993,8.0,0.817881,51
86db4964-cdb3-4734-8c5e-72e64d0a638e,Local Sliced-Wasserstein Feature Sets for Illumination-invariant Face Recognition,6,0.121396,0.707265,"We present a new method for face recognition from digital images acquired
under varying illumination conditions. The method is based on mathematical
modeling of local gradient distributions using the Radon Cumulative
Distribution Transform (R-CDT). We demonstrate that lighting variations cause
certain types of deformations of local image gradient distributions which, when
expressed in R-CDT domain, can be modeled as a subspace. Face recognition is
then performed using a nearest subspace in R-CDT domain of local gradient
distributions. Experiment results demonstrate the proposed method outperforms
other alternatives in several face recognition tasks with challenging
illumination conditions. Python code implementing the proposed method is
available, which is integrated as a part of the software package PyTransKit.",0,1,0,0,0,0,0.333491,18.0,0.860468,40
f768b7e2-cb5f-4f0d-96b6-b91c100a66fe,Deep Structural Causal Shape Models,8,0.166542,0.308789,"Causal reasoning provides a language to ask important interventional and
counterfactual questions beyond purely statistical association. In medical
imaging, for example, we may want to study the causal effect of genetic,
environmental, or lifestyle factors on the normal and pathological variation of
anatomical phenotypes. However, while anatomical shape models of 3D surface
meshes, extracted from automated image segmentation, can be reliably
constructed, there is a lack of computational tooling to enable causal
reasoning about morphological variations. To tackle this problem, we propose
deep structural causal shape models (CSMs), which utilise high-quality mesh
generation techniques, from geometric deep learning, within the expressive
framework of deep structural causal models. CSMs enable subject-specific
prognoses through counterfactual mesh generation (""How would this patient's
brain structure change if they were ten years older?""), which is in contrast to
most current works on purely population-level statistical shape modelling. We
demonstrate the capabilities of CSMs at all levels of Pearl's causal hierarchy
through a number of qualitative and quantitative experiments leveraging a large
dataset of 3D brain structures.",0,0,0,0,0,0,0.794498,9.0,0.87216,81
aaf0df49-fcf7-44da-b6dc-ce7f09f60d2c,"SeasoNet: A Seasonal Scene Classification, segmentation and Retrieval dataset for satellite Imagery over Germany",5,0.0232456,0.417159,"This work presents SeasoNet, a new large-scale multi-label land cover and
land use scene understanding dataset. It includes $1\,759\,830$ images from
Sentinel-2 tiles, with 12 spectral bands and patch sizes of up to $ 120 \
\mathrm{px} \times 120 \ \mathrm{px}$. Each image is annotated with large scale
pixel level labels from the German land cover model LBM-DE2018 with land cover
classes based on the CORINE Land Cover database (CLC) 2018 and a five times
smaller minimum mapping unit (MMU) than the original CLC maps. We provide pixel
synchronous examples from all four seasons, plus an additional snowy set. These
properties make SeasoNet the currently most versatile and biggest remote
sensing scene understanding dataset with possible applications ranging from
scene classification over land cover mapping to content-based cross season
image retrieval and self-supervised feature learning. We provide baseline
results by evaluating state-of-the-art deep networks on the new dataset in
scene classification and semantic segmentation scenarios.",0,1,1,1,0,0,0.38867,6.0,0.613587,14
36ae12af-e3ba-4b96-9f20-8bead6179b73,Investigating data partitioning strategies for crosslinguistic low-resource ASR evaluation,5,0.011732,0.473378,"Many automatic speech recognition (ASR) data sets include a single
pre-defined test set consisting of one or more speakers whose speech never
appears in the training set. This ""hold-speaker(s)-out"" data partitioning
strategy, however, may not be ideal for data sets in which the number of
speakers is very small. This study investigates ten different data split
methods for five languages with minimal ASR training resources. We find that
(1) model performance varies greatly depending on which speaker is selected for
testing; (2) the average word error rate (WER) across all held-out speakers is
comparable not only to the average WER over multiple random splits but also to
any given individual random split; (3) WER is also generally comparable when
the data is split heuristically or adversarially; (4) utterance duration and
intensity are comparatively more predictive factors of variability regardless
of the data split. These results suggest that the widely used hold-speakers-out
approach to ASR data partitioning can yield results that do not reflect model
performance on unseen data or speakers. Random splits can yield more reliable
and generalizable estimates when facing data sparsity.",0,1,0,0,0,0,0.00767229,11.0,0.411297,32
923c42f5-bb19-4951-9448-599b6febba43,DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models,31,0.306151,0.997986,"Traditionally, monocular 3D human pose estimation employs a machine learning
model to predict the most likely 3D pose for a given input image. However, a
single image can be highly ambiguous and induces multiple plausible solutions
for the 2D-3D lifting step which results in overly confident 3D pose
predictors. To this end, we propose \emph{DiffPose}, a conditional diffusion
model, that predicts multiple hypotheses for a given input image. In comparison
to similar approaches, our diffusion model is straightforward and avoids
intensive hyperparameter tuning, complex network structures, mode collapse, and
unstable training. Moreover, we tackle a problem of the common two-step
approach that first estimates a distribution of 2D joint locations via
joint-wise heatmaps and consecutively approximates them based on first- or
second-moment statistics. Since such a simplification of the heatmaps removes
valid information about possibly correct, though labeled unlikely, joint
locations, we propose to represent the heatmaps as a set of 2D joint candidate
samples. To extract information about the original distribution from these
samples we introduce our \emph{embedding transformer} that conditions the
diffusion model. Experimentally, we show that DiffPose slightly improves upon
the state of the art for multi-hypothesis pose estimation for simple poses and
outperforms it by a large margin for highly ambiguous poses.",0,0,0,0,1,0,0.720669,7.0,0.804827,56
a7243b5d-e31e-40b4-bb85-ff30db019190,CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI,15,0.414535,0.5966,"Human language expression is based on the subjective construal of the
situation instead of the objective truth conditions, which means that speakers'
personalities and emotions after cognitive processing have an important
influence on conversation. However, most existing datasets for conversational
AI ignore human personalities and emotions, or only consider part of them. It's
difficult for dialogue systems to understand speakers' personalities and
emotions although large-scale pre-training language models have been widely
used. In order to consider both personalities and emotions in the process of
conversation generation, we propose CPED, a large-scale Chinese personalized
and emotional dialogue dataset, which consists of multi-source knowledge
related to empathy and personal characteristic. These knowledge covers gender,
Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPED
contains more than 12K dialogues of 392 speakers from 40 TV shows. We release
the textual dataset with audio features and video features according to the
copyright claims, privacy issues, terms of service of video platforms. We
provide detailed description of the CPED construction process and introduce
three tasks for conversational AI, including personality recognition, emotion
recognition in conversations as well as personalized and emotional conversation
generation. Finally, we provide baseline systems for these tasks and consider
the function of speakers' personalities and emotions on conversation. Our
motivation is to propose a dataset to be widely adopted by the NLP community as
a new open benchmark for conversational AI research. The full dataset is
available at https://github.com/scutcyr/CPED.",0,1,1,1,0,0,0.921212,10.0,0.932312,85
a2c9e29f-ad72-421e-8765-5439d13e1072,MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer,74,0.784302,0.998943,"Self-supervised monocular depth estimation is an attractive solution that
does not require hard-to-source depth labels for training. Convolutional neural
networks (CNNs) have recently achieved great success in this task. However,
their limited receptive field constrains existing network architectures to
reason only locally, dampening the effectiveness of the self-supervised
paradigm. In the light of the recent successes achieved by Vision Transformers
(ViTs), we propose MonoViT, a brand-new framework combining the global
reasoning enabled by ViT models with the flexibility of self-supervised
monocular depth estimation. By combining plain convolutions with Transformer
blocks, our model can reason locally and globally, yielding depth prediction at
a higher level of detail and accuracy, allowing MonoViT to achieve
state-of-the-art performance on the established KITTI dataset. Moreover,
MonoViT proves its superior generalization capacities on other datasets such as
Make3D and DrivingStereo.",1,1,0,0,1,0,0.802356,7.0,0.839112,73
37986bb9-d9fb-4d20-a74e-6579caa97a70,The Naughtyformer: A Transformer Understands Offensive Humor,2,0.0493322,0.418534,"Jokes are intentionally written to be funny, but not all jokes are created
the same. Some jokes may be fit for a classroom of kindergarteners, but others
are best reserved for a more mature audience. While recent work has shown
impressive results on humor detection in text, here we instead investigate the
more nuanced task of detecting humor subtypes, especially of the less innocent
variety. To that end, we introduce a novel jokes dataset filtered from Reddit
and solve the subtype classification task using a finetuned Transformer dubbed
the Naughtyformer. Moreover, we show that our model is significantly better at
detecting offensiveness in jokes compared to state-of-the-art methods.",0,1,0,1,1,0,0.898686,7.0,0.888415,20
37cf1b32-ed58-4ce3-b634-e053fbab97e1,HICEM: A High-Coverage Emotion Model for Artificial Emotional Intelligence,9,0.0518616,0.526102,"As social robots and other intelligent machines enter the home, artificial
emotional intelligence (AEI) is taking center stage to address users' desire
for deeper, more meaningful human-machine interaction. To accomplish such
efficacious interaction, the next-generation AEI need comprehensive human
emotion models for training. Unlike theory of emotion, which has been the
historical focus in psychology, emotion models are a descriptive tools. In
practice, the strongest models need robust coverage, which means defining the
smallest core set of emotions from which all others can be derived. To achieve
the desired coverage, we turn to word embeddings from natural language
processing. Using unsupervised clustering techniques, our experiments show that
with as few as 15 discrete emotion categories, we can provide maximum coverage
across six major languages--Arabic, Chinese, English, French, Spanish, and
Russian. In support of our findings, we also examine annotations from two
large-scale emotion recognition datasets to assess the validity of existing
emotion models compared to human perception at scale. Because robust,
comprehensive emotion models are foundational for developing real-world
affective computing applications, this work has broad implications in social
robotics, human-machine interaction, mental healthcare, and computational
psychology.",0,0,1,0,0,0,0.0371287,20.0,0.755803,84
735b4e3b-eba9-40b2-9917-f8866a69eee9,Target-aware Molecular Graph Generation,20,0.306201,0.680885,"Generating molecules with desired biological activities has attracted growing
attention in drug discovery. Previous molecular generation models are designed
as chemocentric methods that hardly consider the drug-target interaction,
limiting their practical applications. In this paper, we aim to generate
molecular drugs in a target-aware manner that bridges biological activity and
molecular design. To solve this problem, we compile a benchmark dataset from
several publicly available datasets and build baselines in a unified framework.
Building on the recent advantages of flow-based molecular generation models, we
propose SiamFlow, which forces the flow to fit the distribution of target
sequence embeddings in latent space. Specifically, we employ an alignment loss
and a uniform loss to bring target sequence embeddings and drug graph
embeddings into agreements while avoiding collapse. Furthermore, we formulate
the alignment into a one-to-many problem by learning spaces of target sequence
embeddings. Experiments quantitatively show that our proposed method learns
meaningful representations in the latent space toward the target-aware
molecular graph generation and provides an alternative approach to bridge
biology and chemistry in drug discovery.",0,1,0,0,0,0,0.79352,8.0,0.855804,81
bef53db0-6075-4a0d-a72e-3d57bc82afbe,Phone2Proc: Bringing Robust Robots Into Our Chaotic World,6,0.273087,0.514767,"Training embodied agents in simulation has become mainstream for the embodied
AI community. However, these agents often struggle when deployed in the
physical world due to their inability to generalize to real-world environments.
In this paper, we present Phone2Proc, a method that uses a 10-minute phone scan
and conditional procedural generation to create a distribution of training
scenes that are semantically similar to the target environment. The generated
scenes are conditioned on the wall layout and arrangement of large objects from
the scan, while also sampling lighting, clutter, surface textures, and
instances of smaller objects with randomized placement and materials.
Leveraging just a simple RGB camera, training with Phone2Proc shows massive
improvements from 34.7% to 70.7% success rate in sim-to-real ObjectNav
performance across a test suite of over 200 trials in diverse real-world
environments, including homes, offices, and RoboTHOR. Furthermore, Phone2Proc's
diverse distribution of generated scenes makes agents remarkably robust to
changes in the real world, such as human movement, object rearrangement,
lighting changes, or clutter.",0,1,0,0,1,0,0.896503,5.0,0.841911,77
0fa93757-e7e2-44b0-bc6c-470b5585cc7e,Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,41,0.44048,0.694067,"Despite their strong performance on many tasks, pre-trained language models
have been shown to struggle on out-of-distribution compositional
generalization. Meanwhile, recent work has shown considerable improvements on
many NLP tasks from model scaling. Can scaling up model size also improve
compositional generalization in semantic parsing? We evaluate encoder-decoder
models up to 11B parameters and decoder-only models up to 540B parameters, and
compare model scaling curves for three different methods for applying a
pre-trained language model to a new task: fine-tuning all parameters, prompt
tuning, and in-context learning. We observe that fine-tuning generally has flat
or negative scaling curves on out-of-distribution compositional generalization
in semantic parsing evaluations. In-context learning has positive scaling
curves, but is generally outperformed by much smaller fine-tuned models.
Prompt-tuning can outperform fine-tuning, suggesting further potential
improvements from scaling as it exhibits a more positive scaling curve.
Additionally, we identify several error trends that vary with model scale. For
example, larger models are generally better at modeling the syntax of the
output space, but are also more prone to certain types of overfitting. Overall,
our study highlights limitations of current techniques for effectively
leveraging model scale for compositional generalization, while our analysis
also suggests promising directions for future work.",0,0,0,0,0,0,0.905846,3.0,0.750139,87
4a5d162c-439c-47bf-982b-507d2051e6d9,DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships,5,0.073023,0.117042,"In this paper, we propose DimonGen, which aims to generate diverse sentences
describing concept relationships in various everyday scenarios. To support
this, we first create a benchmark dataset for this task by adapting the
existing CommonGen dataset. We then propose a two-stage model called MoREE to
generate the target sentences. MoREE consists of a mixture of retrievers model
that retrieves diverse context sentences related to the given concepts, and a
mixture of generators model that generates diverse sentences based on the
retrieved contexts. We conduct experiments on the DimonGen task and show that
MoREE outperforms strong baselines in terms of both the quality and diversity
of the generated sentences. Our results demonstrate that MoREE is able to
generate diverse sentences that reflect different relationships between
concepts, leading to a comprehensive understanding of concept relationships.",1,1,1,1,0,0,0.69573,8.0,0.820547,45
177cdf62-b7e0-4412-a094-742c5bc4fc96,Epistemic AI platform accelerates innovation by connecting biomedical knowledge,1,0.00438388,0.0678157,"Epistemic AI accelerates biomedical discovery by finding hidden connections
in the network of biomedical knowledge. The Epistemic AI web-based software
platform embodies the concept of knowledge mapping, an interactive process that
relies on a knowledge graph in combination with natural language processing
(NLP), information retrieval, relevance feedback, and network analysis.
Knowledge mapping reduces information overload, prevents costly mistakes, and
minimizes missed opportunities in the research process. The platform combines
state-of-the-art methods for information extraction with machine learning,
artificial intelligence and network analysis. Starting from a single biological
entity, such as a gene or disease, users may: a) construct a map of connections
to that entity, b) map an entire domain of interest, and c) gain insight into
large biological networks of knowledge. Knowledge maps provide clarity and
organization, simplifying the day-to-day research processes.",0,1,0,0,0,0,0.000317008,17.0,0.43142,75
4e314d52-a2eb-4e76-ad22-c2e82b27d857,Multi-Class 3D Object Detection with Single-Class Supervision,1,0.0,0.0345281,"While multi-class 3D detectors are needed in many robotics applications,
training them with fully labeled datasets can be expensive in labeling cost. An
alternative approach is to have targeted single-class labels on disjoint data
samples. In this paper, we are interested in training a multi-class 3D object
detection model, while using these single-class labeled data. We begin by
detailing the unique stance of our ""Single-Class Supervision"" (SCS) setting
with respect to related concepts such as partial supervision and semi
supervision. Then, based on the case study of training the multi-class version
of Range Sparse Net (RSN), we adapt a spectrum of algorithms -- from supervised
learning to pseudo-labeling -- to fully exploit the properties of our SCS
setting, and perform extensive ablation studies to identify the most effective
algorithm and practice. Empirical experiments on the Waymo Open Dataset show
that proper training under SCS can approach or match full supervision training
while saving labeling costs.",0,1,0,0,0,0,0.944018,6.0,0.908216,38
949eb9f5-beee-483f-8004-110d20a572ed,Unsupervised and Few-shot Parsing from Pretrained Language Models,3,0.0147273,0.055583,"Pretrained language models are generally acknowledged to be able to encode
syntax [Tenney et al., 2019, Jawahar et al., 2019, Hewitt and Manning, 2019].
In this article, we propose UPOA, an Unsupervised constituent Parsing model
that calculates an Out Association score solely based on the self-attention
weight matrix learned in a pretrained language model as the syntactic distance
for span segmentation. We further propose an enhanced version, UPIO, which
exploits both inside association and outside association scores for estimating
the likelihood of a span. Experiments with UPOA and UPIO disclose that the
linear projection matrices for the query and key in the self-attention
mechanism play an important role in parsing. We therefore extend the
unsupervised models to few-shot parsing models (FPOA, FPIO) that use a few
annotated trees to learn better linear projection matrices for parsing.
Experiments on the Penn Treebank demonstrate that our unsupervised parsing
model UPIO achieves results comparable to the state of the art on short
sentences (length <= 10). Our few-shot parsing model FPIO trained with only 20
annotated trees outperforms a previous few-shot parsing method trained with 50
annotated trees. Experiments on cross-lingual parsing show that both
unsupervised and few-shot parsing methods are better than previous methods on
most languages of SPMRL [Seddah et al., 2013].",0,0,0,0,1,0,0.2096,7.0,0.56333,51
01d83098-3e11-4d3e-bc67-499b07b8c4f5,PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales,41,0.120924,0.64001,"Neural language models (LMs) have achieved impressive results on various
language-based reasoning tasks by utilizing latent knowledge encoded in their
own pretrained parameters. To make this reasoning process more explicit, recent
works retrieve a rationalizing LM's internal knowledge by training or prompting
it to generate free-text rationales, which can be used to guide task
predictions made by either the same LM or a separate reasoning LM. However,
rationalizing LMs require expensive rationale annotation and/or computation,
without any assurance that their generated rationales improve LM task
performance or faithfully reflect LM decision-making. In this paper, we propose
PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns
to faithfully reason over rationales via counterfactual regularization. First,
PINTO maps out a suitable reasoning process for the task input by prompting a
frozen rationalizing LM to generate a free-text rationale. Second, PINTO's
reasoning LM is fine-tuned to solve the task using the generated rationale as
context, while regularized to output less confident predictions when the
rationale is perturbed. Across four datasets, we show that PINTO significantly
improves the generalization ability of the reasoning LM, yielding higher
performance on both in-distribution and out-of-distribution test sets. Also, we
find that PINTO's rationales are more faithful to its task predictions than
those generated by competitive baselines.",1,0,0,0,0,1,0.19868,7.0,0.554744,66
9f93c85c-a45e-428f-841f-a43037b39bd0,MetaEMS: A Meta Reinforcement Learning-based Control Framework for Building Energy Management System,6,0.0713453,0.390883,"The building sector has been recognized as one of the primary sectors for
worldwide energy consumption. Improving the energy efficiency of the building
sector can help reduce the operation cost and reduce the greenhouse gas
emission. The energy management system (EMS) can monitor and control the
operations of built-in appliances in buildings, so an efficient EMS is of
crucial importance to improve the building operation efficiency and maintain
safe operations. With the growing penetration of renewable energy and
electrical appliances, increasing attention has been paid to the development of
intelligent building EMS. Recently, reinforcement learning (RL) has been
applied for building EMS and has shown promising potential. However, most of
the current RL-based EMS solutions would need a large amount of data to learn a
reliable control policy, which limits the applicability of these solutions in
the real world. In this work, we propose MetaEMS, which can help achieve better
energy management performance with the benefits of RL and meta-learning.
Experiment results showcase that our proposed MetaEMS can adapt faster to
environment changes and perform better in most situations compared with other
baselines.",0,1,0,0,0,0,0.482674,6.0,0.662276,51
94625059-62c4-4285-a5c2-a5051aad85d9,Adaptation Approaches for Nearest Neighbor Language Models,6,0.0158459,0.371983,"Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced
impressive gains over purely parametric LMs, by leveraging large-scale
neighborhood retrieval over external memory datastores. However, there has been
little investigation into adapting such models for new domains. This work
attempts to fill that gap and suggests the following approaches for adapting
$k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding
neighborhood retrieval over an additional adaptation datastore, and 3) adapting
the weights (scores) of retrieved neighbors using a learned Rescorer module. We
study each adaptation strategy separately, as well as the combined performance
improvement through ablation experiments and an extensive set of evaluations
run over seven adaptation domains. Our combined adaptation approach
consistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM)
baselines that construct datastores from the adaptation data. On average, we
see perplexity improvements of 17.1% and 16% for these respective baselines,
across domains.",0,1,0,0,0,0,0.683878,5.0,0.706348,28
837cd79e-9603-4bc0-9919-601329515093,Towards True Detail Restoration for Super-Resolution: A Benchmark and a Quality Metric,5,0.111986,0.0809544,"Super-resolution (SR) has become a widely researched topic in recent years.
SR methods can improve overall image and video quality and create new
possibilities for further content analysis. But the SR mainstream focuses
primarily on increasing the naturalness of the resulting image despite
potentially losing context accuracy. Such methods may produce an incorrect
digit, character, face, or other structural object even though they otherwise
yield good visual quality. Incorrect detail restoration can cause errors when
detecting and identifying objects both manually and automatically. To analyze
the detail-restoration capabilities of image and video SR models, we developed
a benchmark based on our own video dataset, which contains complex patterns
that SR models generally fail to correctly restore. We assessed 32 recent SR
models using our benchmark and compared their ability to preserve scene
context. We also conducted a crowd-sourced comparison of restored details and
developed an objective assessment metric that outperforms other quality metrics
by correlation with subjective scores for this task. In conclusion, we provide
a deep analysis of benchmark results that yields insights for future SR-based
work.",0,1,0,0,0,0,0.874341,5.0,0.824026,45
35c11a95-be13-443d-9767-418d982df015,Addressing Client Drift in Federated Continual Learning with Adaptive Optimization,4,0.0200451,0.115949,"Federated learning has been extensively studied and is the prevalent method
for privacy-preserving distributed learning in edge devices. Correspondingly,
continual learning is an emerging field targeted towards learning multiple
tasks sequentially. However, there is little attention towards additional
challenges emerging when federated aggregation is performed in a continual
learning system. We identify \textit{client drift} as one of the key weaknesses
that arise when vanilla federated averaging is applied in such a system,
especially since each client can independently have different order of tasks.
We outline a framework for performing Federated Continual Learning (FCL) by
using NetTailor as a candidate continual learning approach and show the extent
of the problem of client drift. We show that adaptive federated optimization
can reduce the adverse impact of client drift and showcase its effectiveness on
CIFAR100, MiniImagenet, and Decathlon benchmarks. Further, we provide an
empirical analysis highlighting the interplay between different hyperparameters
such as client and server learning rates, the number of local training
iterations, and communication rounds. Finally, we evaluate our framework on
useful characteristics of federated learning systems such as scalability,
robustness to the skewness in clients' data distribution, and stragglers.",0,1,0,0,0,0,0.483969,9.0,0.775272,59
59b4442a-521d-471f-8a3a-53c2454325ed,Adaptive Testing of Computer Vision Models,21,0.0734164,0.658418,"Vision models often fail systematically on groups of data that share common
semantic characteristics (e.g., rare objects or unusual scenes), but
identifying these failure modes is a challenge. We introduce AdaVision, an
interactive process for testing vision models which helps users identify and
fix coherent failure modes. Given a natural language description of a coherent
group, AdaVision retrieves relevant images from LAION-5B with CLIP. The user
then labels a small amount of data for model correctness, which is used in
successive retrieval rounds to hill-climb towards high-error regions, refining
the group definition. Once a group is saturated, AdaVision uses GPT-3 to
suggest new group descriptions for the user to explore. We demonstrate the
usefulness and generality of AdaVision in user studies, where users find major
bugs in state-of-the-art classification, object detection, and image captioning
models. These user-discovered groups have failure rates 2-3x higher than those
surfaced by automatic error clustering methods. Finally, finetuning on examples
found with AdaVision fixes the discovered bugs when evaluated on unseen
examples, without degrading in-distribution accuracy, and while also improving
performance on out-of-distribution datasets.",1,1,1,0,0,0,0.462417,5.0,0.582722,69
a130c686-62d3-45d7-ac85-6c91c1622496,Cross-Architecture Self-supervised Video Representation Learning,18,0.116075,0.612859,"In this paper, we present a new cross-architecture contrastive learning
(CACL) framework for self-supervised video representation learning. CACL
consists of a 3D CNN and a video transformer which are used in parallel to
generate diverse positive pairs for contrastive learning. This allows the model
to learn strong representations from such diverse yet meaningful pairs.
Furthermore, we introduce a temporal self-supervised learning module able to
predict an Edit distance explicitly between two video sequences in the temporal
order. This enables the model to learn a rich temporal representation that
compensates strongly to the video-level representation learned by the CACL. We
evaluate our method on the tasks of video retrieval and action recognition on
UCF101 and HMDB51 datasets, where our method achieves excellent performance,
surpassing the state-of-the-art methods such as VideoMoCo and MoCo+BE by a
large margin. The code is made available at https://github.com/guoshengcv/CACL.",1,0,0,0,1,0,0.845,5.0,0.802693,45
86230e6d-df16-4015-bfe2-dc32e20c35e9,ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields,12,0.192474,0.47641,"While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline
methods with an emphasis on visual fidelity, our paper addresses the online use
case that prioritises real-time adaptability. We present ParticleNeRF, a new
approach that dynamically adapts to changes in the scene geometry by learning
an up-to-date representation online, every 200ms. ParticleNeRF achieves this
using a novel particle-based parametric encoding. We couple features to
particles in space and backpropagate the photometric reconstruction loss into
the particles' position gradients, which are then interpreted as velocity
vectors. Governed by a lightweight physics system to handle collisions, this
lets the features move freely with the changing scene geometry. We demonstrate
ParticleNeRF on various dynamic scenes containing translating, rotating,
articulated, and deformable objects. ParticleNeRF is the first online dynamic
NeRF and achieves fast adaptability with better visual fidelity than
brute-force online InstantNGP and other baseline approaches on dynamic scenes
with online constraints. Videos of our system can be found at our project
website https://sites.google.com/view/particlenerf.",1,1,1,1,0,0,0.967164,2.0,0.809548,33
df13d9ff-ef78-4c70-a9cc-f7491b97268c,Contextually Enhanced ES-dRNN with Dynamic Attention for Short-Term Load Forecasting,4,0.0616207,0.414567,"In this paper, we propose a new short-term load forecasting (STLF) model
based on contextually enhanced hybrid and hierarchical architecture combining
exponential smoothing (ES) and a recurrent neural network (RNN). The model is
composed of two simultaneously trained tracks: the context track and the main
track. The context track introduces additional information to the main track.
It is extracted from representative series and dynamically modulated to adjust
to the individual series forecasted by the main track. The RNN architecture
consists of multiple recurrent layers stacked with hierarchical dilations and
equipped with recently proposed attentive dilated recurrent cells. These cells
enable the model to capture short-term, long-term and seasonal dependencies
across time series as well as to weight dynamically the input information. The
model produces both point forecasts and predictive intervals. The experimental
part of the work performed on 35 forecasting problems shows that the proposed
model outperforms in terms of accuracy its predecessor as well as standard
statistical models and state-of-the-art machine learning models.",1,1,0,0,1,0,0.413888,8.0,0.720458,59
0965388a-a40e-4c3d-ab3f-397a78c6373f,IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes,30,0.463819,0.627442,"Indoor scenes exhibit significant appearance variations due to myriad
interactions between arbitrarily diverse object shapes, spatially-changing
materials, and complex lighting. Shadows, highlights, and inter-reflections
caused by visible and invisible light sources require reasoning about
long-range interactions for inverse rendering, which seeks to recover the
components of image formation, namely, shape, material, and lighting. In this
work, our intuition is that the long-range attention learned by transformer
architectures is ideally suited to solve longstanding challenges in
single-image inverse rendering. We demonstrate with a specific instantiation of
a dense vision transformer, IRISformer, that excels at both single-task and
multi-task reasoning required for inverse rendering. Specifically, we propose a
transformer architecture to simultaneously estimate depths, normals,
spatially-varying albedo, roughness and lighting from a single image of an
indoor scene. Our extensive evaluations on benchmark datasets demonstrate
state-of-the-art results on each of the above tasks, enabling applications like
object insertion and material editing in a single unconstrained real image,
with greater photorealism than prior works. Code and data are publicly released
at https://github.com/ViLab-UCSD/IRISformer.",1,1,0,0,1,0,0.944776,8.0,0.931752,49
1a243fd7-8f3c-4514-8662-4916863079fc,Joint Learning-Based Stabilization of Multiple Unknown Linear Systems,5,0.0,0.256302,"Learning-based control of linear systems received a lot of attentions
recently. In popular settings, the true dynamical models are unknown to the
decision-maker and need to be interactively learned by applying control inputs
to the systems. Unlike the matured literature of efficient reinforcement
learning policies for adaptive control of a single system, results on joint
learning of multiple systems are not currently available. Especially, the
important problem of fast and reliable joint-stabilization remains unaddressed
and so is the focus of this work. We propose a novel joint learning-based
stabilization algorithm for quickly learning stabilizing policies for all
systems understudy, from the data of unstable state trajectories. The presented
procedure is shown to be notably effective such that it stabilizes the family
of dynamical systems in an extremely short time period.",0,0,0,0,0,0,0.0175502,8.0,0.294589,30
ab16e516-fcdd-4f11-816d-33484202c5d8,Enriching Relation Extraction with OpenIE,1,0.00759644,0.0893831,"Relation extraction (RE) is a sub-discipline of information extraction (IE)
which focuses on the prediction of a relational predicate from a
natural-language input unit (such as a sentence, a clause, or even a short
paragraph consisting of multiple sentences and/or clauses). Together with
named-entity recognition (NER) and disambiguation (NED), RE forms the basis for
many advanced IE tasks such as knowledge-base (KB) population and verification.
In this work, we explore how recent approaches for open information extraction
(OpenIE) may help to improve the task of RE by encoding structured information
about the sentences' principal units, such as subjects, objects, verbal
phrases, and adverbials, into various forms of vectorized (and hence
unstructured) representations of the sentences. Our main conjecture is that the
decomposition of long and possibly convoluted sentences into multiple smaller
clauses via OpenIE even helps to fine-tune context-sensitive language models
such as BERT (and its plethora of variants) for RE. Our experiments over two
annotated corpora, KnowledgeNet and FewRel, demonstrate the improved accuracy
of our enriched models compared to existing RE approaches. Our best results
reach 92% and 71% of F1 score for KnowledgeNet and FewRel, respectively,
proving the effectiveness of our approach on competitive benchmarks.",0,1,0,0,0,0,0.377928,9.0,0.738388,54
c0ae58b4-9e97-4a9e-84bd-0759f05613bb,TERMinator: A system for scientific texts processing,1,0.0104812,0.0642775,"This paper is devoted to the extraction of entities and semantic relations
between them from scientific texts, where we consider scientific terms as
entities. In this paper, we present a dataset that includes annotations for two
tasks and develop a system called TERMinator for the study of the influence of
language models on term recognition and comparison of different approaches for
relation extraction. Experiments show that language models pre-trained on the
target language are not always show the best performance. Also adding some
heuristic approaches may improve the overall quality of the particular task.
The developed tool and the annotated corpus are publicly available at
https://github.com/iis-research-team/terminator and may be useful for other
researchers.",1,1,0,1,0,0,0.236432,7.0,0.582902,37
d4260d10-4756-4fda-a75c-c72817f80000,DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech,16,0.588755,0.965861,"The majority of current Text-to-Speech (TTS) datasets, which are collections
of individual utterances, contain few conversational aspects. In this paper, we
introduce DailyTalk, a high-quality conversational speech dataset designed for
conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the
open-domain dialogue dataset DailyDialog inheriting its annotated attributes.
On top of our dataset, we extend prior work as our baseline, where a
non-autoregressive TTS is conditioned on historical information in a dialogue.
From the baseline experiment with both general and our novel metrics, we show
that DailyTalk can be used as a general TTS dataset, and more than that, our
baseline can represent contextual information from DailyTalk. The DailyTalk
dataset and baseline code are freely available for academic use with CC-BY-SA
4.0 license.",1,1,1,1,0,0,0.765719,7.0,0.823282,19
1259f461-e3ec-4ff4-9b43-f0a69172b844,Putting AI Ethics into Practice: The Hourglass Model of Organizational AI Governance,8,0.389863,0.543743,"The organizational use of artificial intelligence (AI) has rapidly spread
across various sectors. Alongside the awareness of the benefits brought by AI,
there is a growing consensus on the necessity of tackling the risks and
potential harms, such as bias and discrimination, brought about by advanced AI
technologies. A multitude of AI ethics principles have been proposed to tackle
these risks, but the outlines of organizational processes and practices for
ensuring socially responsible AI development are in a nascent state. To address
the paucity of comprehensive governance models, we present an AI governance
framework, the hourglass model of organizational AI governance, which targets
organizations that develop and use AI systems. The framework is designed to
help organizations deploying AI systems translate ethical AI principles into
practice and align their AI systems and processes with the forthcoming European
AI Act. The hourglass framework includes governance requirements at the
environmental, organizational, and AI system levels. At the AI system level, we
connect governance requirements to AI system life cycles to ensure governance
throughout the system's life span. The governance model highlights the systemic
nature of AI governance and opens new research avenues into its practical
implementation, the mechanisms that connect different AI governance layers, and
the dynamics between the AI governance actors. The model also offers a starting
point for organizational decision-makers to consider the governance components
needed to ensure social acceptability, mitigate risks, and realize the
potential of AI.",0,1,0,0,0,0,0.908509,5.0,0.852498,82
1be6fd32-a077-4753-9e05-6ff963b3bf7e,Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents,41,0.397537,0.715892,"At the heart of improving conversational AI is the open problem of how to
evaluate conversations. Issues with automatic metrics are well known (Liu et
al., 2016, arXiv:1603.08023), with human evaluations still considered the gold
standard. Unfortunately, how to perform human evaluations is also an open
problem: differing data collection methods have varying levels of human
agreement and statistical sensitivity, resulting in differing amounts of human
annotation hours and labor costs. In this work we compare five different
crowdworker-based human evaluation methods and find that different methods are
best depending on the types of models compared, with no clear winner across the
board. While this highlights the open problems in the area, our analysis leads
to advice of when to use which one, and possible future directions.",0,1,0,0,0,0,0.652764,6.0,0.741118,60
6cd9fd7d-7c51-4c06-9539-36b9d909f027,Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries,27,0.768012,0.854259,"Learning under a continuously changing data distribution with incorrect
labels is a desirable real-world problem yet challenging. A large body of
continual learning (CL) methods, however, assumes data streams with clean
labels, and online learning scenarios under noisy data streams are yet
underexplored. We consider a more practical CL task setup of an online learning
from blurry data stream with corrupted labels, where existing CL methods
struggle. To address the task, we first argue the importance of both diversity
and purity of examples in the episodic memory of continual learning models. To
balance diversity and purity in the episodic memory, we propose a novel
strategy to manage and use the memory by a unified approach of label noise
aware diverse sampling and robust learning with semi-supervised learning. Our
empirical validations on four real-world or synthetic noise datasets (CIFAR10
and 100, mini-WebVision, and Food-101N) exhibit that our method significantly
outperforms prior arts in this realistic and challenging continual learning
scenario. Code and data splits are available in
https://github.com/clovaai/puridiver.",1,0,0,0,0,0,0.987995,7.0,0.982465,40
52aa9c64-f13a-41c1-b11d-9240477535b2,Help Me Explore: Minimal Social Interventions for Graph-Based Autotelic Agents,6,0.0766334,0.572333,"In the quest for autonomous agents learning open-ended repertoires of skills,
most works take a Piagetian perspective: learning trajectories are the results
of interactions between developmental agents and their physical environment.
The Vygotskian perspective, on the other hand, emphasizes the centrality of the
socio-cultural environment: higher cognitive functions emerge from
transmissions of socio-cultural processes internalized by the agent. This paper
argues that both perspectives could be coupled within the learning of autotelic
agents to foster their skill acquisition. To this end, we make two
contributions: 1) a novel social interaction protocol called Help Me Explore
(HME), where autotelic agents can benefit from both individual and socially
guided exploration. In social episodes, a social partner suggests goals at the
frontier of the learning agent knowledge. In autotelic episodes, agents can
either learn to master their own discovered goals or autonomously rehearse
failed social goals; 2) GANGSTR, a graph-based autotelic agent for manipulation
domains capable of decomposing goals into sequences of intermediate sub-goals.
We show that when learning within HME, GANGSTR overcomes its individual
learning limits by mastering the most complex configurations (e.g. stacks of 5
blocks) with only few social interventions.",1,0,1,0,0,0,0.310894,11.0,0.763879,71
c73e516e-cb87-45b7-8c81-c258feba2e44,PointInst3D: Segmenting 3D Instances by Points,14,0.140376,0.691061,"The current state-of-the-art methods in 3D instance segmentation typically
involve a clustering step, despite the tendency towards heuristics, greedy
algorithms, and a lack of robustness to the changes in data statistics. In
contrast, we propose a fully-convolutional 3D point cloud instance segmentation
method that works in a per-point prediction fashion. In doing so it avoids the
challenges that clustering-based methods face: introducing dependencies among
different tasks of the model. We find the key to its success is assigning a
suitable target to each sampled point. Instead of the commonly used static or
distance-based assignment strategies, we propose to use an Optimal Transport
approach to optimally assign target masks to the sampled points according to
the dynamic matching costs. Our approach achieves promising results on both
ScanNet and S3DIS benchmarks. The proposed approach removes intertask
dependencies and thus represents a simpler and more flexible 3D instance
segmentation framework than other competing methods, while achieving improved
segmentation accuracy.",0,1,0,0,1,0,0.850355,7.0,0.861736,43
9f20a40d-82b7-47c0-ad2f-3a8887ea8863,Target-Guided Open-Domain Conversation Planning,6,0.190058,0.204206,"Prior studies addressing target-oriented conversational tasks lack a crucial
notion that has been intensively studied in the context of goal-oriented
artificial intelligence agents, namely, planning. In this study, we propose the
task of Target-Guided Open-Domain Conversation Planning (TGCP) task to evaluate
whether neural conversational agents have goal-oriented conversation planning
abilities. Using the TGCP task, we investigate the conversation planning
abilities of existing retrieval models and recent strong generative models. The
experimental results reveal the challenges facing current technology.",0,0,1,0,0,0,0.526989,7.0,0.728723,38
bbdfe7a8-dd10-42f5-8f3d-aebd416f949f,Focal Length and Object Pose Estimation via Render and Compare,11,0.205226,0.914306,"We introduce FocalPose, a neural render-and-compare method for jointly
estimating the camera-object 6D pose and camera focal length given a single RGB
input image depicting a known object. The contributions of this work are
twofold. First, we derive a focal length update rule that extends an existing
state-of-the-art render-and-compare 6D pose estimator to address the joint
estimation task. Second, we investigate several different loss functions for
jointly estimating the object pose and focal length. We find that a combination
of direct focal length regression with a reprojection loss disentangling the
contribution of translation, rotation, and focal length leads to improved
results. We show results on three challenging benchmark datasets that depict
known 3D models in uncontrolled settings. We demonstrate that our focal length
and 6D pose estimates have lower error than the existing state-of-the-art
methods.",0,1,0,0,1,0,0.739925,11.0,0.880753,50
500806a0-eb85-4b9a-bb88-0406c9f30e22,NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis,68,0.418641,0.956993,"Sentiment analysis is one of the most widely studied applications in NLP, but
most work focuses on languages with large amounts of data. We introduce the
first large-scale human-annotated Twitter sentiment dataset for the four most
widely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and
Yor\`ub\'a ) consisting of around 30,000 annotated tweets per language (and
14,000 for Nigerian-Pidgin), including a significant fraction of code-mixed
tweets. We propose text collection, filtering, processing and labeling methods
that enable us to create datasets for these low-resource languages. We evaluate
a rangeof pre-trained models and transfer strategies on the dataset. We find
that language-specific models and language-adaptivefine-tuning generally
perform best. We release the datasets, trained models, sentiment lexicons, and
code to incentivizeresearch on sentiment analysis in under-represented
languages.",0,1,1,1,0,0,0.436515,5.0,0.566953,81
41e69fac-fba3-4486-aa54-50e39be6acec,DICE: Data-Efficient Clinical Event Extraction with Generative Models,11,0.35416,0.784135,"Event extraction for the clinical domain is an under-explored research area.
The lack of training data along with the high volume of domain-specific
terminologies with vague entity boundaries makes the task especially
challenging. In this paper, we introduce DICE, a robust and data-efficient
generative model for clinical event extraction. DICE frames event extraction as
a conditional generation problem and introduces a contrastive learning
objective to accurately decide the boundaries of biomedical mentions. DICE also
trains an auxiliary mention identification task jointly with event extraction
tasks to better identify entity mention boundaries, and further introduces
special markers to incorporate identified entity mentions as trigger and
argument candidates for their respective tasks. To benchmark clinical event
extraction, we compose MACCROBAT-EE, the first clinical event extraction
dataset with argument annotation, based on an existing clinical information
extraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art
performances of DICE for clinical and news domain event extraction, especially
under low data settings.",0,1,1,1,1,0,0.84335,5.0,0.801554,66
345e3520-81f4-453a-b340-3f5d727ddbaf,Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution,3,0.0135018,0.108426,"Nowadays, thermal infrared satellite remote sensors enable to extract very
interesting information at large scale, in particular Land Surface Temperature
(LST). However such data are limited in spatial and/or temporal resolutions
which prevents from an analysis at fine scales. For example, MODIS satellite
provides daily acquisitions with 1Km spatial resolutions which is not
sufficient to deal with highly heterogeneous environments as agricultural
parcels. Therefore, image super-resolution is a crucial task to better exploit
MODIS LSTs. This issue is tackled in this paper. We introduce a deep
learning-based algorithm, named Multi-residual U-Net, for super-resolution of
MODIS LST single-images. Our proposed network is a modified version of U-Net
architecture, which aims at super-resolving the input LST image from 1Km to
250m per pixel. The results show that our Multi-residual U-Net outperforms
other state-of-the-art methods.",1,1,0,0,1,0,0.0210258,12.0,0.54493,22
b024d150-25e0-402c-9efc-919c0ade24ba,GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language,7,0.024493,0.385402,"Helping end users comprehend the abstract distribution shifts can greatly
facilitate AI deployment. Motivated by this, we propose a novel task, dataset
explanation. Given two image data sets, dataset explanation aims to
automatically point out their dataset-level distribution shifts with natural
language. Current techniques for monitoring distribution shifts provide
inadequate information to understand datasets with the goal of improving data
quality. Therefore, we introduce GSCLIP, a training-free framework to solve the
dataset explanation task. In GSCLIP, we propose the selector as the first
quantitative evaluation method to identify explanations that are proper to
summarize dataset shifts. Furthermore, we leverage this selector to demonstrate
the superiority of a generator based on language model generation. Systematic
evaluation on natural data shift verifies that GSCLIP, a combined system of a
hybrid generator group and an efficient selector is not only easy-to-use but
also powerful for dataset explanation at scale.",0,0,1,0,0,0,0.418123,7.0,0.682449,32
4fc48b61-7264-4afe-bb11-cb0504b83ca8,Proceedings of the 2022 XCSP3 Competition,3,0.00674559,0.162854,"This document represents the proceedings of the 2022 XCSP3 Competition. The
results of this competition of constraint solvers were presented at FLOC
(Federated Logic Conference) 2022 Olympic Games, held in Haifa, Israel from
31th July 2022 to 7th August, 2022.",0,1,0,0,0,0,1.80426e-06,19.0,0.219221,95
809a632d-5c83-4e0c-875f-0b1825df7400,ClearPose: Large-scale Transparent Object Dataset and Benchmark,22,0.233736,0.877727,"Transparent objects are ubiquitous in household settings and pose distinct
challenges for visual sensing and perception systems. The optical properties of
transparent objects leave conventional 3D sensors alone unreliable for object
depth and pose estimation. These challenges are highlighted by the shortage of
large-scale RGB-Depth datasets focusing on transparent objects in real-world
settings. In this work, we contribute a large-scale real-world RGB-Depth
transparent object dataset named ClearPose to serve as a benchmark dataset for
segmentation, scene-level depth completion and object-centric pose estimation
tasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth
frames and 5M instance annotations covering 63 household objects. The dataset
includes object categories commonly used in daily life under various lighting
and occluding conditions as well as challenging test scenarios such as cases of
occlusion by opaque or translucent objects, non-planar orientations, presence
of liquids, etc. We benchmark several state-of-the-art depth completion and
object pose estimation deep neural networks on ClearPose. The dataset and
benchmarking source code is available at https://github.com/opipari/ClearPose.",1,1,1,1,0,0,0.521753,5.0,0.617249,27
ee6e9efc-acfd-4202-abf2-4e1595b79fd8,Neighbor Correspondence Matching for Flow-based Video Frame Synthesis,5,0.13189,0.263295,"Video frame synthesis, which consists of interpolation and extrapolation, is
an essential video processing technique that can be applied to various
scenarios. However, most existing methods cannot handle small objects or large
motion well, especially in high-resolution videos such as 4K videos. To
eliminate such limitations, we introduce a neighbor correspondence matching
(NCM) algorithm for flow-based frame synthesis. Since the current frame is not
available in video frame synthesis, NCM is performed in a
current-frame-agnostic fashion to establish multi-scale correspondences in the
spatial-temporal neighborhoods of each pixel. Based on the powerful motion
representation capability of NCM, we further propose to estimate intermediate
flows for frame synthesis in a heterogeneous coarse-to-fine scheme.
Specifically, the coarse-scale module is designed to leverage neighbor
correspondences to capture large motion, while the fine-scale module is more
computationally efficient to speed up the estimation process. Both modules are
trained progressively to eliminate the resolution gap between training dataset
and real-world videos. Experimental results show that NCM achieves
state-of-the-art performance on several benchmarks. In addition, NCM can be
applied to various practical scenarios such as video compression to achieve
better performance.",0,1,0,0,1,0,0.966376,7.0,0.94459,42
d4164fb9-a4a3-4419-8e4a-9df84e7a4c9f,Uncertainty-aware Personal Assistant for Making Personalized Privacy Decisions,7,0.140688,0.683977,"Many software systems, such as online social networks enable users to share
information about themselves. While the action of sharing is simple, it
requires an elaborate thought process on privacy: what to share, with whom to
share, and for what purposes. Thinking about these for each piece of content to
be shared is tedious. Recent approaches to tackle this problem build personal
assistants that can help users by learning what is private over time and
recommending privacy labels such as private or public to individual content
that a user considers sharing. However, privacy is inherently ambiguous and
highly personal. Existing approaches to recommend privacy decisions do not
address these aspects of privacy sufficiently. Ideally, a personal assistant
should be able to adjust its recommendation based on a given user, considering
that user's privacy understanding. Moreover, the personal assistant should be
able to assess when its recommendation would be uncertain and let the user make
the decision on her own. Accordingly, this paper proposes a personal assistant
that uses evidential deep learning to classify content based on its privacy
label. An important characteristic of the personal assistant is that it can
model its uncertainty in its decisions explicitly, determine that it does not
know the answer, and delegate from making a recommendation when its uncertainty
is high. By factoring in the user's own understanding of privacy, such as risk
factors or own labels, the personal assistant can personalize its
recommendations per user. We evaluate our proposed personal assistant using a
well-known data set. Our results show that our personal assistant can
accurately identify uncertain cases, personalize them to its user's needs, and
thus helps users preserve their privacy well.",0,1,0,0,0,0,0.125699,12.0,0.698593,49
2ab0ab90-8cd0-4dc5-9c86-bb5a6325fc50,Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation,9,0.046464,0.501909,"Single-Image Super-Resolution can support robotic tasks in environments where
a reliable visual stream is required to monitor the mission, handle
teleoperation or study relevant visual details. In this work, we propose an
efficient Generative Adversarial Network model for real-time Super-Resolution,
called EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We
adopt a tailored architecture of the original SRGAN and model quantization to
boost the execution on CPU and Edge TPU devices, achieving up to 200 fps
inference. We further optimize our model by distilling its knowledge to a
smaller version of the network and obtain remarkable improvements compared to
the standard training approach. Our experiments show that our fast and
lightweight model preserves considerably satisfying image quality compared to
heavier state-of-the-art models. Finally, we conduct experiments on image
transmission with bandwidth degradation to highlight the advantages of the
proposed system for mobile robotic applications.",1,1,0,0,1,0,0.241117,7.0,0.586125,75
50e40109-6d30-440f-afed-9c1f2fc52575,PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices,37,0.362397,0.871218,"Neural radiance-density field methods have become increasingly popular for
the task of novel-view rendering. Their recent extension to hash-based
positional encoding ensures fast training and inference with visually pleasing
results. However, density-based methods struggle with recovering accurate
surface geometry. Hybrid methods alleviate this issue by optimizing the density
based on an underlying SDF. However, current SDF methods are overly smooth and
miss fine geometric details. In this work, we combine the strengths of these
two lines of work in a novel hash-based implicit surface representation. We
propose improvements to the two areas by replacing the voxel hash encoding with
a permutohedral lattice which optimizes faster, especially for higher
dimensions. We additionally propose a regularization scheme which is crucial
for recovering high-frequency geometric detail. We evaluate our method on
multiple datasets and show that we can recover geometric detail at the level of
pores and wrinkles while using only RGB images for supervision. Furthermore,
using sphere tracing we can render novel views at 30 fps on an RTX 3090. Code
is publicly available at: https://radualexandru.github.io/permuto_sdf",1,1,0,0,0,0,0.931774,6.0,0.89637,37
451327b8-027e-4b16-8c33-490b3573b4a3,Solutions to preference manipulation in recommender systems require knowledge of meta-preferences,4,0.134933,0.160338,"Iterative machine learning algorithms used to power recommender systems often
change people's preferences by trying to learn them. Further a recommender can
better predict what a user will do by making its users more predictable. Some
preference changes on the part of the user are self-induced and desired whether
the recommender caused them or not. This paper proposes that solutions to
preference manipulation in recommender systems must take into account certain
meta-preferences (preferences over another preference) in order to respect the
autonomy of the user and not be manipulative.",0,0,0,0,0,0,0.545801,6.0,0.692309,30
01baf525-564c-45d3-85d5-a928a7fabf05,RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection,11,0.201362,0.845796,"This paper presents a Refinement Pyramid Transformer (RePFormer) for robust
facial landmark detection. Most facial landmark detectors focus on learning
representative image features. However, these CNN-based feature representations
are not robust enough to handle complex real-world scenarios due to ignoring
the internal structure of landmarks, as well as the relations between landmarks
and context. In this work, we formulate the facial landmark detection task as
refining landmark queries along pyramid memories. Specifically, a pyramid
transformer head (PTH) is introduced to build both homologous relations among
landmarks and heterologous relations between landmarks and cross-scale
contexts. Besides, a dynamic landmark refinement (DLR) module is designed to
decompose the landmark regression into an end-to-end refinement procedure,
where the dynamically aggregated queries are transformed to residual
coordinates predictions. Extensive experimental results on four facial landmark
detection benchmarks and their various subsets demonstrate the superior
performance and high robustness of our framework.",0,1,0,0,1,0,0.905464,9.0,0.916523,29
265fc7d5-8b3e-4526-8b07-4578135fd712,Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment,49,0.486501,0.872701,"Training a generative adversarial network (GAN) with limited data has been a
challenging task. A feasible solution is to start with a GAN well-trained on a
large scale source domain and adapt it to the target domain with a few samples,
termed as few shot generative model adaption. However, existing methods are
prone to model overfitting and collapse in extremely few shot setting (less
than 10). To solve this problem, we propose a relaxed spatial structural
alignment method to calibrate the target generative models during the adaption.
We design a cross-domain spatial structural consistency loss comprising the
self-correlation and disturbance correlation consistency loss. It helps align
the spatial structural information between the synthesis image pairs of the
source and target domains. To relax the cross-domain alignment, we compress the
original latent space of generative models to a subspace. Image pairs generated
from the subspace are pulled closer. Qualitative and quantitative experiments
show that our method consistently surpasses the state-of-the-art methods in few
shot setting.",1,1,1,0,1,0,0.814283,6.0,0.818576,42
67e16ee7-2a91-4e72-a293-b5c76c666559,Medical Dialogue Response Generation with Pivotal Information Recalling,6,0.193022,0.447659,"Medical dialogue generation is an important yet challenging task. Most
previous works rely on the attention mechanism and large-scale pretrained
language models. However, these methods often fail to acquire pivotal
information from the long dialogue history to yield an accurate and informative
response, due to the fact that the medical entities usually scatters throughout
multiple utterances along with the complex relationships between them. To
mitigate this problem, we propose a medical response generation model with
Pivotal Information Recalling (MedPIR), which is built on two components, i.e.,
knowledge-aware dialogue graph encoder and recall-enhanced generator. The
knowledge-aware dialogue graph encoder constructs a dialogue graph by
exploiting the knowledge relationships between entities in the utterances, and
encodes it with a graph attention network. Then, the recall-enhanced generator
strengthens the usage of these pivotal information by generating a summary of
the dialogue before producing the actual response. Experimental results on two
large-scale medical dialogue datasets show that MedPIR outperforms the strong
baselines in BLEU scores and medical entities F1 measure.",0,1,0,0,1,0,0.838556,6.0,0.831895,34
9a3e208b-b0d8-4ca6-8983-2a0e0c951e47,Natural Language Deduction through Search over Statement Compositions,37,0.515611,0.592454,"In settings from fact-checking to question answering, we frequently want to
know whether a collection of evidence (premises) entails a hypothesis. Existing
methods primarily focus on the end-to-end discriminative version of this task,
but less work has treated the generative version in which a model searches over
the space of statements entailed by the premises to constructively derive the
hypothesis. We propose a system for doing this kind of deductive reasoning in
natural language by decomposing the task into separate steps coordinated by a
search procedure, producing a tree of intermediate conclusions that faithfully
reflects the system's reasoning process. Our experiments on the EntailmentBank
dataset (Dalvi et al., 2021) demonstrate that the proposed system can
successfully prove true statements while rejecting false ones. Moreover, it
produces natural language explanations with a 17% absolute higher step validity
than those produced by an end-to-end T5 model.",0,0,0,0,0,0,0.883227,4.0,0.788719,38
52dd073f-7873-4beb-b1e5-508d34ebebee,Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing,12,0.0590652,0.39433,"A common recent approach to semantic parsing augments sequence-to-sequence
models by retrieving and appending a set of training samples, called exemplars.
The effectiveness of this recipe is limited by the ability to retrieve
informative exemplars that help produce the correct parse, which is especially
challenging in low-resource settings. Existing retrieval is commonly based on
similarity of query and exemplar inputs. We propose GandR, a retrieval
procedure that retrieves exemplars for which outputs are also similar.
GandRfirst generates a preliminary prediction with input-based retrieval. Then,
it retrieves exemplars with outputs similar to the preliminary prediction which
are used to generate a final prediction. GandR sets the state of the art on
multiple low-resource semantic parsing tasks.",1,1,0,0,1,0,0.690013,4.0,0.637153,19
99bb0c9c-d2cc-4522-8179-1fc2d1042163,TAPE: Task-Agnostic Prior Embedding for Image Restoration,23,0.0763032,0.746813,"Learning a generalized prior for natural image restoration is an important
yet challenging task. Early methods mostly involved handcrafted priors
including normalized sparsity, l_0 gradients, dark channel priors, etc.
Recently, deep neural networks have been used to learn various image priors but
do not guarantee to generalize. In this paper, we propose a novel approach that
embeds a task-agnostic prior into a transformer. Our approach, named
Task-Agnostic Prior Embedding (TAPE), consists of two stages, namely,
task-agnostic pre-training and task-specific fine-tuning, where the first stage
embeds prior knowledge about natural images into the transformer and the second
stage extracts the knowledge to assist downstream image restoration.
Experiments on various types of degradation validate the effectiveness of TAPE.
The image restoration performance in terms of PSNR is improved by as much as
1.45dB and even outperforms task-specific algorithms. More importantly, TAPE
shows the ability of disentangling generalized image priors from degraded
images, which enjoys favorable transfer ability to unknown downstream tasks.",0,0,1,0,1,0,0.553206,5.0,0.634893,92
4d4624d8-ea6e-443d-afa2-9daf45458bb1,DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition,13,0.39635,0.827567,"The task of joint dialog sentiment classification (DSC) and act recognition
(DAR) aims to simultaneously predict the sentiment label and act label for each
utterance in a dialog. In this paper, we put forward a new framework which
models the explicit dependencies via integrating \textit{prediction-level
interactions} other than semantics-level interactions, more consistent with
human intuition. Besides, we propose a speaker-aware temporal graph (SATG) and
a dual-task relational temporal graph (DRTG) to introduce \textit{temporal
relations} into dialog understanding and dual-task reasoning. To implement our
framework, we propose a novel model dubbed DARER, which first generates the
context-, speaker- and temporal-sensitive utterance representations via
modeling SATG, then conducts recurrent dual-task relational reasoning on DRTG,
in which process the estimated label distributions act as key clues in
prediction-level interactions. Experiment results show that DARER outperforms
existing models by large margins while requiring much less computation resource
and costing less training time. Remarkably, on DSC task in Mastodon, DARER
gains a relative improvement of about 25% over previous best model in terms of
F1, with less than 50% parameters and about only 60% required GPU memory.",1,0,1,0,1,0,0.894757,6.0,0.867025,28
7da17f29-1636-461d-aa35-92a0afce16fd,Federated Learning with Heterogeneous Architectures using Graph HyperNetworks,18,0.737563,0.632121,"Standard Federated Learning (FL) techniques are limited to clients with
identical network architectures. This restricts potential use-cases like
cross-platform training or inter-organizational collaboration when both data
privacy and architectural proprietary are required. We propose a new FL
framework that accommodates heterogeneous client architecture by adopting a
graph hypernetwork for parameter sharing. A property of the graph hyper network
is that it can adapt to various computational graphs, thereby allowing
meaningful parameter sharing across models. Unlike existing solutions, our
framework does not limit the clients to share the same architecture type, makes
no use of external data and does not require clients to disclose their model
architecture. Compared with distillation-based and non-graph hypernetwork
baselines, our method performs notably better on standard benchmarks. We
additionally show encouraging generalization performance to unseen
architectures.",0,1,0,0,1,0,0.986486,7.0,0.978589,53
9a8f7ddf-ed0d-4eda-9ded-6b175f40313c,Post-hoc analysis of Arabic transformer models,1,0.00427476,0.0182287,"Arabic is a Semitic language which is widely spoken with many dialects. Given
the success of pre-trained language models, many transformer models trained on
Arabic and its dialects have surfaced. While there have been an extrinsic
evaluation of these models with respect to downstream NLP tasks, no work has
been carried out to analyze and compare their internal representations. We
probe how linguistic information is encoded in the transformer models, trained
on different Arabic dialects. We perform a layer and neuron analysis on the
models using morphological tagging tasks for different dialects of Arabic and a
dialectal identification task. Our analysis enlightens interesting findings
such as: i) word morphology is learned at the lower and middle layers, ii)
while syntactic dependencies are predominantly captured at the higher layers,
iii) despite a large overlap in their vocabulary, the MSA-based models fail to
capture the nuances of Arabic dialects, iv) we found that neurons in embedding
layers are polysemous in nature, while the neurons in middle layers are
exclusive to specific properties",0,0,0,0,0,0,0.0811219,8.0,0.490095,59
36413736-69c9-4a59-92f1-8be7a700973a,DETR++: Taming Your Multi-Scale Detection Transformer,5,0.117343,0.252993,"Convolutional Neural Networks (CNN) have dominated the field of detection
ever since the success of AlexNet in ImageNet classification [12]. With the
sweeping reform of Transformers [27] in natural language processing, Carion et
al. [2] introduce the Transformer-based detection method, i.e., DETR. However,
due to the quadratic complexity in the self-attention mechanism in the
Transformer, DETR is never able to incorporate multi-scale features as
performed in existing CNN-based detectors, leading to inferior results in small
object detection. To mitigate this issue and further improve performance of
DETR, in this work, we investigate different methods to incorporate multi-scale
features and find that a Bi-directional Feature Pyramid (BiFPN) works best with
DETR in further raising the detection precision. With this discovery, we
propose DETR++, a new architecture that improves detection results by 1.9% AP
on MS COCO 2017, 11.5% AP on RICO icon detection, and 9.1% AP on RICO layout
extraction over existing baselines.",0,1,0,0,1,0,0.933684,9.0,0.932082,35
bcbd311b-30dd-4fbe-b14c-c4ba655aaeee,RuCoLA: Russian Corpus of Linguistic Acceptability,13,0.461586,0.738787,"Linguistic acceptability (LA) attracts the attention of the research
community due to its many uses, such as testing the grammatical knowledge of
language models and filtering implausible texts with acceptability classifiers.
However, the application scope of LA in languages other than English is limited
due to the lack of high-quality resources. To this end, we introduce the
Russian Corpus of Linguistic Acceptability (RuCoLA), built from the ground up
under the well-established binary LA approach. RuCoLA consists of $9.8$k
in-domain sentences from linguistic publications and $3.6$k out-of-domain
sentences produced by generative models. The out-of-domain set is created to
facilitate the practical use of acceptability for improving language
generation. Our paper describes the data collection protocol and presents a
fine-grained analysis of acceptability classification experiments with a range
of baseline approaches. In particular, we demonstrate that the most widely used
language models still fall behind humans by a large margin, especially when
detecting morphological and semantic errors. We release RuCoLA, the code of
experiments, and a public leaderboard (rucola-benchmark.com) to assess the
linguistic competence of language models for Russian.",0,1,1,1,0,0,0.659707,7.0,0.780803,98
33690b7a-0b2a-45d2-a9a7-ef85984464a3,Housekeep: Tidying Virtual Households using Commonsense Reasoning,50,0.51953,0.719296,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the
home for embodied AI. In Housekeep, an embodied agent must tidy a house by
rearranging misplaced objects without explicit instructions specifying which
objects need to be rearranged. Instead, the agent must learn from and is
evaluated against human preferences of which objects belong where in a tidy
house. Specifically, we collect a dataset of where humans typically place
objects in tidy and untidy houses constituting 1799 objects, 268 object
categories, 585 placements, and 105 rooms. Next, we propose a modular baseline
approach for Housekeep that integrates planning, exploration, and navigation.
It leverages a fine-tuned large language model (LLM) trained on an internet
text corpus for effective planning. We show that our baseline agent generalizes
to rearranging unseen objects in unknown environments. See our webpage for more
details: https://yashkant.github.io/housekeep/",0,1,1,1,0,0,0.798283,6.0,0.810187,86
10ded5c5-616a-4e77-9c29-f107f3428ad5,MAFNet: A Multi-Attention Fusion Network for RGB-T Crowd Counting,2,0.0355796,0.200516,"RGB-Thermal (RGB-T) crowd counting is a challenging task, which uses thermal
images as complementary information to RGB images to deal with the decreased
performance of unimodal RGB-based methods in scenes with low-illumination or
similar backgrounds. Most existing methods propose well-designed structures for
cross-modal fusion in RGB-T crowd counting. However, these methods have
difficulty in encoding cross-modal contextual semantic information in RGB-T
image pairs. Considering the aforementioned problem, we propose a two-stream
RGB-T crowd counting network called Multi-Attention Fusion Network (MAFNet),
which aims to fully capture long-range contextual information from the RGB and
thermal modalities based on the attention mechanism. Specifically, in the
encoder part, a Multi-Attention Fusion (MAF) module is embedded into different
stages of the two modality-specific branches for cross-modal fusion at the
global level. In addition, a Multi-modal Multi-scale Aggregation (MMA)
regression head is introduced to make full use of the multi-scale and
contextual information across modalities to generate high-quality crowd density
maps. Extensive experiments on two popular datasets show that the proposed
MAFNet is effective for RGB-T crowd counting and achieves the state-of-the-art
performance.",0,1,0,0,1,0,0.648461,7.0,0.776428,64
13d6e38b-7e8e-4f0a-8274-8703a963039e,Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior,10,0.191739,0.237442,"Inverse rendering is an ill-posed problem. Previous work has sought to
resolve this by focussing on priors for object or scene shape or appearance. In
this work, we instead focus on a prior for natural illuminations. Current
methods rely on spherical harmonic lighting or other generic representations
and, at best, a simplistic prior on the parameters. We propose a conditional
neural field representation based on a variational auto-decoder with a SIREN
network and, extending Vector Neurons, build equivariance directly into the
network. Using this, we develop a rotation-equivariant, high dynamic range
(HDR) neural illumination model that is compact and able to express complex,
high-frequency features of natural environment maps. Training our model on a
curated dataset of 1.6K HDR environment maps of natural scenes, we compare it
against traditional representations, demonstrate its applicability for an
inverse rendering task and show environment map completion from partial
observations. A PyTorch implementation, our dataset and trained models can be
found at jadgardner.github.io/RENI.",0,0,0,0,0,0,0.889039,7.0,0.882624,73
3f9aad6b-885a-488c-b106-131dfb098d19,On the Forward Invariance of Neural ODEs,6,0.0208408,0.271577,"We propose a new method to ensure neural ordinary differential equations
(ODEs) satisfy output specifications by using invariance set propagation. Our
approach uses a class of control barrier functions to transform output
specifications into constraints on the parameters and inputs of the learning
system. This setup allows us to achieve output specification guarantees simply
by changing the constrained parameters/inputs both during training and
inference. Moreover, we demonstrate that our invariance set propagation through
data-controlled neural ODEs not only maintains generalization performance but
also creates an additional degree of robustness by enabling causal manipulation
of the system's parameters/inputs. We test our method on a series of
representation learning tasks, including modeling physical dynamics and
convexity portraits, as well as safe collision avoidance for autonomous
vehicles.",1,0,0,0,0,0,0.123909,6.0,0.394628,63
b5d5208b-9e62-42b9-8f9d-e496473c22f9,Algorithms for Weighted Pushdown Automata,3,0.0695329,0.872058,"Weighted pushdown automata (WPDAs) are at the core of many natural language
processing tasks, like syntax-based statistical machine translation and
transition-based dependency parsing. As most existing dynamic programming
algorithms are designed for context-free grammars (CFGs), algorithms for PDAs
often resort to a PDA-to-CFG conversion. In this paper, we develop novel
algorithms that operate directly on WPDAs. Our algorithms are inspired by
Lang's algorithm, but use a more general definition of pushdown automaton and
either reduce the space requirements by a factor of $|\Gamma|$ (the size of the
stack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the
number of states). When run on the same class of PDAs as Lang's algorithm, our
algorithm is both more space-efficient by a factor of $|\Gamma|$ and more
time-efficient by a factor of $|Q| \cdot |\Gamma|$.",1,0,0,0,0,0,0.0404637,37.0,0.870373,37
da36b491-7f4d-4cee-9509-f333f6c79973,Universal Spam Detection using Transfer Learning of BERT Model,25,0.048379,0.350299,"Deep learning transformer models become important by training on text data
based on self-attention mechanisms. This manuscript demonstrated a novel
universal spam detection model using pre-trained Google's Bidirectional Encoder
Representations from Transformers (BERT) base uncased models with four datasets
by efficiently classifying ham or spam emails in real-time scenarios. Different
methods for Enron, Spamassain, Lingspam, and Spamtext message classification
datasets, were used to train models individually in which a single model was
obtained with acceptable performance on four datasets. The Universal Spam
Detection Model (USDM) was trained with four datasets and leveraged
hyperparameters from each model. The combined model was finetuned with the same
hyperparameters from these four models separately. When each model using its
corresponding dataset, an F1-score is at and above 0.9 in individual models. An
overall accuracy reached 97%, with an F1 score of 0.96. Research results and
implications were discussed.",0,1,0,0,0,0,0.0583688,8.0,0.447438,69
0d1164af-4052-4c30-ba31-01bdabb053b7,Learning to Detect Mobile Objects from LiDAR Scans Without Labels,22,0.293008,0.40574,"Current 3D object detectors for autonomous driving are almost entirely
trained on human-annotated data. Although of high quality, the generation of
such data is laborious and costly, restricting them to a few specific locations
and object types. This paper proposes an alternative approach entirely based on
unlabeled data, which can be collected cheaply and in abundance almost
everywhere on earth. Our approach leverages several simple common sense
heuristics to create an initial set of approximate seed labels. For example,
relevant traffic participants are generally not persistent across multiple
traversals of the same route, do not fly, and are never under ground. We
demonstrate that these seed labels are highly effective to bootstrap a
surprisingly accurate detector through repeated self-training without a single
human annotated label.",1,1,0,0,0,0,0.39005,9.0,0.742901,71
7ed23a2c-4e04-4bc9-8335-06e12be4fb4f,Lagrangian Manifold Monte Carlo on Monge Patches,8,0.0935623,0.494666,"The efficiency of Markov Chain Monte Carlo (MCMC) depends on how the
underlying geometry of the problem is taken into account. For distributions
with strongly varying curvature, Riemannian metrics help in efficient
exploration of the target distribution. Unfortunately, they have significant
computational overhead due to e.g. repeated inversion of the metric tensor, and
current geometric MCMC methods using the Fisher information matrix to induce
the manifold are in practice slow. We propose a new alternative Riemannian
metric for MCMC, by embedding the target distribution into a higher-dimensional
Euclidean space as a Monge patch and using the induced metric determined by
direct geometric reasoning. Our metric only requires first-order gradient
information and has fast inverse and determinants, and allows reducing the
computational complexity of individual iterations from cubic to quadratic in
the problem dimensionality. We demonstrate how Lagrangian Monte Carlo in this
metric efficiently explores the target distributions.",1,0,0,0,0,0,0.0218868,19.0,0.714723,35
d127a589-afae-425b-badb-1ba5c5dc275d,EA$^2$E: Improving Consistency with Event Awareness for Document-Level Argument Extraction,9,0.250706,0.771406,"Events are inter-related in documents. Motivated by the
one-sense-per-discourse theory, we hypothesize that a participant tends to play
consistent roles across multiple events in the same document. However recent
work on document-level event argument extraction models each individual event
in isolation and therefore causes inconsistency among extracted arguments
across events, which will further cause discrepancy for downstream applications
such as event knowledge base population, question answering, and hypothesis
generation. In this work, we formulate event argument consistency as the
constraints from event-event relations under the document-level setting. To
improve consistency we introduce the Event-Aware Argument Extraction (EA$^2$E)
model with augmented context for training and inference. Experiment results on
WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA$^2$E
compared to baseline methods.",0,1,0,0,0,0,0.959366,6.0,0.925783,19
abfe7816-bac2-4de7-9ba0-961ccdf5c0ac,Community Question Answering Entity Linking via Leveraging Auxiliary Data,6,0.0916558,0.659144,"Community Question Answering (CQA) platforms contain plenty of CQA texts
(i.e., questions and answers corresponding to the question) where named
entities appear ubiquitously. In this paper, we define a new task of CQA entity
linking (CQAEL) as linking the textual entity mentions detected from CQA texts
with their corresponding entities in a knowledge base. This task can facilitate
many downstream applications including expert finding and knowledge base
enrichment. Traditional entity linking methods mainly focus on linking entities
in news documents, and are suboptimal over this new task of CQAEL since they
cannot effectively leverage various informative auxiliary data involved in the
CQA platform to aid entity linking, such as parallel answers and two types of
meta-data (i.e., topic tags and users). To remedy this crucial issue, we
propose a novel transformer-based framework to effectively harness the
knowledge delivered by different kinds of auxiliary data to promote the linking
performance. We validate the superiority of our framework through extensive
experiments over a newly released CQAEL data set against state-of-the-art
entity linking methods.",0,1,1,0,1,0,0.372209,6.0,0.604337,33
93ef2775-d407-4697-88e1-de7e95a4abf2,SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds,52,0.676647,0.818192,"With the recent availability and affordability of commercial depth sensors
and 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets
have been publicized to facilitate research in 3D computer vision. However,
existing datasets either cover relatively small areas or have limited semantic
annotations. Fine-grained understanding of urban-scale 3D scenes is still in
its infancy. In this paper, we introduce SensatUrban, an urban-scale UAV
photogrammetry point cloud dataset consisting of nearly three billion points
collected from three UK cities, covering 7.6 km^2. Each point in the dataset
has been labelled with fine-grained semantic annotations, resulting in a
dataset that is three times the size of the previous existing largest
photogrammetric point cloud dataset. In addition to the more commonly
encountered categories such as road and vegetation, urban-level categories
including rail, bridge, and river are also included in our dataset. Based on
this dataset, we further build a benchmark to evaluate the performance of
state-of-the-art segmentation algorithms. In particular, we provide a
comprehensive analysis and identify several key challenges limiting urban-scale
point cloud understanding. The dataset is available at
http://point-cloud-analysis.cs.ox.ac.uk.",0,1,1,1,0,0,0.946244,6.0,0.910546,105
df171a4c-e48f-4cf1-8a9b-f67bbda24ce8,Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification,4,0.111262,0.518091,"Semi-supervised learning (SSL) is a common approach to learning predictive
models using not only labeled examples, but also unlabeled examples. While SSL
for the simple tasks of classification and regression has received a lot of
attention from the research community, this is not properly investigated for
complex prediction tasks with structurally dependent variables. This is the
case of multi-label classification and hierarchical multi-label classification
tasks, which may require additional information, possibly coming from the
underlying distribution in the descriptive space provided by unlabeled
examples, to better face the challenging task of predicting simultaneously
multiple class labels.
  In this paper, we investigate this aspect and propose a (hierarchical)
multi-label classification method based on semi-supervised learning of
predictive clustering trees. We also extend the method towards ensemble
learning and propose a method based on the random forest approach. Extensive
experimental evaluation conducted on 23 datasets shows significant advantages
of the proposed method and its extension with respect to their supervised
counterparts. Moreover, the method preserves interpretability and reduces the
time complexity of classical tree-based models.",1,0,0,0,0,0,0.0169755,23.0,0.753179,72
ed6be0e3-f336-4ab6-ac83-56418425f8b1,BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog,26,0.467661,0.897849,"A typical end-to-end task-oriented dialog system transfers context into
dialog state, and upon which generates a response, which usually faces the
problem of error propagation from both previously generated inaccurate dialog
states and responses, especially in low-resource scenarios. To alleviate these
issues, we propose BORT, a back and denoising reconstruction approach for
end-to-end task-oriented dialog system. Squarely, to improve the accuracy of
dialog states, back reconstruction is used to reconstruct the original input
context from the generated dialog states since inaccurate dialog states cannot
recover the corresponding input context. To enhance the denoising capability of
the model to reduce the impact of error propagation, denoising reconstruction
is used to reconstruct the corrupted dialog state and response. Extensive
experiments conducted on MultiWOZ 2.0 and CamRest676 show the effectiveness of
BORT. Furthermore, BORT demonstrates its advanced capabilities in the zero-shot
domain and low-resource scenarios.",0,1,0,0,0,0,0.877164,6.0,0.855171,42
8e88f6da-6b05-407e-a544-41aab514996b,Using Human Perception to Regularize Transfer Learning,6,0.0235408,0.266951,"Recent trends in the machine learning community show that models with
fidelity toward human perceptual measurements perform strongly on vision tasks.
Likewise, human behavioral measurements have been used to regularize model
performance. But can we transfer latent knowledge gained from this across
different learning objectives? In this work, we introduce PERCEP-TL (Perceptual
Transfer Learning), a methodology for improving transfer learning with the
regularization power of psychophysical labels in models. We demonstrate which
models are affected the most by perceptual transfer learning and find that
models with high behavioral fidelity -- including vision transformers --
improve the most from this regularization by as much as 1.9\% Top@1 accuracy
points. These findings suggest that biologically inspired learning agents can
benefit from human behavioral measurements as regularizers and psychophysical
learned representations can be transferred to independent evaluation tasks.",0,0,1,0,0,0,0.233408,8.0,0.633194,82
49c9f5a7-2b80-4c8b-85ac-d973377b8776,Speaker adaptation for Wav2vec2 based dysarthric ASR,16,0.100021,0.622052,"Dysarthric speech recognition has posed major challenges due to lack of
training data and heavy mismatch in speaker characteristics. Recent ASR systems
have benefited from readily available pretrained models such as wav2vec2 to
improve the recognition performance. Speaker adaptation using fMLLR and
xvectors have provided major gains for dysarthric speech with very little
adaptation data. However, integration of wav2vec2 with fMLLR features or
xvectors during wav2vec2 finetuning is yet to be explored. In this work, we
propose a simple adaptation network for fine-tuning wav2vec2 using fMLLR
features. The adaptation network is also flexible to handle other speaker
adaptive features such as xvectors. Experimental analysis show steady
improvements using our proposed approach across all impairment severity levels
and attains 57.72\% WER for high severity in UASpeech dataset. We also
performed experiments on German dataset to substantiate the consistency of our
proposed approach across diverse domains.",0,1,0,0,0,0,0.356692,4.0,0.393038,26
181b3683-7a1e-4170-8bc2-de7d65933b63,Graph Enhanced Contrastive Learning for Radiology Findings Summarization,31,0.390402,0.978534,"The impression section of a radiology report summarizes the most prominent
observation from the findings section and is the most important section for
radiologists to communicate to physicians. Summarizing findings is
time-consuming and can be prone to error for inexperienced radiologists, and
thus automatic impression generation has attracted substantial attention. With
the encoder-decoder framework, most previous studies explore incorporating
extra knowledge (e.g., static pre-defined clinical ontologies or extra
background information). Yet, they encode such knowledge by a separate encoder
to treat it as an extra input to their models, which is limited in leveraging
their relations with the original findings. To address the limitation, we
propose a unified framework for exploiting both extra knowledge and the
original findings in an integrated way so that the critical information (i.e.,
key words and their relations) can be extracted in an appropriate way to
facilitate impression generation. In detail, for each input findings, it is
encoded by a text encoder, and a graph is constructed through its entities and
dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is
adopted to model relation information in the constructed graph. Finally, to
emphasize the key words in the findings, contrastive learning is introduced to
map positive samples (constructed by masking non-key words) closer and push
apart negative ones (constructed by masking key words). The experimental
results on OpenI and MIMIC-CXR confirm the effectiveness of our proposed
method.",1,1,0,0,1,0,0.828297,5.0,0.791401,43
28f121f1-8369-42c5-a500-aa0453dd0791,Automatic Creativity Measurement in Scratch Programs Across Modalities,5,0.0631309,0.27607,"Promoting creativity is considered an important goal of education, but
creativity is notoriously hard to measure.In this paper, we make the journey
fromdefining a formal measure of creativity that is efficientlycomputable to
applying the measure in a practical domain. The measure is general and relies
on coretheoretical concepts in creativity theory, namely fluency, flexibility,
and originality, integratingwith prior cognitive science literature. We adapted
the general measure for projects in the popular visual programming language
Scratch.We designed a machine learning model for predicting the creativity of
Scratch projects, trained and evaluated on human expert creativity assessments
in an extensive user study. Our results show that opinions about creativity in
Scratch varied widely across experts. The automatic creativity assessment
aligned with the assessment of the human experts more than the experts agreed
with each other. This is a first step in providing computational models for
measuring creativity that can be applied to educational technologies, and to
scale up the benefit of creativity education in schools.",0,1,0,0,0,0,0.0612611,13.0,0.6638,56
217d6db8-ecc5-4afc-8053-c966d2fcfe6c,Weakly-supervised segmentation of referring expressions,15,0.25612,0.463169,"Visual grounding localizes regions (boxes or segments) in the image
corresponding to given referring expressions. In this work we address image
segmentation from referring expressions, a problem that has so far only been
addressed in a fully-supervised setting. A fully-supervised setup, however,
requires pixel-wise supervision and is hard to scale given the expense of
manual annotation. We therefore introduce a new task of weakly-supervised image
segmentation from referring expressions and propose Text grounded semantic
SEGgmentation (TSEG) that learns segmentation masks directly from image-level
referring expressions without pixel-level annotations. Our transformer-based
method computes patch-text similarities and guides the classification objective
during training with a new multi-label patch assignment mechanism. The
resulting visual grounding model segments image regions corresponding to given
natural language expressions. Our approach TSEG demonstrates promising results
for weakly-supervised referring expression segmentation on the challenging
PhraseCut and RefCOCO datasets. TSEG also shows competitive performance when
evaluated in a zero-shot setting for semantic segmentation on Pascal VOC.",0,1,1,0,0,0,0.949957,9.0,0.943052,69
11226cc6-ecb7-4b6e-af22-c92049c6fccc,Evaluation Beyond Task Performance: Analyzing Concepts in AlphaZero in Hex,5,0.00519753,0.346767,"AlphaZero, an approach to reinforcement learning that couples neural networks
and Monte Carlo tree search (MCTS), has produced state-of-the-art strategies
for traditional board games like chess, Go, shogi, and Hex. While researchers
and game commentators have suggested that AlphaZero uses concepts that humans
consider important, it is unclear how these concepts are captured in the
network. We investigate AlphaZero's internal representations in the game of Hex
using two evaluation techniques from natural language processing (NLP): model
probing and behavioral tests. In doing so, we introduce new evaluation tools to
the RL community and illustrate how evaluations other than task performance can
be used to provide a more complete picture of a model's strengths and
weaknesses. Our analyses in the game of Hex reveal interesting patterns and
generate some testable hypotheses about how such models learn in general. For
example, we find that MCTS discovers concepts before the neural network learns
to encode them. We also find that concepts related to short-term end-game
planning are best encoded in the final layers of the model, whereas concepts
related to long-term planning are encoded in the middle layers of the model.",1,0,0,0,0,0,0.00984011,9.0,0.308246,86
01039c3a-8003-4619-9079-f55622a3c7c7,Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates,6,0.116586,0.32746,"Calibration strengthens the trustworthiness of black-box models by producing
better accurate confidence estimates on given examples. However, little is
known about if model explanations can help confidence calibration. Intuitively,
humans look at important features attributions and decide whether the model is
trustworthy. Similarly, the explanations can tell us when the model may or may
not know. Inspired by this, we propose a method named CME that leverages model
explanations to make the model less confident with non-inductive attributions.
The idea is that when the model is not highly confident, it is difficult to
identify strong indications of any class, and the tokens accordingly do not
have high attribution scores for any class and vice versa. We conduct extensive
experiments on six datasets with two popular pre-trained language models in the
in-domain and out-of-domain settings. The results show that CME improves
calibration performance in all settings. The expected calibration errors are
further reduced when combined with temperature scaling. Our findings highlight
that model explanations can help calibrate posterior estimates.",1,0,0,0,0,0,0.732641,8.0,0.833445,42
0551bcd2-aa86-4d9d-9370-5d53c5dee033,UC-OWOD: Unknown-Classified Open World Object Detection,33,0.281881,0.879053,"Open World Object Detection (OWOD) is a challenging computer vision problem
that requires detecting unknown objects and gradually learning the identified
unknown classes. However, it cannot distinguish unknown instances as multiple
unknown classes. In this work, we propose a novel OWOD problem called
Unknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to
detect unknown instances and classify them into different unknown classes.
Besides, we formulate the problem and devise a two-stage object detector to
solve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative
classification head are used to detect known and unknown objects. Then,
similarity-based unknown classification and unknown clustering refinement
modules are constructed to distinguish multiple unknown classes. Moreover, two
novel evaluation protocols are designed to evaluate unknown-class detection.
Abundant experiments and visualizations prove the effectiveness of the proposed
method. Code is available at https://github.com/JohnWuzh/UC-OWOD.",1,1,1,0,0,0,0.575384,9.0,0.803971,62
0dc6ee54-5872-4032-984c-86068c2e48f4,"A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets",22,0.355514,0.927264,"In recent years, interest has arisen in using machine learning to improve the
efficiency of automatic medical consultation and enhance patient experience. In
this article, we propose two frameworks to support automatic medical
consultation, namely doctor-patient dialogue understanding and task-oriented
interaction. We create a new large medical dialogue dataset with multi-level
finegrained annotations and establish five independent tasks, including named
entity recognition, dialogue act classification, symptom label inference,
medical report generation and diagnosis-oriented dialogue policy. We report a
set of benchmark results for each task, which shows the usability of the
dataset and sets a baseline for future studies. Both code and data is available
from https://github.com/lemuria-wchen/imcs21.",1,1,1,1,0,0,0.596042,6.0,0.715389,55
98f9ae2e-94f6-4951-a221-ade1c957a69d,"Textual Stylistic Variation: Choices, Genres and Individuals",7,0.0753736,0.202921,"This chapter argues for more informed target metrics for the statistical
processing of stylistic variation in text collections. Much as operationalised
relevance proved a useful goal to strive for in information retrieval, research
in textual stylistics, whether application oriented or philologically inclined,
needs goals formulated in terms of pertinence, relevance, and utility - notions
that agree with reader experience of text. Differences readers are aware of are
mostly based on utility - not on textual characteristics per se. Mostly,
readers report stylistic differences in terms of genres. Genres, while vague
and undefined, are well-established and talked about: very early on, readers
learn to distinguish genres. This chapter discusses variation given by genre,
and contrasts it to variation occasioned by individual choice.",0,0,0,0,0,1,3.7216e-06,54.0,0.738689,27
9ebf677f-a53f-4308-8f88-28a72d30d405,Studying Bias in GANs through the Lens of Race,20,0.0746475,0.978034,"In this work, we study how the performance and evaluation of generative image
models are impacted by the racial composition of their training datasets. By
examining and controlling the racial distributions in various training
datasets, we are able to observe the impacts of different training
distributions on generated image quality and the racial distributions of the
generated images. Our results show that the racial compositions of generated
images successfully preserve that of the training data. However, we observe
that truncation, a technique used to generate higher quality images during
inference, exacerbates racial imbalances in the data. Lastly, when examining
the relationship between image quality and race, we find that the highest
perceived visual quality images of a given race come from a distribution where
that race is well-represented, and that annotators consistently prefer
generated images of white people over those of Black people.",0,0,0,0,0,0,0.603407,8.0,0.789054,53
4e3aa44a-60e0-4ed1-94f7-fa1f50e4be3d,Hierarchical Pronunciation Assessment with Multi-Aspect Attention,9,0.3394,0.448352,"Automatic pronunciation assessment is a major component of a
computer-assisted pronunciation training system. To provide in-depth feedback,
scoring pronunciation at various levels of granularity such as phoneme, word,
and utterance, with diverse aspects such as accuracy, fluency, and
completeness, is essential. However, existing multi-aspect multi-granularity
methods simultaneously predict all aspects at all granularity levels;
therefore, they have difficulty in capturing the linguistic hierarchy of
phoneme, word, and utterance. This limitation further leads to neglecting
intimate cross-aspect relations at the same linguistic unit. In this paper, we
propose a Hierarchical Pronunciation Assessment with Multi-aspect Attention
(HiPAMA) model, which hierarchically represents the granularity levels to
directly capture their linguistic structures and introduces multi-aspect
attention that reflects associations across aspects at the same level to create
more connotative representations. By obtaining relational information from both
the granularity- and aspect-side, HiPAMA can take full advantage of multi-task
learning. Remarkable improvements in the experimental results on the
speachocean762 datasets demonstrate the robustness of HiPAMA, particularly in
the difficult-to-assess aspects.",0,1,0,0,0,0,0.328146,12.0,0.789045,22
19ba0b33-690e-4d0c-8f95-f483bfd64c6d,PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition,21,0.121109,0.717369,"3D Point cloud is becoming a critical data representation in many real-world
applications like autonomous driving, robotics, and medical imaging. Although
the success of deep learning further accelerates the adoption of 3D point
clouds in the physical world, deep learning is notorious for its vulnerability
to adversarial attacks. In this work, we first identify that the
state-of-the-art empirical defense, adversarial training, has a major
limitation in applying to 3D point cloud models due to gradient obfuscation. We
further propose PointDP, a purification strategy that leverages diffusion
models to defend against 3D adversarial attacks. We extensively evaluate
PointDP on six representative 3D point cloud architectures, and leverage 10+
strong and adaptive attacks to demonstrate its lower-bound robustness. Our
evaluation shows that PointDP achieves significantly better robustness than
state-of-the-art purification methods under strong attacks. Results of
certified defenses on randomized smoothing combined with PointDP will be
included in the near future.",0,1,0,0,1,0,0.647747,7.0,0.77615,86
536305f6-58c2-44b0-bf08-c689ea358bb5,Transformer-based Entity Typing in Knowledge Graphs,11,0.146873,0.741669,"We investigate the knowledge graph entity typing task which aims at inferring
plausible entity types. In this paper, we propose a novel Transformer-based
Entity Typing (TET) approach, effectively encoding the content of neighbors of
an entity. More precisely, TET is composed of three different mechanisms: a
local transformer allowing to infer missing types of an entity by independently
encoding the information provided by each of its neighbors; a global
transformer aggregating the information of all neighbors of an entity into a
single long sequence to reason about more complex entity types; and a context
transformer integrating neighbors content based on their contribution to the
type inference through information exchange between neighbor pairs.
Furthermore, TET uses information about class membership of types to
semantically strengthen the representation of an entity. Experiments on two
real-world datasets demonstrate the superior performance of TET compared to the
state-of-the-art.",0,0,0,0,1,0,0.421605,7.0,0.684024,43
1b086712-279f-4ec0-9bea-3e8b60fafa91,Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint,32,0.0823967,0.805876,"Active learning is a promising alternative to alleviate the issue of high
annotation cost in the computer vision tasks by consciously selecting more
informative samples to label. Active learning for object detection is more
challenging and existing efforts on it are relatively rare. In this paper, we
propose a novel hybrid approach to address this problem, where the
instance-level uncertainty and diversity are jointly considered in a bottom-up
manner. To balance the computational complexity, the proposed approach is
designed as a two-stage procedure. At the first stage, an Entropy-based
Non-Maximum Suppression (ENMS) is presented to estimate the uncertainty of
every image, which performs NMS according to the entropy in the feature space
to remove predictions with redundant information gains. At the second stage, a
diverse prototype (DivProto) strategy is explored to ensure the diversity
across images by progressively converting it into the intra-class and
inter-class diversities of the entropy-based class-specific prototypes.
Extensive experiments are conducted on MS COCO and Pascal VOC, and the proposed
approach achieves state of the art results and significantly outperforms the
other counterparts, highlighting its superiority.",1,1,0,0,1,0,0.142728,10.0,0.651976,42
0ed804d5-3224-4a67-b0d1-e6997f2a190c,DirectTracker: 3D Multi-Object Tracking Using Direct Image Alignment and Photometric Bundle Adjustment,5,0.133189,0.262746,"Direct methods have shown excellent performance in the applications of visual
odometry and SLAM. In this work we propose to leverage their effectiveness for
the task of 3D multi-object tracking. To this end, we propose DirectTracker, a
framework that effectively combines direct image alignment for the short-term
tracking and sliding-window photometric bundle adjustment for 3D object
detection. Object proposals are estimated based on the sparse sliding-window
pointcloud and further refined using an optimization-based cost function that
carefully combines 3D and 2D cues to ensure consistency in image and world
space. We propose to evaluate 3D tracking using the recently introduced
higher-order tracking accuracy (HOTA) metric and the generalized intersection
over union similarity measure to mitigate the limitations of the conventional
use of intersection over union for the evaluation of vision-based trackers. We
perform evaluation on the KITTI Tracking benchmark for the Car class and show
competitive performance in tracking objects both in 2D and 3D.",1,1,0,0,0,0,0.934599,9.0,0.932649,43
46f066a7-d068-4038-b92a-51d8efce0a8d,Collaborating Domain-shared and Target-specific Feature Clustering for Cross-domain 3D Action Recognition,7,0.0634683,0.550491,"In this work, we consider the problem of cross-domain 3D action recognition
in the open-set setting, which has been rarely explored before. Specifically,
there is a source domain and a target domain that contain the skeleton
sequences with different styles and categories, and our purpose is to cluster
the target data by utilizing the labeled source data and unlabeled target data.
For such a challenging task, this paper presents a novel approach dubbed CoDT
to collaboratively cluster the domain-shared features and target-specific
features. CoDT consists of two parallel branches. One branch aims to learn
domain-shared features with supervised learning in the source domain, while the
other is to learn target-specific features using contrastive learning in the
target domain. To cluster the features, we propose an online clustering
algorithm that enables simultaneous promotion of robust pseudo label generation
and feature clustering. Furthermore, to leverage the complementarity of
domain-shared features and target-specific features, we propose a novel
collaborative clustering strategy to enforce pair-wise relationship consistency
between the two branches. We conduct extensive experiments on multiple
cross-domain 3D action recognition datasets, and the results demonstrate the
effectiveness of our method.",0,0,1,0,0,0,0.775536,7.0,0.827435,98
9d2396a7-36b9-4c5c-866d-523cbb3b1259,Self-Supervised Moving Vehicle Detection from Audio-Visual Cues,4,0.119892,0.162096,"Robust detection of moving vehicles is a critical task for any autonomously
operating outdoor robot or self-driving vehicle. Most modern approaches for
solving this task rely on training image-based detectors using large-scale
vehicle detection datasets such as nuScenes or the Waymo Open Dataset.
Providing manual annotations is an expensive and laborious exercise that does
not scale well in practice. To tackle this problem, we propose a
self-supervised approach that leverages audio-visual cues to detect moving
vehicles in videos. Our approach employs contrastive learning for localizing
vehicles in images from corresponding pairs of images and recorded audio. In
extensive experiments carried out with a real-world dataset, we demonstrate
that our approach provides accurate detections of moving vehicles and does not
require manual annotations. We furthermore show that our model can be used as a
teacher to supervise an audio-only detection model. This student model is
invariant to illumination changes and thus effectively bridges the domain gap
inherent to models leveraging exclusively vision as the predominant modality.",0,1,0,1,0,0,0.628457,8.0,0.797575,27
4d403386-9589-4939-9a59-3b55a33e6bd7,drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM,3,0.0166295,0.0550841,"This paper describes our system for SemEval-2022 Task 2 Multilingual
Idiomaticity Detection and Sentence Embedding sub-task B. We modify a standard
BERT sentence transformer by adding embeddings for each idioms, which are
created using BERTRAM and a small number of contexts. We show that this
technique increases the quality of idiom representations and leads to better
performance on the task. We also perform analysis on our final results and show
that the quality of the produced idiom embeddings is highly sensitive to the
quality of the input contexts.",1,1,0,0,0,0,0.200011,6.0,0.48178,20
7e2f13de-c604-4a81-a8a3-e957612d9e7f,GMMSeg: Gaussian Mixture based Generative Semantic Segmentation Models,54,0.255507,0.871951,"Prevalent semantic segmentation solutions are, in essence, a dense
discriminative classifier of p(class|pixel feature). Though straightforward,
this de facto paradigm neglects the underlying data distribution p(pixel
feature|class), and struggles to identify out-of-distribution data. Going
beyond this, we propose GMMSeg, a new family of segmentation models that rely
on a dense generative classifier for the joint distribution p(pixel
feature,class). For each class, GMMSeg builds Gaussian Mixture Models (GMMs)
via Expectation-Maximization (EM), so as to capture class-conditional
densities. Meanwhile, the deep dense representation is end-to-end trained in a
discriminative manner, i.e., maximizing p(class|pixel feature). This endows
GMMSeg with the strengths of both generative and discriminative models. With a
variety of segmentation architectures and backbones, GMMSeg outperforms the
discriminative counterparts on three closed-set datasets. More impressively,
without any modification, GMMSeg even performs well on open-world datasets. We
believe this work brings fundamental insights into the related fields.",1,0,0,0,1,0,0.263333,9.0,0.689461,143
6e9e0a66-3877-4e98-b7cd-4b3cb3bd8efd,Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding,17,0.238668,0.349816,"Dialogue understanding tasks often necessitate abundant annotated data to
achieve good performance and that presents challenges in low-resource settings.
To alleviate this barrier, we explore few-shot data augmentation for dialogue
understanding by prompting large pre-trained language models and present a
novel approach that iterates on augmentation quality by applying
weakly-supervised filters. We evaluate our methods on the emotion and act
classification tasks in DailyDialog and the intent classification task in
Facebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our
augmented data mixed with few-shot ground truth data are able to approach or
surpass existing state-of-the-art performance on both datasets. For DailyDialog
specifically, using 10% of the ground truth data we outperform the current
state-of-the-art model which uses 100% of the data.",1,1,0,1,1,0,0.826661,4.0,0.737902,58
2cef28ec-d750-4874-82b5-3cc33957a719,Streaming Radiance Fields for 3D Video Synthesis,33,0.320727,0.592729,"We present an explicit-grid based method for efficiently reconstructing
streaming radiance fields for novel view synthesis of real world dynamic
scenes. Instead of training a single model that combines all the frames, we
formulate the dynamic modeling problem with an incremental learning paradigm in
which per-frame model difference is trained to complement the adaption of a
base model on the current frame. By exploiting the simple yet effective tuning
strategy with narrow bands, the proposed method realizes a feasible framework
for handling video sequences on-the-fly with high training efficiency. The
storage overhead induced by using explicit grid representations can be
significantly reduced through the use of model difference based compression. We
also introduce an efficient strategy to further accelerate model optimization
for each frame. Experiments on challenging video sequences demonstrate that our
approach is capable of achieving a training speed of 15 seconds per-frame with
competitive rendering quality, which attains $1000 \times$ speedup over the
state-of-the-art implicit methods. Code is available at
https://github.com/AlgoHunt/StreamRF.",1,1,0,0,1,0,0.965196,5.0,0.920383,58
215d7f3f-405c-43ee-8225-4903af403158,An Executable Formal Model of the VHDL in Isabelle/HOL,1,0.0248249,0.0884039,"In the hardware design process, hardware components are usually described in
a hardware description language. Most of the hardware description languages,
such as Verilog and VHDL, do not have mathematical foundation and hence are not
fit for formal reasoning about the design. To enable formal reasoning in one of
the most commonly used description language VHDL, we define a formal model of
the VHDL language in Isabelle/HOL. Our model targets the functional part of
VHDL designs used in industry, specifically the design of the LEON3 processor's
integer unit. We cover a wide range of features in the VHDL language that are
usually not modelled in the literature and define a novel operational semantics
for it. Furthermore, our model can be exported to OCaml code for execution,
turning the formal model into a VHDL simulator. We have tested our simulator
against simple designs used in the literature, as well as the div32 module in
the LEON3 design. The Isabelle/HOL code is publicly available:
https://zhehou.github.io/apps/VHDLModel.zip",0,0,0,0,0,0,5.51448e-13,41.0,0.272301,26
493490ea-3632-4101-866e-4afc18439e8b,Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation,13,0.225318,0.489762,"Despite recent advances in appearance-based gaze estimation techniques, the
need for training data that covers the target head pose and gaze distribution
remains a crucial challenge for practical deployment. This work examines a
novel approach for synthesizing gaze estimation training data based on
monocular 3D face reconstruction. Unlike prior works using multi-view
reconstruction, photo-realistic CG models, or generative neural networks, our
approach can manipulate and extend the head pose range of existing training
data without any additional requirements. We introduce a projective matching
procedure to align the reconstructed 3D facial mesh with the camera coordinate
system and synthesize face images with accurate gaze labels. We also propose a
mask-guided gaze estimation model and data augmentation strategies to further
improve the estimation accuracy by taking advantage of synthetic training data.
Experiments using multiple public datasets show that our approach significantly
improves the estimation performance on challenging cross-dataset settings with
non-overlapping gaze distributions.",1,1,0,0,0,0,0.869481,9.0,0.900185,68
6a836fc7-b3a2-4c1f-a6d4-e39acbbadf60,A Unified Framework for Multi-intent Spoken Language Understanding with prompting,1,0.0149523,0.0285613,"Multi-intent Spoken Language Understanding has great potential for widespread
implementation. Jointly modeling Intent Detection and Slot Filling in it
provides a channel to exploit the correlation between intents and slots.
However, current approaches are apt to formulate these two sub-tasks
differently, which leads to two issues: 1) It hinders models from effective
extraction of shared features. 2) Pretty complicated structures are involved to
enhance expression ability while causing damage to the interpretability of
frameworks. In this work, we describe a Prompt-based Spoken Language
Understanding (PromptSLU) framework, to intuitively unify two sub-tasks into
the same form by offering a common pre-trained Seq2Seq model. In detail, ID and
SF are completed by concisely filling the utterance into task-specific prompt
templates as input, and sharing output formats of key-value pairs sequence.
Furthermore, variable intents are predicted first, then naturally embedded into
prompts to guide slot-value pairs inference from a semantic perspective.
Finally, we are inspired by prevalent multi-task learning to introduce an
auxiliary sub-task, which helps to learn relationships among provided labels.
Experiment results show that our framework outperforms several state-of-the-art
baselines on two public datasets.",0,1,0,0,1,0,0.653924,7.0,0.778552,34
98290e9a-1642-4c4f-b10a-714e6cd6edd6,Positive Pair Distillation Considered Harmful: Continual Meta Metric Learning for Lifelong Object Re-Identification,1,0.0330548,0.175464,"Lifelong object re-identification incrementally learns from a stream of
re-identification tasks. The objective is to learn a representation that can be
applied to all tasks and that generalizes to previously unseen
re-identification tasks. The main challenge is that at inference time the
representation must generalize to previously unseen identities. To address this
problem, we apply continual meta metric learning to lifelong object
re-identification. To prevent forgetting of previous tasks, we use knowledge
distillation and explore the roles of positive and negative pairs. Based on our
observation that the distillation and metric losses are antagonistic, we
propose to remove positive pairs from distillation to robustify model updates.
Our method, called Distillation without Positive Pairs (DwoPP), is evaluated on
extensive intra-domain experiments on person and vehicle re-identification
datasets, as well as inter-domain experiments on the LReID benchmark. Our
experiments demonstrate that DwoPP significantly outperforms the
state-of-the-art. The code is here: https://github.com/wangkai930418/DwoPP_code",0,1,1,0,1,0,0.93386,8.0,0.923714,81
73c3914f-aa0c-4199-847b-910d13977b07,Elimination of Non-Novel Segments at Multi-Scale for Few-Shot Segmentation,2,0.0168244,0.0517038,"Few-shot segmentation aims to devise a generalizing model that segments query
images from unseen classes during training with the guidance of a few support
images whose class tally with the class of the query. There exist two
domain-specific problems mentioned in the previous works, namely spatial
inconsistency and bias towards seen classes. Taking the former problem into
account, our method compares the support feature map with the query feature map
at multi scales to become scale-agnostic. As a solution to the latter problem,
a supervised model, called as base learner, is trained on available classes to
accurately identify pixels belonging to seen classes. Hence, subsequent meta
learner has a chance to discard areas belonging to seen classes with the help
of an ensemble learning model that coordinates meta learner with the base
learner. We simultaneously address these two vital problems for the first time
and achieve state-of-the-art performances on both PASCAL-5i and COCO-20i
datasets.",0,1,0,0,1,0,0.65071,7.0,0.777302,30
bb1d6353-0682-49a8-bc3d-bd6ecc831d09,TimeLMs: Diachronic Language Models from Twitter,129,0.686994,0.849355,"Despite its importance, the time variable has been largely neglected in the
NLP and language model literature. In this paper, we present TimeLMs, a set of
language models specialized on diachronic Twitter data. We show that a
continual learning strategy contributes to enhancing Twitter-based language
models' capacity to deal with future and out-of-distribution tweets, while
making them competitive with standardized and more monolithic benchmarks. We
also perform a number of qualitative analyses showing how they cope with trends
and peaks in activity involving specific named entities or concept drift.",1,1,0,0,0,0,0.732178,6.0,0.777708,39
a59ae511-13c2-4f31-80ce-11d7995807e8,AuxMix: Semi-Supervised Learning with Unconstrained Unlabeled Data,7,0.0200597,0.402709,"Semi-supervised learning (SSL) has seen great strides when labeled data is
scarce but unlabeled data is abundant. Critically, most recent work assume that
such unlabeled data is drawn from the same distribution as the labeled data. In
this work, we show that state-of-the-art SSL algorithms suffer a degradation in
performance in the presence of unlabeled auxiliary data that does not
necessarily possess the same class distribution as the labeled set. We term
this problem as Auxiliary-SSL and propose AuxMix, an algorithm that leverages
self-supervised learning tasks to learn generic features in order to mask
auxiliary data that are not semantically similar to the labeled set. We also
propose to regularize learning by maximizing the predicted entropy for
dissimilar auxiliary samples. We show an improvement of 5% over existing
baselines on a ResNet-50 model when trained on CIFAR10 dataset with 4k labeled
samples and all unlabeled data is drawn from the Tiny-ImageNet dataset. We
report competitive results on several datasets and conduct ablation studies.",0,0,1,0,1,0,0.542388,7.0,0.734903,48
6c4b4aa6-0830-4b50-a1f3-a2092ce92ebf,SPE: Symmetrical Prompt Enhancement for Fact Probing,3,0.0221424,0.145904,"Pretrained language models (PLMs) have been shown to accumulate factual
knowledge during pretrainingng (Petroni et al., 2019). Recent works probe PLMs
for the extent of this knowledge through prompts either in discrete or
continuous forms. However, these methods do not consider symmetry of the task:
object prediction and subject prediction. In this work, we propose Symmetrical
Prompt Enhancement (SPE), a continuous prompt-based method for factual probing
in PLMs that leverages the symmetry of the task by constructing symmetrical
prompts for subject and object prediction. Our results on a popular factual
probing dataset, LAMA, show significant improvement of SPE over previous
probing methods.",0,1,0,0,0,0,0.795569,5.0,0.770548,32
c17067e1-8d3d-4d75-911b-e37b9f0edb60,Splatting-based Synthesis for Video Frame Interpolation,18,0.4534,0.620001,"Frame interpolation is an essential video processing technique that adjusts
the temporal resolution of an image sequence. While deep learning has brought
great improvements to the area of video frame interpolation, techniques that
make use of neural networks can typically not easily be deployed in practical
applications like a video editor since they are either computationally too
demanding or fail at high resolutions. In contrast, we propose a deep learning
approach that solely relies on splatting to synthesize interpolated frames.
This splatting-based synthesis for video frame interpolation is not only much
faster than similar approaches, especially for multi-frame interpolation, but
can also yield new state-of-the-art results at high resolutions.",0,1,0,0,1,0,0.897574,5.0,0.842826,72
0afc3560-1a16-47e8-b96a-c552005af16d,Implicit Two-Tower Policies,2,0.104069,0.307883,"We present a new class of structured reinforcement learning
policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are
chosen based on the attention scores of their learnable latent representations
with those of the input states. By explicitly disentangling action from state
processing in the policy stack, we achieve two main goals: substantial
computational gains and better performance. Our architectures are compatible
with both: discrete and continuous action spaces. By conducting tests on 15
environments from OpenAI Gym and DeepMind Control Suite, we show that
ITT-architectures are particularly suited for blackbox/evolutionary
optimization and the corresponding policy training algorithms outperform their
vanilla unstructured implicit counterparts as well as commonly used explicit
policies. We complement our analysis by showing how techniques such as hashing
and lazy tower updates, critically relying on the two-tower structure of ITTs,
can be applied to obtain additional computational improvements.",1,0,0,0,0,1,0.733077,7.0,0.809828,56
f19db7db-0d06-45dc-a428-aefee2bb6362,AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages,27,0.167359,0.512352,"In recent years, multilingual pre-trained language models have gained
prominence due to their remarkable performance on numerous downstream Natural
Language Processing tasks (NLP). However, pre-training these large multilingual
language models requires a lot of training data, which is not available for
African Languages. Active learning is a semi-supervised learning algorithm, in
which a model consistently and dynamically learns to identify the most
beneficial samples to train itself on, in order to achieve better optimization
and performance on downstream tasks. Furthermore, active learning effectively
and practically addresses real-world data scarcity. Despite all its benefits,
active learning, in the context of NLP and especially multilingual language
models pretraining, has received little consideration. In this paper, we
present AfroLM, a multilingual language model pretrained from scratch on 23
African languages (the largest effort to date) using our novel self-active
learning framework. Pretrained on a dataset significantly (14x) smaller than
existing baselines, AfroLM outperforms many multilingual pretrained language
models (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text
classification, and sentiment analysis). Additional out-of-domain sentiment
analysis experiments show that \textbf{AfroLM} is able to generalize well
across various domains. We release the code source, and our datasets used in
our framework at https://github.com/bonaventuredossou/MLM_AL.",1,1,1,1,1,0,0.307968,7.0,0.627318,38
9e237042-f7a8-48e5-98cf-82258e1d9657,Low Complexity Channel estimation with Neural Network Solutions,10,0.0461652,0.542754,"Research on machine learning for channel estimation, especially neural
network solutions for wireless communications, is attracting significant
current interest. This is because conventional methods cannot meet the present
demands of the high speed communication. In the paper, we deploy a general
residual convolutional neural network to achieve channel estimation for the
orthogonal frequency-division multiplexing (OFDM) signals in a downlink
scenario. Our method also deploys a simple interpolation layer to replace the
transposed convolutional layer used in other networks to reduce the computation
cost. The proposed method is more easily adapted to different pilot patterns
and packet sizes. Compared with other deep learning methods for channel
estimation, our results for 3GPP channel models suggest improved mean squared
error performance for our approach.",0,1,0,0,0,0,0.00625092,23.0,0.709507,16
77b8e087-48c6-4c97-be06-dde74b35e212,Translation Word-Level Auto-Completion: What can we achieve out of the box?,5,0.00839552,0.313823,"Research on Machine Translation (MT) has achieved important breakthroughs in
several areas. While there is much more to be done in order to build on this
success, we believe that the language industry needs better ways to take full
advantage of current achievements. Due to a combination of factors, including
time, resources, and skills, businesses tend to apply pragmatism into their AI
workflows. Hence, they concentrate more on outcomes, e.g. delivery, shipping,
releases, and features, and adopt high-level working production solutions,
where possible. Among the features thought to be helpful for translators are
sentence-level and word-level translation auto-suggestion and auto-completion.
Suggesting alternatives can inspire translators and limit their need to refer
to external resources, which hopefully boosts their productivity. This work
describes our submissions to WMT's shared task on word-level auto-completion,
for the Chinese-to-English, English-to-Chinese, German-to-English, and
English-to-German language directions. We investigate the possibility of using
pre-trained models and out-of-the-box features from available libraries. We
employ random sampling to generate diverse alternatives, which reveals good
results. Furthermore, we introduce our open-source API, based on CTranslate2,
to serve translations, auto-suggestions, and auto-completions.",0,1,0,0,0,0,0.10895,10.0,0.623082,14
8704352f-b729-41ed-9649-338189ef6fa7,Multi-Focus Image Fusion based on Gradient Transform,2,0.0490877,0.103278,"Multi-focus image fusion is a challenging field of study that aims to provide
a completely focused image by integrating focused and un-focused pixels. Most
existing methods suffer from shift variance, misregistered images, and
data-dependent. In this study, we introduce a novel gradient information-based
multi-focus image fusion method that is robust for the aforementioned problems.
The proposed method first generates gradient images from original images by
using Halftoning-Inverse Halftoning (H-IH) transform. Then, Energy of Gradient
(EOG) and Standard Deviation functions are used as the focus measurement on the
gradient images to form a fused image. Finally, in order to enhance the fused
image a decision fusion approach is applied with the majority voting method.
The proposed method is compared with 17 different novel and conventional
techniques both visually and objectively. For objective evaluation, 6 different
quantitative metrics are used. It is observed that the proposed method is
promising according to visual evaluation and 83.3% success is achieved by being
first in five out of six metrics according to objective evaluation.",0,1,0,0,1,0,0.0165772,26.0,0.780738,44
835c7818-9820-4795-a32e-0f58e3c51a6e,Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues,8,0.0607013,0.447327,"Movies reflect society and also hold power to transform opinions. Social
biases and stereotypes present in movies can cause extensive damage due to
their reach. These biases are not always found to be the need of storyline but
can creep in as the author's bias. Movie production houses would prefer to
ascertain that the bias present in a script is the story's demand. Today, when
deep learning models can give human-level accuracy in multiple tasks, having an
AI solution to identify the biases present in the script at the writing stage
can help them avoid the inconvenience of stalled release, lawsuits, etc. Since
AI solutions are data intensive and there exists no domain specific data to
address the problem of biases in scripts, we introduce a new dataset of movie
scripts that are annotated for identity bias. The dataset contains dialogue
turns annotated for (i) bias labels for seven categories, viz., gender,
race/ethnicity, religion, age, occupation, LGBTQ, and other, which contains
biases like body shaming, personality bias, etc. (ii) labels for sensitivity,
stereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated
with context awareness, (iv) target groups and reason for bias labels and (v)
expert-driven group-validation process for high quality annotations. We also
report various baseline performances for bias identification and category
detection on our dataset.",0,1,1,1,0,0,0.545347,7.0,0.736084,48
0734d8be-ce4f-4979-ba71-aa96973882fb,"Embodied, Situated, and Grounded Intelligence: Implications for AI",1,0.0188049,0.099763,"In April of 2022, the Santa Fe Institute hosted a workshop on embodied,
situated, and grounded intelligence as part of the Institute's Foundations of
Intelligence project. The workshop brought together computer scientists,
psychologists, philosophers, social scientists, and others to discuss the
science of embodiment and related issues in human intelligence, and its
implications for building robust, human-level AI. In this report, we summarize
each of the talks and the subsequent discussions. We also draw out a number of
key themes and identify important frontiers for future research.",0,0,0,0,0,0,3.90083e-05,67.0,0.82446,15
215d3a98-25b0-43cf-b38d-89a08051c05d,Speech Emotion Recognition using Self-Supervised Features,73,0.608083,0.997567,"Self-supervised pre-trained features have consistently delivered state-of-art
results in the field of natural language processing (NLP); however, their
merits in the field of speech emotion recognition (SER) still need further
investigation. In this paper we introduce a modular End-to- End (E2E) SER
system based on an Upstream + Downstream architecture paradigm, which allows
easy use/integration of a large variety of self-supervised features. Several
SER experiments for predicting categorical emotion classes from the IEMOCAP
dataset are performed. These experiments investigate interactions among
fine-tuning of self-supervised feature models, aggregation of frame-level
features into utterance-level features and back-end classification networks.
The proposed monomodal speechonly based system not only achieves SOTA results,
but also brings light to the possibility of powerful and well finetuned
self-supervised acoustic features that reach results similar to the results
achieved by SOTA multimodal systems using both Speech and Text modalities.",0,1,0,0,1,0,0.882149,4.0,0.787647,26
ed8dccbc-37a7-45f4-904e-56fefccfa8fc,Domain Generalization Strategy to Train Classifiers Robust to Spatial-Temporal Shift,5,0.056032,0.637755,"Deep learning-based weather prediction models have advanced significantly in
recent years. However, data-driven models based on deep learning are difficult
to apply to real-world applications because they are vulnerable to
spatial-temporal shifts. A weather prediction task is especially susceptible to
spatial-temporal shifts when the model is overfitted to locality and
seasonality. In this paper, we propose a training strategy to make the weather
prediction model robust to spatial-temporal shifts. We first analyze the effect
of hyperparameters and augmentations of the existing training strategy on the
spatial-temporal shift robustness of the model. Next, we propose an optimal
combination of hyperparameters and augmentation based on the analysis results
and a test-time augmentation. We performed all experiments on the W4C22
Transfer dataset and achieved the 1st performance.",1,1,0,0,1,0,0.453162,6.0,0.64762,15
315f2e5a-6922-4509-815c-047745c10a75,Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement,20,0.380066,0.982031,"Diabetic retinopathy (DR) and diabetic macular edema (DME) are leading causes
of permanent blindness worldwide. Designing an automatic grading system with
good generalization ability for DR and DME is vital in clinical practice.
However, prior works either grade DR or DME independently, without considering
internal correlations between them, or grade them jointly by shared feature
representation, yet ignoring potential generalization issues caused by
difficult samples and data bias. Aiming to address these problems, we propose a
framework for joint grading with the dynamic difficulty-aware weighted loss
(DAW) and the dual-stream disentangled learning architecture (DETACH). Inspired
by curriculum learning, DAW learns from simple samples to difficult samples
dynamically via measuring difficulty adaptively. DETACH separates features of
grading tasks to avoid potential emphasis on the bias. With the addition of DAW
and DETACH, the model learns robust disentangled feature representations to
explore internal correlations between DR and DME and achieve better grading
performance. Experiments on three benchmarks show the effectiveness and
robustness of our framework under both the intra-dataset and cross-dataset
tests.",0,1,0,0,0,0,0.680348,9.0,0.835783,38
54394357-f0de-4dfa-b54a-921ccc70c6e5,A Novel Channel Identification Architecture for mmWave Systems Based on Eigen Features,1,0.0132165,0.157672,"Millimeter wave (mmWave) communication technique has been developed rapidly
because of many advantages of high speed, large bandwidth, and ultra-low delay.
However, mmWave communications systems suffer from fast fading and frequent
blocking. Hence, the ideal communication environment for mmWave is line of
sight (LOS) channel. To improve the efficiency and capacity of mmWave system,
and to better build the Internet of Everything (IoE) service network, this
paper focuses on the channel identification technique in line-of- sight (LOS)
and non-LOS (NLOS) environments. Considering the limited computing ability of
user equipments (UEs), this paper proposes a novel channel identification
architecture based on eigen features, i.e. eigenmatrix and eigenvector (EMEV)
of channel state information (CSI). Furthermore, this paper explores clustered
delay line (CDL) channel identification with mmWave, which is defined by the
3rd generation partnership project (3GPP). Ther experimental results show that
the EMEV based scheme can achieve identification accuracy of 99.88% assuming
perfect CSI. In the robustness test, the maximum noise can be tolerated is SNR=
16 dB, with the threshold acc \geq 95%. What is more, the novel architecture
based on EMEV feature will reduce the comprehensive overhead by about 90%.",1,1,0,0,0,0,0.104251,8.0,0.523019,20
238737e5-eaf8-4922-a9a8-93f04ba71a6b,Sufficient Statistic Memory Approximate Message Passing,9,0.0456714,0.838186,"Approximate message passing (AMP) type algorithms have been widely used in
the signal reconstruction of certain large random linear systems. A key feature
of the AMP-type algorithms is that their dynamics can be correctly described by
state evolution. However, state evolution does not necessarily guarantee the
convergence of iterative algorithms. To solve the convergence problem of
AMP-type algorithms in principle, this paper proposes a memory AMP (MAMP) under
a sufficient statistic condition, named sufficient statistic MAMP (SS-MAMP). We
show that the covariance matrices of SS-MAMP are L-banded and convergent. Given
an arbitrary MAMP, we can construct the SS-MAMP by damping, which not only
ensures the convergence, but also preserves the orthogonality, i.e., its
dynamics can be correctly described by state evolution.",0,0,1,0,0,0,0.00738594,10.0,0.348609,32
96ec42b6-62c6-4535-8611-189be1cd91ba,A Non-Anatomical Graph Structure for isolated hand gesture separation in continuous gesture sequences,1,0.00646076,0.142023,"Continuous Hand Gesture Recognition (CHGR) has been extensively studied by
researchers in the last few decades. Recently, one model has been presented to
deal with the challenge of the boundary detection of isolated gestures in a
continuous gesture video [17]. To enhance the model performance and also
replace the handcrafted feature extractor in the presented model in [17], we
propose a GCN model and combine it with the stacked Bi-LSTM and Attention
modules to push the temporal information in the video stream. Considering the
breakthroughs of GCN models for skeleton modality, we propose a two-layer GCN
model to empower the 3D hand skeleton features. Finally, the class
probabilities of each isolated gesture are fed to the post-processing module,
borrowed from [17]. Furthermore, we replace the anatomical graph structure with
some non-anatomical graph structures. Due to the lack of a large dataset,
including both the continuous gesture sequences and the corresponding isolated
gestures, three public datasets in Dynamic Hand Gesture Recognition (DHGR),
RKS-PERSIANSIGN, and ASLVID, are used for evaluation. Experimental results show
the superiority of the proposed model in dealing with isolated gesture
boundaries detection in continuous gesture sequences",0,1,0,0,1,0,0.524447,3.0,0.364625,29
fefce975-423d-4540-b3aa-3c586b7b453a,HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing,34,0.182495,0.45056,"Deep learning algorithms are dependent on the availability of large-scale
annotated clinical text datasets. The lack of such publicly available datasets
is the biggest bottleneck for the development of clinical Natural Language
Processing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep
learning models to classify instances from new classes of which no training
data have been seen before. Prompt-based learning is an emerging ZSL technique
where we define task-based templates for NLP tasks. We developed a novel
prompt-based clinical NLP framework called HealthPrompt and applied the
paradigm of prompt-based learning on clinical texts. In this technique, rather
than fine-tuning a Pre-trained Language Model(PLM), the task definitions are
tuned by defining a prompt template. We performed an in-depth analysis of
HealthPrompt on six different PLMs in a no-data setting. Our experiments prove
that prompts effectively capture the context of clinical texts and perform
remarkably well without any training data.",0,0,0,0,0,0,0.80311,6.0,0.812689,39
6f349f28-f3fc-4567-87d1-3b60a2fb4646,EmbryosFormer: Deformable Transformer and Collaborative Encoding-Decoding for Embryos Stage Development Classification,5,0.0290894,0.337315,"The timing of cell divisions in early embryos during the In-Vitro
Fertilization (IVF) process is a key predictor of embryo viability. However,
observing cell divisions in Time-Lapse Monitoring (TLM) is a time-consuming
process and highly depends on experts. In this paper, we propose EmbryosFormer,
a computational model to automatically detect and classify cell divisions from
original time-lapse images. Our proposed network is designed as an
encoder-decoder deformable transformer with collaborative heads. The
transformer contracting path predicts per-image labels and is optimized by a
classification head. The transformer expanding path models the temporal
coherency between embryo images to ensure monotonic non-decreasing constraint
and is optimized by a segmentation head. Both contracting and expanding paths
are synergetically learned by a collaboration head. We have benchmarked our
proposed EmbryosFormer on two datasets: a public dataset with mouse embryos
with 8-cell stage and an in-house dataset with human embryos with 4-cell stage.
Source code: https://github.com/UARK-AICV/Embryos.",0,1,0,1,1,0,0.415745,5.0,0.553916,50
e1cb0862-3c7b-4e0d-98f2-e327e134b1be,GreenKGC: A Lightweight Knowledge Graph Completion Method,8,0.206269,0.529388,"Knowledge graph completion (KGC) aims to discover missing relationships
between entities in knowledge graphs (KGs). Most prior KGC work focuses on
learning embeddings for entities and relations through a simple scoring
function. Yet, a higher-dimensional embedding space is usually required for a
better reasoning capability, which leads to a larger model size and hinders
applicability to real-world problems (e.g., large-scale KGs or mobile/edge
computing). A lightweight modularized KGC solution, called GreenKGC, is
proposed in this work to address this issue. GreenKGC consists of three
modules: representation learning, feature pruning, and decision learning, to
extract discriminant KG features and make accurate predictions on missing
relationships using classifiers and negative sampling. Experimental results
demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in
most datasets. In addition, low-dimensional GreenKGC can achieve competitive or
even better performance against high-dimensional models with a much smaller
model size.",0,1,0,0,1,0,0.898852,8.0,0.902453,51
2bb8657e-f2a1-4cad-9458-1080ba52e6e4,Debugging using Orthogonal Gradient Descent,1,0.0,0.204956,"In this report we consider the following problem: Given a trained model that
is partially faulty, can we correct its behaviour without having to train the
model from scratch? In other words, can we ``debug"" neural networks similar to
how we address bugs in our mathematical models and standard computer code. We
base our approach on the hypothesis that debugging can be treated as a two-task
continual learning problem. In particular, we employ a modified version of a
continual learning algorithm called Orthogonal Gradient Descent (OGD) to
demonstrate, via two simple experiments on the MNIST dataset, that we can
in-fact \textit{unlearn} the undesirable behaviour while retaining the general
performance of the model, and we can additionally \textit{relearn} the
appropriate behaviour, both without having to train the model from scratch.",0,1,0,0,0,0,0.425886,6.0,0.633607,12
fbc07ad2-050e-4b45-a236-3684151258ae,Non-Contrastive Learning Meets Language-Image Pre-Training,8,0.048899,0.288636,"Contrastive language-image pre-training (CLIP) serves as a de-facto standard
to align images and texts. Nonetheless, the loose correlation between images
and texts of web-crawled data renders the contrastive objective data
inefficient and craving for a large training batch size. In this work, we
explore the validity of non-contrastive language-image pre-training (nCLIP),
and study whether nice properties exhibited in visual self-supervised models
can emerge. We empirically observe that the non-contrastive objective nourishes
representation learning while sufficiently underperforming under zero-shot
recognition. Based on the above study, we further introduce xCLIP, a
multi-tasking framework combining CLIP and nCLIP, and show that nCLIP aids CLIP
in enhancing feature semantics. The synergy between two objectives lets xCLIP
enjoy the best of both worlds: superior performance in both zero-shot transfer
and representation learning. Systematic evaluation is conducted spanning a wide
variety of downstream tasks including zero-shot classification, out-of-domain
classification, retrieval, visual representation learning, and textual
representation learning, showcasing a consistent performance gain and
validating the effectiveness of xCLIP.",0,0,0,0,0,0,0.879191,6.0,0.856488,108
294d0db9-38d6-46a9-815a-0109cfa7dded,Geometric Graph Representation Learning via Maximizing Rate Reduction,13,0.0821363,0.487427,"Learning discriminative node representations benefits various downstream
tasks in graph analysis such as community detection and node classification.
Existing graph representation learning methods (e.g., based on random walk and
contrastive learning) are limited to maximizing the local similarity of
connected nodes. Such pair-wise learning schemes could fail to capture the
global distribution of representations, since it has no explicit constraints on
the global geometric properties of representation space. To this end, we
propose Geometric Graph Representation Learning (G2R) to learn node
representations in an unsupervised manner via maximizing rate reduction. In
this way, G2R maps nodes in distinct groups (implicitly stored in the adjacency
matrix) into different subspaces, while each subspace is compact and different
subspaces are dispersedly distributed. G2R adopts a graph neural network as the
encoder and maximizes the rate reduction with the adjacency matrix.
Furthermore, we theoretically and empirically demonstrate that rate reduction
maximization is equivalent to maximizing the principal angles between different
subspaces. Experiments on real-world datasets show that G2R outperforms various
baselines on node classification and community detection tasks.",0,0,0,0,0,0,0.913151,10.0,0.928403,51
8204e5ef-325f-4f15-8785-a534a3fd6c5e,"Video Question Answering: Datasets, Algorithms and Challenges",54,0.488468,0.856297,"Video Question Answering (VideoQA) aims to answer natural language questions
according to the given videos. It has earned increasing attention with recent
research trends in joint vision and language understanding. Yet, compared with
ImageQA, VideoQA is largely underexplored and progresses slowly. Although
different algorithms have continually been proposed and shown success on
different VideoQA datasets, we find that there lacks a meaningful survey to
categorize them, which seriously impedes its advancements. This paper thus
provides a clear taxonomy and comprehensive analyses to VideoQA, focusing on
the datasets, algorithms, and unique challenges. We then point out the research
trend of studying beyond factoid QA to inference QA towards the cognition of
video contents, Finally, we conclude some promising directions for future
exploration.",1,0,0,0,0,0,0.736614,3.0,0.559615,130
73b77607-2e69-42c0-8e70-295a3707bbf1,A General Framework for Modelling Conditional Reasoning -- Preliminary Report,2,0.0344419,0.0295116,"We introduce and investigate here a formalisation for conditionals that
allows the definition of a broad class of reasoning systems. This framework
covers the most popular kinds of conditional reasoning in logic-based KR: the
semantics we propose is appropriate for a structural analysis of those
conditionals that do not satisfy closure properties associated to classical
logics.",0,0,0,0,0,1,1.168e-07,54.0,0.674588,25
9e5c98f5-8b8b-4db2-a15f-4ae9bfc73f2d,Semi-analytical Industrial Cooling System Model for Reinforcement Learning,5,0.172966,0.604499,"We present a hybrid industrial cooling system model that embeds analytical
solutions within a multi-physics simulation. This model is designed for
reinforcement learning (RL) applications and balances simplicity with
simulation fidelity and interpretability. The model's fidelity is evaluated
against real world data from a large scale cooling system. This is followed by
a case study illustrating how the model can be used for RL research. For this,
we develop an industrial task suite that allows specifying different problem
settings and levels of complexity, and use it to evaluate the performance of
different RL algorithms.",0,1,0,0,0,0,0.610653,8.0,0.791522,41
ff3d0c9d-5a2c-4e4a-b2b7-a743994d1ee0,Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,16,0.321203,0.642142,"Augmenting pretrained language models with retrievers has shown promise in
effectively solving common NLP problems, such as language modeling and question
answering. In this paper, we evaluate the strengths and weaknesses of popular
retriever-augmented language models, namely kNN-LM, REALM, DPR + FiD,
Contriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved
statements across different tasks. Our findings indicate that the simple
similarity metric employed by retrievers is insufficient for retrieving all the
necessary statements for reasoning. Additionally, the language models do not
exhibit strong reasoning even when provided with only the required statements.
Furthermore, when combined with imperfect retrievers, the performance of the
language models becomes even worse, e.g., Flan-T5's performance drops by 28.6%
when retrieving 5 statements using Contriever. While larger language models
improve performance, there is still a substantial room for enhancement. Our
further analysis indicates that multihop retrieve-and-read is promising for
large language models like GPT-3.5, but does not generalize to other language
models like Flan-T5-xxl.",1,0,0,0,0,0,0.973683,4.0,0.920467,35
3239d0aa-4acb-46fc-b0f3-54a3e9cad0bb,Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models,5,0.0208017,0.486446,"Structural probing work has found evidence for latent syntactic information
in pre-trained language models. However, much of this analysis has focused on
monolingual models, and analyses of multilingual models have employed
correlational methods that are confounded by the choice of probing tasks. In
this study, we causally probe multilingual language models (XGLM and
multilingual BERT) as well as monolingual BERT-based models across various
languages; we do this by performing counterfactual perturbations on neuron
activations and observing the effect on models' subject-verb agreement
probabilities. We observe where in the model and to what extent syntactic
agreement is encoded in each language. We find significant neuron overlap
across languages in autoregressive multilingual language models, but not masked
language models. We also find two distinct layer-wise effect patterns and two
distinct sets of neurons used for syntactic agreement, depending on whether the
subject and verb are separated by other tokens. Finally, we find that
behavioral analyses of language models are likely underestimating how sensitive
masked language models are to syntactic information.",1,0,0,0,0,0,0.351268,6.0,0.592155,44
700cd16a-d8ac-4199-8123-cf8a90f98ab9,Detecting the Role of an Entity in Harmful Memes: Techniques and Their Limitations,5,0.0726456,0.245924,"Harmful or abusive online content has been increasing over time, raising
concerns for social media platforms, government agencies, and policymakers.
Such harmful or abusive content can have major negative impact on society,
e.g., cyberbullying can lead to suicides, rumors about COVID-19 can cause
vaccine hesitance, promotion of fake cures for COVID-19 can cause health harms
and deaths. The content that is posted and shared online can be textual,
visual, or a combination of both, e.g., in a meme. Here, we describe our
experiments in detecting the roles of the entities (hero, villain, victim) in
harmful memes, which is part of the CONSTRAINT-2022 shared task, as well as our
system for the task. We further provide a comparative analysis of different
experimental settings (i.e., unimodal, multimodal, attention, and
augmentation). For reproducibility, we make our experimental code publicly
available. \url{https://github.com/robi56/harmful_memes_block_fusion}",1,1,0,0,0,0,0.583697,6.0,0.70976,82
36936b4f-e605-4773-9750-c899df47704f,Counterfactual Plans under Distributional Ambiguity,22,0.174109,0.535598,"Counterfactual explanations are attracting significant attention due to the
flourishing applications of machine learning models in consequential domains. A
counterfactual plan consists of multiple possibilities to modify a given
instance so that the model's prediction will be altered. As the predictive
model can be updated subject to the future arrival of new data, a
counterfactual plan may become ineffective or infeasible with respect to the
future values of the model parameters. In this work, we study the
counterfactual plans under model uncertainty, in which the distribution of the
model parameters is partially prescribed using only the first- and
second-moment information. First, we propose an uncertainty quantification tool
to compute the lower and upper bounds of the probability of validity for any
given counterfactual plan. We then provide corrective methods to adjust the
counterfactual plan to improve the validity measure. The numerical experiments
validate our bounds and demonstrate that our correction increases the
robustness of the counterfactual plans in different real-world datasets.",1,0,0,0,0,0,0.419411,7.0,0.683032,40
1f8da53c-0d30-4ad6-be1b-ce42347542bb,Uncertainty-aware Panoptic Segmentation,13,0.620202,0.689514,"Reliable scene understanding is indispensable for modern autonomous systems.
Current learning-based methods typically try to maximize their performance
based on segmentation metrics that only consider the quality of the
segmentation. However, for the safe operation of a system in the real world it
is crucial to consider the uncertainty in the prediction as well. In this work,
we introduce the novel task of uncertainty-aware panoptic segmentation, which
aims to predict per-pixel semantic and instance segmentations, together with
per-pixel uncertainty estimates. We define two novel metrics to facilitate its
quantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the
panoptic Expected Calibration Error (pECE). We further propose the novel
top-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.
Our architecture employs a simple yet effective panoptic fusion module that
leverages the predicted uncertainties. Furthermore, we provide several strong
baselines combining state-of-the-art panoptic segmentation networks with
sampling-free uncertainty estimation techniques. Extensive evaluations show
that our EvPSNet achieves the new state-of-the-art for the standard Panoptic
Quality (PQ), as well as for our uncertainty-aware panoptic metrics. We make
the code available at: \url{https://github.com/kshitij3112/EvPSNet}",1,1,1,0,1,0,0.953671,7.0,0.930413,41
cc40275f-a671-4baf-ba9e-f2a6c2409169,Prompt-based Conservation Learning for Multi-hop Question Answering,4,0.045194,0.101293,"Multi-hop question answering (QA) requires reasoning over multiple documents
to answer a complex question and provide interpretable supporting evidence.
However, providing supporting evidence is not enough to demonstrate that a
model has performed the desired reasoning to reach the correct answer. Most
existing multi-hop QA methods fail to answer a large fraction of sub-questions,
even if their parent questions are answered correctly. In this paper, we
propose the Prompt-based Conservation Learning (PCL) framework for multi-hop
QA, which acquires new knowledge from multi-hop QA tasks while conserving old
knowledge learned on single-hop QA tasks, mitigating forgetting. Specifically,
we first train a model on existing single-hop QA tasks, and then freeze this
model and expand it by allocating additional sub-networks for the multi-hop QA
task. Moreover, to condition pre-trained language models to stimulate the kind
of reasoning required for specific multi-hop questions, we learn soft prompts
for the novel sub-networks to perform type-specific reasoning. Experimental
results on the HotpotQA benchmark show that PCL is competitive for multi-hop QA
and retains good performance on the corresponding single-hop sub-questions,
demonstrating the efficacy of PCL in mitigating knowledge loss by forgetting.",0,1,0,0,0,0,0.77002,6.0,0.795944,34
7456c2af-51b0-45ab-bba8-24407bd30cfa,UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating Explanations in Recommendation,2,0.0492454,0.0271996,"Personalized natural language generation for explainable recommendations
plays a key role in justifying why a recommendation might match a user's
interests. Existing models usually control the generation process by aspect
planning. While promising, these aspect-planning methods struggle to generate
specific information correctly, which prevents generated explanations from
being convincing. In this paper, we claim that introducing lexical constraints
can alleviate the above issues. We propose a model, UCEpic, that generates
high-quality personalized explanations for recommendation results by unifying
aspect planning and lexical constraints in an insertion-based generation
manner.
  Methodologically, to ensure text generation quality and robustness to various
lexical constraints, we pre-train a non-personalized text generator via our
proposed robust insertion process. Then, to obtain personalized explanations
under this framework of insertion-based generation, we design a method of
incorporating aspect planning and personalized references into the insertion
process. Hence, UCEpic unifies aspect planning and lexical constraints into one
framework and generates explanations for recommendations under different
settings. Compared to previous recommendation explanation generators controlled
by only aspects, UCEpic incorporates specific information from keyphrases and
then largely improves the diversity and informativeness of generated
explanations for recommendations on datasets such as RateBeer and Yelp.",1,1,0,0,0,0,0.520601,8.0,0.760373,46
adf5be7a-f21e-433d-8f08-27e10a049be8,Measuring Data,6,0.014657,0.368616,"We identify the task of measuring data to quantitatively characterize the
composition of machine learning data and datasets. Similar to an object's
height, width, and volume, data measurements quantify different attributes of
data along common dimensions that support comparison. Several lines of research
have proposed what we refer to as measurements, with differing terminology; we
bring some of this work together, particularly in fields of computer vision and
language, and build from it to motivate measuring data as a critical component
of responsible AI development. Measuring data aids in systematically building
and analyzing machine learning (ML) data towards specific goals and gaining
better control of what modern ML systems will learn. We conclude with a
discussion of the many avenues of future work, the limitations of data
measurements, and how to leverage these measurement approaches in research and
practice.",0,0,1,0,0,1,0.00206611,13.0,0.400731,107
4b04e23d-a759-4308-9b59-604e1047adfa,Identifying Spurious Correlations and Correcting them with an Explanation-based Learning,11,0.18033,0.504485,"Identifying spurious correlations learned by a trained model is at the core
of refining a trained model and building a trustworthy model. We present a
simple method to identify spurious correlations that have been learned by a
model trained for image classification problems. We apply image-level
perturbations and monitor changes in certainties of predictions made using the
trained model. We demonstrate this approach using an image classification
dataset that contains images with synthetically generated spurious regions and
show that the trained model was overdependent on spurious regions. Moreover, we
remove the learned spurious correlations with an explanation based learning
approach.",0,1,0,0,0,1,0.540268,6.0,0.689732,54
85b0be45-817f-40d4-9164-4217763c0e6e,ITTR: Unpaired Image-to-Image Translation with Transformers,12,0.030164,0.569248,"Unpaired image-to-image translation is to translate an image from a source
domain to a target domain without paired training data. By utilizing CNN in
extracting local semantics, various techniques have been developed to improve
the translation performance. However, CNN-based generators lack the ability to
capture long-range dependency to well exploit global semantics. Recently,
Vision Transformers have been widely investigated for recognition tasks. Though
appealing, it is inappropriate to simply transfer a recognition-based vision
transformer to image-to-image translation due to the generation difficulty and
the computation limitation. In this paper, we propose an effective and
efficient architecture for unpaired Image-to-Image Translation with
Transformers (ITTR). It has two main designs: 1) hybrid perception block (HPB)
for token mixing from different receptive fields to utilize global semantics;
2) dual pruned self-attention (DPSA) to sharply reduce the computational
complexity. Our ITTR outperforms the state-of-the-arts for unpaired
image-to-image translation on six benchmark datasets.",0,1,0,0,1,0,0.68446,5.0,0.706667,57
3007ae09-5025-40df-bcbd-492715a3b192,The SIGMORPHON 2022 Shared Task on Morpheme Segmentation,26,0.141542,0.458899,"The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems
to decompose a word into a sequence of morphemes and covered most types of
morphology: compounds, derivations, and inflections. Subtask 1, word-level
morpheme segmentation, covered 5 million words in 9 languages (Czech, English,
Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13
system submissions from 7 teams and the best system averaged 97.29% F1 score
across all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2,
sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages
(Czech, English, Mongolian), received 10 system submissions from 3 teams, and
the best systems outperformed all three state-of-the-art subword tokenization
methods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis
and support any type of future studies, we released all system predictions, the
evaluation script, and all gold standard datasets.",1,1,0,0,0,0,0.017013,9.0,0.369484,82
fcf526fe-41ad-4a50-ac0f-45bab206e39c,Dual-Pixel Raindrop Removal,1,0.00612822,0.167805,"Removing raindrops in images has been addressed as a significant task for
various computer vision applications. In this paper, we propose the first
method using a Dual-Pixel (DP) sensor to better address the raindrop removal.
Our key observation is that raindrops attached to a glass window yield
noticeable disparities in DP's left-half and right-half images, while almost no
disparity exists for in-focus backgrounds. Therefore, DP disparities can be
utilized for robust raindrop detection. The DP disparities also brings the
advantage that the occluded background regions by raindrops are shifted between
the left-half and the right-half images. Therefore, fusing the information from
the left-half and the right-half images can lead to more accurate background
texture recovery. Based on the above motivation, we propose a DP Raindrop
Removal Network (DPRRN) consisting of DP raindrop detection and DP fused
raindrop removal. To efficiently generate a large amount of training data, we
also propose a novel pipeline to add synthetic raindrops to real-world
background DP images. Experimental results on synthetic and real-world datasets
demonstrate that our DPRRN outperforms existing state-of-the-art methods,
especially showing better robustness to real-world situations. Our source code
and datasets are available at http://www.ok.sc.e.titech.ac.jp/res/SIR/.",1,1,0,0,1,0,0.265616,6.0,0.535876,42
ec598a52-b12d-4681-bcc3-8154cd0fbb41,MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning,72,0.539825,0.914962,"Instruction tuning, a new learning paradigm that fine-tunes pre-trained
language models on tasks specified through instructions, has shown promising
zero-shot performance on various natural language processing tasks. However, it
has yet to be explored for vision and multimodal tasks. In this work, we
introduce MUL-TIINSTRUCT, the first multimodal instruction tuning benchmark
dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq
format covering 10 broad categories. The tasks are derived from 21 existing
open-source datasets and each task is equipped with 5 expert-written
instructions. We take OFA as the base pre-trained model for multimodal
instruction tuning, and to further improve its zero-shot performance, we
explore multiple transfer learning strategies to leverage the large-scale
NATURAL INSTRUCTIONS dataset. Experimental results demonstrate strong zero-shot
performance on various unseen multimodal tasks and the benefit of transfer
learning from a text-only instruction dataset. We also design a new evaluation
metric - Sensitivity, to evaluate how sensitive the model is to the variety of
instructions. Our results indicate that fine-tuning the model on a diverse set
of tasks and instructions leads to a reduced sensitivity to variations in
instructions for each task.",1,0,1,1,0,0,0.952405,4.0,0.876018,66
e4efda48-99f9-4568-95de-f4e52ce7d323,MoViDNN: A Mobile Platform for Evaluating Video Quality Enhancement with Deep Neural Networks,2,0.0240148,0.0857231,"Deep neural network (DNN) based approaches have been intensively studied to
improve video quality thanks to their fast advancement in recent years. These
approaches are designed mainly for desktop devices due to their high
computational cost. However, with the increasing performance of mobile devices
in recent years, it became possible to execute DNN based approaches in mobile
devices. Despite having the required computational power, utilizing DNNs to
improve the video quality for mobile devices is still an active research area.
In this paper, we propose an open-source mobile platform, namely MoViDNN, to
evaluate DNN based video quality enhancement methods, such as super-resolution,
denoising, and deblocking. Our proposed platform can be used to evaluate the
DNN based approaches both objectively and subjectively. For objective
evaluation, we report common metrics such as execution time, PSNR, and SSIM.
For subjective evaluation, Mean Score Opinion (MOS) is reported. The proposed
platform is available publicly at https://github.com/cd-athena/MoViDNN",1,1,0,0,0,0,0.327625,7.0,0.638084,27
cc6dbda7-2d43-4653-ad2e-8b4f97b35214,LiDARCap: Long-range Marker-less 3D Human Motion Capture with LiDAR Point Clouds,20,0.253525,0.933123,"Existing motion capture datasets are largely short-range and cannot yet fit
the need of long-range applications. We propose LiDARHuman26M, a new human
motion capture dataset captured by LiDAR at a much longer range to overcome
this limitation. Our dataset also includes the ground truth human motions
acquired by the IMU system and the synchronous RGB images. We further present a
strong baseline method, LiDARCap, for LiDAR point cloud human motion capture.
Specifically, we first utilize PointNet++ to encode features of points and then
employ the inverse kinematics solver and SMPL optimizer to regress the pose
through aggregating the temporally encoded features hierarchically.
Quantitative and qualitative experiments show that our method outperforms the
techniques based only on RGB images. Ablation experiments demonstrate that our
dataset is challenging and worthy of further research. Finally, the experiments
on the KITTI Dataset and the Waymo Open Dataset show that our method can be
generalized to different LiDAR sensor settings.",0,1,1,1,0,0,0.732781,8.0,0.833495,72
78a2aeda-1c41-46d2-b661-5b1010758d70,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,6,0.060157,0.140372,"This paper investigates models of event implications. Specifically, how well
models predict entity state-changes, by targeting their understanding of
physical attributes. Nominally, Large Language models (LLM) have been exposed
to procedural knowledge about how objects interact, yet our benchmarking shows
they fail to reason about the world. Conversely, we also demonstrate that
existing approaches often misrepresent the surprising abilities of LLMs via
improper task encodings and that proper model prompting can dramatically
improve performance of reported baseline results across multiple tasks. In
particular, our results indicate that our prompting technique is especially
useful for unseen attributes (out-of-domain) or when only limited data is
available.",0,0,0,0,0,0,0.86812,7.0,0.870937,47
13d870b0-fd2f-4a5a-ab04-ddcad234724d,SNaC: Coherence Error Detection for Narrative Summarization,21,0.225332,0.878623,"Progress in summarizing long texts is inhibited by the lack of appropriate
evaluation frameworks. When a long summary must be produced to appropriately
cover the facets of that text, that summary needs to present a coherent
narrative to be understandable by a reader, but current automatic and human
evaluation methods fail to identify gaps in coherence. In this work, we
introduce SNaC, a narrative coherence evaluation framework rooted in
fine-grained annotations for long summaries. We develop a taxonomy of coherence
errors in generated narrative summaries and collect span-level annotations for
6.6k sentences across 150 book and movie screenplay summaries. Our work
provides the first characterization of coherence errors generated by
state-of-the-art summarization models and a protocol for eliciting coherence
judgments from crowd annotators. Furthermore, we show that the collected
annotations allow us to train a strong classifier for automatically localizing
coherence errors in generated summaries as well as benchmarking past work in
coherence modeling. Finally, our SNaC framework can support future work in long
document summarization and coherence evaluation, including improved
summarization modeling and post-hoc summary correction.",1,1,1,1,0,0,0.602895,6.0,0.718506,51
825fc8e2-8b3d-44ae-9bb0-a366e67f33f2,Robust Trajectory Prediction against Adversarial Attacks,16,0.125737,0.790037,"Trajectory prediction using deep neural networks (DNNs) is an essential
component of autonomous driving (AD) systems. However, these methods are
vulnerable to adversarial attacks, leading to serious consequences such as
collisions. In this work, we identify two key ingredients to defend trajectory
prediction models against adversarial attacks including (1) designing effective
adversarial training methods and (2) adding domain-specific data augmentation
to mitigate the performance degradation on clean data. We demonstrate that our
method is able to improve the performance by 46% on adversarial data and at the
cost of only 3% performance degradation on clean data, compared to the model
trained with clean data. Additionally, compared to existing robust methods, our
method can improve performance by 21% on adversarial examples and 9% on clean
data. Our robust model is evaluated with a planner to study its downstream
impacts. We demonstrate that our model can significantly reduce the severe
accident rates (e.g., collisions and off-road driving).",0,1,0,0,1,0,0.802893,7.0,0.839351,40
2163282b-daa3-4f50-9d30-4d3d441b3013,Learning Meta Representations of One-shot Relations for Temporal Knowledge Graph Link Prediction,6,0.138646,0.317203,"Few-shot relational learning for static knowledge graphs (KGs) has drawn
greater interest in recent years, while few-shot learning for temporal
knowledge graphs (TKGs) has hardly been studied. Compared to KGs, TKGs contain
rich temporal information, thus requiring temporal reasoning techniques for
modeling. This poses a greater challenge in learning few-shot relations in the
temporal context. In this paper, we follow the previous work that focuses on
few-shot relational learning on static KGs and extend two fundamental TKG
reasoning tasks, i.e., interpolated and extrapolated link prediction, to the
one-shot setting. We propose four new large-scale benchmark datasets and
develop a TKG reasoning model for learning one-shot relations in TKGs.
Experimental results show that our model can achieve superior performance on
all datasets in both TKG link prediction tasks.",0,0,1,1,1,0,0.865392,6.0,0.847733,46
159503bd-8648-4487-87e4-2ea2f05356bb,Mixture of Input-Output Hidden Markov Models for Heterogeneous Disease Progression Modeling,4,0.0692642,0.561354,"A particular challenge for disease progression modeling is the heterogeneity
of a disease and its manifestations in the patients. Existing approaches often
assume the presence of a single disease progression characteristics which is
unlikely for neurodegenerative disorders such as Parkinson's disease. In this
paper, we propose a hierarchical time-series model that can discover multiple
disease progression dynamics. The proposed model is an extension of an
input-output hidden Markov model that takes into account the clinical
assessments of patients' health status and prescribed medications. We
illustrate the benefits of our model using a synthetically generated dataset
and a real-world longitudinal dataset for Parkinson's disease.",1,0,0,0,0,0,0.0808849,17.0,0.759865,38
05d1c26f-2171-4fd7-8f51-e1a3cbd2d381,Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding,8,0.0210682,0.27441,"Contrastive learning has become a new paradigm for unsupervised sentence
embeddings. Previous studies focus on instance-wise contrastive learning,
attempting to construct positive pairs with textual data augmentation. In this
paper, we propose a novel Contrastive learning method with Prompt-derived
Virtual semantic Prototypes (ConPVP). Specifically, with the help of prompts,
we construct virtual semantic prototypes to each instance, and derive negative
prototypes by using the negative form of the prompts. Using a prototypical
contrastive loss, we enforce the anchor sentence embedding to be close to its
corresponding semantic prototypes, and far apart from the negative prototypes
as well as the prototypes of other sentences. Extensive experimental results on
semantic textual similarity, transfer, and clustering tasks demonstrate the
effectiveness of our proposed model compared to strong baselines. Code is
available at https://github.com/lemon0830/promptCSE.",1,0,0,0,0,0,0.429325,6.0,0.635402,66
bd0e479d-923e-4e65-80dc-c781faf1e8e5,Data Feedback Loops: Model-driven Amplification of Dataset Biases,20,0.0590073,0.649881,"Datasets scraped from the internet have been critical to the successes of
large-scale machine learning. Yet, this very success puts the utility of future
internet-derived datasets at potential risk, as model outputs begin to replace
human annotations as a source of supervision.
  In this work, we first formalize a system where interactions with one model
are recorded as history and scraped as training data in the future. We then
analyze its stability over time by tracking changes to a test-time bias
statistic (e.g. gender bias of model predictions). We find that the degree of
bias amplification is closely linked to whether the model's outputs behave like
samples from the training distribution, a behavior which we characterize and
define as consistent calibration. Experiments in three conditional prediction
scenarios - image classification, visual role-labeling, and language generation
- demonstrate that models that exhibit a sampling-like behavior are more
calibrated and thus more stable. Based on this insight, we propose an
intervention to help calibrate and stabilize unstable feedback systems.
  Code is available at https://github.com/rtaori/data_feedback.",1,0,0,0,0,0,0.30073,6.0,0.560425,89
75e00f30-ae78-40d0-9080-163e52f6c27f,Near Perfect GAN Inversion,18,0.0626512,0.430688,"To edit a real photo using Generative Adversarial Networks (GANs), we need a
GAN inversion algorithm to identify the latent vector that perfectly reproduces
it. Unfortunately, whereas existing inversion algorithms can synthesize images
similar to real photos, they cannot generate the identical clones needed in
most applications. Here, we derive an algorithm that achieves near perfect
reconstructions of photos. Rather than relying on encoder- or
optimization-based methods to find an inverse mapping on a fixed generator
$G(\cdot)$, we derive an approach to locally adjust $G(\cdot)$ to more
optimally represent the photos we wish to synthesize. This is done by locally
tweaking the learned mapping $G(\cdot)$ s.t. $\| {\bf x} - G({\bf z})
\|<\epsilon$, with ${\bf x}$ the photo we wish to reproduce, ${\bf z}$ the
latent vector, $\|\cdot\|$ an appropriate metric, and $\epsilon > 0$ a small
scalar. We show that this approach can not only produce synthetic images that
are indistinguishable from the real photos we wish to replicate, but that these
images are readily editable. We demonstrate the effectiveness of the derived
algorithm on a variety of datasets including human faces, animals, and cars,
and discuss its importance for diversity and inclusion.",0,0,0,0,0,0,0.752968,5.0,0.745155,34
899a63c5-4afc-4e2c-9aa1-fd32efbcc97e,A Novel Self-Knowledge Distillation Approach with Siamese Representation Learning for Action Recognition,7,0.0209832,0.339619,"Knowledge distillation is an effective transfer of knowledge from a heavy
network (teacher) to a small network (student) to boost students' performance.
Self-knowledge distillation, the special case of knowledge distillation, has
been proposed to remove the large teacher network training process while
preserving the student's performance. This paper introduces a novel
Self-knowledge distillation approach via Siamese representation learning, which
minimizes the difference between two representation vectors of the two
different views from a given sample. Our proposed method, SKD-SRL, utilizes
both soft label distillation and the similarity of representation vectors.
Therefore, SKD-SRL can generate more consistent predictions and representations
in various views of the same data point. Our benchmark has been evaluated on
various standard datasets. The experimental results have shown that SKD-SRL
significantly improves the accuracy compared to existing supervised learning
and knowledge distillation methods regardless of the networks.",0,1,0,0,0,0,0.610478,6.0,0.72195,34
1c0c2c7f-0ea6-4b46-8a75-df140fe41cc3,Bending Graphs: Hierarchical Shape Matching using Gated Optimal Transport,16,0.0766498,0.531451,"Shape matching has been a long-studied problem for the computer graphics and
vision community. The objective is to predict a dense correspondence between
meshes that have a certain degree of deformation. Existing methods either
consider the local description of sampled points or discover correspondences
based on global shape information. In this work, we investigate a hierarchical
learning design, to which we incorporate local patch-level information and
global shape-level structures. This flexible representation enables
correspondence prediction and provides rich features for the matching stage.
Finally, we propose a novel optimal transport solver by recurrently updating
features on non-confident nodes to learn globally consistent correspondences
between the shapes. Our results on publicly available datasets suggest robust
performance in presence of severe deformations without the need for extensive
training or refinement.",0,0,0,0,0,0,0.260745,8.0,0.649201,79
98b80260-9111-4574-abb8-d01a3d4d1446,Incorporating Causal Analysis into Diversified and Logical Response Generation,2,0.0208492,0.246968,"Although the Conditional Variational AutoEncoder (CVAE) model can generate
more diversified responses than the traditional Seq2Seq model, the responses
often have low relevance with the input words or are illogical with the
question. A causal analysis is carried out to study the reasons behind, and a
methodology of searching for the mediators and mitigating the confounding bias
in dialogues is provided. Specifically, we propose to predict the mediators to
preserve relevant information and auto-regressively incorporate the mediators
into generating process. Besides, a dynamic topic graph guided conditional
variational autoencoder (TGG-CVAE) model is utilized to complement the semantic
space and reduce the confounding bias in responses. Extensive experiments
demonstrate that the proposed model is able to generate both relevant and
informative responses, and outperforms the state-of-the-art in terms of
automatic metrics and human evaluations.",0,0,0,0,1,0,0.141308,9.0,0.612106,51
615d42f7-d108-4ad1-841e-8c4b3c612147,Semantic Image Synthesis via Diffusion Models,105,0.590618,0.905988,"Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable
success in various image generation tasks compared with Generative Adversarial
Nets (GANs). Recent work on semantic image synthesis mainly follows the
\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality
or diversity of generated images. In this paper, we propose a novel framework
based on DDPM for semantic image synthesis. Unlike previous conditional
diffusion model directly feeds the semantic layout and noisy image as input to
a U-Net structure, which may not fully leverage the information in the input
semantic mask, our framework processes semantic layout and noisy image
differently. It feeds noisy image to the encoder of the U-Net structure while
the semantic layout to the decoder by multi-layer spatially-adaptive
normalization operators. To further improve the generation quality and semantic
interpretability in semantic image synthesis, we introduce the classifier-free
guidance sampling strategy, which acknowledge the scores of an unconditional
model for sampling process. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our proposed method, achieving
state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS).",1,0,0,0,1,0,0.833354,5.0,0.794766,62
ff4ad3b0-30a0-4656-aef8-1144345ff616,Data-driven Feature Tracking for Event Cameras,12,0.175239,0.957945,"Because of their high temporal resolution, increased resilience to motion
blur, and very sparse output, event cameras have been shown to be ideal for
low-latency and low-bandwidth feature tracking, even in challenging scenarios.
Existing feature tracking methods for event cameras are either handcrafted or
derived from first principles but require extensive parameter tuning, are
sensitive to noise, and do not generalize to different scenarios due to
unmodeled effects. To tackle these deficiencies, we introduce the first
data-driven feature tracker for event cameras, which leverages low-latency
events to track features detected in a grayscale frame. We achieve robust
performance via a novel frame attention module, which shares information across
feature tracks. By directly transferring zero-shot from synthetic to real data,
our data-driven tracker outperforms existing approaches in relative feature age
by up to 120% while also achieving the lowest latency. This performance gap is
further increased to 130% by adapting our tracker to real data with a novel
self-supervision strategy.",1,1,0,0,1,0,0.256045,8.0,0.646551,53
6cc24dee-b86c-4cba-b72b-a8a37a6de82b,MoEC: Mixture of Expert Clusters,5,0.267634,0.610312,"Sparsely Mixture of Experts (MoE) has received great interest due to its
promising scaling capability with affordable computational overhead. MoE
converts dense layers into sparse experts, and utilizes a gated routing network
to make experts conditionally activated. However, as the number of experts
grows, MoE with outrageous parameters suffers from overfitting and sparse data
allocation. Such problems are especially severe on tasks with limited data,
thus hindering the progress for MoE models to improve performance by scaling
up. In this work, we propose Mixture of Expert Clusters - a general approach to
enable expert layers to learn more diverse and appropriate knowledge by
imposing variance-based constraints on the routing stage. We further propose a
cluster-level expert dropout strategy specifically designed for the expert
cluster structure. Our experiments reveal that MoEC could improve performance
on machine translation and natural language understanding tasks, and raise the
performance upper bound for scaling up experts under limited data. We also
verify that MoEC plays a positive role in mitigating overfitting and sparse
data allocation.",0,1,0,0,0,1,0.943107,5.0,0.888736,32
03e87339-1b9f-4a16-8078-c55a1d9cbc54,Exploiting Global and Local Hierarchies for Hierarchical Text Classification,14,0.159497,0.306679,"Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as a structured
labels hierarchy corresponding to each text sample. It is dynamic and relevant
to text samples, which is ignored in previous methods. To exploit global and
local hierarchies,we propose Hierarchy-guided BERT with Global and Local
hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local
hierarchies.Moreover,HBGL avoids the intentional fusion of semantic and
hierarchical modules by directly modeling semantic and hierarchical information
with BERT.Compared with the state-of-the-art method HGCLR,our method achieves
significant improvement on three benchmark datasets.",1,1,0,0,1,0,0.513187,6.0,0.676981,18
24749b87-12b5-4693-ade3-4380b3cb55f6,Diffeomorphic Counterfactuals with Generative Models,6,0.0535534,0.510276,"Counterfactuals can explain classification decisions of neural networks in a
human interpretable way. We propose a simple but effective method to generate
such counterfactuals. More specifically, we perform a suitable diffeomorphic
coordinate transformation and then perform gradient ascent in these coordinates
to find counterfactuals which are classified with great confidence as a
specified target class. We propose two methods to leverage generative models to
construct such suitable coordinate systems that are either exactly or
approximately diffeomorphic. We analyze the generation process theoretically
using Riemannian differential geometry and validate the quality of the
generated counterfactuals using various qualitative and quantitative measures.",1,0,0,0,0,0,0.55818,9.0,0.798696,119
213328de-6f93-4994-aae1-f5f2179e4e4f,Perturbation Augmentation for Fairer NLP,35,0.170846,0.892518,"Unwanted and often harmful social biases are becoming ever more salient in
NLP research, affecting both models and datasets. In this work, we ask whether
training on demographically perturbed data leads to fairer language models. We
collect a large dataset of human annotated text perturbations and train a
neural perturbation model, which we show outperforms heuristic alternatives. We
find that (i) language models (LMs) pre-trained on demographically perturbed
corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE
datasets exhibit less demographic bias on downstream tasks, and (iii) fairness
improvements do not come at the expense of performance on downstream tasks.
Lastly, we discuss outstanding questions about how best to evaluate the
(un)fairness of large language models. We hope that this exploration of neural
demographic perturbation will help drive more improvement towards fairer NLP.",0,1,1,1,0,0,0.306356,6.0,0.564147,145
10c6600d-c901-4b4a-b183-5bf25b75646c,CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,94,0.717415,0.933969,"Constructing benchmarks that test the abilities of modern natural language
understanding models is difficult - pre-trained language models exploit
artifacts in benchmarks to achieve human parity, but still fail on adversarial
examples and make errors that demonstrate a lack of common sense. In this work,
we propose gamification as a framework for data construction. The goal of
players in the game is to compose questions that mislead a rival AI while using
specific phrases for extra points. The game environment leads to enhanced user
engagement and simultaneously gives the game designer control over the
collected data, allowing us to collect high-quality data at scale. Using our
method we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and
demonstrate its difficulty for models that are orders-of-magnitude larger than
the AI used in the game itself. Our best baseline, the T5-based Unicorn with
11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3
(52.9%) in a few-shot inference setup. Both score well below human performance
which is at 94.1%.",1,0,1,1,1,1,0.943573,5.0,0.889309,64
1055cc29-a8d8-4ef5-8d44-0fc13a887230,PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation,9,0.157191,0.824345,"In this paper, we propose a new deep learning-based method for estimating
room layout given a pair of 360 panoramas. Our system, called Position-aware
Stereo Merging Network or PSMNet, is an end-to-end joint layout-pose estimator.
PSMNet consists of a Stereo Pano Pose (SP2) transformer and a novel
Cross-Perspective Projection (CP2) layer. The stereo-view SP2 transformer is
used to implicitly infer correspondences between views, and can handle noisy
poses. The pose-aware CP2 layer is designed to render features from the
adjacent view to the anchor (reference) view, in order to perform view fusion
and estimate the visible layout. Our experiments and analysis validate our
method, which significantly outperforms the state-of-the-art layout estimators,
especially for large and complex room spaces.",0,1,0,0,1,0,0.403423,7.0,0.67571,45
99dfb34d-4e0a-4913-872e-4139d8acbd27,Fusing Convolutional Neural Network and Geometric Constraint for Image-based Indoor Localization,12,0.087012,0.82384,"This paper proposes a new image-based localization framework that explicitly
localizes the camera/robot by fusing Convolutional Neural Network (CNN) and
sequential images' geometric constraints. The camera is localized using a
single or few observed images and training images with 6-degree-of-freedom pose
labels. A Siamese network structure is adopted to train an image descriptor
network, and the visually similar candidate image in the training set is
retrieved to localize the testing image geometrically. Meanwhile, a
probabilistic motion model predicts the pose based on a constant velocity
assumption. The two estimated poses are finally fused using their uncertainties
to yield an accurate pose prediction. This method leverages the geometric
uncertainty and is applicable in indoor scenarios predominated by diffuse
illumination. Experiments on simulation and real data sets demonstrate the
efficiency of our proposed method. The results further show that combining the
CNN-based framework with geometric constraint achieves better accuracy when
compared with CNN-only methods, especially when the training data size is
small.",0,1,0,0,0,0,0.141343,13.0,0.731479,42
35a8d24e-125a-41f8-a641-96c0c8e09077,StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes,6,0.0226987,0.274136,"Analyzing ethnic or religious bias is important for improving fairness,
accountability, and transparency of natural language processing models.
However, many techniques rely on human-compiled lists of bias terms, which are
expensive to create and are limited in coverage. In this study, we present a
fully data-driven pipeline for generating a knowledge graph (KG) of cultural
knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5
nationalities and can easily be extended to include more entities. Our human
evaluation shows that the majority (59.2%) of non-singleton entries are
coherent and complete stereotypes. We further show that performing intermediate
masked language model training on the verbalized KG leads to a higher level of
cultural awareness in the model and has the potential to increase
classification performance on knowledge-crucial samples on a related task,
i.e., hate speech detection.",1,1,0,1,0,0,0.23131,7.0,0.579316,47
c4d734b8-da77-4bbb-a637-5b5223200a12,Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings with Multivariate Occupancy Time Series,5,0.0715324,0.607655,"In this work, we address the problem of unsupervised moving object
segmentation (MOS) in 4D LiDAR data recorded from a stationary sensor, where no
ground truth annotations are involved. Deep learning-based state-of-the-art
methods for LiDAR MOS strongly depend on annotated ground truth data, which is
expensive to obtain and scarce in existence. To close this gap in the
stationary setting, we propose a novel 4D LiDAR representation based on
multivariate time series that relaxes the problem of unsupervised MOS to a time
series clustering problem. More specifically, we propose modeling the change in
occupancy of a voxel by a multivariate occupancy time series (MOTS), which
captures spatio-temporal occupancy changes on the voxel level and its
surrounding neighborhood. To perform unsupervised MOS, we train a neural
network in a self-supervised manner to encode MOTS into voxel-level feature
representations, which can be partitioned by a clustering algorithm into moving
or stationary. Experiments on stationary scenes from the Raw KITTI dataset show
that our fully unsupervised approach achieves performance that is comparable to
that of supervised state-of-the-art approaches.",1,1,0,0,1,0,0.491741,6.0,0.666688,38
2ba62675-df7c-4b98-b3f5-a3c8c784871d,Do Bayesian Neural Networks Need To Be Fully Stochastic?,25,0.220667,0.811019,"We investigate the benefit of treating all the parameters in a Bayesian
neural network stochastically and find compelling theoretical and empirical
evidence that this standard construction may be unnecessary. To this end, we
prove that expressive predictive distributions require only small amounts of
stochasticity. In particular, partially stochastic networks with only $n$
stochastic biases are universal probabilistic predictors for $n$-dimensional
predictive problems. In empirical investigations, we find no systematic benefit
of full stochasticity across four different inference modalities and eight
datasets; partially stochastic networks can match and sometimes even outperform
fully stochastic networks, despite their reduced memory costs.",0,0,0,0,0,0,0.430451,7.0,0.68799,69
2ed319ae-706f-4f1e-8396-ce17bbcf9bb2,ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries,44,0.75026,0.93915,"Perception and prediction are two separate modules in the existing autonomous
driving systems. They interact with each other via hand-picked features such as
agent bounding boxes and trajectories. Due to this separation, prediction, as a
downstream module, only receives limited information from the perception
module. To make matters worse, errors from the perception modules can propagate
and accumulate, adversely affecting the prediction results. In this work, we
propose ViP3D, a query-based visual trajectory prediction pipeline that
exploits rich information from raw videos to directly predict future
trajectories of agents in a scene. ViP3D employs sparse agent queries to
detect, track, and predict throughout the pipeline, making it the first fully
differentiable vision-based trajectory prediction approach. Instead of using
historical feature maps and trajectories, useful information from previous
timestamps is encoded in agent queries, which makes ViP3D a concise streaming
prediction method. Furthermore, extensive experimental results on the nuScenes
dataset show the strong vision-based prediction performance of ViP3D over
traditional pipelines and previous end-to-end models.",1,1,0,0,1,0,0.960092,5.0,0.912062,72
ca8af5e7-c866-41c6-85d2-a58a0d1385ae,"""Think Before You Speak"": Improving Multi-Action Dialog Policy by Planning Single-Action Dialogs",7,0.096714,0.635567,"Multi-action dialog policy (MADP), which generates multiple atomic dialog
actions per turn, has been widely applied in task-oriented dialog systems to
provide expressive and efficient system responses. Existing MADP models usually
imitate action combinations from the labeled multi-action dialog samples. Due
to data limitations, they generalize poorly toward unseen dialog flows. While
interactive learning and reinforcement learning algorithms can be applied to
incorporate external data sources of real users and user simulators, they take
significant manual effort to build and suffer from instability. To address
these issues, we propose Planning Enhanced Dialog Policy (PEDP), a novel
multi-task learning framework that learns single-action dialog dynamics to
enhance multi-action prediction. Our PEDP method employs model-based planning
for conceiving what to express before deciding the current response through
simulating single-action dialogs. Experimental results on the MultiWOZ dataset
demonstrate that our fully supervised learning-based method achieves a solid
task success rate of 90.6%, improving 3% compared to the state-of-the-art
methods.",1,1,0,0,1,0,0.286999,6.0,0.551109,26
1387f247-daad-46ef-ba90-7cadda0da83c,Givens Coordinate Descent Methods for Rotation Matrix Learning in Trainable Embedding Indexes,3,0.032749,0.126786,"Product quantization (PQ) coupled with a space rotation, is widely used in
modern approximate nearest neighbor (ANN) search systems to significantly
compress the disk storage for embeddings and speed up the inner product
computation. Existing rotation learning methods, however, minimize quantization
distortion for fixed embeddings, which are not applicable to an end-to-end
training scenario where embeddings are updated constantly. In this paper, based
on geometric intuitions from Lie group theory, in particular the special
orthogonal group $SO(n)$, we propose a family of block Givens coordinate
descent algorithms to learn rotation matrix that are provably convergent on any
convex objectives. Compared to the state-of-the-art SVD method, the Givens
algorithms are much more parallelizable, reducing runtime by orders of
magnitude on modern GPUs, and converge more stably according to experimental
studies. They further improve upon vanilla product quantization significantly
in an end-to-end training scenario.",0,1,0,0,0,0,0.227783,13.0,0.772128,47
7f509986-dcd2-421a-988a-7da85697ebc1,MetaASSIST: Robust Dialogue State Tracking with Meta Learning,4,0.140529,0.22356,"Existing dialogue datasets contain lots of noise in their state annotations.
Such noise can hurt model training and ultimately lead to poor generalization
performance. A general framework named ASSIST has recently been proposed to
train robust dialogue state tracking (DST) models. It introduces an auxiliary
model to generate pseudo labels for the noisy training set. These pseudo labels
are combined with vanilla labels by a common fixed weighting parameter to train
the primary DST model. Notwithstanding the improvements of ASSIST on DST,
tuning the weighting parameter is challenging. Moreover, a single parameter
shared by all slots and all instances may be suboptimal. To overcome these
limitations, we propose a meta learning-based framework MetaASSIST to
adaptively learn the weighting parameter. Specifically, we propose three
schemes with varying degrees of flexibility, ranging from slot-wise to both
slot-wise and instance-wise, to convert the weighting parameter into learnable
functions. These functions are trained in a meta-learning manner by taking the
validation set as meta data. Experimental results demonstrate that all three
schemes can achieve competitive performance. Most impressively, we achieve a
state-of-the-art joint goal accuracy of 80.10% on MultiWOZ 2.4.",1,1,0,0,1,0,0.637356,6.0,0.734132,39
4122cc56-2b20-406f-9128-8b1d8643ab0c,Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems,15,0.0735545,0.80854,"Do all instances need inference through the big models for a correct
prediction? Perhaps not; some instances are easy and can be answered correctly
by even small capacity models. This provides opportunities for improving the
computational efficiency of systems. In this work, we present an explorative
study on 'model cascading', a simple technique that utilizes a collection of
models of varying capacities to accurately yet efficiently output predictions.
Through comprehensive experiments in multiple task settings that differ in the
number of models available for cascading (K value), we show that cascading
improves both the computational efficiency and the prediction accuracy. For
instance, in K=3 setting, cascading saves up to 88.93% computation cost and
consistently achieves superior prediction accuracy with an improvement of up to
2.18%. We also study the impact of introducing additional models in the cascade
and show that it further increases the efficiency improvements. Finally, we
hope that our work will facilitate development of efficient NLP systems making
their widespread adoption in real-world applications possible.",0,1,0,0,0,0,0.479554,6.0,0.660748,59
a9fc0d61-294c-4469-86a3-a2865f4904cb,GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost,10,0.24769,0.56207,"Large pre-trained models have revolutionized natural language processing
(NLP) research and applications, but high training costs and limited data
resources have prevented their benefits from being shared equally amongst
speakers of all the world's languages. To address issues of cross-linguistic
access to such models and reduce energy consumption for sustainability during
large-scale model training, this study proposes an effective and
energy-efficient framework called GreenPLM that uses bilingual lexicons to
directly ""translate"" pre-trained language models of one language into another
at almost no additional cost. We validate this approach in 18 languages' BERT
models and show that this framework is comparable to, if not better than, other
heuristics with high training costs. In addition, given lightweight continued
pre-training on limited data where available, this framework outperforms the
original monolingual language models in six out of seven tested languages with
up to 200x less pre-training efforts. Aiming at the Leave No One Behind
Principle (LNOB), our approach manages to reduce inequalities between languages
and energy consumption greatly. We make our codes and models publicly available
here: \url{https://github.com/qcznlp/GreenPLMs}",1,1,0,0,1,0,0.898125,6.0,0.869416,69
251f156a-2106-4b7a-a722-ea0ba61e86ca,Adaptive Local-Component-aware Graph Convolutional Network for One-shot Skeleton-based Action Recognition,8,0.0909582,0.721547,"Skeleton-based action recognition receives increasing attention because the
skeleton representations reduce the amount of training data by eliminating
visual information irrelevant to actions. To further improve the sample
efficiency, meta-learning-based one-shot learning solutions were developed for
skeleton-based action recognition. These methods find the nearest neighbor
according to the similarity between instance-level global average embedding.
However, such measurement holds unstable representativity due to inadequate
generalized learning on local invariant and noisy features, while intuitively,
more fine-grained recognition usually relies on determining key local body
movements. To address this limitation, we present the Adaptive
Local-Component-aware Graph Convolutional Network, which replaces the
comparison metric with a focused sum of similarity measurements on aligned
local embedding of action-critical spatial/temporal segments. Comprehensive
one-shot experiments on the public benchmark of NTU-RGB+D 120 indicate that our
method provides a stronger representation than the global embedding and helps
our model reach state-of-the-art.",0,1,0,0,1,0,0.444008,8.0,0.732229,42
3bc98672-f6e1-4502-92ef-50d001b8c2d7,Consent as a Foundation for Responsible Autonomy,6,0.0279579,0.692958,"This paper focuses on a dynamic aspect of responsible autonomy, namely, to
make intelligent agents be responsible at run time. That is, it considers
settings where decision making by agents impinges upon the outcomes perceived
by other agents. For an agent to act responsibly, it must accommodate the
desires and other attitudes of its users and, through other agents, of their
users.
  The contribution of this paper is twofold. First, it provides a conceptual
analysis of consent, its benefits and misuses, and how understanding consent
can help achieve responsible autonomy. Second, it outlines challenges for AI
(in particular, for agents and multiagent systems) that merit investigation to
form as a basis for modeling consent in multiagent systems and applying consent
to achieve responsible autonomy.",0,0,0,0,0,0,0.00259802,13.0,0.418374,53
2a76fdc4-c303-4302-9f35-f7acf9329249,Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN,8,0.205554,0.35693,"This paper presents a new deformable convolution-based video frame
interpolation (VFI) method, using a coarse to fine 3D CNN to enhance the
multi-flow prediction. This model first extracts spatio-temporal features at
multiple scales using a 3D CNN, and estimates multi-flows using these features
in a coarse-to-fine manner. The estimated multi-flows are then used to warp the
original input frames as well as context maps, and the warped results are fused
by a synthesis network to produce the final output. This VFI approach has been
fully evaluated against 12 state-of-the-art VFI methods on three commonly used
test databases. The results evidently show the effectiveness of the proposed
method, which offers superior interpolation performance over other state of the
art algorithms, with PSNR gains up to 0.19dB.",1,1,0,0,1,0,0.933946,9.0,0.932244,31
647ddd4f-bc5e-403e-a916-e8cb2a72a15e,Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning,9,0.0918609,0.692917,"Synchronizing decisions across multiple agents in realistic settings is
problematic since it requires agents to wait for other agents to terminate and
communicate about termination reliably. Ideally, agents should learn and
execute asynchronously instead. Such asynchronous methods also allow temporally
extended actions that can take different amounts of time based on the situation
and action executed. Unfortunately, current policy gradient methods are not
applicable in asynchronous settings, as they assume that agents synchronously
reason about action selection at every time step. To allow asynchronous
learning and decision-making, we formulate a set of asynchronous multi-agent
actor-critic methods that allow agents to directly optimize asynchronous
policies in three standard training paradigms: decentralized learning,
centralized learning, and centralized training for decentralized execution.
Empirical results (in simulation and hardware) in a variety of realistic
domains demonstrate the superiority of our approaches in large multi-agent
problems and validate the effectiveness of our algorithms for learning
high-quality and asynchronous solutions.",0,0,0,0,0,0,0.0924194,9.0,0.561915,98
7127f8c0-9048-4f42-97ff-6b2facdfc514,CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation,10,0.187507,0.873409,"Practical dialog systems need to deal with various knowledge sources, noisy
user expressions, and the shortage of annotated data. To better solve the above
problems, we propose CGoDial, new challenging and comprehensive Chinese
benchmark for multi-domain Goal-oriented Dialog evaluation. It contains 96,763
dialog sessions and 574,949 dialog turns totally, covering three datasets with
different knowledge sources: 1) a slot-based dialog (SBD) dataset with
table-formed knowledge, 2) a flow-based dialog (FBD) dataset with tree-formed
knowledge, and a retrieval-based dialog (RBD) dataset with candidate-formed
knowledge. To bridge the gap between academic benchmarks and spoken dialog
scenarios, we either collect data from real conversations or add spoken
features to existing datasets via crowd-sourcing. The proposed experimental
settings include the combinations of training with either the entire training
set or a few-shot training set, and testing with either the standard test set
or a hard test subset, which can assess model capabilities in terms of general
prediction, fast adaptability and reliable robustness.",1,1,1,1,0,0,0.515736,7.0,0.724164,67
a6c0fc23-71be-4657-abaa-756b02a453e1,Formal Algorithms for Transformers,44,0.0879173,0.703051,"This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.",0,0,0,0,0,1,0.265337,9.0,0.690447,33
164232e5-6614-4935-87d4-593cd5e8fbc7,The DLCC Node Classification Benchmark for Analyzing Knowledge Graph Embeddings,6,0.055891,0.501291,"Knowledge graph embedding is a representation learning technique that
projects entities and relations in a knowledge graph to continuous vector
spaces. Embeddings have gained a lot of uptake and have been heavily used in
link prediction and other downstream prediction tasks. Most approaches are
evaluated on a single task or a single group of tasks to determine their
overall performance. The evaluation is then assessed in terms of how well the
embedding approach performs on the task at hand. Still, it is hardly evaluated
(and often not even deeply understood) what information the embedding
approaches are actually learning to represent.
  To fill this gap, we present the DLCC (Description Logic Class Constructors)
benchmark, a resource to analyze embedding approaches in terms of which kinds
of classes they can represent. Two gold standards are presented, one based on
the real-world knowledge graph DBpedia and one synthetic gold standard. In
addition, an evaluation framework is provided that implements an experiment
protocol so that researchers can directly use the gold standard. To demonstrate
the use of DLCC, we compare multiple embedding approaches using the gold
standards. We find that many DL constructors on DBpedia are actually learned by
recognizing different correlated patterns than those defined in the gold
standard and that specific DL constructors, such as cardinality constraints,
are particularly hard to be learned for most embedding approaches.",0,0,1,1,0,0,0.124751,7.0,0.482144,26
8954fa3c-ae16-4f93-b6d2-b9614c990979,Positive-Unlabeled Learning with Adversarial Data Augmentation for Knowledge Graph Completion,22,0.425598,0.812222,"Most real-world knowledge graphs (KG) are far from complete and
comprehensive. This problem has motivated efforts in predicting the most
plausible missing facts to complete a given KG, i.e., knowledge graph
completion (KGC). However, existing KGC methods suffer from two main issues, 1)
the false negative issue, i.e., the sampled negative training instances may
include potential true facts; and 2) the data sparsity issue, i.e., true facts
account for only a tiny part of all possible facts. To this end, we propose
positive-unlabeled learning with adversarial data augmentation (PUDA) for KGC.
In particular, PUDA tailors positive-unlabeled risk estimator for the KGC task
to deal with the false negative issue. Furthermore, to address the data
sparsity issue, PUDA achieves a data augmentation strategy by unifying
adversarial training and positive-unlabeled learning under the
positive-unlabeled minimax game. Extensive experimental results on real-world
benchmark datasets demonstrate the effectiveness and compatibility of our
proposed method.",0,0,0,0,0,0,0.872885,9.0,0.901618,38
021d632f-798e-46a5-9990-5eaeba93ad97,Representation Uncertainty in Self-Supervised Learning as Variational Inference,7,0.0370276,0.146583,"In this study, a novel self-supervised learning (SSL) method is proposed,
which considers SSL in terms of variational inference to learn not only
representation but also representation uncertainties. SSL is a method of
learning representations without labels by maximizing the similarity between
image representations of different augmented views of an image. Meanwhile,
variational autoencoder (VAE) is an unsupervised representation learning method
that trains a probabilistic generative model with variational inference. Both
VAE and SSL can learn representations without labels, but their relationship
has not been investigated in the past. Herein, the theoretical relationship
between SSL and variational inference has been clarified. Furthermore, a novel
method, namely variational inference SimSiam (VI-SimSiam), has been proposed.
VI-SimSiam can predict the representation uncertainty by interpreting SimSiam
with variational inference and defining the latent space distribution. The
present experiments qualitatively show that VI- SimSiam could learn uncertainty
by comparing input images and predicted uncertainties. Additionally, we
described a relationship between estimated uncertainty and classification
accuracy.",0,0,0,0,0,0,0.52312,7.0,0.72716,62
288a7d8c-3122-4656-ba56-8f138524e830,Peano: Learning Formal Mathematical Reasoning,9,0.234077,0.66256,"General mathematical reasoning is computationally undecidable, but humans
routinely solve new problems. Moreover, discoveries developed over centuries
are taught to subsequent generations quickly. What structure enables this, and
how might that inform automated mathematical reasoning? We posit that central
to both puzzles is the structure of procedural abstractions underlying
mathematics. We explore this idea in a case study on 5 sections of beginning
algebra on the Khan Academy platform. To define a computational foundation, we
introduce Peano, a theorem-proving environment where the set of valid actions
at any point is finite. We use Peano to formalize introductory algebra problems
and axioms, obtaining well-defined search problems. We observe existing
reinforcement learning methods for symbolic reasoning to be insufficient to
solve harder problems. Adding the ability to induce reusable abstractions
(""tactics"") from its own solutions allows an agent to make steady progress,
solving all problems. Furthermore, these abstractions induce an order to the
problems, seen at random during training. The recovered order has significant
agreement with the expert-designed Khan Academy curriculum, and
second-generation agents trained on the recovered curriculum learn
significantly faster. These results illustrate the synergistic role of
abstractions and curricula in the cultural transmission of mathematics.",1,0,0,0,0,0,0.316316,17.0,0.848451,40
eca89768-9ebc-4e9a-a6fd-731a7199cdfa,On the Decentralization of Blockchain-enabled Asynchronous Federated Learning,4,0.0749963,0.279853,"Federated learning (FL), thanks in part to the emergence of the edge
computing paradigm, is expected to enable true real-time applications in
production environments. However, its original dependence on a central server
for orchestration raises several concerns in terms of security, privacy, and
scalability. To solve some of these worries, blockchain technology is expected
to bring decentralization, robustness, and enhanced trust to FL. The
empowerment of FL through blockchain (also referred to as FLchain), however,
has some implications in terms of ledger inconsistencies and age of information
(AoI), which are naturally inherited from the blockchain's fully decentralized
operation. Such issues stem from the fact that, given the temporary ledger
versions in the blockchain, FL devices may use different models for training,
and that, given the asynchronicity of the FL operation, stale local updates
(computed using outdated models) may be generated. In this paper, we shed light
on the implications of the FLchain setting and study the effect that both the
AoI and ledger inconsistencies have on the FL performance. To that end, we
provide a faithful simulation tool that allows capturing the decentralized and
asynchronous nature of the FLchain operation.",1,0,0,0,0,0,0.83909,5.0,0.798637,35
89e0e131-d122-4b67-8d2d-25277b856fa2,Adaptive User-Centered Multimodal Interaction towards Reliable and Trusted Automotive Interfaces,5,0.112994,0.333235,"With the recently increasing capabilities of modern vehicles, novel
approaches for interaction emerged that go beyond traditional touch-based and
voice command approaches. Therefore, hand gestures, head pose, eye gaze, and
speech have been extensively investigated in automotive applications for object
selection and referencing. Despite these significant advances, existing
approaches mostly employ a one-model-fits-all approach unsuitable for varying
user behavior and individual differences. Moreover, current referencing
approaches either consider these modalities separately or focus on a stationary
situation, whereas the situation in a moving vehicle is highly dynamic and
subject to safety-critical constraints. In this paper, I propose a research
plan for a user-centered adaptive multimodal fusion approach for referencing
external objects from a moving vehicle. The proposed plan aims to provide an
open-source framework for user-centered adaptation and personalization using
user observations and heuristics, multimodal fusion, clustering,
transfer-of-learning for model adaptation, and continuous learning, moving
towards trusted human-centered artificial intelligence.",0,0,0,0,0,0,0.0203123,15.0,0.633618,49
b3b6debf-c095-4331-8c8c-00ef838f15f0,QFF: Quantized Fourier Features for Neural Field Representations,4,0.0339177,0.209651,"Multilayer perceptrons (MLPs) learn high frequencies slowly. Recent
approaches encode features in spatial bins to improve speed of learning
details, but at the cost of larger model size and loss of continuity. Instead,
we propose to encode features in bins of Fourier features that are commonly
used for positional encoding. We call these Quantized Fourier Features (QFF).
As a naturally multiresolution and periodic representation, our experiments
show that using QFF can result in smaller model size, faster training, and
better quality outputs for several applications, including Neural Image
Representations (NIR), Neural Radiance Field (NeRF) and Signed Distance
Function (SDF) modeling. QFF are easy to code, fast to compute, and serve as a
simple drop-in addition to many neural field representations.",0,1,0,0,0,0,0.961413,4.0,0.892676,41
08e6ef18-f2c1-4517-a5af-fbf0a58c11ae,Standing on the Shoulders of Giant Frozen Language Models,38,0.202454,0.325353,"Huge pretrained language models (LMs) have demonstrated surprisingly good
zero-shot capabilities on a wide variety of tasks. This gives rise to the
appealing vision of a single, versatile model with a wide range of
functionalities across disparate applications. However, current leading
techniques for leveraging a ""frozen"" LM -- i.e., leaving its weights untouched
-- still often underperform fine-tuning approaches which modify these weights
in a task-dependent way. Those, in turn, suffer forgetfulness and compromise
versatility, suggesting a tradeoff between performance and versatility. The
main message of this paper is that current frozen-model techniques such as
prompt tuning are only the tip of the iceberg, and more powerful methods for
leveraging frozen LMs can do just as well as fine tuning in challenging domains
without sacrificing the underlying model's versatility. To demonstrate this, we
introduce three novel methods for leveraging frozen models: input-dependent
prompt tuning, frozen readers, and recursive LMs, each of which vastly improves
on current frozen-model approaches. Indeed, some of our methods even outperform
fine-tuning approaches in domains currently dominated by the latter. The
computational cost of each method is higher than that of existing frozen model
methods, but still negligible relative to a single pass through a huge frozen
LM. Each of these methods constitutes a meaningful contribution in its own
right, but by presenting these contributions together we aim to convince the
reader of a broader message that goes beyond the details of any given method:
that frozen models have untapped potential and that fine-tuning is often
unnecessary.",0,1,0,0,0,1,0.938385,3.0,0.805158,38
6753d187-9c26-4d2c-bd1b-0d0890dedd4c,Biometric Signature Verification Using Recurrent Neural Networks,18,0.614023,0.737732,"Architectures based on Recurrent Neural Networks (RNNs) have been
successfully applied to many different tasks such as speech or handwriting
recognition with state-of-the-art results. The main contribution of this work
is to analyse the feasibility of RNNs for on-line signature verification in
real practical scenarios. We have considered a system based on Long Short-Term
Memory (LSTM) with a Siamese architecture whose goal is to learn a similarity
metric from pairs of signatures. For the experimental work, the BiosecurID
database comprised of 400 users and 4 separated acquisition sessions are
considered. Our proposed LSTM RNN system has outperformed the results of recent
published works on the BiosecurID benchmark in figures ranging from 17.76% to
28.00% relative verification performance improvement for skilled forgeries.",0,1,0,0,1,0,0.915643,8.0,0.911984,22
a8c7c584-547b-48db-9c7a-ede85eb41525,A Multi-label Continual Learning Framework to Scale Deep Learning Approaches for Packaging Equipment Monitoring,6,0.0719317,0.336297,"Continual Learning aims to learn from a stream of tasks, being able to
remember at the same time both new and old tasks. While many approaches were
proposed for single-class classification, multi-label classification in the
continual scenario remains a challenging problem. For the first time, we study
multi-label classification in the Domain Incremental Learning scenario.
Moreover, we propose an efficient approach that has a logarithmic complexity
with regard to the number of tasks, and can be applied also in the Class
Incremental Learning scenario. We validate our approach on a real-world
multi-label Alarm Forecasting problem from the packaging industry. For the sake
of reproducibility, the dataset and the code used for the experiments are
publicly available.",0,1,1,0,0,0,0.209533,9.0,0.660328,51
53d506ca-77b0-40f2-b2d4-c27b8635fddb,Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training,8,0.288203,0.600442,"In cross-lingual speech synthesis, the speech in various languages can be
synthesized for a monoglot speaker. Normally, only the data of monoglot
speakers are available for model training, thus the speaker similarity is
relatively low between the synthesized cross-lingual speech and the native
language recordings. Based on the multilingual transformer text-to-speech
model, this paper studies a multi-task learning framework to improve the
cross-lingual speaker similarity. To further improve the speaker similarity,
joint training with a speaker classifier is proposed. Here, a scheme similar to
parallel scheduled sampling is proposed to train the transformer model
efficiently to avoid breaking the parallel training mechanism when introducing
joint training. By using multi-task learning and speaker classifier joint
training, in subjective and objective evaluations, the cross-lingual speaker
similarity can be consistently improved for both the seen and unseen speakers
in the training set.",0,1,0,0,0,0,0.741754,7.0,0.813359,21
4a169a9a-9542-4e49-940a-a068f22af8c1,Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering,60,0.38058,0.900871,"Recent works on knowledge base question answering (KBQA) retrieve subgraphs
for easier reasoning. A desired subgraph is crucial as a small one may exclude
the answer but a large one might introduce more noises. However, the existing
retrieval is either heuristic or interwoven with the reasoning, causing
reasoning on the partial subgraphs, which increases the reasoning bias when the
intermediate supervision is missing. This paper proposes a trainable subgraph
retriever (SR) decoupled from the subsequent reasoning process, which enables a
plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive
experiments demonstrate SR achieves significantly better retrieval and QA
performance than existing retrieval methods. Via weakly supervised pre-training
as well as the end-to-end fine-tuning, SRl achieves new state-of-the-art
performance when combined with NSM, a subgraph-oriented reasoner, for
embedding-based KBQA methods.",0,1,0,0,1,0,0.619393,6.0,0.725993,33
b5e4c6ba-4bbe-4b08-8079-451ca2efe3f1,Evidential Temporal-aware Graph-based Social Event Detection via Dempster-Shafer Theory,8,0.13566,0.458979,"The rising popularity of online social network services has attracted lots of
research on mining social media data, especially on mining social events.
Social event detection, due to its wide applications, has now become a trivial
task. State-of-the-art approaches exploiting Graph Neural Networks (GNNs)
usually follow a two-step strategy: 1) constructing text graphs based on
various views (\textit{co-user}, \textit{co-entities} and
\textit{co-hashtags}); and 2) learning a unified text representation by a
specific GNN model. Generally, the results heavily rely on the quality of the
constructed graphs and the specific message passing scheme. However, existing
methods have deficiencies in both aspects: 1) They fail to recognize the noisy
information induced by unreliable views. 2) Temporal information which works as
a vital indicator of events is neglected in most works. To this end, we propose
ETGNN, a novel Evidential Temporal-aware Graph Neural Network. Specifically, we
construct view-specific graphs whose nodes are the texts and edges are
determined by several types of shared elements respectively. To incorporate
temporal information into the message passing scheme, we introduce a novel
temporal-aware aggregator which assigns weights to neighbours according to an
adaptive time exponential decay formula. Considering the view-specific
uncertainty, the representations of all views are converted into mass functions
through evidential deep learning (EDL) neural networks, and further combined
via Dempster-Shafer theory (DST) to make the final detection. Experimental
results on three real-world datasets demonstrate the effectiveness of ETGNN in
accuracy, reliability and robustness in social event detection.",0,1,0,0,0,0,0.407469,11.0,0.794824,32
49893058-eaff-489a-a39a-37762c9d86d8,Path-Specific Objectives for Safer Agent Incentives,16,0.605348,0.56309,"We present a general framework for training safe agents whose naive
incentives are unsafe. As an example, manipulative or deceptive behaviour can
improve rewards but should be avoided. Most approaches fail here: agents
maximize expected return by any means necessary. We formally describe settings
with 'delicate' parts of the state which should not be used as a means to an
end. We then train agents to maximize the causal effect of actions on the
expected return which is not mediated by the delicate parts of state, using
Causal Influence Diagram analysis. The resulting agents have no incentive to
control the delicate state. We further show how our framework unifies and
generalizes existing proposals.",0,0,0,0,0,0,0.928009,9.0,0.928668,30
8b7b36cd-9faa-478b-9ead-1d21c45bd038,Interpretable Research Replication Prediction via Variational Contextual Consistency Sentence Masking,3,0.031064,0.134553,"Research Replication Prediction (RRP) is the task of predicting whether a
published research result can be replicated or not. Building an interpretable
neural text classifier for RRP promotes the understanding of why a research
paper is predicted as replicable or non-replicable and therefore makes its
real-world application more reliable and trustworthy. However, the prior works
on model interpretation mainly focused on improving the model interpretability
at the word/phrase level, which are insufficient especially for long research
papers in RRP. Furthermore, the existing methods cannot utilize a large size of
unlabeled dataset to further improve the model interpretability. To address
these limitations, we aim to build an interpretable neural model which can
provide sentence-level explanations and apply weakly supervised approach to
further leverage the large corpus of unlabeled datasets to boost the
interpretability in addition to improving prediction performance as existing
works have done. In this work, we propose the Variational Contextual
Consistency Sentence Masking (VCCSM) method to automatically extract key
sentences based on the context in the classifier, using both labeled and
unlabeled datasets. Results of our experiments on RRP along with European
Convention of Human Rights (ECHR) datasets demonstrate that VCCSM is able to
improve the model interpretability for the long document classification tasks
using the area over the perturbation curve and post-hoc accuracy as evaluation
metrics.",0,1,0,0,0,0,0.68054,9.0,0.835842,49
105bc028-9539-4c55-8779-626bec6f157f,HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding,65,0.769879,0.999656,"Encoding a driving scene into vector representations has been an essential
task for autonomous driving that can benefit downstream tasks e.g. trajectory
prediction. The driving scene often involves heterogeneous elements such as the
different types of objects (agents, lanes, traffic signs) and the semantic
relations between objects are rich and diverse. Meanwhile, there also exist
relativity across elements, which means that the spatial relation is a relative
concept and need be encoded in a ego-centric manner instead of in a global
coordinate system. Based on these observations, we propose Heterogeneous
Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a
heterogeneous graph with different types of nodes and edges. For heterogeneous
graph construction, we connect different types of nodes according to diverse
semantic relations. For spatial relation encoding, the coordinates of the node
as well as its in-edges are in the local node-centric coordinate system. For
the aggregation module in the graph neural network (GNN), we adopt the
transformer structure in a hierarchical way to fit the heterogeneous nature of
inputs. Experimental results show that HDGT achieves state-of-the-art
performance for the task of trajectory prediction, on INTERACTION Prediction
Challenge and Waymo Open Motion Challenge.",1,1,0,0,1,0,0.949904,5.0,0.897422,71
4a3f3218-9de6-4796-94df-6c743f654c18,Robust Task-Oriented Dialogue Generation with Contrastive Pre-training and Adversarial Filtering,3,0.087836,0.217116,"Data artifacts incentivize machine learning models to learn non-transferable
generalizations by taking advantage of shortcuts in the data, and there is
growing evidence that data artifacts play a role for the strong results that
deep learning models achieve in recent natural language processing benchmarks.
In this paper, we focus on task-oriented dialogue and investigate whether
popular datasets such as MultiWOZ contain such data artifacts. We found that by
only keeping frequent phrases in the training examples, state-of-the-art models
perform similarly compared to the variant trained with full data, suggesting
they exploit these spurious correlations to solve the task. Motivated by this,
we propose a contrastive learning based framework to encourage the model to
ignore these cues and focus on learning generalisable patterns. We also
experiment with adversarial filtering to remove ""easy"" training instances so
that the model would focus on learning from the ""harder"" instances. We conduct
a number of generalization experiments -- e.g., cross-domain/dataset and
adversarial tests -- to assess the robustness of our approach and found that it
works exceptionally well.",0,1,0,0,0,0,0.964522,6.0,0.932697,51
4cd405d9-6467-49a5-b084-17fcec6966a1,Spacecraft Pose Estimation Based on Unsupervised Domain Adaptation and on a 3D-Guided Loss Combination,4,0.042633,0.398691,"Spacecraft pose estimation is a key task to enable space missions in which
two spacecrafts must navigate around each other. Current state-of-the-art
algorithms for pose estimation employ data-driven techniques. However, there is
an absence of real training data for spacecraft imaged in space conditions due
to the costs and difficulties associated with the space environment. This has
motivated the introduction of 3D data simulators, solving the issue of data
availability but introducing a large gap between the training (source) and test
(target) domains. We explore a method that incorporates 3D structure into the
spacecraft pose estimation pipeline to provide robustness to intensity domain
shift and we present an algorithm for unsupervised domain adaptation with
robust pseudo-labelling. Our solution has ranked second in the two categories
of the 2021 Pose Estimation Challenge organised by the European Space Agency
and the Stanford University, achieving the lowest average error over the two
categories.",1,1,0,0,1,0,0.178432,13.0,0.751052,44
2ee889a7-bbf5-4160-ad2e-c98895d88797,CS-Insights: A System for Analyzing Computer Science Research,2,0.000934067,0.0638774,"This paper presents CS-Insights, an interactive web application to analyze
computer science publications from DBLP through multiple perspectives. The
dedicated interfaces allow its users to identify trends in research activity,
productivity, accessibility, author's productivity, venues' statistics, topics
of interest, and the impact of computer science research on other fields.
CS-Insightsis publicly available, and its modular architecture can be easily
adapted to domains other than computer science.",0,1,0,0,0,0,0.0764079,5.0,0.171675,20
631e3b42-08dd-41ea-9a11-c2d13933a7fc,The Best Decisions Are Not the Best Advice: Making Adherence-Aware Recommendations,9,0.0977855,0.804561,"Many high-stake decisions follow an expert-in-loop structure in that a human
operator receives recommendations from an algorithm but is the ultimate
decision maker. Hence, the algorithm's recommendation may differ from the
actual decision implemented in practice. However, most algorithmic
recommendations are obtained by solving an optimization problem that assumes
recommendations will be perfectly implemented. We propose an adherence-aware
optimization framework to capture the dichotomy between the recommended and the
implemented policy and analyze the impact of partial adherence on the optimal
recommendation. We show that overlooking the partial adherence phenomenon, as
is currently being done by most recommendation engines, can lead to arbitrarily
severe performance deterioration, compared with both the current human baseline
performance and what is expected by the recommendation algorithm. Our framework
also provides useful tools to analyze the structure and to compute optimal
recommendation policies that are naturally immune against such human
deviations, and are guaranteed to improve upon the baseline policy.",0,0,0,0,0,0,0.155885,9.0,0.62394,52
1ce2b83b-6cd8-4b58-bfb2-6d6af471cd45,Inference and dynamic decision-making for deteriorating systems with probabilistic dependencies through Bayesian networks and deep reinforcement learning,15,0.0972186,0.912096,"In the context of modern environmental and societal concerns, there is an
increasing demand for methods able to identify management strategies for civil
engineering systems, minimizing structural failure risks while optimally
planning inspection and maintenance (I&M) processes. Most available methods
simplify the I&M decision problem to the component level due to the
computational complexity associated with global optimization methodologies
under joint system-level state descriptions. In this paper, we propose an
efficient algorithmic framework for inference and decision-making under
uncertainty for engineering systems exposed to deteriorating environments,
providing optimal management strategies directly at the system level. In our
approach, the decision problem is formulated as a factored partially observable
Markov decision process, whose dynamics are encoded in Bayesian network
conditional structures. The methodology can handle environments under equal or
general, unequal deterioration correlations among components, through Gaussian
hierarchical structures and dynamic Bayesian networks. In terms of policy
optimization, we adopt a deep decentralized multi-agent actor-critic (DDMAC)
reinforcement learning approach, in which the policies are approximated by
actor neural networks guided by a critic network. By including deterioration
dependence in the simulated environment, and by formulating the cost model at
the system level, DDMAC policies intrinsically consider the underlying
system-effects. This is demonstrated through numerical experiments conducted
for both a 9-out-of-10 system and a steel frame under fatigue deterioration.
Results demonstrate that DDMAC policies offer substantial benefits when
compared to state-of-the-art heuristic approaches. The inherent consideration
of system-effects by DDMAC strategies is also interpreted based on the learned
policies.",0,0,0,0,0,0,0.0019945,14.0,0.441014,63
3204f043-1038-4fe4-918c-e30ab2d68760,MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality Assessment,6,0.183194,0.337584,"The acoustic environment can degrade speech quality during communication
(e.g., video call, remote presentation, outside voice recording), and its
impact is often unknown. Objective metrics for speech quality have proven
challenging to develop given the multi-dimensionality of factors that affect
speech quality and the difficulty of collecting labeled data. Hypothesizing the
impact of acoustics on speech quality, this paper presents MOSRA: a
non-intrusive multi-dimensional speech quality metric that can predict room
acoustics parameters (SNR, STI, T60, DRR, and C50) alongside the overall mean
opinion score (MOS) for speech quality. By explicitly optimizing the model to
learn these room acoustics parameters, we can extract more informative features
and improve the generalization for the MOS task when the training data is
limited. Furthermore, we also show that this joint training method enhances the
blind estimation of room acoustics, improving the performance of current
state-of-the-art models. An additional side-effect of this joint prediction is
the improvement in the explainability of the predictions, which is a valuable
feature for many applications.",0,1,0,0,0,0,0.512385,7.0,0.722799,29
60f06cb0-4d7c-427f-a4d4-cbd0c2a4c0e4,Unified Vision and Language Prompt Learning,80,0.664756,0.635048,"Prompt tuning, a parameter- and data-efficient transfer learning paradigm
that tunes only a small number of parameters in a model's input space, has
become a trend in the vision community since the emergence of large
vision-language models like CLIP. We present a systematic study on two
representative prompt tuning methods, namely text prompt tuning and visual
prompt tuning. A major finding is that none of the unimodal prompt tuning
methods performs consistently well: text prompt tuning fails on data with high
intra-class visual variances while visual prompt tuning cannot handle low
inter-class variances. To combine the best from both worlds, we propose a
simple approach called Unified Prompt Tuning (UPT), which essentially learns a
tiny neural network to jointly optimize prompts across different modalities.
Extensive experiments on over 11 vision datasets show that UPT achieves a
better trade-off than the unimodal counterparts on few-shot learning
benchmarks, as well as on domain generalization benchmarks. Code and models
will be released to facilitate future research.",1,1,0,0,0,0,0.973128,4.0,0.91903,44
67b751b7-c58f-4fe7-b698-f7b765944dda,RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures,14,0.161545,0.504984,"We introduce RIM-Net, a neural network which learns recursive implicit fields
for unsupervised inference of hierarchical shape structures. Our network
recursively decomposes an input 3D shape into two parts, resulting in a binary
tree hierarchy. Each level of the tree corresponds to an assembly of shape
parts, represented as implicit functions, to reconstruct the input shape. At
each node of the tree, simultaneous feature decoding and shape decomposition
are carried out by their respective feature and part decoders, with weight
sharing across the same hierarchy level. As an implicit field decoder, the part
decoder is designed to decompose a sub-shape, via a two-way branched
reconstruction, where each branch predicts a set of parameters defining a
Gaussian to serve as a local point distribution for shape reconstruction. With
reconstruction losses accounted for at each hierarchy level and a decomposition
loss at each node, our network training does not require any ground-truth
segmentations, let alone hierarchies. Through extensive experiments and
comparisons to state-of-the-art alternatives, we demonstrate the quality,
consistency, and interpretability of hierarchical structural inference by
RIM-Net.",0,0,0,0,0,0,0.71922,10.0,0.862972,32
599b23e2-a4d6-4fc9-a4a8-981a116d2b53,Neural Networks with Divisive normalization for image segmentation with application in cityscapes dataset,1,0.000856726,0.0201018,"One of the key problems in computer vision is adaptation: models are too
rigid to follow the variability of the inputs. The canonical computation that
explains adaptation in sensory neuroscience is divisive normalization, and it
has appealing effects on image manifolds. In this work we show that including
divisive normalization in current deep networks makes them more invariant to
non-informative changes in the images. In particular, we focus on U-Net
architectures for image segmentation. Experiments show that the inclusion of
divisive normalization in the U-Net architecture leads to better segmentation
results with respect to conventional U-Net. The gain increases steadily when
dealing with images acquired in bad weather conditions. In addition to the
results on the Cityscapes and Foggy Cityscapes datasets, we explain these
advantages through visualization of the responses: the equalization induced by
the divisive normalization leads to more invariant features to local changes in
contrast and illumination.",0,0,0,0,0,0,0.000256952,14.0,0.294576,32
a08524ba-0e58-442e-ba42-1c404f4e91b4,Scene Graph Modification as Incremental Structure Expanding,2,0.0204907,0.272233,"A scene graph is a semantic representation that expresses the objects,
attributes, and relationships between objects in a scene. Scene graphs play an
important role in many cross modality tasks, as they are able to capture the
interactions between images and texts. In this paper, we focus on scene graph
modification (SGM), where the system is required to learn how to update an
existing scene graph based on a natural language query. Unlike previous
approaches that rebuilt the entire scene graph, we frame SGM as a graph
expansion task by introducing the incremental structure expanding (ISE). ISE
constructs the target graph by incrementally expanding the source graph without
changing the unmodified structure. Based on ISE, we further propose a model
that iterates between nodes prediction and edges prediction, inferring more
accurate and harmonious expansion decisions progressively. In addition, we
construct a challenging dataset that contains more complicated queries and
larger scene graphs than existing datasets. Experiments on four benchmarks
demonstrate the effectiveness of our approach, which surpasses the previous
state-of-the-art model by large margins.",1,1,1,1,1,0,0.250733,9.0,0.683118,62
79448034-1d13-4aa9-99ee-449e035d0344,Best-$k$ Search Algorithm for Neural Text Generation,4,0.03876,0.0761536,"Modern natural language generation paradigms require a good decoding strategy
to obtain quality sequences out of the model. Beam search yields high-quality
but low diversity outputs; stochastic approaches suffer from high variance and
sometimes low quality, but the outputs tend to be more natural and creative. In
this work, we propose a deterministic search algorithm balancing both quality
and diversity. We first investigate the vanilla best-first search (BFS)
algorithm and then propose the Best-$k$ Search algorithm. Inspired by BFS, we
greedily expand the top $k$ nodes, instead of only the first node, to boost
efficiency and diversity. Upweighting recently discovered nodes accompanied by
heap pruning ensures the completeness of the search procedure. Experiments on
four NLG tasks, including question generation, commonsense generation, text
summarization, and translation, show that best-$k$ search yields more diverse
and natural outputs compared to strong baselines, while our approach maintains
high text quality. The proposed algorithm is parameter-free, lightweight,
efficient, and easy to use.",0,1,0,0,0,1,0.598125,6.0,0.716338,63
dfd3fe06-e6ea-4671-a204-41bde04083e8,Mixed Differential Privacy in Computer Vision,38,0.222372,0.966163,"We introduce AdaMix, an adaptive differentially private algorithm for
training deep neural network classifiers using both private and public image
data. While pre-training language models on large public datasets has enabled
strong differential privacy (DP) guarantees with minor loss of accuracy, a
similar practice yields punishing trade-offs in vision tasks. A few-shot or
even zero-shot learning baseline that ignores private data can outperform
fine-tuning on a large private dataset. AdaMix incorporates few-shot training,
or cross-modal zero-shot learning, on public data prior to private fine-tuning,
to improve the trade-off. AdaMix reduces the error increase from the
non-private upper bound from the 167-311\% of the baseline, on average across 6
datasets, to 68-92\% depending on the desired privacy level selected by the
user. AdaMix tackles the trade-off arising in visual classification, whereby
the most privacy sensitive data, corresponding to isolated points in
representation space, are also critical for high classification accuracy. In
addition, AdaMix comes with strong theoretical privacy guarantees and
convergence analysis.",1,1,0,0,1,0,0.554482,7.0,0.739716,61
26de8a97-12c7-477a-96a3-b721a444b99e,Semantic Enhanced Text-to-SQL Parsing via Iteratively Learning Schema Linking Graph,18,0.590368,0.763441,"The generalizability to new databases is of vital importance to Text-to-SQL
systems which aim to parse human utterances into SQL statements. Existing works
achieve this goal by leveraging the exact matching method to identify the
lexical matching between the question words and the schema items. However,
these methods fail in other challenging scenarios, such as the synonym
substitution in which the surface form differs between the corresponding
question words and schema items. In this paper, we propose a framework named
ISESL-SQL to iteratively build a semantic enhanced schema-linking graph between
question tokens and database schemas. First, we extract a schema linking graph
from PLMs through a probing procedure in an unsupervised manner. Then the
schema linking graph is further optimized during the training process through a
deep graph learning method. Meanwhile, we also design an auxiliary task called
graph regularization to improve the schema information mentioned in the
schema-linking graph. Extensive experiments on three benchmarks demonstrate
that ISESL-SQL could consistently outperform the baselines and further
investigations show its generalizability and robustness.",0,0,0,0,0,0,0.936448,5.0,0.880861,48
86fc9239-15a1-4afa-a6cc-3d01a24e7d7d,Near-Optimal Deployment Efficiency in Reward-Free Reinforcement Learning with Linear Function Approximation,9,0.012437,0.250876,"We study the problem of deployment efficient reinforcement learning (RL) with
linear function approximation under the \emph{reward-free} exploration setting.
This is a well-motivated problem because deploying new policies is costly in
real-life RL applications. Under the linear MDP setting with feature dimension
$d$ and planning horizon $H$, we propose a new algorithm that collects at most
$\widetilde{O}(\frac{d^2H^5}{\epsilon^2})$ trajectories within $H$ deployments
to identify $\epsilon$-optimal policy for any (possibly data-dependent) choice
of reward functions. To the best of our knowledge, our approach is the first to
achieve optimal deployment complexity and optimal $d$ dependence in sample
complexity at the same time, even if the reward is known ahead of time. Our
novel techniques include an exploration-preserving policy discretization and a
generalized G-optimal experiment design, which could be of independent
interest. Lastly, we analyze the related problem of regret minimization in
low-adaptive RL and provide information-theoretic lower bounds for switching
cost and batch complexity.",0,0,0,0,1,0,0.128187,5.0,0.28082,75
8644dc3f-f273-4b88-af24-5a59ae13bdc7,AARGH! End-to-end Retrieval-Generation for Task-Oriented Dialog,7,0.183438,0.473217,"We introduce AARGH, an end-to-end task-oriented dialog system combining
retrieval and generative approaches in a single model, aiming at improving
dialog management and lexical diversity of outputs. The model features a new
response selection method based on an action-aware training objective and a
simplified single-encoder retrieval architecture which allow us to build an
end-to-end retrieval-enhanced generation model where retrieval and generation
share most of the parameters. On the MultiWOZ dataset, we show that our
approach produces more diverse outputs while maintaining or improving state
tracking and context-to-response generation performance, compared to
state-of-the-art baselines.",1,1,0,0,1,0,0.863399,7.0,0.868435,64
90c756f1-d1ae-4d49-880f-bebb02905e83,SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels,19,0.687974,0.833861,"Deep neural networks are prone to overfitting noisy labels, resulting in poor
generalization performance. To overcome this problem, we present a simple and
effective method self-ensemble label correction (SELC) to progressively correct
noisy labels and refine the model. We look deeper into the memorization
behavior in training with noisy labels and observe that the network outputs are
reliable in the early stage. To retain this reliable knowledge, SELC uses
ensemble predictions formed by an exponential moving average of network outputs
to update the original noisy labels. We show that training with SELC refines
the model by gradually reducing supervision from noisy labels and increasing
supervision from ensemble predictions. Despite its simplicity, compared with
many state-of-the-art methods, SELC obtains more promising and stable results
in the presence of class-conditional, instance-dependent, and real-world label
noise. The code is available at https://github.com/MacLLL/SELC.",1,1,0,0,1,0,0.970983,7.0,0.950665,44
904fe38e-76d9-4163-9223-6a15ea94138e,Combining Attention Module and Pixel Shuffle for License Plate Super-Resolution,7,0.577233,0.895923,"The License Plate Recognition (LPR) field has made impressive advances in the
last decade due to novel deep learning approaches combined with the increased
availability of training data. However, it still has some open issues,
especially when the data come from low-resolution (LR) and low-quality
images/videos, as in surveillance systems. This work focuses on license plate
(LP) reconstruction in LR and low-quality images. We present a Single-Image
Super-Resolution (SISR) approach that extends the attention/transformer module
concept by exploiting the capabilities of PixelShuffle layers and that has an
improved loss function based on LPR predictions. For training the proposed
architecture, we use synthetic images generated by applying heavy Gaussian
noise in terms of Structural Similarity Index Measure (SSIM) to the original
high-resolution (HR) images. In our experiments, the proposed method
outperformed the baselines both quantitatively and qualitatively. The datasets
we created for this work are publicly available to the research community at
https://github.com/valfride/lpr-rsr/",1,1,0,1,0,0,0.932641,9.0,0.931441,37
378836c0-03bf-4151-99b9-66604f8bc5f1,Stochastic analysis of the Elo rating algorithm in round-robin tournaments,2,0.0125385,0.367307,"The Elo algorithm, renowned for its simplicity, is widely used for rating in
sports tournaments and other applications. However, despite its widespread use,
a detailed understanding of the convergence characteristics of the Elo
algorithm is still lacking. Aiming to fill this gap, this paper presents a
comprehensive (stochastic) analysis of the Elo algorithm, considering
round-robin tournaments. Specifically, analytical expressions are derived
describing the evolution of the skills and performance metrics. Then, taking
into account the relationship between the behavior of the algorithm and the
step-size value, which is a hyperparameter that can be controlled, design
guidelines and discussions about the performance of the algorithm are provided.
Experimental results are shown confirming the accuracy of the analysis and
illustrating the applicability of the theoretical findings using real-world
data obtained from SuperLega, the Italian volleyball league.",0,0,0,0,0,0,8.37478e-05,18.0,0.38905,63
981d037e-b225-42b9-afa0-2e25f6bac308,Nonparametric Masked Language Modeling,34,0.221852,0.768225,"Existing language models (LMs) predict tokens with a softmax over a finite
vocabulary, which can make it difficult to predict rare tokens or phrases. We
introduce NPM, the first nonparametric masked language model that replaces this
softmax with a nonparametric distribution over every phrase in a reference
corpus. NPM fills in the [MASK] solely from retrieving a token from a text
corpus. We show that NPM can be efficiently trained with a contrastive
objective and an in-batch approximation to full corpus retrieval. Zero-shot
evaluation on 16 tasks including classification, fact probing and question
answering demonstrates that NPM outperforms significantly larger parametric
models, either with or without a retrieve-and-generate approach. It is
particularly better at dealing with rare patterns (word senses or facts) and
predicting rare or nearly unseen words (e.g., non-Latin script). We release the
model and code at github.com/facebookresearch/NPM.",1,0,0,0,0,0,0.580222,6.0,0.708172,96
3b75a105-d79a-47c3-8405-a7fbd3fcffa8,Computational historical linguistics and language diversity in South Asia,7,0.115383,0.837177,"South Asia is home to a plethora of languages, many of which severely lack
access to new language technologies. This linguistic diversity also results in
a research environment conducive to the study of comparative, contact, and
historical linguistics -- fields which necessitate the gathering of extensive
data from many languages. We claim that data scatteredness (rather than
scarcity) is the primary obstacle in the development of South Asian language
technology, and suggest that the study of language history is uniquely aligned
with surmounting this obstacle. We review recent developments in and at the
intersection of South Asian NLP and historical-comparative linguistics,
describing our and others' current efforts in this area. We also offer new
strategies towards breaking the data barrier.",0,0,0,0,0,0,0.00562029,12.0,0.434334,132
997f93dc-8c78-410e-8001-a1bf77b85bd4,SimPer: Simple Self-Supervised Learning of Periodic Targets,27,0.0963721,0.64874,"From human physiology to environmental evolution, important processes in
nature often exhibit meaningful and strong periodic or quasi-periodic changes.
Due to their inherent label scarcity, learning useful representations for
periodic tasks with limited or no supervision is of great benefit. Yet,
existing self-supervised learning (SSL) methods overlook the intrinsic
periodicity in data, and fail to learn representations that capture periodic or
frequency attributes. In this paper, we present SimPer, a simple contrastive
SSL regime for learning periodic information in data. To exploit the periodic
inductive bias, SimPer introduces customized augmentations, feature similarity
measures, and a generalized contrastive loss for learning efficient and robust
periodic representations. Extensive experiments on common real-world tasks in
human behavior analysis, environmental sensing, and healthcare domains verify
the superior performance of SimPer compared to state-of-the-art SSL methods,
highlighting its intriguing properties including better data efficiency,
robustness to spurious correlations, and generalization to distribution shifts.
Code and data are available at: https://github.com/YyzHarry/SimPer.",1,0,1,0,1,0,0.214985,7.0,0.567423,50
6c1f7c50-1613-4b68-9f2c-f6667d3c0e1b,MVP: Multi-task Supervised Pre-training for Natural Language Generation,17,0.110353,0.453646,"Pre-trained language models (PLMs) have achieved remarkable success in
natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are
pre-trained in an unsupervised manner using the large-scale general corpus. In
the meanwhile, an increasing number of models pre-trained with labeled data
(i.e. ""supervised pre-training"") showcase superior performance compared to
unsupervised pre-trained models. Motivated by the success of supervised
pre-training, we propose Multi-task superVised Pre-training (MVP) for natural
language generation. We collect a large-scale natural language generation
corpus, MVPCorpus, from $77$ datasets over $11$ diverse NLG tasks. Then we
unify these examples into a general text-to-text format to pre-train the text
generation model MVP in a supervised manner. For each task, we further
pre-train specific soft prompts to stimulate the model's capacity to perform a
specific task. Our MVP model can be seen as a practice that utilizes recent
instruction tuning on relatively small PLMs. Extensive experiments have
demonstrated the effectiveness and generality of our MVP model in a number of
NLG tasks, which achieves state-of-the-art performance on $13$ out of $17$
datasets, outperforming BART by $9.3\%$ and Flan-T5 by $5.8\%$.",1,1,0,1,1,0,0.583034,6.0,0.709458,200
77381d0a-fb5c-4a61-9522-1182a59a196c,FAPM: Fast Adaptive Patch Memory for Real-time Industrial Anomaly Detection,14,0.346217,0.923662,"Feature embedding-based methods have shown exceptional performance in
detecting industrial anomalies by comparing features of target images with
normal images. However, some methods do not meet the speed requirements of
real-time inference, which is crucial for real-world applications. To address
this issue, we propose a new method called Fast Adaptive Patch Memory (FAPM)
for real-time industrial anomaly detection. FAPM utilizes patch-wise and
layer-wise memory banks that store the embedding features of images at the
patch and layer level, respectively, which eliminates unnecessary repetitive
computations. We also propose patch-wise adaptive coreset sampling for faster
and more accurate detection. FAPM performs well in both accuracy and speed
compared to other state-of-the-art methods",0,1,0,0,1,0,0.955576,5.0,0.905292,24
4d4c23bc-51ad-4bf3-b693-54723190de50,Optimization of Directional Landmark Deployment for Visual Observer on SE(3),2,0.0211517,0.084995,"An optimization method is proposed in this paper for novel deployment of
given number of directional landmarks (location and pose) within a given region
in the 3-D task space. This new deployment technique is built on the geometric
models of both landmarks and the monocular camera. In particular, a new concept
of Multiple Coverage Probability (MCP) is defined to characterize the
probability of at least n landmarks being covered simultaneously by a camera at
a fixed position. The optimization is conducted with respect to the position
and pose of the given number of landmarks to maximize MCP through globally
exploration of the given 3-D space. By adopting the elimination genetic
algorithm, the global optimal solutions can be obtained, which are then applied
to improve the convergent performance of the visual observer on SE(3) as a
demonstration example. Both simulation and experimental results are presented
to validate the effectiveness of the proposed landmark deployment optimization
method.",0,1,0,0,0,0,0.00109834,14.0,0.398368,38
01b6bf6e-6d19-475b-b65f-7bca4225e696,Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions,15,0.0371343,0.643779,"We describe an augmented intelligence system for simplifying and enhancing
the modeling experience for operations research. Using this system, the user
receives a suggested formulation of an optimization problem based on its
description. To facilitate this process, we build an intuitive user interface
system that enables the users to validate and edit the suggestions. We
investigate controlled generation techniques to obtain an automatic suggestion
of formulation. Then, we evaluate their effectiveness with a newly created
dataset of linear programming problems drawn from various application domains.",0,1,0,1,0,0,0.0411384,8.0,0.402585,46
ca481c33-ffec-49ae-8776-bb2ee4792294,FairDistillation: Mitigating Stereotyping in Language Models,5,0.0452761,0.226542,"Large pre-trained language models are successfully being used in a variety of
tasks, across many languages. With this ever-increasing usage, the risk of
harmful side effects also rises, for example by reproducing and reinforcing
stereotypes. However, detecting and mitigating these harms is difficult to do
in general and becomes computationally expensive when tackling multiple
languages or when considering different biases. To address this, we present
FairDistillation: a cross-lingual method based on knowledge distillation to
construct smaller language models while controlling for specific biases. We
found that our distillation method does not negatively affect the downstream
performance on most tasks and successfully mitigates stereotyping and
representational harms. We demonstrate that FairDistillation can create fairer
language models at a considerably lower cost than alternative approaches.",0,1,0,0,0,0,0.783719,6.0,0.802766,41
33fa5ba3-7337-4c2e-b23f-3a7b21313257,Neural PCA for Flow-Based Representation Learning,1,0.00114899,0.0239131,"Of particular interest is to discover useful representations solely from
observations in an unsupervised generative manner. However, the question of
whether existing normalizing flows provide effective representations for
downstream tasks remains mostly unanswered despite their strong ability for
sample generation and density estimation. This paper investigates this problem
for such a family of generative models that admits exact invertibility. We
propose Neural Principal Component Analysis (Neural-PCA) that operates in full
dimensionality while capturing principal components in \emph{descending} order.
Without exploiting any label information, the principal components recovered
store the most informative elements in their \emph{leading} dimensions and
leave the negligible in the \emph{trailing} ones, allowing for clear
performance improvements of $5\%$-$10\%$ in downstream tasks. Such improvements
are empirically found consistent irrespective of the number of latent trailing
dimensions dropped. Our work suggests that necessary inductive bias be
introduced into generative modelling when representation quality is of
interest.",1,0,0,0,0,0,0.149996,9.0,0.619286,32
38a2262f-f36d-423b-a9a0-1a41c561793c,Bias-Eliminated Semantic Refinement for Any-Shot Learning,9,0.130726,0.166073,"When training samples are scarce, the semantic embedding technique, ie,
describing class labels with attributes, provides a condition to generate
visual features for unseen objects by transferring the knowledge from seen
objects. However, semantic descriptions are usually obtained in an external
paradigm, such as manual annotation, resulting in weak consistency between
descriptions and visual features. In this paper, we refine the coarse-grained
semantic description for any-shot learning tasks, ie, zero-shot learning (ZSL),
generalized zero-shot learning (GZSL), and few-shot learning (FSL). A new
model, namely, the semantic refinement Wasserstein generative adversarial
network (SRWGAN) model, is designed with the proposed multihead representation
and hierarchical alignment techniques. Unlike conventional methods, semantic
refinement is performed with the aim of identifying a bias-eliminated condition
for disjoint-class feature generation and is applicable in both inductive and
transductive settings. We extensively evaluate model performance on six
benchmark datasets and observe state-of-the-art results for any-shot learning;
eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset
and 82.2% harmonic accuracy for the Oxford Flowers (FLO) dataset in the
standard GZSL setting. Various visualizations are also provided to show the
bias-eliminated generation of SRWGAN. Our code is available.",1,1,0,0,1,0,0.546051,9.0,0.79495,83
5af61ddc-1d66-478f-b796-a73cc1f31ef2,Learning Representations for Hyper-Relational Knowledge Graphs,10,0.289364,0.782163,"Knowledge graphs (KGs) have gained prominence for their ability to learn
representations for uni-relational facts. Recently, research has focused on
modeling hyper-relational facts, which move beyond the restriction of
uni-relational facts and allow us to represent more complex and real-world
information. However, existing approaches for learning representations on
hyper-relational KGs majorly focus on enhancing the communication from
qualifiers to base triples while overlooking the flow of information from base
triple to qualifiers. This can lead to suboptimal qualifier representations,
especially when a large amount of qualifiers are presented. It motivates us to
design a framework that utilizes multiple aggregators to learn representations
for hyper-relational facts: one from the perspective of the base triple and the
other one from the perspective of the qualifiers. Experiments demonstrate the
effectiveness of our framework for hyper-relational knowledge graph completion
across multiple datasets. Furthermore, we conduct an ablation study that
validates the importance of the various components in our framework. The code
to reproduce our results can be found at
\url{https://github.com/HarryShomer/QUAD}.",1,0,0,0,0,0,0.958353,8.0,0.943372,23
47831805-a861-4191-95a3-9f733d026a94,Multimodal and Explainable Internet Meme Classification,4,0.0924058,0.107133,"In the current context where online platforms have been effectively
weaponized in a variety of geo-political events and social issues, Internet
memes make fair content moderation at scale even more difficult. Existing work
on meme classification and tracking has focused on black-box methods that do
not explicitly consider the semantics of the memes or the context of their
creation. In this paper, we pursue a modular and explainable architecture for
Internet meme understanding. We design and implement multimodal classification
methods that perform example- and prototype-based reasoning over training
cases, while leveraging both textual and visual SOTA models to represent the
individual cases. We study the relevance of our modular and explainable models
in detecting harmful memes on two existing tasks: Hate Speech Detection and
Misogyny Classification. We compare the performance between example- and
prototype-based methods, and between text, vision, and multimodal models,
across different categories of harmfulness (e.g., stereotype and
objectification). We devise a user-friendly interface that facilitates the
comparative analysis of examples retrieved by all of our models for any given
meme, informing the community about the strengths and limitations of these
explainable methods.",1,1,0,0,0,0,0.694489,7.0,0.794422,54
10678165-3079-4a6d-be34-5cae74d5fbc9,Automated Reinforcement Learning: An Overview,9,0.175506,0.296383,"Reinforcement Learning and recently Deep Reinforcement Learning are popular
methods for solving sequential decision making problems modeled as Markov
Decision Processes. RL modeling of a problem and selecting algorithms and
hyper-parameters require careful considerations as different configurations may
entail completely different performances. These considerations are mainly the
task of RL experts; however, RL is progressively becoming popular in other
fields where the researchers and system designers are not RL experts. Besides,
many modeling decisions, such as defining state and action space, size of
batches and frequency of batch updating, and number of timesteps are typically
made manually. For these reasons, automating different components of RL
framework is of great importance and it has attracted much attention in recent
years. Automated RL provides a framework in which different components of RL
including MDP modeling, algorithm selection and hyper-parameter optimization
are modeled and defined automatically. In this article, we explore the
literature and present recent work that can be used in automated RL. Moreover,
we discuss the challenges, open questions and research directions in AutoRL.",0,0,0,0,0,0,0.387913,10.0,0.767901,88
00398edd-514f-4942-a4f7-4bb779cdd548,Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem,8,0.175946,0.440457,"Math word problem solver requires both precise relation reasoning about
quantities in the text and reliable generation for the diverse equation.
Current sequence-to-tree or relation extraction methods regard this only from a
fixed view, struggling to simultaneously handle complex semantics and diverse
equations. However, human solving naturally involves two consistent reasoning
views: top-down and bottom-up, just as math equations also can be expressed in
multiple equivalent forms: pre-order and post-order. We propose a multi-view
consistent contrastive learning for a more complete semantics-to-equation
mapping. The entire process is decoupled into two independent but consistent
views: top-down decomposition and bottom-up construction, and the two reasoning
views are aligned in multi-granularity for consistency, enhancing global
generation and precise reasoning. Experiments on multiple datasets across two
languages show our approach significantly outperforms the existing baselines,
especially on complex problems. We also show after consistent alignment,
multi-view can absorb the merits of both views and generate more diverse
results consistent with the mathematical laws.",1,0,0,0,0,0,0.40517,7.0,0.676519,51
380cc7d7-f50d-4451-b60b-d15bc25a2160,LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs,28,0.280682,0.806713,"Recent advance in 2D CNNs has revealed that large kernels are important.
However, when directly applying large convolutional kernels in 3D CNNs, severe
difficulties are met, where those successful module designs in 2D become
surprisingly ineffective on 3D networks, including the popular depth-wise
convolution. To address this vital challenge, we instead propose the
spatial-wise partition convolution and its large-kernel module. As a result, it
avoids the optimization and efficiency issues of naive 3D large kernels. Our
large-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D
tasks of semantic segmentation and object detection. It achieves 73.9% mIoU on
the ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection
benchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance
further boosts to 74.2% NDS with a simple multi-modal fusion. In addition,
LargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object
detection. For the first time, we show that large kernels are feasible and
essential for 3D visual tasks.",1,1,0,0,1,0,0.895369,5.0,0.840948,82
abcb0993-4c83-4a22-983b-19c081abc473,Designing Biological Sequences via Meta-Reinforcement Learning and Bayesian Optimization,4,0.02665,0.171368,"The ability to accelerate the design of biological sequences can have a
substantial impact on the progress of the medical field. The problem can be
framed as a global optimization problem where the objective is an expensive
black-box function such that we can query large batches restricted with a
limitation of a low number of rounds. Bayesian Optimization is a principled
method for tackling this problem. However, the astronomically large state space
of biological sequences renders brute-force iterating over all possible
sequences infeasible. In this paper, we propose MetaRLBO where we train an
autoregressive generative model via Meta-Reinforcement Learning to propose
promising sequences for selection via Bayesian Optimization. We pose this
problem as that of finding an optimal policy over a distribution of MDPs
induced by sampling subsets of the data acquired in the previous rounds. Our
in-silico experiments show that meta-learning over such ensembles provides
robustness against reward misspecification and achieves competitive results
compared to existing strong baselines.",0,0,0,0,0,0,0.215098,11.0,0.724778,39
edbbc7d2-a978-4361-b158-845e52f277d0,Learning to Estimate Shapley Values with Vision Transformers,19,0.257192,0.61673,"Transformers have become a default architecture in computer vision, but
understanding what drives their predictions remains a challenging problem.
Current explanation approaches rely on attention values or input gradients, but
these provide a limited view of a model's dependencies. Shapley values offer a
theoretically sound alternative, but their computational cost makes them
impractical for large, high-dimensional models. In this work, we aim to make
Shapley values practical for vision transformers (ViTs). To do so, we first
leverage an attention masking approach to evaluate ViTs with partial
information, and we then develop a procedure to generate Shapley value
explanations via a separate, learned explainer model. Our experiments compare
Shapley values to many baseline methods (e.g., attention rollout, GradCAM,
LRP), and we find that our approach provides more accurate explanations than
existing methods for ViTs.",0,0,0,0,0,0,0.926075,7.0,0.90684,79
7d71e67c-fde9-4e61-a9dc-03f33b601922,On Adversarial Robustness of Deep Image Deblurring,9,0.0829998,0.534022,"Recent approaches employ deep learning-based solutions for the recovery of a
sharp image from its blurry observation. This paper introduces adversarial
attacks against deep learning-based image deblurring methods and evaluates the
robustness of these neural networks to untargeted and targeted attacks. We
demonstrate that imperceptible distortion can significantly degrade the
performance of state-of-the-art deblurring networks, even producing drastically
different content in the output, indicating the strong need to include
adversarially robust training not only in classification but also for image
recovery.",0,1,0,0,0,0,0.201927,9.0,0.655708,28
d95ac121-0654-4a76-b5bf-15d869214f5e,A Novel Viewport-Adaptive Motion Compensation Technique for Fisheye Video,1,0.015235,0.126104,"Although fisheye cameras are in high demand in many application areas due to
their large field of view, many image and video signal processing tasks such as
motion compensation suffer from the introduced strong radial distortions. A
recently proposed projection-based approach takes the fisheye projection into
account to improve fisheye motion compensation. However, the approach does not
consider the large field of view of fisheye lenses that requires the
consideration of different motion planes in 3D space. We propose a novel
viewport-adaptive motion compensation technique that applies the motion vectors
in different perspective viewports in order to realize these motion planes.
Thereby, some pixels are mapped to so-called virtual image planes and require
special treatment to obtain reliable mappings between the perspective viewports
and the original fisheye image. While the state-of-the-art ultra wide-angle
compensation is sufficiently accurate, we propose a virtual image plane
compensation that leads to perfect mappings. All in all, we achieve average
gains of +2.40 dB in terms of PSNR compared to the state of the art in fisheye
motion compensation.",0,1,0,0,1,0,0.000150836,31.0,0.664236,17
5beb1367-72c5-4001-9ce2-3ff0e28997b3,Application of DatasetGAN in medical imaging: preliminary studies,2,0.0754345,0.103798,"Generative adversarial networks (GANs) have been widely investigated for many
potential applications in medical imaging. DatasetGAN is a recently proposed
framework based on modern GANs that can synthesize high-quality segmented
images while requiring only a small set of annotated training images. The
synthesized annotated images could be potentially employed for many medical
imaging applications, where images with segmentation information are required.
However, to the best of our knowledge, there are no published studies focusing
on its applications to medical imaging. In this work, preliminary studies were
conducted to investigate the utility of DatasetGAN in medical imaging. Three
improvements were proposed to the original DatasetGAN framework, considering
the unique characteristics of medical images. The synthesized segmented images
by DatasetGAN were visually evaluated. The trained DatasetGAN was further
analyzed by evaluating the performance of a pre-defined image segmentation
technique, which was trained by the use of the synthesized datasets. The
effectiveness, concerns, and potential usage of DatasetGAN were discussed.",1,1,0,0,0,0,0.990281,5.0,0.984781,20
43eea4b4-7e90-4bf2-ab08-345ece8f9d84,Improving Chinese Story Generation via Awareness of Syntactic Dependencies and Semantics,10,0.161085,0.784396,"Story generation aims to generate a long narrative conditioned on a given
input. In spite of the success of prior works with the application of
pre-trained models, current neural models for Chinese stories still struggle to
generate high-quality long text narratives. We hypothesise that this stems from
ambiguity in syntactically parsing the Chinese language, which does not have
explicit delimiters for word segmentation. Consequently, neural models suffer
from the inefficient capturing of features in Chinese narratives. In this
paper, we present a new generation framework that enhances the feature
capturing mechanism by informing the generation model of dependencies between
words and additionally augmenting the semantic representation learning through
synonym denoising training. We conduct a range of experiments, and the results
demonstrate that our framework outperforms the state-of-the-art Chinese
generation models on all evaluation metrics, demonstrating the benefits of
enhanced dependency and semantic representation learning.",1,1,0,0,1,0,0.509188,7.0,0.721494,29
421997e4-c97f-46ac-96f8-394d12ca739f,ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,58,0.485515,0.571662,"This work introduces a new multi-task, parameter-efficient language model
(LM) tuning method that learns to transfer knowledge across different tasks via
a mixture of soft prompts-small prefix embedding vectors pre-trained for
different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt
Tuning), obtains source prompts as encodings of large-scale source tasks into a
small number of parameters and trains an attention module to interpolate the
source prompts and a newly initialized target prompt for every instance in the
target task. During training, only the target task prompt and the attention
weights, which are shared between tasks in multi-task training, are updated,
while the original LM and source prompts are intact. ATTEMPT is highly
parameter-efficient (e.g., updates 2,300 times fewer parameters than full
fine-tuning) while achieving high task performance using knowledge from
high-resource tasks. Moreover, it is modular using pre-trained soft prompts,
and can flexibly add or remove source prompts for effective knowledge transfer.
Our experimental results across 21 diverse NLP datasets show that ATTEMPT
significantly outperforms prompt tuning and outperforms or matches fully
fine-tuned or other parameter-efficient tuning approaches that use over ten
times more parameters. Finally, ATTEMPT outperforms previous work in few-shot
learning settings.",1,1,1,0,1,0,0.925021,6.0,0.890405,85
7c76c621-ffb6-411e-a029-aab359535938,"Evaluating State of the Art, Forecasting Ensembles- and Meta-learning Strategies for Model Fusion",9,0.0879078,0.402565,"Techniques of hybridisation and ensemble learning are popular model fusion
techniques for improving the predictive power of forecasting methods. With
limited research that instigates combining these two promising approaches, this
paper focuses on the utility of the Exponential-Smoothing-Recurrent Neural
Network (ES-RNN) in the pool of base models for different ensembles. We compare
against some state of the art ensembling techniques and arithmetic model
averaging as a benchmark. We experiment with the M4 forecasting data set of
100,000 time-series, and the results show that the Feature-based Forecast Model
Averaging (FFORMA), on average, is the best technique for late data fusion with
the ES-RNN. However, considering the M4's Daily subset of data, stacking was
the only successful ensemble at dealing with the case where all base model
performances are similar. Our experimental results indicate that we attain
state of the art forecasting results compared to N-BEATS as a benchmark. We
conclude that model averaging is a more robust ensemble than model selection
and stacking strategies. Further, the results show that gradient boosting is
superior for implementing ensemble learning strategies.",1,1,0,0,1,0,0.280474,9.0,0.697707,72
68c58731-d46b-4680-af60-efa9b7c7b722,WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning,2,0.00518467,0.0703281,"A crucial issue of current text generation models is that they often
uncontrollably generate factually inconsistent text with respective of their
inputs. Limited by the lack of annotated data, existing works in evaluating
factual consistency directly transfer the reasoning ability of models trained
on other data-rich upstream tasks like question answering (QA) and natural
language inference (NLI) without any further adaptation. As a result, they
perform poorly on the real generated text and are biased heavily by their
single-source upstream tasks. To alleviate this problem, we propose a weakly
supervised framework that aggregates multiple resources to train a precise and
efficient factual metric, namely WeCheck. WeCheck first utilizes a generative
model to accurately label a real generated sample by aggregating its weak
labels, which are inferred from multiple resources. Then, we train the target
metric model with the weak supervision while taking noises into consideration.
Comprehensive experiments on a variety of tasks demonstrate the strong
performance of WeCheck, which achieves a 3.4\% absolute improvement over
previous state-of-the-art methods on TRUE benchmark on average.",0,1,0,0,1,1,0.413822,6.0,0.627241,53
309bb2ae-1f56-4514-8062-d6da9f44c0f4,DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection,28,0.585519,0.814973,"Graph Anomaly Detection (GAD) has recently become a hot research spot due to
its practicability and theoretical value. Since GAD emphasizes the application
and the rarity of anomalous samples, enriching the varieties of its datasets is
fundamental work. Thus, this paper present DGraph, a real-world dynamic graph
in the finance domain. DGraph overcomes many limitations of current GAD
datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth
nodes. We provide a comprehensive observation of DGraph, revealing that
anomalous nodes and normal nodes generally have different structures, neighbor
distribution, and temporal dynamics. Moreover, it suggests that unlabeled nodes
are also essential for detecting fraudsters. Furthermore, we conduct extensive
experiments on DGraph. Observation and experiments demonstrate that DGraph is
propulsive to advance GAD research and enable in-depth exploration of anomalous
nodes.",1,1,0,1,0,0,0.809148,10.0,0.889512,60
3bc7a9b1-a447-4ad1-aca4-eb130f8f592c,Too much information: why CDCL solvers need to forget learned clauses,2,0.0346916,0.0350367,"Conflict-driven clause learning (CDCL) is a remarkably successful paradigm
for solving the satisfiability problem of propositional logic. Instead of a
simple depth-first backtracking approach, this kind of solver learns the reason
behind occurring conflicts in the form of additional clauses. However, despite
the enormous success of CDCL solvers, there is still only a limited
understanding of what influences the performance of these solvers in what way.
  Considering different measures, this paper demonstrates, quite surprisingly,
that clause learning (without being able to get rid of some clauses) can not
only help the solver but can oftentimes deteriorate the solution process
dramatically. By conducting extensive empirical analysis, we furthermore find
that the runtime distributions of CDCL solvers are multimodal. This
multimodality can be seen as a reason for the deterioration phenomenon
described above. Simultaneously, it also gives an indication of why clause
learning in combination with clause deletion is virtually the de facto standard
of SAT solving, in spite of this phenomenon. As a final contribution, we show
that Weibull mixture distributions can accurately describe the multimodal
distributions. Thus, adding new clauses to a base instance has an inherent
effect of making runtimes long-tailed. This insight provides an explanation as
to why the technique of forgetting clauses is useful in CDCL solvers apart from
the optimization of unit propagation speed.",0,0,0,0,0,0,0.00474724,26.0,0.732413,65
cba0eee4-f461-4b0f-a6b6-51f10ebbb0e0,Active Diffusion and VCA-Assisted Image Segmentation of Hyperspectral Images,6,0.193583,0.281244,"Hyperspectral images encode rich structure that can be exploited for material
discrimination by machine learning algorithms. This article introduces the
Active Diffusion and VCA-Assisted Image Segmentation (ADVIS) for active
material discrimination. ADVIS selects high-purity, high-density pixels that
are far in diffusion distance (a data-dependent metric) from other high-purity,
high-density pixels in the hyperspectral image. The ground truth labels of
these pixels are queried and propagated to the rest of the image. The ADVIS
active learning algorithm is shown to strongly outperform its fully
unsupervised clustering algorithm counterpart, suggesting that the
incorporation of a very small number of carefully-selected ground truth labels
can result in substantially superior material discrimination in hyperspectral
images.",1,1,0,0,0,0,0.284404,11.0,0.754169,17
41c2f2cc-c3df-4b67-b38c-501fc1119dc5,Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again,91,0.852181,0.999943,"The strong few-shot in-context learning capability of large pre-trained
language models (PLMs) such as GPT-3 is highly appealing for application
domains such as biomedicine, which feature high and diverse demands of language
technologies but also high data annotation costs. In this paper, we present the
first systematic and comprehensive study to compare the few-shot performance of
GPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on
two highly representative biomedical information extraction tasks, named entity
recognition and relation extraction. We follow the true few-shot setting to
avoid overestimating models' few-shot performance by model selection over a
large validation set. We also optimize GPT-3's performance with known
techniques such as contextual calibration and dynamic in-context example
retrieval. However, our results show that GPT-3 still significantly
underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3
in-context learning also yields smaller gains in accuracy when more training
data becomes available. Our in-depth analyses further reveal issues of the
in-context learning setting that may be detrimental to information extraction
tasks in general. Given the high cost of experimenting with GPT-3, we hope our
study provides guidance for biomedical researchers and practitioners towards
more promising directions such as fine-tuning small PLMs.",1,1,0,0,0,0,0.963171,4.0,0.896232,47
1a765ca3-4eaf-4a3d-ac7e-faf3568a69c8,Generative Category-Level Shape and Pose Estimation with Semantic Primitives,10,0.287601,0.638949,"Empowering autonomous agents with 3D understanding for daily objects is a
grand challenge in robotics applications. When exploring in an unknown
environment, existing methods for object pose estimation are still not
satisfactory due to the diversity of object shapes. In this paper, we propose a
novel framework for category-level object shape and pose estimation from a
single RGB-D image. To handle the intra-category variation, we adopt a semantic
primitive representation that encodes diverse shapes into a unified latent
space, which is the key to establish reliable correspondences between observed
point clouds and estimated shapes. Then, by using a SIM(3)-invariant shape
descriptor, we gracefully decouple the shape and pose of an object, thus
supporting latent shape optimization of target objects in arbitrary poses.
Extensive experiments show that the proposed method achieves SOTA pose
estimation performance and better generalization in the real-world dataset.
Code and video are available at https://zju3dv.github.io/gCasp.",1,1,0,0,1,0,0.940987,6.0,0.90514,39
cd47a3b8-4e85-4a07-aa40-efa6ba35ae34,Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models,2,0.033373,0.0476995,"Large language models (LLM) trained using the next-token-prediction
objective, such as GPT3 and PaLM, have revolutionized natural language
processing in recent years by showing impressive zero-shot and few-shot
capabilities across a wide range of tasks. In this work, we propose a simple
technique that significantly boosts the performance of LLMs without adding
computational cost. Our key observation is that, by performing the next token
prediction task with randomly selected past tokens masked out, we can improve
the quality of the learned representations for downstream language
understanding tasks. We hypothesize that randomly masking past tokens prevents
over-attending to recent tokens and encourages attention to tokens in the
distant past. We find that our method, Forgetful Causal Masking (FCM),
significantly improves both few-shot and finetuning performance of PaLM. We
further consider a simple extension, T-FCM, which introduces bidirectional
context to causal language model without altering the sequence order, and
further improves finetuning performance.",0,1,0,0,0,1,0.980382,6.0,0.959922,62
f5fa7472-f7b2-46c2-8dc3-d0bacd150f2a,Machine Learning Algorithms for Time Series Analysis and Forecasting,2,0.0118944,0.185869,"Time series data is being used everywhere, from sales records to patients'
health evolution metrics. The ability to deal with this data has become a
necessity, and time series analysis and forecasting are used for the same.
Every Machine Learning enthusiast would consider these as very important tools,
as they deepen the understanding of the characteristics of data. Forecasting is
used to predict the value of a variable in the future, based on its past
occurrences. A detailed survey of the various methods that are used for
forecasting has been presented in this paper. The complete process of
forecasting, from preprocessing to validation has also been explained
thoroughly. Various statistical and deep learning models have been considered,
notably, ARIMA, Prophet and LSTMs. Hybrid versions of Machine Learning models
have also been explored and elucidated. Our work can be used by anyone to
develop a good understanding of the forecasting process, and to identify
various state of the art models which are being used today.",0,1,0,0,0,1,0.198083,6.0,0.479973,35
797cfed1-5684-4db7-a995-1d46c284683e,On an Edge-Preserving Variational Model for Optical Flow Estimation,1,0.00820501,0.102574,"It is well known that classical formulations resembling the Horn and Schunck
model are still largely competitive due to the modern implementation practices.
In most cases, these models outperform many modern flow estimation methods. In
view of this, we propose an effective implementation design for an
edge-preserving $L^1$ regularization approach to optical flow. The mathematical
well-posedness of our proposed model is studied in the space of functions of
bounded variations $BV(\Omega,\mathbb{R}^2)$. The implementation scheme is
designed in multiple steps. The flow field is computed using the robust
Chambolle-Pock primal-dual algorithm. Motivated by the recent studies of Castro
and Donoho we extend the heuristic of iterated median filtering to our flow
estimation. Further, to refine the flow edges we use the weighted median filter
established by Li and Osher as a post-processing step. Our experiments on the
Middlebury dataset show that the proposed method achieves the best average
angular and end-point errors compared to some of the state-of-the-art Horn and
Schunck based variational methods.",0,0,0,0,1,0,0.000397955,35.0,0.730331,41
2a0a738b-5c97-4b1f-ba25-74dd6aa3169e,Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video,16,0.389088,0.707822,"We present HandAvatar, a novel representation for hand animation and
rendering, which can generate smoothly compositional geometry and
self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as
a high-resolution mesh topology to fit personalized hand shapes. Sequentially,
we decompose hand geometry into per-bone rigid parts, and then re-compose
paired geometry encodings to derive an across-part consistent occupancy field.
As for texture modeling, we propose a self-occlusion-aware shading field
(SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record
albedo information under a wide variety of hand poses. Moreover, directed soft
occupancy is designed to describe the ray-to-surface relation, which is
leveraged to generate an illumination field for the disentanglement of
pose-independent albedo and pose-dependent illumination. Trained from monocular
video data, our HandAvatar can perform free-pose hand animation and rendering
while at the same time achieving superior appearance fidelity. We also
demonstrate that HandAvatar provides a route for hand appearance editing.
Project website: https://seanchenxy.github.io/HandAvatarWeb.",0,0,0,0,0,0,0.948739,4.0,0.869851,79
4317538c-cd7e-48cf-aca0-531df53805b0,Applying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset,8,0.0150739,0.436345,"Speech is inherently continuous, where discrete words, phonemes and other
units are not clearly segmented, and so speech recognition has been an active
research problem for decades. In this work we have fine-tuned wav2vec 2.0 to
recognize and transcribe Bengali speech -- training it on the Bengali Common
Voice Speech Dataset. After training for 71 epochs, on a training set
consisting of 36919 mp3 files, we achieved a training loss of 0.3172 and WER of
0.2524 on a validation set of size 7,747. Using a 5-gram language model, the
Levenshtein Distance was 2.6446 on a test set of size 7,747. Then the training
set and validation set were combined, shuffled and split into 85-15 ratio.
Training for 7 more epochs on this combined dataset yielded an improved
Levenshtein Distance of 2.60753 on the test set. Our model was the best
performing one, achieving a Levenshtein Distance of 6.234 on a hidden dataset,
which was 1.1049 units lower than other competing submissions.",0,1,0,0,0,0,0.354379,4.0,0.390995,12
9e3793a9-cbc9-4636-b0e0-6f65b73ac2a8,Learning a General Clause-to-Clause Relationships for Enhancing Emotion-Cause Pair Extraction,6,0.391537,0.761617,"Emotion-cause pair extraction (ECPE) is an emerging task aiming to extract
potential pairs of emotions and corresponding causes from documents. Previous
approaches have focused on modeling the pair-to-pair relationship and achieved
promising results. However, the clause-to-clause relationship, which
fundamentally symbolizes the underlying structure of a document, has still been
in its research infancy. In this paper, we define a novel clause-to-clause
relationship. To learn it applicably, we propose a general clause-level
encoding model named EA-GAT comprising E-GAT and Activation Sort. E-GAT is
designed to aggregate information from different types of clauses; Activation
Sort leverages the individual emotion/cause prediction and the sort-based
mapping to propel the clause to a more favorable representation. Since EA-GAT
is a clause-level encoding model, it can be broadly integrated with any
previous approach. Experimental results show that our approach has a
significant advantage over all current approaches on the Chinese and English
benchmark corpus, with an average of $2.1\%$ and $1.03\%$.",0,0,1,0,1,0,0.854342,4.0,0.761566,38
479bb8c4-bc98-433f-908e-52a52fdd83b9,Procedural Image Programs for Representation Learning,7,0.300631,0.750991,"Learning image representations using synthetic data allows training neural
networks without some of the concerns associated with real images, such as
privacy and bias. Existing work focuses on a handful of curated generative
processes which require expert knowledge to design, making it hard to scale up.
To overcome this, we propose training with a large dataset of twenty-one
thousand programs, each one generating a diverse set of synthetic images. These
programs are short code snippets, which are easy to modify and fast to execute
using OpenGL. The proposed dataset can be used for both supervised and
unsupervised representation learning, and reduces the gap between pre-training
with real and procedurally generated images by 38%.",1,1,0,1,1,0,0.954953,6.0,0.92033,48
41402635-1f84-4c20-8ff5-110c0f7e881c,Whodunit? Learning to Contrast for Authorship Attribution,10,0.229823,0.773398,"Authorship attribution is the task of identifying the author of a given text.
The key is finding representations that can differentiate between authors.
Existing approaches typically use manually designed features that capture a
dataset's content and style, but these approaches are dataset-dependent and
yield inconsistent performance across corpora. In this work, we propose
\textit{learning} author-specific representations by fine-tuning pre-trained
generic language representations with a contrastive objective (Contra-X). We
show that Contra-X learns representations that form highly separable clusters
for different authors. It advances the state-of-the-art on multiple human and
machine authorship attribution benchmarks, enabling improvements of up to 6.8%
over cross-entropy fine-tuning. However, we find that Contra-X improves overall
accuracy at the cost of sacrificing performance for some authors. Resolving
this tension will be an important direction for future work. To the best of our
knowledge, we are the first to integrate contrastive learning with pre-trained
language model fine-tuning for authorship attribution.",1,1,0,0,1,0,0.794287,7.0,0.835542,48
c59930cf-8a0d-437f-8066-a68598bbfc4b,Gradient-based Uncertainty for Monocular Depth Estimation,18,0.251295,0.808156,"In monocular depth estimation, disturbances in the image context, like moving
objects or reflecting materials, can easily lead to erroneous predictions. For
that reason, uncertainty estimates for each pixel are necessary, in particular
for safety-critical applications such as automated driving. We propose a post
hoc uncertainty estimation approach for an already trained and thus fixed depth
estimation model, represented by a deep neural network. The uncertainty is
estimated with the gradients which are extracted with an auxiliary loss
function. To avoid relying on ground-truth information for the loss definition,
we present an auxiliary loss function based on the correspondence of the depth
prediction for an image and its horizontally flipped counterpart. Our approach
achieves state-of-the-art uncertainty estimation results on the KITTI and NYU
Depth V2 benchmarks without the need to retrain the neural network. Models and
code are publicly available at https://github.com/jhornauer/GrUMoDepth.",1,1,0,0,1,0,0.559632,9.0,0.799143,32
7b2745bf-3e24-42c9-b03c-27448fcaa9d0,Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence,7,0.0320714,0.715379,"The logical negation property (LNP), which implies generating different
predictions for semantically opposite inputs, is an important property that a
trustworthy language model must satisfy. However, much recent evidence shows
that large-size pre-trained language models (PLMs) do not satisfy this
property. In this paper, we perform experiments using probing tasks to assess
PLM's LNP understanding. Unlike previous studies that only examined negation
expressions, we expand the boundary of the investigation to lexical semantics.
Through experiments, we observe that PLMs violate the LNP frequently. To
alleviate the issue, we propose a novel intermediate training task, names
meaning-matching, designed to directly learn a meaning-text correspondence,
instead of relying on the distributional hypothesis. Through multiple
experiments, we find that the task enables PLMs to learn lexical semantic
information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm
that it is a safe intermediate task that guarantees a similar or better
performance of downstream tasks. Finally, we observe that our proposed approach
outperforms our previous counterparts despite its time and resource efficiency.",0,0,0,0,0,0,0.481231,6.0,0.66157,53
1d08830b-c52a-4fee-b824-3fee31e477de,Self-supervised models of audio effectively explain human cortical responses to speech,30,0.233728,0.815524,"Self-supervised language models are very effective at predicting high-level
cortical responses during language comprehension. However, the best current
models of lower-level auditory processing in the human brain rely on either
hand-constructed acoustic filters or representations from supervised audio
neural networks. In this work, we capitalize on the progress of self-supervised
speech representation learning (SSL) to create new state-of-the-art models of
the human auditory system. Compared against acoustic baselines, phonemic
features, and supervised models, representations from the middle layers of
self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently
yield the best prediction performance for fMRI recordings within the auditory
cortex (AC). Brain areas involved in low-level auditory processing exhibit a
preference for earlier SSL model layers, whereas higher-level semantic areas
prefer later layers. We show that these trends are due to the models' ability
to encode information at multiple linguistic levels (acoustic, phonetic, and
lexical) along their representation depth. Overall, these results show that
self-supervised models effectively capture the hierarchy of information
relevant to different stages of speech processing in human cortex.",0,0,0,0,1,0,0.519225,9.0,0.786564,51
b33d51ac-7827-49e1-8adf-ec445c85a954,Improving Image Clustering through Sample Ranking and Its Application to remote--sensing images,1,0.00209876,0.0852847,"Image clustering is a very useful technique that is widely applied to various
areas, including remote sensing. Recently, visual representations by
self-supervised learning have greatly improved the performance of image
clustering. To further improve the well-trained clustering models, this paper
proposes a novel method by first ranking samples within each cluster based on
the confidence in their belonging to the current cluster and then using the
ranking to formulate a weighted cross-entropy loss to train the model. For
ranking the samples, we developed a method for computing the likelihood of
samples belonging to the current clusters based on whether they are situated in
densely populated neighborhoods, while for training the model, we give a
strategy for weighting the ranked samples. We present extensive experimental
results that demonstrate that the new technique can be used to improve the
State-of-the-Art image clustering models, achieving accuracy performance gains
ranging from $2.1\%$ to $15.9\%$. Performing our method on a variety of
datasets from remote sensing, we show that our method can be effectively
applied to remote--sensing images.",1,1,0,0,1,0,0.124327,11.0,0.670125,88
17cb10d7-c39a-4fc8-afbe-d8355a5dec12,Multi hash embeddings in spaCy,3,0.0565808,0.125457,"The distributed representation of symbols is one of the key technologies in
machine learning systems today, playing a pivotal role in modern natural
language processing. Traditional word embeddings associate a separate vector
with each word. While this approach is simple and leads to good performance, it
requires a lot of memory for representing a large vocabulary. To reduce the
memory footprint, the default embedding layer in spaCy is a hash embeddings
layer. It is a stochastic approximation of traditional embeddings that provides
unique vectors for a large number of words without explicitly storing a
separate vector for each of them. To be able to compute meaningful
representations for both known and unknown words, hash embeddings represent
each word as a summary of the normalized word form, subword information and
word shape. Together, these features produce a multi-embedding of a word. In
this technical report we lay out a bit of history and introduce the embedding
methods in spaCy in detail. Second, we critically evaluate the hash embedding
architecture with multi-embeddings on Named Entity Recognition datasets from a
variety of domains and languages. The experiments validate most key design
choices behind spaCy's embedders, but we also uncover a few surprising results.",0,1,0,0,0,1,0.844162,21.0,0.952884,27
c8fa2e93-3856-4b0e-a5b0-28814cc51153,TAS-NIR: A VIS+NIR Dataset for Fine-grained Semantic Segmentation in Unstructured Outdoor Environments,4,0.0252924,0.376858,"Vegetation Indices based on paired images of the visible color spectrum (VIS)
and near infrared spectrum (NIR) have been widely used in remote sensing
applications. These vegetation indices are extended for their application in
autonomous driving in unstructured outdoor environments. In this domain we can
combine traditional vegetation indices like the Normalized Difference
Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI) with Convolutional
Neural Networks (CNNs) pre-trained on available VIS datasets. By laying a focus
on learning calibrated CNN outputs, we can provide an approach to fuse known
hand-crafted image features with CNN predictions for different domains as well.
The method is evaluated on a VIS+NIR dataset of semantically annotated images
in unstructured outdoor environments. The dataset is available at
mucar3.de/iros2022-ppniv-tas-nir.",0,1,0,1,0,0,0.0939404,13.0,0.69803,35
8b34747e-11e3-4baf-9ce1-4ca876f58e1c,Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement,3,0.0634298,0.22672,"We propose an effective Structural Prior guided Generative Adversarial
Transformer (SPGAT) to solve low-light image enhancement. Our SPGAT mainly
contains a generator with two discriminators and a structural prior estimator
(SPE). The generator is based on a U-shaped Transformer which is used to
explore non-local information for better clear image restoration. The SPE is
used to explore useful structures from images to guide the generator for better
structural detail estimation. To generate more realistic images, we develop a
new structural prior guided adversarial learning method by building the skip
connections between the generator and discriminators so that the discriminators
can better discriminate between real and fake features. Finally, we propose a
parallel windows-based Swin Transformer block to aggregate different level
hierarchical features for high-quality image restoration. Experimental results
demonstrate that the proposed SPGAT performs favorably against recent
state-of-the-art methods on both synthetic and real-world datasets.",0,1,0,0,1,0,0.952128,7.0,0.928881,74
b64f1335-acbc-44db-b476-d225496db641,Causal Discovery for Fairness,9,0.207572,0.366184,"It is crucial to consider the social and ethical consequences of AI and ML
based decisions for the safe and acceptable use of these emerging technologies.
Fairness, in particular, guarantees that the ML decisions do not result in
discrimination against individuals or minorities. Identifying and measuring
reliably fairness/discrimination is better achieved using causality which
considers the causal relation, beyond mere association, between the sensitive
attribute (e.g. gender, race, religion, etc.) and the decision (e.g. job
hiring, loan granting, etc.). The big impediment to the use of causality to
address fairness, however, is the unavailability of the causal model (typically
represented as a causal graph). Existing causal approaches to fairness in the
literature do not address this problem and assume that the causal model is
available. In this paper, we do not make such assumption and we review the
major algorithms to discover causal relations from observable data. This study
focuses on causal discovery and its impact on fairness. In particular, we show
how different causal discovery approaches may result in different causal models
and, most importantly, how even slight differences between causal models can
have significant impact on fairness/discrimination conclusions. These results
are consolidated by empirical analysis using synthetic and standard fairness
benchmark datasets. The main goal of this study is to highlight the importance
of the causal discovery step to appropriately address fairness using causality.",0,0,0,0,0,0,0.0995581,20.0,0.806776,76
ce2271c8-c384-44cb-ba36-aba0e2c7900d,A Hardware-Aware System for Accelerating Deep Neural Network Optimization,1,0.0172273,0.106901,"Recent advances in Neural Architecture Search (NAS) which extract specialized
hardware-aware configurations (a.k.a. ""sub-networks"") from a hardware-agnostic
""super-network"" have become increasingly popular. While considerable effort has
been employed towards improving the first stage, namely, the training of the
super-network, the search for derivative high-performing sub-networks is still
largely under-explored. For example, some recent network morphism techniques
allow a super-network to be trained once and then have hardware-specific
networks extracted from it as needed. These methods decouple the super-network
training from the sub-network search and thus decrease the computational burden
of specializing to different hardware platforms. We propose a comprehensive
system that automatically and efficiently finds sub-networks from a pre-trained
super-network that are optimized to different performance metrics and hardware
configurations. By combining novel search tactics and algorithms with
intelligent use of predictors, we significantly decrease the time needed to
find optimal sub-networks from a given super-network. Further, our approach
does not require the super-network to be refined for the target task a priori,
thus allowing it to interface with any super-network. We demonstrate through
extensive experiments that our system works seamlessly with existing
state-of-the-art super-network training methods in multiple domains. Moreover,
we show how novel search tactics paired with evolutionary algorithms can
accelerate the search process for ResNet50, MobileNetV3 and Transformer while
maintaining objective space Pareto front diversity and demonstrate an 8x faster
search result than the state-of-the-art Bayesian optimization WeakNAS approach.",0,1,0,0,0,0,0.848485,7.0,0.860798,24
3412999f-6dea-4e7c-b80c-c681f2ee2f51,Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator,10,0.163686,0.524051,"Contextual knowledge is essential for reducing speech recognition errors on
high-valued long-tail words. This paper proposes a novel tree-constrained
pointer generator (TCPGen) component that enables end-to-end ASR models to bias
towards a list of long-tail words obtained using external contextual
information. With only a small overhead in memory use and computation cost,
TCPGen can structure thousands of biasing words efficiently into a symbolic
prefix-tree and creates a neural shortcut between the tree and the final ASR
output to facilitate the recognition of the biasing words. To enhance TCPGen,
we further propose a novel minimum biasing word error (MBWE) loss that directly
optimises biasing word errors during training, along with a biasing-word-driven
language model discounting (BLMD) method during the test. All contextual ASR
systems were evaluated on the public Librispeech audiobook corpus and the data
from the dialogue state tracking challenges (DSTC) with the biasing lists
extracted from the dialogue-system ontology. Consistent word error rate (WER)
reductions were achieved with TCPGen, which were particularly significant on
the biasing words with around 40\% relative reductions in the recognition error
rates. MBWE and BLMD further improved the effectiveness of TCPGen and achieved
more significant WER reductions on the biasing words. TCPGen also achieved
zero-shot learning of words not in the audio training set with large WER
reductions on the out-of-vocabulary words in the biasing list.",0,1,0,0,0,0,0.374093,7.0,0.661779,64
48801d31-d6e4-4293-8c2e-bdbb05266e8e,Understanding the Domain Gap in LiDAR Object Detection Networks,2,0.0364687,0.049866,"In order to make autonomous driving a reality, artificial neural networks
have to work reliably in the open-world. However, the open-world is vast and
continuously changing, so it is not technically feasible to collect and
annotate training datasets which accurately represent this domain. Therefore,
there are always domain gaps between training datasets and the open-world which
must be understood. In this work, we investigate the domain gaps between
high-resolution and low-resolution LiDAR sensors in object detection networks.
Using a unique dataset, which enables us to study sensor resolution domain gaps
independent of other effects, we show two distinct domain gaps - an inference
domain gap and a training domain gap. The inference domain gap is characterised
by a strong dependence on the number of LiDAR points per object, while the
training gap shows no such dependence. These fndings show that different
approaches are required to close these inference and training domain gaps.",0,1,0,1,0,0,0.496164,6.0,0.668827,15
27a4536a-f0c4-4e06-b8fd-e59a8ebc598d,The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization,41,0.168205,0.670134,"We present a novel benchmark and associated evaluation metrics for assessing
the performance of text anonymization methods. Text anonymization, defined as
the task of editing a text document to prevent the disclosure of personal
information, currently suffers from a shortage of privacy-oriented annotated
text resources, making it difficult to properly evaluate the level of privacy
protection offered by various anonymization methods. This paper presents TAB
(Text Anonymization Benchmark), a new, open-source annotated corpus developed
to address this shortage. The corpus comprises 1,268 English-language court
cases from the European Court of Human Rights (ECHR) enriched with
comprehensive annotations about the personal information appearing in each
document, including their semantic category, identifier type, confidential
attributes, and co-reference relations. Compared to previous work, the TAB
corpus is designed to go beyond traditional de-identification (which is limited
to the detection of predefined semantic categories), and explicitly marks which
text spans ought to be masked in order to conceal the identity of the person to
be protected. Along with presenting the corpus and its annotation layers, we
also propose a set of evaluation metrics that are specifically tailored towards
measuring the performance of text anonymization, both in terms of privacy
protection and utility preservation. We illustrate the use of the benchmark and
the proposed metrics by assessing the empirical performance of several baseline
text anonymization models. The full corpus along with its privacy-oriented
annotation guidelines, evaluation scripts and baseline models are available on:
https://github.com/NorskRegnesentral/text-anonymisation-benchmark",1,1,1,1,0,0,0.00943516,11.0,0.430181,116
291c5d16-2f1f-4e4d-8306-24abb3db4f6d,License Plate Privacy in Collaborative Visual Analysis of Traffic Scenes,3,0.167112,0.241607,"Traffic scene analysis is important for emerging technologies such as smart
traffic management and autonomous vehicles. However, such analysis also poses
potential privacy threats. For example, a system that can recognize license
plates may construct patterns of behavior of the corresponding vehicles' owners
and use that for various illegal purposes. In this paper we present a system
that enables traffic scene analysis while at the same time preserving license
plate privacy. The system is based on a multi-task model whose latent space is
selectively compressed depending on the amount of information the specific
features carry about analysis tasks and private information. Effectiveness of
the proposed method is illustrated by experiments on the Cityscapes dataset,
for which we also provide license plate annotations.",0,1,0,1,0,0,0.485589,9.0,0.775799,25
6d5b8728-2a0b-47c8-ab0f-6638ab6931e1,Instance-based Learning for Knowledge Base Completion,4,0.0748353,0.278136,"In this paper, we propose a new method for knowledge base completion (KBC):
instance-based learning (IBL). For example, to answer (Jill Biden, lived city,?
), instead of going directly to Washington D.C., our goal is to find Joe Biden,
who has the same lived city as Jill Biden. Through prototype entities, IBL
provides interpretability. We develop theories for modeling prototypes and
combining IBL with translational models. Experiments on various tasks confirmed
the IBL model's effectiveness and interpretability.
  In addition, IBL shed light on the mechanism of rule-based KBC models.
Previous research has generally agreed that rule-based models provide rules
with semantically compatible premises and hypotheses. We challenge this view.
We begin by demonstrating that some logical rules represent {\it instance-based
equivalence} (i.e. prototypes) rather than semantic compatibility. These are
denoted as {\it IBL rules}. Surprisingly, despite occupying only a small
portion of the rule space, IBL rules outperform non-IBL rules in all four
benchmarks. We use a variety of experiments to demonstrate that rule-based
models work because they have the ability to represent instance-based
equivalence via IBL rules. The findings provide new insights of how rule-based
models work and how to interpret their rules.",1,0,0,0,0,0,0.950231,12.0,0.957441,38
350860ca-4e7d-4d6f-81c9-95aea8034db8,Planes vs. Chairs: Category-guided 3D shape learning without any 3D cues,9,0.07824,0.328445,"We present a novel 3D shape reconstruction method which learns to predict an
implicit 3D shape representation from a single RGB image. Our approach uses a
set of single-view images of multiple object categories without viewpoint
annotation, forcing the model to learn across multiple object categories
without 3D supervision. To facilitate learning with such minimal supervision,
we use category labels to guide shape learning with a novel categorical metric
learning approach. We also utilize adversarial and viewpoint regularization
techniques to further disentangle the effects of viewpoint and shape. We obtain
the first results for large-scale (more than 50 categories) single-viewpoint
shape prediction using a single model without any 3D cues. We are also the
first to examine and quantify the benefit of class information in single-view
supervised 3D shape reconstruction. Our method achieves superior performance
over state-of-the-art methods on ShapeNet-13, ShapeNet-55 and Pascal3D+.",1,1,0,0,1,0,0.686571,8.0,0.817393,66
a80f2e54-4490-4a02-8209-ebef2968e647,"REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research",18,0.0042405,0.311757,"Transparency around limitations can improve the scientific rigor of research,
help ensure appropriate interpretation of research findings, and make research
claims more credible. Despite these benefits, the machine learning (ML)
research community lacks well-developed norms around disclosing and discussing
limitations. To address this gap, we conduct an iterative design process with
30 ML and ML-adjacent researchers to develop and test REAL ML, a set of guided
activities to help ML researchers recognize, explore, and articulate the
limitations of their research. Using a three-stage interview and survey study,
we identify ML researchers' perceptions of limitations, as well as the
challenges they face when recognizing, exploring, and articulating limitations.
We develop REAL ML to address some of these practical challenges, and highlight
additional cultural challenges that will require broader shifts in community
norms to address. We hope our study and REAL ML help move the ML research
community toward more active and appropriate engagement with limitations.",0,0,0,0,0,1,0.000535269,12.0,0.238174,32
de83b9ca-3372-439c-874f-9299823ba4dc,Reinforcement Learning with Prior Policy Guidance for Motion Planning of Dual-Arm Free-Floating Space Robot,12,0.436021,0.948033,"Reinforcement learning methods as a promising technique have achieved
superior results in the motion planning of free-floating space robots. However,
due to the increase in planning dimension and the intensification of system
dynamics coupling, the motion planning of dual-arm free-floating space robots
remains an open challenge. In particular, the current study cannot handle the
task of capturing a non-cooperative object due to the lack of the pose
constraint of the end-effectors. To address the problem, we propose a novel
algorithm, EfficientLPT, to facilitate RL-based methods to improve planning
accuracy efficiently. Our core contributions are constructing a mixed policy
with prior knowledge guidance and introducing infinite norm to build a more
reasonable reward function. Furthermore, our method successfully captures a
rotating object with different spinning speeds.",0,1,0,0,0,0,0.684514,6.0,0.755581,41
2fc52132-d87a-459c-b87a-6ce5495a5f23,A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots,13,0.466884,0.684959,"A slot value might be provided segment by segment over multiple-turn
interactions in a dialog, especially for some important information such as
phone numbers and names. It is a common phenomenon in daily life, but little
attention has been paid to it in previous work. To fill the gap, this paper
defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds
a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset
includes a total of 40K dialogs and 500K utterances from four different
domains: Chinese names, phone numbers, ID numbers and license plate numbers.
The data is well annotated with sub-slot values, slot values, dialog states and
actions. We find some new linguistic phenomena and interactive manners in SSTOD
which raise critical challenges of building dialog agents for the task. We test
three state-of-the-art dialog models on SSTOD and find they cannot handle the
task well on any of the four domains. We also investigate an improved model by
involving slot knowledge in a plug-in manner. More work should be done to meet
the new challenges raised from SSTOD which widely exists in real-life
applications. The dataset and code are publicly available via
https://github.com/shunjiu/SSTOD.",1,0,1,1,0,0,0.924182,9.0,0.926459,35
09f1ce99-ed5c-4c73-8eb4-ff7c053a9ea8,Comparison of biomedical relationship extraction methods and models for knowledge graph creation,16,0.387878,0.868319,"Biomedical research is growing at such an exponential pace that scientists,
researchers, and practitioners are no more able to cope with the amount of
published literature in the domain. The knowledge presented in the literature
needs to be systematized in such a way that claims and hypotheses can be easily
found, accessed, and validated. Knowledge graphs can provide such a framework
for semantic knowledge representation from literature. However, in order to
build a knowledge graph, it is necessary to extract knowledge as relationships
between biomedical entities and normalize both entities and relationship types.
In this paper, we present and compare few rule-based and machine learning-based
(Naive Bayes, Random Forests as examples of traditional machine learning
methods and DistilBERT, PubMedBERT, T5 and SciFive-based models as examples of
modern deep learning transformers) methods for scalable relationship extraction
from biomedical literature, and for the integration into the knowledge graphs.
We examine how resilient are these various methods to unbalanced and fairly
small datasets. Our experiments show that transformer-based models handle well
both small (due to pre-training on a large dataset) and unbalanced datasets.
The best performing model was the PubMedBERT-based model fine-tuned on balanced
data, with a reported F1-score of 0.92. DistilBERT-based model followed with
F1-score of 0.89, performing faster and with lower resource requirements.
BERT-based models performed better then T5-based generative models.",0,1,0,0,0,0,0.606873,9.0,0.813542,80
335bfdb7-df7d-492b-a80b-eb700742ea44,"Markov categories, causal theories, and the do-calculus",3,0.0296365,0.07146,"We give a category-theoretic treatment of causal models that formalizes the
syntax for causal reasoning over a directed acyclic graph (DAG) by associating
a free Markov category with the DAG in a canonical way. This framework enables
us to define and study important concepts in causal reasoning from an abstract
and ""purely causal"" point of view, such as causal independence/separation,
causal conditionals, and decomposition of intervention effects. Our results
regarding these concepts abstract away from the details of the commonly adopted
causal models such as (recursive) structural equation models or causal Bayesian
networks. They are therefore more widely applicable and in a way conceptually
clearer. Our results are also intimately related to Judea Pearl's celebrated
do-calculus, and yield a syntactic version of a core part of the calculus that
is inherited in all causal models. In particular, it induces a simpler and
specialized version of Pearl's do-calculus in the context of causal Bayesian
networks, which we show is as strong as the full version.",0,0,0,0,0,0,0.0706427,26.0,0.837571,18
aecdf56b-3035-4533-8a44-0e74a8bea6a6,EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing,20,0.0575464,0.418941,"The success of Pre-Trained Models (PTMs) has reshaped the development of
Natural Language Processing (NLP). Yet, it is not easy to obtain
high-performing models and deploy them online for industrial practitioners. To
bridge this gap, EasyNLP is designed to make it easy to build NLP applications,
which supports a comprehensive suite of NLP algorithms. It further features
knowledge-enhanced pre-training, knowledge distillation and few-shot learning
functionalities for large-scale PTMs, and provides a unified framework of model
training, inference and deployment for real-world applications. Currently,
EasyNLP has powered over ten business units within Alibaba Group and is
seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.
The source code of our EasyNLP toolkit is released at GitHub
(https://github.com/alibaba/EasyNLP).",1,1,0,0,0,1,0.662071,6.0,0.745345,37
3c2306a9-354a-4dd0-9037-708ea5bcbd19,Discriminator-Cooperated Feature Map Distillation for GAN Compression,3,0.0348299,0.184085,"Despite excellent performance in image generation, Generative Adversarial
Networks (GANs) are notorious for its requirements of enormous storage and
intensive computation. As an awesome ''performance maker'', knowledge
distillation is demonstrated to be particularly efficacious in exploring
low-priced GANs. In this paper, we investigate the irreplaceability of teacher
discriminator and present an inventive discriminator-cooperated distillation,
abbreviated as DCD, towards refining better feature maps from the generator. In
contrast to conventional pixel-to-pixel match methods in feature map
distillation, our DCD utilizes teacher discriminator as a transformation to
drive intermediate results of the student generator to be perceptually close to
corresponding outputs of the teacher generator. Furthermore, in order to
mitigate mode collapse in GAN compression, we construct a collaborative
adversarial training paradigm where the teacher discriminator is from scratch
established to co-train with student generator in company with our DCD. Our DCD
shows superior results compared with existing GAN compression methods. For
instance, after reducing over 40x MACs and 80x parameters of CycleGAN, we well
decrease FID metric from 61.53 to 48.24 while the current SoTA method merely
has 51.92. This work's source code has been made accessible at
https://github.com/poopit/DCD-official.",1,1,0,0,1,0,0.444086,7.0,0.69401,51
0ba0061e-83bb-4ee5-a3d5-f6770ac62899,Block-Segmentation Vectors for Arousal Prediction using Semi-supervised Learning,1,0.0347259,0.0157475,"To handle emotional expressions in computer applications, Russell's circum-
plex model has been useful for representing emotions according to valence and
arousal. In SentiWordNet, the level of valence is automatically assigned to a
large number of synsets (groups of synonyms in WordNet) using semi-supervised
learning. However, when assigning the level of arousal, the existing method
proposed for SentiWordNet reduces the accuracy of sentiment prediction. In this
paper, we propose a block-segmentation vector for predicting the arousal levels
of many synsets from a small number of labeled words using semi-supervised
learning. We analyze the distribution of arousal and non-arousal words in a
corpus of sentences by comparing it with the distribution of valence words. We
address the problem that arousal level prediction fails when arousal and
non-arousal words are mixed together in some sentences. To capture the features
of such arousal and non-arousal words, we generate word vectors based on
inverted indexes by block IDs, where the corpus is divided into blocks in the
flow of sentences. In the evaluation experiment, we show that the results of
arousal prediction with the block-segmentation vectors outperform the results
of the previous method in SentiWordNet.",0,1,0,0,0,0,0.0365054,37.0,0.867535,19
40fa99f3-e9c1-4dfc-8070-dbc93b6db989,Controllable Dialogue Simulation with In-Context Learning,20,0.415333,0.868075,"Building dialogue systems requires a large corpus of annotated dialogues.
Such datasets are usually created via crowdsourcing, which is expensive and
time-consuming. In this paper, we propose \textsc{Dialogic}, a novel dialogue
simulation method based on large language model in-context learning to automate
dataset creation. Seeded with a few annotated dialogues, \textsc{Dialogic}
automatically selects in-context examples for demonstration and prompts GPT-3
to generate new dialogues and annotations in a controllable way. Our method can
rapidly expand a small set of dialogue data with minimum or zero \textit{human
involvement} and \textit{parameter update} and is thus much more cost-efficient
and time-saving than crowdsourcing. Experimental results on the MultiWOZ
dataset demonstrate that training a model on the simulated dialogues leads to
even better performance than using the same amount of human-generated dialogues
under the challenging low-resource settings, with as few as 85 dialogues as a
seed. When enough data is available, our method can still serve as an effective
data augmentation method. Human evaluation results also show that our simulated
dialogues have near-human fluency and annotation accuracy. The code and data
are available at \textbf{\url{https://github.com/Leezekun/dialogic}}.",1,1,1,1,1,0,0.928389,5.0,0.872003,57
607e0011-d8c0-4d36-835b-c9da7b9bf552,HYPRO: A Hybridly Normalized Probabilistic Model for Long-Horizon Prediction of Event Sequences,19,0.316889,0.559377,"In this paper, we tackle the important yet under-investigated problem of
making long-horizon prediction of event sequences. Existing state-of-the-art
models do not perform well at this task due to their autoregressive structure.
We propose HYPRO, a hybridly normalized probabilistic model that naturally fits
this task: its first part is an autoregressive base model that learns to
propose predictions; its second part is an energy function that learns to
reweight the proposals such that more realistic predictions end up with higher
probabilities. We also propose efficient training and inference algorithms for
this model. Experiments on multiple real-world datasets demonstrate that our
proposed HYPRO model can significantly outperform previous models at making
long-horizon predictions of future events. We also conduct a range of ablation
studies to investigate the effectiveness of each component of our proposed
methods.",1,0,1,0,1,0,0.624929,8.0,0.796377,57
b4b86031-7006-42d4-a97b-4210981c4a41,Online Learning of Reusable Abstract Models for Object Goal Navigation,12,0.469329,0.551848,"In this paper, we present a novel approach to incrementally learn an Abstract
Model of an unknown environment, and show how an agent can reuse the learned
model for tackling the Object Goal Navigation task. The Abstract Model is a
finite state machine in which each state is an abstraction of a state of the
environment, as perceived by the agent in a certain position and orientation.
The perceptions are high-dimensional sensory data (e.g., RGB-D images), and the
abstraction is reached by exploiting image segmentation and the Taskonomy model
bank. The learning of the Abstract Model is accomplished by executing actions,
observing the reached state, and updating the Abstract Model with the acquired
information. The learned models are memorized by the agent, and they are reused
whenever it recognizes to be in an environment that corresponds to the stored
model. We investigate the effectiveness of the proposed approach for the Object
Goal Navigation task, relying on public benchmarks. Our results show that the
reuse of learned Abstract Models can boost performance on Object Goal
Navigation.",0,1,0,0,0,0,0.923635,8.0,0.916917,53
12388b5f-ea2b-4eb6-b04e-0ce1cf3992f8,METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals,29,0.0422637,0.323764,"We present an efficient method of pretraining large-scale autoencoding
language models using training signals generated by an auxiliary model.
Originated in ELECTRA, this training strategy has demonstrated
sample-efficiency to pretrain models at the scale of hundreds of millions of
parameters. In this work, we conduct a comprehensive empirical study, and
propose a recipe, namely ""Model generated dEnoising TRaining Objective""
(METRO), which incorporates some of the best modeling techniques developed
recently to speed up, stabilize, and enhance pretrained language models without
compromising model effectiveness. The resultant models, METRO-LM, consisting of
up to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,
SuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in
that they often outperform previous large models with significantly smaller
model sizes and lower pretraining cost.",0,1,0,0,1,0,0.591366,5.0,0.655912,73
5a37a305-329f-4bf4-8e64-3f00d5f99a62,Identifying Moments of Change from Longitudinal User Text,25,0.0690164,0.763271,"Identifying changes in individuals' behaviour and mood, as observed via
content shared on online platforms, is increasingly gaining importance. Most
research to-date on this topic focuses on either: (a) identifying individuals
at risk or with a certain mental health condition given a batch of posts or (b)
providing equivalent labels at the post level. A disadvantage of such work is
the lack of a strong temporal component and the inability to make longitudinal
assessments following an individual's trajectory and allowing timely
interventions. Here we define a new task, that of identifying moments of change
in individuals on the basis of their shared content online. The changes we
consider are sudden shifts in mood (switches) or gradual mood progression
(escalations). We have created detailed guidelines for capturing moments of
change and a corpus of 500 manually annotated user timelines (18.7K posts). We
have developed a variety of baseline models drawing inspiration from related
tasks and show that the best performance is obtained through context aware
sequential modelling. We also introduce new metrics for capturing rare events
in temporal windows.",0,1,1,1,0,0,0.0945483,8.0,0.510146,64
c8e7935f-a910-4302-a03f-2f1516f925c9,Visual Acoustic Matching,34,0.278793,0.983029,"We introduce the visual acoustic matching task, in which an audio clip is
transformed to sound like it was recorded in a target environment. Given an
image of the target environment and a waveform for the source audio, the goal
is to re-synthesize the audio to match the target room acoustics as suggested
by its visible geometry and materials. To address this novel task, we propose a
cross-modal transformer model that uses audio-visual attention to inject visual
properties into the audio and generate realistic audio output. In addition, we
devise a self-supervised training objective that can learn acoustic matching
from in-the-wild Web videos, despite their lack of acoustically mismatched
audio. We demonstrate that our approach successfully translates human speech to
a variety of real-world environments depicted in images, outperforming both
traditional acoustic matching and more heavily supervised baselines.",0,0,1,0,0,0,0.407178,6.0,0.623687,79
4d3a4386-66e5-4624-950f-5e5a4cbba998,An Empirical Study on Disentanglement of Negative-free Contrastive Learning,6,0.0468768,0.432771,"Negative-free contrastive learning methods have attracted a lot of attention
with simplicity and impressive performances for large-scale pretraining.
However, its disentanglement property remains unexplored. In this paper, we
examine negative-free contrastive learning methods to study the disentanglement
property empirically. We find that existing disentanglement metrics fail to
make meaningful measurements for high-dimensional representation models, so we
propose a new disentanglement metric based on Mutual Information between latent
representations and data factors. With this proposed metric, we benchmark the
disentanglement property of negative-free contrastive learning on both popular
synthetic datasets and a real-world dataset CelebA. Our study shows that the
investigated methods can learn a well-disentangled subset of representation. As
far as we know, we are the first to extend the study of disentangled
representation learning to high-dimensional representation space and introduce
negative-free contrastive learning methods into this area. The source code of
this paper is available at
\url{https://github.com/noahcao/disentanglement_lib_med}.",1,0,0,0,0,0,0.834545,8.0,0.872228,60
4a55587e-8397-44cd-aabb-c88824a43724,pymdp: A Python library for active inference in discrete state spaces,25,0.345271,0.480415,"Active inference is an account of cognition and behavior in complex systems
which brings together action, perception, and learning under the theoretical
mantle of Bayesian inference. Active inference has seen growing applications in
academic research, especially in fields that seek to model human or animal
behavior. While in recent years, some of the code arising from the active
inference literature has been written in open source languages like Python and
Julia, to-date, the most popular software for simulating active inference
agents is the DEM toolbox of SPM, a MATLAB library originally developed for the
statistical analysis and modelling of neuroimaging data. Increasing interest in
active inference, manifested both in terms of sheer number as well as
diversifying applications across scientific disciplines, has thus created a
need for generic, widely-available, and user-friendly code for simulating
active inference in open-source scientific computing languages like Python. The
Python package we present here, pymdp (see
https://github.com/infer-actively/pymdp), represents a significant step in this
direction: namely, we provide the first open-source package for simulating
active inference with partially-observable Markov Decision Processes or POMDPs.
We review the package's structure and explain its advantages like modular
design and customizability, while providing in-text code blocks along the way
to demonstrate how it can be used to build and run active inference processes
with ease. We developed pymdp to increase the accessibility and exposure of the
active inference framework to researchers, engineers, and developers with
diverse disciplinary backgrounds. In the spirit of open-source software, we
also hope that it spurs new innovation, development, and collaboration in the
growing active inference community.",0,1,0,1,0,0,0.463866,6.0,0.652992,68
416d6687-a08a-4556-b592-f2908a031c0f,TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving,137,0.585491,0.999975,"How should we integrate representations from complementary sensors for
autonomous driving? Geometry-based fusion has shown promise for perception
(e.g. object detection, motion forecasting). However, in the context of
end-to-end driving, we find that imitation learning based on existing sensor
fusion methods underperforms in complex driving scenarios with a high density
of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate
image and LiDAR representations using self-attention. Our approach uses
transformer modules at multiple resolutions to fuse perspective view and bird's
eye view feature maps. We experimentally validate its efficacy on a challenging
new benchmark with long routes and dense traffic, as well as the official
leaderboard of the CARLA urban driving simulator. At the time of submission,
TransFuser outperforms all prior work on the CARLA leaderboard in terms of
driving score by a large margin. Compared to geometry-based fusion, TransFuser
reduces the average collisions per kilometer by 48%.",1,1,0,0,1,0,0.435143,5.0,0.566103,134
7937e22a-037b-4359-83df-e2930196f4d4,FisheyeDistill: Self-Supervised Monocular Depth Estimation with Ordinal Distillation for Fisheye Cameras,2,0.0285263,0.150861,"In this paper, we deal with the problem of monocular depth estimation for
fisheye cameras in a self-supervised manner. A known issue of self-supervised
depth estimation is that it suffers in low-light/over-exposure conditions and
in large homogeneous regions. To tackle this issue, we propose a novel ordinal
distillation loss that distills the ordinal information from a large teacher
model. Such a teacher model, since having been trained on a large amount of
diverse data, can capture the depth ordering information well, but lacks in
preserving accurate scene geometry. Combined with self-supervised losses, we
show that our model can not only generate reasonable depth maps in challenging
environments but also better recover the scene geometry. We further leverage
the fisheye cameras of an AR-Glasses device to collect an indoor dataset to
facilitate evaluation.",0,1,0,1,0,0,0.890641,8.0,0.898121,41
6b272198-5c8d-4f2a-8adf-02ebc9685ea0,HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance,18,0.267341,0.802133,"Marker-less monocular 3D human motion capture (MoCap) with scene interactions
is a challenging research topic relevant for extended reality, robotics and
virtual avatar generation. Due to the inherent depth ambiguity of monocular
settings, 3D motions captured with existing methods often contain severe
artefacts such as incorrect body-scene inter-penetrations, jitter and body
floating. To tackle these issues, we propose HULC, a new approach for 3D human
MoCap which is aware of the scene geometry. HULC estimates 3D poses and dense
body-environment surface contacts for improved 3D localisations, as well as the
absolute scale of the subject. Furthermore, we introduce a 3D pose trajectory
optimisation based on a novel pose manifold sampling that resolves erroneous
body-environment inter-penetrations. Although the proposed method requires less
structured inputs compared to existing scene-aware monocular MoCap algorithms,
it produces more physically-plausible poses: HULC significantly and
consistently outperforms the existing approaches in various experiments and on
different metrics. Project page: https://vcai.mpi-inf.mpg.de/projects/HULC/.",0,1,0,1,1,0,0.743121,7.0,0.813918,70
75d76650-a6c8-42ea-b839-80c66b42f4bb,Self-Distribution Distillation: Efficient Uncertainty Estimation,6,0.0265872,0.430207,"Deep learning is increasingly being applied in safety-critical domains. For
these scenarios it is important to know the level of uncertainty in a model's
prediction to ensure appropriate decisions are made by the system. Deep
ensembles are the de-facto standard approach to obtaining various measures of
uncertainty. However, ensembles often significantly increase the resources
required in the training and/or deployment phases. Approaches have been
developed that typically address the costs in one of these phases. In this work
we propose a novel training approach, self-distribution distillation (S2D),
which is able to efficiently train a single model that can estimate
uncertainties. Furthermore it is possible to build ensembles of these models
and apply hierarchical ensemble distillation approaches. Experiments on
CIFAR-100 showed that S2D models outperformed standard models and Monte-Carlo
dropout. Additional out-of-distribution detection experiments on LSUN, Tiny
ImageNet, SVHN showed that even a standard deep ensemble can be outperformed
using S2D based ensembles and novel distilled models.",0,1,0,0,0,0,0.635102,9.0,0.822074,48
24874699-eda5-4a44-ae5f-d3bf28be6195,Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization,23,0.232756,0.776092,"In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D .",1,1,0,0,1,0,0.7004,6.0,0.762883,55
dfdb9f1e-96da-4ce1-957c-acc6364e2307,TrimBERT: Tailoring BERT for Trade-offs,3,0.0131534,0.127162,"Models based on BERT have been extremely successful in solving a variety of
natural language processing (NLP) tasks. Unfortunately, many of these large
models require a great deal of computational resources and/or time for
pre-training and fine-tuning which limits wider adoptability. While
self-attention layers have been well-studied, a strong justification for
inclusion of the intermediate layers which follow them remains missing in the
literature. In this work, we show that reducing the number of intermediate
layers in BERT-Base results in minimal fine-tuning accuracy loss of downstream
tasks while significantly decreasing model size and training time. We further
mitigate two key bottlenecks, by replacing all softmax operations in the
self-attention layers with a computationally simpler alternative and removing
half of all layernorm operations. This further decreases the training time
while maintaining a high level of fine-tuning accuracy.",1,1,0,0,0,0,0.783397,6.0,0.802604,21
648b08dd-6306-4272-b5f7-3cc6d2259d32,ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping,1,0.0408405,0.169812,"In this paper, a novel solution is introduced for visual Simultaneous
Localization and Mapping (vSLAM) that is built up of Deep Learning components.
The proposed architecture is a highly modular framework in which each component
offers state of the art results in their respective fields of vision-based deep
learning solutions. The paper shows that with the synergic integration of these
individual building blocks, a functioning and efficient all-through deep neural
(ATDN) vSLAM system can be created. The Embedding Distance Loss function is
introduced and using it the ATDN architecture is trained. The resulting system
managed to achieve 4.4% translation and 0.0176 deg/m rotational error on a
subset of the KITTI dataset. The proposed architecture can be used for
efficient and low-latency autonomous driving (AD) aiding database creation as
well as a basis for autonomous vehicle (AV) control.",0,1,0,0,1,0,0.809291,12.0,0.907964,45
53f84974-bf35-45cd-b79e-47fc8a34df10,A hybrid quantum image edge detector for the NISQ era,5,0.109032,0.326619,"Edges are image locations where the gray value intensity changes suddenly.
They are among the most important features to understand and segment an image.
Edge detection is a standard task in digital image processing, solved for
example using filtering techniques. However, the amount of data to be processed
grows rapidly and pushes even supercomputers to their limits. Quantum computing
promises exponentially lower memory usage in terms of the number of qubits
compared to the number of classical bits. In this paper, we propose a hybrid
method for quantum edge detection based on the idea of a quantum artificial
neuron. Our method can be practically implemented on quantum computers,
especially on those of the current noisy intermediate-scale quantum era. We
compare six variants of the method to reduce the number of circuits and thus
the time required for the quantum edge detection. Taking advantage of the
scalability of our method, we can practically detect edges in images
considerably larger than reached before.",0,0,0,0,0,0,0.219639,10.0,0.699623,33
5bd025cf-ffcd-42ac-b07a-9aea9f9d6100,Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval,10,0.0108589,0.301088,"State-of-the-art neural (re)rankers are notoriously data-hungry which --
given the lack of large-scale training data in languages other than English --
makes them rarely used in multilingual and cross-lingual retrieval settings.
Current approaches therefore commonly transfer rankers trained on English data
to other languages and cross-lingual setups by means of multilingual encoders:
they fine-tune all parameters of pretrained massively multilingual Transformers
(MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy
them in the target language(s). In this work, we show that two
parameter-efficient approaches to cross-lingual transfer, namely Sparse
Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more
effective zero-shot transfer to multilingual and cross-lingual retrieval tasks.
We first train language adapters (or SFTMs) via Masked Language Modelling and
then train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping
all other parameters fixed. At inference, this modular design allows us to
compose the ranker by applying the (re)ranking adapter (or SFTM) trained with
source language data together with the language adapter (or SFTM) of a target
language. We carry out a large scale evaluation on the CLEF-2003 and HC4
benchmarks and additionally, as another contribution, extend the former with
queries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed
parameter-efficient methods outperform standard zero-shot transfer with full
MMT fine-tuning, while being more modular and reducing training times. The
gains are particularly pronounced for low-resource languages, where our
approaches also substantially outperform the competitive machine
translation-based rankers.",1,1,0,0,0,0,0.466634,3.0,0.308739,69
30ae4fe8-ba8d-419d-ae2a-0e843e6802a7,The Conceptual VAE,3,0.0664905,0.170641,"In this report we present a new model of concepts, based on the framework of
variational autoencoders, which is designed to have attractive properties such
as factored conceptual domains, and at the same time be learnable from data.
The model is inspired by, and closely related to, the Beta-VAE model of
concepts, but is designed to be more closely connected with language, so that
the names of concepts form part of the graphical model. We provide evidence
that our model -- which we call the Conceptual VAE -- is able to learn
interpretable conceptual representations from simple images of coloured shapes
together with the corresponding concept labels. We also show how the model can
be used as a concept classifier, and how it can be adapted to learn from fewer
labels per instance. Finally, we formally relate our model to Gardenfors'
theory of conceptual spaces, showing how the Gaussians we use to represent
concepts can be formalised in terms of ""fuzzy concepts"" in such a space.",0,0,0,0,0,0,0.278715,15.0,0.818128,39
2a0d42b1-6fe7-41fc-be8b-0b352d662c47,CP3: Unifying Point Cloud Completion by Pretrain-Prompt-Predict Paradigm,7,0.0687194,0.288586,"Point cloud completion aims to predict complete shape from its partial
observation. Current approaches mainly consist of generation and refinement
stages in a coarse-to-fine style. However, the generation stage often lacks
robustness to tackle different incomplete variations, while the refinement
stage blindly recovers point clouds without the semantic awareness. To tackle
these challenges, we unify point cloud Completion by a generic
Pretrain-Prompt-Predict paradigm, namely CP3. Inspired by prompting approaches
from NLP, we creatively reinterpret point cloud generation and refinement as
the prompting and predicting stages, respectively. Then, we introduce a concise
self-supervised pretraining stage before prompting. It can effectively increase
robustness of point cloud generation, by an Incompletion-Of-Incompletion (IOI)
pretext task. Moreover, we develop a novel Semantic Conditional Refinement
(SCR) network at the predicting stage. It can discriminatively modulate
multi-scale refinement with the guidance of semantics. Finally, extensive
experiments demonstrate that our CP3 outperforms the state-of-the-art methods
with a large margin.",0,0,0,0,1,0,0.748202,6.0,0.785335,53
6f65ee21-18cf-4333-b61f-d220bfc7f2de,Deep Deformable 3D Caricatures with Learned Shape Control,9,0.187041,0.19438,"A 3D caricature is an exaggerated 3D depiction of a human face. The goal of
this paper is to model the variations of 3D caricatures in a compact parameter
space so that we can provide a useful data-driven toolkit for handling 3D
caricature deformations. To achieve the goal, we propose an MLP-based framework
for building a deformable surface model, which takes a latent code and produces
a 3D surface. In the framework, a SIREN MLP models a function that takes a 3D
position on a fixed template surface and returns a 3D displacement vector for
the input position. We create variations of 3D surfaces by learning a
hypernetwork that takes a latent code and produces the parameters of the MLP.
Once learned, our deformable model provides a nice editing space for 3D
caricatures, supporting label-based semantic editing and point-handle-based
deformation, both of which produce highly exaggerated and natural 3D caricature
shapes. We also demonstrate other applications of our deformable model, such as
automatic 3D caricature creation.",1,0,0,0,0,0,0.722634,8.0,0.829914,53
06f4f94c-63f5-4010-81ed-f665ec546c00,Towards Improving Faithfulness in Abstractive Summarization,14,0.0784668,0.666072,"Despite the success achieved in neural abstractive summarization based on
pre-trained language models, one unresolved issue is that the generated
summaries are not always faithful to the input document. There are two possible
causes of the unfaithfulness problem: (1) the summarization model fails to
understand or capture the gist of the input text, and (2) the model over-relies
on the language model to generate fluent but inadequate words. In this work, we
propose a Faithfulness Enhanced Summarization model (FES), which is designed
for addressing these two problems and improving faithfulness in abstractive
summarization. For the first problem, we propose to use question-answering (QA)
to examine whether the encoder fully grasps the input document and can answer
the questions on the key information in the input. The QA attention on the
proper input words can also be used to stipulate how the decoder should attend
to the source. For the second problem, we introduce a max-margin loss defined
on the difference between the language and the summarization model, aiming to
prevent the overconfidence of the language model. Extensive experiments on two
benchmark summarization datasets, CNN/DM and XSum, demonstrate that our model
significantly outperforms strong baselines. The evaluation of factual
consistency also shows that our model generates more faithful summaries than
baselines.",1,1,0,0,1,0,0.406649,7.0,0.677202,58
d6366757-6b64-41a7-8304-5a9abbc9bd3b,UWC: Unit-wise Calibration Towards Rapid Network Compression,1,0.055094,0.0889593,"This paper introduces a post-training quantization~(PTQ) method achieving
highly efficient Convolutional Neural Network~ (CNN) quantization with high
performance. Previous PTQ methods usually reduce compression error via
performing layer-by-layer parameters calibration. However, with lower
representational ability of extremely compressed parameters (e.g., the
bit-width goes less than 4), it is hard to eliminate all the layer-wise errors.
This work addresses this issue via proposing a unit-wise feature reconstruction
algorithm based on an observation of second order Taylor series expansion of
the unit-wise error. It indicates that leveraging the interaction between
adjacent layers' parameters could compensate layer-wise errors better. In this
paper, we define several adjacent layers as a Basic-Unit, and present a
unit-wise post-training algorithm which can minimize quantization error. This
method achieves near-original accuracy on ImageNet and COCO when quantizing
FP32 models to INT4 and INT3.",0,1,0,0,0,0,0.988338,10.0,0.988378,34
4e77dfe1-2e4f-4fe6-ba23-df78a3e54a93,Generating Compressed Combinatory Proof Structures -- An Approach to Automated First-Order Theorem Proving,5,0.09617,0.312119,"Representing a proof tree by a combinator term that reduces to the tree lets
subtle forms of duplication within the tree materialize as duplicated subterms
of the combinator term. In a DAG representation of the combinator term these
straightforwardly factor into shared subgraphs. To search for proofs,
combinator terms can be enumerated, like clausal tableaux, interwoven with
unification of formulas that are associated with nodes of the enumerated
structures. To restrict the search space, the enumeration can be based on proof
schemas defined as parameterized combinator terms. We introduce here this
""combinator term as proof structure"" approach to automated first-order proving,
present an implementation and first experimental results. The approach builds
on a term view of proof structures rooted in condensed detachment and the
connection method. It realizes features known from the connection structure
calculus, which has not been implemented so far.",0,0,0,0,0,0,1.64669e-05,26.0,0.514477,58
7ca20d89-ae66-415a-bbb5-c29701277e39,Confidence Threshold Neural Diving,1,0.00107612,0.0313719,"Finding a better feasible solution in a shorter time is an integral part of
solving Mixed Integer Programs. We present a post-hoc method based on Neural
Diving to build heuristics more flexibly. We hypothesize that variables with
higher confidence scores are more definite to be included in the optimal
solution. For our hypothesis, we provide empirical evidence that confidence
threshold technique produces partial solutions leading to final solutions with
better primal objective values. Our method won 2nd place in the primal task on
the NeurIPS 2021 ML4CO competition. Also, our method shows the best score among
other learning-based methods in the competition.",0,1,0,0,0,0,0.126976,4.0,0.0984842,16
625c1db0-c15e-4c34-b63c-4e7fcc3f1598,Adaptive Weighted Guided Image Filtering for Depth Enhancement in Shape-From-Focus,8,0.0123031,0.306067,"Existing shape from focus (SFF) techniques cannot preserve depth edges and
fine structural details from a sequence of multi-focus images. Moreover, noise
in the sequence of multi-focus images affects the accuracy of the depth map. In
this paper, a novel depth enhancement algorithm for the SFF based on an
adaptive weighted guided image filtering (AWGIF) is proposed to address the
above issues. The AWGIF is applied to decompose an initial depth map which is
estimated by the traditional SFF into a base layer and a detail layer. In order
to preserve the edges accurately in the refined depth map, the guidance image
is constructed from the multi-focus image sequence, and the coefficient of the
AWGIF is utilized to suppress the noise while enhancing the fine depth details.
Experiments on real and synthetic objects demonstrate the superiority of the
proposed algorithm in terms of anti-noise, and the ability to preserve depth
edges and fine structural details compared to existing methods.",0,1,0,0,0,0,0.00152266,10.0,0.190403,39
060dab27-b589-41c6-917c-71bb0a7feb3c,CNLL: A Semi-supervised Approach For Continual Noisy Label Learning,8,0.0739648,0.421929,"The task of continual learning requires careful design of algorithms that can
tackle catastrophic forgetting. However, the noisy label, which is inevitable
in a real-world scenario, seems to exacerbate the situation. While very few
studies have addressed the issue of continual learning under noisy labels, long
training time and complicated training schemes limit their applications in most
cases. In contrast, we propose a simple purification technique to effectively
cleanse the online data stream that is both cost-effective and more accurate.
After purification, we perform fine-tuning in a semi-supervised fashion that
ensures the participation of all available samples. Training in this fashion
helps us learn a better representation that results in state-of-the-art (SOTA)
performance. Through extensive experimentation on 3 benchmark datasets, MNIST,
CIFAR10 and CIFAR100, we show the effectiveness of our proposed approach. We
achieve a 24.8% performance gain for CIFAR10 with 20% noise over previous SOTA
methods. Our code is publicly available.",1,1,0,0,1,0,0.644128,7.0,0.774744,83
5768fdb7-3d61-43a6-9d1f-efd757828f79,Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words,8,0.10912,0.465183,"The standard BERT adopts subword-based tokenization, which may break a word
into two or more wordpieces (e.g., converting ""lossless"" to ""loss"" and ""less"").
This will bring inconvenience in following situations: (1) what is the best way
to obtain the contextual vector of a word that is divided into multiple
wordpieces? (2) how to predict a word via cloze test without knowing the number
of wordpieces in advance? In this work, we explore the possibility of
developing BERT-style pretrained model over a vocabulary of words instead of
wordpieces. We call such word-level BERT model as WordBERT. We train models
with different vocabulary sizes, initialization configurations and languages.
Results show that, compared to standard wordpiece-based BERT, WordBERT makes
significant improvements on cloze test and machine reading comprehension. On
many other natural language understanding tasks, including POS tagging,
chunking and NER, WordBERT consistently performs better than BERT. Model
analysis indicates that the major advantage of WordBERT over BERT lies in the
understanding for low-frequency words and rare words. Furthermore, since the
pipeline is language-independent, we train WordBERT for Chinese language and
obtain significant gains on five natural language understanding datasets.
Lastly, the analyse on inference speed illustrates WordBERT has comparable time
cost to BERT in natural language understanding tasks.",0,0,1,0,0,1,0.882336,9.0,0.905704,26
17f88e41-72e1-472c-981a-fb019ca41dd0,Contrastive Learning of Coarse-Grained Force Fields,12,0.249657,0.644812,"Coarse-grained models have proven helpful for simulating complex systems over
long timescales to provide molecular insights into various processes.
Methodologies for systematic parameterization of the underlying energy
function, or force field that describes the interactions among different
components of the system are of great interest for ensuring simulation
accuracy. We present a new method, potential contrasting, to enable efficient
learning of force fields that can accurately reproduce the conformational
distribution produced with all-atom simulations. Potential contrasting
generalizes the noise contrastive estimation method with umbrella sampling to
better learn the complex energy landscape of molecular systems. When applied to
the Trp-cage protein, we found that the technique produces force fields that
thoroughly capture the thermodynamics of the folding process despite the use of
only $\alpha$-Carbons in the coarse-grained model. We further showed that
potential contrasting could be applied over large datasets that combine the
conformational ensembles of many proteins to ensure the transferability of
coarse-grained force fields. We anticipate potential contrasting to be a
powerful tool for building general-purpose coarse-grained force fields.",0,0,0,0,0,0,0.194646,19.0,0.834751,77
434bf510-0361-43cc-be3a-e61a696386c4,Towards Stroke Patients' Upper-limb Automatic Motor Assessment Using Smartwatches,5,0.153957,0.853605,"Assessing the physical condition in rehabilitation scenarios is a challenging
problem, since it involves Human Activity Recognition (HAR) and kinematic
analysis methods. In addition, the difficulties increase in unconstrained
rehabilitation scenarios, which are much closer to the real use cases. In
particular, our aim is to design an upper-limb assessment pipeline for stroke
patients using smartwatches. We focus on the HAR task, as it is the first part
of the assessing pipeline. Our main target is to automatically detect and
recognize four key movements inspired by the Fugl-Meyer assessment scale, which
are performed in both constrained and unconstrained scenarios. In addition to
the application protocol and dataset, we propose two detection and
classification baseline methods. We believe that the proposed framework,
dataset and baseline results will serve to foster this research field.",1,1,0,1,0,0,0.233799,10.0,0.706747,38
dc3f3494-f9de-4b98-8a8c-63b8deae8bbc,Visualize Before You Write: Imagination-Guided Open-Ended Text Generation,27,0.105236,0.394969,"Recent advances in text-to-image synthesis make it possible to visualize
machine imaginations for a given context. On the other hand, when generating
text, human writers are gifted at creative visualization, which enhances their
writings by forming imaginations as blueprints before putting down the stories
in words. Inspired by such a cognitive process, we ask the natural question of
whether we can endow machines with the same ability to utilize visual
information and construct a general picture of the context to guide text
generation. In this work, we propose iNLG that uses machine-generated images to
guide language models in open-ended text generation. The experiments and
analyses demonstrate the effectiveness of iNLG on open-ended text generation
tasks, including text completion, story generation, and concept-to-text
generation in both few-shot and full-data scenarios. Both automatic metrics and
human evaluations verify that the text snippets generated by our iNLG are
coherent and informative while displaying minor degeneration.",1,0,0,0,0,1,0.500527,7.0,0.717938,87
3296f199-df92-4cd9-a52b-32bc8e04d42f,A Streamlit-based Artificial Intelligence Trust Platform for Next-Generation Wireless Networks,3,0.0281452,0.527545,"With the rapid development and integration of artificial intelligence (AI)
methods in next-generation networks (NextG), AI algorithms have provided
significant advantages for NextG in terms of frequency spectrum usage,
bandwidth, latency, and security. A key feature of NextG is the integration of
AI, i.e., self-learning architecture based on self-supervised algorithms, to
improve the performance of the network. A secure AI-powered structure is also
expected to protect NextG networks against cyber-attacks. However, AI itself
may be attacked, i.e., model poisoning targeted by attackers, and it results in
cybersecurity violations. This paper proposes an AI trust platform using
Streamlit for NextG networks that allows researchers to evaluate, defend,
certify, and verify their AI models and applications against adversarial
threats of evasion, poisoning, extraction, and interference.",1,1,0,0,0,0,0.393,6.0,0.615978,15
8569c238-795e-4667-9111-7aaa6fdd555c,CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification,66,0.525253,0.724332,"Existing computer vision research in artwork struggles with artwork's
fine-grained attributes recognition and lack of curated annotated datasets due
to their costly creation. To the best of our knowledge, we are one of the first
methods to use CLIP (Contrastive Language-Image Pre-Training) to train a neural
network on a variety of artwork images and text descriptions pairs. CLIP is
able to learn directly from free-form art descriptions, or, if available,
curated fine-grained labels. Model's zero-shot capability allows predicting
accurate natural language description for a given image, without directly
optimizing for the task. Our approach aims to solve 2 challenges: instance
retrieval and fine-grained artwork attribute recognition. We use the iMet
Dataset, which we consider the largest annotated artwork dataset. In this
benchmark we achieved competitive results using only self-supervision.",1,1,0,0,0,0,0.941781,5.0,0.887122,31
631567b8-5db0-4346-a3f6-ae35e15a35c6,Retrieval of surgical phase transitions using reinforcement learning,6,0.0553446,0.696355,"In minimally invasive surgery, surgical workflow segmentation from video
analysis is a well studied topic. The conventional approach defines it as a
multi-class classification problem, where individual video frames are
attributed a surgical phase label.
  We introduce a novel reinforcement learning formulation for offline phase
transition retrieval. Instead of attempting to classify every video frame, we
identify the timestamp of each phase transition. By construction, our model
does not produce spurious and noisy phase transitions, but contiguous phase
blocks. We investigate two different configurations of this model. The first
does not require processing all frames in a video (only <60% and <20% of frames
in 2 different applications), while producing results slightly under the
state-of-the-art accuracy. The second configuration processes all video frames,
and outperforms the state-of-the art at a comparable computational cost.
  We compare our method against the recent top-performing frame-based
approaches TeCNO and Trans-SVNet on the public dataset Cholec80 and also on an
in-house dataset of laparoscopic sacrocolpopexy. We perform both a frame-based
(accuracy, precision, recall and F1-score) and an event-based (event ratio)
evaluation of our algorithms.",0,1,0,0,1,0,0.227696,6.0,0.506204,21
4ca5cc6a-b595-4b01-b533-9674bb8f80f5,Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation,5,0.0882325,0.221472,"Differentiable planning promises end-to-end differentiability and adaptivity.
However, an issue prevents it from scaling up to larger-scale problems: they
need to differentiate through forward iteration layers to compute gradients,
which couples forward computation and backpropagation, and needs to balance
forward planner performance and computational cost of the backward pass. To
alleviate this issue, we propose to differentiate through the Bellman
fixed-point equation to decouple forward and backward passes for Value
Iteration Network and its variants, which enables constant backward cost (in
planning horizon) and flexible forward budget and helps scale up to large
tasks. We study the convergence stability, scalability, and efficiency of the
proposed implicit version of VIN and its variants and demonstrate their
superiorities on a range of planning tasks: 2D navigation, visual navigation,
and 2-DOF manipulation in configuration space and workspace.",0,0,0,0,0,0,0.586974,7.0,0.752506,43
5b2814ec-8541-4418-ab2c-939f29266d26,Sparse Adversarial Attack in Multi-agent Reinforcement Learning,8,0.0425496,0.420824,"Cooperative multi-agent reinforcement learning (cMARL) has many real
applications, but the policy trained by existing cMARL algorithms is not robust
enough when deployed. There exist also many methods about adversarial attacks
on the RL system, which implies that the RL system can suffer from adversarial
attacks, but most of them focused on single agent RL. In this paper, we propose
a \textit{sparse adversarial attack} on cMARL systems. We use (MA)RL with
regularization to train the attack policy. Our experiments show that the policy
trained by the current cMARL algorithm can obtain poor performance when only
one or a few agents in the team (e.g., 1 of 8 or 5 of 25) were attacked at a
few timesteps (e.g., attack 3 of total 40 timesteps).",0,1,0,0,0,0,0.197157,6.0,0.479099,37
74cdc112-1d34-4400-970d-fdd01e3c1a8f,Font Shape-to-Impression Translation,3,0.149983,0.42676,"Different fonts have different impressions, such as elegant, scary, and cool.
This paper tackles part-based shape-impression analysis based on the
Transformer architecture, which is able to handle the correlation among local
parts by its self-attention mechanism. This ability will reveal how
combinations of local parts realize a specific impression of a font. The
versatility of Transformer allows us to realize two very different approaches
for the analysis, i.e., multi-label classification and translation. A
quantitative evaluation shows that our Transformer-based approaches estimate
the font impressions from a set of local parts more accurately than other
approaches. A qualitative evaluation then indicates the important local parts
for a specific impression.",0,0,0,0,0,0,0.795846,9.0,0.872621,23
a5734241-c045-4bf6-8de7-ef4813bf5386,Prototype-Based Layered Federated Cross-Modal Hashing,2,0.133262,0.26788,"Recently, deep cross-modal hashing has gained increasing attention. However,
in many practical cases, data are distributed and cannot be collected due to
privacy concerns, which greatly reduces the cross-modal hashing performance on
each client. And due to the problems of statistical heterogeneity, model
heterogeneity, and forcing each client to accept the same parameters, applying
federated learning to cross-modal hash learning becomes very tricky. In this
paper, we propose a novel method called prototype-based layered federated
cross-modal hashing. Specifically, the prototype is introduced to learn the
similarity between instances and classes on server, reducing the impact of
statistical heterogeneity (non-IID) on different clients. And we monitor the
distance between local and global prototypes to further improve the
performance. To realize personalized federated learning, a hypernetwork is
deployed on server to dynamically update different layers' weights of local
model. Experimental results on benchmark datasets show that our method
outperforms state-of-the-art methods.",0,1,0,0,1,0,0.870797,5.0,0.821326,29
b758f144-b640-4aef-bf8a-cd54f21c181b,Universal Adaptive Data Augmentation,4,0.0904511,0.371904,"Existing automatic data augmentation (DA) methods either ignore updating DA's
parameters according to the target model's state during training or adopt
update strategies that are not effective enough. In this work, we design a
novel data augmentation strategy called ""Universal Adaptive Data Augmentation""
(UADA). Different from existing methods, UADA would adaptively update DA's
parameters according to the target model's gradient information during
training: given a pre-defined set of DA operations, we randomly decide types
and magnitudes of DA operations for every data batch during training, and
adaptively update DA's parameters along the gradient direction of the loss
concerning DA's parameters. In this way, UADA can increase the training loss of
the target networks, and the target networks would learn features from harder
samples to improve the generalization. Moreover, UADA is very general and can
be utilized in numerous tasks, e.g., image classification, semantic
segmentation and object detection. Extensive experiments with various models
are conducted on CIFAR-10, CIFAR-100, ImageNet, tiny-ImageNet, Cityscapes, and
VOC07+12 to prove the significant performance improvements brought by UADA.",0,1,0,0,1,1,0.937311,7.0,0.915608,39
64869df1-2e5f-4b96-bdb2-fa38dcf5dc10,Considerations for meaningful sign language machine translation based on glosses,16,0.547467,0.678136,"Automatic sign language processing is gaining popularity in Natural Language
Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in
particular, sign language translation based on glosses is a prominent approach.
In this paper, we review recent works on neural gloss translation. We find that
limitations of glosses in general and limitations of specific datasets are not
discussed in a transparent manner and that there is no common standard for
evaluation.
  To address these issues, we put forward concrete recommendations for future
research on gloss translation. Our suggestions advocate awareness of the
inherent limitations of gloss-based approaches, realistic datasets, stronger
baselines and convincing evaluation.",0,0,0,0,0,0,0.74825,6.0,0.785359,43
24a64b6c-e196-45a0-81b7-66990166e8a4,Revisiting Neural Scaling Laws in Language and Vision,46,0.0630439,0.722463,"The remarkable progress in deep learning in recent years is largely driven by
improvements in scale, where bigger models are trained on larger datasets for
longer schedules. To predict the benefit of scale empirically, we argue for a
more rigorous methodology based on the extrapolation loss, instead of reporting
the best-fitting (interpolating) parameters. We then present a recipe for
estimating scaling law parameters reliably from learning curves. We demonstrate
that it extrapolates more accurately than previous methods in a wide range of
architecture families across several domains, including image classification,
neural machine translation (NMT) and language modeling, in addition to tasks
from the BIG-Bench evaluation benchmark. Finally, we release a benchmark
dataset comprising of 90 evaluation tasks to facilitate research in this
domain.",1,0,0,0,0,1,0.332132,5.0,0.496679,50
57859df6-455c-4b9c-a049-68e9fc3fe8cf,VRKitchen2.0-IndoorKit: A Tutorial for Augmented Indoor Scene Building in Omniverse,2,0.0243866,0.206621,"With the recent progress of simulations by 3D modeling software and game
engines, many researchers have focused on Embodied AI tasks in the virtual
environment. However, the research community lacks a platform that can easily
serve both indoor scene synthesis and model benchmarking with various
algorithms. Meanwhile, computer graphics-related tasks need a toolkit for
implementing advanced synthesizing techniques. To facilitate the study of
indoor scene building methods and their potential robotics applications, we
introduce INDOORKIT: a built-in toolkit for NVIDIA OMNIVERSE that provides
flexible pipelines for indoor scene building, scene randomizing, and animation
controls. Besides, combining Python coding in the animation software INDOORKIT
assists researchers in creating real-time training and controlling avatars and
robotics. The source code for this toolkit is available at
https://github.com/realvcla/VRKitchen2.0-Tutorial, and the tutorial along with
the toolkit is available at https://vrkitchen20-tutorial.readthedocs.io/en/",1,1,0,0,0,0,0.824321,5.0,0.788785,11
d3563bed-29b3-49c2-8b1a-1c9f13c6700a,AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift,7,0.0279516,0.221109,"We introduce AmsterTime: a challenging dataset to benchmark visual place
recognition (VPR) in presence of a severe domain shift. AmsterTime offers a
collection of 2,500 well-curated images matching the same scene from a street
view matched to historical archival image data from Amsterdam city. The image
pairs capture the same place with different cameras, viewpoints, and
appearances. Unlike existing benchmark datasets, AmsterTime is directly
crowdsourced in a GIS navigation platform (Mapillary). We evaluate various
baselines, including non-learning, supervised and self-supervised methods,
pre-trained on different relevant datasets, for both verification and retrieval
tasks. Our result credits the best accuracy to the ResNet-101 model pre-trained
on the Landmarks dataset for both verification and retrieval tasks by 84% and
24%, respectively. Additionally, a subset of Amsterdam landmarks is collected
for feature evaluation in a classification task. Classification labels are
further used to extract the visual explanations using Grad-CAM for inspection
of the learned similar visuals in a deep metric learning models.",0,1,1,1,1,0,0.184868,12.0,0.733576,35
3ff32280-a97e-47c3-8526-2a6325dcf297,IDEA: Increasing Text Diversity via Online Multi-Label Recognition for Vision-Language Pre-training,8,0.0715145,0.367706,"Vision-Language Pre-training (VLP) with large-scale image-text pairs has
demonstrated superior performance in various fields. However, the image-text
pairs co-occurrent on the Internet typically lack explicit alignment
information, which is suboptimal for VLP. Existing methods proposed to adopt an
off-the-shelf object detector to utilize additional image tag information.
However, the object detector is time-consuming and can only identify the
pre-defined object categories, limiting the model capacity. Inspired by the
observation that the texts incorporate incomplete fine-grained image
information, we introduce IDEA, which stands for increasing text diversity via
online multi-label recognition for VLP. IDEA shows that multi-label learning
with image tags extracted from the texts can be jointly optimized during VLP.
Moreover, IDEA can identify valuable image tags online to provide more explicit
textual supervision. Comprehensive experiments demonstrate that IDEA can
significantly boost the performance on multiple downstream datasets with a
small extra computational cost.",1,1,0,0,0,0,0.936824,5.0,0.881291,54
e0424d87-1a38-43c5-8dd6-ea32ff7849a5,Does Wikidata Support Analogical Reasoning?,2,0.0454848,0.0460714,"Analogical reasoning methods have been built over various resources,
including commonsense knowledge bases, lexical resources, language models, or
their combination. While the wide coverage of knowledge about entities and
events make Wikidata a promising resource for analogical reasoning across
situations and domains, Wikidata has not been employed for this task yet. In
this paper, we investigate whether the knowledge in Wikidata supports
analogical reasoning. Specifically, we study whether relational knowledge is
modeled consistently in Wikidata, observing that relevant relational
information is typically missing or modeled in an inconsistent way. Our further
experiments show that Wikidata can be used to create data for analogy
classification, but this requires much manual effort. To facilitate future work
that can support analogies, we discuss key desiderata, and devise a set of
metrics to guide an automatic method for extracting analogies from Wikidata.",0,0,0,0,0,0,0.448669,7.0,0.696011,30
25c9b681-042d-44c3-aded-d0af11c78695,uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers,5,0.20373,0.750648,"The task of Chinese Spelling Check (CSC) is aiming to detect and correct
spelling errors that can be found in the text. While manually annotating a
high-quality dataset is expensive and time-consuming, thus the scale of the
training dataset is usually very small (e.g., SIGHAN15 only contains 2339
samples for training), therefore supervised-learning based models usually
suffer the data sparsity limitation and over-fitting issue, especially in the
era of big language models. In this paper, we are dedicated to investigating
the \textbf{unsupervised} paradigm to address the CSC problem and we propose a
framework named \textbf{uChecker} to conduct unsupervised spelling error
detection and correction. Masked pretrained language models such as BERT are
introduced as the backbone model considering their powerful language diagnosis
capability. Benefiting from the various and flexible MASKing operations, we
propose a Confusionset-guided masking strategy to fine-train the masked
language model to further improve the performance of unsupervised detection and
correction. Experimental results on standard datasets demonstrate the
effectiveness of our proposed model uChecker in terms of character-level and
sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of spelling
error detection and correction respectively.",0,0,0,0,0,0,0.859757,5.0,0.813148,34
f24147dd-8817-45a1-a1b8-38d0f81500bb,"Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",11,0.181247,0.672362,"As natural language processing systems become more widespread, it is
necessary to address fairness issues in their implementation and deployment to
ensure that their negative impacts on society are understood and minimized.
However, there is limited work that studies fairness using a multilingual and
intersectional framework or on downstream tasks. In this paper, we introduce
four multilingual Equity Evaluation Corpora, supplementary test sets designed
to measure social biases, and a novel statistical framework for studying
unisectional and intersectional social biases in natural language processing.
We use these tools to measure gender, racial, ethnic, and intersectional social
biases across five models trained on emotion regression tasks in English,
Spanish, and Arabic. We find that many systems demonstrate statistically
significant unisectional and intersectional social biases.",1,0,1,1,0,0,0.946021,8.0,0.932732,37
37499101-bd78-451c-9b3b-ca60d45f9d30,Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions,19,0.0548027,0.752118,"Role-oriented dialogue summarization is to generate summaries for different
roles in the dialogue, e.g., merchants and consumers. Existing methods handle
this task by summarizing each role's content separately and thus are prone to
ignore the information from other roles. However, we believe that other roles'
content could benefit the quality of summaries, such as the omitted information
mentioned by other roles. Therefore, we propose a novel role interaction
enhanced method for role-oriented dialogue summarization. It adopts cross
attention and decoder self-attention interactions to interactively acquire
other roles' critical information. The cross attention interaction aims to
select other roles' critical dialogue utterances, while the decoder
self-attention interaction aims to obtain key information from other roles'
summaries. Experimental results have shown that our proposed method
significantly outperforms strong baselines on two public role-oriented dialogue
summarization datasets. Extensive analyses have demonstrated that other roles'
content could help generate summaries with more complete semantics and correct
topic structures.",1,1,0,0,1,0,0.142823,6.0,0.420079,37
5dd54e45-196d-41b0-9a5e-af0ba9fa2098,Spatial-temporal Concept based Explanation of 3D ConvNets,3,0.0235525,0.0513935,"Recent studies have achieved outstanding success in explaining 2D image
recognition ConvNets. On the other hand, due to the computation cost and
complexity of video data, the explanation of 3D video recognition ConvNets is
relatively less studied. In this paper, we present a 3D ACE (Automatic
Concept-based Explanation) framework for interpreting 3D ConvNets. In our
approach: (1) videos are represented using high-level supervoxels, which is
straightforward for human to understand; and (2) the interpreting framework
estimates a score for each voxel, which reflects its importance in the decision
procedure. Experiments show that our method can discover spatial-temporal
concepts of different importance-levels, and thus can explore the influence of
the concepts on a target task, such as action classification, in-depth. The
codes are publicly available.",1,0,0,0,0,0,0.383695,10.0,0.766492,45
01cafb4a-650d-4a69-a622-5e0f554d1053,Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness,15,0.457243,0.56065,"With Artificial intelligence (AI) to aid or automate decision-making
advancing rapidly, a particular concern is its fairness. In order to create
reliable, safe and trustworthy systems through human-centred artificial
intelligence (HCAI) design, recent efforts have produced user interfaces (UIs)
for AI experts to investigate the fairness of AI models. In this work, we
provide a design space exploration that supports not only data scientists but
also domain experts to investigate AI fairness. Using loan applications as an
example, we held a series of workshops with loan officers and data scientists
to elicit their requirements. We instantiated these requirements into FairHIL,
a UI to support human-in-the-loop fairness investigations, and describe how
this UI could be generalized to other use cases. We evaluated FairHIL through a
think-aloud user study. Our work contributes better designs to investigate an
AI model's fairness-and move closer towards responsible AI.",0,1,0,0,0,0,0.959634,10.0,0.955676,77
021d1f74-3bdc-405f-ae3c-54e91fe348d8,The Role of ImageNet Classes in Frchet Inception Distance,125,0.961125,0.99988,"Fr\'echet Inception Distance (FID) is the primary metric for ranking models
in data-driven generative modeling. While remarkably successful, the metric is
known to sometimes disagree with human judgement. We investigate a root cause
of these discrepancies, and visualize what FID ""looks at"" in generated images.
We show that the feature space that FID is (typically) computed in is so close
to the ImageNet classifications that aligning the histograms of Top-$N$
classifications between sets of generated and real images can reduce FID
substantially -- without actually improving the quality of results. Thus, we
conclude that FID is prone to intentional or accidental distortions. As a
practical example of an accidental distortion, we discuss a case where an
ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while
being worse in terms of human evaluation.",1,0,0,0,0,0,0.965772,6.0,0.934479,72
880d4ad1-f7ae-4521-99a8-6b030aeaa132,C3KG: A Chinese Commonsense Conversation Knowledge Graph,10,0.0691488,0.411014,"Existing commonsense knowledge bases often organize tuples in an isolated
manner, which is deficient for commonsense conversational models to plan the
next steps. To fill the gap, we curate a large-scale multi-turn human-written
conversation corpus, and create the first Chinese commonsense conversation
knowledge graph which incorporates both social commonsense knowledge and dialog
flow information. To show the potential of our graph, we develop a
graph-conversation matching approach, and benchmark two graph-grounded
conversational tasks.",1,0,1,1,0,0,0.276327,6.0,0.543624,35
3eaa0c02-47ae-4d90-9aa7-e234393b6817,Identifying Electrocardiogram Abnormalities Using a Handcrafted-Rule-Enhanced Neural Network,5,0.0573161,0.132401,"A large number of people suffer from life-threatening cardiac abnormalities,
and electrocardiogram (ECG) analysis is beneficial to determining whether an
individual is at risk of such abnormalities. Automatic ECG classification
methods, especially the deep learning based ones, have been proposed to detect
cardiac abnormalities using ECG records, showing good potential to improve
clinical diagnosis and help early prevention of cardiovascular diseases.
However, the predictions of the known neural networks still do not
satisfactorily meet the needs of clinicians, and this phenomenon suggests that
some information used in clinical diagnosis may not be well captured and
utilized by these methods. In this paper, we introduce some rules into
convolutional neural networks, which help present clinical knowledge to deep
learning based ECG analysis, in order to improve automated ECG diagnosis
performance. Specifically, we propose a Handcrafted-Rule-enhanced Neural
Network (called HRNN) for ECG classification with standard 12-lead ECG input,
which consists of a rule inference module and a deep learning module.
Experiments on two large-scale public ECG datasets show that our new approach
considerably outperforms existing state-of-the-art methods. Further, our
proposed approach not only can improve the diagnosis performance, but also can
assist in detecting mislabelled ECG samples. Our codes are available at
https://github.com/alwaysbyx/ecg_processing.",1,1,0,0,1,0,0.333512,7.0,0.641214,47
62e57313-b0e6-44fb-8f44-0d9f5dd1f48c,Online Hybrid Lightweight Representations Learning: Its Application to Visual Tracking,2,0.0400691,0.11996,"This paper presents a novel hybrid representation learning framework for
streaming data, where an image frame in a video is modeled by an ensemble of
two distinct deep neural networks; one is a low-bit quantized network and the
other is a lightweight full-precision network. The former learns coarse primary
information with low cost while the latter conveys residual information for
high fidelity to original representations. The proposed parallel architecture
is effective to maintain complementary information since fixed-point arithmetic
can be utilized in the quantized network and the lightweight model provides
precise representations given by a compact channel-pruned network. We
incorporate the hybrid representation technique into an online visual tracking
task, where deep neural networks need to handle temporal variations of target
appearances in real-time. Compared to the state-of-the-art real-time trackers
based on conventional deep neural networks, our tracking algorithm demonstrates
competitive accuracy on the standard benchmarks with a small fraction of
computational cost and memory footprint.",0,1,0,0,1,0,0.882369,8.0,0.893933,30
704c7e5d-e37d-4eeb-a1f5-c5adbe253e83,Self-consistent Reasoning For Solving Math Word Problems,5,0.126482,0.451123,"Math word problems (MWPs) is a task that automatically derives solution
expression from a giving math problems in text. The previous studies suffer
from spurious correlations between input text and output expression. To
mitigate this issue, we propose a self-consistent reasoning framework called
SCR, which attempts to adopt a pruning strategy to correct the output
distribution shift so as to implicitly fix those spurious correlative samples.
Specifically, we firstly obtain a sub-network by pruning a roberta2tree model,
for the sake to use the gap on output distribution between the original
roberta2tree model and the pruned sub-network to expose spurious correlative
samples. Then, we calibrate the output distribution shift by applying symmetric
Kullback-Leibler divergence to alleviate spurious correlations. In addition,
SCR generates equivalent expressions, thereby, capturing the original text's
logic rather than relying on hints from original text. Extensive experiments on
two large-scale benchmarks demonstrate that our model substantially outperforms
the strong baseline methods.",0,1,0,0,1,0,0.665885,4.0,0.620619,35
3847c8e5-eb76-47fa-9d82-76268737de53,Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging,21,0.125924,0.956756,"Training vision or language models on large datasets can take days, if not
weeks. We show that averaging the weights of the k latest checkpoints, each
collected at the end of an epoch, can speed up the training progression in
terms of loss and accuracy by dozens of epochs, corresponding to time savings
up to ~68 and ~30 GPU hours when training a ResNet50 on ImageNet and
RoBERTa-Base model on WikiText-103, respectively. We also provide the code and
model checkpoint trajectory to reproduce the results and facilitate research on
reusing historical weights for faster convergence.",0,1,0,0,0,1,0.37901,9.0,0.738794,42
cfcc793a-4c06-4dc4-a813-131191fbcdfc,SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes,5,0.0436871,0.124992,"Is it possible to train a general metric for evaluating text generation
quality without human annotated ratings? Existing learned metrics either
perform unsatisfactorily across text generation tasks or require human ratings
for training on specific tasks. In this paper, we propose SESCORE2, a
self-supervised approach for training a model-based metric for text generation
evaluation. The key concept is to synthesize realistic model mistakes by
perturbing sentences retrieved from a corpus. The primary advantage of the
SESCORE2 is its ease of extension to many other languages while providing
reliable severity estimation. We evaluate SESCORE2 and previous methods on four
text generation tasks across three languages. SESCORE2 outperforms unsupervised
metric PRISM on four text generation evaluation benchmarks, with a Kendall
improvement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised
BLEURT and COMET on multiple text generation tasks. The code and data are
available at https://github.com/xu1998hz/SEScore2.",1,1,0,0,1,1,0.525949,7.0,0.728303,51
e538b5d1-44b4-49c1-9e02-721ba6ee4a8c,Testing predictive automated driving systems: lessons learned and future recommendations,9,0.235309,0.638137,"Conventional vehicles are certified through classical approaches, where
different physical certification tests are set up on test tracks to assess
required safety levels. These approaches are well suited for vehicles with
limited complexity and limited interactions with other entities as last-second
resources. However, these approaches do not allow to evaluate safety with real
behaviors for critical and edge cases, nor to evaluate the ability to
anticipate them in the mid or long term. This is particularly relevant for
automated and autonomous driving functions that make use of advanced predictive
systems to anticipate future actions and motions to be considered in the path
planning layer. In this paper, we present and analyze the results of physical
tests on proving grounds of several predictive systems in automated driving
functions developed within the framework of the BRAVE project. Based on our
experience in testing predictive automated driving functions, we identify the
main limitations of current physical testing approaches when dealing with
predictive systems, analyze the main challenges ahead, and provide a set of
practical actions and recommendations to consider in future physical testing
procedures for automated and autonomous driving functions.",0,1,0,0,0,0,0.245982,8.0,0.64074,46
5e9a6c65-76fd-44c7-b4d4-d7cf3d6b23df,Real Time Egocentric Segmentation for Video-self Avatar in Mixed Reality,5,0.0779334,0.6145,"In this work we present our real-time egocentric body segmentation algorithm.
Our algorithm achieves a frame rate of 66 fps for an input resolution of
640x480, thanks to our shallow network inspired in Thundernet's architecture.
Besides, we put a strong emphasis on the variability of the training data. More
concretely, we describe the creation process of our Egocentric Bodies
(EgoBodies) dataset, composed of almost 10,000 images from three datasets,
created both from synthetic methods and real capturing. We conduct experiments
to understand the contribution of the individual datasets; compare Thundernet
model trained with EgoBodies with simpler and more complex previous approaches
and discuss their corresponding performance in a real-life setup in terms of
segmentation quality and inference times. The described trained semantic
segmentation algorithm is already integrated in an end-to-end system for Mixed
Reality (MR), making it possible for users to see his/her own body while being
immersed in a MR scene.",0,1,0,1,0,0,0.233239,8.0,0.63309,23
921d8a03-0468-409c-a87c-107023621a1a,Tensor4D : Efficient Neural 4D Decomposition for High-fidelity Dynamic Reconstruction and Rendering,81,0.595492,1.0,"We present Tensor4D, an efficient yet effective approach to dynamic scene
modeling. The key of our solution is an efficient 4D tensor decomposition
method so that the dynamic scene can be directly represented as a 4D
spatio-temporal tensor. To tackle the accompanying memory issue, we decompose
the 4D tensor hierarchically by projecting it first into three time-aware
volumes and then nine compact feature planes. In this way, spatial information
over time can be simultaneously captured in a compact and memory-efficient
manner. When applying Tensor4D for dynamic scene reconstruction and rendering,
we further factorize the 4D fields to different scales in the sense that
structural motions and dynamic detailed changes can be learned from coarse to
fine. The effectiveness of our method is validated on both synthetic and
real-world scenes. Extensive experiments show that our method is able to
achieve high-quality dynamic reconstruction and rendering from sparse-view
camera rigs or even a monocular camera. The code and dataset will be released
at https://liuyebin.com/tensor4d/tensor4d.html.",0,1,0,0,0,0,0.81003,4.0,0.724478,80
7ae58a86-14dd-4af0-8ab5-877bb86e67ce,Empowering Graph Representation Learning with Test-Time Graph Transformation,33,0.263764,0.967751,"As powerful tools for representation learning on graphs, graph neural
networks (GNNs) have facilitated various applications from drug discovery to
recommender systems. Nevertheless, the effectiveness of GNNs is immensely
challenged by issues related to data quality, such as distribution shift,
abnormal features and adversarial attacks. Recent efforts have been made on
tackling these issues from a modeling perspective which requires additional
cost of changing model architectures or re-training model parameters. In this
work, we provide a data-centric view to tackle these issues and propose a graph
transformation framework named GTrans which adapts and refines graph data at
test time to achieve better performance. We provide theoretical analysis on the
design of the framework and discuss why adapting graph data works better than
adapting the model. Extensive experiments have demonstrated the effectiveness
of GTrans on three distinct scenarios for eight benchmark datasets where
suboptimal data is presented. Remarkably, GTrans performs the best in most
cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on
three experimental settings. Code is released at
https://github.com/ChandlerBang/GTrans.",1,0,1,1,0,0,0.707181,5.0,0.719222,88
0c3b756c-26ca-469a-a409-e28ae238b0cd,Estimating Confidence of Predictions of Individual Classifiers and Their Ensembles for the Genre Classification Task,2,0.0561782,0.0392013,"Genre identification is a subclass of non-topical text classification. The
main difference between this task and topical classification is that genres,
unlike topics, usually do not correspond to simple keywords, and thus they need
to be defined in terms of their functions in communication. Neural models based
on pre-trained transformers, such as BERT or XLM-RoBERTa, demonstrate SOTA
results in many NLP tasks, including non-topical classification. However, in
many cases, their downstream application to very large corpora, such as those
extracted from social media, can lead to unreliable results because of dataset
shifts, when some raw texts do not match the profile of the training set. To
mitigate this problem, we experiment with individual models as well as with
their ensembles. To evaluate the robustness of all models we use a prediction
confidence metric, which estimates the reliability of a prediction in the
absence of a gold standard label. We can evaluate robustness via the confidence
gap between the correctly classified texts and the misclassified ones on a
labeled test corpus, higher gaps make it easier to improve our confidence that
our classifier made the right decision. Our results show that for all of the
classifiers tested in this study, there is a confidence gap, but for the
ensembles, the gap is bigger, meaning that ensembles are more robust than their
individual models.",1,1,0,0,0,0,0.212496,9.0,0.662088,16
12887741-99d9-430b-998a-ecf270413266,Reinforcement Learning for Economic Policy: A New Frontier?,2,0.00277725,0.0470455,"Agent-based computational economics is a field with a rich academic history,
yet one which has struggled to enter mainstream policy design toolboxes,
plagued by the challenges associated with representing a complex and dynamic
reality. The field of Reinforcement Learning (RL), too, has a rich history, and
has recently been at the centre of several exponential developments. Modern RL
implementations have been able to achieve unprecedented levels of
sophistication, handling previously unthinkable degrees of complexity. This
review surveys the historical barriers of classical agent-based techniques in
economic modelling, and contemplates whether recent developments in RL can
overcome any of them.",0,0,0,0,0,0,0.00333241,10.0,0.268817,56
24913e11-364e-4cc6-b402-ba5ec8ed5e22,"Decentralized, Communication- and Coordination-free Learning in Structured Matching Markets",4,0.0313522,0.395144,"We study the problem of online learning in competitive settings in the
context of two-sided matching markets. In particular, one side of the market,
the agents, must learn about their preferences over the other side, the firms,
through repeated interaction while competing with other agents for successful
matches. We propose a class of decentralized, communication- and
coordination-free algorithms that agents can use to reach to their stable match
in structured matching markets. In contrast to prior works, the proposed
algorithms make decisions based solely on an agent's own history of play and
requires no foreknowledge of the firms' preferences. Our algorithms are
constructed by splitting up the statistical problem of learning one's
preferences, from noisy observations, from the problem of competing for firms.
We show that under realistic structural assumptions on the underlying
preferences of the agents and firms, the proposed algorithms incur a regret
which grows at most logarithmically in the time horizon. Our results show that,
in the case of matching markets, competition need not drastically affect the
performance of decentralized, communication and coordination free online
learning algorithms.",0,0,0,0,0,0,0.0786345,10.0,0.588829,32
5778c2c5-0ed3-44eb-b569-bf4380460a12,Rosenblatt's first theorem and frugality of deep learning,2,0.000867505,0.044098,"First Rosenblatt's theorem about omnipotence of shallow networks states that
elementary perceptrons can solve any classification problem if there are no
discrepancies in the training set. Minsky and Papert considered elementary
perceptrons with restrictions on the neural inputs: a bounded number of
connections or a relatively small diameter of the receptive field for each
neuron at the hidden layer. They proved that under these constraints, an
elementary perceptron cannot solve some problems, such as the connectivity of
input images or the parity of pixels in them. In this note, we demonstrated
first Rosenblatt's theorem at work, showed how an elementary perceptron can
solve a version of the travel maze problem, and analysed the complexity of that
solution. We constructed also a deep network algorithm for the same problem. It
is much more efficient. The shallow network uses an exponentially large number
of neurons on the hidden layer (Rosenblatt's $A$-elements), whereas for the
deep network the second order polynomial complexity is sufficient. We
demonstrated that for the same complex problem deep network can be much smaller
and reveal a heuristic behind this effect.",0,0,0,0,0,0,0.00117317,9.0,0.0714564,23
0591de13-8c13-463b-910e-f3b5babcf656,On the Effect of Anticipation on Reading Times,7,0.107814,0.726785,"Over the past two decades, numerous studies have demonstrated how less
predictable (i.e., higher surprisal) words take more time to read. In general,
these studies have implicitly assumed the reading process is purely responsive:
Readers observe a new word and allocate time to process it as required. We
argue that prior results are also compatible with a reading process that is at
least partially anticipatory: Readers could make predictions about a future
word and allocate time to process it based on their expectation. In this work,
we operationalize this anticipation as a word's contextual entropy. We assess
the effect of anticipation on reading by comparing how well surprisal and
contextual entropy predict reading times on four naturalistic reading datasets:
two self-paced and two eye-tracking. Experimentally, across datasets and
analyses, we find substantial evidence for effects of contextual entropy over
surprisal on a word's reading time (RT): in fact, entropy is sometimes better
than surprisal in predicting a word's RT. Spillover effects, however, are
generally not captured by entropy, but only by surprisal. Further, we
hypothesize four cognitive mechanisms through which contextual entropy could
impact RTs -- three of which we are able to design experiments to analyze.
Overall, our results support a view of reading that is not just responsive, but
also anticipatory.",0,0,0,0,0,0,0.00934556,16.0,0.60765,61
0baf7a80-cc0b-4655-ba0e-0498e7189154,TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation,9,0.0984631,0.597776,"Knowledge graph embedding (KGE) aims to learn continuous vectors of relations
and entities in knowledge graph. Recently, transition-based KGE methods have
achieved promising performance, where the single relation vector learns to
translate head entity to tail entity. However, this scoring pattern is not
suitable for complex scenarios where the same entity pair has different
relations. Previous models usually focus on the improvement of entity
representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single
relation vector. In this paper, we propose a novel transition-based method,
TranS, for knowledge graph embedding. The single relation vector in traditional
scoring patterns is replaced with synthetic relation representation, which can
solve these issues effectively and efficiently. Experiments on a large
knowledge graph dataset, ogbl-wikikg2, show that our model achieves
state-of-the-art results.",0,1,0,0,1,0,0.240664,11.0,0.736428,48
2358bf7f-404d-4d14-9810-b102e08c0fc2,HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction,78,0.86827,0.99988,"We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by
4 participants interacting with 800 different object instances from 16
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities.",0,0,1,1,0,0,0.962133,6.0,0.929413,54
2f1f7c36-4b09-4a9c-841c-5b40e11989b8,Multi-channel Attentive Graph Convolutional Network With Sentiment Fusion For Multimodal Sentiment Analysis,10,0.0994577,0.402761,"Nowadays, with the explosive growth of multimodal reviews on social media
platforms, multimodal sentiment analysis has recently gained popularity because
of its high relevance to these social media posts. Although most previous
studies design various fusion frameworks for learning an interactive
representation of multiple modalities, they fail to incorporate sentimental
knowledge into inter-modality learning. This paper proposes a Multi-channel
Attentive Graph Convolutional Network (MAGCN), consisting of two main
components: cross-modality interactive learning and sentimental feature fusion.
For cross-modality interactive learning, we exploit the self-attention
mechanism combined with densely connected graph convolutional networks to learn
inter-modality dynamics. For sentimental feature fusion, we utilize multi-head
self-attention to merge sentimental knowledge into inter-modality feature
representations. Extensive experiments are conducted on three widely-used
datasets. The experimental results demonstrate that the proposed model achieves
competitive performance on accuracy and F1 scores compared to several
state-of-the-art approaches.",0,0,0,0,1,0,0.736087,8.0,0.834668,25
778f3c43-91b7-45a2-a632-9f932c45e7e6,Enhanced Bi-directional Motion Estimation for Video Frame Interpolation,12,0.539874,0.83706,"We present a novel simple yet effective algorithm for motion-based video
frame interpolation. Existing motion-based interpolation methods typically rely
on a pre-trained optical flow model or a U-Net based pyramid network for motion
estimation, which either suffer from large model size or limited capacity in
handling complex and large motion cases. In this work, by carefully integrating
intermediateoriented forward-warping, lightweight feature encoder, and
correlation volume into a pyramid recurrent framework, we derive a compact
model to simultaneously estimate the bidirectional motion between input frames.
It is 15 times smaller in size than PWC-Net, yet enables more reliable and
flexible handling of challenging motion cases. Based on estimated
bi-directional motion, we forward-warp input frames and their context features
to intermediate frame, and employ a synthesis network to estimate the
intermediate frame from warped representations. Our method achieves excellent
performance on a broad range of video frame interpolation benchmarks. Code and
trained models are available at \url{https://github.com/srcn-ivl/EBME}.",1,1,0,0,0,0,0.953624,9.0,0.945839,38
cda5f1f2-8b8c-4035-8ab9-e36caf3b550b,TransLog: A Unified Transformer-based Framework for Log Anomaly Detection,14,0.457448,0.483908,"Log anomaly detection is a key component in the field of artificial
intelligence for IT operations (AIOps). Considering log data of variant
domains, retraining the whole network for unknown domains is inefficient in
real industrial scenarios especially for low-resource domains. However,
previous deep models merely focused on extracting the semantics of log sequence
in the same domain, leading to poor generalization on multi-domain logs.
Therefore, we propose a unified Transformer-based framework for log anomaly
detection (\ourmethod{}), which is comprised of the pretraining and
adapter-based tuning stage. Our model is first pretrained on the source domain
to obtain shared semantic knowledge of log data. Then, we transfer the
pretrained model to the target domain via the adapter-based tuning. The
proposed method is evaluated on three public datasets including one source
domain and two target domains. The experimental results demonstrate that our
simple yet efficient approach, with fewer trainable parameters and lower
training costs in the target domain, achieves state-of-the-art performance on
three benchmarks.",0,1,0,0,1,0,0.970195,6.0,0.941176,33
4c8b9db8-76b8-46fa-9633-27894750cf8c,PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6 DoF Tracking,11,0.0683842,0.190899,"Estimating the relative pose of a new object without prior knowledge is a
hard problem, while it is an ability very much needed in robotics and Augmented
Reality. We present a method for tracking the 6D motion of objects in RGB video
sequences when neither the training images nor the 3D geometry of the objects
are available. In contrast to previous works, our method can therefore consider
unknown objects in open world instantly, without requiring any prior
information or a specific training phase. We consider two architectures, one
based on two frames, and the other relying on a Transformer Encoder, which can
exploit an arbitrary number of past frames. We train our architectures using
only synthetic renderings with domain randomization. Our results on challenging
datasets are on par with previous works that require much more information
(training images of the target objects, 3D models, and/or depth data). Our
source code is available at https://github.com/nv-nguyen/pizza",1,1,0,0,0,0,0.18972,8.0,0.603962,59
42235e75-cac0-43cc-b980-7bac63154079,Modelling Commonsense Properties using Pre-Trained Bi-Encoders,8,0.129431,0.214022,"Grasping the commonsense properties of everyday concepts is an important
prerequisite to language understanding. While contextualised language models
are reportedly capable of predicting such commonsense properties with
human-level accuracy, we argue that such results have been inflated because of
the high similarity between training and test concepts. This means that models
which capture concept similarity can perform well, even if they do not capture
any knowledge of the commonsense properties themselves. In settings where there
is no overlap between the properties that are considered during training and
testing, we find that the empirical performance of standard language models
drops dramatically. To address this, we study the possibility of fine-tuning
language models to explicitly model concepts and their properties. In
particular, we train separate concept and property encoders on two types of
readily available data: extracted hyponym-hypernym pairs and generic sentences.
Our experimental results show that the resulting encoders allow us to predict
commonsense properties with much higher accuracy than is possible by directly
fine-tuning language models. We also present experimental results for the
related task of unsupervised hypernym discovery.",1,0,0,0,0,0,0.448822,7.0,0.696077,56
5bdbd65b-986a-4a64-a98b-10ac0b65286d,Abstract Flow for Temporal Semantic Segmentation on the Permutohedral Lattice,8,0.082323,0.17443,"Semantic segmentation is a core ability required by autonomous agents, as
being able to distinguish which parts of the scene belong to which object class
is crucial for navigation and interaction with the environment. Approaches
which use only one time-step of data cannot distinguish between moving objects
nor can they benefit from temporal integration. In this work, we extend a
backbone LatticeNet to process temporal point cloud data. Additionally, we take
inspiration from optical flow methods and propose a new module called Abstract
Flow which allows the network to match parts of the scene with similar abstract
features and gather the information temporally. We obtain state-of-the-art
results on the SemanticKITTI dataset that contains LiDAR scans from real urban
environments. We share the PyTorch implementation of TemporalLatticeNet at
https://github.com/AIS-Bonn/temporal_latticenet .",1,1,0,0,1,0,0.751103,10.0,0.872038,22
31fe4dbf-fe94-4d28-9bfc-67b89fda187b,Are Transformers Effective for Time Series Forecasting?,463,0.920559,1.0,"Recently, there has been a surge of Transformer-based solutions for the
long-term time series forecasting (LTSF) task. Despite the growing performance
over the past few years, we question the validity of this line of research in
this work. Specifically, Transformers is arguably the most successful solution
to extract the semantic correlations among the elements in a long sequence.
However, in time series modeling, we are to extract the temporal relations in
an ordered set of continuous points. While employing positional encoding and
using tokens to embed sub-series in Transformers facilitate preserving some
ordering information, the nature of the \emph{permutation-invariant}
self-attention mechanism inevitably results in temporal information loss. To
validate our claim, we introduce a set of embarrassingly simple one-layer
linear models named LTSF-Linear for comparison. Experimental results on nine
real-life datasets show that LTSF-Linear surprisingly outperforms existing
sophisticated Transformer-based LTSF models in all cases, and often by a large
margin. Moreover, we conduct comprehensive empirical studies to explore the
impacts of various design elements of LTSF models on their temporal relation
extraction capability. We hope this surprising finding opens up new research
directions for the LTSF task. We also advocate revisiting the validity of
Transformer-based solutions for other time series analysis tasks (e.g., anomaly
detection) in the future. Code is available at:
\url{https://github.com/cure-lab/LTSF-Linear}.",1,0,0,0,0,0,0.37379,9.0,0.736825,36
05ca3619-1b7a-445e-be50-6fa64ba2a698,"SF-PATE: Scalable, Fair, and Private Aggregation of Teacher Ensembles",5,0.0818227,0.323759,"A critical concern in data-driven processes is to build models whose outcomes
do not discriminate against some demographic groups, including gender,
ethnicity, or age. To ensure non-discrimination in learning tasks, knowledge of
the group attributes is essential. However, in practice, these attributes may
not be available due to legal and ethical requirements. To address this
challenge, this paper studies a model that protects the privacy of the
individuals' sensitive information while also allowing it to learn
non-discriminatory predictors. A key characteristic of the proposed model is to
enable the adoption of off-the-selves and non-private fair models to create a
privacy-preserving and fair model. The paper analyzes the relation between
accuracy, privacy, and fairness, and the experimental evaluation illustrates
the benefits of the proposed models on several prediction tasks. In particular,
this proposal is the first to allow both scalable and accurate training of
private and fair models for very large neural networks.",0,1,0,0,0,0,0.592414,6.0,0.713737,42
318821a1-f06a-4d82-88bd-1e863efa1b69,Context-Situated Pun Generation,7,0.352186,0.135753,"Previous work on pun generation commonly begins with a given pun word (a pair
of homophones for heterographic pun generation and a polyseme for homographic
pun generation) and seeks to generate an appropriate pun. While this may enable
efficient pun generation, we believe that a pun is most entertaining if it fits
appropriately within a given context, e.g., a given situation or dialogue. In
this work, we propose a new task, context-situated pun generation, where a
specific context represented by a set of keywords is provided, and the task is
to first identify suitable pun words that are appropriate for the context, then
generate puns based on the context keywords and the identified pun words. We
collect CUP (Context-sitUated Pun), containing 4.5k tuples of context words and
pun pairs. Based on the new data and setup, we propose a pipeline system for
context-situated pun generation, including a pun word retrieval module that
identifies suitable pun words for a given context, and a generation module that
generates puns from context keywords and pun words. Human evaluation shows that
69% of our top retrieved pun words can be used to generate context-situated
puns, and our generation module yields successful puns 31% of the time given a
plausible tuple of context words and pun pair, almost tripling the yield of a
state-of-the-art pun generation model. With an end-to-end evaluation, our
pipeline system with the top-1 retrieved pun pair for a given context can
generate successful puns 40% of the time, better than all other modeling
variations but 32% lower than the human success rate. This highlights the
difficulty of the task, and encourages more research in this direction.",0,0,1,1,0,0,0.723086,9.0,0.848953,39
95ccaaab-c7b9-4c88-ab35-8ca1e54ee055,Open-Vocabulary Temporal Action Detection with Off-the-Shelf Image-Text Features,1,0.0176083,0.141031,"Detecting actions in untrimmed videos should not be limited to a small,
closed set of classes. We present a simple, yet effective strategy for
open-vocabulary temporal action detection utilizing pretrained image-text
co-embeddings. Despite being trained on static images rather than videos, we
show that image-text co-embeddings enable openvocabulary performance
competitive with fully-supervised models. We show that the performance can be
further improved by ensembling the image-text features with features encoding
local motion, like optical flow based features, or other modalities, like
audio. In addition, we propose a more reasonable open-vocabulary evaluation
setting for the ActivityNet data set, where the category splits are based on
similarity rather than random assignment.",0,1,0,0,0,0,0.562914,7.0,0.743052,53
fc45b8ff-4714-40e2-bed6-8f05001e164c,Handwritten Arabic Character Recognition for Children Writ-ing Using Convolutional Neural Network and Stroke Identification,8,0.0761427,0.891498,"Automatic Arabic handwritten recognition is one of the recently studied
problems in the field of Machine Learning. Unlike Latin languages, Arabic is a
Semitic language that forms a harder challenge, especially with variability of
patterns caused by factors such as writer age. Most of the studies focused on
adults, with only one recent study on children. Moreover, much of the recent
Machine Learning methods focused on using Convolutional Neural Networks, a
powerful class of neural networks that can extract complex features from
images. In this paper we propose a convolutional neural network (CNN) model
that recognizes children handwriting with an accuracy of 91% on the Hijja
dataset, a recent dataset built by collecting images of the Arabic characters
written by children, and 97% on Arabic Handwritten Character Dataset. The
results showed a good improvement over the proposed model from the Hijja
dataset authors, yet it reveals a bigger challenge to solve for children Arabic
handwritten character recognition. Moreover, we proposed a new approach using
multi models instead of single model based on the number of strokes in a
character, and merged Hijja with AHCD which reached an averaged prediction
accuracy of 96%.",0,1,0,1,1,0,0.0360165,7.0,0.297867,27
3eab7308-69a3-4063-9be6-86109790f127,Selection Strategies for Commonsense Knowledge,2,0.030402,0.148966,"Selection strategies are broadly used in first-order logic theorem proving to
select those parts of a large knowledge base that are necessary to proof a
theorem at hand. Usually, these selection strategies do not take the meaning of
symbol names into account. In knowledge bases with commonsense knowledge,
symbol names are usually chosen to have a meaning and this meaning provides
valuable information for selection strategies. We introduce the vector-based
selection strategy, a purely statistical selection technique for commonsense
knowledge based on word embeddings. We compare different commonsense knowledge
selection techniques for the purpose of theorem proving and demonstrate the
usefulness of vector-based selection with a case study.",0,1,0,0,0,0,0.00326752,17.0,0.568734,29
7b7b7e67-1576-44c7-9fe3-26096ce6af76,Less Is More: Linear Layers on CLIP Features as Powerful VizWiz Model,1,0.0109953,0.0188489,"Current architectures for multi-modality tasks such as visual question
answering suffer from their high complexity. As a result, these architectures
are difficult to train and require high computational resources. To address
these problems we present a CLIP-based architecture that does not require any
fine-tuning of the feature extractors. A simple linear classifier is used on
the concatenated features of the image and text encoder. During training an
auxiliary loss is added which operates on the answer types. The resulting
classification is then used as an attention gate on the answer class selection.
On the VizWiz 2022 Visual Question Answering Challenge we achieve 60.15 %
accuracy on Task 1: Predict Answer to a Visual Question and AP score of 83.78 %
on Task 2: Predict Answerability of a Visual Question.",0,1,0,0,1,0,0.981862,4.0,0.944822,7
9abbb64f-340d-4df4-8756-1fe5e4139e4b,Sample Constrained Treatment Effect Estimation,4,0.0773218,0.513598,"Treatment effect estimation is a fundamental problem in causal inference. We
focus on designing efficient randomized controlled trials, to accurately
estimate the effect of some treatment on a population of $n$ individuals. In
particular, we study sample-constrained treatment effect estimation, where we
must select a subset of $s \ll n$ individuals from the population to experiment
on. This subset must be further partitioned into treatment and control groups.
Algorithms for partitioning the entire population into treatment and control
groups, or for choosing a single representative subset, have been well-studied.
The key challenge in our setting is jointly choosing a representative subset
and a partition for that set.
  We focus on both individual and average treatment effect estimation, under a
linear effects model. We give provably efficient experimental designs and
corresponding estimators, by identifying connections to discrepancy
minimization and leverage-score-based sampling used in randomized numerical
linear algebra. Our theoretical results obtain a smooth transition to known
guarantees when $s$ equals the population size. We also empirically demonstrate
the performance of our algorithms.",0,0,0,0,0,0,0.129674,15.0,0.761098,56
3387b397-f465-4fd5-8c4e-10bc9a9708ee,WebtoonMe: A Data-Centric Approach for Full-Body Portrait Stylization,9,0.163464,0.864665,"Full-body portrait stylization, which aims to translate portrait photography
into a cartoon style, has drawn attention recently. However, most methods have
focused only on converting face regions, restraining the feasibility of use in
real-world applications. A recently proposed two-stage method expands the
rendering area to full bodies, but the outputs are less plausible and fail to
achieve quality robustness of non-face regions. Furthermore, they cannot
reflect diverse skin tones. In this study, we propose a data-centric solution
to build a production-level full-body portrait stylization system. Based on the
two-stage scheme, we construct a novel and advanced dataset preparation
paradigm that can effectively resolve the aforementioned problems. Experiments
reveal that with our pipeline, high-quality portrait stylization can be
achieved without additional losses or architectural changes.",1,1,0,0,0,0,0.727186,6.0,0.775355,9
2e639c47-e224-496b-949e-1c140dc1efb3,News Summarization and Evaluation in the Era of GPT-3,232,0.996195,0.999999,"The recent success of prompting large language models like GPT-3 has led to a
paradigm shift in NLP research. In this paper, we study its impact on text
summarization, focusing on the classic benchmark domain of news summarization.
First, we investigate how GPT-3 compares against fine-tuned models trained on
large summarization datasets. We show that not only do humans overwhelmingly
prefer GPT-3 summaries, prompted using only a task description, but these also
do not suffer from common dataset-specific issues such as poor factuality.
Next, we study what this means for evaluation, particularly the role of gold
standard test sets. Our experiments show that both reference-based and
reference-free automatic metrics cannot reliably evaluate GPT-3 summaries.
Finally, we evaluate models on a setting beyond generic summarization,
specifically keyword-based summarization, and show how dominant fine-tuning
approaches compare to prompting.
  To support further research, we release: (a) a corpus of 10K generated
summaries from fine-tuned and prompt-based models across 4 standard
summarization benchmarks, (b) 1K human preference judgments comparing different
systems for generic- and keyword-based summarization.",0,0,0,1,0,1,0.88983,5.0,0.836324,85
9d29d27e-054e-4ef5-95d0-81a7f689a12a,Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control,20,0.159378,0.739434,"Abstractive summarization systems leveraging pre-training language models
have achieved superior results on benchmark datasets. However, such models have
been shown to be more prone to hallucinate facts that are unfaithful to the
input context. In this paper, we propose a method to remedy entity-level
extrinsic hallucinations with Entity Coverage Control (ECC). We first compute
entity coverage precision and prepend the corresponding control code for each
training example, which implicitly guides the model to recognize faithfulness
contents in the training phase. We further extend our method via intermediate
fine-tuning on large but noisy data extracted from Wikipedia to unlock
zero-shot summarization. We show that the proposed method leads to more
faithful and salient abstractive summarization in supervised fine-tuning and
zero-shot settings according to our experimental results on three benchmark
datasets XSum, Pubmed, and SAMSum of very different domains and styles.",0,1,0,0,0,0,0.771144,6.0,0.796498,39
47dde95c-6cee-4202-9ac9-46e3fdcce8d1,Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions,50,0.22821,0.996129,"Probabilistic diffusion models have achieved state-of-the-art results for
image synthesis, inpainting, and text-to-image tasks. However, they are still
in the early stages of generating complex 3D shapes. This work proposes
Diffusion-SDF, a generative model for shape completion, single-view
reconstruction, and reconstruction of real-scanned point clouds. We use neural
signed distance functions (SDFs) as our 3D representation to parameterize the
geometry of various signals (e.g., point clouds, 2D images) through neural
networks. Neural SDFs are implicit functions and diffusing them amounts to
learning the reversal of their neural network weights, which we solve using a
custom modulation module. Extensive experiments show that our method is capable
of both realistic unconditional generation and conditional generation from
partial inputs. This work expands the domain of diffusion models from learning
2D, explicit representations, to 3D, implicit representations.",1,0,0,0,0,0,0.65634,5.0,0.691289,70
ac790832-c724-4c01-942b-40cab6caf482,Few-shot Single-view 3D Reconstruction with Memory Prior Contrastive Network,12,0.257186,0.594711,"3D reconstruction of novel categories based on few-shot learning is appealing
in real-world applications and attracts increasing research interests. Previous
approaches mainly focus on how to design shape prior models for different
categories. Their performance on unseen categories is not very competitive. In
this paper, we present a Memory Prior Contrastive Network (MPCN) that can store
shape prior knowledge in a few-shot learning based 3D reconstruction framework.
With the shape memory, a multi-head attention module is proposed to capture
different parts of a candidate shape prior and fuse these parts together to
guide 3D reconstruction of novel categories. Besides, we introduce a 3D-aware
contrastive learning method, which can not only complement the retrieval
accuracy of memory network, but also better organize image features for
downstream tasks. Compared with previous few-shot 3D reconstruction methods,
MPCN can handle the inter-class variability without category annotations.
Experimental results on a benchmark synthetic dataset and the Pascal3D+
real-world dataset show that our model outperforms the current state-of-the-art
methods significantly.",0,1,1,0,1,0,0.804393,8.0,0.860019,50
ff80e544-faf5-4972-86d0-497802ba57a1,Long-Tail Prediction Uncertainty Aware Trajectory Planning for Self-driving Vehicles,12,0.0805315,0.666931,"A typical trajectory planner of autonomous driving commonly relies on
predicting the future behavior of surrounding obstacles. Recently, deep
learning technology has been widely adopted to design prediction models due to
their impressive performance. However, such models may fail in the ""long-tail""
driving cases where the training data is sparse or unavailable, leading to
planner failures. To this end, this work proposes a trajectory planner to
consider the prediction model uncertainty arising from insufficient data for
safer performance. Firstly, an ensemble network structure estimates the
prediction model's uncertainty due to insufficient training data. Then a
trajectory planner is designed to consider the worst-case arising from
prediction uncertainty. The results show that the proposed method can improve
the safety of trajectory planning under the prediction uncertainty caused by
insufficient data. At the same time, with sufficient data, the framework will
not lead to overly conservative results. This technology helps to improve the
safety and reliability of autonomous vehicles under the long-tail data
distribution of the real world.",0,1,0,0,0,0,0.615397,4.0,0.586272,27
84961cb6-7cb5-4d20-89ae-110d08f83968,Beyond Hard Labels: Investigating data label distributions,10,0.150295,0.254992,"High-quality data is a key aspect of modern machine learning. However, labels
generated by humans suffer from issues like label noise and class ambiguities.
We raise the question of whether hard labels are sufficient to represent the
underlying ground truth distribution in the presence of these inherent
imprecision. Therefore, we compare the disparity of learning with hard and soft
labels quantitatively and qualitatively for a synthetic and a real-world
dataset. We show that the application of soft labels leads to improved
performance and yields a more regular structure of the internal feature space.",0,0,0,0,0,0,0.688374,5.0,0.70882,28
07c611be-7af9-477d-805e-065fa21c1af4,HyperNST: Hyper-Networks for Neural Style Transfer,6,0.038597,0.401853,"We present HyperNST; a neural style transfer (NST) technique for the artistic
stylization of images, based on Hyper-networks and the StyleGAN2 architecture.
Our contribution is a novel method for inducing style transfer parameterized by
a metric space, pre-trained for style-based visual search (SBVS). We show for
the first time that such space may be used to drive NST, enabling the
application and interpolation of styles from an SBVS system. The technical
contribution is a hyper-network that predicts weight updates to a StyleGAN2
pre-trained over a diverse gamut of artistic content (portraits), tailoring the
style parameterization on a per-region basis using a semantic map of the facial
regions. We show HyperNST to exceed state of the art in content preservation
for our stylized content while retaining good style transfer performance.",0,1,0,0,1,0,0.761938,5.0,0.750376,27
1c46b6ca-4c4c-48ed-a8b9-271590eca9a8,AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning,8,0.17891,0.806545,"Biomedical named entity recognition (BioNER) seeks to automatically recognize
biomedical entities in natural language text, serving as a necessary foundation
for downstream text mining tasks and applications such as information
extraction and question answering. Manually labeling training data for the
BioNER task is costly, however, due to the significant domain expertise
required for accurate annotation. The resulting data scarcity causes current
BioNER approaches to be prone to overfitting, to suffer from limited
generalizability, and to address a single entity type at a time (e.g., gene or
disease). We therefore propose a novel all-in-one (AIO) scheme that uses
external data from existing annotated resources to enhance the accuracy and
stability of BioNER models. We further present AIONER, a general-purpose BioNER
tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER
on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and
compares favorably to other state-of-the-art approaches such as multi-task
learning. We further demonstrate the practical utility of AIONER in three
independent tasks to recognize entity types not previously seen in training
data, as well as the advantages of AIONER over existing methods for processing
biomedical text at a large scale (e.g., the entire PubMed data).",0,1,0,0,1,0,0.209079,11.0,0.721864,65
14242df4-5dc1-47db-aa71-c73d10b4cf8f,Disentangling Architecture and Training for Optical Flow,18,0.0779578,0.844879,"How important are training details and datasets to recent optical flow models
like RAFT? And do they generalize? To explore these questions, rather than
develop a new model, we revisit three prominent models, PWC-Net, IRR-PWC and
RAFT, with a common set of modern training techniques and datasets, and observe
significant performance gains, demonstrating the importance and generality of
these training details. Our newly trained PWC-Net and IRR-PWC models show
surprisingly large improvements, up to 30% versus original published results on
Sintel and KITTI 2015 benchmarks. They outperform the more recent Flow1D on
KITTI 2015 while being 3x faster during inference. Our newly trained RAFT
achieves an Fl-all score of 4.31% on KITTI 2015, more accurate than all
published optical flow methods at the time of writing. Our results demonstrate
the benefits of separating the contributions of models, training techniques and
datasets when analyzing performance gains of optical flow methods. Our source
code will be publicly available.",1,1,0,0,1,0,0.352929,7.0,0.651263,64
85d3cc2c-cc3e-47ca-ab4c-c053099df15f,Attend to the Right Context: A Plug-and-Play Module for Content-Controllable Summarization,2,0.0511752,0.128067,"Content-Controllable Summarization generates summaries focused on the given
controlling signals. Due to the lack of large-scale training corpora for the
task, we propose a plug-and-play module RelAttn to adapt any general
summarizers to the content-controllable summarization task. RelAttn first
identifies the relevant content in the source documents, and then makes the
model attend to the right context by directly steering the attention weight. We
further apply an unsupervised online adaptive parameter searching algorithm to
determine the degree of control in the zero-shot setting, while such parameters
are learned in the few-shot setting. By applying the module to three backbone
summarization models, experiments show that our method effectively improves all
the summarizers, and outperforms the prefix-based method and a widely used
plug-and-play model in both zero- and few-shot settings. Tellingly, more
benefit is observed in the scenarios when more control is needed.",1,1,0,0,0,0,0.496902,7.0,0.716443,31
6c4ecd8b-b950-42bf-87aa-2e9bbb016a88,"A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding",15,0.0284398,0.21396,"Collecting sufficient labeled data for spoken language understanding (SLU) is
expensive and time-consuming. Recent studies achieved promising results by
using pre-trained models in low-resource scenarios. Inspired by this, we aim to
ask: which (if any) pre-training strategies can improve performance across SLU
benchmarks? To answer this question, we employ four types of pre-trained models
and their combinations for SLU. We leverage self-supervised speech and language
models (LM) pre-trained on large quantities of unpaired data to extract strong
speech and text representations. We also explore using supervised models
pre-trained on larger external automatic speech recognition (ASR) or SLU
corpora. We conduct extensive experiments on the SLU Evaluation (SLUE)
benchmark and observe self-supervised pre-trained models to be more powerful,
with pre-trained LM and speech models being most beneficial for the Sentiment
Analysis and Named Entity Recognition task, respectively.",0,1,0,0,0,0,0.226916,6.0,0.505552,53
c946d71b-44c3-4058-9fdc-ff919814f629,Hyperbolic Deep Reinforcement Learning,8,0.0275481,0.395546,"We propose a new class of deep reinforcement learning (RL) algorithms that
model latent representations in hyperbolic space. Sequential decision-making
requires reasoning about the possible future consequences of current behavior.
Consequently, capturing the relationship between key evolving features for a
given task is conducive to recovering effective policies. To this end,
hyperbolic geometry provides deep RL models with a natural basis to precisely
encode this inherently hierarchical information. However, applying existing
methodologies from the hyperbolic deep learning literature leads to fatal
optimization instabilities due to the non-stationarity and variance
characterizing RL gradient estimators. Hence, we design a new general method
that counteracts such optimization challenges and enables stable end-to-end
learning with deep hyperbolic representations. We empirically validate our
framework by applying it to popular on-policy and off-policy RL algorithms on
the Procgen and Atari 100K benchmarks, attaining near universal performance and
generalization benefits. Given its natural fit, we hope future RL research will
consider hyperbolic representations as a standard tool.",1,0,0,0,0,0,0.13437,9.0,0.606076,97
5ae281b0-f098-428f-a90c-426ae11d1717,Improving Probabilistic Models in Text Classification via Active Learning,1,0.00476583,0.0474675,"Social scientists often classify text documents to use the resulting labels
as an outcome or a predictor in empirical research. Automated text
classification has become a standard tool, since it requires less human coding.
However, scholars still need many human-labeled documents to train automated
classifiers. To reduce labeling costs, we propose a new algorithm for text
classification that combines a probabilistic model with active learning. The
probabilistic model uses both labeled and unlabeled data, and active learning
concentrates labeling efforts on difficult documents to classify. Our
validation study shows that the classification performance of our algorithm is
comparable to state-of-the-art methods at a fraction of the computational cost.
Moreover, we replicate two recently published articles and reach the same
substantive conclusions with only a small proportion of the original labeled
data used in those studies. We provide activeText, an open-source software to
implement our method.",0,0,0,0,0,0,0.0136582,10.0,0.410402,49
0ae64fa5-f293-430c-96e0-bb2b4e33c997,LAMNER: Code Comment Generation Using Character Language Model and Named Entity Recognition,1,0.00805869,0.0289566,"Code comment generation is the task of generating a high-level natural
language description for a given code method or function. Although researchers
have been studying multiple ways to generate code comments automatically,
previous work mainly considers representing a code token in its entirety
semantics form only (e.g., a language model is used to learn the semantics of a
code token), and additional code properties such as the tree structure of a
code are included as an auxiliary input to the model. There are two
limitations: 1) Learning the code token in its entirety form may not be able to
capture information succinctly in source code, and 2) The code token does not
contain additional syntactic information, inherently important in programming
languages.
  In this paper, we present LAnguage Model and Named Entity Recognition
(LAMNER), a code comment generator capable of encoding code constructs
effectively and capturing the structural property of a code token. A
character-level language model is used to learn the semantic representation to
encode a code token. For the structural property of a token, a Named Entity
Recognition model is trained to learn the different types of code tokens. These
representations are then fed into an encoder-decoder architecture to generate
code comments. We evaluate the generated comments from LAMNER and other
baselines on a popular Java dataset with four commonly used metrics. Our
results show that LAMNER is effective and improves over the best baseline model
in BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, and CIDEr by 14.34%,
18.98%, 21.55%, 23.00%, 10.52%, 1.44%, and 25.86%, respectively. Additionally,
we fused LAMNER's code representation with the baseline models, and the fused
models consistently showed improvement over the non-fused models. The human
evaluation further shows that LAMNER produces high-quality code comments.",0,1,0,0,1,0,0.37785,8.0,0.705653,87
1ed5d2f0-86a6-4837-bb53-8deee77ca5da,PoissonMat: Remodeling Matrix Factorization using Poisson Distribution and Solving the Cold Start Problem without Input Data,9,0.0302377,0.789991,"Matrix Factorization is one of the most successful recommender system
techniques over the past decade. However, the classic probabilistic theory
framework for matrix factorization is modeled using normal distributions. To
find better probabilistic models, algorithms such as RankMat, ZeroMat and
DotMat have been invented in recent years. In this paper, we model the user
rating behavior in recommender system as a Poisson process, and design an
algorithm that relies on no input data to solve the recommendation problem and
the cold start issue at the same time. We prove the superiority of our
algorithm in comparison with matrix factorization, random placement, Zipf
placement, ZeroMat, DotMat, etc.",0,0,0,0,1,0,0.0243853,11.0,0.517191,21
c4c85114-e54d-4956-a8b8-4f3ddb50ba07,Learning to Prove Trigonometric Identities,1,0.0153005,0.18601,"Automatic theorem proving with deep learning methods has attracted attentions
recently. In this paper, we construct an automatic proof system for
trigonometric identities. We define the normalized form of trigonometric
identities, design a set of rules for the proof and put forward a method which
can generate theoretically infinite trigonometric identities. Our goal is not
only to complete the proof, but to complete the proof in as few steps as
possible. For this reason, we design a model to learn proof data generated by
random BFS (rBFS), and it is proved theoretically and experimentally that the
model can outperform rBFS after a simple imitation learning. After further
improvement through reinforcement learning, we get AutoTrig, which can give
proof steps for identities in almost as short steps as BFS (theoretically
shortest method), with a time cost of only one-thousandth. In addition,
AutoTrig also beats Sympy, Matlab and human in the synthetic dataset, and
performs well in many generalization tasks.",0,0,0,0,0,0,0.443217,6.0,0.642568,56
94f8bccf-6883-4863-a56b-97f35010bdc8,Multimodal Hate Speech Detection from Bengali Memes and Texts,19,0.35726,0.888433,"Numerous machine learning (ML) and deep learning (DL)-based approaches have
been proposed to utilize textual data from social media for anti-social
behavior analysis like cyberbullying, fake news detection, and identification
of hate speech mainly for highly-resourced languages such as English. However,
despite having a lot of diversity and millions of native speakers, some
languages like Bengali are under-resourced, which is due to a lack of
computational resources for natural language processing (NLP). Similar to other
languages, Bengali social media contents also include images along with texts
(e.g., multimodal memes are posted by embedding short texts into images on
Facebook). Therefore, only the textual data is not enough to judge them since
images might give extra context to make a proper judgement. This paper is about
hate speech detection from multimodal Bengali memes and texts. We prepared the
only multimodal hate speech dataset for-a-kind of problem for Bengali, which we
use to train state-of-the-art neural architectures (e.g., Bi-LSTM/Conv-LSTM
with word embeddings, ConvNets + pre-trained language models, e.g., monolingual
Bangla BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) to jointly
analyze textual and visual information for hate speech detection. Conv-LSTM and
XLM-RoBERTa models performed best for texts, yielding F1 scores of 0.78 and
0.82, respectively. As of memes, ResNet-152 and DenseNet-161 models yield F1
scores of 0.78 and 0.79, respectively. As for multimodal fusion, XLM-RoBERTa +
DenseNet-161 performed the best, yielding an F1 score of 0.83. Our study
suggests that text modality is most useful for hate speech detection, while
memes are moderately useful.",1,1,1,1,1,0,0.653451,7.0,0.778368,34
06b57b08-805d-4457-9e64-fe9cff4b4f08,Deep Model-Based Super-Resolution with Non-uniform Blur,4,0.258145,0.450162,"We propose a state-of-the-art method for super-resolution with non-uniform
blur. Single-image super-resolution methods seek to restore a high-resolution
image from blurred, subsampled, and noisy measurements. Despite their
impressive performance, existing techniques usually assume a uniform blur
kernel. Hence, these techniques do not generalize well to the more general case
of non-uniform blur. Instead, in this paper, we address the more realistic and
computationally challenging case of spatially-varying blur. To this end, we
first propose a fast deep plug-and-play algorithm, based on linearized ADMM
splitting techniques, which can solve the super-resolution problem with
spatially-varying blur. Second, we unfold our iterative algorithm into a single
network and train it end-to-end. In this way, we overcome the intricacy of
manually tuning the parameters involved in the optimization scheme. Our
algorithm presents remarkable performance and generalizes well after a single
training to a large family of spatially-varying blur kernels, noise levels and
scale factors.",1,1,0,0,1,0,0.904293,12.0,0.936956,77
8e975648-7c5c-4478-b2a9-53b18933f6a1,From Indoor To Outdoor: Unsupervised Domain Adaptive Gait Recognition,5,0.211158,0.540927,"Gait recognition is an important AI task, which has been progressed rapidly
with the development of deep learning. However, existing learning based gait
recognition methods mainly focus on the single domain, especially the
constrained laboratory environment. In this paper, we study a new problem of
unsupervised domain adaptive gait recognition (UDA-GR), that learns a gait
identifier with supervised labels from the indoor scenes (source domain), and
is applied to the outdoor wild scenes (target domain). For this purpose, we
develop an uncertainty estimation and regularization based UDA-GR method.
Specifically, we investigate the characteristic of gaits in the indoor and
outdoor scenes, for estimating the gait sample uncertainty, which is used in
the unsupervised fine-tuning on the target domain to alleviate the noises of
the pseudo labels. We also establish a new benchmark for the proposed problem,
experimental results on which show the effectiveness of the proposed method. We
will release the benchmark and source code in this work to the public.",0,1,1,1,0,0,0.884729,7.0,0.880127,46
e1e8a859-41a0-4b86-9a28-a42b5e540edf,Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting,26,0.280224,0.987607,"In document classification for, e.g., legal and biomedical text, we often
deal with hundreds of classes, including very infrequent ones, as well as
temporal concept drift caused by the influence of real world events, e.g.,
policy changes, conflicts, or pandemics. Class imbalance and drift can
sometimes be mitigated by resampling the training data to simulate (or
compensate for) a known target distribution, but what if the target
distribution is determined by unknown future events? Instead of simply
resampling uniformly to hedge our bets, we focus on the underlying optimization
algorithms used to train such document classifiers and evaluate several
group-robust optimization algorithms, initially proposed to mitigate
group-level disparities. Reframing group-robust algorithms as adaptation
algorithms under concept drift, we find that Invariant Risk Minimization and
Spectral Decoupling outperform sampling-based approaches to class imbalance and
concept drift, and lead to much better performance on minority classes. The
effect is more pronounced the larger the label set.",1,0,0,0,0,0,0.691875,6.0,0.758957,49
ae00bdba-1f4a-4ad2-a757-17eb420d7dfe,A Robust Document Image Watermarking Scheme using Deep Neural Network,14,0.436866,0.610994,"Watermarking is an important copyright protection technology which generally
embeds the identity information into the carrier imperceptibly. Then the
identity can be extracted to prove the copyright from the watermarked carrier
even after suffering various attacks. Most of the existing watermarking
technologies take the nature images as carriers. Different from the natural
images, document images are not so rich in color and texture, and thus have
less redundant information to carry watermarks. This paper proposes an
end-to-end document image watermarking scheme using the deep neural network.
Specifically, an encoder and a decoder are designed to embed and extract the
watermark. A noise layer is added to simulate the various attacks that could be
encountered in reality, such as the Cropout, Dropout, Gaussian blur, Gaussian
noise, Resize, and JPEG Compression. A text-sensitive loss function is designed
to limit the embedding modification on characters. An embedding strength
adjustment strategy is proposed to improve the quality of watermarked image
with little loss of extraction accuracy. Experimental results show that the
proposed document image watermarking technology outperforms three
state-of-the-arts in terms of the robustness and image quality.",1,1,1,1,1,0,0.638684,10.0,0.84084,41
931412fc-5c3a-4b9e-bca4-2fc007b98a41,Image Augmentation Based Momentum Memory Intrinsic Reward for Sparse Reward Visual Scenes,1,0.0180814,0.0374025,"Many scenes in real life can be abstracted to the sparse reward visual
scenes, where it is difficult for an agent to tackle the task under the
condition of only accepting images and sparse rewards. We propose to decompose
this problem into two sub-problems: the visual representation and the sparse
reward. To address them, a novel framework IAMMIR combining the self-supervised
representation learning with the intrinsic motivation is presented. For visual
representation, a representation driven by a combination of the imageaugmented
forward dynamics and the reward is acquired. For sparse rewards, a new type of
intrinsic reward is designed, the Momentum Memory Intrinsic Reward (MMIR). It
utilizes the difference of the outputs from the current model (online network)
and the historical model (target network) to present the agent's state
familiarity. Our method is evaluated on the visual navigation task with sparse
rewards in Vizdoom. Experiments demonstrate that our method achieves the state
of the art performance in sample efficiency, at least 2 times faster than the
existing methods reaching 100% success rate.",0,1,0,0,1,0,0.982358,8.0,0.973272,37
7954be40-1d7e-4bbb-91b7-17e0c8b83003,Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors,12,0.478819,0.656093,"Robustness of machine learning models on ever-changing real-world data is
critical, especially for applications affecting human well-being such as
content moderation. New kinds of abusive language continually emerge in online
discussions in response to current events (e.g., COVID-19), and the deployed
abuse detection systems should be updated regularly to remain accurate. In this
paper, we show that general abusive language classifiers tend to be fairly
reliable in detecting out-of-domain explicitly abusive utterances but fail to
detect new types of more subtle, implicit abuse. Next, we propose an
interpretability technique, based on the Testing Concept Activation Vector
(TCAV) method from computer vision, to quantify the sensitivity of a trained
model to the human-defined concepts of explicit and implicit abusive language,
and use that to explain the generalizability of the model on new data, in this
case, COVID-related anti-Asian hate speech. Extending this technique, we
introduce a novel metric, Degree of Explicitness, for a single instance and
show that the new metric is beneficial in suggesting out-of-domain unlabeled
examples to effectively enrich the training data with informative, implicitly
abusive texts.",1,1,0,0,0,0,0.9818,5.0,0.955688,41
83e2979d-26e9-4a2d-b77d-3a030f91d8c5,BanglaSarc: A Dataset for Sarcasm Detection,4,0.120792,0.0722387,"Being one of the most widely spoken language in the world, the use of Bangla
has been increasing in the world of social media as well. Sarcasm is a positive
statement or remark with an underlying negative motivation that is extensively
employed in today's social media platforms. There has been a significant
improvement in sarcasm detection in English over the previous many years,
however the situation regarding Bangla sarcasm detection remains unchanged. As
a result, it is still difficult to identify sarcasm in bangla, and a lack of
high-quality data is a major contributing factor. This article proposes
BanglaSarc, a dataset constructed specifically for bangla textual data sarcasm
detection. This dataset contains of 5112 comments/status and contents collected
from various online social platforms such as Facebook, YouTube, along with a
few online blogs. Due to the limited amount of data collection of categorized
comments in Bengali, this dataset will aid in the of study identifying sarcasm,
recognizing people's emotion, detecting various types of Bengali expressions,
and other domains. The dataset is publicly available at
https://www.kaggle.com/datasets/sakibapon/banglasarc.",1,1,1,1,0,0,0.232543,6.0,0.510217,8
1037bc3e-e3d4-4dfd-a0af-e4fdbb691c3d,A Computational Inflection for Scientific Discovery,16,0.0512513,0.708901,"We stand at the foot of a significant inflection in the trajectory of
scientific discovery. As society continues on its fast-paced digital
transformation, so does humankind's collective scientific knowledge and
discourse. We now read and write papers in digitized form, and a great deal of
the formal and informal processes of science are captured digitally --
including papers, preprints and books, code and datasets, conference
presentations, and interactions in social networks and collaboration and
communication platforms. The transition has led to the creation and growth of a
tremendous amount of information -- much of which is available for public
access -- opening exciting opportunities for computational models and systems
that analyze and harness it. In parallel, exponential growth in data processing
power has fueled remarkable advances in artificial intelligence, including
large neural language models capable of learning powerful representations from
unstructured text. Dramatic changes in scientific communication -- such as the
advent of the first scientific journal in the 17th century -- have historically
catalyzed revolutions in scientific thought. The confluence of societal and
computational trends suggests that computer science is poised to ignite a
revolution in the scientific process itself.",0,0,0,0,0,1,0.408095,4.0,0.43627,72
b8fc29c6-194c-41a6-bff7-0f16b0bbe488,Fine-tuning Image Transformers using Learnable Memory,35,0.129251,0.413635,"In this paper we propose augmenting Vision Transformer models with learnable
memory tokens. Our approach allows the model to adapt to new tasks, using few
parameters, while optionally preserving its capabilities on previously learned
tasks. At each layer we introduce a set of learnable embedding vectors that
provide contextual information useful for specific datasets. We call these
""memory tokens"". We show that augmenting a model with just a handful of such
tokens per layer significantly improves accuracy when compared to conventional
head-only fine-tuning, and performs only slightly below the significantly more
expensive full fine-tuning. We then propose an attention-masking approach that
enables extension to new downstream tasks, with a computation reuse. In this
setup in addition to being parameters efficient, models can execute both old
and new tasks as a part of single inference at a small incremental cost.",0,0,0,0,0,0,0.811504,5.0,0.780519,46
44f298fa-d7d9-4d23-86f9-c49f791b5552,Unifying and Boosting Gradient-Based Training-Free Neural Architecture Search,22,0.0861346,0.759727,"Neural architecture search (NAS) has gained immense popularity owing to its
ability to automate neural architecture design. A number of training-free
metrics are recently proposed to realize NAS without training, hence making NAS
more scalable. Despite their competitive empirical performances, a unified
theoretical understanding of these training-free metrics is lacking. As a
consequence, (a) the relationships among these metrics are unclear, (b) there
is no theoretical interpretation for their empirical performances, and (c)
there may exist untapped potential in existing training-free NAS, which
probably can be unveiled through a unified theoretical understanding. To this
end, this paper presents a unified theoretical analysis of gradient-based
training-free NAS, which allows us to (a) theoretically study their
relationships, (b) theoretically guarantee their generalization performances,
and (c) exploit our unified theoretical understanding to develop a novel
framework named hybrid NAS (HNAS) which consistently boosts training-free NAS
in a principled way. Remarkably, HNAS can enjoy the advantages of both
training-free (i.e., the superior search efficiency) and training-based (i.e.,
the remarkable search effectiveness) NAS, which we have demonstrated through
extensive experiments.",0,0,0,0,0,0,0.439973,6.0,0.640906,78
1fca88a7-aea8-4ea8-a41a-1ea39a49eab5,A context-aware knowledge transferring strategy for CTC-based ASR,9,0.158344,0.747647,"Non-autoregressive automatic speech recognition (ASR) modeling has received
increasing attention recently because of its fast decoding speed and superior
performance. Among representatives, methods based on the connectionist temporal
classification (CTC) are still a dominating stream. However, the theoretically
inherent flaw, the assumption of independence between tokens, creates a
performance barrier for the school of works. To mitigate the challenge, we
propose a context-aware knowledge transferring strategy, consisting of a
knowledge transferring module and a context-aware training strategy, for
CTC-based ASR. The former is designed to distill linguistic information from a
pre-trained language model, and the latter is framed to modulate the
limitations caused by the conditional independence assumption. As a result, a
knowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is
presented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2
datasets demonstrate the effectiveness of the proposed method.",1,1,0,0,0,0,0.636876,5.0,0.680698,38
094078ac-a67f-4156-b25c-a77a57b9cdb8,Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning,2,0.0195066,0.0395259,"Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the
new paradigm for using pre-trained language models (PLMs). Up to now, various
DETs with distinct design elements have been proposed, achieving performance on
par with fine-tuning. However, the mechanisms behind the above success are
still under-explored, especially the connections among various DETs. To fathom
the mystery, we hypothesize that the adaptations of different DETs could all be
reparameterized as low-dimensional optimizations in a unified optimization
subspace, which could be found by jointly decomposing independent solutions of
different DETs. Then we explore the connections among different DETs by
conducting optimization within the subspace. In experiments, we find that, for
a certain DET, conducting optimization simply in the subspace could achieve
comparable performance to its original space, and the found solution in the
subspace could be transferred to another DET and achieve non-trivial
performance. We also visualize the performance landscape of the subspace and
find that there exists a substantial region where different DETs all perform
well. Finally, we extend our analysis and show the strong connections between
fine-tuning and DETs.",1,0,0,0,0,0,0.88686,8.0,0.896186,95
1144356c-88fa-42a6-aa23-0a577e6bc1f1,Does Video Compression Impact Tracking Accuracy?,3,0.0224245,0.340454,"Everyone ""knows"" that compressing a video will degrade the accuracy of object
tracking. Yet, a literature search on this topic reveals that there is very
little documented evidence for this presumed fact. Part of the reason is that,
until recently, there were no object tracking datasets for uncompressed video,
which made studying the effects of compression on tracking accuracy difficult.
In this paper, using a recently published dataset that contains tracking
annotations for uncompressed videos, we examined the degradation of tracking
accuracy due to video compression using rigorous statistical methods.
Specifically, we examined the impact of quantization parameter (QP) and motion
search range (MSR) on Multiple Object Tracking Accuracy (MOTA). The results
show that QP impacts MOTA at the 95% confidence level, while there is
insufficient evidence to claim that MSR impacts MOTA. Moreover, regression
analysis allows us to derive a quantitative relationship between MOTA and QP
for the specific tracker used in the experiments.",0,1,0,1,0,0,0.0238275,13.0,0.589668,22
aca1dc36-445a-4ade-8c7e-4795a59b8ffe,USHER: Unbiased Sampling for Hindsight Experience Replay,1,0.0347096,0.154423,"Dealing with sparse rewards is a long-standing challenge in reinforcement
learning (RL). Hindsight Experience Replay (HER) addresses this problem by
reusing failed trajectories for one goal as successful trajectories for
another. This allows for both a minimum density of reward and for
generalization across multiple goals. However, this strategy is known to result
in a biased value function, as the update rule underestimates the likelihood of
bad outcomes in a stochastic environment. We propose an asymptotically unbiased
importance-sampling-based algorithm to address this problem without sacrificing
performance on deterministic environments. We show its effectiveness on a range
of robotic systems, including challenging high dimensional stochastic
environments.",1,1,0,0,0,0,0.444383,9.0,0.762109,14
b1a9542b-1513-40e2-9130-29b3301c18ed,Spatial Pruned Sparse Convolution for Efficient 3D Object Detection,16,0.0939565,0.92663,"3D scenes are dominated by a large number of background points, which is
redundant for the detection task that mainly needs to focus on foreground
objects. In this paper, we analyze major components of existing sparse 3D CNNs
and find that 3D CNNs ignore the redundancy of data and further amplify it in
the down-sampling process, which brings a huge amount of extra and unnecessary
computational overhead. Inspired by this, we propose a new convolution operator
named spatial pruned sparse convolution (SPS-Conv), which includes two
variants, spatial pruned submanifold sparse convolution (SPSS-Conv) and spatial
pruned regular sparse convolution (SPRS-Conv), both of which are based on the
idea of dynamically determining crucial areas for redundancy reduction. We
validate that the magnitude can serve as important cues to determine crucial
areas which get rid of the extra computations of learning-based methods. The
proposed modules can easily be incorporated into existing sparse 3D CNNs
without extra architectural modifications. Extensive experiments on the KITTI,
Waymo and nuScenes datasets demonstrate that our method can achieve more than
50% reduction in GFLOPs without compromising the performance.",0,1,0,0,0,0,0.561701,6.0,0.699669,42
3510782f-afd9-4c78-926e-cfba6024ba99,CoLI-Machine Learning Approaches for Code-mixed Language Identification at the Word Level in Kannada-English Texts,12,0.27504,0.931666,"The task of automatically identifying a language used in a given text is
called Language Identification (LI). India is a multilingual country and many
Indians especially youths are comfortable with Hindi and English, in addition
to their local languages. Hence, they often use more than one language to post
their comments on social media. Texts containing more than one language are
called ""code-mixed texts"" and are a good source of input for LI. Languages in
these texts may be mixed at sentence level, word level or even at sub-word
level. LI at word level is a sequence labeling problem where each and every
word in a sentence is tagged with one of the languages in the predefined set of
languages. In order to address word level LI in code-mixed Kannada-English
(Kn-En) texts, this work presents i) the construction of code-mixed Kn-En
dataset called CoLI-Kenglish dataset, ii) code-mixed Kn-En embedding and iii)
learning models using Machine Learning (ML), Deep Learning (DL) and Transfer
Learning (TL) approaches. Code-mixed Kn-En texts are extracted from Kannada
YouTube video comments to construct CoLI-Kenglish dataset and code-mixed Kn-En
embedding. The words in CoLI-Kenglish dataset are grouped into six major
categories, namely, ""Kannada"", ""English"", ""Mixed-language"", ""Name"", ""Location""
and ""Other"". The learning models, namely, CoLI-vectors and CoLI-ngrams based on
ML, CoLI-BiLSTM based on DL and CoLI-ULMFiT based on TL approaches are built
and evaluated using CoLI-Kenglish dataset. The performances of the learning
models illustrated, the superiority of CoLI-ngrams model, compared to other
models with a macro average F1-score of 0.64. However, the results of all the
learning models were quite competitive with each other.",0,1,0,1,0,0,0.385083,10.0,0.766957,25
0ce131b1-ef04-4fd1-b205-128dac21ed1d,Vehicle-road Cooperative Simulation and 3D Visualization System,1,0.0,0.0983417,"The safety of single-vehicle autonomous driving technology is limited due to
the limits of perception capability of on-board sensors. In contrast,
vehicle-road collaboration technology can overcome those limits and improve the
traffic safety and efficiency, by expanding the sensing range, improving the
perception accuracy, and reducing the response time. However, such a technology
is still under development; it requires rigorous testing and verification
methods to ensure the reliability and trustworthiness of the technology. In
this thesis, we focus on three major tasks: (1) analyze the functional
characteristics related to the scenarios of vehicle-road cooperations,
highlightening the differences between vehicle-road cooperative systems and
traditional single-vehicle autonomous driving systems; (2) refine and classifiy
the functional characteristics of vehicle-road cooperative systems; (3) design
and develop a simulation system, and provide a visual interface to facilitate
development and analysis. The efficiency and effectiveness the proposed method
are verfied by experiments.",0,1,0,0,0,0,0.467511,8.0,0.741104,37
9493c86c-b9ce-44f8-b693-913da87cec32,Class-Aware Universum Inspired Re-Balance Learning for Long-Tailed Recognition,1,0.0107975,0.051928,"Data augmentation for minority classes is an effective strategy for
long-tailed recognition, thus developing a large number of methods. Although
these methods all ensure the balance in sample quantity, the quality of the
augmented samples is not always satisfactory for recognition, being prone to
such problems as over-fitting, lack of diversity, semantic drift, etc. For
these issues, we propose the Class-aware Universum Inspired Re-balance
Learning(CaUIRL) for long-tailed recognition, which endows the Universum with
class-aware ability to re-balance individual minority classes from both sample
quantity and quality. In particular, we theoretically prove that the
classifiers learned by CaUIRL are consistent with those learned under the
balanced condition from a Bayesian perspective. In addition, we further develop
a higher-order mixup approach, which can automatically generate class-aware
Universum(CaU) data without resorting to any external data. Unlike the
traditional Universum, such generated Universum additionally takes the domain
similarity, class separability, and sample diversity into account. Extensive
experiments on benchmark datasets demonstrate the surprising advantages of our
method, especially the top1 accuracy in minority classes is improved by 1.9% 6%
compared to the state-of-the-art method.",0,1,0,0,1,0,0.527873,9.0,0.789284,63
ddb9976c-86e8-4260-b2ec-aaa52d2c7b27,Provable Safe Reinforcement Learning with Binary Feedback,3,0.0772442,0.221671,"Safety is a crucial necessity in many applications of reinforcement learning
(RL), whether robotic, automotive, or medical. Many existing approaches to safe
RL rely on receiving numeric safety feedback, but in many cases this feedback
can only take binary values; that is, whether an action in a given state is
safe or unsafe. This is particularly true when feedback comes from human
experts. We therefore consider the problem of provable safe RL when given
access to an offline oracle providing binary feedback on the safety of state,
action pairs. We provide a novel meta algorithm, SABRE, which can be applied to
any MDP setting given access to a blackbox PAC RL algorithm for that setting.
SABRE applies concepts from active learning to reinforcement learning to
provably control the number of queries to the safety oracle. SABRE works by
iteratively exploring the state space to find regions where the agent is
currently uncertain about safety. Our main theoretical results shows that,
under appropriate technical assumptions, SABRE never takes unsafe actions
during training, and is guaranteed to return a near-optimal safe policy with
high probability. We provide a discussion of how our meta-algorithm may be
applied to various settings studied in both theoretical and empirical
frameworks.",0,0,1,0,0,0,0.761967,6.0,0.791995,50
0d15c148-6afc-407c-ac86-564eed8693e9,Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models,8,0.384118,0.667477,"This paper presents exploratory work on whether and to what extent biases
against queer and trans people are encoded in large language models (LLMs) such
as BERT. We also propose a method for reducing these biases in downstream
tasks: finetuning the models on data written by and/or about queer people. To
measure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,
modeled after other bias-detection benchmarks but addressing homophobic and
transphobic biases. We found that BERT shows significant homophobic bias, but
this bias can be mostly mitigated by finetuning BERT on a natural language
corpus written by members of the LGBTQ+ community.",1,0,1,1,0,0,0.992696,7.0,0.997674,32
422bc89e-4fa2-4d48-98da-9c4e5f6d654a,Semantic Probabilistic Layers for Neuro-Symbolic Learning,38,0.676913,0.98409,"We design a predictive layer for structured-output prediction (SOP) that can
be plugged into any neural network guaranteeing its predictions are consistent
with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer
(SPL) can model intricate correlations, and hard constraints, over a structured
output space all while being amenable to end-to-end learning via maximum
likelihood. SPLs combine exact probabilistic inference with logical reasoning
in a clean and modular way, learning complex distributions and restricting
their support to solutions of the constraint. As such, they can faithfully, and
efficiently, model complex SOP tasks beyond the reach of alternative
neuro-symbolic approaches. We empirically demonstrate that SPLs outperform
these competitors in terms of accuracy on challenging SOP tasks including
hierarchical multi-label classification, pathfinding and preference learning,
while retaining perfect constraint satisfaction.",1,0,0,0,0,0,0.503795,9.0,0.781664,100
0d6e3259-776f-4144-9eb5-a914a6b848d3,End-to-End Semantic Video Transformer for Zero-Shot Action Recognition,2,0.0257293,0.0575037,"While video action recognition has been an active area of research for
several years, zero-shot action recognition has only recently started gaining
traction. In this work, we propose a novel end-to-end trained transformer model
which is capable of capturing long range spatiotemporal dependencies
efficiently, contrary to existing approaches which use 3D-CNNs. Moreover, to
address a common ambiguity in the existing works about classes that can be
considered as previously unseen, we propose a new experimentation setup that
satisfies the zero-shot learning premise for action recognition by avoiding
overlap between the training and testing classes. The proposed approach
significantly outperforms the state of the arts in zero-shot action recognition
in terms of the the top-1 accuracy on UCF-101, HMDB-51 and ActivityNet
datasets. The code and proposed experimentation setup are available in GitHub:
https://github.com/Secure-and-Intelligent-Systems-Lab/SemanticVideoTransformer",1,0,0,0,1,0,0.40826,10.0,0.774561,53
292fa82f-badb-442f-ae81-e5b179db9f6b,QUIC-FL: Quick Unbiased Compression for Federated Learning,8,0.0382395,0.256254,"Distributed Mean Estimation (DME), in which $n$ clients communicate vectors
to a parameter server that estimates their average, is a fundamental building
block in communication-efficient federated learning. In this paper, we improve
on previous DME techniques that achieve the optimal $O(1/n)$ Normalized Mean
Squared Error (NMSE) guarantee by asymptotically improving the complexity for
either encoding or decoding (or both). To achieve this, we formalize the
problem in a novel way that allows us to use off-the-shelf mathematical solvers
to design the quantization.",0,1,0,0,0,0,0.152196,7.0,0.512771,112
7303ec02-ee4b-492e-8445-0c78e7ee622b,Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion,6,0.0421187,0.375688,"Deep 3D point cloud models are sensitive to adversarial attacks, which poses
threats to safety-critical applications such as autonomous driving. Robust
training and defend-by-denoising are typical strategies for defending
adversarial perturbations. However, they either induce massive computational
overhead or rely heavily upon specified priors, limiting generalized robustness
against attacks of all kinds. To remedy it, this paper introduces a novel
distortion-aware defense framework that can rebuild the pristine data
distribution with a tailored intensity estimator and a diffusion model. To
perform distortion-aware forward diffusion, we design a distortion estimation
algorithm that is obtained by summing the distance of each point to the
best-fitting plane of its local neighboring points, which is based on the
observation of the local spatial properties of the adversarial point cloud. By
iterative diffusion and reverse denoising, the perturbed point cloud under
various distortions can be restored back to a clean distribution. This approach
enables effective defense against adaptive attacks with varying noise budgets,
enhancing the robustness of existing 3D deep recognition models.",0,1,0,0,0,0,0.359089,7.0,0.654368,46
60fab58f-ebbc-4219-9a68-d3e61dc3c60e,Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models,135,0.888675,0.907968,"Pre-trained vision-language models (e.g., CLIP) have shown promising
zero-shot generalization in many downstream tasks with properly designed text
prompts. Instead of relying on hand-engineered prompts, recent works learn
prompts using the training data from downstream tasks. While effective,
training on domain-specific data reduces a model's generalization capability to
unseen new domains. In this work, we propose test-time prompt tuning (TPT), a
method that can learn adaptive prompts on the fly with a single test sample.
For image classification, TPT optimizes the prompt by minimizing the entropy
with confidence selection so that the model has consistent predictions across
different augmented views of each test sample. In evaluating generalization to
natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP
by 3.6% on average, surpassing previous prompt tuning approaches that require
additional task-specific training data. In evaluating cross-dataset
generalization with unseen categories, TPT performs on par with the
state-of-the-art approaches that use additional training data. Project page:
https://azshue.github.io/TPT.",1,1,0,0,0,1,0.941066,5.0,0.886263,79
86b3cc65-c3e9-4b22-8790-da555db8a431,Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation,6,0.0944575,0.52127,"Multi-modal Machine Translation (MMT) enables the use of visual information
to enhance the quality of translations. The visual information can serve as a
valuable piece of context information to decrease the ambiguity of input
sentences. Despite the increasing popularity of such a technique, good and
sizeable datasets are scarce, limiting the full extent of their potential.
Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It
is estimated that about 100 to 150 million people speak the language, with more
than 80 million indigenous speakers. This is more than any of the other Chadic
languages. Despite a large number of speakers, the Hausa language is considered
low-resource in natural language processing (NLP). This is due to the absence
of sufficient resources to implement most NLP tasks. While some datasets exist,
they are either scarce, machine-generated, or in the religious domain.
Therefore, there is a need to create training and evaluation data for
implementing machine learning tasks and bridging the research gap in the
language. This work presents the Hausa Visual Genome (HaVG), a dataset that
contains the description of an image or a section within the image in Hausa and
its equivalent in English. To prepare the dataset, we started by translating
the English description of the images in the Hindi Visual Genome (HVG) into
Hausa automatically. Afterward, the synthetic Hausa data was carefully
post-edited considering the respective images. The dataset comprises 32,923
images and their descriptions that are divided into training, development,
test, and challenge test set. The Hausa Visual Genome is the first dataset of
its kind and can be used for Hausa-English machine translation, multi-modal
research, and image description, among various other natural language
processing and generation tasks.",0,1,1,1,0,0,0.189361,10.0,0.682959,34
6d352eed-75eb-482c-a1f8-f9b7c5c0560b,Perceiver-VL: Efficient Vision-and-Language Modeling with Iterative Latent Attention,5,0.0132459,0.110867,"We present Perceiver-VL, a vision-and-language framework that efficiently
handles high-dimensional multimodal inputs such as long videos and text.
Powered by the iterative latent cross-attention of Perceiver, our framework
scales with linear complexity, in contrast to the quadratic complexity of
self-attention used in many state-of-the-art transformer-based models. To
further improve the efficiency of our framework, we also study applying
LayerDrop on cross-attention layers and introduce a mixed-stream architecture
for cross-modal retrieval. We evaluate Perceiver-VL on diverse video-text and
image-text benchmarks, where Perceiver-VL achieves the lowest GFLOPs and
latency while maintaining competitive performance. In addition, we also provide
comprehensive analyses of various aspects of our framework, including
pretraining data, scalability of latent size and input size, dropping
cross-attention layers at inference to reduce latency, modality aggregation
strategy, positional encoding, and weight initialization strategy. Our code and
checkpoints are available at: https://github.com/zinengtang/Perceiver_VL",1,1,0,0,0,0,0.636261,7.0,0.771688,91
68753d87-c22d-41d9-81e6-378dca60c031,Lexicon-injected Semantic Parsing for Task-Oriented Dialog,2,0.0698606,0.400481,"Recently, semantic parsing using hierarchical representations for dialog
systems has captured substantial attention. Task-Oriented Parse (TOP), a tree
representation with intents and slots as labels of nested tree nodes, has been
proposed for parsing user utterances. Previous TOP parsing methods are limited
on tackling unseen dynamic slot values (e.g., new songs and locations added),
which is an urgent matter for real dialog systems. To mitigate this issue, we
first propose a novel span-splitting representation for span-based parser that
outperforms existing methods. Then we present a novel lexicon-injected semantic
parser, which collects slot labels of tree representation as a lexicon, and
injects lexical features to the span representation of parser. An additional
slot disambiguation technique is involved to remove inappropriate span match
occurrences from the lexicon. Our best parser produces a new state-of-the-art
result (87.62%) on the TOP dataset, and demonstrates its adaptability to
frequently updated slot lexicon entries in real task-oriented dialog, with no
need of retraining.",0,1,0,0,1,0,0.958075,10.0,0.954489,38
4a4dd1fc-7d68-420c-a454-c289e9283db5,Mitigating Attacks on Artificial Intelligence-based Spectrum Sensing for Cellular Network Signals,2,0.0783589,0.303256,"Cellular networks (LTE, 5G, and beyond) are dramatically growing with high
demand from consumers and more promising than the other wireless networks with
advanced telecommunication technologies. The main goal of these networks is to
connect billions of devices, systems, and users with high-speed data
transmission, high cell capacity, and low latency, as well as to support a wide
range of new applications, such as virtual reality, metaverse, telehealth,
online education, autonomous and flying vehicles, advanced manufacturing, and
many more. To achieve these goals, spectrum sensing has been paid more
attention, along with new approaches using artificial intelligence (AI) methods
for spectrum management in cellular networks. This paper provides a
vulnerability analysis of spectrum sensing approaches using AI-based semantic
segmentation models for identifying cellular network signals under adversarial
attacks with and without defensive distillation methods. The results showed
that mitigation methods can significantly reduce the vulnerabilities of
AI-based spectrum sensing models against adversarial attacks.",0,1,0,0,0,0,0.758785,7.0,0.820382,21
2ce4fa10-7507-4b12-b50a-56945a31b482,Data-Efficient Backdoor Attacks,18,0.333731,0.829504,"Recent studies have proven that deep neural networks are vulnerable to
backdoor attacks. Specifically, by mixing a small number of poisoned samples
into the training set, the behavior of the trained model can be maliciously
controlled. Existing attack methods construct such adversaries by randomly
selecting some clean data from the benign set and then embedding a trigger into
them. However, this selection strategy ignores the fact that each poisoned
sample contributes inequally to the backdoor injection, which reduces the
efficiency of poisoning. In this paper, we formulate improving the poisoned
data efficiency by the selection as an optimization problem and propose a
Filtering-and-Updating Strategy (FUS) to solve it. The experimental results on
CIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the
same attack success rate can be achieved with only 47% to 75% of the poisoned
sample volume compared to the random selection strategy. More importantly, the
adversaries selected according to one setting can generalize well to other
settings, exhibiting strong transferability. The prototype code of our method
is now available at https://github.com/xpf/Data-Efficient-Backdoor-Attacks.",1,1,0,0,0,0,0.89336,7.0,0.885183,35
817bf9f0-b922-47ea-936d-74f675a30252,Gradient-Based Constrained Sampling from Language Models,35,0.514404,0.452526,"Large pretrained language models generate fluent text but are notoriously
hard to controllably sample from. In this work, we study constrained sampling
from such language models: generating text that satisfies user-defined
constraints, while maintaining fluency and the model's performance in a
downstream task. We propose MuCoLa -- a sampling procedure that combines the
log-likelihood of the language model with arbitrary (differentiable)
constraints in a single energy function, and then generates samples in a
non-autoregressive manner. Specifically, it initializes the entire output
sequence with noise and follows a Markov chain defined by Langevin Dynamics
using the gradients of the energy function. We evaluate MuCoLa on text
generation with soft and hard constraints as well as their combinations
obtaining significant improvements over competitive baselines for toxicity
avoidance, sentiment control, and keyword-guided generation.",1,0,1,0,0,0,0.914084,5.0,0.857688,95
5a9c748d-ed35-4a79-9658-5dda5387c173,Commonsense and Named Entity Aware Knowledge Grounded Dialogue Generation,12,0.156163,0.133075,"Grounding dialogue on external knowledge and interpreting linguistic patterns
in dialogue history context, such as ellipsis, anaphora, and co-references is
critical for dialogue comprehension and generation. In this paper, we present a
novel open-domain dialogue generation model which effectively utilizes the
large-scale commonsense and named entity based knowledge in addition to the
unstructured topic-specific knowledge associated with each utterance. We
enhance the commonsense knowledge with named entity-aware structures using
co-references. Our proposed model utilizes a multi-hop attention layer to
preserve the most accurate and critical parts of the dialogue history and the
associated knowledge. In addition, we employ a Commonsense and Named Entity
Enhanced Attention Module, which starts with the extracted triples from various
sources and gradually finds the relevant supporting set of triples using
multi-hop attention with the query vector obtained from the interactive
dialogue-knowledge module. Empirical results on two benchmark dataset
demonstrate that our model significantly outperforms the state-of-the-art
methods in terms of both automatic evaluation metrics and human judgment. Our
code is publicly available at
\href{https://github.com/deekshaVarshney/CNTF}{https://github.com/deekshaVarshney/CNTF};
\href{https://www.iitp.ac.in/~ai-nlp-ml/resources/codes/CNTF.zip}{https://www.iitp.ac.in/-ai-nlp-ml/resources/
codes/CNTF.zip}.",1,0,0,0,1,0,0.411606,8.0,0.719545,54
9b6e1656-918d-4b43-9a4a-5736765169da,Finding Dataset Shortcuts with Grammar Induction,8,0.0674024,0.43653,"Many NLP datasets have been found to contain shortcuts: simple decision rules
that achieve surprisingly high accuracy. However, it is difficult to discover
shortcuts automatically. Prior work on automatic shortcut detection has focused
on enumerating features like unigrams or bigrams, which can find only low-level
shortcuts, or relied on post-hoc model interpretability methods like saliency
maps, which reveal qualitative patterns without a clear statistical
interpretation. In this work, we propose to use probabilistic grammars to
characterize and discover shortcuts in NLP datasets. Specifically, we use a
context-free grammar to model patterns in sentence classification datasets and
use a synchronous context-free grammar to model datasets involving sentence
pairs. The resulting grammars reveal interesting shortcut features in a number
of datasets, including both simple and high-level features, and automatically
identify groups of test examples on which conventional classifiers fail.
Finally, we show that the features we discover can be used to generate
diagnostic contrast examples and incorporated into standard robust optimization
methods to improve worst-group accuracy.",0,0,0,0,0,0,0.281982,8.0,0.660714,53
a7c8850e-7dd2-43b4-bd97-db5cd161de28,Inception-Based Crowd Counting -- Being Fast while Remaining Accurate,1,0.0210643,0.102104,"Recent sophisticated CNN-based algorithms have demonstrated their
extraordinary ability to automate counting crowds from images, thanks to their
structures which are designed to address the issue of various head scales.
However, these complicated architectures also increase computational complexity
enormously, making real-time estimation implausible. Thus, in this paper, a new
method, based on Inception-V3, is proposed to reduce the amount of computation.
This proposed approach (ICC), exploits the first five inception blocks and the
contextual module designed in CAN to extract features at different receptive
fields, thereby being context-aware. The employment of these two different
strategies can also increase the model's robustness. Experiments show that ICC
can at best reduce 85.3 percent calculations with 24.4 percent performance
loss. This high efficiency contributes significantly to the deployment of crowd
counting models in surveillance systems to guard the public safety. The code
will be available at https://github.com/YIMINGMA/CrowdCounting-ICC,and its
pre-trained weights on the Crowd Counting dataset, which comprises a large
variety of scenes from surveillance perspectives, will also open-sourced.",0,1,0,0,0,0,0.951057,10.0,0.949486,49
1b912c64-2fd8-43d5-85ae-c7a35c299517,CoHS-CQG: Context and History Selection for Conversational Question Generation,6,0.0924415,0.348421,"Conversational question generation (CQG) serves as a vital task for machines
to assist humans, such as interactive reading comprehension, through
conversations. Compared to traditional single-turn question generation (SQG),
CQG is more challenging in the sense that the generated question is required
not only to be meaningful, but also to align with the occurred conversation
history. While previous studies mainly focus on how to model the flow and
alignment of the conversation, there has been no thorough study to date on
which parts of the context and history are necessary for the model. We argue
that shortening the context and history is crucial as it can help the model to
optimise more on the conversational alignment property. To this end, we propose
CoHS-CQG, a two-stage CQG framework, which adopts a CoHS module to shorten the
context and history of the input. In particular, CoHS selects contiguous
sentences and history turns according to their relevance scores by a top-p
strategy. Our model achieves state-of-the-art performances on CoQA in both the
answer-aware and answer-unaware settings.",1,0,0,0,1,0,0.529133,7.0,0.729587,35
51157c79-2525-40a9-9aac-5fa54e7de135,Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges,17,0.295904,0.547843,"This is a report on the NSF Future Directions Workshop on Automatic
Evaluation of Dialog. The workshop explored the current state of the art along
with its limitations and suggested promising directions for future work in this
important and very rapidly changing area of research.",0,0,0,0,0,0,0.593833,6.0,0.714384,105
321934a5-e0eb-4d1b-aa3a-bec74dedc488,Low Light Video Enhancement by Learning on Static Videos with Cross-Frame Attention,1,0.049642,0.151477,"The design of deep learning methods for low light video enhancement remains a
challenging problem owing to the difficulty in capturing low light and ground
truth video pairs. This is particularly hard in the context of dynamic scenes
or moving cameras where a long exposure ground truth cannot be captured. We
approach this problem by training a model on static videos such that the model
can generalize to dynamic videos. Existing methods adopting this approach
operate frame by frame and do not exploit the relationships among neighbouring
frames. We overcome this limitation through a selfcross dilated attention
module that can effectively learn to use information from neighbouring frames
even when dynamics between the frames are different during training and test
times. We validate our approach through experiments on multiple datasets and
show that our method outperforms other state-of-the-art video enhancement
algorithms when trained only on static videos.",0,1,0,0,1,0,0.944961,7.0,0.922167,34
a15aea73-7736-4f97-8908-1f3be6f3ff35,The GatedTabTransformer. An enhanced deep learning architecture for tabular modeling,6,0.0839026,0.421026,"There is an increasing interest in the application of deep learning
architectures to tabular data. One of the state-of-the-art solutions is
TabTransformer which incorporates an attention mechanism to better track
relationships between categorical features and then makes use of a standard MLP
to output its final logits. In this paper we propose multiple modifications to
the original TabTransformer performing better on binary classification tasks
for three separate datasets with more than 1% AUROC gains. Inspired by gated
MLP, linear projections are implemented in the MLP block and multiple
activation functions are tested. We also evaluate the importance of specific
hyper parameters during training.",1,1,0,0,0,0,0.893607,6.0,0.866219,28
2a965375-c4c9-40e7-b2a2-0dfd7548d4fd,Chemotaxis of sea urchin sperm cells through deep reinforcement learning,2,0.0556769,0.540249,"By imitating biological microswimmers, microrobots can be designed to
accomplish targeted delivery of cargos and biomedical manipulations at
microscale. However, it is still a great challenge to enable microrobots to
maneuver in a complex environment. Machine learning algorithms offer a tool to
boost mobility and flexibility of a synthetic microswimmer, hence could help us
design truly smart microrobots. In this work, we investigate how a model of sea
urchin sperm cell can self-learn chemotactic motion in a chemoattractant
concentration field. We employ an artificial neural network to act as a
decision-making agent and facilitate the sperm cell to discover efficient
maneuver strategies through a deep reinforcement learning (DRL) algorithm. Our
results show that chemotactic behaviours, very similar to the realistic ones,
can be achieved by the DRL utilizing only limited environmental information. In
most cases, the DRL algorithm discovers more efficient strategies than the
human-devised one. Furthermore, the DRL can even utilize an external
disturbance to facilitate the chemotactic motion if the extra flow information
is also taken into account by the artificial neural network. Our results
provide insights to the chemotactic process of sea urchin sperm cells and also
prepare guidance for the intelligent maneuver of microrobots.",0,0,0,0,0,0,0.397053,12.0,0.8091,55
3e31bc07-738d-4da1-bed7-ce2c8672dd98,Linear Leaky-Integrate-and-Fire Neuron Model Based Spiking Neural Networks and Its Mapping Relationship to Deep Neural Networks,12,0.0955669,0.782246,"Spiking neural networks (SNNs) are brain-inspired machine learning algorithms
with merits such as biological plausibility and unsupervised learning
capability. Previous works have shown that converting Artificial Neural
Networks (ANNs) into SNNs is a practical and efficient approach for
implementing an SNN. However, the basic principle and theoretical groundwork
are lacking for training a non-accuracy-loss SNN. This paper establishes a
precise mathematical mapping between the biological parameters of the Linear
Leaky-Integrate-and-Fire model (LIF)/SNNs and the parameters of ReLU-AN/Deep
Neural Networks (DNNs). Such mapping relationship is analytically proven under
certain conditions and demonstrated by simulation and real data experiments. It
can serve as the theoretical basis for the potential combination of the
respective merits of the two categories of neural networks.",0,0,0,0,0,0,0.290871,14.0,0.808759,73
3364c45f-47fe-4d94-8619-70e14d3116cc,Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification,2,0.00862292,0.124493,"The performance of learning models heavily relies on the availability and
adequacy of training data. To address the dataset adequacy issue, researchers
have extensively explored data augmentation (DA) as a promising approach. DA
generates new data instances through transformations applied to the available
data, thereby increasing dataset size and variability. This approach has
enhanced model performance and accuracy, particularly in addressing class
imbalance problems in classification tasks. However, few studies have explored
DA for the Arabic language, relying on traditional approaches such as
paraphrasing or noising-based techniques. In this paper, we propose a new
Arabic DA method that employs the recent powerful modeling technique, namely
the AraGPT-2, for the augmentation process. The generated sentences are
evaluated in terms of context, semantics, diversity, and novelty using the
Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT
transformer is used on sentiment classification tasks to evaluate the
classification performance of the augmented Arabic dataset. The experiments
were conducted on four sentiment Arabic datasets: AraSarcasm, ASTD, ATT, and
MOVIE. The selected datasets vary in size, label number, and unbalanced
classes. The results show that the proposed methodology enhanced the Arabic
sentiment text classification on all datasets with an increase in F1 score by
4% in AraSarcasm, 6% in ASTD, 9% in ATT, and 13% in MOVIE.",1,1,1,0,0,0,0.0869311,6.0,0.332175,62
82a86ddf-d5cc-4013-87e7-6cb54aa4d037,Adversarial and Random Transformations for Robust Domain Adaptation and Generalization,4,0.0120058,0.252622,"Data augmentation has been widely used to improve generalization in training
deep neural networks. Recent works show that using worst-case transformations
or adversarial augmentation strategies can significantly improve the accuracy
and robustness. However, due to the non-differentiable properties of image
transformations, searching algorithms such as reinforcement learning or
evolution strategy have to be applied, which are not computationally practical
for large scale problems. In this work, we show that by simply applying
consistency training with random data augmentation, state-of-the-art results on
domain adaptation (DA) and generalization (DG) can be obtained. To further
improve the accuracy and robustness with adversarial examples, we propose a
differentiable adversarial data augmentation method based on spatial
transformer networks (STN). The combined adversarial and random transformations
based method outperforms the state-of-the-art on multiple DA and DG benchmark
datasets. Besides, the proposed method shows desirable robustness to
corruption, which is also validated on commonly used datasets.",0,1,0,0,0,0,0.146234,10.0,0.654603,80
2001efb3-ae9e-49fb-bdc3-c81635d521a1,PVO: Panoptic Visual Odometry,14,0.188346,0.889759,"We present PVO, a novel panoptic visual odometry framework to achieve more
comprehensive modeling of the scene motion, geometry, and panoptic segmentation
information. Our PVO models visual odometry (VO) and video panoptic
segmentation (VPS) in a unified view, which makes the two tasks mutually
beneficial. Specifically, we introduce a panoptic update module into the VO
Module with the guidance of image panoptic segmentation. This Panoptic-Enhanced
VO Module can alleviate the impact of dynamic objects in the camera pose
estimation with a panoptic-aware dynamic mask. On the other hand, the
VO-Enhanced VPS Module also improves the segmentation accuracy by fusing the
panoptic segmentation result of the current frame on the fly to the adjacent
frames, using geometric information such as camera pose, depth, and optical
flow obtained from the VO Module. These two modules contribute to each other
through recurrent iterative optimization. Extensive experiments demonstrate
that PVO outperforms state-of-the-art methods in both visual odometry and video
panoptic segmentation tasks.",1,1,0,0,1,0,0.443948,7.0,0.69395,64
7938fbdf-5760-468e-bf51-f204f384cb25,Multi-Grid Redundant Bounding Box Annotation for Accurate Object Detection,1,0.00963142,0.0327656,"Modern leading object detectors are either two-stage or one-stage networks
repurposed from a deep CNN-based backbone classifier network. YOLOv3 is one
such very-well known state-of-the-art one-shot detector that takes in an input
image and divides it into an equal-sized grid matrix. The grid cell having the
center of an object is the one responsible for detecting the particular object.
This paper presents a new mathematical approach that assigns multiple grids per
object for accurately tight-fit bounding box prediction. We also propose an
effective offline copy-paste data augmentation for object detection. Our
proposed method significantly outperforms some current state-of-the-art object
detectors with a prospect for further better performance.",0,1,0,0,0,0,0.854192,10.0,0.904573,21
4c2a73f9-486a-47f6-8342-2ea0c048a54f,Extreme Masking for Learning Instance and Distributed Visual Representations,18,0.0939325,0.524127,"The paper presents a scalable approach for learning spatially distributed
visual representations over individual tokens and a holistic instance
representation simultaneously. We use self-attention blocks to represent
spatially distributed tokens, followed by cross-attention blocks to aggregate
the holistic image instance. The core of the approach is the use of extremely
large token masking (75\%-90\%) as the data augmentation for supervision. Our
model, named ExtreMA, follows the plain BYOL approach where the instance
representation from the unmasked subset is trained to predict that from the
intact input. Instead of encouraging invariance across inputs, the model is
required to capture informative variations in an image. The paper makes three
contributions: 1) It presents random masking as a strong and computationally
efficient data augmentation for siamese representation learning. 2) With
multiple sampling per instance, extreme masking greatly speeds up learning and
improves performance with more data. 3) ExtreMA obtains stronger linear probing
performance than masked modeling methods, and better transfer performance than
prior contrastive models.",0,0,0,0,0,0,0.895585,5.0,0.84113,55
a5524c62-8604-460b-a4d2-584f8dba2d52,Panini-Net: GAN Prior Based Degradation-Aware Feature Interpolation for Face Restoration,14,0.207558,0.380888,"Emerging high-quality face restoration (FR) methods often utilize pre-trained
GAN models (\textit{i.e.}, StyleGAN2) as GAN Prior. However, these methods
usually struggle to balance realness and fidelity when facing various
degradation levels. Besides, there is still a noticeable visual quality gap
compared with pre-trained GAN models. In this paper, we propose a novel GAN
Prior based degradation-aware feature interpolation network, dubbed Panini-Net,
for FR tasks by explicitly learning the abstract representations to distinguish
various degradations. Specifically, an unsupervised degradation representation
learning (UDRL) strategy is first developed to extract degradation
representations (DR) of the input degraded images. Then, a degradation-aware
feature interpolation (DAFI) module is proposed to dynamically fuse the two
types of informative features (\textit{i.e.}, features from input images and
features from GAN Prior) with flexible adaption to various degradations based
on DR. Ablation studies reveal the working mechanism of DAFI and its potential
for editable FR. Extensive experiments demonstrate that our Panini-Net achieves
state-of-the-art performance for multi-degradation face restoration and face
super-resolution. The source code is available at
https://github.com/jianzhangcs/panini.",0,1,0,0,1,0,0.930951,5.0,0.874748,53
c210d292-3b0f-4394-b7ea-240d17b69c41,Unbiased Heterogeneous Scene Graph Generation with Relation-aware Message Passing Neural Network,8,0.32894,0.762315,"Recent scene graph generation (SGG) frameworks have focused on learning
complex relationships among multiple objects in an image. Thanks to the nature
of the message passing neural network (MPNN) that models high-order
interactions between objects and their neighboring objects, they are dominant
representation learning modules for SGG. However, existing MPNN-based
frameworks assume the scene graph as a homogeneous graph, which restricts the
context-awareness of visual relations between objects. That is, they overlook
the fact that the relations tend to be highly dependent on the objects with
which the relations are associated. In this paper, we propose an unbiased
heterogeneous scene graph generation (HetSGG) framework that captures
relation-aware context using message passing neural networks. We devise a novel
message passing layer, called relation-aware message passing neural network
(RMP), that aggregates the contextual information of an image considering the
predicate type between objects. Our extensive evaluations demonstrate that
HetSGG outperforms state-of-the-art methods, especially outperforming on tail
predicate classes.",1,0,0,0,1,0,0.911086,8.0,0.909296,52
b56eedf4-9736-417c-8427-459f8bcc3b25,Proportional Fairness in Federated Learning,12,0.180531,0.471004,"With the increasingly broad deployment of federated learning (FL) systems in
the real world, it is critical but challenging to ensure fairness in FL, i.e.
reasonably satisfactory performances for each of the numerous diverse clients.
In this work, we introduce and study a new fairness notion in FL, called
proportional fairness (PF), which is based on the relative change of each
client's performance. From its connection with the bargaining games, we propose
PropFair, a novel and easy-to-implement algorithm for finding proportionally
fair solutions in FL and study its convergence properties. Through extensive
experiments on vision and language datasets, we demonstrate that PropFair can
approximately find PF solutions, and it achieves a good balance between the
average performances of all clients and of the worst 10% clients. Our code is
available at
\url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL}.",1,0,1,0,0,0,0.826473,7.0,0.850142,79
abcc7890-9269-4f86-88e5-07a07f815f1d,An Improved RaftStereo Trained with A Mixed Dataset for the Robust Vision Challenge 2022,2,0.0304834,0.164502,"Stereo-matching is a fundamental problem in computer vision. Despite recent
progress by deep learning, improving the robustness is ineluctable when
deploying stereo-matching models to real-world applications. Different from the
common practices, i.e., developing an elaborate model to achieve robustness, we
argue that collecting multiple available datasets for training is a cheaper way
to increase generalization ability. Specifically, this report presents an
improved RaftStereo trained with a mixed dataset of seven public datasets for
the robust vision challenge (denoted as iRaftStereo_RVC). When evaluated on the
training sets of Middlebury, KITTI-2015, and ETH3D, the model outperforms its
counterparts trained with only one dataset, such as the popular Sceneflow.
After fine-tuning the pre-trained model on the three datasets of the challenge,
it ranks at 2nd place on the stereo leaderboard, demonstrating the benefits of
mixed dataset pre-training.",0,1,0,1,0,0,0.507852,8.0,0.755828,29
abbce865-9131-4b08-a06d-7d84a530a862,Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting,7,0.329413,0.33315,"Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in
producing rational and factual responses. However, to establish long-term
relationships with users, the KDG model needs the capability to generate
responses in a desired style or attribute. Thus, we study a new problem:
Stylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two
challenges: (1) How to train a SKDG model where no <context, knowledge,
stylized response> triples are available. (2) How to cohere with context and
preserve the knowledge when generating a stylized response. In this paper, we
propose a novel disentangled template rewriting (DTR) method which generates
responses via combing disentangled style templates (from monolingual stylized
corpus) and content templates (from KDG corpus). The entire framework is
end-to-end differentiable and learned without supervision. Extensive
experiments on two benchmarks indicate that DTR achieves a significant
improvement on all evaluation metrics compared with previous state-of-the-art
stylized dialogue generation methods. Besides, DTR achieves comparable
performance with the state-of-the-art KDG methods in standard KDG evaluation
setting.",1,1,1,0,1,0,0.92922,6.0,0.894072,42
e311e88f-a5bb-4b03-b5ea-719f5ed355a6,A Task-aware Dual Similarity Network for Fine-grained Few-shot Learning,2,0.013715,0.0701678,"The goal of fine-grained few-shot learning is to recognize sub-categories
under the same super-category by learning few labeled samples. Most of the
recent approaches adopt a single similarity measure, that is, global or local
measure alone. However, for fine-grained images with high intra-class variance
and low inter-class variance, exploring global invariant features and
discriminative local details is quite essential. In this paper, we propose a
Task-aware Dual Similarity Network(TDSNet), which applies global features and
local patches to achieve better performance. Specifically, a local feature
enhancement module is adopted to activate the features with strong
discriminability. Besides, task-aware attention exploits the important patches
among the entire task. Finally, both the class prototypes obtained by global
features and discriminative local patches are employed for prediction.
Extensive experiments on three fine-grained datasets demonstrate that the
proposed TDSNet achieves competitive performance by comparing with other
state-of-the-art algorithms.",0,1,0,0,1,0,0.201802,8.0,0.612585,33
4aba1579-a53b-4886-a3d3-bacd9aa82244,Feedback Gradient Descent: Efficient and Stable Optimization with Orthogonality for DNNs,6,0.0,0.596777,"The optimization with orthogonality has been shown useful in training deep
neural networks (DNNs). To impose orthogonality on DNNs, both computational
efficiency and stability are important. However, existing methods utilizing
Riemannian optimization or hard constraints can only ensure stability while
those using soft constraints can only improve efficiency. In this paper, we
propose a novel method, named Feedback Gradient Descent (FGD), to our
knowledge, the first work showing high efficiency and stability simultaneously.
FGD induces orthogonality based on the simple yet indispensable Euler
discretization of a continuous-time dynamical system on the tangent bundle of
the Stiefel manifold. In particular, inspired by a numerical integration method
on manifolds called Feedback Integrators, we propose to instantiate it on the
tangent bundle of the Stiefel manifold for the first time. In the extensive
image classification experiments, FGD comprehensively outperforms the existing
state-of-the-art methods in terms of accuracy, efficiency, and stability.",0,0,0,0,1,0,0.0964723,12.0,0.675197,75
1a6c0a05-ef70-407d-b180-3475b62d859a,The Pump Scheduling Problem: A Real-World Scenario for Reinforcement Learning,1,0.00138959,0.0325333,"Deep Reinforcement Learning (DRL) has achieved remarkable success in
scenarios such as games and has emerged as a potential solution for control
tasks. That is due to its ability to leverage scalability and handle complex
dynamics. However, few works have targeted environments grounded in real-world
settings. Indeed, real-world scenarios can be challenging, especially when
faced with the high dimensionality of the state space and unknown reward
function. We release a testbed consisting of an environment simulator and
demonstrations of human operation concerning pump scheduling of a real-world
water distribution facility to facilitate research. The pump scheduling problem
can be viewed as a decision process to decide when to operate pumps to supply
water while limiting electricity consumption and meeting system constraints. To
provide a starting point, we release a well-documented codebase, present an
overview of some challenges that can be addressed and provide a baseline
representation of the problem. The code and dataset are available at
https://gitlab.com/hdonancio/pumpscheduling.",0,1,0,1,0,0,0.0569787,8.0,0.444334,46
0242c5dc-25c7-4e9a-8beb-d68a5d9d7b8e,Sim2Real Instance-Level Style Transfer for 6D Pose Estimation,6,0.116588,0.312469,"In recent years, synthetic data has been widely used in the training of 6D
pose estimation networks, in part because it automatically provides perfect
annotation at low cost. However, there are still non-trivial domain gaps, such
as differences in textures/materials, between synthetic and real data. These
gaps have a measurable impact on performance. To solve this problem, we
introduce a simulation to reality (sim2real) instance-level style transfer for
6D pose estimation network training. Our approach transfers the style of target
objects individually, from synthetic to real, without human intervention. This
improves the quality of synthetic data for training pose estimation networks.
We also propose a complete pipeline from data collection to the training of a
pose estimation network and conduct extensive evaluation on a real-world
robotic platform. Our evaluation shows significant improvement achieved by our
method in both pose estimation performance and the realism of images adapted by
the style transfer.",0,1,0,0,0,0,0.826066,7.0,0.84995,32
630f2ea8-65f0-4a7a-aad2-a27cac294a80,Parameter-Efficient Abstractive Question Answering over Tables or Text,11,0.066061,0.329895,"A long-term ambition of information seeking QA systems is to reason over
multi-modal contexts and generate natural answers to user queries. Today,
memory intensive pre-trained language models are adapted to downstream tasks
such as QA by fine-tuning the model on QA data in a specific modality like
unstructured text or structured tables. To avoid training such memory-hungry
models while utilizing a uniform architecture for each modality,
parameter-efficient adapters add and train small task-specific bottle-neck
layers between transformer layers. In this work, we study parameter-efficient
abstractive QA in encoder-decoder models over structured tabular data and
unstructured textual data using only 1.5% additional parameters for each
modality. We also ablate over adapter layers in both encoder and decoder
modules to study the efficiency-performance trade-off and demonstrate that
reducing additional trainable parameters down to 0.7%-1.0% leads to comparable
results. Our models out-perform current state-of-the-art models on tabular QA
datasets such as Tablesum and FeTaQA, and achieve comparable performance on a
textual QA dataset such as NarrativeQA using significantly less trainable
parameters than fine-tuning.",0,1,0,0,1,0,0.315937,7.0,0.631743,42
63de5b2b-e2f9-4c99-aba4-13b8a3863567,A Simple Information-Based Approach to Unsupervised Domain-Adaptive Aspect-Based Sentiment Analysis,2,0.00754019,0.0991208,"Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task which aims to extract the aspects from sentences and identify their
corresponding sentiments. Aspect term extraction (ATE) is the crucial step for
ABSA. Due to the expensive annotation for aspect terms, we often lack labeled
target domain data for fine-tuning. To address this problem, many approaches
have been proposed recently to transfer common knowledge in an unsupervised
way, but such methods have too many modules and require expensive multi-stage
preprocessing. In this paper, we propose a simple but effective technique based
on mutual information maximization, which can serve as an additional component
to enhance any kind of model for cross-domain ABSA and ATE. Furthermore, we
provide some analysis of this approach. Experiment results show that our
proposed method outperforms the state-of-the-art methods for cross-domain ABSA
by 4.32% Micro-F1 on average over 10 different domain pairs. Apart from that,
our method can be extended to other sequence labeling tasks, such as named
entity recognition (NER).",1,1,0,0,1,0,0.470959,8.0,0.742386,57
cd5f14eb-60ab-43d7-b32f-3da9f482d88f,"Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2",63,0.178379,0.884732,"The field of image synthesis has made great strides in the last couple of
years. Recent models are capable of generating images with astonishing quality.
Fine-grained evaluation of these models on some interesting categories such as
faces is still missing. Here, we conduct a quantitative comparison of three
popular systems including Stable Diffusion, Midjourney, and DALL-E 2 in their
ability to generate photorealistic faces in the wild. We find that Stable
Diffusion generates better faces than the other systems, according to the FID
score. We also introduce a dataset of generated faces in the wild dubbed GFW,
including a total of 15,076 faces. Furthermore, we hope that our study spurs
follow-up research in assessing the generative models and improving them. Data
and code are available at data and code, respectively.",0,1,0,1,0,0,0.661238,8.0,0.808725,14
d0eadf21-5318-4028-8d13-8b0c018ae07d,"Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots",9,0.368898,0.52892,"Few-shot table-to-text generation is a task of composing fluent and faithful
sentences to convey table content using limited data. Despite many efforts
having been made towards generating impressive fluent sentences by fine-tuning
powerful pre-trained language models, the faithfulness of generated content
still needs to be improved. To this end, this paper proposes a novel approach
Attend, Memorize and Generate (called AMG), inspired by the text generation
process of humans. In particular, AMG (1) attends over the multi-granularity of
context using a novel strategy based on table slot level and traditional
token-by-token level attention to exploit both the table structure and natural
linguistic information; (2) dynamically memorizes the table slot allocation
states; and (3) generates faithful sentences according to both the context and
memory allocation states. Comprehensive experiments with human evaluation on
three domains (i.e., humans, songs, and books) of the Wiki dataset show that
our model can generate higher qualified texts when compared with several
state-of-the-art baselines, in both fluency and faithfulness.",1,1,0,0,1,0,0.970308,6.0,0.941356,53
52606f6e-5948-4df4-9550-2a4c41ecb9c1,Unsupervised Domain Adaptation for Point Cloud Semantic Segmentation via Graph Matching,7,0.0209155,0.34518,"Unsupervised domain adaptation for point cloud semantic segmentation has
attracted great attention due to its effectiveness in learning with unlabeled
data. Most of existing methods use global-level feature alignment to transfer
the knowledge from the source domain to the target domain, which may cause the
semantic ambiguity of the feature space. In this paper, we propose a
graph-based framework to explore the local-level feature alignment between the
two domains, which can reserve semantic discrimination during adaptation.
Specifically, in order to extract local-level features, we first dynamically
construct local feature graphs on both domains and build a memory bank with the
graphs from the source domain. In particular, we use optimal transport to
generate the graph matching pairs. Then, based on the assignment matrix, we can
align the feature distributions between the two domains with the graph-based
local feature loss. Furthermore, we consider the correlation between the
features of different categories and formulate a category-guided contrastive
loss to guide the segmentation model to learn discriminative features on the
target domain. Extensive experiments on different synthetic-to-real and
real-to-real domain adaptation scenarios demonstrate that our method can
achieve state-of-the-art performance.",0,1,0,0,1,0,0.364589,7.0,0.657109,29
b86818f4-6847-4045-9ba5-2891599b4162,MonoGround: Detecting Monocular 3D Objects from the Ground,20,0.187132,0.696527,"Monocular 3D object detection has attracted great attention for its
advantages in simplicity and cost. Due to the ill-posed 2D to 3D mapping
essence from the monocular imaging process, monocular 3D object detection
suffers from inaccurate depth estimation and thus has poor 3D detection
results. To alleviate this problem, we propose to introduce the ground plane as
a prior in the monocular 3d object detection. The ground plane prior serves as
an additional geometric condition to the ill-posed mapping and an extra source
in depth estimation. In this way, we can get a more accurate depth estimation
from the ground. Meanwhile, to take full advantage of the ground plane prior,
we propose a depth-align training strategy and a precise two-stage depth
inference method tailored for the ground plane prior. It is worth noting that
the introduced ground plane prior requires no extra data sources like LiDAR,
stereo images, and depth information. Extensive experiments on the KITTI
benchmark show that our method could achieve state-of-the-art results compared
with other methods while maintaining a very fast speed. Our code and models are
available at https://github.com/cfzd/MonoGround.",1,1,0,0,1,0,0.749229,6.0,0.785829,45
62df594f-253e-4895-acf6-52295eda6ada,On Mitigating Hard Clusters for Face Clustering,9,0.0546557,0.355252,"Face clustering is a promising way to scale up face recognition systems using
large-scale unlabeled face images. It remains challenging to identify small or
sparse face image clusters that we call hard clusters, which is caused by the
heterogeneity, \ie, high variations in size and sparsity, of the clusters.
Consequently, the conventional way of using a uniform threshold (to identify
clusters) often leads to a terrible misclassification for the samples that
should belong to hard clusters. We tackle this problem by leveraging the
neighborhood information of samples and inferring the cluster memberships (of
samples) in a probabilistic way. We introduce two novel modules,
Neighborhood-Diffusion-based Density (NDDe) and Transition-Probability-based
Distance (TPDi), based on which we can simply apply the standard Density Peak
Clustering algorithm with a uniform threshold. Our experiments on multiple
benchmarks show that each module contributes to the final performance of our
method, and by incorporating them into other advanced face clustering methods,
these two modules can boost the performance of these methods to a new
state-of-the-art. Code is available at:
https://github.com/echoanran/On-Mitigating-Hard-Clusters.",1,1,0,0,1,0,0.257452,12.0,0.764899,33
3bf69f76-747c-4343-a83c-0bf911b20b5f,Partially Does It: Towards Scene-Level FG-SBIR with Partial Input,18,0.507731,0.924695,"We scrutinise an important observation plaguing scene-level sketch research
-- that a significant portion of scene sketches are ""partial"". A quick pilot
study reveals: (i) a scene sketch does not necessarily contain all objects in
the corresponding photo, due to the subjective holistic interpretation of
scenes, (ii) there exists significant empty (white) regions as a result of
object-level abstraction, and as a result, (iii) existing scene-level
fine-grained sketch-based image retrieval methods collapse as scene sketches
become more partial. To solve this ""partial"" problem, we advocate for a simple
set-based approach using optimal transport (OT) to model cross-modal region
associativity in a partially-aware fashion. Importantly, we improve upon OT to
further account for holistic partialness by comparing intra-modal adjacency
matrices. Our proposed method is not only robust to partial scene-sketches but
also yields state-of-the-art performance on existing datasets.",0,1,1,0,1,0,0.890022,7.0,0.883201,91
391c379d-03b6-4b4e-8ac9-7066853d21f1,Bilinear value networks,6,0.0332129,0.41405,"The dominant framework for off-policy multi-goal reinforcement learning
involves estimating goal conditioned Q-value function. When learning to achieve
multiple goals, data efficiency is intimately connected with the generalization
of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a,
g) using monolithic neural networks. To improve the generalization of the
Q-function, we propose a bilinear decomposition that represents the Q-value via
a low-rank approximation in the form of a dot product between two vector
fields. The first vector field, f(s, a), captures the environment's local
dynamics at the state s; whereas the second component, {\phi}(s, g), captures
the global relationship between the current state and the goal. We show that
our bilinear decomposition scheme substantially improves data efficiency, and
has superior transfer to out-of-distribution goals compared to prior methods.
Empirical evidence is provided on the simulated Fetch robot task-suite and
dexterous manipulation with a Shadow hand.",1,0,0,0,0,0,0.539935,12.0,0.844788,45
0acb79c0-ef30-4c90-9d2e-f3a831536f7c,PaCo: Parameter-Compositional Multi-Task Reinforcement Learning,11,0.136883,0.656791,"The purpose of multi-task reinforcement learning (MTRL) is to train a single
policy that can be applied to a set of different tasks. Sharing parameters
allows us to take advantage of the similarities among tasks. However, the gaps
between contents and difficulties of different tasks bring us challenges on
both which tasks should share the parameters and what parameters should be
shared, as well as the optimization challenges due to parameter sharing. In
this work, we introduce a parameter-compositional approach (PaCo) as an attempt
to address these challenges. In this framework, a policy subspace represented
by a set of parameters is learned. Policies for all the single tasks lie in
this subspace and can be composed by interpolating with the learned set. It
allows not only flexible parameter sharing but also a natural way to improve
training. We demonstrate the state-of-the-art performance on Meta-World
benchmarks, verifying the effectiveness of the proposed approach.",1,0,0,0,1,0,0.387836,10.0,0.767875,42
210219ff-fe21-4d75-9c75-db0795fa686c,Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task,10,0.0495483,0.676742,"Although transformer-based Neural Language Models demonstrate impressive
performance on a variety of tasks, their generalization abilities are not well
understood. They have been shown to perform strongly on subject-verb number
agreement in a wide array of settings, suggesting that they learned to track
syntactic dependencies during their training even without explicit supervision.
In this paper, we examine the extent to which BERT is able to perform
lexically-independent subject-verb number agreement (NA) on targeted syntactic
templates. To do so, we disrupt the lexical patterns found in naturally
occurring stimuli for each targeted structure in a novel fine-grained analysis
of BERT's behavior. Our results on nonce sentences suggest that the model
generalizes well for simple templates, but fails to perform
lexically-independent syntactic generalization when as little as one attractor
is present.",1,0,0,0,0,0,0.0853301,7.0,0.4248,11
57f62288-62b4-4e7c-8b5d-00f0b2ec2d42,Enriching Abusive Language Detection with Community Context,3,0.080407,0.152051,"Uses of pejorative expressions can be benign or actively empowering. When
models for abuse detection misclassify these expressions as derogatory, they
inadvertently censor productive conversations held by marginalized groups. One
way to engage with non-dominant perspectives is to add context around
conversations. Previous research has leveraged user- and thread-level features,
but it often neglects the spaces within which productive conversations take
place. Our paper highlights how community context can improve classification
outcomes in abusive language detection. We make two main contributions to this
end. First, we demonstrate that online communities cluster by the nature of
their support towards victims of abuse. Second, we establish how community
context improves accuracy and reduces the false positive rates of
state-of-the-art abusive language classifiers. These findings suggest a
promising direction for context-aware models in abusive language research.",0,1,0,0,0,0,0.439256,7.0,0.69189,71
4953dd6a-adc1-43d8-b712-1350fcf34ef1,SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models,40,0.897813,0.787109,"Recent research showed promising results on combining pretrained language
models (LMs) with canonical utterance for few-shot semantic parsing. The
canonical utterance is often lengthy and complex due to the compositional
structure of formal languages. Learning to generate such canonical utterance
requires significant amount of data to reach high performance. Fine-tuning with
only few-shot samples, the LMs can easily forget pretrained knowledge, overfit
spurious biases, and suffer from compositionally out-of-distribution
generalization errors. To tackle these issues, we propose a novel few-shot
semantic parsing method -- SeqZero. SeqZero decomposes the problem into a
sequence of sub-problems, which correspond to the sub-clauses of the formal
language. Based on the decomposition, the LMs only need to generate short
answers using prompts for predicting sub-clauses. Thus, SeqZero avoids
generating a long canonical utterance at once. Moreover, SeqZero employs not
only a few-shot model but also a zero-shot model to alleviate the overfitting.
In particular, SeqZero brings out the merits from both models via ensemble
equipped with our proposed constrained rescaling. SeqZero achieves SOTA
performance of BART-based models on GeoQuery and EcommerceQuery, which are two
few-shot datasets with compositional data split.",0,1,0,0,1,0,0.983032,4.0,0.948944,52
079d0444-d612-4ab8-9f92-52e488ddebdb,HSADML: Hyper-Sphere Angular Deep Metric based Learning for Brain Tumor Classification,2,0.0346976,0.117283,"Brain Tumors are abnormal mass of clustered cells penetrating regions of
brain. Their timely identification and classification help doctors to provide
appropriate treatment. However, Classifi-cation of Brain Tumors is quite
intricate because of high-intra class similarity and low-inter class
variability. Due to morphological similarity amongst various MRI-Slices of
different classes the challenge deepens more. This all leads to hampering
generalizability of classification models. To this end, this paper proposes
HSADML, a novel framework which enables deep metric learning (DML) using
SphereFace Loss. SphereFace loss embeds the features into a
hyperspheric-manifold and then imposes margin on the embeddings to enhance
differentiability between the classes. With utilization of SphereFace loss
based deep metric learning it is ensured that samples from class clustered
together while the different ones are pushed apart. Results reflects the
promi-nence in the approach, the proposed framework achieved state-of-the-art
98.69% validation accu-racy using k-NN (k=1) and this is significantly higher
than normal SoftMax Loss training which though obtains 98.47% validation
accuracy but that too with limited inter-class separability and intra-class
closeness. Experimental analysis done over various classifiers and loss
function set-tings suggests potential in the approach.",0,1,0,0,1,0,0.800235,10.0,0.886718,29
390837e0-dcd3-4691-b40a-f0acc0821e78,Towards Sequence-Level Training for Visual Tracking,15,0.0504005,0.844956,"Despite the extensive adoption of machine learning on the task of visual
object tracking, recent learning-based approaches have largely overlooked the
fact that visual tracking is a sequence-level task in its nature; they rely
heavily on frame-level training, which inevitably induces inconsistency between
training and testing in terms of both data distributions and task objectives.
This work introduces a sequence-level training strategy for visual tracking
based on reinforcement learning and discusses how a sequence-level design of
data sampling, learning objectives, and data augmentation can improve the
accuracy and robustness of tracking algorithms. Our experiments on standard
benchmarks including LaSOT, TrackingNet, and GOT-10k demonstrate that four
representative tracking models, SiamRPN++, SiamAttn, TransT, and TrDiMP,
consistently improve by incorporating the proposed methods in training without
modifying architectures.",1,1,0,0,0,0,0.274853,8.0,0.656929,50
1c46fa75-7ed7-4134-9a07-2f9b0381af88,Toxicity Detection for Indic Multilingual Social Media Content,2,0.0464966,0.0808696,"Toxic content is one of the most critical issues for social media platforms
today. India alone had 518 million social media users in 2020. In order to
provide a good experience to content creators and their audience, it is crucial
to flag toxic comments and the users who post that. But the big challenge is
identifying toxicity in low resource Indic languages because of the presence of
multiple representations of the same text. Moreover, the posts/comments on
social media do not adhere to a particular format, grammar or sentence
structure; this makes the task of abuse detection even more challenging for
multilingual social media platforms. This paper describes the system proposed
by team 'Moj Masti' using the data provided by ShareChat/Moj in \emph{IIIT-D
Multilingual Abusive Comment Identification} challenge. We focus on how we can
leverage multilingual transformer based pre-trained and fine-tuned models to
approach code-mixed/code-switched classification tasks. Our best performing
system was an ensemble of XLM-RoBERTa and MuRIL which achieved a Mean F-1 score
of 0.9 on the test data/leaderboard. We also observed an increase in the
performance by adding transliterated data. Furthermore, using weak metadata,
ensembling and some post-processing techniques boosted the performance of our
system, thereby placing us 1st on the leaderboard.",0,1,0,0,0,0,0.991992,5.0,0.992973,10
621159f8-77d5-4859-8cc6-a948a553213f,imitation: Clean Imitation Learning Implementations,14,0.156196,0.396492,"imitation provides open-source implementations of imitation and reward
learning algorithms in PyTorch. We include three inverse reinforcement learning
(IRL) algorithms, three imitation learning algorithms and a preference
comparison algorithm. The implementations have been benchmarked against
previous results, and automated tests cover 98% of the code. Moreover, the
algorithms are implemented in a modular fashion, making it simple to develop
novel algorithms in the framework. Our source code, including documentation and
examples, is available at https://github.com/HumanCompatibleAI/imitation",1,1,0,0,0,1,0.620709,9.0,0.817726,16
831d288a-cd9b-46e5-a880-6edba9e387d9,Learning Audio-Visual embedding for Person Verification in the Wild,3,0.0943285,0.450167,"It has already been observed that audio-visual embedding is more robust than
uni-modality embedding for person verification. Here, we proposed a novel
audio-visual strategy that considers aggregators from a fusion perspective.
First, we introduced weight-enhanced attentive statistics pooling for the first
time in face verification. We find that a strong correlation exists between
modalities during pooling, so joint attentive pooling is proposed which
contains cycle consistency to learn the implicit inter-frame weight. Finally,
each modality is fused with a gated attention mechanism to gain robust
audio-visual embedding. All the proposed models are trained on the VoxCeleb2
dev dataset and the best system obtains 0.18%, 0.27%, and 0.49% EER on three
official trial lists of VoxCeleb1 respectively, which is to our knowledge the
best-published results for person verification.",0,1,0,0,1,0,0.832414,8.0,0.871336,29
1f21e844-0b9e-4f6b-9a51-a883f4ee33c5,Spatial Transformer Network on Skeleton-based Gait Recognition,14,0.199926,0.885097,"Skeleton-based gait recognition models usually suffer from the robustness
problem, as the Rank-1 accuracy varies from 90\% in normal walking cases to
70\% in walking with coats cases. In this work, we propose a state-of-the-art
robust skeleton-based gait recognition model called Gait-TR, which is based on
the combination of spatial transformer frameworks and temporal convolutional
networks. Gait-TR achieves substantial improvements over other skeleton-based
gait models with higher accuracy and better robustness on the well-known gait
dataset CASIA-B. Particularly in walking with coats cases, Gait-TR get a 90\%
Rank-1 gait recognition accuracy rate, which is higher than the best result of
silhouette-based models, which usually have higher accuracy than the
silhouette-based gait recognition models. Moreover, our experiment on CASIA-B
shows that the spatial transformer can extract gait features from the human
skeleton better than the widely used graph convolutional network.",0,1,1,0,1,0,0.661092,5.0,0.69388,55
a5e2b5f3-70a6-4831-a06d-83a7f8b29749,Guiding Attention using Partial-Order Relationships for Image Captioning,3,0.0316309,0.245772,"The use of attention models for automated image captioning has enabled many
systems to produce accurate and meaningful descriptions for images. Over the
years, many novel approaches have been proposed to enhance the attention
process using different feature representations. In this paper, we extend this
approach by creating a guided attention network mechanism, that exploits the
relationship between the visual scene and text-descriptions using spatial
features from the image, high-level information from the topics, and temporal
context from caption generation, which are embedded together in an ordered
embedding space. A pairwise ranking objective is used for training this
embedding space which allows similar images, topics and captions in the shared
semantic space to maintain a partial order in the visual-semantic hierarchy and
hence, helps the model to produce more visually accurate captions. The
experimental results based on MSCOCO dataset shows the competitiveness of our
approach, with many state-of-the-art models on various evaluation metrics.",0,1,0,0,1,0,0.349198,10.0,0.754554,37
086bc19a-c3bc-4e17-b590-b8ec906382f8,A Structure-Guided Diffusion Model for Large-Hole Image Completion,1,0.0102786,0.0847646,"Image completion techniques have made significant progress in filling missing
regions (i.e., holes) in images. However, large-hole completion remains
challenging due to limited structural information. In this paper, we address
this problem by integrating explicit structural guidance into diffusion-based
image completion, forming our structure-guided diffusion model (SGDM). It
consists of two cascaded diffusion probabilistic models: structure and texture
generators. The structure generator generates an edge image representing
plausible structures within the holes, which is then used for guiding the
texture generation process. To train both generators jointly, we devise a novel
strategy that leverages optimal Bayesian denoising, which denoises the output
of the structure generator in a single step and thus allows backpropagation.
Our diffusion-based approach enables a diversity of plausible completions,
while the editable edges allow for editing parts of an image. Our experiments
on natural scene (Places) and face (CelebA-HQ) datasets demonstrate that our
method achieves a superior or comparable visual quality compared to
state-of-the-art approaches. The code is available for research purposes at
https://github.com/UdonDa/Structure_Guided_Diffusion_Model.",1,1,0,0,1,0,0.787997,6.0,0.804927,74
67d21991-c4b5-46ac-a714-308442be0ec2,Meta-Learning Sparse Compression Networks,17,0.0683941,0.604255,"Recent work in Deep Learning has re-imagined the representation of data as
functions mapping from a coordinate space to an underlying continuous signal.
When such functions are approximated by neural networks this introduces a
compelling alternative to the more common multi-dimensional array
representation. Recent work on such Implicit Neural Representations (INRs) has
shown that - following careful architecture search - INRs can outperform
established compression methods such as JPEG (e.g. Dupont et al., 2021). In
this paper, we propose crucial steps towards making such ideas scalable:
Firstly, we employ state-of-the-art network sparsification techniques to
drastically improve compression. Secondly, introduce the first method allowing
for sparsification to be employed in the inner-loop of commonly used
Meta-Learning algorithms, drastically improving both compression and the
computational cost of learning INRs. The generality of this formalism allows us
to present results on diverse data modalities such as images, manifolds, signed
distance functions, 3D shapes and scenes, several of which establish new
state-of-the-art results.",0,1,0,0,0,0,0.444232,7.0,0.694074,54
a8784e67-a607-4924-9a35-f18fb6ab6e75,Language Models are Multilingual Chain-of-Thought Reasoners,155,0.872479,0.972928,"We evaluate the reasoning abilities of large language models in multilingual
settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by
manually translating 250 grade-school math problems from the GSM8K dataset
(Cobbe et al., 2021) into ten typologically diverse languages. We find that the
ability to solve MGSM problems via chain-of-thought prompting emerges with
increasing model scale, and that models have strikingly strong multilingual
reasoning abilities, even in underrepresented languages such as Bengali and
Swahili. Finally, we show that the multilingual reasoning abilities of language
models extend to other tasks such as commonsense reasoning and word-in-context
semantic judgment. The MGSM benchmark is publicly available at
https://github.com/google-research/url-nlp.",0,0,1,1,0,0,0.88857,4.0,0.794113,53
2a3f7f71-d0c4-4804-806a-10ac57bcd53a,Explainability as statistical inference,2,0.0,0.0428235,"A wide variety of model explanation approaches have been proposed in recent
years, all guided by very different rationales and heuristics. In this paper,
we take a new route and cast interpretability as a statistical inference
problem. We propose a general deep probabilistic model designed to produce
interpretable predictions. The model parameters can be learned via maximum
likelihood, and the method can be adapted to any predictor network architecture
and any type of prediction problem. Our method is a case of amortized
interpretability models, where a neural network is used as a selector to allow
for fast interpretation at inference time. Several popular interpretability
methods are shown to be particular cases of regularised maximum likelihood for
our general model. We propose new datasets with ground truth selection which
allow for the evaluation of the features importance map. Using these datasets,
we show experimentally that using multiple imputation provides more reasonable
interpretations.",0,0,0,1,0,1,0.0265698,9.0,0.419557,66
65be2c83-75e4-4811-abd7-afd53051b0ad,Graph Augmentation Learning,12,0.0761005,0.365285,"Graph Augmentation Learning (GAL) provides outstanding solutions for graph
learning in handling incomplete data, noise data, etc. Numerous GAL methods
have been proposed for graph-based applications such as social network analysis
and traffic flow forecasting. However, the underlying reasons for the
effectiveness of these GAL methods are still unclear. As a consequence, how to
choose optimal graph augmentation strategy for a certain application scenario
is still in black box. There is a lack of systematic, comprehensive, and
experimentally validated guideline of GAL for scholars. Therefore, in this
survey, we in-depth review GAL techniques from macro (graph), meso (subgraph),
and micro (node/edge) levels. We further detailedly illustrate how GAL enhance
the data quality and the model performance. The aggregation mechanism of
augmentation strategies and graph learning models are also discussed by
different application scenarios, i.e., data-specific, model-specific, and
hybrid scenarios. To better show the outperformance of GAL, we experimentally
validate the effectiveness and adaptability of different GAL strategies in
different downstream tasks. Finally, we share our insights on several open
issues of GAL, including heterogeneity, spatio-temporal dynamics, scalability,
and generalization.",1,0,0,0,0,0,0.6523,4.0,0.611361,69
78c43c63-4a87-40b7-b583-2e442f5b001c,Fuzzy Rough Sets Based on Fuzzy Quantification,2,0.0244451,0.266716,"One of the weaknesses of classical (fuzzy) rough sets is their sensitivity to
noise, which is particularly undesirable for machine learning applications. One
approach to solve this issue is by making use of fuzzy quantifiers, as done by
the vaguely quantified fuzzy rough set (VQFRS) model. While this idea is
intuitive, the VQFRS model suffers from both theoretical flaws as well as from
suboptimal performance in applications. In this paper, we improve on VQFRS by
introducing fuzzy quantifier-based fuzzy rough sets (FQFRS), an intuitive
generalization of fuzzy rough sets that makes use of general unary and binary
quantification models. We show how several existing models fit in this
generalization as well as how it inspires novel ones. Several binary
quantification models are proposed to be used with FQFRS. We conduct a
theoretical study of their properties, and investigate their potential by
applying them to classification problems. In particular, we highlight Yager's
Weighted Implication-based (YWI) binary quantification model, which induces a
fuzzy rough set model that is both a significant improvement on VQFRS, as well
as a worthy competitor to the popular ordered weighted averaging based fuzzy
rough set (OWAFRS) model.",0,0,0,0,0,0,0.000155414,25.0,0.584849,38
ab76beb7-9817-41de-b1c2-da4fc168b17c,Extension: Adaptive Sampling with Implicit Radiance Field,5,0.0579617,0.142682,"This manuscript discusses the extension of adaptive light field sampling with
implicit radiance fields.",0,1,0,0,0,0,0.010228,11.0,0.437552,29
0debcd85-b90d-4bb7-93dd-033017b51929,Mining Error Templates for Grammatical Error Correction,2,0.077524,0.222005,"Some grammatical error correction (GEC) systems incorporate hand-crafted
rules and achieve positive results. However, manually defining rules is
time-consuming and laborious. In view of this, we propose a method to mine
error templates for GEC automatically. An error template is a regular
expression aiming at identifying text errors. We use the web crawler to acquire
such error templates from the Internet. For each template, we further select
the corresponding corrective action by using the language model perplexity as a
criterion. We have accumulated 1,119 error templates for Chinese GEC based on
this method. Experimental results on the newly proposed CTC-2021 Chinese GEC
benchmark show that combing our error templates can effectively improve the
performance of a strong GEC system, especially on two error types with very
little training data. Our error templates are available at
\url{https://github.com/HillZhang1999/gec_error_template}.",1,1,0,0,0,0,0.445288,11.0,0.805614,17
499ef5c4-5bf7-4108-bb3a-75d020cdaf2f,Anomaly detection optimization using big data and deep learning to reduce false-positive,35,0.125829,0.561707,"Anomaly-based Intrusion Detection System (IDS) has been a hot research topic
because of its ability to detect new threats rather than only memorized
signatures threats of signature-based IDS. Especially after the availability of
advanced technologies that increase the number of hacking tools and increase
the risk impact of an attack. The problem of any anomaly-based model is its
high false-positive rate. The high false-positive rate is the reason why
anomaly IDS is not commonly applied in practice. Because anomaly-based models
classify an unseen pattern as a threat where it may be normal but not included
in the training dataset. This type of problem is called overfitting where the
model is not able to generalize. Optimizing Anomaly-based models by having a
big training dataset that includes all possible normal cases may be an optimal
solution but could not be applied in practice. Although we can increase the
number of training samples to include much more normal cases, still we need a
model that has more ability to generalize. In this research paper, we propose
applying deep model instead of traditional models because it has more ability
to generalize. Thus, we will obtain less false-positive by using big data and
deep model. We made a comparison between machine learning and deep learning
algorithms in the optimization of anomaly-based IDS by decreasing the
false-positive rate. We did an experiment on the NSL-KDD benchmark and compared
our results with one of the best used classifiers in traditional learning in
IDS optimization. The experiment shows 10% lower false-positive by using deep
learning instead of traditional learning.",0,1,0,0,0,0,0.000840339,13.0,0.331483,41
26632efa-4be2-4642-86cc-22c740821fdd,Causal Balancing for Domain Generalization,10,0.0550113,0.471673,"While machine learning models rapidly advance the state-of-the-art on various
real-world tasks, out-of-domain (OOD) generalization remains a challenging
problem given the vulnerability of these models to spurious correlations. We
propose a balanced mini-batch sampling strategy to transform a biased data
distribution into a spurious-free balanced distribution, based on the
invariance of the underlying causal mechanisms for the data generation process.
We argue that the Bayes optimal classifiers trained on such balanced
distribution are minimax optimal across a diverse enough environment space. We
also provide an identifiability guarantee of the latent variable model of the
proposed data generation process, when utilizing enough train environments.
Experiments are conducted on DomainBed, demonstrating empirically that our
method obtains the best performance across 20 baselines reported on the
benchmark.",1,0,0,0,1,0,0.461242,6.0,0.651681,79
5d35185e-725a-4a85-9a62-425ef8de6395,Graph Reasoning Transformer for Image Parsing,11,0.0956118,0.614242,"Capturing the long-range dependencies has empirically proven to be effective
on a wide range of computer vision tasks. The progressive advances on this
topic have been made through the employment of the transformer framework with
the help of the multi-head attention mechanism. However, the attention-based
image patch interaction potentially suffers from problems of redundant
interactions of intra-class patches and unoriented interactions of inter-class
patches. In this paper, we propose a novel Graph Reasoning Transformer (GReaT)
for image parsing to enable image patches to interact following a relation
reasoning pattern. Specifically, the linearly embedded image patches are first
projected into the graph space, where each node represents the implicit visual
center for a cluster of image patches and each edge reflects the relation
weight between two adjacent nodes. After that, global relation reasoning is
performed on this graph accordingly. Finally, all nodes including the relation
information are mapped back into the original space for subsequent processes.
Compared to the conventional transformer, GReaT has higher interaction
efficiency and a more purposeful interaction pattern. Experiments are carried
out on the challenging Cityscapes and ADE20K datasets. Results show that GReaT
achieves consistent performance gains with slight computational overheads on
the state-of-the-art transformer baselines.",1,0,0,0,1,0,0.871928,7.0,0.872988,94
5d12e717-c269-4205-8758-1eb705a7827a,Surgical Fine-Tuning Improves Adaptation to Distribution Shifts,109,0.271679,0.906729,"A common approach to transfer learning under distribution shift is to
fine-tune the last few layers of a pre-trained model, preserving learned
features while also adapting to the new task. This paper shows that in such
settings, selectively fine-tuning a subset of layers (which we term surgical
fine-tuning) matches or outperforms commonly used fine-tuning approaches.
Moreover, the type of distribution shift influences which subset is more
effective to tune: for example, for image corruptions, fine-tuning only the
first few layers works best. We validate our findings systematically across
seven real-world data tasks spanning three types of distribution shifts.
Theoretically, we prove that for two-layer neural networks in an idealized
setting, first-layer tuning can outperform fine-tuning all layers. Intuitively,
fine-tuning more parameters on a small target dataset can cause information
learned during pre-training to be forgotten, and the relevant information
depends on the type of shift.",0,0,0,0,0,1,0.253772,6.0,0.527007,99
f82655ac-6b6b-4046-bb3b-8941f910f2ed,Efficient Speech Translation with Dynamic Latent Perceivers,2,0.0144169,0.23705,"Transformers have been the dominant architecture for Speech Translation in
recent years, achieving significant improvements in translation quality. Since
speech signals are longer than their textual counterparts, and due to the
quadratic complexity of the Transformer, a down-sampling step is essential for
its adoption in Speech Translation. Instead, in this research, we propose to
ease the complexity by using a Perceiver encoder to map the speech inputs to a
fixed-length latent representation. Furthermore, we introduce a novel way of
training Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent
spaces without any additional computational overhead. Speech-to-Text Perceivers
with DLA can match the performance of Transformer baselines across three
language pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to
DLA at inference, and can be flexibly deployed with various computational
budgets, without significant drops in translation quality.",1,1,0,0,0,0,0.813039,7.0,0.843926,25
e3f9ed24-b5b3-4bcc-963f-9e27a4d2a2b3,"BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking",10,0.0963886,0.468595,"Estimating human motion from video is an active research area due to its many
potential applications. Most state-of-the-art methods predict human shape and
posture estimates for individual images and do not leverage the temporal
information available in video. Many ""in the wild"" sequences of human motion
are captured by a moving camera, which adds the complication of conflated
camera and human motion to the estimation. We therefore present BodySLAM, a
monocular SLAM system that jointly estimates the position, shape, and posture
of human bodies, as well as the camera trajectory. We also introduce a novel
human motion model to constrain sequential body postures and observe the scale
of the scene. Through a series of experiments on video sequences of human
motion captured by a moving monocular camera, we demonstrate that BodySLAM
improves estimates of all human body parameters and camera poses when compared
to estimating these separately.",1,1,0,0,0,0,0.418002,10.0,0.777676,64
ac6be3b9-f92e-4b66-b50d-74abba392ca6,ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering,44,0.614723,0.75274,"With the recent advance in large pre-trained language models, researchers
have achieved record performances in NLP tasks that mostly focus on language
pattern matching. The community is experiencing the shift of the challenge from
how to model language to the imitation of complex reasoning abilities like
human beings. In this work, we investigate the application domain of finance
that involves real-world, complex numerical reasoning. We propose a new
large-scale dataset, ConvFinQA, aiming to study the chain of numerical
reasoning in conversational question answering. Our dataset poses great
challenge in modeling long-range, complex numerical reasoning paths in
real-world conversations. We conduct comprehensive experiments and analyses
with both the neural symbolic methods and the prompting-based methods, to
provide insights into the reasoning mechanisms of these two divisions. We
believe our new dataset should serve as a valuable resource to push forward the
exploration of real-world, complex reasoning tasks as the next research focus.
Our dataset and code is publicly available at
https://github.com/czyssrs/ConvFinQA.",0,0,1,1,0,0,0.875713,7.0,0.875059,32
a8498ce7-9d51-40c7-8fde-534e167a30b5,EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain,20,0.268838,0.929675,"Existing summarization datasets come with two main drawbacks: (1) They tend
to focus on overly exposed domains, such as news articles or wiki-like texts,
and (2) are primarily monolingual, with few multilingual datasets. In this
work, we propose a novel dataset, called EUR-Lex-Sum, based on manually curated
document summaries of legal acts from the European Union law platform
(EUR-Lex). Documents and their respective summaries exist as cross-lingual
paragraph-aligned data in several of the 24 official European languages,
enabling access to various cross-lingual and lower-resourced summarization
setups. We obtain up to 1,500 document/summary pairs per language, including a
subset of 375 cross-lingually aligned legal acts with texts available in all 24
languages. In this work, the data acquisition process is detailed and key
characteristics of the resource are compared to existing summarization
resources. In particular, we illustrate challenging sub-problems and open
questions on the dataset that could help the facilitation of future research in
the direction of domain-specific cross-lingual summarization. Limited by the
extreme length and language diversity of samples, we further conduct
experiments with suitable extractive monolingual and cross-lingual baselines
for future work. Code for the extraction as well as access to our data and
baselines is available online at: https://github.com/achouhan93/eur-lex-sum.",1,0,1,1,0,0,0.575251,7.0,0.747911,53
233e088c-6660-4f63-8f75-9e2a24b0327a,Human-in-the-loop online multi-agent approach to increase trustworthiness in ML models through trust scores and data augmentation,1,0.0,0.0650973,"Increasing a ML model accuracy is not enough, we must also increase its
trustworthiness. This is an important step for building resilient AI systems
for safety-critical applications such as automotive, finance, and healthcare.
For that purpose, we propose a multi-agent system that combines both machine
and human agents. In this system, a checker agent calculates a trust score of
each instance (which penalizes overconfidence and overcautiousness in
predictions) using an agreement-based method and ranks it; then an improver
agent filters the anomalous instances based on a human rule-based procedure
(which is considered safe), gets the human labels, applies geometric data
augmentation, and retrains with the augmented data using transfer learning. We
evaluate the system on corrupted versions of the MNIST and FashionMNIST
datasets. We get an improvement in accuracy and trust score with just few
additional labels compared to a baseline approach.",0,1,0,0,0,0,0.0776467,10.0,0.587512,42
1fab9e4d-aa8a-4142-9878-9f424a1cdafd,Transformer based Urdu Handwritten Text Optical Character Reader,3,0.205137,0.437774,"Extracting Handwritten text is one of the most important components of
digitizing information and making it available for large scale setting.
Handwriting Optical Character Reader (OCR) is a research problem in computer
vision and natural language processing computing, and a lot of work has been
done for English, but unfortunately, very little work has been done for low
resourced languages such as Urdu. Urdu language script is very difficult
because of its cursive nature and change of shape of characters based on it's
relative position, therefore, a need arises to propose a model which can
understand complex features and generalize it for every kind of handwriting
style. In this work, we propose a transformer based Urdu Handwritten text
extraction model. As transformers have been very successful in Natural Language
Understanding task, we explore them further to understand complex Urdu
Handwriting.",0,1,0,0,0,0,0.732205,6.0,0.77772,45
441dfcb5-ce68-43af-add5-4f0222ca658d,MTet: Multi-domain Translation for English and Vietnamese,6,0.146684,0.253422,"We introduce MTet, the largest publicly available parallel corpus for
English-Vietnamese translation. MTet consists of 4.2M high-quality training
sentence pairs and a multi-domain test set refined by the Vietnamese research
community. Combining with previous works on English-Vietnamese translation, we
grow the existing parallel dataset to 6.2M sentence pairs. We also release the
first pretrained model EnViT5 for English and Vietnamese languages. Combining
both resources, our model significantly outperforms previous state-of-the-art
results by up to 2 points in translation BLEU score, while being 1.6 times
smaller.",0,1,0,1,1,0,0.788156,7.0,0.832863,37
e8c60f41-892e-4fe9-bba2-d966750e01cb,Feature Engineering and Classification Models for Partial Discharge in Power Transformers,1,0.0267314,0.230516,"To ensure reliability, power transformers are monitored for partial discharge
(PD) events, which are symptoms of transformer failure. Since failures can have
catastrophic cascading consequences, it is critical to preempt them as early as
possible. Our goal is to classify PDs as corona, floating, particle, or void,
to gain an understanding of the failure location. Using phase resolved PD
signal data, we create a small set of features, which can be used to classify
PDs with high accuracy. This set of features consists of the total magnitude,
the maximum magnitude, and the length of the longest empty band. These features
represent the entire signal and not just a single phase, so the feature set has
a fixed size and is easily comprehensible. With both Random Forest and SVM
classification methods, we attain a 99% classification accuracy, which is
significantly higher than classification using phase based feature sets such as
phase magnitude. Furthermore, we develop a stacking ensemble to combine several
classification models, resulting in a superior model that outperforms existing
methods in both accuracy and variance.",0,1,0,0,0,0,0.000747771,34.0,0.740957,15
8381af8b-6cb2-4673-afb2-81f80be7e964,Masked Event Modeling: Self-Supervised Pretraining for Event Cameras,9,0.12126,0.603911,"Event cameras asynchronously capture brightness changes with low latency,
high temporal resolution, and high dynamic range. However, annotation of event
data is a costly and laborious process, which limits the use of deep learning
methods for classification and other semantic tasks with the event modality. To
reduce the dependency on labeled event data, we introduce Masked Event Modeling
(MEM), a self-supervised framework for events. Our method pretrains a neural
network on unlabeled events, which can originate from any event camera
recording. Subsequently, the pretrained model is finetuned on a downstream
task, leading to a consistent improvement of the task accuracy. For example,
our method reaches state-of-the-art classification accuracy across three
datasets, N-ImageNet, N-Cars, and N-Caltech101, increasing the top-1 accuracy
of previous work by significant margins. When tested on real-world event data,
MEM is even superior to supervised RGB-based pretraining. The models pretrained
with MEM are also label-efficient and generalize well to the dense task of
semantic image segmentation.",1,1,0,0,1,0,0.848936,5.0,0.805433,79
9c5f168e-575a-4b28-8c24-1e987f8431f8,SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation,12,0.123782,0.535058,"Feature regression is a simple way to distill large neural network models to
smaller ones. We show that with simple changes to the network architecture,
regression can outperform more complex state-of-the-art approaches for
knowledge distillation from self-supervised models. Surprisingly, the addition
of a multi-layer perceptron head to the CNN backbone is beneficial even if used
only during distillation and discarded in the downstream task. Deeper
non-linear projections can thus be used to accurately mimic the teacher without
changing inference architecture and time. Moreover, we utilize independent
projection heads to simultaneously distill multiple teacher networks. We also
find that using the same weakly augmented image as input for both teacher and
student networks aids distillation. Experiments on ImageNet dataset demonstrate
the efficacy of the proposed changes in various self-supervised distillation
settings.",1,1,0,0,1,0,0.966386,8.0,0.951528,54
32a5fa80-a61c-4f0e-b060-bdad75bc0b26,On the Influence of Explainable AI on Automation Bias,16,0.351977,0.570767,"Artificial intelligence (AI) is gaining momentum, and its importance for the
future of work in many areas, such as medicine and banking, is continuously
rising. However, insights on the effective collaboration of humans and AI are
still rare. Typically, AI supports humans in decision-making by addressing
human limitations. However, it may also evoke human bias, especially in the
form of automation bias as an over-reliance on AI advice. We aim to shed light
on the potential to influence automation bias by explainable AI (XAI). In this
pre-test, we derive a research model and describe our study design.
Subsequentially, we conduct an online experiment with regard to hotel review
classifications and discuss first results. We expect our research to contribute
to the design and development of safe hybrid intelligence systems.",0,0,0,0,0,0,0.575879,11.0,0.839737,53
841847b7-670c-4747-bd64-e2d8f1232f2d,On Robust Incremental Learning over Many Multilingual Steps,1,0.00955979,0.0969509,"Recent work in incremental learning has introduced diverse approaches to
tackle catastrophic forgetting from data augmentation to optimized training
regimes. However, most of them focus on very few training steps. We propose a
method for robust incremental learning over dozens of fine-tuning steps using
data from a variety of languages. We show that a combination of
data-augmentation and an optimized training regime allows us to continue
improving the model even for as many as fifty training steps. Crucially, our
augmentation strategy does not require retaining access to previous training
data and is suitable in scenarios with privacy constraints.",0,1,0,0,0,0,0.420336,8.0,0.723019,26
580034de-5650-48dc-a2e2-f6880a2e723b,Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction,6,0.132442,0.758732,"Recently, prompt-tuning has attracted growing interests in event argument
extraction (EAE). However, the existing prompt-tuning methods have not achieved
satisfactory performance due to the lack of consideration of entity
information. In this paper, we propose a bi-directional iterative prompt-tuning
method for EAE, where the EAE task is treated as a cloze-style task to take
full advantage of entity information and pre-trained language models (PLMs).
Furthermore, our method explores event argument interactions by introducing the
argument roles of contextual entities into prompt construction. Since template
and verbalizer are two crucial components in a cloze-style prompt, we propose
to utilize the role label semantic knowledge to construct a semantic verbalizer
and design three kinds of templates for the EAE task. Experiments on the ACE
2005 English dataset with standard and low-resource settings show that the
proposed method significantly outperforms the peer state-of-the-art methods.
Our code is available at https://github.com/HustMinsLab/BIP.",1,1,0,0,1,0,0.898363,5.0,0.843504,33
918beab3-c176-4332-b44e-c144f6b54f53,Changing the Representation: Examining Language Representation for Neural Sign Language Production,14,0.397973,0.636322,"Neural Sign Language Production (SLP) aims to automatically translate from
spoken language sentences to sign language videos. Historically the SLP task
has been broken into two steps; Firstly, translating from a spoken language
sentence to a gloss sequence and secondly, producing a sign language video
given a sequence of glosses. In this paper we apply Natural Language Processing
techniques to the first step of the SLP pipeline. We use language models such
as BERT and Word2Vec to create better sentence level embeddings, and apply
several tokenization techniques, demonstrating how these improve performance on
the low resource translation task of Text to Gloss. We introduce Text to
HamNoSys (T2H) translation, and show the advantages of using a phonetic
representation for sign language translation rather than a sign level gloss
representation. Furthermore, we use HamNoSys to extract the hand shape of a
sign and use this as additional supervision during training, further increasing
the performance on T2H. Assembling best practise, we achieve a BLEU-4 score of
26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art
baselines.",0,1,0,0,1,0,0.778486,13.0,0.907759,41
3df3fd2b-5bcb-4e99-b057-e30f427a4619,Balancing Discriminability and Transferability for Source-Free Domain Adaptation,65,0.197828,0.977451,"Conventional domain adaptation (DA) techniques aim to improve domain
transferability by learning domain-invariant representations; while
concurrently preserving the task-discriminability knowledge gathered from the
labeled source data. However, the requirement of simultaneous access to labeled
source and unlabeled target renders them unsuitable for the challenging
source-free DA setting. The trivial solution of realizing an effective original
to generic domain mapping improves transferability but degrades task
discriminability. Upon analyzing the hurdles from both theoretical and
empirical standpoints, we derive novel insights to show that a mixup between
original and corresponding translated generic samples enhances the
discriminability-transferability trade-off while duly respecting the
privacy-oriented source-free setting. A simple but effective realization of the
proposed insights on top of the existing source-free DA approaches yields
state-of-the-art performance with faster convergence. Beyond single-source, we
also outperform multi-source prior-arts across both classification and semantic
segmentation benchmarks.",1,0,0,0,1,0,0.461811,4.0,0.477948,97
233c9c96-6942-48a0-b51b-16c4955af797,Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments,15,0.0299357,0.257618,"Recent work in Vision-and-Language Navigation (VLN) has presented two
environmental paradigms with differing realism -- the standard VLN setting
built on topological environments where navigation is abstracted away, and the
VLN-CE setting where agents must navigate continuous 3D environments using
low-level actions. Despite sharing the high-level task and even the underlying
instruction-path data, performance on VLN-CE lags behind VLN significantly. In
this work, we explore this gap by transferring an agent from the abstract
environment of VLN to the continuous environment of VLN-CE. We find that this
sim-2-sim transfer is highly effective, improving over the prior state of the
art in VLN-CE by +12% success rate. While this demonstrates the potential for
this direction, the transfer does not fully retain the original performance of
the agent in the abstract setting. We present a sequence of experiments to
identify what differences result in performance degradation, providing clear
directions for further improvement.",0,1,0,0,1,0,0.233216,5.0,0.412923,28
33342f4a-1958-4233-8dbe-cac864cbc973,Efficient Visual Tracking via Hierarchical Cross-Attention Transformer,20,0.0620213,0.832323,"In recent years, target tracking has made great progress in accuracy. This
development is mainly attributed to powerful networks (such as transformers)
and additional modules (such as online update and refinement modules). However,
less attention has been paid to tracking speed. Most state-of-the-art trackers
are satisfied with the real-time speed on powerful GPUs. However, practical
applications necessitate higher requirements for tracking speed, especially
when edge platforms with limited resources are used. In this work, we present
an efficient tracking method via a hierarchical cross-attention transformer
named HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on
the edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our
HCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100,
UAV123, and VOT2020. Code and models are available at
https://github.com/chenxin-dlut/HCAT.",1,1,0,0,0,0,0.672479,7.0,0.785785,47
2b2f979b-9ef7-4668-83e4-c18ef78ebbfb,Learning Sketches for Decomposing Planning Problems into Subproblems of Bounded Width: Extended Version,15,0.289967,0.53093,"Recently, sketches have been introduced as a general language for
representing the subgoal structure of instances drawn from the same domain.
Sketches are collections of rules of the form C -> E over a given set of
features where C expresses Boolean conditions and E expresses qualitative
changes. Each sketch rule defines a subproblem: going from a state that
satisfies C to a state that achieves the change expressed by E or a goal state.
Sketches can encode simple goal serializations, general policies, or
decompositions of bounded width that can be solved greedily, in polynomial
time, by the SIW_R variant of the SIW algorithm. Previous work has shown the
computational value of sketches over benchmark domains that, while tractable,
are challenging for domain-independent planners. In this work, we address the
problem of learning sketches automatically given a planning domain, some
instances of the target class of problems, and the desired bound on the sketch
width. We present a logical formulation of the problem, an implementation using
the ASP solver Clingo, and experimental results. The sketch learner and the
SIW_R planner yield a domain-independent planner that learns and exploits
domain structure in a crisp and explicit form.",0,0,0,0,0,0,0.158334,10.0,0.663246,55
31e4d9f1-923f-4b3d-8699-95f9ffca54ce,RbA: Segmenting Unknown Regions Rejected by All,14,0.0913939,0.97515,"Standard semantic segmentation models owe their success to curated datasets
with a fixed set of semantic categories, without contemplating the possibility
of identifying unknown objects from novel categories. Existing methods in
outlier detection suffer from a lack of smoothness and objectness in their
predictions, due to limitations of the per-pixel classification paradigm.
Furthermore, additional training for detecting outliers harms the performance
of known classes. In this paper, we explore another paradigm with region-level
classification to better segment unknown objects. We show that the object
queries in mask classification tend to behave like one \vs all classifiers.
Based on this finding, we propose a novel outlier scoring function called RbA
by defining the event of being an outlier as being rejected by all known
classes. Our extensive experiments show that mask classification improves the
performance of the existing outlier detection methods, and the best results are
achieved with the proposed RbA. We also propose an objective to optimize RbA
using minimal outlier supervision. Further fine-tuning with outliers improves
the unknown performance, and unlike previous methods, it does not degrade the
inlier performance.",0,1,0,0,1,0,0.456388,9.0,0.766164,88
736846ad-a90c-42b6-a7a2-c4cab53e9bad,Sparse Conditional Hidden Markov Model for Weakly Supervised Named Entity Recognition,5,0.149987,0.650643,"Weakly supervised named entity recognition methods train label models to
aggregate the token annotations of multiple noisy labeling functions (LFs)
without seeing any manually annotated labels. To work well, the label model
needs to contextually identify and emphasize well-performed LFs while
down-weighting the under-performers. However, evaluating the LFs is challenging
due to the lack of ground truths. To address this issue, we propose the sparse
conditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire
emission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating
its diagonal elements, which are considered as the reliability scores of the
LFs. The sparse scores are then expanded to the full-fledged emission matrix
with pre-defined expansion functions. We also augment the emission with
weighted XOR scores, which track the probabilities of an LF observing incorrect
entities. Sparse-CHMM is optimized through unsupervised learning with a
three-stage training pipeline that reduces the training difficulty and prevents
the model from falling into local optima. Compared with the baselines in the
Wrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on
five comprehensive datasets. Experiments show that each component of
Sparse-CHMM is effective, and the estimated LF reliabilities strongly correlate
with true LF F1 scores.",1,1,0,0,1,0,0.75242,8.0,0.840523,32
6e1861f7-c6ab-485f-bf6e-f0a816303982,Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis,33,0.539627,0.972621,"Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment
analysis. To better comprehend long complicated sentences and obtain accurate
aspect-specific information, linguistic and commonsense knowledge are generally
required in this task. However, most current methods employ complicated and
inefficient approaches to incorporate external knowledge, e.g., directly
searching the graph nodes. Additionally, the complementarity between external
knowledge and linguistic information has not been thoroughly studied. To this
end, we propose a knowledge graph augmented network KGAN, which aims to
effectively incorporate external knowledge with explicitly syntactic and
contextual information. In particular, KGAN captures the sentiment feature
representations from multiple different perspectives, i.e., context-, syntax-
and knowledge-based. First, KGAN learns the contextual and syntactic
representations in parallel to fully extract the semantic features. Then, KGAN
integrates the knowledge graphs into the embedding space, based on which the
aspect-specific knowledge representations are further obtained via an attention
mechanism. Last, we propose a hierarchical fusion module to complement these
multi-view representations in a local-to-global manner. Extensive experiments
on five popular ABSA benchmarks demonstrate the effectiveness and robustness of
our KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN
achieves a new record of state-of-the-art performance among all datasets.",1,0,0,0,1,0,0.788106,7.0,0.832842,80
6876bbce-5274-4fad-94fc-18378b1ecf3c,Smooth Robust Tensor Completion for Background/Foreground Separation with Missing Pixels: Novel Algorithm with Convergence Guarantee,6,0.225297,0.619791,"The objective of this study is to address the problem of
background/foreground separation with missing pixels by combining the video
acquisition, video recovery, background/foreground separation into a single
framework. To achieve this, a smooth robust tensor completion (SRTC) model is
proposed to recover the data and decompose it into the static background and
smooth foreground, respectively. Specifically, the static background is modeled
by the low-rank tucker decomposition and the smooth foreground (moving objects)
is modeled by the spatiotemporal continuity, which is enforced by the total
variation regularization. An efficient algorithm based on tensor proximal
alternating minimization (tenPAM) is implemented to solve the proposed model
with global convergence guarantee under very mild conditions. Extensive
experiments on real data demonstrate that the proposed method significantly
outperforms the state-of-the-art approaches for background/foreground
separation with missing pixels.",0,1,0,0,1,0,0.326102,15.0,0.830725,53
48bb48f3-de82-4c8b-88c8-57d76ba8a8d2,VidConv: A modernized 2D ConvNet for Efficient Video Recognition,2,0.00703948,0.107343,"Since being introduced in 2020, Vision Transformers (ViT) has been steadily
breaking the record for many vision tasks and are often described as
``all-you-need"" to replace ConvNet. Despite that, ViTs are generally
computational, memory-consuming, and unfriendly for embedded devices. In
addition, recent research shows that standard ConvNet if redesigned and trained
appropriately can compete favorably with ViT in terms of accuracy and
scalability. In this paper, we adopt the modernized structure of ConvNet to
design a new backbone for action recognition. Particularly, our main target is
to serve for industrial product deployment, such as FPGA boards in which only
standard operations are supported. Therefore, our network simply consists of 2D
convolutions, without using any 3D convolution, long-range attention plugin, or
Transformer blocks. While being trained with much fewer epochs (5x-10x), our
backbone surpasses the methods using (2+1)D and 3D convolution, and achieve
comparable results with ViT on two benchmark datasets.",0,1,0,0,0,0,0.666837,6.0,0.747512,42
aa675a0e-1877-4469-90bb-f05387c45f41,Learning General Inventory Management Policy for Large Supply Chain Network,1,0.0295889,0.219238,"Inventory management in warehouses directly affects profits made by
manufacturers. Particularly, large manufacturers produce a very large variety
of products that are handled by a significantly large number of retailers. In
such a case, the computational complexity of classical inventory management
algorithms is inordinately large. In recent years, learning-based approaches
have become popular for addressing such problems. However, previous studies
have not been managed systems where both the number of products and retailers
are large. This study proposes a reinforcement learning-based warehouse
inventory management algorithm that can be used for supply chain systems where
both the number of products and retailers are large. To solve the computational
problem of handling large systems, we provide a means of approximate simulation
of the system in the training phase. Our experiments on both real and
artificial data demonstrate that our algorithm with approximated simulation can
successfully handle large supply chain networks.",0,1,0,0,0,0,0.119014,42.0,0.912494,21
0dbc5027-3050-4aa0-a3c0-87fefbba77f3,VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension,12,0.173662,0.909546,"One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on
Vietnamese Language and Speech Processing (VLSP 2021). This task attracted 77
participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the challenge, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% in F1-score and 67.43% in Exact Match on the
private test set. The Vietnamese MRC systems proposed by the top 3 teams use
XLM-RoBERTa, a powerful pre-trained language model based on the transformer
architecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further
explore the Vietnamese machine reading comprehension task and related tasks
such as question answering, question generation, and natural language
inference.",0,1,1,1,0,0,0.600879,5.0,0.661108,42
100c4224-ea33-4fe9-956a-c8c6de65f05a,ClusterGNN: Cluster-based Coarse-to-Fine Graph Neural Network for Efficient Feature Matching,48,0.155469,0.989159,"Graph Neural Networks (GNNs) with attention have been successfully applied
for learning visual feature matching. However, current methods learn with
complete graphs, resulting in a quadratic complexity in the number of features.
Motivated by a prior observation that self- and cross- attention matrices
converge to a sparse representation, we propose ClusterGNN, an attentional GNN
architecture which operates on clusters for learning the feature matching task.
Using a progressive clustering module we adaptively divide keypoints into
different subgraphs to reduce redundant connectivity, and employ a
coarse-to-fine paradigm for mitigating miss-classification within images. Our
approach yields a 59.7% reduction in runtime and 58.4% reduction in memory
consumption for dense detection, compared to current state-of-the-art GNN-based
matching, while achieving a competitive performance on various computer vision
tasks.",0,1,0,0,1,0,0.560632,7.0,0.742151,47
7b095e53-15fc-4ff1-84e8-2c707789ecea,NU HLT at CMCL 2022 Shared Task: Multilingual and Crosslingual Prediction of Human Reading Behavior in Universal Language Space,1,0.0134325,0.0164904,"In this paper, we present a unified model that works for both multilingual
and crosslingual prediction of reading times of words in various languages. The
secret behind the success of this model is in the preprocessing step where all
words are transformed to their universal language representation via the
International Phonetic Alphabet (IPA). To the best of our knowledge, this is
the first study to favorable exploit this phonological property of language for
the two tasks. Various feature types were extracted covering basic frequencies,
n-grams, information theoretic, and psycholinguistically-motivated predictors
for model training. A finetuned Random Forest model obtained best performance
for both tasks with 3.8031 and 3.9065 MAE scores for mean first fixation
duration (FFDAvg) and mean total reading time (TRTAvg) respectively.",0,1,0,0,0,0,0.371276,8.0,0.702853,38
020c6e12-1493-4ea9-98e2-f2f0fb308ff1,Learning Texture Transformer Network for Light Field Super-Resolution,1,0.0161851,0.17142,"Hand-held light field cameras suffer from low spatial resolution due to the
inherent spatio-angular tradeoff. In this paper, we propose a method to improve
the spatial resolution of light field images with the aid of the Texture
Transformer Network (TTSR). The proposed method consists of three modules: the
first module produces an all-in focus high-resolution perspective image which
serves as a reference image for the second module, i.e. TTSR, which in turn
produces a high-resolution light field. The last module refines the spatial
resolution by imposing a light field prior. The results demonstrate around 4 dB
to 6 dB PSNR gain over a bicubically resized light field image",0,1,0,0,0,0,0.333064,9.0,0.72076,7
be1542ad-76da-4425-85a9-167497988157,BIASeD: Bringing Irrationality into Automated System Design,5,0.0582689,0.278079,"Human perception, memory and decision-making are impacted by tens of
cognitive biases and heuristics that influence our actions and decisions.
Despite the pervasiveness of such biases, they are generally not leveraged by
today's Artificial Intelligence (AI) systems that model human behavior and
interact with humans. In this theoretical paper, we claim that the future of
human-machine collaboration will entail the development of AI systems that
model, understand and possibly replicate human cognitive biases. We propose the
need for a research agenda on the interplay between human cognitive biases and
Artificial Intelligence. We categorize existing cognitive biases from the
perspective of AI systems, identify three broad areas of interest and outline
research directions for the design of AI systems that have a better
understanding of our own biases.",0,0,0,0,0,0,0.00898792,19.0,0.667537,134
ac49d3c4-4c18-4c0c-abbf-65a2f8b3be2d,Human Pose Driven Object Effects Recommendation,2,0.0085613,0.265977,"In this paper, we research the new topic of object effects recommendation in
micro-video platforms, which is a challenging but important task for many
practical applications such as advertisement insertion. To avoid the problem of
introducing background bias caused by directly learning video content from
image frames, we propose to utilize the meaningful body language hidden in 3D
human pose for recommendation. To this end, in this work, a novel human pose
driven object effects recommendation network termed PoseRec is introduced.
PoseRec leverages the advantages of 3D human pose detection and learns
information from multi-frame 3D human pose for video-item registration,
resulting in high quality object effects recommendation performance. Moreover,
to solve the inherent ambiguity and sparsity issues that exist in object
effects recommendation, we further propose a novel item-aware implicit
prototype learning module and a novel pose-aware transductive hard-negative
mining module to better learn pose-item relationships. What's more, to
benchmark methods for the new research topic, we build a new dataset for object
effects recommendation named Pose-OBE. Extensive experiments on Pose-OBE
demonstrate that our method can achieve superior performance than strong
baselines.",0,1,1,1,1,0,0.281363,5.0,0.456621,61
e59dcaaf-6ecc-4ad6-baed-f03e2ed103df,Dual-Scale Single Image Dehazing Via Neural Augmentation,30,0.394723,0.927653,"Model-based single image dehazing algorithms restore haze-free images with
sharp edges and rich details for real-world hazy images at the expense of low
PSNR and SSIM values for synthetic hazy images. Data-driven ones restore
haze-free images with high PSNR and SSIM values for synthetic hazy images but
with low contrast, and even some remaining haze for real world hazy images. In
this paper, a novel single image dehazing algorithm is introduced by combining
model-based and data-driven approaches. Both transmission map and atmospheric
light are first estimated by the model-based methods, and then refined by
dual-scale generative adversarial networks (GANs) based approaches. The
resultant algorithm forms a neural augmentation which converges very fast while
the corresponding data-driven approach might not converge. Haze-free images are
restored by using the estimated transmission map and atmospheric light as well
as the Koschmiederlaw. Experimental results indicate that the proposed
algorithm can remove haze well from real-world and synthetic hazy images.",0,1,0,0,0,1,0.693277,11.0,0.868874,62
ed477016-e408-4877-b190-c5ab4dffda82,Occluded Person Re-Identification via Relational Adaptive Feature Correction Learning,12,0.0798689,0.828262,"Occluded person re-identification (Re-ID) in images captured by multiple
cameras is challenging because the target person is occluded by pedestrians or
objects, especially in crowded scenes. In addition to the processes performed
during holistic person Re-ID, occluded person Re-ID involves the removal of
obstacles and the detection of partially visible body parts. Most existing
methods utilize the off-the-shelf pose or parsing networks as pseudo labels,
which are prone to error. To address these issues, we propose a novel Occlusion
Correction Network (OCNet) that corrects features through relational-weight
learning and obtains diverse and representative features without using external
networks. In addition, we present a simple concept of a center feature in order
to provide an intuitive solution to pedestrian occlusion scenarios.
Furthermore, we suggest the idea of Separation Loss (SL) for focusing on
different parts between global features and part features. We conduct extensive
experiments on five challenging benchmark datasets for occluded and holistic
Re-ID tasks to demonstrate that our method achieves superior performance to
state-of-the-art methods especially on occluded scene.",0,1,0,0,1,0,0.693319,7.0,0.793961,18
dfa87f09-30da-4ea1-a834-c9de0b8e8bb5,Acquiring and Modelling Abstract Commonsense Knowledge via Conceptualization,20,0.211442,0.766864,"Conceptualization, or viewing entities and situations as instances of
abstract concepts in mind and making inferences based on that, is a vital
component in human intelligence for commonsense reasoning. Although recent
artificial intelligence has made progress in acquiring and modelling
commonsense, attributed to large neural language models and commonsense
knowledge graphs (CKGs), conceptualization is yet to thoroughly be introduced,
making current approaches ineffective to cover knowledge about countless
diverse entities and situations in the real world. To address the problem, we
thoroughly study the possible role of conceptualization in commonsense
reasoning, and formulate a framework to replicate human conceptual induction
from acquiring abstract knowledge about abstract concepts. Aided by the
taxonomy Probase, we develop tools for contextualized conceptualization on
ATOMIC, a large-scale human annotated CKG. We annotate a dataset for the
validity of conceptualizations for ATOMIC on both event and triple level,
develop a series of heuristic rules based on linguistic features, and train a
set of neural models, so as to generate and verify abstract knowledge. Based on
these components, a pipeline to acquire abstract knowledge is built. A large
abstract CKG upon ATOMIC is then induced, ready to be instantiated to infer
about unseen entities or situations. Furthermore, experiments find directly
augmenting data with abstract triples to be helpful in commonsense modelling.",0,0,0,0,0,0,0.323941,6.0,0.575454,116
848dd161-bc06-4d26-a707-7f4a8a7d9837,Solvability of orbit-finite systems of linear equations,2,0.0496009,0.144644,"We study orbit-finite systems of linear equations, in the setting of sets
with atoms. Our principal contribution is a decision procedure for solvability
of such systems. The procedure works for every field (and even commutative
ring) under mild effectiveness assumptions, and reduces a given orbit-finite
system to a number of finite ones: exponentially many in general, but
polynomially many when atom dimension of input systems is fixed. Towards
obtaining the procedure we push further the theory of vector spaces generated
by orbit-finite sets, and show that each such vector space admits an
orbit-finite basis. This fundamental property is a key tool in our development,
but should be also of wider interest.",0,0,0,0,0,0,0.0138386,14.0,0.579802,38
b4de6889-2a51-4f73-a531-4b61ef185cfc,JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional Networks,14,0.099205,0.831292,"Graph Convolutional Network (GCN) has exhibited strong empirical performance
in many real-world applications. The vast majority of existing works on GCN
primarily focus on the accuracy while ignoring how confident or uncertain a GCN
is with respect to its predictions. Despite being a cornerstone of trustworthy
graph mining, uncertainty quantification on GCN has not been well studied and
the scarce existing efforts either fail to provide deterministic quantification
or have to change the training procedure of GCN by introducing additional
parameters or architectures. In this paper, we propose the first
frequentist-based approach named JuryGCN in quantifying the uncertainty of GCN,
where the key idea is to quantify the uncertainty of a node as the width of
confidence interval by a jackknife estimator. Moreover, we leverage the
influence functions to estimate the change in GCN parameters without
re-training to scale up the computation. The proposed JuryGCN is capable of
quantifying uncertainty deterministically without modifying the GCN
architecture or introducing additional parameters. We perform extensive
experimental evaluation on real-world datasets in the tasks of both active
learning and semi-supervised node classification, which demonstrate the
efficacy of the proposed method.",0,0,0,0,0,0,0.279014,9.0,0.69702,54
fca01e29-4788-46ff-b5ba-256b06611a27,Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion,2,0.0794036,0.334307,"Multilingual speech recognition has drawn significant attention as an
effective way to compensate data scarcity for low-resource languages.
End-to-end (e2e) modelling is preferred over conventional hybrid systems,
mainly because of no lexicon requirement. However, hybrid DNN-HMMs still
outperform e2e models in limited data scenarios. Furthermore, the problem of
manual lexicon creation has been alleviated by publicly available trained
models of grapheme-to-phoneme (G2P) and text to IPA transliteration for a lot
of languages. In this paper, a novel approach of hybrid DNN-HMM acoustic models
fusion is proposed in a multilingual setup for the low-resource languages.
Posterior distributions from different monolingual acoustic models, against a
target language speech signal, are fused together. A separate regression neural
network is trained for each source-target language pair to transform posteriors
from source acoustic model to the target language. These networks require very
limited data as compared to the ASR training. Posterior fusion yields a
relative gain of 14.65% and 6.5% when compared with multilingual and
monolingual baselines respectively. Cross-lingual model fusion shows that the
comparable results can be achieved without using posteriors from the language
dependent ASR.",0,1,0,0,0,0,0.711617,7.0,0.801209,36
1bf4f8d0-95b1-4e0b-a4d5-ac46217148fe,CrowdFormer: Weakly-supervised Crowd counting with Improved Generalizability,16,0.606836,0.807851,"Convolutional neural networks (CNNs) have dominated the field of computer
vision for nearly a decade due to their strong ability to learn local features.
However, due to their limited receptive field, CNNs fail to model the global
context. On the other hand, transformer, an attention-based architecture can
model the global context easily. Despite this, there are limited studies that
investigate the effectiveness of transformers in crowd counting. In addition,
the majority of the existing crowd counting methods are based on the regression
of density maps which requires point-level annotation of each person present in
the scene. This annotation task is laborious and also error-prone. This has led
to increased focus on weakly-supervised crowd counting methods which require
only the count-level annotations. In this paper, we propose a weakly-supervised
method for crowd counting using a pyramid vision transformer. We have conducted
extensive evaluations to validate the effectiveness of the proposed method. Our
method is comparable to the state-of-the-art on the benchmark crowd datasets.
More importantly, it shows remarkable generalizability.",0,1,0,0,1,0,0.977103,6.0,0.953237,63
739876d2-43da-4367-8f5c-7aa88777a90d,Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction,26,0.198394,0.74584,"The DocRED dataset is one of the most popular and widely used benchmarks for
document-level relation extraction (RE). It adopts a recommend-revise
annotation scheme so as to have a large-scale annotated dataset. However, we
find that the annotation of DocRED is incomplete, i.e., false negative samples
are prevalent. We analyze the causes and effects of the overwhelming false
negative problem in the DocRED dataset. To address the shortcoming, we
re-annotate 4,053 documents in the DocRED dataset by adding the missed relation
triples back to the original DocRED. We name our revised DocRED dataset
Re-DocRED. We conduct extensive experiments with state-of-the-art neural models
on both datasets, and the experimental results show that the models trained and
evaluated on our Re-DocRED achieve performance improvements of around 13 F1
points. Moreover, we conduct a comprehensive analysis to identify the potential
areas for further improvement. Our dataset is publicly available at
https://github.com/tonytan48/Re-DocRED.",1,1,0,1,1,0,0.569647,5.0,0.64399,53
28dc92f4-bd49-4e3c-80c7-b50786fdd600,Higher-order Clustering and Pooling for Graph Neural Networks,21,0.324737,0.560582,"Graph Neural Networks achieve state-of-the-art performance on a plethora of
graph classification tasks, especially due to pooling operators, which
aggregate learned node embeddings hierarchically into a final graph
representation. However, they are not only questioned by recent work showing on
par performance with random pooling, but also ignore completely higher-order
connectivity patterns. To tackle this issue, we propose HoscPool, a
clustering-based graph pooling operator that captures higher-order information
hierarchically, leading to richer graph representations. In fact, we learn a
probabilistic cluster assignment matrix end-to-end by minimising relaxed
formulations of motif spectral clustering in our objective function, and we
then extend it to a pooling operator. We evaluate HoscPool on graph
classification tasks and its clustering component on graphs with ground-truth
community structure, achieving best performance. Lastly, we provide a deep
empirical analysis of pooling operators' inner functioning.",0,0,0,0,0,0,0.622459,7.0,0.766328,54
ab1c64e4-650e-4639-b51d-614e1a4203aa,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,12,0.132514,0.251293,"Explicit decomposition modeling, which involves breaking down complex tasks
into more straightforward and often more interpretable sub-tasks, has long been
a central theme in developing robust and interpretable NLU systems. However,
despite the many datasets and resources built as part of this effort, the
majority have small-scale annotations and limited scope, which is insufficient
to solve general decomposition tasks. In this paper, we look at large-scale
intermediate pre-training of decomposition-based transformers using distant
supervision from comparable texts, particularly large-scale parallel news. We
show that with such intermediate pre-training, developing robust
decomposition-based models for a diverse range of tasks becomes more feasible.
For example, on semantic parsing, our model, DecompT5, improves 20% to 30% on
two datasets, Overnight and TORQUE, over the baseline language model. We
further use DecompT5 to build a novel decomposition-based QA system named
DecompEntail, improving over state-of-the-art models, including GPT-3, on both
HotpotQA and StrategyQA by 8% and 4%, respectively.",0,1,1,0,1,0,0.784693,7.0,0.831363,39
427d9452-a8f2-44db-a7e0-e6265914ed5e,Deep-Attack over the Deep Reinforcement Learning,8,0.118883,0.680989,"Recent adversarial attack developments have made reinforcement learning more
vulnerable, and different approaches exist to deploy attacks against it, where
the key is how to choose the right timing of the attack. Some work tries to
design an attack evaluation function to select critical points that will be
attacked if the value is greater than a certain threshold. This approach makes
it difficult to find the right place to deploy an attack without considering
the long-term impact. In addition, there is a lack of appropriate indicators of
assessment during attacks. To make the attacks more intelligent as well as to
remedy the existing problems, we propose the reinforcement learning-based
attacking framework by considering the effectiveness and stealthy
spontaneously, while we also propose a new metric to evaluate the performance
of the attack model in these two aspects. Experimental results show the
effectiveness of our proposed model and the goodness of our proposed evaluation
metric. Furthermore, we validate the transferability of the model, and also its
robustness under the adversarial training.",0,0,0,0,0,0,0.25151,8.0,0.643956,33
dd317a2a-b85a-4d90-8f0e-71a9e1121bdf,Explainable Action Advising for Multi-Agent Reinforcement Learning,8,0.411462,0.316183,"Action advising is a knowledge transfer technique for reinforcement learning
based on the teacher-student paradigm. An expert teacher provides advice to a
student during training in order to improve the student's sample efficiency and
policy performance. Such advice is commonly given in the form of state-action
pairs. However, it makes it difficult for the student to reason with and apply
to novel states. We introduce Explainable Action Advising, in which the teacher
provides action advice as well as associated explanations indicating why the
action was chosen. This allows the student to self-reflect on what it has
learned, enabling advice generalization and leading to improved sample
efficiency and learning performance - even in environments where the teacher is
sub-optimal. We empirically show that our framework is effective in both
single-agent and multi-agent scenarios, yielding improved policy returns and
convergence rates when compared to state-of-the-art methods",1,1,0,0,1,0,0.596737,10.0,0.829423,35
7f205e33-3a02-4c8e-969a-5bd2e77b9628,Active Learning by Feature Mixing,50,0.121013,0.913428,"The promise of active learning (AL) is to reduce labelling costs by selecting
the most valuable examples to annotate from a pool of unlabelled data.
Identifying these examples is especially challenging with high-dimensional data
(e.g. images, videos) and in low-data regimes. In this paper, we propose a
novel method for batch AL called ALFA-Mix. We identify unlabelled instances
with sufficiently-distinct features by seeking inconsistencies in predictions
resulting from interventions on their representations. We construct
interpolations between representations of labelled and unlabelled instances
then examine the predicted labels. We show that inconsistencies in these
predictions help discovering features that the model is unable to recognise in
the unlabelled instances. We derive an efficient implementation based on a
closed-form solution to the optimal interpolation causing changes in
predictions. Our method outperforms all recent AL approaches in 30 different
settings on 12 benchmarks of images, videos, and non-visual data. The
improvements are especially significant in low-data regimes and on self-trained
vision transformers, where ALFA-Mix outperforms the state-of-the-art in 59% and
43% of the experiments respectively.",1,1,0,0,1,1,0.35863,9.0,0.730996,45
54a1bf6a-5264-42ad-8f05-6949bfbe9035,Generalizability of Adversarial Robustness Under Distribution Shifts,4,0.0285742,0.296147,"Recent progress in empirical and certified robustness promises to deliver
reliable and deployable Deep Neural Networks (DNNs). Despite that success, most
existing evaluations of DNN robustness have been done on images sampled from
the same distribution on which the model was trained. However, in the real
world, DNNs may be deployed in dynamic environments that exhibit significant
distribution shifts. In this work, we take a first step towards thoroughly
investigating the interplay between empirical and certified adversarial
robustness on one hand and domain generalization on another. To do so, we train
robust models on multiple domains and evaluate their accuracy and robustness on
an unseen domain. We observe that: (1) both empirical and certified robustness
generalize to unseen domains, and (2) the level of generalizability does not
correlate well with input visual similarity, measured by the FID between source
and target domains. We also extend our study to cover a real-world medical
application, in which adversarial augmentation significantly boosts the
generalization of robustness with minimal effect on clean data accuracy.",0,0,0,0,0,0,0.434491,8.0,0.728562,80
392829af-f9dc-45fc-be57-432d662fc7e7,V$^2$L: Leveraging Vision and Vision-language Models into Large-scale Product Retrieval,2,0.0165647,0.119061,"Product retrieval is of great importance in the ecommerce domain. This paper
introduces our 1st-place solution in eBay eProduct Visual Search Challenge
(FGVC9), which is featured for an ensemble of about 20 models from vision
models and vision-language models. While model ensemble is common, we show that
combining the vision models and vision-language models brings particular
benefits from their complementarity and is a key factor to our superiority.
Specifically, for the vision models, we use a two-stage training pipeline which
first learns from the coarse labels provided in the training set and then
conducts fine-grained self-supervised training, yielding a coarse-to-fine
metric learning manner. For the vision-language models, we use the textual
description of the training image as the supervision signals for fine-tuning
the image-encoder (feature extractor). With these designs, our solution
achieves 0.7623 MAR@10, ranking the first place among all the competitors. The
code is available at: \href{https://github.com/WangWenhao0716/V2L}{V$^2$L}.",0,1,0,0,0,0,0.917684,3.0,0.768577,26
f693628c-e780-4ba4-8aa7-4d821dcf63f4,Training Mixed-Domain Translation Models via Federated Learning,13,0.0659563,0.359718,"Training mixed-domain translation models is a complex task that demands
tailored architectures and costly data preparation techniques. In this work, we
leverage federated learning (FL) in order to tackle the problem. Our
investigation demonstrates that with slight modifications in the training
process, neural machine translation (NMT) engines can be easily adapted when an
FL-based aggregation is applied to fuse different domains. Experimental results
also show that engines built via FL are able to perform on par with
state-of-the-art baselines that rely on centralized training techniques. We
evaluate our hypothesis in the presence of five datasets with different sizes,
from different domains, to translate from German into English and discuss how
FL and NMT can mutually benefit from each other. In addition to providing
benchmarking results on the union of FL and NMT, we also propose a novel
technique to dynamically control the communication bandwidth by selecting
impactful parameters during FL updates. This is a significant achievement
considering the large size of NMT engines that need to be exchanged between FL
parties.",0,1,0,0,1,0,0.403735,7.0,0.675855,42
387644ff-93ef-47c0-87ca-c34ea832e1e5,User-Centric Gender Rewriting,8,0.149077,0.563221,"In this paper, we define the task of gender rewriting in contexts involving
two users (I and/or You) - first and second grammatical persons with
independent grammatical gender preferences. We focus on Arabic, a
gender-marking morphologically rich language. We develop a multi-step system
that combines the positive aspects of both rule-based and neural rewriting
models. Our results successfully demonstrate the viability of this approach on
a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5
on a blind test set. Our proposed system improves over previous work on the
first-person-only version of this task, by 3.05 absolute increase in M2 F0.5.
We demonstrate a use case of our gender rewriting system by using it to
post-edit the output of a commercial MT system to provide personalized outputs
based on the users' grammatical gender preferences. We make our code, data, and
models publicly available.",1,1,1,1,1,0,0.886679,6.0,0.861459,65
f6a74232-4984-48fb-807e-6dd045fea6d8,ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs,11,0.0940353,0.672696,"As Abstract Meaning Representation (AMR) implicitly involves compound
semantic annotations, we hypothesize auxiliary tasks which are semantically or
formally related can better enhance AMR parsing. We find that 1) Semantic role
labeling (SRL) and dependency parsing (DP), would bring more performance gain
than other tasks e.g. MT and summarization in the text-to-AMR transition even
with much less data. 2) To make a better fit for AMR, data from auxiliary tasks
should be properly ""AMRized"" to PseudoAMR before training. Knowledge from
shallow level parsing tasks can be better transferred to AMR Parsing with
structure transform. 3) Intermediate-task learning is a better paradigm to
introduce auxiliary tasks to AMR parsing, compared to multitask learning. From
an empirical perspective, we propose a principled method to involve auxiliary
tasks to boost AMR parsing. Extensive experiments show that our method achieves
new state-of-the-art performance on different benchmarks especially in
topology-related scores.",1,1,0,0,1,0,0.219056,6.0,0.498869,49
c85b71a9-fc4e-4f7c-aa98-4e4be263626a,Memory-Based Model Editing at Scale,152,0.580005,0.999571,"Even the largest neural networks make errors, and once-correct predictions
can become invalid as the world changes. Model editors make local updates to
the behavior of base (pre-trained) models to inject updated knowledge or
correct undesirable behaviors. Existing model editors have shown promise, but
also suffer from insufficient expressiveness: they struggle to accurately model
an edit's intended scope (examples affected by the edit), leading to inaccurate
predictions for test inputs loosely related to the edit, and they often fail
altogether after many edits. As a higher-capacity alternative, we propose
Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model
(SERAC), which stores edits in an explicit memory and learns to reason over
them to modulate the base model's predictions as needed. To enable more
rigorous evaluation of model editors, we introduce three challenging language
model editing problems based on question answering, fact-checking, and dialogue
generation. We find that only SERAC achieves high performance on all three
problems, consistently outperforming existing approaches to model editing by a
significant margin. Code, data, and additional project information will be made
available at https://sites.google.com/view/serac-editing.",0,0,0,0,1,0,0.702904,6.0,0.764039,51
30eb50dd-c375-4dcf-a7eb-42aded86fc46,Individual Topology Structure of Eye Movement Trajectories,2,0.0373418,0.249477,"Traditionally, extracting patterns from eye movement data relies on
statistics of different macro-events such as fixations and saccades. This
requires an additional preprocessing step to separate the eye movement
subtypes, often with a number of parameters on which the classification results
depend. Besides that, definitions of such macro events are formulated in
different ways by different researchers.
  We propose an application of a new class of features to the quantitative
analysis of personal eye movement trajectories structure. This new class of
features based on algebraic topology allows extracting patterns from different
modalities of gaze such as time series of coordinates and amplitudes, heatmaps,
and point clouds in a unified way at all scales from micro to macro. We
experimentally demonstrate the competitiveness of the new class of features
with the traditional ones and their significant synergy while being used
together for the person authentication task on the recently published eye
movement trajectories dataset.",0,0,0,0,0,0,0.393093,14.0,0.835441,18
59a9af27-7e8f-4664-872c-cd35f8b1ba59,Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents,16,0.278685,0.475873,"Text semantic matching is a fundamental task that has been widely used in
various scenarios, such as community question answering, information retrieval,
and recommendation. Most state-of-the-art matching models, e.g., BERT, directly
perform text comparison by processing each word uniformly. However, a query
sentence generally comprises content that calls for different levels of
matching granularity. Specifically, keywords represent factual information such
as action, entity, and event that should be strictly matched, while intents
convey abstract concepts and ideas that can be paraphrased into various
expressions. In this work, we propose a simple yet effective training strategy
for text semantic matching in a divide-and-conquer manner by disentangling
keywords from intents. Our approach can be easily combined with pre-trained
language models (PLM) without influencing their inference efficiency, achieving
stable performance improvements against a wide range of PLMs on three
benchmarks.",0,1,0,0,0,1,0.730412,8.0,0.832656,52
80a230ee-f4a5-46bb-8ee7-7f311f2d8cb4,Layer Adaptive Deep Neural Networks for Out-of-distribution Detection,2,0.0281089,0.15381,"During the forward pass of Deep Neural Networks (DNNs), inputs gradually
transformed from low-level features to high-level conceptual labels. While
features at different layers could summarize the important factors of the
inputs at varying levels, modern out-of-distribution (OOD) detection methods
mostly focus on utilizing their ending layer features. In this paper, we
proposed a novel layer-adaptive OOD detection framework (LA-OOD) for DNNs that
can fully utilize the intermediate layers' outputs. Specifically, instead of
training a unified OOD detector at a fixed ending layer, we train multiple
One-Class SVM OOD detectors simultaneously at the intermediate layers to
exploit the full spectrum characteristics encoded at varying depths of DNNs. We
develop a simple yet effective layer-adaptive policy to identify the best layer
for detecting each potential OOD example. LA-OOD can be applied to any existing
DNNs and does not require access to OOD samples during the training. Using
three DNNs of varying depth and architectures, our experiments demonstrate that
LA-OOD is robust against OODs of varying complexity and can outperform
state-of-the-art competitors by a large margin on some real-world datasets.",1,1,0,0,1,0,0.491868,10.0,0.80005,20
71a1acce-cabf-4ea9-8177-5bd19eb2eeb5,SimpleTrack: Rethinking and Improving the JDE Approach for Multi-Object Tracking,26,0.227473,0.605118,"Joint detection and embedding (JDE) based methods usually estimate bounding
boxes and embedding features of objects with a single network in Multi-Object
Tracking (MOT). In the tracking stage, JDE-based methods fuse the target motion
information and appearance information by applying the same rule, which could
fail when the target is briefly lost or blocked. To overcome this problem, we
propose a new association matrix, the Embedding and Giou matrix, which combines
embedding cosine distance and Giou distance of objects. To further improve the
performance of data association, we develop a simple, effective tracker named
SimpleTrack, which designs a bottom-up fusion method for Re-identity and
proposes a new tracking strategy based on our EG matrix. The experimental
results indicate that SimpleTrack has powerful data association capability,
e.g., 61.6 HOTA and 76.3 IDF1 on MOT17. In addition, we apply the EG matrix to
5 different state-of-the-art JDE-based methods and achieve significant
improvements in IDF1, HOTA and IDsw metrics, and increase the tracking speed of
these methods by about 20%.",0,1,0,0,1,0,0.83675,4.0,0.746313,52
fbf96439-4465-4592-828f-3143fdcfcb05,Unpacking Large Language Models with Conceptual Consistency,11,0.254339,0.418932,"If a Large Language Model (LLM) answers ""yes"" to the question ""Are mountains
tall?"" then does it know what a mountain is? Can you rely on it responding
correctly or incorrectly to other questions about mountains? The success of
Large Language Models (LLMs) indicates they are increasingly able to answer
queries like these accurately, but that ability does not necessarily imply a
general understanding of concepts relevant to the anchor query. We propose
conceptual consistency to measure a LLM's understanding of relevant concepts.
This novel metric measures how well a model can be characterized by finding out
how consistent its responses to queries about conceptually relevant background
knowledge are. To compute it we extract background knowledge by traversing
paths between concepts in a knowledge base and then try to predict the model's
response to the anchor query from the background knowledge. We investigate the
performance of current LLMs in a commonsense reasoning setting using the CSQA
dataset and the ConceptNet knowledge base. While conceptual consistency, like
other metrics, does increase with the scale of the LLM used, we find that
popular models do not necessarily have high conceptual consistency. Our
analysis also shows significant variation in conceptual consistency across
different kinds of relations, concepts, and prompts. This serves as a step
toward building models that humans can apply a theory of mind to, and thus
interact with intuitively.",0,0,0,0,0,0,0.937485,5.0,0.882052,39
8cf3bcbd-bee4-4cd2-aaf7-a719c22a1215,Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation,200,0.957684,0.945983,"In this paper we present Mask DINO, a unified object detection and
segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising
Anchor Boxes) by adding a mask prediction branch which supports all image
segmentation tasks (instance, panoptic, and semantic). It makes use of the
query embeddings from DINO to dot-product a high-resolution pixel embedding map
to predict a set of binary masks. Some key components in DINO are extended for
segmentation through a shared architecture and training process. Mask DINO is
simple, efficient, and scalable, and it can benefit from joint large-scale
detection and segmentation datasets. Our experiments show that Mask DINO
significantly outperforms all existing specialized segmentation methods, both
on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably,
Mask DINO establishes the best results to date on instance segmentation (54.5
AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation
(60.8 mIoU on ADE20K) among models under one billion parameters. Code is
available at \url{https://github.com/IDEACVR/MaskDINO}.",1,1,0,0,1,0,0.946205,6.0,0.910504,45
c166812c-6c3e-441c-bd84-c6a743f9b581,ASSIST: Towards Label Noise-Robust Dialogue State Tracking,15,0.365118,0.447257,"The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state
tracking (DST). However, substantial noise has been discovered in its state
annotations. Such noise brings about huge challenges for training DST models
robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have
been published recently, there are still lots of noisy labels, especially in
the training set. Besides, it is costly to rectify all the problematic
annotations. In this paper, instead of improving the annotation quality
further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt
dIalogue State Tracking), to train DST models robustly from noisy labels.
ASSIST first generates pseudo labels for each sample in the training set by
using an auxiliary model trained on a small clean dataset, then puts the
generated pseudo labels and vanilla noisy labels together to train the primary
model. We show the validity of ASSIST theoretically. Experimental results also
demonstrate that ASSIST improves the joint goal accuracy of DST by up to
$28.16\%$ on MultiWOZ 2.0 and $8.41\%$ on MultiWOZ 2.4, compared to using only
the vanilla noisy labels.",1,1,0,0,0,0,0.946553,5.0,0.893049,47
86fd0831-593a-430e-85b0-edc70b3270bc,Dynamic Dialogue Policy for Continual Reinforcement Learning,9,0.20689,0.7011,"Continual learning is one of the key components of human learning and a
necessary requirement of artificial intelligence. As dialogue can potentially
span infinitely many topics and tasks, a task-oriented dialogue system must
have the capability to continually learn, dynamically adapting to new
challenges while preserving the knowledge it already acquired. Despite the
importance, continual reinforcement learning of the dialogue policy has
remained largely unaddressed. The lack of a framework with training protocols,
baseline models and suitable metrics, has so far hindered research in this
direction. In this work we fill precisely this gap, enabling research in
dialogue policy optimisation to go from static to dynamic learning. We provide
a continual learning algorithm, baseline architectures and metrics for
assessing continual learning models. Moreover, we propose the dynamic dialogue
policy transformer (DDPT), a novel dynamic architecture that can integrate new
knowledge seamlessly, is capable of handling large state spaces and obtains
significant zero-shot performance when being exposed to unseen domains, without
any growth in network parameter size.",0,0,0,0,0,0,0.71119,6.0,0.767879,54
181a0140-cfc7-4a97-84f8-30ef88f2c369,Arbitrary Point Cloud Upsampling with Spherical Mixture of Gaussians,4,0.0635778,0.180146,"Generating dense point clouds from sparse raw data benefits downstream 3D
understanding tasks, but existing models are limited to a fixed upsampling
ratio or to a short range of integer values. In this paper, we present
APU-SMOG, a Transformer-based model for Arbitrary Point cloud Upsampling (APU).
The sparse input is firstly mapped to a Spherical Mixture of Gaussians (SMOG)
distribution, from which an arbitrary number of points can be sampled. Then,
these samples are fed as queries to the Transformer decoder, which maps them
back to the target surface. Extensive qualitative and quantitative evaluations
show that APU-SMOG outperforms state-of-the-art fixed-ratio methods, while
effectively enabling upsampling with any scaling factor, including non-integer
values, with a single trained model. The code is available at
https://github.com/apusmog/apusmog/",0,1,0,0,1,0,0.641077,6.0,0.735818,41
7d88d0c9-693a-4136-98da-1963a547e18c,Safe Reinforcement Learning via Shielding under Partial Observability,22,0.509655,0.675844,"Safe exploration is a common problem in reinforcement learning (RL) that aims
to prevent agents from making disastrous decisions while exploring their
environment. A family of approaches to this problem assume domain knowledge in
the form of a (partial) model of this environment to decide upon the safety of
an action. A so-called shield forces the RL agent to select only safe actions.
However, for adoption in various applications, one must look beyond enforcing
safety and also ensure the applicability of RL with good performance. We extend
the applicability of shields via tight integration with state-of-the-art deep
RL, and provide an extensive, empirical study in challenging, sparse-reward
environments under partial observability. We show that a carefully integrated
shield ensures safety and can improve the convergence rate and final
performance of RL agents. We furthermore show that a shield can be used to
bootstrap state-of-the-art RL agents: they remain safe after initial learning
in a shielded setting, allowing us to disable a potentially too conservative
shield eventually.",0,0,0,0,0,0,0.403123,14.0,0.837786,44
62c3bb82-f716-4e95-8620-9bcf79ddb4ed,Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?,1,0.0359182,0.157672,"Task-oriented dialogue (TOD) systems are mainly based on the
slot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down
into smaller, controllable units (i.e., slots) to fulfill a specific task. A
series of approaches based on this framework achieved remarkable success on
various TOD benchmarks. However, we argue that the current TOD benchmarks are
limited to surrogate real-world scenarios and that the current TOD models are
still a long way to cover the scenarios. In this position paper, we first
identify current status and limitations of SF-TOD systems. After that, we
explore the WebTOD framework, the alternative direction for building a scalable
TOD system when a web/mobile interface is available. In WebTOD, the dialogue
system learns how to understand the web/mobile interface that the human agent
interacts with, powered by a large-scale language model.",0,0,0,0,0,0,0.76213,6.0,0.792074,57
8d5dfba4-e9c0-4c5f-877f-07ed76b187bf,Attribution-aware Weight Transfer: A Warm-Start Initialization for Class-Incremental Semantic Segmentation,8,0.105738,0.625651,"In class-incremental semantic segmentation (CISS), deep learning
architectures suffer from the critical problems of catastrophic forgetting and
semantic background shift. Although recent works focused on these issues,
existing classifier initialization methods do not address the background shift
problem and assign the same initialization weights to both background and new
foreground class classifiers. We propose to address the background shift with a
novel classifier initialization method which employs gradient-based attribution
to identify the most relevant weights for new classes from the classifier's
weights for the previous background and transfers these weights to the new
classifier. This warm-start weight initialization provides a general solution
applicable to several CISS methods. Furthermore, it accelerates learning of new
classes while mitigating forgetting. Our experiments demonstrate significant
improvement in mIoU compared to the state-of-the-art CISS methods on the
Pascal-VOC 2012, ADE20K and Cityscapes datasets.",0,1,0,0,1,0,0.677374,10.0,0.85139,58
31a7ea13-e4df-4de9-b407-547d88066586,Attention Option-Critic,4,0.0542097,0.448617,"Temporal abstraction in reinforcement learning is the ability of an agent to
learn and use high-level behaviors, called options. The option-critic
architecture provides a gradient-based end-to-end learning method to construct
options. We propose an attention-based extension to this framework, which
enables the agent to learn to focus different options on different aspects of
the observation space. We show that this leads to behaviorally diverse options
which are also capable of state abstraction, and prevents the degeneracy
problems of option domination and frequent option switching that occur in
option-critic, while achieving a similar sample complexity. We also demonstrate
the more efficient, interpretable, and reusable nature of the learned options
in comparison with option-critic, through different transfer learning tasks.
Experimental results in a relatively simple four-rooms environment and the more
complex ALE (Arcade Learning Environment) showcase the efficacy of our
approach.",0,0,0,0,0,0,0.569052,16.0,0.888645,29
8fe7bbd7-7e78-4154-9544-62f2b60cd3bc,The Self-Optimal-Transport Feature Transform,15,0.0403095,0.71508,"The Self-Optimal-Transport (SOT) feature transform is designed to upgrade the
set of features of a data instance to facilitate downstream matching or
grouping related tasks. The transformed set encodes a rich representation of
high order relations between the instance features. Distances between
transformed features capture their direct original similarity and their third
party agreement regarding similarity to other features in the set. A particular
min-cost-max-flow fractional matching problem, whose entropy regularized
version can be approximated by an optimal transport (OT) optimization, results
in our transductive transform which is efficient, differentiable, equivariant,
parameterless and probabilistically interpretable. Empirically, the transform
is highly effective and flexible in its use, consistently improving networks it
is inserted into, in a variety of tasks and training schemes. We demonstrate
its merits through the problem of unsupervised clustering and its efficiency
and wide applicability for few-shot-classification, with state-of-the-art
results, and large-scale person re-identification.",0,0,0,0,0,0,0.523864,7.0,0.727461,54
bb0b97cc-6461-4630-a657-b01b2feb175c,Fast Light-Weight Near-Field Photometric Stereo,8,0.325563,0.381422,"We introduce the first end-to-end learning-based solution to near-field
Photometric Stereo (PS), where the light sources are close to the object of
interest. This setup is especially useful for reconstructing large immobile
objects. Our method is fast, producing a mesh from 52 512$\times$384 resolution
images in about 1 second on a commodity GPU, thus potentially unlocking several
AR/VR applications. Existing approaches rely on optimization coupled with a
far-field PS network operating on pixels or small patches. Using optimization
makes these approaches slow and memory intensive (requiring 17GB GPU and 27GB
of CPU memory) while using only pixels or patches makes them highly susceptible
to noise and calibration errors. To address these issues, we develop a
recursive multi-resolution scheme to estimate surface normal and depth maps of
the whole image at each step. The predicted depth map at each scale is then
used to estimate `per-pixel lighting' for the next scale. This design makes our
approach almost 45$\times$ faster and 2$^{\circ}$ more accurate (11.3$^{\circ}$
vs. 13.3$^{\circ}$ Mean Angular Error) than the state-of-the-art near-field PS
reconstruction technique, which uses iterative optimization.",0,1,1,0,1,0,0.268294,12.0,0.768918,35
7ea20d2e-f92b-4728-9ba7-f7f5dec9d94a,Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words,32,0.186382,0.902884,"Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,
QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in
which word similarities estimated by cosine over BERT embeddings are
understated and trace this effect to training data frequency. We find that
relative to human judgements, cosine similarity underestimates the similarity
of frequent words with other instances of the same word or other words across
contexts, even after controlling for polysemy and other factors. We conjecture
that this underestimation of similarity for high frequency words is due to
differences in the representational geometry of high and low frequency words
and provide a formal argument for the two-dimensional case.",1,0,0,0,0,0,0.35444,6.0,0.594033,32
c349f159-1e53-49c7-a8d4-3bfc5909e1a4,Label-enhanced Prototypical Network with Contrastive Learning for Multi-label Few-shot Aspect Category Detection,13,0.618081,0.826386,"Multi-label aspect category detection allows a given review sentence to
contain multiple aspect categories, which is shown to be more practical in
sentiment analysis and attracting increasing attention. As annotating large
amounts of data is time-consuming and labor-intensive, data scarcity occurs
frequently in real-world scenarios, which motivates multi-label few-shot aspect
category detection. However, research on this problem is still in infancy and
few methods are available. In this paper, we propose a novel label-enhanced
prototypical network (LPN) for multi-label few-shot aspect category detection.
The highlights of LPN can be summarized as follows. First, it leverages label
description as auxiliary knowledge to learn more discriminative prototypes,
which can retain aspect-relevant information while eliminating the harmful
effect caused by irrelevant aspects. Second, it integrates with contrastive
learning, which encourages that the sentences with the same aspect label are
pulled together in embedding space while simultaneously pushing apart the
sentences with different aspect labels. In addition, it introduces an adaptive
multi-label inference module to predict the aspect count in the sentence, which
is simple yet effective. Extensive experimental results on three datasets
demonstrate that our proposed model LPN can consistently achieve
state-of-the-art performance.",0,1,0,0,1,0,0.972836,7.0,0.953304,41
a00f56c7-2516-4f66-b537-3df03bc5f2a0,DisenHCN: Disentangled Hypergraph Convolutional Networks for Spatiotemporal Activity Prediction,7,0.0649771,0.518827,"Spatiotemporal activity prediction, aiming to predict user activities at a
specific location and time, is crucial for applications like urban planning and
mobile advertising. Existing solutions based on tensor decomposition or graph
embedding suffer from the following two major limitations: 1) ignoring the
fine-grained similarities of user preferences; 2) user's modeling is entangled.
In this work, we propose a hypergraph neural network model called DisenHCN to
bridge the above gaps. In particular, we first unify the fine-grained user
similarity and the complex matching between user preferences and spatiotemporal
activity into a heterogeneous hypergraph. We then disentangle the user
representations into different aspects (location-aware, time-aware, and
activity-aware) and aggregate corresponding aspect's features on the
constructed hypergraph, capturing high-order relations from different aspects
and disentangles the impact of each aspect for final prediction. Extensive
experiments show that our DisenHCN outperforms the state-of-the-art methods by
14.23% to 18.10% on four real-world datasets. Further studies also convincingly
verify the rationality of each component in our DisenHCN.",0,0,0,0,1,0,0.382773,10.0,0.766182,56
3b16ca88-a203-4d8c-b263-e3956a9c36bc,On the Utility of Prediction Sets in Human-AI Teams,17,0.0302787,0.430628,"Research on human-AI teams usually provides experts with a single label,
which ignores the uncertainty in a model's recommendation. Conformal prediction
(CP) is a well established line of research that focuses on building a
theoretically grounded, calibrated prediction set, which may contain multiple
labels. We explore how such prediction sets impact expert decision-making in
human-AI teams. Our evaluation on human subjects finds that set valued
predictions positively impact experts. However, we notice that the predictive
sets provided by CP can be very large, which leads to unhelpful AI assistants.
To mitigate this, we introduce D-CP, a method to perform CP on some examples
and defer to experts. We prove that D-CP can reduce the prediction set size of
non-deferred examples. We show how D-CP performs in quantitative and in human
subject experiments ($n=120$). Our results suggest that CP prediction sets
improve human-AI team performance over showing the top-1 prediction alone, and
that experts find D-CP prediction sets are more useful than CP prediction sets.",1,1,0,0,0,0,0.0765455,6.0,0.310041,30
9b9187d7-78a9-41c4-bca3-f1a4a242f425,The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding,5,0.138332,0.42494,"In a real-world dialogue system, generated text must be truthful and
informative while remaining fluent and adhering to a prescribed style.
Satisfying these constraints simultaneously is difficult for the two
predominant paradigms in language generation: neural language modeling and
rule-based generation. We describe a hybrid architecture for dialogue response
generation that combines the strengths of both paradigms. The first component
of this architecture is a rule-based content selection model defined using a
new formal framework called dataflow transduction, which uses declarative rules
to transduce a dialogue agent's actions and their results (represented as
dataflow graphs) into context-free grammars representing the space of
contextually acceptable responses. The second component is a constrained
decoding procedure that uses these grammars to constrain the output of a neural
language model, which selects fluent utterances. Our experiments show that this
system outperforms both rule-based and learned approaches in human evaluations
of fluency, relevance, and truthfulness.",1,0,0,0,0,0,0.966806,8.0,0.95199,60
8d847d3a-c340-42f2-8652-a59851b00310,Dialog Inpainting: Turning Documents into Dialogs,47,0.277915,0.899513,"Many important questions (e.g. ""How to eat healthier?"") require conversation
to establish context and explore in depth. However, conversational question
answering (ConvQA) systems have long been stymied by scarce training data that
is expensive to collect. To address this problem, we propose a new technique
for synthetically generating diverse and high-quality dialog data: dialog
inpainting. Our approach takes the text of any document and transforms it into
a two-person dialog between the writer and an imagined reader: we treat
sentences from the article as utterances spoken by the writer, and then use a
dialog inpainter to predict what the imagined reader asked or said in between
each of the writer's utterances. By applying this approach to passages from
Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets
totalling 19 million diverse information-seeking dialogs -- 1,000x larger than
the largest existing ConvQA dataset. Furthermore, human raters judge the answer
adequacy and conversationality of WikiDialog to be as good or better than
existing manually-collected datasets. Using our inpainted data to pre-train
ConvQA retrieval systems, we significantly advance state-of-the-art across
three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains
on standard evaluation metrics.",1,1,1,1,1,0,0.785962,5.0,0.764677,54
bcc1667d-cabd-4552-a435-1fdf5a6e5642,Hybrid-Regressive Neural Machine Translation,5,0.0967859,0.637648,"In this work, we empirically confirm that non-autoregressive translation with
an iterative refinement mechanism (IR-NAT) suffers from poor acceleration
robustness because it is more sensitive to decoding batch size and computing
device setting than autoregressive translation (AT). Inspired by it, we attempt
to investigate how to combine the strengths of autoregressive and
non-autoregressive translation paradigms better. To this end, we demonstrate
through synthetic experiments that prompting a small number of AT's predictions
can promote one-shot non-autoregressive translation to achieve the equivalent
performance of IR-NAT. Following this line, we propose a new two-stage
translation prototype called hybrid-regressive translation (HRT). Specifically,
HRT first generates discontinuous sequences via autoregression (e.g., make a
prediction every k tokens, k>1) and then fills in all previously skipped tokens
at once in a non-autoregressive manner. We also propose a bag of techniques to
effectively and efficiently train HRT without adding any model parameters. HRT
achieves the state-of-the-art BLEU score of 28.49 on the WMT En-De task and is
at least 1.5x faster than AT, regardless of batch size and device. In addition,
another bonus of HRT is that it successfully inherits the good characteristics
of AT in the deep-encoder-shallow-decoder architecture. Concretely, compared to
the vanilla HRT with a 6-layer encoder and 6-layer decoder, the inference speed
of HRT with a 12-layer encoder and 1-layer decoder is further doubled on both
GPU and CPU without BLEU loss.",0,1,0,0,1,0,0.744643,7.0,0.814541,43
3a7a93a7-c72e-4ae7-b8c2-73ddf98c02ee,Vector Quantized Semantic Communication System,9,0.13161,0.343774,"Although analog semantic communication systems have received considerable
attention in the literature, there is less work on digital semantic
communication systems. In this paper, we develop a deep learning (DL)-enabled
vector quantized (VQ) semantic communication system for image transmission,
named VQ-DeepSC. Specifically, we propose a convolutional neural network
(CNN)-based transceiver to extract multi-scale semantic features of images and
introduce multi-scale semantic embedding spaces to perform semantic feature
quantization, rendering the data compatible with digital communication systems.
Furthermore, we employ adversarial training to improve the quality of received
images by introducing a PatchGAN discriminator. Experimental results
demonstrate that the proposed VQ-DeepSC is more robustness than BPG in digital
communication systems and has comparable MS-SSIM performance to the DeepJSCC
method.",0,1,0,0,0,0,0.736951,12.0,0.889984,16
6d20e9f5-536f-43d5-9c9f-c96b77794487,Delta Distillation for Efficient Video Processing,8,0.0242794,0.279359,"This paper aims to accelerate video stream processing, such as object
detection and semantic segmentation, by leveraging the temporal redundancies
that exist between video frames. Instead of propagating and warping features
using motion alignment, such as optical flow, we propose a novel knowledge
distillation schema coined as Delta Distillation. In our proposal, the student
learns the variations in the teacher's intermediate features over time. We
demonstrate that these temporal variations can be effectively distilled due to
the temporal redundancies within video frames. During inference, both teacher
and student cooperate for providing predictions: the former by providing
initial representations extracted only on the key-frame, and the latter by
iteratively estimating and applying deltas for the successive frames. Moreover,
we consider various design choices to learn optimal student architectures
including an end-to-end learnable architecture search. By extensive experiments
on a wide range of architectures, including the most efficient ones, we
demonstrate that delta distillation sets a new state of the art in terms of
accuracy vs. efficiency trade-off for semantic segmentation and object
detection in videos. Finally, we show that, as a by-product, delta distillation
improves the temporal consistency of the teacher model.",0,1,0,0,1,0,0.306666,8.0,0.673263,56
8d0c9375-3841-4566-8afc-c37363288160,Multi-label Transformer for Action Unit Detection,11,0.266015,0.483982,"Action Unit (AU) Detection is the branch of affective computing that aims at
recognizing unitary facial muscular movements. It is key to unlock unbiased
computational face representations and has therefore aroused great interest in
the past few years. One of the main obstacles toward building efficient deep
learning based AU detection system is the lack of wide facial image databases
annotated by AU experts. In that extent the ABAW challenge paves the way toward
better AU detection as it involves a 2M frames AU annotated dataset. In this
paper, we present our submission to the ABAW3 challenge. In a nutshell, we
applied a multi-label detection transformer that leverage multi-head attention
to learn which part of the face image is the most relevant to predict each AU.",0,1,0,0,0,0,0.971812,6.0,0.943801,30
944dade1-4577-4e64-8f22-fafe5065ae8c,Robust Speech Recognition via Large-Scale Weak Supervision,1206,1.0,1.0,"We study the capabilities of speech processing systems trained simply to
predict large amounts of transcripts of audio on the internet. When scaled to
680,000 hours of multilingual and multitask supervision, the resulting models
generalize well to standard benchmarks and are often competitive with prior
fully supervised results but in a zero-shot transfer setting without the need
for any fine-tuning. When compared to humans, the models approach their
accuracy and robustness. We are releasing models and inference code to serve as
a foundation for further work on robust speech processing.",1,1,0,1,0,0,0.954495,7.0,0.931244,100
9907b102-64a7-4125-9c45-723cb7cf6a78,Graph Coloring with Physics-Inspired Graph Neural Networks,14,0.143333,0.358783,"We show how graph neural networks can be used to solve the canonical graph
coloring problem. We frame graph coloring as a multi-class node classification
problem and utilize an unsupervised training strategy based on the statistical
physics Potts model. Generalizations to other multi-class problems such as
community detection, data clustering, and the minimum clique cover problem are
straightforward. We provide numerical benchmark results and illustrate our
approach with an end-to-end application for a real-world scheduling use case
within a comprehensive encode-process-decode framework. Our optimization
approach performs on par or outperforms existing solvers, with the ability to
scale to problems with millions of variables.",1,1,0,0,0,0,0.309937,12.0,0.783245,51
80194f22-b9dd-4c7f-8b90-c7343d81d652,Affective Idiosyncratic Responses to Music,1,0.064043,0.370868,"Affective responses to music are highly personal. Despite consensus that
idiosyncratic factors play a key role in regulating how listeners emotionally
respond to music, precisely measuring the marginal effects of these variables
has proved challenging. To address this gap, we develop computational methods
to measure affective responses to music from over 403M listener comments on a
Chinese social music platform. Building on studies from music psychology in
systematic and quasi-causal analyses, we test for musical, lyrical, contextual,
demographic, and mental health effects that drive listener affective responses.
Finally, motivated by the social phenomenon known as w\v{a}ng-y\`i-y\'un, we
identify influencing factors of platform user self-disclosures, the social
support they receive, and notable differences in discloser user activity.",1,0,0,1,0,0,0.458341,20.0,0.895068,124
5db72a08-bb4d-4637-97ef-c0ad307a9cbd,Scribble-Supervised LiDAR Semantic Segmentation,48,0.373013,0.707702,"Densely annotating LiDAR point clouds remains too expensive and
time-consuming to keep up with the ever growing volume of data. While current
literature focuses on fully-supervised performance, developing efficient
methods that take advantage of realistic weak supervision have yet to be
explored. In this paper, we propose using scribbles to annotate LiDAR point
clouds and release ScribbleKITTI, the first scribble-annotated dataset for
LiDAR semantic segmentation. Furthermore, we present a pipeline to reduce the
performance gap that arises when using such weak annotations. Our pipeline
comprises of three stand-alone contributions that can be combined with any
LiDAR semantic segmentation model to achieve up to 95.7% of the
fully-supervised performance while using only 8% labeled points. Our scribble
annotations and code are available at github.com/ouenal/scribblekitti.",1,1,1,1,0,0,0.708781,5.0,0.720112,59
e950d196-9a30-4e04-a22a-db6f04c5c367,Risk-Driven Design of Perception Systems,4,0.0369742,0.403213,"Modern autonomous systems rely on perception modules to process complex
sensor measurements into state estimates. These estimates are then passed to a
controller, which uses them to make safety-critical decisions. It is therefore
important that we design perception systems to minimize errors that reduce the
overall safety of the system. We develop a risk-driven approach to designing
perception systems that accounts for the effect of perceptual errors on the
performance of the fully-integrated, closed-loop system. We formulate a risk
function to quantify the effect of a given perceptual error on overall safety,
and show how we can use it to design safer perception systems by including a
risk-dependent term in the loss function and generating training data in
risk-sensitive regions. We evaluate our techniques on a realistic vision-based
aircraft detect and avoid application and show that risk-driven design reduces
collision risk by 37% over a baseline system.",1,0,0,0,0,0,0.162482,9.0,0.628969,51
2eb7835b-6fff-45c9-a695-30f32030cd5b,NICO++: Towards Better Benchmarking for Domain Generalization,47,0.202544,0.941617,"Despite the remarkable performance that modern deep neural networks have
achieved on independent and identically distributed (I.I.D.) data, they can
crash under distribution shifts. Most current evaluation methods for domain
generalization (DG) adopt the leave-one-out strategy as a compromise on the
limited number of domains. We propose a large-scale benchmark with extensive
labeled domains named NICO++ along with more rational evaluation methods for
comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose
two metrics to quantify covariate shift and concept shift, respectively. Two
novel generalization bounds from the perspective of data construction are
proposed to prove that limited concept shift and significant covariate shift
favor the evaluation capability for generalization. Through extensive
experiments, NICO++ shows its superior evaluation capability compared with
current DG datasets and its contribution in alleviating unfairness caused by
the leak of oracle knowledge in model selection.",1,0,0,1,0,0,0.444151,7.0,0.694039,113
810a66dd-0e09-4bfb-93ca-08d2dcb801ce,Track Targets by Dense Spatio-Temporal Position Encoding,7,0.276597,0.693672,"In this work, we propose a novel paradigm to encode the position of targets
for target tracking in videos using transformers. The proposed paradigm, Dense
Spatio-Temporal (DST) position encoding, encodes spatio-temporal position
information in a pixel-wise dense fashion. The provided position encoding
provides location information to associate targets across frames beyond
appearance matching by comparing objects in two bounding boxes. Compared to the
typical transformer positional encoding, our proposed encoding is applied to
the 2D CNN features instead of the projected feature vectors to avoid losing
positional information. Moreover, the designed DST encoding can represent the
location of a single-frame object and the evolution of the location of the
trajectory among frames uniformly. Integrated with the DST encoding, we build a
transformer-based multi-object tracking model. The model takes a video clip as
input and conducts the target association in the clip. It can also perform
online inference by associating existing trajectories with objects from the
new-coming frames. Experiments on video multi-object tracking (MOT) and
multi-object tracking and segmentation (MOTS) datasets demonstrate the
effectiveness of the proposed DST position encoding.",0,0,0,0,1,0,0.985338,5.0,0.966198,41
19fc0655-f653-4eb5-9f80-9dbefbad4b39,Improving Chinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity,7,0.238874,0.788199,"Chinese spelling check (CSC) is a fundamental NLP task that detects and
corrects spelling errors in Chinese texts. As most of these spelling errors are
caused by phonetic similarity, effectively modeling the pronunciation of
Chinese characters is a key factor for CSC. In this paper, we consider
introducing an auxiliary task of Chinese pronunciation prediction (CPP) to
improve CSC, and, for the first time, systematically discuss the adaptivity and
granularity of this auxiliary task. We propose SCOPE which builds on top of a
shared encoder two parallel decoders, one for the primary CSC task and the
other for a fine-grained auxiliary CPP task, with a novel adaptive weighting
scheme to balance the two tasks. In addition, we design a delicate iterative
correction strategy for further improvements during inference. Empirical
evaluation shows that SCOPE achieves new state-of-the-art on three CSC
benchmarks, demonstrating the effectiveness and superiority of the auxiliary
CPP task. Comprehensive ablation studies further verify the positive effects of
adaptivity and granularity of the task. Code and data used in this paper are
publicly available at https://github.com/jiahaozhenbang/SCOPE.",1,1,0,0,1,0,0.146156,17.0,0.796791,28
e29de5ac-a743-472c-b846-ef1a02db6c5a,Detecting Methane Plumes using PRISMA: Deep Learning Model and Data Augmentation,5,0.359581,0.764123,"The new generation of hyperspectral imagers, such as PRISMA, has improved
significantly our detection capability of methane (CH4) plumes from space at
high spatial resolution (30m). We present here a complete framework to identify
CH4 plumes using images from the PRISMA satellite mission and a deep learning
model able to detect plumes over large areas. To compensate for the relative
scarcity of PRISMA images, we trained our model by transposing high resolution
plumes from Sentinel-2 to PRISMA. Our methodology thus avoids computationally
expensive synthetic plume generation from Large Eddy Simulations by generating
a broad and realistic training database, and paves the way for large-scale
detection of methane plumes using future hyperspectral sensors (EnMAP, EMIT,
CarbonMapper).",0,1,0,0,0,0,0.984107,16.0,0.988232,16
a0759b15-1960-4570-89ab-9ddb0e8a3bb1,BESS: Balanced Entity Sampling and Sharing for Large-Scale Knowledge Graph Completion,2,0.0669563,0.205606,"We present the award-winning submission to the WikiKG90Mv2 track of
OGB-LSC@NeurIPS 2022. The task is link-prediction on the large-scale knowledge
graph WikiKG90Mv2, consisting of 90M+ nodes and 600M+ edges. Our solution uses
a diverse ensemble of $85$ Knowledge Graph Embedding models combining five
different scoring functions (TransE, TransH, RotatE, DistMult, ComplEx) and two
different loss functions (log-sigmoid, sampled softmax cross-entropy). Each
individual model is trained in parallel on a Graphcore Bow Pod$_{16}$ using
BESS (Balanced Entity Sampling and Sharing), a new distribution framework for
KGE training and inference based on balanced collective communications between
workers. Our final model achieves a validation MRR of 0.2922 and a
test-challenge MRR of 0.2562, winning the first place in the competition. The
code is publicly available at:
https://github.com/graphcore/distributed-kge-poplar/tree/2022-ogb-submission.",1,1,0,0,1,0,0.85723,11.0,0.914237,22
23fe0311-87e0-484b-852e-582d90febbb7,Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model,9,0.133022,0.53828,"Contextual ASR, which takes a list of bias terms as input along with audio,
has drawn recent interest as ASR use becomes more widespread. We are releasing
contextual biasing lists to accompany the Earnings21 dataset, creating a public
benchmark for this task. We present baseline results on this benchmark using a
pretrained end-to-end ASR model from the WeNet toolkit. We show results for
shallow fusion contextual biasing applied to two different decoding algorithms.
Our baseline results confirm observations that end-to-end models struggle in
particular with words that are rarely or never seen during training, and that
existing shallow fusion techniques do not adequately address this problem. We
propose an alternate spelling prediction model that improves recall of rare
words by 34.7% relative and of out-of-vocabulary words by 97.2% relative,
compared to contextual biasing without alternate spellings. This model is
conceptually similar to ones used in prior work, but is simpler to implement as
it does not rely on either a pronunciation dictionary or an existing
text-to-speech system.",0,1,0,0,0,0,0.328361,6.0,0.578224,22
956db933-53d6-4afd-8d81-8106b360c28f,DEMETR: Diagnosing Evaluation Metrics for Translation,24,0.0916938,0.856547,"While machine translation evaluation metrics based on string overlap (e.g.,
BLEU) have their limitations, their computations are transparent: the BLEU
score assigned to a particular candidate translation can be traced back to the
presence or absence of certain words. The operations of newer learned metrics
(e.g., BLEURT, COMET), which leverage pretrained language models to achieve
higher correlations with human quality judgments than BLEU, are opaque in
comparison. In this paper, we shed light on the behavior of these learned
metrics by creating DEMETR, a diagnostic dataset with 31K English examples
(translated from 10 source languages) for evaluating the sensitivity of MT
evaluation metrics to 35 different linguistic perturbations spanning semantic,
syntactic, and morphological error categories. All perturbations were carefully
designed to form minimal pairs with the actual translation (i.e., differ in
only one aspect). We find that learned metrics perform substantially better
than string-based metrics on DEMETR. Additionally, learned metrics differ in
their sensitivity to various phenomena (e.g., BERTScore is sensitive to
untranslated words but relatively insensitive to gender manipulation, while
COMET is much more sensitive to word repetition than to aspectual changes). We
publicly release DEMETR to spur more informed future development of machine
translation evaluation metrics",0,0,1,1,0,0,0.298869,6.0,0.559183,50
858c027f-fcab-429d-91ae-c70402febf85,DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines,4,0.108356,0.514357,"Dialogue models are able to generate coherent and fluent responses, but they
can still be challenging to control and may produce non-engaging, unsafe
results. This unpredictability diminishes user trust and can hinder the use of
the models in the real world. To address this, we introduce DialGuide, a novel
framework for controlling dialogue model behavior using natural language rules,
or guidelines. These guidelines provide information about the context they are
applicable to and what should be included in the response, allowing the models
to generate responses that are more closely aligned with the developer's
expectations and intent. We evaluate DialGuide on three tasks in open-domain
dialogue response generation: guideline selection, response generation, and
response entailment verification. Our dataset contains 10,737 positive and
15,467 negative dialogue context-response-guideline triplets across two domains
- chit-chat and safety. We provide baseline models for the tasks and benchmark
their performance. We also demonstrate that DialGuide is effective in the
dialogue safety domain, producing safe and engaging responses that follow
developer guidelines.",1,1,0,1,0,0,0.751343,6.0,0.786846,71
4b9757d7-1837-4cff-bc36-233c85d92d65,Synonym Detection Using Syntactic Dependency And Neural Embeddings,1,0.0163933,0.0174928,"Recent advances on the Vector Space Model have significantly improved some
NLP applications such as neural machine translation and natural language
generation. Although word co-occurrences in context have been widely used in
counting-/predicting-based distributional models, the role of syntactic
dependencies in deriving distributional semantics has not yet been thoroughly
investigated. By comparing various Vector Space Models in detecting synonyms in
TOEFL, we systematically study the salience of syntactic dependencies in
accounting for distributional similarity. We separate syntactic dependencies
into different groups according to their various grammatical roles and then use
context-counting to construct their corresponding raw and SVD-compressed
matrices. Moreover, using the same training hyperparameters and corpora, we
study typical neural embeddings in the evaluation. We further study the
effectiveness of injecting human-compiled semantic knowledge into neural
embeddings on computing distributional similarity. Our results show that the
syntactically conditioned contexts can interpret lexical semantics better than
the unconditioned ones, whereas retrofitting neural embeddings with semantic
knowledge can significantly improve synonym detection.",0,0,0,0,0,0,0.118417,32.0,0.88498,58
216e6838-dc41-4f6f-a3a7-abc6f0618e45,Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity,36,0.437553,0.817573,"This paper analyzes three formal models of Transformer encoders that differ
in the form of their self-attention mechanism: unique hard attention (UHAT);
generalized unique hard attention (GUHAT), which generalizes UHAT; and
averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers,
viewed as string acceptors, can only recognize formal languages in the
complexity class AC$^0$, the class of languages recognizable by families of
Boolean circuits of constant depth and polynomial size. This upper bound
subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages
or the PARITY language, since those languages are outside AC$^0$ (Furst et al.,
1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are
recognizable by AHAT networks, implying that AHAT can recognize languages that
UHAT and GUHAT cannot.",0,0,0,0,0,0,0.792407,7.0,0.834717,11
9865f5b2-6795-43cb-ada0-24a75a7a1822,Amortized Inference for Heterogeneous Reconstruction in Cryo-EM,20,0.421813,0.538722,"Cryo-electron microscopy (cryo-EM) is an imaging modality that provides
unique insights into the dynamics of proteins and other building blocks of
life. The algorithmic challenge of jointly estimating the poses, 3D structure,
and conformational heterogeneity of a biomolecule from millions of noisy and
randomly oriented 2D projections in a computationally efficient manner,
however, remains unsolved. Our method, cryoFIRE, performs ab initio
heterogeneous reconstruction with unknown poses in an amortized framework,
thereby avoiding the computationally expensive step of pose search while
enabling the analysis of conformational heterogeneity. Poses and conformation
are jointly estimated by an encoder while a physics-based decoder aggregates
the images into an implicit neural representation of the conformational space.
We show that our method can provide one order of magnitude speedup on datasets
containing millions of images without any loss of accuracy. We validate that
the joint estimation of poses and conformations can be amortized over the size
of the dataset. For the first time, we prove that an amortized method can
extract interpretable dynamic information from experimental datasets.",0,1,0,0,1,0,0.651754,7.0,0.777708,45
818ce70b-3ed8-446e-9d65-43d0ae9d193e,Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation,13,0.201012,0.80541,"Fashion attribute editing is a task that aims to convert the semantic
attributes of a given fashion image while preserving the irrelevant regions.
Previous works typically employ conditional GANs where the generator explicitly
learns the target attributes and directly execute the conversion. These
approaches, however, are neither scalable nor generic as they operate only with
few limited attributes and a separate generator is required for each dataset or
attribute set. Inspired by the recent advancement of diffusion models, we
explore the classifier-guided diffusion that leverages the off-the-shelf
diffusion model pretrained on general visual semantics such as Imagenet. In
order to achieve a generic editing pipeline, we pose this as multi-attribute
image manipulation task, where the attribute ranges from item category, fabric,
pattern to collar and neckline. We empirically show that conventional methods
fail in our challenging setting, and study efficient adaptation scheme that
involves recently introduced attention-pooling technique to obtain a
multi-attribute classifier guidance. Based on this, we present a mask-free
fashion attribute editing framework that leverages the classifier logits and
the cross-attention map for manipulation. We empirically demonstrate that our
framework achieves convincing sample quality and attribute alignments.",0,0,0,0,0,0,0.967545,4.0,0.905627,38
3a80b5af-2a81-4653-8d90-68e312fba8e2,Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences,27,0.348644,0.816864,"We propose Probabilistic Warp Consistency, a weakly-supervised learning
objective for semantic matching. Our approach directly supervises the dense
matching scores predicted by the network, encoded as a conditional probability
distribution. We first construct an image triplet by applying a known warp to
one of the images in a pair depicting different instances of the same object
class. Our probabilistic learning objectives are then derived using the
constraints arising from the resulting image triplet. We further account for
occlusion and background clutter present in real image pairs by extending our
probabilistic output space with a learnable unmatched state. To supervise it,
we design an objective between image pairs depicting different object classes.
We validate our method by applying it to four recent semantic matching
architectures. Our weakly-supervised approach sets a new state-of-the-art on
four challenging semantic matching benchmarks. Lastly, we demonstrate that our
objective also brings substantial improvements in the strongly-supervised
regime, when combined with keypoint annotations.",1,1,0,0,1,0,0.553969,7.0,0.739512,59
6a280c55-6406-42d5-a354-59daf42842b9,The Fairness Field Guide: Perspectives from Social and Formal Sciences,4,0.0701397,0.277595,"Over the past several years, a slew of different methods to measure the
fairness of a machine learning model have been proposed. However, despite the
growing number of publications and implementations, there is still a critical
lack of literature that explains the interplay of fair machine learning with
the social sciences of philosophy, sociology, and law. We hope to remedy this
issue by accumulating and expounding upon the thoughts and discussions of fair
machine learning produced by both social and formal (specifically machine
learning and statistics) sciences in this field guide. Specifically, in
addition to giving the mathematical and algorithmic backgrounds of several
popular statistical and causal-based fair machine learning methods, we explain
the underlying philosophical and legal thoughts that support them. Further, we
explore several criticisms of the current approaches to fair machine learning
from sociological and philosophical viewpoints. It is our hope that this field
guide will help fair machine learning practitioners better understand how their
algorithms align with important humanistic values (such as fairness) and how we
can, as a field, design methods and metrics to better serve oppressed and
marginalized populaces.",0,0,0,0,0,0,0.680797,8.0,0.81541,118
98db860c-9885-4c3e-b0ca-2efd2df384fe,In-BoXBART: Get Instructions into Biomedical Multi-Task Learning,17,0.124081,0.402515,"Single-task models have proven pivotal in solving specific tasks; however,
they have limitations in real-world applications where multi-tasking is
necessary and domain shifts are exhibited. Recently, instructional prompts have
shown significant improvement towards multi-task generalization; however, the
effect of instructional prompts and Multi-Task Learning (MTL) has not been
systematically studied in the biomedical domain. Motivated by this, this paper
explores the impact of instructional prompts for biomedical MTL. We introduce
the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)
various categories. Using this meta-dataset, we propose a unified model termed
In-BoXBART, that can jointly learn all tasks of the BoX without any
task-specific modules. To the best of our knowledge, this is the first attempt
to propose a unified model in the biomedical domain and use instructions to
achieve generalization across several biomedical tasks. Experimental results
indicate that the proposed model: 1) outperforms the single-task baseline by
~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)
shows ~23% improvement compared to the single-task baseline in few-shot
learning (i.e., 32 instances per task) on an average. Our analysis indicates
that there is significant room for improvement across tasks in the BoX,
implying the scope for future research direction.",0,1,1,1,0,0,0.798703,3.0,0.620807,66
01fd442a-a12e-4c23-ad2a-88af47ae53cf,Project proposal: A modular reinforcement learning based automated theorem prover,1,0.00138959,0.0317988,"We propose to build a reinforcement learning prover of independent
components: a deductive system (an environment), the proof state representation
(how an agent sees the environment), and an agent training algorithm. To that
purpose, we contribute an additional Vampire-based environment to
$\texttt{gym-saturation}$ package of OpenAI Gym environments for saturation
provers. We demonstrate a prototype of using $\texttt{gym-saturation}$ together
with a popular reinforcement learning framework (Ray $\texttt{RLlib}$).
Finally, we discuss our plans for completing this work in progress to a
competitive automated theorem prover.",1,0,0,1,0,0,0.00351772,10.0,0.274238,35
cf9fe356-d3ef-4c33-8200-e61c7d2b8f0a,From Zero to Production: Baltic-Ukrainian Machine Translation Systems to Aid Refugees,1,0.00168477,0.0711584,"In this paper, we examine the development and usage of six low-resource
machine translation systems translating between the Ukrainian language and each
of the official languages of the Baltic states. We developed these systems in
reaction to the escalating Ukrainian refugee crisis caused by the Russian
military aggression in Ukraine in the hope that they might be helpful for
refugees and public administrations. Now, two months after MT systems were made
public, we analyze their usage patterns and statistics. Our findings show that
the Latvian-Ukrainian and Lithuanian-Ukrainian systems are integrated into the
public services of Baltic states, leading to more than 127 million translated
sentences for the Lithuanian-Ukrainian system. Motivated by these findings, we
further enhance our MT systems by better Ukrainian toponym translation and
publish an improved version of the Lithuanian-Ukrainian system.",0,1,0,0,0,0,0.0649069,9.0,0.521014,28
10967893-6eb9-4ef7-8ff4-2093aaf33e2c,TINYCD: A (Not So) Deep Learning Model For Change Detection,29,0.105563,0.838808,"In this paper, we present a lightweight and effective change detection model,
called TinyCD. This model has been designed to be faster and smaller than
current state-of-the-art change detection models due to industrial needs.
Despite being from 13 to 140 times smaller than the compared change detection
models, and exposing at least a third of the computational complexity, our
model outperforms the current state-of-the-art models by at least $1\%$ on both
F1 score and IoU on the LEVIR-CD dataset, and more than $8\%$ on the WHU-CD
dataset. To reach these results, TinyCD uses a Siamese U-Net architecture
exploiting low-level features in a globally temporal and locally spatial way.
In addition, it adopts a new strategy to mix features in the space-time domain
both to merge the embeddings obtained from the Siamese backbones, and, coupled
with an MLP block, it forms a novel space-semantic attention mechanism, the Mix
and Attention Mask Block (MAMB). Source code, models and results are available
here: https://github.com/AndreaCodegoni/Tiny_model_4_CD",1,1,0,0,1,0,0.281762,8.0,0.660599,76
799ccd80-12f6-45b9-a1ad-2234f58edcc7,Open Vocabulary Extreme Classification Using Generative Models,10,0.420068,0.241337,"The extreme multi-label classification (XMC) task aims at tagging content
with a subset of labels from an extremely large label set. The label vocabulary
is typically defined in advance by domain experts and assumed to capture all
necessary tags. However in real world scenarios this label set, although large,
is often incomplete and experts frequently need to refine it. To develop
systems that simplify this process, we introduce the task of open vocabulary
XMC (OXMC): given a piece of content, predict a set of labels, some of which
may be outside of the known tag set. Hence, in addition to not having training
data for some labels - as is the case in zero-shot classification - models need
to invent some labels on-the-fly. We propose GROOV, a fine-tuned seq2seq model
for OXMC that generates the set of labels as a flat sequence and is trained
using a novel loss independent of predicted label order. We show the efficacy
of the approach, experimenting with popular XMC datasets for which GROOV is
able to predict meaningful labels outside the given vocabulary while performing
on par with state-of-the-art solutions for known labels.",0,1,1,0,0,0,0.918834,7.0,0.901621,32
6f1802e2-6e25-491d-b170-0931a59991b5,Object Permanence Emerges in a Random Walk along Memory,11,0.116904,0.102205,"This paper proposes a self-supervised objective for learning representations
that localize objects under occlusion - a property known as object permanence.
A central question is the choice of learning signal in cases of total
occlusion. Rather than directly supervising the locations of invisible objects,
we propose a self-supervised objective that requires neither human annotation,
nor assumptions about object dynamics. We show that object permanence can
emerge by optimizing for temporal coherence of memory: we fit a Markov walk
along a space-time graph of memories, where the states in each time step are
non-Markovian features from a sequence encoder. This leads to a memory
representation that stores occluded objects and predicts their motion, to
better localize them. The resulting model outperforms existing approaches on
several datasets of increasing complexity and realism, despite requiring
minimal supervision, and hence being broadly applicable.",0,0,0,0,1,0,0.671318,10.0,0.849732,58
45fb04c4-2cd7-4df1-a6aa-d70e100626b7,Unsupervised Image-to-Image Translation with Generative Prior,26,0.0616316,0.839001,"Unsupervised image-to-image translation aims to learn the translation between
two visual domains without paired data. Despite the recent progress in image
translation models, it remains challenging to build mappings between complex
domains with drastic visual discrepancies. In this work, we present a novel
framework, Generative Prior-guided UNsupervised Image-to-image Translation
(GP-UNIT), to improve the overall quality and applicability of the translation
algorithm. Our key insight is to leverage the generative prior from pre-trained
class-conditional GANs (e.g., BigGAN) to learn rich content correspondences
across various domains. We propose a novel coarse-to-fine scheme: we first
distill the generative prior to capture a robust coarse-level content
representation that can link objects at an abstract semantic level, based on
which fine-level content features are adaptively learned for more accurate
multi-level content correspondences. Extensive experiments demonstrate the
superiority of our versatile framework over state-of-the-art methods in robust,
high-quality and diversified translations, even for challenging and distant
domains.",1,1,0,0,1,0,0.502912,7.0,0.71892,47
42cdefcc-1fd4-4b97-a667-001c28ce8977,Beyond Prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations,12,0.0764579,0.252121,"Recent work has demonstrated that pre-trained language models (PLMs) are
zero-shot learners. However, most existing zero-shot methods involve heavy
human engineering or complicated self-training pipelines, hindering their
application to new situations. In this work, we show that zero-shot text
classification can be improved simply by clustering texts in the embedding
spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian
Gaussian Mixture Model after initializing cluster positions and shapes using
class names. Despite its simplicity, this approach achieves superior or
comparable performance on both topic and sentiment classification datasets and
outperforms prior works significantly on unbalanced datasets. We further
explore the applicability of our clustering approach by evaluating it on 14
datasets with more diverse topics, text lengths, and numbers of classes. Our
approach achieves an average of 20% absolute improvement over prompt-based
zero-shot learning. Finally, we compare different PLM embedding spaces and find
that texts are well-clustered by topics even if the PLM is not explicitly
pre-trained to generate meaningful sentence embeddings. This work indicates
that PLM embeddings can categorize texts without task-specific fine-tuning,
thus providing a new way to analyze and utilize their knowledge and zero-shot
learning ability.",1,1,0,0,1,1,0.564607,7.0,0.743721,55
b8a6121c-14da-4bdf-aaf9-5388a9d14f32,Seg4Reg+: Consistency Learning between Spine Segmentation and Cobb Angle Regression,11,0.522232,0.265817,"Automated methods for Cobb angle estimation are of high demand for scoliosis
assessment. Existing methods typically calculate the Cobb angle from landmark
estimation, or simply combine the low-level task (e.g., landmark detection and
spine segmentation) with the Cobb angle regression task, without fully
exploring the benefits from each other. In this study, we propose a novel
multi-task framework, named Seg4Reg+, which jointly optimizes the segmentation
and regression networks. We thoroughly investigate both local and global
consistency and knowledge transfer between each other. Specifically, we propose
an attention regularization module leveraging class activation maps (CAMs) from
image-segmentation pairs to discover additional supervision in the regression
network, and the CAMs can serve as a region-of-interest enhancement gate to
facilitate the segmentation task in turn. Meanwhile, we design a novel triangle
consistency learning to train the two networks jointly for global optimization.
The evaluations performed on the public AASCE Challenge dataset demonstrate the
effectiveness of each module and superior performance of our model to the
state-of-the-art methods.",0,1,0,0,1,0,0.593939,11.0,0.844236,20
43cc387b-d578-4b5e-976d-4c5aa917f470,Repairing Bugs in Python Assignments Using Large Language Models,36,0.691624,0.996491,"Students often make mistakes on their introductory programming assignments as
part of their learning process. Unfortunately, providing custom repairs for
these mistakes can require a substantial amount of time and effort from class
instructors. Automated program repair (APR) techniques can be used to
synthesize such fixes. Prior work has explored the use of symbolic and neural
techniques for APR in the education domain. Both types of approaches require
either substantial engineering efforts or large amounts of data and training.
We propose to use a large language model trained on code, such as Codex, to
build an APR system -- MMAPR -- for introductory Python programming
assignments. Our system can fix both syntactic and semantic mistakes by
combining multi-modal prompts, iterative querying, test-case-based selection of
few-shots, and program chunking. We evaluate MMAPR on 286 real student programs
and compare to a baseline built by combining a state-of-the-art Python syntax
repair engine, BIFI, and state-of-the-art Python semantic repair engine for
student assignments, Refactory. We find that MMAPR can fix more programs and
produce smaller patches on average.",0,1,0,0,0,0,0.871602,5.0,0.821936,51
30c9417e-1837-42ff-a7c9-11b95bbc52e5,Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency,2,0.0426216,0.325421,"The knowledge graph (KG) stores a large amount of structural knowledge, while
it is not easy for direct human understanding. Knowledge graph-to-text
(KG-to-text) generation aims to generate easy-to-understand sentences from the
KG, and at the same time, maintains semantic consistency between generated
sentences and the KG. Existing KG-to-text generation methods phrase this task
as a sequence-to-sequence generation task with linearized KG as input and
consider the consistency issue of the generated texts and KG through a simple
selection between decoded sentence word and KG node word at each time step.
However, the linearized KG order is commonly obtained through a heuristic
search without data-driven optimization. In this paper, we optimize the
knowledge description order prediction under the order supervision extracted
from the caption and further enhance the consistency of the generated sentences
and KG through syntactic and semantic regularization. We incorporate the
Part-of-Speech (POS) syntactic tags to constrain the positions to copy words
from the KG and employ a semantic context scoring function to evaluate the
semantic fitness for each word in its local context when decoding each word in
the generated sentence. Extensive experiments are conducted on two datasets,
WebNLG and DART, and achieve state-of-the-art performances.",0,0,0,0,0,0,0.752081,6.0,0.787201,40
84dad1b1-d5e3-4b1b-8ea3-0ba08bff4ece,A simple language-agnostic yet very strong baseline system for hate speech and offensive content identification,7,0.141822,0.794308,"For automatically identifying hate speech and offensive content in tweets, a
system based on a classical supervised algorithm only fed with character
n-grams, and thus completely language-agnostic, is proposed by the SATLab team.
After its optimization in terms of the feature weighting and the classifier
parameters, it reached, in the multilingual HASOC 2021 challenge, a medium
performance level in English, the language for which it is easy to develop deep
learning approaches relying on many external linguistic resources, but a far
better level for the two less resourced language, Hindi and Marathi. It ends
even first when performances are averaged over the three tasks in these
languages, outperforming many deep learning approaches. These performances
suggest that it is an interesting reference level to evaluate the benefits of
using more complex approaches such as deep learning or taking into account
complementary resources.",0,1,0,0,0,0,0.603851,5.0,0.662729,27
8ba3cf92-280f-4650-b0b4-72c02261fc58,On Label Granularity and Object Localization,5,0.0914579,0.395277,"Weakly supervised object localization (WSOL) aims to learn representations
that encode object location using only image-level category labels. However,
many objects can be labeled at different levels of granularity. Is it an
animal, a bird, or a great horned owl? Which image-level labels should we use?
In this paper we study the role of label granularity in WSOL. To facilitate
this investigation we introduce iNatLoc500, a new large-scale fine-grained
benchmark dataset for WSOL. Surprisingly, we find that choosing the right
training label granularity provides a much larger performance boost than
choosing the best WSOL algorithm. We also show that changing the label
granularity can significantly improve data efficiency.",0,1,1,1,0,0,0.576692,9.0,0.804371,64
f80c7fe8-71b4-4337-9774-d8d3acdb8331,Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization,11,0.0366389,0.214299,"In long document controllable summarization, where labeled data is scarce,
pretrained models struggle to adapt to the task and effectively respond to user
queries. In this paper, we introduce Socratic pretraining, a question-driven,
unsupervised pretraining objective specifically designed to improve
controllability in summarization tasks. By training a model to generate and
answer relevant questions in a given context, Socratic pretraining enables the
model to more effectively adhere to user-provided queries and identify relevant
content to be summarized. We demonstrate the effectiveness of this approach
through extensive experimentation on two summarization domains, short stories
and dialogue, and multiple control strategies: keywords, questions, and factoid
QA pairs. Our pretraining method relies only on unlabeled documents and a
question generation system and outperforms pre-finetuning approaches that use
additional supervised data. Furthermore, our results show that Socratic
pretraining cuts task-specific labeled data requirements in half, is more
faithful to user-provided queries, and achieves state-of-the-art performance on
QMSum and SQuALITY.",1,1,0,0,1,0,0.33072,5.0,0.495629,62
268ad15f-e0ba-4a39-a2c8-ff3b3b2a8110,The Effect of Preferences in Abstract Argumentation Under a Claim-Centric View,4,0.0415843,0.198583,"In this paper, we study the effect of preferences in abstract argumentation
under a claim-centric perspective. Recent work has revealed that semantical and
computational properties can change when reasoning is performed on claim-level
rather than on the argument-level, while under certain natural restrictions
(arguments with the same claims have the same outgoing attacks) these
properties are conserved. We now investigate these effects when, in addition,
preferences have to be taken into account and consider four prominent
reductions to handle preferences between arguments. As we shall see, these
reductions give rise to different classes of claim-augmented argumentation
frameworks, and behave differently in terms of semantic properties and
computational complexity. This strengthens the view that the actual choice for
handling preferences has to be taken with care.",0,0,0,0,0,0,0.00352886,17.0,0.573268,40
83bd75ec-1b8e-4bab-90d6-9e7367aace0f,Dataset Distillation via Factorization,90,0.506473,0.942004,"In this paper, we study \xw{dataset distillation (DD)}, from a novel
perspective and introduce a \emph{dataset factorization} approach, termed
\emph{HaBa}, which is a plug-and-play strategy portable to any existing DD
baseline. Unlike conventional DD approaches that aim to produce distilled and
representative samples, \emph{HaBa} explores decomposing a dataset into two
components: data \emph{Ha}llucination networks and \emph{Ba}ses, where the
latter is fed into the former to reconstruct image samples. The flexible
combinations between bases and hallucination networks, therefore, equip the
distilled data with exponential informativeness gain, which largely increase
the representation capability of distilled datasets. To furthermore increase
the data efficiency of compression results, we further introduce a pair of
adversarial contrastive constraints on the resultant hallucination networks and
bases, which increase the diversity of generated images and inject more
discriminant information into the factorization. Extensive comparisons and
experiments demonstrate that our method can yield significant improvement on
downstream classification tasks compared with previous state of the arts, while
reducing the total number of compressed parameters by up to 65\%. Moreover,
distilled datasets by our approach also achieve \textasciitilde10\% higher
accuracy than baseline methods in cross-architecture generalization. Our code
is available \href{https://github.com/Huage001/DatasetFactorization}{here}.",0,0,0,0,1,0,0.628301,7.0,0.768597,72
dab2a3c8-61bb-4c34-a58b-6213a9867ec1,Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation and Instance Generation,14,0.223657,0.655283,"We study the problem of few-shot Fine-grained Entity Typing (FET), where only
a few annotated entity mentions with contexts are given for each entity type.
Recently, prompt-based tuning has demonstrated superior performance to standard
fine-tuning in few-shot scenarios by formulating the entity type classification
task as a ''fill-in-the-blank'' problem. This allows effective utilization of
the strong language modeling capability of Pre-trained Language Models (PLMs).
Despite the success of current prompt-based tuning approaches, two major
challenges remain: (1) the verbalizer in prompts is either manually designed or
constructed from external knowledge bases, without considering the target
corpus and label hierarchy information, and (2) current approaches mainly
utilize the representation power of PLMs, but have not explored their
generation power acquired through extensive general-domain pre-training. In
this work, we propose a novel framework for few-shot FET consisting of two
modules: (1) an entity type label interpretation module automatically learns to
relate type labels to the vocabulary by jointly leveraging few-shot instances
and the label hierarchy, and (2) a type-based contextualized instance generator
produces new instances based on given instances to enlarge the training set for
better generalization. On three benchmark datasets, our model outperforms
existing methods by significant margins. Code can be found at
https://github.com/teapot123/Fine-Grained-Entity-Typing.",0,1,0,0,1,0,0.710914,6.0,0.767751,60
1d04875a-ea92-4f3f-acfb-4f9b2547fa08,Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation,16,0.169168,0.919731,"Continual relation extraction (CRE) aims to continually learn new relations
from a class-incremental data stream. CRE model usually suffers from
catastrophic forgetting problem, i.e., the performance of old relations
seriously degrades when the model learns new relations. Most previous work
attributes catastrophic forgetting to the corruption of the learned
representations as new relations come, with an implicit assumption that the CRE
models have adequately learned the old relations. In this paper, through
empirical studies we argue that this assumption may not hold, and an important
reason for catastrophic forgetting is that the learned representations do not
have good robustness against the appearance of analogous relations in the
subsequent learning process. To address this issue, we encourage the model to
learn more precise and robust representations through a simple yet effective
adversarial class augmentation mechanism (ACA), which is easy to implement and
model-agnostic. Experimental results show that ACA can consistently improve the
performance of state-of-the-art CRE models on two popular benchmarks.",1,1,0,0,0,0,0.520557,7.0,0.726122,43
cc744f11-8744-4ce8-9287-a30997011678,HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks,13,0.0432695,0.237342,"The workflow of pretraining and fine-tuning has emerged as a popular paradigm
for solving various NLP and V&L (Vision-and-Language) downstream tasks. With
the capacity of pretrained models growing rapidly, how to perform
parameter-efficient fine-tuning has become fairly important for quick transfer
learning and deployment. In this paper, we design a novel unified
parameter-efficient transfer learning framework that works effectively on both
pure language and V&L tasks. In particular, we use a shared hypernetwork that
takes trainable hyper-embeddings as input, and outputs weights for fine-tuning
different small modules in a pretrained language model, such as tuning the
parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and
feed-forward blocks (i.e., adapter-tuning). We define a set of embeddings
(e.g., layer, block, task and visual embeddings) as the key components to
calculate hyper-embeddings, which thus can support both pure language and V&L
tasks. Our proposed framework adds fewer trainable parameters in multi-task
learning while achieving superior performances and transfer ability compared to
state-of-the-art methods. Empirical results on the GLUE benchmark and multiple
V&L tasks confirm the effectiveness of our framework on both textual and visual
modalities.",0,1,0,0,0,0,0.803664,5.0,0.775574,43
ccca76bc-5531-4d33-a69d-ca94ca64f833,Transformation-Equivariant 3D Object Detection for Autonomous Driving,32,0.820934,0.880815,"3D object detection received increasing attention in autonomous driving
recently. Objects in 3D scenes are distributed with diverse orientations.
Ordinary detectors do not explicitly model the variations of rotation and
reflection transformations. Consequently, large networks and extensive data
augmentation are required for robust detection. Recent equivariant networks
explicitly model the transformation variations by applying shared networks on
multiple transformed point clouds, showing great potential in object geometry
modeling. However, it is difficult to apply such networks to 3D object
detection in autonomous driving due to its large computation cost and slow
reasoning speed. In this work, we present TED, an efficient
Transformation-Equivariant 3D Detector to overcome the computation cost and
speed issues. TED first applies a sparse convolution backbone to extract
multi-channel transformation-equivariant voxel features; and then aligns and
aggregates these equivariant features into lightweight and compact
representations for high-performance 3D object detection. On the highly
competitive KITTI 3D car detection leaderboard, TED ranked 1st among all
submissions with competitive efficiency.",0,1,0,0,1,0,0.98145,6.0,0.962279,44
92751b14-239d-46ab-81b7-a3427ef77414,Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution,9,0.433111,0.599197,"Generating agents that can achieve zero-shot coordination (ZSC) with unseen
partners is a new challenge in cooperative multi-agent reinforcement learning
(MARL). Recently, some studies have made progress in ZSC by exposing the agents
to diverse partners during the training process. They usually involve self-play
when training the partners, implicitly assuming that the tasks are homogeneous.
However, many real-world tasks are heterogeneous, and hence previous methods
may be inefficient. In this paper, we study the heterogeneous ZSC problem for
the first time and propose a general method based on coevolution, which
coevolves two populations of agents and partners through three sub-processes:
pairing, updating and selection. Experimental results on various heterogeneous
tasks highlight the necessity of considering the heterogeneous setting and
demonstrate that our proposed method is a promising solution for heterogeneous
ZSC tasks.",1,0,1,0,0,0,0.878061,6.0,0.855752,60
7ba53a1c-3231-416b-a8c3-9e75c8656a85,GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator,20,0.0754919,0.652066,"Pre-trained models have achieved remarkable success in natural language
processing (NLP). However, existing pre-training methods underutilize the
benefits of language understanding for generation. Inspired by the idea of
Generative Adversarial Networks (GANs), we propose a GAN-style model for
encoder-decoder pre-training by introducing an auxiliary discriminator,
unifying the ability of language understanding and generation in a single
model. Our model, named as GanLM, is trained with two pre-training objectives:
replaced token detection and replaced token denoising. Specifically, given
masked source sentences, the generator outputs the target distribution and the
discriminator predicts whether the target sampled tokens from distribution are
incorrect. The target sentence is replaced with misclassified tokens to
construct noisy previous context, which is used to generate the gold sentence.
In general, both tasks improve the ability of language understanding and
generation by selectively using the denoising data. Extensive experiments in
language generation benchmarks show that GanLM with the powerful language
understanding capability outperforms various strong pre-trained language models
(PLMs) and achieves state-of-the-art performance.",0,0,0,0,1,0,0.498374,7.0,0.71705,60
150d49b6-13b6-4e90-ae08-2cef3b38dcd4,Residual Swin Transformer Channel Attention Network for Image Demosaicing,9,0.0756632,0.448439,"Image demosaicing is problem of interpolating full- resolution color images
from raw sensor (color filter array) data. During last decade, deep neural
networks have been widely used in image restoration, and in particular, in
demosaicing, attaining significant performance improvement. In recent years,
vision transformers have been designed and successfully used in various
computer vision applications. One of the recent methods of image restoration
based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art
performance with a smaller number of parameters than neural network-based
methods. Inspired by the success of SwinIR, we propose in this paper a novel
Swin Transformer-based network for image demosaicing, called RSTCANet. To
extract image features, RSTCANet stacks several residual Swin Transformer
Channel Attention blocks (RSTCAB), introducing the channel attention for each
two successive ST blocks. Extensive experiments demonstrate that RSTCANet out-
performs state-of-the-art image demosaicing methods, and has a smaller number
of parameters.",0,1,0,0,1,0,0.79537,7.0,0.836018,41
b42888e9-cc29-4055-b44f-600780bcfca4,DIMBA: Discretely Masked Black-Box Attack in Single Object Tracking,18,0.0710226,0.746282,"The adversarial attack can force a CNN-based model to produce an incorrect
output by craftily manipulating human-imperceptible input. Exploring such
perturbations can help us gain a deeper understanding of the vulnerability of
neural networks, and provide robustness to deep learning against miscellaneous
adversaries. Despite extensive studies focusing on the robustness of image,
audio, and NLP, works on adversarial examples of visual object tracking --
especially in a black-box manner -- are quite lacking. In this paper, we
propose a novel adversarial attack method to generate noises for single object
tracking under black-box settings, where perturbations are merely added on
initial frames of tracking sequences, which is difficult to be noticed from the
perspective of a whole video clip. Specifically, we divide our algorithm into
three components and exploit reinforcement learning for localizing important
frame patches precisely while reducing unnecessary computational queries
overhead. Compared to existing techniques, our method requires fewer queries on
initialized frames of a video to manipulate competitive or even better attack
performance. We test our algorithm in both long-term and short-term datasets,
including OTB100, VOT2018, UAV123, and LaSOT. Extensive experiments demonstrate
the effectiveness of our method on three mainstream types of trackers:
discrimination, Siamese-based, and reinforcement learning-based trackers.",0,1,1,0,0,0,0.290589,11.0,0.756497,72
f64ae866-c9b7-4b1c-9e21-1586fca60e23,"A General Framework for the Representation of Function and Affordance: A Cognitive, Causal, and Grounded Approach, and a Step Toward AGI",3,0.0414053,0.289852,"In AI research, so far, the attention paid to the characterization and
representation of function and affordance has been sporadic and sparse, even
though this aspect features prominently in an intelligent system's functioning.
In the sporadic and sparse, though commendable efforts so far devoted to the
characterization and understanding of function and affordance, there has also
been no general framework that could unify all the different use domains and
situations related to the representation and application of functional
concepts. This paper develops just such a general framework, with an approach
that emphasizes the fact that the representations involved must be explicitly
cognitive and conceptual, and they must also contain causal characterizations
of the events and processes involved, as well as employ conceptual constructs
that are grounded in the referents to which they refer, in order to achieve
maximal generality. The basic general framework is described, along with a set
of basic guiding principles with regards to the representation of
functionality. To properly and adequately characterize and represent
functionality, a descriptive representation language is needed. This language
is defined and developed, and many examples of its use are described. The
general framework is developed based on an extension of the general language
meaning representational framework called conceptual dependency. To support the
general characterization and representation of functionality, the basic
conceptual dependency framework is enhanced with representational devices
called structure anchor and conceptual dependency elaboration, together with
the definition of a set of ground level concepts. These novel representational
constructs are defined, developed, and described. A general framework dealing
with functionality would represent a major step toward achieving Artificial
General Intelligence.",0,0,0,0,0,0,0.0010021,16.0,0.467838,112
d7f37da9-714b-494b-aec8-cbfff261a6f3,The Inverse of Exact Renormalization Group Flows as Statistical Inference,9,0.0177005,0.877106,"We build on the view of the Exact Renormalization Group (ERG) as an
instantiation of Optimal Transport described by a functional
convection-diffusion equation. We provide a new information theoretic
perspective for understanding the ERG through the intermediary of Bayesian
Statistical Inference. This connection is facilitated by the Dynamical Bayesian
Inference scheme, which encodes Bayesian inference in the form of a one
parameter family of probability distributions solving an integro-differential
equation derived from Bayes' law. In this note, we demonstrate how the
Dynamical Bayesian Inference equation is, itself, equivalent to a diffusion
equation which we dub Bayesian Diffusion. Identifying the features that define
Bayesian Diffusion, and mapping them onto the features that define the ERG, we
obtain a dictionary outlining how renormalization can be understood as the
inverse of statistical inference.",0,0,0,0,0,0,1.30162e-05,22.0,0.415511,81
87aef697-0a5d-419d-8586-fed5443fb2cb,A Class-Aware Representation Refinement Framework for Graph Classification,2,0.00635586,0.137489,"Graph Neural Networks (GNNs) are widely used for graph representation
learning. Despite its prevalence, GNN suffers from two drawbacks in the graph
classification task, the neglect of graph-level relationships, and the
generalization issue. Each graph is treated separately in GNN message
passing/graph pooling, and existing methods to address overfitting operate on
each individual graph. This makes the graph representations learnt less
effective in the downstream classification. In this paper, we propose a
Class-Aware Representation rEfinement (CARE) framework for the task of graph
classification. CARE computes simple yet powerful class representations and
injects them to steer the learning of graph representations towards better
class separability. CARE is a plug-and-play framework that is highly flexible
and able to incorporate arbitrary GNN backbones without significantly
increasing the computational cost. We also theoretically prove that CARE has a
better generalization upper bound than its GNN backbone through
Vapnik-Chervonenkis (VC) dimension analysis. Our extensive experiments with 10
well-known GNN backbones on 9 benchmark datasets validate the superiority and
effectiveness of CARE over its GNN counterparts.",0,1,0,0,0,0,0.242084,7.0,0.586784,58
6e9c8e10-256e-49e6-aa0f-3e48811c0ed3,Protea: Client Profiling within Federated Systems using Flower,3,0.0668571,0.135456,"Federated Learning (FL) has emerged as a prospective solution that
facilitates the training of a high-performing centralised model without
compromising the privacy of users. While successful, research is currently
limited by the possibility of establishing a realistic large-scale FL system at
the early stages of experimentation. Simulation can help accelerate this
process. To facilitate efficient scalable FL simulation of heterogeneous
clients, we design and implement Protea, a flexible and lightweight client
profiling component within federated systems using the FL framework Flower. It
allows automatically collecting system-level statistics and estimating the
resources needed for each client, thus running the simulation in a
resource-aware fashion. The results show that our design successfully increases
parallelism for 1.66 $\times$ faster wall-clock time and 2.6$\times$ better GPU
utilisation, which enables large-scale experiments on heterogeneous clients.",0,1,0,0,0,0,0.808204,5.0,0.778428,24
11c42af2-1ea3-4c28-821e-f62f7740ce15,Rethinking Performance Gains in Image Dehazing Networks,22,0.326704,0.863311,"Image dehazing is an active topic in low-level vision, and many image
dehazing networks have been proposed with the rapid development of deep
learning. Although these networks' pipelines work fine, the key mechanism to
improving image dehazing performance remains unclear. For this reason, we do
not target to propose a dehazing network with fancy modules; rather, we make
minimal modifications to popular U-Net to obtain a compact dehazing network.
Specifically, we swap out the convolutional blocks in U-Net for residual blocks
with the gating mechanism, fuse the feature maps of main paths and skip
connections using the selective kernel, and call the resulting U-Net variant
gUNet. As a result, with a significantly reduced overhead, gUNet is superior to
state-of-the-art methods on multiple image dehazing datasets. Finally, we
verify these key designs to the performance gain of image dehazing networks
through extensive ablation studies.",1,1,0,0,1,0,0.929768,9.0,0.929707,62
0621e379-52b2-4c8c-9503-91ba1696a152,Far Away in the Deep Space: Dense Nearest-Neighbor-Based Out-of-Distribution Detection,2,0.0334368,0.0555147,"The key to out-of-distribution detection is density estimation of the
in-distribution data or of its feature representations. This is particularly
challenging for dense anomaly detection in domains where the in-distribution
data has a complex underlying structure. Nearest-Neighbors approaches have been
shown to work well in object-centric data domains, such as industrial
inspection and image classification. In this paper, we show that
nearest-neighbor approaches also yield state-of-the-art results on dense
novelty detection in complex driving scenes when working with an appropriate
feature representation. In particular, we find that transformer-based
architectures produce representations that yield much better similarity metrics
for the task. We identify the multi-head structure of these models as one of
the reasons, and demonstrate a way to transfer some of the improvements to
CNNs. Ultimately, the approach is simple and non-invasive, i.e., it does not
affect the primary segmentation performance, refrains from training on examples
of anomalies, and achieves state-of-the-art results on RoadAnomaly,
StreetHazards, and SegmentMeIfYouCan-Anomaly.",1,1,0,0,1,0,0.891468,4.0,0.797097,72
399e2e99-1978-4c5e-9bf9-73ed043a1248,An IoT Cloud and Big Data Architecture for the Maintenance of Home Appliances,2,0.0433563,0.20373,"Billions of interconnected Internet of Things (IoT) sensors and devices
collect tremendous amounts of data from real-world scenarios. Big data is
generating increasing interest in a wide range of industries. Once data is
analyzed through compute-intensive Machine Learning (ML) methods, it can derive
critical business value for organizations. Powerfulplatforms are essential to
handle and process such massive collections of information cost-effectively and
conveniently. This work introduces a distributed and scalable platform
architecture that can be deployed for efficient real-world big data collection
and analytics. The proposed system was tested with a case study for Predictive
Maintenance of Home Appliances, where current and vibration sensors with high
acquisition frequency were connected to washing machines and refrigerators. The
introduced platform was used to collect, store, and analyze the data. The
experimental results demonstrated that the presented system could be
advantageous for tackling real-world IoT scenarios in a cost-effective and
local approach.",0,1,0,0,0,0,0.0165508,12.0,0.524798,15
33b14d5e-7ebd-4b96-8653-6991fd0d33b3,Processing the structure of documents: Logical Layout Analysis of historical newspapers in French,4,0.122934,0.495934,"Background. In recent years, libraries and archives led important
digitisation campaigns that opened the access to vast collections of historical
documents. While such documents are often available as XML ALTO documents, they
lack information about their logical structure. In this paper, we address the
problem of Logical Layout Analysis applied to historical documents in French.
We propose a rule-based method, that we evaluate and compare with two
Machine-Learning models, namely RIPPER and Gradient Boosting. Our data set
contains French newspapers, periodicals and magazines, published in the first
half of the twentieth century in the Franche-Comt\'e Region. Results. Our
rule-based system outperforms the two other models in nearly all evaluations.
It has especially better Recall results, indicating that our system covers more
types of every logical label than the other two models. When comparing RIPPER
with Gradient Boosting, we can observe that Gradient Boosting has better
Precision scores but RIPPER has better Recall scores. Conclusions. The
evaluation shows that our system outperforms the two Machine Learning models,
and provides significantly higher Recall. It also confirms that our system can
be used to produce annotated data sets that are large enough to envisage
Machine Learning or Deep Learning approaches for the task of Logical Layout
Analysis. Combining rules and Machine Learning models into hybrid systems could
potentially provide even better performances. Furthermore, as the layout in
historical documents evolves rapidly, one possible solution to overcome this
problem would be to apply Rule Learning algorithms to bootstrap rule sets
adapted to different publication periods.",0,1,0,0,0,0,0.0236046,18.0,0.70312,39
a5923c8b-1d16-472d-9e6e-93a8c0a43ff2,Optimal Representations for Covariate Shift,51,0.188739,0.83161,"Machine learning systems often experience a distribution shift between
training and testing. In this paper, we introduce a simple variational
objective whose optima are exactly the set of all representations on which risk
minimizers are guaranteed to be robust to any distribution shift that preserves
the Bayes predictor, e.g., covariate shifts. Our objective has two components.
First, a representation must remain discriminative for the task, i.e., some
predictor must be able to simultaneously minimize the source and target risk.
Second, the representation's marginal support needs to be the same across
source and target. We make this practical by designing self-supervised
objectives that only use unlabelled data and augmentations to train robust
representations. Our objectives give insights into the robustness of CLIP, and
further improve CLIP's representations to achieve SOTA results on DomainBed.",1,0,0,0,0,0,0.793983,7.0,0.835408,73
9234bd3b-5c64-4c2c-ac05-e030420274a6,Language-free Training for Zero-shot Video Grounding,10,0.228548,0.22102,"Given an untrimmed video and a language query depicting a specific temporal
moment in the video, video grounding aims to localize the time interval by
understanding the text and video simultaneously. One of the most challenging
issues is an extremely time- and cost-consuming annotation collection,
including video captions in a natural language form and their corresponding
temporal regions. In this paper, we present a simple yet novel training
framework for video grounding in the zero-shot setting, which learns a network
with only video data without any annotation. Inspired by the recent
language-free paradigm, i.e. training without language data, we train the
network without compelling the generation of fake (pseudo) text queries into a
natural language form. Specifically, we propose a method for learning a video
grounding model by selecting a temporal interval as a hypothetical correct
answer and considering the visual feature selected by our method in the
interval as a language feature, with the help of the well-aligned
visual-language space of CLIP. Extensive experiments demonstrate the prominence
of our language-free training framework, outperforming the existing zero-shot
video grounding method and even several weakly-supervised approaches with large
margins on two standard datasets.",0,1,0,0,1,0,0.815339,6.0,0.81914,62
77f29bfe-d064-49c9-9e30-427e16dc1a28,Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach,15,0.137936,0.437965,"Large Language Models have demonstrated remarkable few-shot performance, but
the performance can be sensitive to the selection of few-shot instances. We
propose PATRON, a new method that uses prompt-based uncertainty estimation for
data selection for pre-trained language model fine-tuning under cold-start
scenarios, i.e., no initial labeled data are available. In PATRON, we design
(1) a prompt-based uncertainty propagation approach to estimate the importance
of data points and (2) a partition-then-rewrite (PTR) strategy to promote
sample diversity when querying for annotations. Experiments on six text
classification datasets show that PATRON outperforms the strongest cold-start
data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON
achieves 91.0% and 92.1% of the fully supervised performance based on vanilla
fine-tuning and prompt-based learning respectively. Our implementation of
PATRON is available at \url{https://github.com/yueyu1030/Patron}.",0,1,0,0,1,0,0.753056,4.0,0.681507,78
58275832-143a-403a-b0ee-1a72b7787e51,High-resolution Face Swapping via Latent Semantics Disentanglement,49,0.55953,0.865615,"We present a novel high-resolution face swapping method using the inherent
prior knowledge of a pre-trained GAN model. Although previous research can
leverage generative priors to produce high-resolution results, their quality
can suffer from the entangled semantics of the latent space. We explicitly
disentangle the latent semantics by utilizing the progressive nature of the
generator, deriving structure attributes from the shallow layers and appearance
attributes from the deeper ones. Identity and pose information within the
structure attributes are further separated by introducing a landmark-driven
structure transfer latent direction. The disentangled latent code produces rich
generative features that incorporate feature blending to produce a plausible
swapping result. We further extend our method to video face swapping by
enforcing two spatio-temporal constraints on the latent space and the image
space. Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art image/video face swapping methods in terms of hallucination
quality and consistency. Code can be found at:
https://github.com/cnnlstm/FSLSD_HiRes.",1,1,0,0,1,0,0.898898,7.0,0.888546,54
8e7576a2-1147-4daa-bd45-29efbc5704de,Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning,17,0.141702,0.691008,"In this paper, we elaborate upon recipes for building multilingual
representation models that are not only competitive with existing
state-of-the-art models but are also more parameter efficient, thereby
promoting better adoption in resource-constrained scenarios and practical
applications. We show that going beyond English-centric bitexts, coupled with a
novel sampling strategy aimed at reducing under-utilization of training data,
substantially boosts performance across model sizes for both Electra and MLM
pre-training objectives. We introduce XY-LENT: X-Y bitext enhanced Language
ENcodings using Transformers which not only achieves state-of-the-art
performance over 5 cross-lingual tasks within all model size bands, is also
competitive across bands. Our XY-LENT XL variant outperforms XLM-RXXL and
exhibits competitive performance with mT5 XXL while being 5x and 6x smaller
respectively. We then show that our proposed method helps ameliorate the curse
of multilinguality, with the XY-LENT XL achieving 99.3% GLUE performance and
98.5% SQuAD 2.0 performance compared to a SoTA English only model in the same
size band. We then analyze our models performance on extremely low resource
languages and posit that scaling alone may not be sufficient for improving the
performance in this scenario",0,1,1,1,1,0,0.797423,7.0,0.836923,52
1a4345b3-81ac-4ad3-b80b-6c08fc49d41d,Modeling Image Composition for Complex Scene Generation,36,0.278607,0.466347,"We present a method that achieves state-of-the-art results on challenging
(few-shot) layout-to-image generation tasks by accurately modeling textures,
structures and relationships contained in a complex scene. After compressing
RGB images into patch tokens, we propose the Transformer with Focal Attention
(TwFA) for exploring dependencies of object-to-object, object-to-patch and
patch-to-patch. Compared to existing CNN-based and Transformer-based generation
models that entangled modeling on pixel-level&patch-level and
object-level&patch-level respectively, the proposed focal attention predicts
the current patch token by only focusing on its highly-related tokens that
specified by the spatial layout, thereby achieving disambiguation during
training. Furthermore, the proposed TwFA largely increases the data efficiency
during training, therefore we propose the first few-shot complex scene
generation strategy based on the well-trained TwFA. Comprehensive experiments
show the superiority of our method, which significantly increases both
quantitative metrics and qualitative visual realism with respect to
state-of-the-art CNN-based and transformer-based methods. Code is available at
https://github.com/JohnDreamer/TwFA.",1,0,0,0,1,0,0.639306,8.0,0.801262,85
135155b2-74a0-45aa-9ad8-a700769bc8d1,Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval,7,0.329827,0.578017,"Multi-document summarization (MDS) assumes a set of topic-related documents
are provided as input. In practice, this document set is not always available;
it would need to be retrieved given an information need, i.e. a question or
topic statement, a setting we dub ""open-domain"" MDS. We study this more
challenging setting by formalizing the task and bootstrapping it using existing
datasets, retrievers and summarizers. Via extensive automatic and human
evaluation, we determine: (1) state-of-the-art summarizers suffer large
reductions in performance when applied to open-domain MDS, (2) additional
training in the open-domain setting can reduce this sensitivity to imperfect
retrieval, and (3) summarizers are insensitive to the retrieval of duplicate
documents and the order of retrieved documents, but highly sensitive to other
errors, like the retrieval of irrelevant documents. Based on our results, we
provide practical guidelines to enable future work on open-domain MDS, e.g. how
to choose the number of retrieved documents to summarize. Our results suggest
that new retrieval and summarization methods and annotated resources for
training and evaluation are necessary for further progress in the open-domain
setting.",0,1,1,0,0,0,0.975202,6.0,0.949681,72
7e9b4b62-908c-442e-8cb2-8a168708b620,Bi-Link: Bridging Inductive Link Predictions from Text via Contrastive Learning of Transformers and Prompts,1,0.0184469,0.157908,"Inductive knowledge graph completion requires models to comprehend the
underlying semantics and logic patterns of relations. With the advance of
pretrained language models, recent research have designed transformers for link
prediction tasks. However, empirical studies show that linearizing triples
affects the learning of relational patterns, such as inversion and symmetry. In
this paper, we propose Bi-Link, a contrastive learning framework with
probabilistic syntax prompts for link predictions. Using grammatical knowledge
of BERT, we efficiently search for relational prompts according to learnt
syntactical patterns that generalize to large knowledge graphs. To better
express symmetric relations, we design a symmetric link prediction model,
establishing bidirectional linking between forward prediction and backward
prediction. This bidirectional linking accommodates flexible self-ensemble
strategies at test time. In our experiments, Bi-Link outperforms recent
baselines on link prediction datasets (WN18RR, FB15K-237, and Wikidata5M).
Furthermore, we construct Zeshel-Ind as an in-domain inductive entity linking
the environment to evaluate Bi-Link. The experimental results demonstrate that
our method yields robust representations which can generalize under domain
shift.",1,0,0,0,0,0,0.952186,7.0,0.928938,40
304708e7-16bb-4968-96f0-753c9a704ddf,Calibration of Machine Reading Systems at Scale,8,0.0,0.330954,"In typical machine learning systems, an estimate of the probability of the
prediction is used to assess the system's confidence in the prediction. This
confidence measure is usually uncalibrated; i.e.\ the system's confidence in
the prediction does not match the true probability of the predicted output. In
this paper, we present an investigation into calibrating open setting machine
reading systems such as open-domain question answering and claim verification
systems. We show that calibrating such complex systems which contain discrete
retrieval and deep reading components is challenging and current calibration
techniques fail to scale to these settings. We propose simple extensions to
existing calibration approaches that allows us to adapt them to these settings.
Our experimental results reveal that the approach works well, and can be useful
to selectively predict answers when question answering systems are posed with
unanswerable or out-of-the-training distribution questions.",0,1,0,0,0,0,0.23311,7.0,0.580584,39
0eb3d49f-40ec-4298-9cae-06fcfdbc20ec,Learning to Generate Novel Classes for Deep Metric Learning,1,0.0,0.0202197,"Deep metric learning aims to learn an embedding space where the distance
between data reflects their class equivalence, even when their classes are
unseen during training. However, the limited number of classes available in
training precludes generalization of the learned embedding space. Motivated by
this, we introduce a new data augmentation approach that synthesizes novel
classes and their embedding vectors. Our approach can provide rich semantic
information to an embedding model and improve its generalization by augmenting
training data with novel classes unavailable in the original data. We implement
this idea by learning and exploiting a conditional generative model, which,
given a class label and a noise, produces a random embedding vector of the
class. Our proposed generator allows the loss to use richer class relations by
augmenting realistic and diverse classes, resulting in better generalization to
unseen samples. Experimental results on public benchmark datasets demonstrate
that our method clearly enhances the performance of proxy-based losses.",0,1,1,1,0,0,0.75186,8.0,0.840321,40
cfce1c54-541f-401a-9fe7-55c7ff93f4ec,Privacy-Preserving Image Classification Using Vision Transformer,15,0.261631,0.864702,"In this paper, we propose a privacy-preserving image classification method
that is based on the combined use of encrypted images and the vision
transformer (ViT). The proposed method allows us not only to apply images
without visual information to ViT models for both training and testing but to
also maintain a high classification accuracy. ViT utilizes patch embedding and
position embedding for image patches, so this architecture is shown to reduce
the influence of block-wise image transformation. In an experiment, the
proposed method for privacy-preserving image classification is demonstrated to
outperform state-of-the-art methods in terms of classification accuracy and
robustness against various attacks.",0,1,0,0,1,0,0.40888,9.0,0.749734,32
abef5d5d-9f00-4569-a289-f38c90ebd205,WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection,12,0.220796,0.451848,"Monocular 3D object detection is one of the most challenging tasks in 3D
scene understanding. Due to the ill-posed nature of monocular imagery, existing
monocular 3D detection methods highly rely on training with the manually
annotated 3D box labels on the LiDAR point clouds. This annotation process is
very laborious and expensive. To dispense with the reliance on 3D box labels,
in this paper we explore the weakly supervised monocular 3D detection.
Specifically, we first detect 2D boxes on the image. Then, we adopt the
generated 2D boxes to select corresponding RoI LiDAR points as the weak
supervision. Eventually, we adopt a network to predict 3D boxes which can
tightly align with associated RoI LiDAR points. This network is learned by
minimizing our newly-proposed 3D alignment loss between the 3D box estimates
and the corresponding RoI LiDAR points. We will illustrate the potential
challenges of the above learning problem and resolve these challenges by
introducing several effective designs into our method. Codes will be available
at https://github.com/SPengLiang/WeakM3D.",1,1,1,0,0,0,0.966924,7.0,0.945281,48
00d9c24c-581f-40bc-95d8-ab3c18adaac9,IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty,20,0.398847,0.546644,"Single image surface normal estimation and depth estimation are closely
related problems as the former can be calculated from the latter. However, the
surface normals computed from the output of depth estimation methods are
significantly less accurate than the surface normals directly estimated by
networks. To reduce such discrepancy, we introduce a novel framework that uses
surface normal and its uncertainty to recurrently refine the predicted
depth-map. The depth of each pixel can be propagated to a query pixel, using
the predicted surface normal as guidance. We thus formulate depth refinement as
a classification of choosing the neighboring pixel to propagate from. Then, by
propagating to sub-pixel points, we upsample the refined, low-resolution
output. The proposed method shows state-of-the-art performance on NYUv2 and
iBims-1 - both in terms of depth and normal. Our refinement module can also be
attached to the existing depth estimation methods to improve their accuracy. We
also show that our framework, only trained for depth estimation, can also be
used for depth completion. The code is available at
https://github.com/baegwangbin/IronDepth.",1,1,0,0,1,0,0.891451,9.0,0.909813,37
a4de530c-3537-4b51-a86b-bad1f8a1f172,Universal Representations: A Unified Look at Multiple Task and Domain Learning,19,0.142885,0.31916,"We propose a unified look at jointly learning multiple vision tasks and
visual domains through universal representations, a single deep neural network.
Learning multiple problems simultaneously involves minimizing a weighted sum of
multiple loss functions with different magnitudes and characteristics and thus
results in unbalanced state of one loss dominating the optimization and poor
results compared to learning a separate model for each problem. To this end, we
propose distilling knowledge of multiple task/domain-specific networks into a
single deep neural network after aligning its representations with the
task/domain-specific ones through small capacity adapters. We rigorously show
that universal representations achieve state-of-the-art performances in
learning of multiple dense prediction problems in NYU-v2 and Cityscapes,
multiple image classification problems from diverse domains in Visual Decathlon
Dataset and cross-domain few-shot learning in MetaDataset. Finally we also
conduct multiple analysis through ablation and qualitative studies.",1,1,0,0,1,1,0.697743,9.0,0.841105,118
4c1c2465-eedb-44a0-b437-0f5490b6735b,Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning,12,0.552901,0.720484,"Recent research shows synthetic data as a source of supervision helps
pretrained language models (PLM) transfer learning to new target tasks/domains.
However, this idea is less explored for spatial language. We provide two new
data resources on multiple spatial language processing tasks. The first dataset
is synthesized for transfer learning on spatial question answering (SQA) and
spatial role labeling (SpRL). Compared to previous SQA datasets, we include a
larger variety of spatial relation types and spatial expressions. Our data
generation process is easily extendable with new spatial expression lexicons.
The second one is a real-world SQA dataset with human-generated questions built
on an existing corpus with SPRL annotations. This dataset can be used to
evaluate spatial language processing models in realistic situations. We show
pretraining with automatically generated data significantly improves the SOTA
results on several SQA and SPRL benchmarks, particularly when the training data
in the target domain is small.",0,1,1,1,0,0,0.485512,7.0,0.711709,51
919e28d5-018b-4a41-9a07-e26f06e7ea71,Mental Disorders on Online Social Media Through the Lens of Language and Behaviour: Analysis and Visualisation,19,0.324046,0.921757,"Due to the worldwide accessibility to the Internet along with the continuous
advances in mobile technologies, physical and digital worlds have become
completely blended, and the proliferation of social media platforms has taken a
leading role over this evolution. In this paper, we undertake a thorough
analysis towards better visualising and understanding the factors that
characterise and differentiate social media users affected by mental disorders.
We perform different experiments studying multiple dimensions of language,
including vocabulary uniqueness, word usage, linguistic style, psychometric
attributes, emotions' co-occurrence patterns, and online behavioural traits,
including social engagement and posting trends. Our findings reveal significant
differences on the use of function words, such as adverbs and verb tense, and
topic-specific vocabulary, such as biological processes. As for emotional
expression, we observe that affected users tend to share emotions more
regularly than control individuals on average. Overall, the monthly posting
variance of the affected groups is higher than the control groups. Moreover, we
found evidence suggesting that language use on micro-blogging platforms is less
distinguishable for users who have a mental disorder than other less
restrictive platforms. In particular, we observe on Twitter less quantifiable
differences between affected and control groups compared to Reddit.",0,0,0,0,0,0,0.414459,9.0,0.75172,63
094a2ff3-8d3c-49e7-bf1b-209f2e486f44,Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation,5,0.213649,0.832176,"Sign language gloss translation aims to translate the sign glosses into
spoken language texts, which is challenging due to the scarcity of labeled
gloss-text parallel data. Back translation (BT), which generates
pseudo-parallel data by translating in-domain spoken language texts into sign
glosses, has been applied to alleviate the data scarcity problem. However, the
lack of large-scale high-quality domain spoken language text data limits the
effect of BT. In this paper, to overcome the limitation, we propose a Prompt
based domain text Generation (PGEN) approach to produce the large-scale
in-domain spoken language text data. Specifically, PGEN randomly concatenates
sentences from the original in-domain spoken language text data as prompts to
induce a pre-trained language model (i.e., GPT-2) to generate spoken language
texts in a similar style. Experimental results on three benchmarks of sign
language gloss translation in varied languages demonstrate that BT with spoken
language texts generated by PGEN significantly outperforms the compared
methods. In addition, as the scale of spoken language texts generated by PGEN
increases, the BT technique can achieve further improvements, demonstrating the
effectiveness of our approach. We release the code and data for facilitating
future research in this field.",0,1,0,1,0,0,0.946778,6.0,0.911114,48
7be0cd1c-aa0d-4dd0-8925-84cb6554ca70,Biasing Like Human: A Cognitive Bias Framework for Scene Graph Generation,1,0.0276119,0.0466027,"Scene graph generation is a sophisticated task because there is no specific
recognition pattern (e.g., ""looking at"" and ""near"" have no conspicuous
difference concerning vision, whereas ""near"" could occur between entities with
different morphology). Thus some scene graph generation methods are trapped
into most frequent relation predictions caused by capricious visual features
and trivial dataset annotations. Therefore, recent works emphasized the
""unbiased"" approaches to balance predictions for a more informative scene
graph. However, human's quick and accurate judgments over relations between
numerous objects should be attributed to ""bias"" (i.e., experience and
linguistic knowledge) rather than pure vision. To enhance the model capability,
inspired by the ""cognitive bias"" mechanism, we propose a novel 3-paradigms
framework that simulates how humans incorporate the label linguistic features
as guidance of vision-based representations to better mine hidden relation
patterns and alleviate noisy visual propagation. Our framework is
model-agnostic to any scene graph model. Comprehensive experiments prove our
framework outperforms baseline modules in several metrics with minimum
parameters increment and achieves new SOTA performance on Visual Genome
dataset.",0,0,0,0,1,0,0.950943,7.0,0.927727,41
61ba05ca-28bf-4281-ac51-f69f64b927b9,Feasibility of Inconspicuous GAN-generated Adversarial Patches against Object Detection,9,0.0546763,0.558144,"Standard approaches for adversarial patch generation lead to noisy
conspicuous patterns, which are easily recognizable by humans. Recent research
has proposed several approaches to generate naturalistic patches using
generative adversarial networks (GANs), yet only a few of them were evaluated
on the object detection use case. Moreover, the state of the art mostly focuses
on suppressing a single large bounding box in input by overlapping it with the
patch directly. Suppressing objects near the patch is a different, more complex
task. In this work, we have evaluated the existing approaches to generate
inconspicuous patches. We have adapted methods, originally developed for
different computer vision tasks, to the object detection use case with YOLOv3
and the COCO dataset. We have evaluated two approaches to generate naturalistic
patches: by incorporating patch generation into the GAN training process and by
using the pretrained GAN. For both cases, we have assessed a trade-off between
performance and naturalistic patch appearance. Our experiments have shown, that
using a pre-trained GAN helps to gain realistic-looking patches while
preserving the performance similar to conventional adversarial patches.",1,1,0,0,0,0,0.344236,9.0,0.725301,31
7f4dc28b-4cba-47d4-a545-15df9b050617,Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning,36,0.388153,0.546475,"Our goal is a question-answering (QA) system that can show how its answers
are implied by its own internal beliefs via a systematic chain of reasoning.
Such a capability would allow better understanding of why a model produced the
answer it did. Our approach is to recursively combine a trained
backward-chaining model, capable of generating a set of premises entailing an
answer hypothesis, with a verifier that checks that the model itself believes
those premises (and the entailment itself) through self-querying. To our
knowledge, this is the first system to generate multistep chains that are both
faithful (the answer follows from the reasoning) and truthful (the chain
reflects the system's own internal beliefs). In evaluation using two different
datasets, users judge that a majority (70%+) of generated chains clearly show
how an answer follows from a set of facts - substantially better than a
high-performance baseline - while preserving answer accuracy. By materializing
model beliefs that systematically support an answer, new opportunities arise
for understanding the model's system of belief, and diagnosing and correcting
its misunderstandings when an answer is wrong.",0,0,0,0,0,0,0.816381,4.0,0.729546,43
aa1ae370-0a83-4218-9629-503b8065e616,Generalization in Automated Process Discovery: A Framework based on Event Log Patterns,1,0.00371715,0.0684373,"The importance of quality measures in process mining has increased. One of
the key quality aspects, generalization, is concerned with measuring the degree
of overfitting of a process model w.r.t. an event log, since the recorded
behavior is just an example of the true behavior of the underlying business
process. Existing generalization measures exhibit several shortcomings that
severely hinder their applicability in practice. For example, they assume the
event log fully fits the discovered process model, and cannot deal with large
real-life event logs and complex process models. More significantly, current
measures neglect generalizations for clear patterns that demand a certain
construct in the model. For example, a repeating sequence in an event log
should be generalized with a loop structure in the model. We address these
shortcomings by proposing a framework of measures that generalize a set of
patterns discovered from an event log with representative traces and check the
corresponding control-flow structures in the process model via their trace
alignment. We instantiate the framework with a generalization measure that uses
tandem repeats to identify repetitive patterns that are compared to the loop
structures and a concurrency oracle to identify concurrent patterns that are
compared to the parallel structures of the process model. In an extensive
qualitative and quantitative evaluation using 74 log-model pairs using against
two baseline generalization measures, we show that the proposed generalization
measure consistently ranks process models that fulfil the observed patterns
with generalizing control-flow structures higher than those which do not, while
the baseline measures disregard those patterns. Further, we show that our
measure can be efficiently computed for datasets two orders of magnitude larger
than the largest dataset the baseline generalization measures can handle.",0,1,0,0,0,0,0.00122517,16.0,0.480406,42
18c56e3e-08d2-4952-9082-0a0b972336f6,Detection of road traffic crashes based on collision estimation,2,0.0640826,0.374645,"This paper introduces a framework based on computer vision that can detect
road traffic crashes (RCTs) by using the installed surveillance/CCTV camera and
report them to the emergency in real-time with the exact location and time of
occurrence of the accident. The framework is built of five modules. We start
with the detection of vehicles by using YOLO architecture; The second module is
the tracking of vehicles using MOSSE tracker, Then the third module is a new
approach to detect accidents based on collision estimation. Then the fourth
module for each vehicle, we detect if there is a car accident or not based on
the violent flow descriptor (ViF) followed by an SVM classifier for crash
prediction. Finally, in the last stage, if there is a car accident, the system
will send a notification to the emergency by using a GPS module that provides
us with the location, time, and date of the accident to be sent to the
emergency with the help of the GSM module. The main objective is to achieve
higher accuracy with fewer false alarms and to implement a simple system based
on pipelining technique.",0,1,0,0,0,0,0.661374,8.0,0.808771,14
303385d4-7c00-4ac3-ada0-7012d15f8ea8,Boosting Entity Mention Detection for Targetted Twitter Streams with Global Contextual Embeddings,1,0.0135276,0.0931433,"Microblogging sites, like Twitter, have emerged as ubiquitous sources of
information. Two important tasks related to the automatic extraction and
analysis of information in Microblogs are Entity Mention Detection (EMD) and
Entity Detection (ED). The state-of-the-art EMD systems aim to model the
non-literary nature of microblog text by training upon offline static datasets.
They extract a combination of surface-level features -- orthographic, lexical,
and semantic -- from individual messages for noisy text modeling and entity
extraction. But given the constantly evolving nature of microblog streams,
detecting all entity mentions from such varying yet limited context of short
messages remains a difficult problem. To this end, we propose a framework named
EMD Globalizer, better suited for the execution of EMD learners on microblog
streams. It deviates from the processing of isolated microblog messages by
existing EMD systems, where learned knowledge from the immediate context of a
message is used to suggest entities. After an initial extraction of entity
candidates by an EMD system, the proposed framework leverages occurrence mining
to find additional candidate mentions that are missed during this first
detection. Aggregating the local contextual representations of these mentions,
a global embedding is drawn from the collective context of an entity candidate
within a stream. The global embeddings are then utilized to separate entities
within the candidates from false positives. All mentions of said entities from
the stream are produced in the framework's final outputs. Our experiments show
that EMD Globalizer can enhance the effectiveness of all existing EMD systems
that we tested (on average by 25.61%) with a small additional computational
overhead.",1,1,0,0,0,0,0.138379,11.0,0.680577,51
0efbf51f-a4d6-4127-b365-68b56913211a,Deep Reinforcement Learning for Exact Combinatorial Optimization: Learning to Branch,8,0.0212765,0.592686,"Branch-and-bound is a systematic enumerative method for combinatorial
optimization, where the performance highly relies on the variable selection
strategy. State-of-the-art handcrafted heuristic strategies suffer from
relatively slow inference time for each selection, while the current machine
learning methods require a significant amount of labeled data. We propose a new
approach for solving the data labeling and inference latency issues in
combinatorial optimization based on the use of the reinforcement learning (RL)
paradigm. We use imitation learning to bootstrap an RL agent and then use
Proximal Policy Optimization (PPO) to further explore global optimal actions.
Then, a value network is used to run Monte-Carlo tree search (MCTS) to enhance
the policy network. We evaluate the performance of our method on four different
categories of combinatorial optimization problems and show that our approach
performs strongly compared to the state-of-the-art machine learning and
heuristics based methods.",0,1,0,0,1,0,0.00886757,13.0,0.513051,39
21817089-4764-453b-b838-8bb7af6e7cdf,"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",28,0.0521652,0.546409,"We introduce Mintaka, a complex, natural, and multilingual dataset designed
for experimenting with end-to-end question-answering models. Mintaka is
composed of 20,000 question-answer pairs collected in English, annotated with
Wikidata entities, and translated into Arabic, French, German, Hindi, Italian,
Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka
includes 8 types of complex questions, including superlative, intersection, and
multi-hop questions, which were naturally elicited from crowd workers. We run
baselines over Mintaka, the best of which achieves 38% hits@1 in English and
31% hits@1 multilingually, showing that existing models have room for
improvement. We release Mintaka at https://github.com/amazon-research/mintaka.",1,0,1,1,0,0,0.31159,6.0,0.567563,39
4d5d64ed-953e-418a-86f0-77ad928dc403,A Psycho-linguistic Analysis of BitChute,2,0.0174857,0.288022,"In order to better support researchers, journalist, and practitioners in
their use of the MeLa-BitChute dataset for exploration and investigative
reporting, we provide new psycho-linguistic metadata for the videos, comments,
and channels in the dataset using LIWC22. This paper describes that metadata
and methods to filter the data using the metadata. In addition, we provide
basic analysis and comparison of the language on BitChute to other social media
platforms. The MeLa-BitChute dataset and LIWC metadata described in this paper
can be found at:
https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRD1VS.",1,1,0,1,0,0,0.121556,11.0,0.667935,29
ee9d81ee-c89d-4a3c-8d00-a7341f1076b3,Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,11,0.118283,0.317675,"Recent research has revealed that neural language models at scale suffer from
poor temporal generalization capability, i.e., the language model pre-trained
on static data from past years performs worse over time on emerging data.
Existing methods mainly perform continual training to mitigate such a
misalignment. While effective to some extent but is far from being addressed on
both the language modeling and downstream tasks. In this paper, we empirically
observe that temporal generalization is closely affiliated with lexical
semantic change, which is one of the essential phenomena of natural languages.
Based on this observation, we propose a simple yet effective lexical-level
masking strategy to post-train a converged language model. Experiments on two
pre-trained language models, two different classification tasks, and four
benchmark datasets demonstrate the effectiveness of our proposed method over
existing temporal adaptation methods, i.e., continual training with new data.
Our code is available at \url{https://github.com/zhaochen0110/LMLM}.",1,1,0,0,0,0,0.244384,9.0,0.679821,52
5cc5aec8-1134-4904-bc57-99fdc1fd23b0,Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training,2,0.102268,0.15688,"In this paper, we present a cross-modal recipe retrieval framework,
Transformer-based Network for Large Batch Training (TNLBT), which is inspired
by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).
TNLBT aims to accomplish retrieval tasks while generating images from recipe
embeddings. We apply the Hierarchical Transformer-based recipe text encoder,
the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial
network architecture to enable better cross-modal embedding learning for recipe
texts and images. In addition, we use self-supervised learning to exploit the
rich information in the recipe texts having no corresponding images. Since
contrastive learning could benefit from a larger batch size according to the
recent literature on self-supervised learning, we adopt a large batch size
during training and have validated its effectiveness. In the experiments, the
proposed framework significantly outperformed the current state-of-the-art
frameworks in both cross-modal recipe retrieval and image generation tasks on
the benchmark Recipe1M. This is the first work which confirmed the
effectiveness of large batch training on cross-modal recipe embeddings.",0,1,0,0,1,0,0.763993,8.0,0.844738,36
70cfefb9-e195-4e48-9ce7-d858c25461a1,LongShortNet: Exploring Temporal and Semantic Features Fusion in Streaming Perception,8,0.128167,0.319893,"Streaming perception is a critical task in autonomous driving that requires
balancing the latency and accuracy of the autopilot system. However, current
methods for streaming perception are limited as they only rely on the current
and adjacent two frames to learn movement patterns. This restricts their
ability to model complex scenes, often resulting in poor detection results. To
address this limitation, we propose LongShortNet, a novel dual-path network
that captures long-term temporal motion and integrates it with short-term
spatial semantics for real-time perception. LongShortNet is notable as it is
the first work to extend long-term temporal modeling to streaming perception,
enabling spatiotemporal feature fusion. We evaluate LongShortNet on the
challenging Argoverse-HD dataset and demonstrate that it outperforms existing
state-of-the-art methods with almost no additional computational cost.",1,1,1,0,1,0,0.45862,6.0,0.650368,40
02bb0a51-df73-4eb4-ac85-deb57070d34f,ConvMAE: Masked Convolution Meets Masked Autoencoders,87,0.109602,0.768117,"Vision Transformers (ViT) become widely-adopted architectures for various
vision tasks. Masked auto-encoding for feature pretraining and multi-scale
hybrid convolution-transformer architectures can further unleash the potentials
of ViT, leading to state-of-the-art performances on image classification,
detection and semantic segmentation. In this paper, our ConvMAE framework
demonstrates that multi-scale hybrid convolution-transformer can learn more
discriminative representations via the mask auto-encoding scheme. However,
directly using the original masking strategy leads to the heavy computational
cost and pretraining-finetuning discrepancy. To tackle the issue, we adopt the
masked convolution to prevent information leakage in the convolution blocks. A
simple block-wise masking strategy is proposed to ensure computational
efficiency. We also propose to more directly supervise the multi-scale features
of the encoder to boost multi-scale features. Based on our pretrained ConvMAE
models, ConvMAE-Base improves ImageNet-1K finetuning accuracy by 1.4% compared
with MAE-Base. On object detection, ConvMAE-Base finetuned for only 25 epochs
surpasses MAE-Base fined-tuned for 100 epochs by 2.9% box AP and 2.2% mask AP
respectively. Code and pretrained models are available at
https://github.com/Alpha-VL/ConvMAE.",1,1,0,0,1,0,0.879902,3.0,0.713905,76
2e50db5f-23dc-4ec3-a239-5e2f604deb1c,Event Collapse in Contrast Maximization Frameworks,17,0.387387,0.560296,"Contrast maximization (CMax) is a framework that provides state-of-the-art
results on several event-based computer vision tasks, such as ego-motion or
optical flow estimation. However, it may suffer from a problem called event
collapse, which is an undesired solution where events are warped into too few
pixels. As prior works have largely ignored the issue or proposed workarounds,
it is imperative to analyze this phenomenon in detail. Our work demonstrates
event collapse in its simplest form and proposes collapse metrics by using
first principles of space-time deformation based on differential geometry and
physics. We experimentally show on publicly available datasets that the
proposed metrics mitigate event collapse and do not harm well-posed warps. To
the best of our knowledge, regularizers based on the proposed metrics are the
only effective solution against event collapse in the experimental settings
considered, compared with other methods. We hope that this work inspires
further research to tackle more complex warp models.",0,0,1,0,0,0,0.62131,7.0,0.765882,57
b7164889-f9ae-4ceb-a32c-898c5c644784,Vision-based Large-scale 3D Semantic Mapping for Autonomous Driving Applications,3,0.0656743,0.126535,"In this paper, we present a complete pipeline for 3D semantic mapping solely
based on a stereo camera system. The pipeline comprises a direct sparse visual
odometry front-end as well as a back-end for global optimization including GNSS
integration, and semantic 3D point cloud labeling. We propose a simple but
effective temporal voting scheme which improves the quality and consistency of
the 3D point labels. Qualitative and quantitative evaluations of our pipeline
are performed on the KITTI-360 dataset. The results show the effectiveness of
our proposed voting scheme and the capability of our pipeline for efficient
large-scale 3D semantic mapping. The large-scale mapping capabilities of our
pipeline is furthermore demonstrated by presenting a very large-scale semantic
map covering 8000 km of roads generated from data collected by a fleet of
vehicles.",0,1,0,0,0,0,0.936355,11.0,0.945798,63
43760f44-cb2a-4ca1-87e9-ae37cc28afe3,Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions,3,0.245986,0.390827,"We explore zero-shot adaptation, where a general-domain model has access to
customer or domain specific parallel data at inference time, but not during
training. We build on the idea of Retrieval Augmented Translation (RAT) where
top-k in-domain fuzzy matches are found for the source sentence, and
target-language translations of those fuzzy-matched sentences are provided to
the translation model at inference time. We propose a novel architecture to
control interactions between a source sentence and the top-k fuzzy
target-language matches, and compare it to architectures from prior work. We
conduct experiments in two language pairs (En-De and En-Fr) by training models
on WMT data and testing them with five and seven multi-domain datasets,
respectively. Our approach consistently outperforms the alternative
architectures, improving BLEU across language pair, domain, and number k of
fuzzy matches.",0,1,0,0,0,0,0.946414,9.0,0.940484,37
2fe35009-1ceb-433d-864a-af08ae7e4a2d,Specializing Multi-domain NMT via Penalizing Low Mutual Information,2,0.0664979,0.113282,"Multi-domain Neural Machine Translation (NMT) trains a single model with
multiple domains. It is appealing because of its efficacy in handling multiple
domains within one model. An ideal multi-domain NMT should learn distinctive
domain characteristics simultaneously, however, grasping the domain peculiarity
is a non-trivial task. In this paper, we investigate domain-specific
information through the lens of mutual information (MI) and propose a new
objective that penalizes low MI to become higher. Our method achieved the
state-of-the-art performance among the current competitive multi-domain NMT
models. Also, we empirically show our objective promotes low MI to be higher
resulting in domain-specialized multi-domain NMT.",0,1,0,0,1,0,0.39602,10.0,0.770581,26
8ce7dc71-7b86-47a2-aceb-9bca1e67cb69,MPA: MultiPath++ Based Architecture for Motion Prediction,11,0.0757725,0.668082,"Autonomous driving technology is developing rapidly and nowadays first
autonomous rides are being provided in city areas. This requires the highest
standards for the safety and reliability of the technology. Motion prediction
part of the general self-driving pipeline plays a crucial role in providing
these qualities. In this work we present one of the solutions for Waymo Motion
Prediction Challenge 2022 based on MultiPath++ ranked the 3rd as of May, 26
2022. Our source code is publicly available on GitHub.",0,1,0,0,0,0,0.627431,6.0,0.729636,18
47c747f1-7fb6-4cd4-9a6c-794db615dce0,CLINICAL: Targeted Active Learning for Imbalanced Medical Image Classification,5,0.0142295,0.26985,"Training deep learning models on medical datasets that perform well for all
classes is a challenging task. It is often the case that a suboptimal
performance is obtained on some classes due to the natural class imbalance
issue that comes with medical data. An effective way to tackle this problem is
by using targeted active learning, where we iteratively add data points to the
training data that belong to the rare classes. However, existing active
learning methods are ineffective in targeting rare classes in medical datasets.
In this work, we propose Clinical (targeted aCtive Learning for ImbalaNced
medICal imAge cLassification) a framework that uses submodular mutual
information functions as acquisition functions to mine critical data points
from rare classes. We apply our framework to a wide-array of medical imaging
datasets on a variety of real-world class imbalance scenarios - namely, binary
imbalance and long-tail imbalance. We show that Clinical outperforms the
state-of-the-art active learning methods by acquiring a diverse set of data
points that belong to the rare classes.",0,1,0,0,1,0,0.0903735,8.0,0.504218,31
f23cac59-328f-4753-b7d6-503788f08390,An Effective Deployment of Contrastive Learning in Multi-label Text Classification,1,0.00848114,0.0414001,"The effectiveness of contrastive learning technology in natural language
processing tasks is yet to be explored and analyzed. How to construct positive
and negative samples correctly and reasonably is the core challenge of
contrastive learning. It is even harder to discover contrastive objects in
multi-label text classification tasks. There are very few contrastive losses
proposed previously. In this paper, we investigate the problem from a different
angle by proposing five novel contrastive losses for multi-label text
classification tasks. These are Strict Contrastive Loss (SCL), Intra-label
Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard
Similarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive
Loss (SLCL). We explore the effectiveness of contrastive learning for
multi-label text classification tasks by the employment of these novel losses
and provide a set of baseline models for deploying contrastive learning
techniques on specific tasks. We further perform an interpretable analysis of
our approach to show how different components of contrastive learning losses
play their roles. The experimental results show that our proposed contrastive
losses can bring improvement to multi-label text classification tasks. Our work
also explores how contrastive learning should be adapted for multi-label text
classification tasks.",0,1,0,0,0,0,0.121385,5.0,0.269156,58
94babbc1-7e89-4dff-bb5e-65776f6cb809,"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",35,0.0666873,0.555441,"Human language offers a powerful window into our thoughts -- we tell stories,
give explanations, and express our beliefs and goals through words. Abundant
evidence also suggests that language plays a developmental role in structuring
our learning. Here, we ask: how much of human-like thinking can be captured by
learning statistical patterns in language alone? We first contribute a new
challenge benchmark for comparing humans and distributional large language
models (LLMs). Our benchmark contains two problem-solving domains (planning and
explanation generation) and is designed to require generalization to new,
out-of-distribution problems expressed in language. We find that humans are far
more robust than LLMs on this benchmark. Next, we propose a hybrid
Parse-and-Solve model, which augments distributional LLMs with a structured
symbolic reasoning module. We find that this model shows more robust adaptation
to out-of-distribution planning problems, demonstrating the promise of hybrid
AI models for more human-like reasoning.",1,0,0,0,0,0,0.74031,3.0,0.563127,18
1fa4bf14-cd38-4333-9053-a1b98c178ec0,Multi-task Learning for Cross-Lingual Sentiment Analysis,6,0.166554,0.764061,"This paper presents a cross-lingual sentiment analysis of news articles using
zero-shot and few-shot learning. The study aims to classify the Croatian news
articles with positive, negative, and neutral sentiments using the Slovene
dataset. The system is based on a trilingual BERT-based model trained in three
languages: English, Slovene, Croatian. The paper analyses different setups
using datasets in two languages and proposes a simple multi-task model to
perform sentiment classification. The evaluation is performed using the
few-shot and zero-shot scenarios in single-task and multi-task experiments for
Croatian and Slovene.",0,1,0,0,0,0,0.386913,16.0,0.85473,23
ff2fc500-04a9-4ef9-97d6-fcf308406251,"Entities, Dates, and Languages: Zero-Shot on Historical Texts with T0",6,0.0584953,0.623256,"In this work, we explore whether the recently demonstrated zero-shot
abilities of the T0 model extend to Named Entity Recognition for
out-of-distribution languages and time periods. Using a historical newspaper
corpus in 3 languages as test-bed, we use prompts to extract possible named
entities. Our results show that a naive approach for prompt-based zero-shot
multilingual Named Entity Recognition is error-prone, but highlights the
potential of such an approach for historical languages lacking labeled
datasets. Moreover, we also find that T0-like models can be probed to predict
the publication date and language of a document, which could be very relevant
for the study of historical texts.",0,1,0,0,0,0,0.708723,3.0,0.533467,31
0fdca95a-9972-4ec3-8ded-8dd41f6baf92,MMINR: Multi-frame-to-Multi-frame Inference with Noise Resistance for Precipitation Nowcasting with Radar,1,0.02003,0.138534,"Precipitation nowcasting based on radar echo maps is essential in
meteorological research. Recently, Convolutional RNNs based methods dominate
this field, but they cannot be solved by parallel computation resulting in
longer inference time. FCN based methods adopt a multi-frame-to-single-frame
inference (MSI) strategy to avoid this problem. They feedback into the model
again to predict the next time step to get multi-frame nowcasting results in
the prediction phase, which will lead to the accumulation of prediction errors.
In addition, precipitation noise is a crucial factor contributing to high
prediction errors because of its unpredictability. To address this problem, we
propose a novel Multi-frame-to-Multi-frame Inference (MMI) model with Noise
Resistance (NR) named MMINR. It avoids error accumulation and resists
precipitation noise\'s negative effect in parallel computation. NR contains a
Noise Dropout Module (NDM) and a Semantic Restore Module (SRM). NDM
deliberately dropout noise simple yet efficient, and SRM supplements semantic
information of features to alleviate the problem of semantic information
mistakenly lost by NDM. Experimental results demonstrate that MMINR can attain
competitive scores compared with other SOTAs. The ablation experiments show
that the proposed NDM and SRM can solve the aforementioned problems.",0,1,0,0,1,0,0.626274,9.0,0.819408,18
fbe81432-7eee-459b-8933-4872b40476e2,Exemplar-free Online Continual Learning,17,0.217419,0.752234,"Targeted for real world scenarios, online continual learning aims to learn
new tasks from sequentially available data under the condition that each data
is observed only once by the learner. Though recent works have made remarkable
achievements by storing part of learned task data as exemplars for knowledge
replay, the performance is greatly relied on the size of stored exemplars while
the storage consumption is a significant constraint in continual learning. In
addition, storing exemplars may not always be feasible for certain applications
due to privacy concerns. In this work, we propose a novel exemplar-free method
by leveraging nearest-class-mean (NCM) classifier where the class mean is
estimated during training phase on all data seen so far through online mean
update criteria. We focus on image classification task and conduct extensive
experiments on benchmark datasets including CIFAR-100 and Food-1k. The results
demonstrate that our method without using any exemplar outperforms
state-of-the-art exemplar-based approaches with large margins under standard
protocol (20 exemplars per class) and is able to achieve competitive
performance even with larger exemplar size (100 exemplars per class).",0,1,0,0,1,0,0.913539,7.0,0.89798,27
a8eca41d-1f58-4a77-9a1f-a4a2969c0896,Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs,6,0.186203,0.443671,"The extraction of aspect terms is a critical step in fine-grained sentiment
analysis of text. Existing approaches for this task have yielded impressive
results when the training and testing data are from the same domain. However,
these methods show a drastic decrease in performance when applied to
cross-domain settings where the domain of the testing data differs from that of
the training data. To address this lack of extensibility and robustness, we
propose a novel approach for automatically constructing domain-specific
knowledge graphs that contain information relevant to the identification of
aspect terms. We introduce a methodology for injecting information from these
knowledge graphs into Transformer models, including two alternative mechanisms
for knowledge insertion: via query enrichment and via manipulation of attention
patterns. We demonstrate state-of-the-art performance on benchmark datasets for
cross-domain aspect term extraction using our approach and investigate how the
amount of external knowledge available to the Transformer impacts model
performance.",0,1,0,0,1,0,0.849607,10.0,0.902952,45
ecabdc66-f64b-4b1c-ac58-9ce4a0386771,Discovering Transferable Forensic Features for CNN-generated Images Detection,21,0.320597,0.727816,"Visual counterfeits are increasingly causing an existential conundrum in
mainstream media with rapid evolution in neural image synthesis methods. Though
detection of such counterfeits has been a taxing problem in the image forensics
community, a recent class of forensic detectors -- universal detectors -- are
able to surprisingly spot counterfeit images regardless of generator
architectures, loss functions, training datasets, and resolutions. This
intriguing property suggests the possible existence of transferable forensic
features (T-FF) in universal detectors. In this work, we conduct the first
analytical study to discover and understand T-FF in universal detectors. Our
contributions are 2-fold: 1) We propose a novel forensic feature relevance
statistic (FF-RS) to quantify and discover T-FF in universal detectors and, 2)
Our qualitative and quantitative investigations uncover an unexpected finding:
color is a critical T-FF in universal detectors. Code and models are available
at https://keshik6.github.io/transferable-forensic-features/",1,0,0,0,0,0,0.925952,7.0,0.906749,104
07489216-6e1e-4f37-b658-a955f688a527,MagicPony: Learning Articulated 3D Animals in the Wild,36,0.552549,0.975782,"We consider the problem of predicting the 3D shape, articulation, viewpoint,
texture, and lighting of an articulated animal like a horse given a single test
image as input. We present a new method, dubbed MagicPony, that learns this
predictor purely from in-the-wild single-view images of the object category,
with minimal assumptions about the topology of deformation. At its core is an
implicit-explicit representation of articulated shape and appearance, combining
the strengths of neural fields and meshes. In order to help the model
understand an object's shape and pose, we distil the knowledge captured by an
off-the-shelf self-supervised vision transformer and fuse it into the 3D model.
To overcome local optima in viewpoint estimation, we further introduce a new
viewpoint sampling scheme that comes at no additional training cost. MagicPony
outperforms prior work on this challenging task and demonstrates excellent
generalisation in reconstructing art, despite the fact that it is only trained
on real images.",1,0,0,0,1,0,0.940448,5.0,0.885524,81
878aedf5-ca59-4dc6-87e4-7a26c4c7ac8e,Indian Commercial Truck License Plate Detection and Recognition for Weighbridge Automation,1,0.082599,0.335434,"Detection and recognition of a licence plate is important when automating
weighbridge services. While many large databases are available for Latin and
Chinese alphanumeric license plates, data for Indian License Plates is
inadequate. In particular, databases of Indian commercial truck license plates
are inadequate, despite the fact that commercial vehicle license plate
recognition plays a profound role in terms of logistics management and
weighbridge automation. Moreover, models to recognise license plates are not
effectively able to generalise to such data due to its challenging nature, and
due to the abundant frequency of handwritten license plates, leading to the
usage of diverse font styles. Thus, a database and effective models to
recognise and detect such license plates are crucial. This paper provides a
database on commercial truck license plates, and using state-of-the-art models
in real-time object Detection: You Only Look Once Version 7, and SceneText
Recognition: Permuted Autoregressive Sequence Models, our method outperforms
the other cited references where the maximum accuracy obtained was less than
90%, while we have achieved 95.82% accuracy in our algorithm implementation on
the presented challenging license plate dataset. Index Terms- Automatic License
Plate Recognition, character recognition, license plate detection, vision
transformer.",0,1,1,1,1,0,0.857088,4.0,0.764024,34
9a98bedc-4561-459e-9502-24cc1ffa8f57,A Deep Learning Anomaly Detection Method in Textual Data,4,0.0298047,0.206494,"In this article, we propose using deep learning and transformer architectures
combined with classical machine learning algorithms to detect and identify text
anomalies in texts. Deep learning model provides a very crucial context
information about the textual data which all textual context are converted to a
numerical representation. We used multiple machine learning methods such as
Sentence Transformers, Auto Encoders, Logistic Regression and Distance
calculation methods to predict anomalies. The method are tested on the texts
data and we used syntactic data from different source injected into the
original text as anomalies or use them as target. Different methods and
algorithm are explained in the field of outlier detection and the results of
the best technique is presented. These results suggest that our algorithm could
potentially reduce false positive rates compared with other anomaly detection
methods that we are testing.",0,1,0,0,0,0,0.00661092,20.0,0.668742,11
4ee88e78-a87d-4f75-9a75-e102c3874b50,CINO: A Chinese Minority Pre-trained Language Model,15,0.108769,0.237494,"Multilingual pre-trained language models have shown impressive performance on
cross-lingual tasks. It greatly facilitates the applications of natural
language processing on low-resource languages. However, there are still some
languages that the current multilingual models do not perform well on. In this
paper, we propose CINO (Chinese Minority Pre-trained Language Model), a
multilingual pre-trained language model for Chinese minority languages. It
covers Standard Chinese, Yue Chinese, and six other ethnic minority languages.
To evaluate the cross-lingual ability of the multilingual model on ethnic
minority languages, we collect documents from Wikipedia and news websites, and
construct two text classification datasets, WCM (Wiki-Chinese-Minority) and
CMNews (Chinese-Minority-News). We show that CINO notably outperforms the
baselines on various classification tasks. The CINO model and the datasets are
publicly available at http://cino.hfl-rc.com.",1,1,1,1,0,0,0.746108,6.0,0.784332,32
bae0eeec-9b7c-44b1-965d-327b7f889bad,A Transparency Index Framework for AI in Education,12,0.312501,0.469098,"Numerous AI ethics checklists and frameworks have been proposed focusing on
different dimensions of ethical AI such as fairness, explainability, and
safety. Yet, no such work has been done on developing transparent AI systems
for real-world educational scenarios. This paper presents a Transparency Index
framework that has been iteratively co-designed with different stakeholders of
AI in education, including educators, ed-tech experts, and AI practitioners. We
map the requirements of transparency for different categories of stakeholders
of AI in education and demonstrate that transparency considerations are
embedded in the entire AI development process from the data collection stage
until the AI system is deployed in the real world and iteratively improved. We
also demonstrate how transparency enables the implementation of other ethical
AI dimensions in Education like interpretability, accountability, and safety.
In conclusion, we discuss the directions for future research in this newly
emerging field. The main contribution of this study is that it highlights the
importance of transparency in developing AI-powered educational technologies
and proposes an index framework for its conceptualization for AI in education.",0,0,1,0,0,0,0.693546,8.0,0.819794,76
a05517a9-6413-41d5-a091-c14f68902879,Towards Abstractive Grounded Summarization of Podcast Transcripts,6,0.051567,0.472924,"Podcasts have recently shown a rapid rise in popularity. Summarization of
podcast transcripts is of practical benefit to both content providers and
consumers. It helps consumers to quickly decide whether they will listen to the
podcasts and reduces the cognitive load of content providers to write
summaries. Nevertheless, podcast summarization faces significant challenges
including factual inconsistencies with respect to the inputs. The problem is
exacerbated by speech disfluencies and recognition errors in transcripts of
spoken language. In this paper, we explore a novel abstractive summarization
method to alleviate these challenges. Specifically, our approach learns to
produce an abstractive summary while grounding summary segments in specific
portions of the transcript to allow for full inspection of summary details. We
conduct a series of analyses of the proposed approach on a large podcast
dataset and show that the approach can achieve promising results. Grounded
summaries bring clear benefits in locating the summary and transcript segments
that contain inconsistent information, and hence significantly improve
summarization quality in both automatic and human evaluation metrics.",1,1,0,0,0,0,0.595918,5.0,0.658399,52
fec7aba1-9b4a-48e8-9f2b-518113db5751,Class-Specific Semantic Reconstruction for Open Set Recognition,30,0.278447,0.76311,"Open set recognition enables deep neural networks (DNNs) to identify samples
of unknown classes, while maintaining high classification accuracy on samples
of known classes. Existing methods basing on auto-encoder (AE) and prototype
learning show great potential in handling this challenging task. In this study,
we propose a novel method, called Class-Specific Semantic Reconstruction
(CSSR), that integrates the power of AE and prototype learning. Specifically,
CSSR replaces prototype points with manifolds represented by class-specific
AEs. Unlike conventional prototype-based methods, CSSR models each known class
on an individual AE manifold, and measures class belongingness through AE's
reconstruction error. Class-specific AEs are plugged into the top of the DNN
backbone and reconstruct the semantic representations learned by the DNN
instead of the raw image. Through end-to-end learning, the DNN and the AEs
boost each other to learn both discriminative and representative information.
The results of experiments conducted on multiple datasets show that the
proposed method achieves outstanding performance in both close and open set
recognition and is sufficiently simple and flexible to incorporate into
existing frameworks.",0,1,0,0,0,0,0.813962,7.0,0.844347,53
5b5d6798-0d92-4ded-bdd3-9c2492f10096,SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues,26,0.785758,0.783454,"Dialogue systems are usually categorized into two types, open-domain and
task-oriented. The first one focuses on chatting with users and making them
engage in the conversations, where selecting a proper topic to fit the dialogue
context is essential for a successful dialogue. The other one focuses on a
specific task instead of casual talks, e.g., finding a movie on Friday night,
or playing a song. These two directions have been studied separately due to
their different purposes. However, how smoothly transitioning from social
chatting to task-oriented dialogues is important for triggering business
opportunities, and there is no public data focusing on such scenarios. Hence,
this paper focuses on investigating the conversations starting from open-domain
social chatting and then gradually transitioning to task-oriented purposes, and
releases a large-scale dataset with detailed annotations for encouraging this
research direction. To achieve this goal, this paper proposes a framework to
automatically generate many dialogues without human involvement, in which any
powerful open-domain dialogue generation model can be easily leveraged. The
human evaluation shows that our generated dialogue data has a natural flow at a
reasonable quality, showing that our released data has a great potential of
guiding future research directions and commercial activities. Furthermore, the
released models allow researchers to automatically generate unlimited dialogues
in the target scenarios, which can greatly benefit semi-supervised and
unsupervised approaches.",0,1,1,1,0,0,0.970941,6.0,0.942375,35
69b52b7e-0bad-4d5c-b212-3d5b2eb170e0,"F-coref: Fast, Accurate and Easy to Use Coreference Resolution",12,0.161072,0.843522,"We introduce fastcoref, a python package for fast, accurate, and easy-to-use
English coreference resolution. The package is pip-installable, and allows two
modes: an accurate mode based on the LingMess architecture, providing
state-of-the-art coreference accuracy, and a substantially faster model,
F-coref, which is the focus of this work. F-coref allows to process 2.8K
OntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the
LingMess model, and to 12 minutes of the popular AllenNLP coreference model)
with only a modest drop in accuracy. The fast speed is achieved through a
combination of distillation of a compact model from the LingMess model, and an
efficient batching implementation using a technique we call leftover batching.
Our code is available at https://github.com/shon-otmazgin/fastcoref",1,1,0,0,1,1,0.560714,7.0,0.742183,38
dadc71e5-9fcb-4457-a505-94d393e428b9,Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour,4,0.0169731,0.178424,"Language models (LMs) trained on raw texts have no direct access to the
physical world. Gordon and Van Durme (2013) point out that LMs can thus suffer
from reporting bias: texts rarely report on common facts, instead focusing on
the unusual aspects of a situation. If LMs are only trained on text corpora and
naively memorise local co-occurrence statistics, they thus naturally would
learn a biased view of the physical world. While prior studies have repeatedly
verified that LMs of smaller scales (e.g., RoBERTa, GPT-2) amplify reporting
bias, it remains unknown whether such trends continue when models are scaled
up. We investigate reporting bias from the perspective of colour in larger
language models (LLMs) such as PaLM and GPT-3. Specifically, we query LLMs for
the typical colour of objects, which is one simple type of perceptually
grounded physical common sense. Surprisingly, we find that LLMs significantly
outperform smaller LMs in determining an object's typical colour and more
closely track human judgments, instead of overfitting to surface patterns
stored in texts. This suggests that very large models of language alone are
able to overcome certain types of reporting bias that are characterized by
local co-occurrences.",0,0,0,0,0,0,0.75454,5.0,0.746066,22
d8e92c1e-19f3-4cc7-a626-9ad0cb51d889,Memory transformers for full context and high-resolution 3D Medical Segmentation,3,0.033757,0.25046,"Transformer models achieve state-of-the-art results for image segmentation.
However, achieving long-range attention, necessary to capture global context,
with high-resolution 3D images is a fundamental challenge. This paper
introduces the Full resolutIoN mEmory (FINE) transformer to overcome this
issue. The core idea behind FINE is to learn memory tokens to indirectly model
full range interactions while scaling well in both memory and computational
costs. FINE introduces memory tokens at two levels: the first one allows full
interaction between voxels within local image regions (patches), the second one
allows full interactions between all regions of the 3D volume. Combined, they
allow full attention over high resolution images, e.g. 512 x 512 x 256 voxels
and above. Experiments on the BCV image segmentation dataset shows better
performances than state-of-the-art CNN and transformer baselines, highlighting
the superiority of our full attention mechanism compared to recent transformer
baselines, e.g. CoTr, and nnFormer.",0,0,0,0,1,0,0.970733,4.0,0.913057,33
abe0bc67-381e-4b99-846e-5de8f9ff2ec8,Multiscale Crowd Counting and Localization By Multitask Point Supervision,16,0.284956,0.909208,"We propose a multitask approach for crowd counting and person localization in
a unified framework. As the detection and localization tasks are
well-correlated and can be jointly tackled, our model benefits from a multitask
solution by learning multiscale representations of encoded crowd images, and
subsequently fusing them. In contrast to the relatively more popular
density-based methods, our model uses point supervision to allow for crowd
locations to be accurately identified. We test our model on two popular crowd
counting datasets, ShanghaiTech A and B, and demonstrate that our method
achieves strong results on both counting and localization tasks, with MSE
measures of 110.7 and 15.0 for crowd counting and AP measures of 0.71 and 0.75
for localization, on ShanghaiTech A and B respectively. Our detailed ablation
experiments show the impact of our multiscale approach as well as the
effectiveness of the fusion module embedded in our network. Our code is
available at: https://github.com/RCVLab-AiimLab/crowd_counting.",1,1,0,0,0,0,0.64571,9.0,0.825279,24
3162a222-9808-41f4-a1b9-bf55af343470,Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies,20,0.121743,0.812857,"We consider infinite-horizon discounted Markov decision processes and study
the convergence rates of the natural policy gradient (NPG) and the Q-NPG
methods with the log-linear policy class. Using the compatible function
approximation framework, both methods with log-linear policies can be written
as inexact versions of the policy mirror descent (PMD) method. We show that
both methods attain linear convergence rates and
$\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexities using a simple,
non-adaptive geometrically increasing step size, without resorting to entropy
or other strongly convex regularization. Lastly, as a byproduct, we obtain
sublinear convergence rates for both methods with arbitrary constant step size.",0,0,0,0,0,0,0.299673,7.0,0.622617,91
a76afe98-9241-47b9-8112-3381bc4fa06b,Distilling a Pretrained Language Model to a Multilingual ASR Model,6,0.0573173,0.26337,"Multilingual speech data often suffer from long-tailed language distribution,
resulting in performance degradation. However, multilingual text data is much
easier to obtain, yielding a more useful general language model. Hence, we are
motivated to distill the rich knowledge embedded inside a well-trained teacher
text model to the student speech model. We propose a novel method called the
Distilling a Language model to a Speech model (Distill-L2S), which aligns the
latent representations of two different modalities. The subtle differences are
handled by the shrinking mechanism, nearest-neighbor interpolation, and a
learnable linear projection layer. We demonstrate the effectiveness of our
distillation method by applying it to the multilingual automatic speech
recognition (ASR) task. We distill the transformer-based cross-lingual language
model (InfoXLM) while fine-tuning the large-scale multilingual ASR model
(XLSR-wav2vec 2.0) for each language. We show the superiority of our method on
20 low-resource languages of the CommonVoice dataset with less than 100 hours
of speech data.",1,1,0,0,0,0,0.733986,6.0,0.778562,47
ee779ab3-a9f8-4d2b-8892-2edb2195d8da,Training Debiased Subnetworks with Contrastive Weight Pruning,6,0.0597307,0.663245,"Neural networks are often biased to spuriously correlated features that
provide misleading statistical evidence that does not generalize. This raises
an interesting question: ``Does an optimal unbiased functional subnetwork exist
in a severely biased network? If so, how to extract such subnetwork?"" While
empirical evidence has been accumulated about the existence of such unbiased
subnetworks, these observations are mainly based on the guidance of
ground-truth unbiased samples. Thus, it is unexplored how to discover the
optimal subnetworks with biased training datasets in practice. To address this,
here we first present our theoretical insight that alerts potential limitations
of existing algorithms in exploring unbiased subnetworks in the presence of
strong spurious correlations. We then further elucidate the importance of
bias-conflicting samples on structure learning. Motivated by these
observations, we propose a Debiased Contrastive Weight Pruning (DCWP)
algorithm, which probes unbiased subnetworks without expensive group
annotations. Experimental results demonstrate that our approach significantly
outperforms state-of-the-art debiasing methods despite its considerable
reduction in the number of parameters.",0,0,0,0,0,0,0.807192,6.0,0.814824,44
ef84e5e3-3fdd-4ee5-9624-c1fade6f30eb,EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning,18,0.0659125,0.354405,"Pre-trained vision-language models (VLMs) have achieved impressive results in
a range of vision-language tasks. However, popular VLMs usually consist of
hundreds of millions of parameters which brings challenges for fine-tuning and
deployment in real-world applications due to space, memory, and latency
constraints. In this work, we introduce a distilling then pruning framework to
compress large vision-language models into smaller, faster, and more accurate
ones. We first shrink the size of a pre-trained large VLM and apply knowledge
distillation in the vision-language pre-training stage to obtain a
task-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm
to automatically infer the importance of vision and language modalities for
different downstream tasks and adaptively remove redundant structures and
neurons in different encoders with controllable target sparsity. We apply our
framework to train EfficientVLM, a fast and accurate vision-language model
consisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers,
accounting for only 93 million parameters in total, which is 44.3% of the
teacher model. EfficientVLM retains 98.4% performance of the teacher model and
accelerates its inference speed by 2.2x. EfficientVLM achieves a large absolute
improvement over previous SoTA efficient VLMs of similar sizes by a large
margin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2
(+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation
(CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.",1,1,0,0,1,0,0.439694,7.0,0.692083,83
fef69058-fc01-4078-a071-198850ef4d80,Improving Zero-Shot Event Extraction via Sentence Simplification,5,0.050298,0.348123,"The success of sites such as ACLED and Our World in Data have demonstrated
the massive utility of extracting events in structured formats from large
volumes of textual data in the form of news, social media, blogs and discussion
forums. Event extraction can provide a window into ongoing geopolitical crises
and yield actionable intelligence. With the proliferation of large pretrained
language models, Machine Reading Comprehension (MRC) has emerged as a new
paradigm for event extraction in recent times. In this approach, event argument
extraction is framed as an extractive question-answering task. One of the key
advantages of the MRC-based approach is its ability to perform zero-shot
extraction. However, the problem of long-range dependencies, i.e., large
lexical distance between trigger and argument words and the difficulty of
processing syntactically complex sentences plague MRC-based approaches. In this
paper, we present a general approach to improve the performance of MRC-based
event extraction by performing unsupervised sentence simplification guided by
the MRC model itself. We evaluate our approach on the ICEWS geopolitical event
extraction dataset, with specific attention to `Actor' and `Target' argument
roles. We show how such context simplification can improve the performance of
MRC-based event extraction by more than 5% for actor extraction and more than
10% for target extraction.",0,1,0,0,0,0,0.454303,8.0,0.736147,34
8c922d62-a662-449a-9310-d2b372ce229a,Hidden Agenda: a Social Deduction Game with Diverse Learned Equilibria,5,0.15566,0.581517,"A key challenge in the study of multiagent cooperation is the need for
individual agents not only to cooperate effectively, but to decide with whom to
cooperate. This is particularly critical in situations when other agents have
hidden, possibly misaligned motivations and goals. Social deduction games offer
an avenue to study how individuals might learn to synthesize potentially
unreliable information about others, and elucidate their true motivations. In
this work, we present Hidden Agenda, a two-team social deduction game that
provides a 2D environment for studying learning agents in scenarios of unknown
team alignment. The environment admits a rich set of strategies for both teams.
Reinforcement learning agents trained in Hidden Agenda show that agents can
learn a variety of behaviors, including partnering and voting without need for
communication in natural language.",0,1,0,0,0,0,0.451342,8.0,0.735025,36
06a9e8da-8822-407f-890e-35d192946e86,Same Author or Just Same Topic? Towards Content-Independent Style Representations,21,0.206398,0.323622,"Linguistic style is an integral component of language. Recent advances in the
development of style representations have increasingly used training objectives
from authorship verification (AV): Do two texts have the same author? The
assumption underlying the AV training task (same author approximates same
writing style) enables self-supervised and, thus, extensive training. However,
a good performance on the AV task does not ensure good ""general-purpose"" style
representations. For example, as the same author might typically write about
certain topics, representations trained on AV might also encode content
information instead of style alone. We introduce a variation of the AV training
task that controls for content using conversation or domain labels. We evaluate
whether known style dimensions are represented and preferred over content
information through an original variation to the recently proposed STEL
framework. We find that representations trained by controlling for conversation
are better than representations trained with domain or no content control at
representing style independent from content.",0,0,0,0,0,0,0.138702,9.0,0.609873,45
49edd048-7af2-4b82-a80d-c40230de6c52,DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation,29,0.100916,0.802446,"This paper proposes a simple baseline framework for video-based 2D/3D human
pose estimation that can achieve 10 times efficiency improvement over existing
works without any performance degradation, named DeciWatch. Unlike current
solutions that estimate each frame in a video, DeciWatch introduces a simple
yet effective sample-denoise-recover framework that only watches sparsely
sampled frames, taking advantage of the continuity of human motions and the
lightweight pose representation. Specifically, DeciWatch uniformly samples less
than 10% video frames for detailed estimation, denoises the estimated 2D/3D
poses with an efficient Transformer architecture, and then accurately recovers
the rest of the frames using another Transformer-based network. Comprehensive
experimental results on three video-based human pose estimation and body mesh
recovery tasks with four datasets validate the efficiency and effectiveness of
DeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.",1,0,0,0,0,0,0.3726,4.0,0.40684,65
4801caef-7208-407c-a0e5-b519d4cf85c6,Optimizing Relevance Maps of Vision Transformers Improves Robustness,22,0.367252,0.5879,"It has been observed that visual classification models often rely mostly on
the image background, neglecting the foreground, which hurts their robustness
to distribution changes. To alleviate this shortcoming, we propose to monitor
the model's relevancy signal and manipulate it such that the model is focused
on the foreground object. This is done as a finetuning step, involving
relatively few samples consisting of pairs of images and their associated
foreground masks. Specifically, we encourage the model's relevancy map (i) to
assign lower relevance to background regions, (ii) to consider as much
information as possible from the foreground, and (iii) we encourage the
decisions to have high confidence. When applied to Vision Transformer (ViT)
models, a marked improvement in robustness to domain shifts is observed.
Moreover, the foreground masks can be obtained automatically, from a
self-supervised variant of the ViT model itself; therefore no additional
supervision is required.",1,1,0,0,0,0,0.925153,7.0,0.906159,75
a5651004-9eeb-41ff-8b42-9079625598e1,Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors,12,0.292747,0.583465,"Multi-scale features have been proven highly effective for object detection
but often come with huge and even prohibitive extra computation costs,
especially for the recent Transformer-based detectors. In this paper, we
propose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm
that enables efficient use of multi-scale features in Transformer-based object
detectors. The core idea is to exploit sparse multi-scale features from just a
few crucial locations, and it is achieved with two novel designs. First, IMFA
rearranges the Transformer encoder-decoder pipeline so that the encoded
features can be iteratively updated based on the detection predictions. Second,
IMFA sparsely samples scale-adaptive features for refined detection from just a
few keypoint locations under the guidance of prior detection predictions. As a
result, the sampled multi-scale features are sparse yet still highly beneficial
for object detection. Extensive experiments show that the proposed IMFA boosts
the performance of multiple Transformer-based object detectors significantly
yet with only slight computational overhead.",1,1,0,0,0,0,0.944711,5.0,0.890722,100
86adc214-1ae1-42c4-910f-bc96f68f26e0,Causal Discovery of Dynamic Models for Predicting Human Spatial Interactions,8,0.163864,0.295162,"Exploiting robots for activities in human-shared environments, whether
warehouses, shopping centres or hospitals, calls for such robots to understand
the underlying physical interactions between nearby agents and objects. In
particular, modelling cause-and-effect relations between the latter can help to
predict unobserved human behaviours and anticipate the outcome of specific
robot interventions. In this paper, we propose an application of causal
discovery methods to model human-robot spatial interactions, trying to
understand human behaviours from real-world sensor data in two possible
scenarios: humans interacting with the environment, and humans interacting with
obstacles. New methods and practical solutions are discussed to exploit, for
the first time, a state-of-the-art causal discovery algorithm in some
challenging human environments, with potential application in many service
robotics scenarios. To demonstrate the utility of the causal models obtained
from real-world datasets, we present a comparison between causal and non-causal
prediction approaches. Our results show that the causal model correctly
captures the underlying interactions of the considered scenarios and improves
its prediction accuracy.",0,1,0,0,0,0,0.152151,8.0,0.573634,23
4906adc1-bd14-407a-8fa1-29b728f28cc4,Learning a Grammar Inducer from Massive Uncurated Instructional Videos,2,0.0783098,0.375781,"Video-aided grammar induction aims to leverage video information for finding
more accurate syntactic grammars for accompanying text. While previous work
focuses on building systems for inducing grammars on text that are well-aligned
with video content, we investigate the scenario, in which text and video are
only in loose correspondence. Such data can be found in abundance online, and
the weak correspondence is similar to the indeterminacy problem studied in
language acquisition. Furthermore, we build a new model that can better learn
video-span correlation without manually designed features adopted by previous
work. Experiments show that our model trained only on large-scale YouTube data
with no text-video alignment reports strong and robust performances across
three unseen datasets, despite domain shift and noisy label issues. Furthermore
our model yields higher F1 scores than the previous state-of-the-art systems
trained on in-domain data.",0,0,0,0,1,0,0.80019,11.0,0.897003,61
3532890e-314a-4f70-8646-bfcf27e86c08,A Contrastive Framework for Neural Text Generation,146,0.796305,0.919322,"Text generation is of great importance to many natural language processing
applications. However, maximization-based decoding methods (e.g. beam search)
of neural language models often lead to degenerate solutions -- the generated
text is unnatural and contains undesirable repetitions. Existing approaches
introduce stochasticity via sampling or modify training objectives to decrease
probabilities of certain tokens (e.g., unlikelihood training). However, they
often lead to solutions that lack coherence. In this work, we show that an
underlying reason for model degeneration is the anisotropic distribution of
token representations. We present a contrastive solution: (i) SimCTG, a
contrastive training objective to calibrate the model's representation space,
and (ii) a decoding method -- contrastive search -- to encourage diversity
while maintaining coherence in the generated text. Extensive experiments and
analyses on three benchmarks from two languages demonstrate that our proposed
approach significantly outperforms current state-of-the-art text generation
methods as evaluated by both human and automatic metrics.",1,0,0,0,1,1,0.844039,4.0,0.752535,93
2100eff6-1065-4750-b0fd-bd621381162f,Adversarial Masking for Self-Supervised Learning,58,0.183457,0.717541,"We propose ADIOS, a masked image model (MIM) framework for self-supervised
learning, which simultaneously learns a masking function and an image encoder
using an adversarial objective. The image encoder is trained to minimise the
distance between representations of the original and that of a masked image.
The masking function, conversely, aims at maximising this distance. ADIOS
consistently improves on state-of-the-art self-supervised learning (SSL)
methods on a variety of tasks and datasets -- including classification on
ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and
iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao
et al., 2021) -- while generating semantically meaningful masks. Unlike modern
MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch
tokenisation construction of Vision Transformers, and can be implemented with
convolutional backbones. We further demonstrate that the masks learned by ADIOS
are more effective in improving representation learning of SSL methods than
masking schemes used in popular MIM models. Code is available at
https://github.com/YugeTen/adios.",1,0,0,0,1,0,0.919729,4.0,0.828938,42
a61f1267-548a-4f08-a932-cacaf23c8e08,A Holistic Framework for Analyzing the COVID-19 Vaccine Debate,18,0.260255,0.821673,"The Covid-19 pandemic has led to infodemic of low quality information leading
to poor health decisions. Combating the outcomes of this infodemic is not only
a question of identifying false claims, but also reasoning about the decisions
individuals make. In this work we propose a holistic analysis framework
connecting stance and reason analysis, and fine-grained entity level moral
sentiment analysis. We study how to model the dependencies between the
different level of analysis and incorporate human insights into the learning
process. Experiments show that our framework provides reliable predictions even
in the low-supervision settings.",0,0,0,0,0,0,0.865628,4.0,0.771819,38
f2b348df-d8b7-421e-a368-ba4622cdc01b,Overlap-guided Gaussian Mixture Models for Point Cloud Registration,8,0.167864,0.436437,"Probabilistic 3D point cloud registration methods have shown competitive
performance in overcoming noise, outliers, and density variations. However,
registering point cloud pairs in the case of partial overlap is still a
challenge. This paper proposes a novel overlap-guided probabilistic
registration approach that computes the optimal transformation from matched
Gaussian Mixture Model (GMM) parameters. We reformulate the registration
problem as the problem of aligning two Gaussian mixtures such that a
statistical discrepancy measure between the two corresponding mixtures is
minimized. We introduce a Transformer-based detection module to detect
overlapping regions, and represent the input point clouds using GMMs by guiding
their alignment through overlap scores computed by this detection module.
Experiments show that our method achieves superior registration accuracy and
efficiency than state-of-the-art methods when handling point clouds with
partial overlap and different densities on synthetic and real-world datasets.
https://github.com/gfmei/ogmm",1,1,1,0,1,0,0.511055,8.0,0.756975,55
ecca6894-bc53-4449-a19e-05485592040c,Amodal Panoptic Segmentation,31,0.552086,0.637,"Humans have the remarkable ability to perceive objects as a whole, even when
parts of them are occluded. This ability of amodal perception forms the basis
of our perceptual and cognitive understanding of our world. To enable robots to
reason with this capability, we formulate and propose a novel task that we name
amodal panoptic segmentation. The goal of this task is to simultaneously
predict the pixel-wise semantic segmentation labels of the visible regions of
stuff classes and the instance segmentation labels of both the visible and
occluded regions of thing classes. To facilitate research on this new task, we
extend two established benchmark datasets with pixel-level amodal panoptic
segmentation labels that we make publicly available as KITTI-360-APS and
BDD100K-APS. We present several strong baselines, along with the amodal
panoptic quality (APQ) and amodal parsing coverage (APC) metrics to quantify
the performance in an interpretable manner. Furthermore, we propose the novel
amodal panoptic segmentation network (APSNet), as a first step towards
addressing this task by explicitly modeling the complex relationships between
the occluders and occludes. Extensive experimental evaluations demonstrate that
APSNet achieves state-of-the-art performance on both benchmarks and more
importantly exemplifies the utility of amodal recognition. The benchmarks are
available at http://amodal-panoptic.cs.uni-freiburg.de.",1,0,1,1,1,0,0.89181,7.0,0.884259,42
469a44bc-bd89-4333-80d6-f5427d4b8eb4,Targeted Honeyword Generation with Language Models,2,0.0927864,0.0214113,"Honeywords are fictitious passwords inserted into databases in order to
identify password breaches. The major difficulty is how to produce honeywords
that are difficult to distinguish from real passwords. Although the generation
of honeywords has been widely investigated in the past, the majority of
existing research assumes attackers have no knowledge of the users. These
honeyword generating techniques (HGTs) may utterly fail if attackers exploit
users' personally identifiable information (PII) and the real passwords include
users' PII. In this paper, we propose to build a more secure and trustworthy
authentication system that employs off-the-shelf pre-trained language models
which require no further training on real passwords to produce honeywords while
retaining the PII of the associated real password, therefore significantly
raising the bar for attackers.
  We conducted a pilot experiment in which individuals are asked to distinguish
between authentic passwords and honeywords when the username is provided for
GPT-3 and a tweaking technique. Results show that it is extremely difficult to
distinguish the real passwords from the artifical ones for both techniques. We
speculate that a larger sample size could reveal a significant difference
between the two HGT techniques, favouring our proposed approach.",0,1,0,0,0,0,0.489999,12.0,0.832922,32
9826c48f-2b75-4f80-8b55-188fccb2a44c,Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition,50,0.556373,0.537487,"Self-supervised learning (SSL) is a powerful tool that allows learning of
underlying representations from unlabeled data. Transformer based models such
as wav2vec 2.0 and HuBERT are leading the field in the speech domain. Generally
these models are fine-tuned on a small amount of labeled data for a downstream
task such as Automatic Speech Recognition (ASR). This involves re-training the
majority of the model for each task. Adapters are small lightweight modules
which are commonly used in Natural Language Processing (NLP) to adapt
pre-trained models to new tasks. In this paper we propose applying adapters to
wav2vec 2.0 to reduce the number of parameters required for downstream ASR
tasks, and increase scalability of the model to multiple tasks or languages.
Using adapters we can perform ASR while training fewer than 10% of parameters
per task compared to full fine-tuning with little degradation of performance.
Ablations show that applying adapters into just the top few layers of the
pre-trained network gives similar performance to full transfer, supporting the
theory that higher pre-trained layers encode more phonemic information, and
further optimizing efficiency.",0,1,0,0,0,0,0.956233,5.0,0.906246,20
946240bd-31ef-45eb-a0a2-2d470f1829fa,Identifying Ethical Issues in AI Partners in Human-AI Co-Creation,9,0.379525,0.65119,"Human-AI co-creativity involves humans and AI collaborating on a shared
creative product as partners. In many existing co-creative systems, users
communicate with the AI using buttons or sliders. However, typically, the AI in
co-creative systems cannot communicate back to humans, limiting their potential
to be perceived as partners. This paper starts with an overview of a
comparative study with 38 participants to explore the impact of AI-to-human
communication on user perception and engagement in co-creative systems and the
results show improved collaborative experience and user engagement with the
system incorporating AI-to-human communication. The results also demonstrate
that users perceive co-creative AI as more reliable, personal and intelligent
when it can communicate with the users. The results indicate a need to identify
potential ethical issues from an engaging communicating co-creative AI. Later
in the paper, we present some potential ethical issues in human-AI co-creation
and propose to use participatory design fiction as the research methodology to
investigate the ethical issues associated with a co-creative AI that
communicates with users.",0,0,0,0,0,0,0.848345,12.0,0.918758,25
5a0d7cb9-4071-4207-928f-58bc374e9acc,Artificial Intelligence in Concrete Materials: A Scientometric View,1,0.0683689,0.0960761,"Artificial intelligence (AI) has emerged as a transformative and versatile
tool, breaking new frontiers across scientific domains. Among its most
promising applications, AI research is blossoming in concrete science and
engineering, where it has offered new insights towards mixture design
optimization and service life prediction of cementitious systems. This chapter
aims to uncover the main research interests and knowledge structure of the
existing literature on AI for concrete materials. To begin with, a total of 389
journal articles published from 1990 to 2020 were retrieved from the Web of
Science. Scientometric tools such as keyword co-occurrence analysis and
documentation co-citation analysis were adopted to quantify features and
characteristics of the research field. The findings bring to light pressing
questions in data-driven concrete research and suggest future opportunities for
the concrete community to fully utilize the capabilities of AI techniques.",0,1,0,0,0,0,0.926482,12.0,0.945833,111
da003ede-a671-41b0-a64d-e647f2c8157c,NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models,26,0.0882836,0.783808,"Neural image caption generation (NICG) models have received massive attention
from the research community due to their excellent performance in visual
understanding. Existing work focuses on improving NICG model accuracy while
efficiency is less explored. However, many real-world applications require
real-time feedback, which highly relies on the efficiency of NICG models.
Recent research observed that the efficiency of NICG models could vary for
different inputs. This observation brings in a new attack surface of NICG
models, i.e., An adversary might be able to slightly change inputs to cause the
NICG models to consume more computational resources. To further understand such
efficiency-oriented threats, we propose a new attack approach, NICGSlowDown, to
evaluate the efficiency robustness of NICG models. Our experimental results
show that NICGSlowDown can generate images with human-unnoticeable
perturbations that will increase the NICG model latency up to 483.86%. We hope
this research could raise the community's concern about the efficiency
robustness of NICG models.",1,0,0,0,0,0,0.367773,11.0,0.782798,43
ac73f0af-a8f0-4465-8b1a-9c2aa2a419d9,Challenges in Visual Anomaly Detection for Mobile Robots,1,0.0231728,0.0664023,"We consider the task of detecting anomalies for autonomous mobile robots
based on vision. We categorize relevant types of visual anomalies and discuss
how they can be detected by unsupervised deep learning methods. We propose a
novel dataset built specifically for this task, on which we test a
state-of-the-art approach; we finally discuss deployment in a real scenario.",1,1,1,1,0,0,0.913534,13.0,0.945064,11
60f99a52-39db-4b7f-bd40-d51af45aeebe,Unsupervised Non-transferable Text Classification,2,0.00662072,0.185904,"Training a good deep learning model requires substantial data and computing
resources, which makes the resulting neural model a valuable intellectual
property. To prevent the neural network from being undesirably exploited,
non-transferable learning has been proposed to reduce the model generalization
ability in specific target domains. However, existing approaches require
labeled data for the target domain which can be difficult to obtain.
Furthermore, they do not have the mechanism to still recover the model's
ability to access the target domain. In this paper, we propose a novel
unsupervised non-transferable learning method for the text classification task
that does not require annotated target domain data. We further introduce a
secret key component in our approach for recovering the access to the target
domain, where we design both an explicit and an implicit method for doing so.
Extensive experiments demonstrate the effectiveness of our approach.",1,1,0,0,0,0,0.325556,8.0,0.682352,40
d4d36e37-3fac-4970-9a64-9bbec6d92ca5,Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection,26,0.392833,0.887628,"In some scenarios, classifier requires detecting out-of-distribution samples
far from its training data. With desirable characteristics, reconstruction
autoencoder-based methods deal with this problem by using input reconstruction
error as a metric of novelty vs. normality. We formulate the essence of such
approach as a quadruplet domain translation with an intrinsic bias to only
query for a proxy of conditional data uncertainty. Accordingly, an improvement
direction is formalized as maximumly compressing the autoencoder's latent space
while ensuring its reconstructive power for acting as a described domain
translator. From it, strategies are introduced including semantic
reconstruction, data certainty decomposition and normalized L2 distance to
substantially improve original methods, which together establish
state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of
CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method
works without any additional data, hard-to-implement structure, time-consuming
pipeline, and even harming the classification accuracy of known classes.",0,0,0,0,1,0,0.667068,10.0,0.848571,33
4b3bc78d-3b67-4783-acfe-ad5fc5e6d0de,Towards an Accountable and Reproducible Federated Learning: A FactSheets Approach,5,0.0307652,0.153716,"Federated Learning (FL) is a novel paradigm for the shared training of models
based on decentralized and private data. With respect to ethical guidelines, FL
is promising regarding privacy, but needs to excel vis-\`a-vis transparency and
trustworthiness. In particular, FL has to address the accountability of the
parties involved and their adherence to rules, law and principles. We introduce
AF^2 Framework, where we instrument FL with accountability by fusing verifiable
claims with tamper-evident facts, into reproducible arguments. We build on AI
FactSheets for instilling transparency and trustworthiness into the AI
lifecycle and expand it to incorporate dynamic and nested facts, as well as
complex model compositions in FL. Based on our approach, an auditor can
validate, reproduce and certify a FL process. This can be directly applied in
practice to address the challenges of AI engineering and ethics.",0,1,0,0,0,0,0.397254,6.0,0.618309,39
8c6908a9-ee43-4131-b025-751b780cee98,Fingerprint Liveness Detection Based on Quality Measures,34,0.435271,0.773064,"A new fingerprint parameterization for liveness detection based on quality
measures is presented. The novel feature set is used in a complete liveness
detection system and tested on the development set of the LivDET competition,
comprising over 4,500 real and fake images acquired with three different
optical sensors. The proposed solution proves to be robust to the multi-sensor
scenario, and presents an overall rate of 93% of correctly classified samples.
Furthermore, the liveness detection method presented has the added advantage
over previously studied techniques of needing just one image from a finger to
decide whether it is real or fake.",0,1,0,0,0,0,0.762071,7.0,0.821753,25
08ae0cce-e7f8-4989-a519-4be4d05c41aa,Scalable Multi-view Clustering with Graph Filtering,14,0.201393,0.431922,"With the explosive growth of multi-source data, multi-view clustering has
attracted great attention in recent years. Most existing multi-view methods
operate in raw feature space and heavily depend on the quality of original
feature representation. Moreover, they are often designed for feature data and
ignore the rich topology structure information. Accordingly, in this paper, we
propose a generic framework to cluster both attribute and graph data with
heterogeneous features. It is capable of exploring the interplay between
feature and structure. Specifically, we first adopt graph filtering technique
to eliminate high-frequency noise to achieve a clustering-friendly smooth
representation. To handle the scalability challenge, we develop a novel
sampling strategy to improve the quality of anchors. Extensive experiments on
attribute and graph benchmarks demonstrate the superiority of our approach with
respect to state-of-the-art approaches.",1,0,0,0,0,0,0.641089,6.0,0.735824,41
492620cf-7e70-49d8-bea6-fa0a2a802d2e,Towards a Defense Against Federated Backdoor Attacks Under Continuous Training,1,0.00611937,0.0381327,"Backdoor attacks are dangerous and difficult to prevent in federated learning
(FL), where training data is sourced from untrusted clients over long periods
of time. These difficulties arise because: (a) defenders in FL do not have
access to raw training data, and (b) a new phenomenon we identify called
backdoor leakage causes models trained continuously to eventually suffer from
backdoors due to cumulative errors in defense mechanisms. We propose shadow
learning, a framework for defending against backdoor attacks in the FL setting
under long-range training. Shadow learning trains two models in parallel: a
backbone model and a shadow model. The backbone is trained without any defense
mechanism to obtain good performance on the main task. The shadow model
combines filtering of malicious clients with early-stopping to control the
attack success rate even as the data distribution changes. We theoretically
motivate our design and show experimentally that our framework significantly
improves upon existing defenses against backdoor attacks.",0,1,0,0,1,0,0.466451,5.0,0.585135,90
ea31f4db-d5c3-42d8-8c06-187d6f2a962a,mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark,2,0.0610957,0.0698883,"Robust 2004 is an information retrieval benchmark whose large number of
judgments per query make it a reliable evaluation dataset. In this paper, we
present mRobust04, a multilingual version of Robust04 that was translated to 8
languages using Google Translate. We also provide results of three different
multilingual retrievers on this dataset. The dataset is available at
https://huggingface.co/datasets/unicamp-dl/mrobust",0,1,0,1,0,0,0.821166,4.0,0.73341,19
41cd846f-bea9-4538-a8ff-bed3dd9cbf1b,Improving the Adversarial Robustness of NLP Models by Information Bottleneck,16,0.071836,0.679301,"Existing studies have demonstrated that adversarial examples can be directly
attributed to the presence of non-robust features, which are highly predictive,
but can be easily manipulated by adversaries to fool NLP models. In this study,
we explore the feasibility of capturing task-specific robust features, while
eliminating the non-robust ones by using the information bottleneck theory.
Through extensive experiments, we show that the models trained with our
information bottleneck-based method are able to achieve a significant
improvement in robust accuracy, exceeding performances of all the previously
reported defense methods while suffering almost no performance drop in clean
accuracy on SST-2, AGNEWS and IMDB datasets.",0,0,0,0,1,0,0.29433,9.0,0.704084,65
8ca2baad-f6f8-4701-850b-9b12947ee440,ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration,8,0.222757,0.694703,"Establishing voxelwise semantic correspondence across distinct imaging
modalities is a foundational yet formidable computer vision task. Current
multi-modality registration techniques maximize hand-crafted inter-domain
similarity functions, are limited in modeling nonlinear intensity-relationships
and deformations, and may require significant re-engineering or underperform on
new tasks, datasets, and domain pairs. This work presents ContraReg, an
unsupervised contrastive representation learning approach to multi-modality
deformable registration. By projecting learned multi-scale local patch features
onto a jointly learned inter-domain embedding space, ContraReg obtains
representations useful for non-rigid multi-modality alignment. Experimentally,
ContraReg achieves accurate and robust results with smooth and invertible
deformations across a series of baselines and ablations on a neonatal T1-T2
brain MRI registration task with all methods validated over a wide range of
deformation regularization strengths.",0,0,0,0,0,0,0.775552,7.0,0.827442,45
f801d81a-d648-42b0-9944-77c23eb32a62,Parametrically Retargetable Decision-Makers Tend To Seek Power,13,0.127794,0.561686,"If capable AI agents are generally incentivized to seek power in service of
the objectives we specify for them, then these systems will pose enormous
risks, in addition to enormous benefits. In fully observable environments, most
reward functions have an optimal policy which seeks power by keeping options
open and staying alive. However, the real world is neither fully observable,
nor must trained agents be even approximately reward-optimal. We consider a
range of models of AI decision-making, from optimal, to random, to choices
informed by learning and interacting with an environment. We discover that many
decision-making functions are retargetable, and that retargetability is
sufficient to cause power-seeking tendencies. Our functional criterion is
simple and broad. We show that a range of qualitatively dissimilar
decision-making procedures incentivize agents to seek power. We demonstrate the
flexibility of our results by reasoning about learned policy incentives in
Montezuma's Revenge. These results suggest a safety risk: Eventually,
retargetable training procedures may train real-world agents which seek power
over humans.",0,0,0,0,0,1,0.113378,13.0,0.713316,36
f5d2d50a-142b-4b4b-81d3-fa63f937d2a7,BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision,109,0.969742,0.999951,"We present a novel bird's-eye-view (BEV) detector with perspective
supervision, which converges faster and better suits modern image backbones.
Existing state-of-the-art BEV detectors are often tied to certain depth
pre-trained backbones like VoVNet, hindering the synergy between booming image
backbones and BEV detectors. To address this limitation, we prioritize easing
the optimization of BEV detectors by introducing perspective space supervision.
To this end, we propose a two-stage BEV detector, where proposals from the
perspective head are fed into the bird's-eye-view head for final predictions.
To evaluate the effectiveness of our model, we conduct extensive ablation
studies focusing on the form of supervision and the generality of the proposed
detector. The proposed method is verified with a wide spectrum of traditional
and modern image backbones and achieves new SoTA results on the large-scale
nuScenes dataset. The code shall be released soon.",0,1,0,0,1,0,0.979121,5.0,0.948712,46
18a76fe4-3076-4bc0-89dc-cf9c99fac75d,Creating a morphological and syntactic tagged corpus for the Uzbek language,4,0.0318061,0.132752,"Nowadays, creation of the tagged corpora is becoming one of the most
important tasks of Natural Language Processing (NLP). There are not enough
tagged corpora to build machine learning models for the low-resource Uzbek
language. In this paper, we tried to fill that gap by developing a novel Part
Of Speech (POS) and syntactic tagset for creating the syntactic and
morphologically tagged corpus of the Uzbek language. This work also includes
detailed description and presentation of a web-based application to work on a
tagging as well. Based on the developed annotation tool and the software, we
share our experience results of the first stage of the tagged corpus creation",0,1,0,1,0,0,0.0244699,7.0,0.241802,27
3a47accd-9dc4-4f7c-8366-af22794b5195,Spectral Probing,2,0.0768276,0.133584,"Linguistic information is encoded at varying timescales (subwords, phrases,
etc.) and communicative levels, such as syntax and semantics. Contextualized
embeddings have analogously been found to capture these phenomena at
distinctive layers and frequencies. Leveraging these findings, we develop a
fully learnable frequency filter to identify spectral profiles for any given
task. It enables vastly more granular analyses than prior handcrafted filters,
and improves on efficiency. After demonstrating the informativeness of spectral
probing over manual filters in a monolingual setting, we investigate its
multilingual characteristics across seven diverse NLP tasks in six languages.
Our analyses identify distinctive spectral profiles which quantify cross-task
similarity in a linguistically intuitive manner, while remaining consistent
across languages-highlighting their potential as robust, lightweight task
descriptors.",1,0,0,0,0,0,0.969759,9.0,0.960324,31
0c0ee7f5-64c8-45c4-9da4-69894eb1d3e5,Does Simultaneous Speech Translation need Simultaneous Models?,12,0.376044,0.793159,"In simultaneous speech translation (SimulST), finding the best trade-off
between high translation quality and low latency is a challenging task. To meet
the latency constraints posed by the different application scenarios, multiple
dedicated SimulST models are usually trained and maintained, generating high
computational costs. In this paper, motivated by the increased social and
environmental impact caused by these costs, we investigate whether a single
model trained offline can serve not only the offline but also the simultaneous
task without the need for any additional training or adaptation. Experiments on
en->{de, es} indicate that, aside from facilitating the adoption of
well-established offline techniques and architectures without affecting
latency, the offline solution achieves similar or better translation quality
compared to the same model trained in simultaneous settings, as well as being
competitive with the SimulST state of the art.",0,1,0,0,1,0,0.813313,4.0,0.727089,66
c1df873f-b2a8-42f2-8587-88704f62aa08,The Third International Verification of Neural Networks Competition (VNN-COMP 2022): Summary and Results,22,0.136196,0.857211,"This report summarizes the 3rd International Verification of Neural Networks
Competition (VNN-COMP 2022), held as a part of the 5th Workshop on Formal
Methods for ML-Enabled Autonomous Systems (FoMLAS), which was collocated with
the 34th International Conference on Computer-Aided Verification (CAV).
VNN-COMP is held annually to facilitate the fair and objective comparison of
state-of-the-art neural network verification tools, encourage the
standardization of tool interfaces, and bring together the neural network
verification community. To this end, standardized formats for networks (ONNX)
and specification (VNN-LIB) were defined, tools were evaluated on equal-cost
hardware (using an automatic evaluation pipeline based on AWS instances), and
tool parameters were chosen by the participants before the final test sets were
made public. In the 2022 iteration, 11 teams participated on a diverse set of
12 scored benchmarks. This report summarizes the rules, benchmarks,
participating tools, results, and lessons learned from this iteration of this
competition.",1,1,0,0,0,0,0.138471,6.0,0.414512,53
4053632c-370f-4de9-a5c0-720a079d8ebf,LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding,88,0.955441,0.999998,"Structured document understanding has attracted considerable attention and
made significant progress recently, owing to its crucial role in intelligent
document processing. However, most existing related models can only deal with
the document data of specific language(s) (typically English) included in the
pre-training collection, which is extremely limited. To address this issue, we
propose a simple yet effective Language-independent Layout Transformer (LiLT)
for structured document understanding. LiLT can be pre-trained on the
structured documents of a single language and then directly fine-tuned on other
languages with the corresponding off-the-shelf monolingual/multilingual
pre-trained textual models. Experimental results on eight languages have shown
that LiLT can achieve competitive or even superior performance on diverse
widely-used downstream benchmarks, which enables language-independent benefit
from the pre-training of document layout structure. Code and model are publicly
available at https://github.com/jpWang/LiLT.",1,1,0,0,0,0,0.946146,6.0,0.910442,45
87a379ef-c5da-42c4-8baf-d3e08794ecdf,MMGL: Multi-Scale Multi-View Global-Local Contrastive learning for Semi-supervised Cardiac Image Segmentation,11,0.217531,0.509484,"With large-scale well-labeled datasets, deep learning has shown significant
success in medical image segmentation. However, it is challenging to acquire
abundant annotations in clinical practice due to extensive expertise
requirements and costly labeling efforts. Recently, contrastive learning has
shown a strong capacity for visual representation learning on unlabeled data,
achieving impressive performance rivaling supervised learning in many domains.
In this work, we propose a novel multi-scale multi-view global-local
contrastive learning (MMGL) framework to thoroughly explore global and local
features from different scales and views for robust contrastive learning
performance, thereby improving segmentation performance with limited
annotations. Extensive experiments on the MM-WHS dataset demonstrate the
effectiveness of MMGL framework on semi-supervised cardiac image segmentation,
outperforming the state-of-the-art contrastive learning methods by a large
margin.",0,1,0,0,1,0,0.769244,5.0,0.754674,24
4509c81e-fafa-48ea-8a3c-72f0523411e3,Quantum policy gradient algorithms,9,0.0323763,0.614996,"Understanding the power and limitations of quantum access to data in machine
learning tasks is primordial to assess the potential of quantum computing in
artificial intelligence. Previous works have already shown that speed-ups in
learning are possible when given quantum access to reinforcement learning
environments. Yet, the applicability of quantum algorithms in this setting
remains very limited, notably in environments with large state and action
spaces. In this work, we design quantum algorithms to train state-of-the-art
reinforcement learning policies by exploiting quantum interactions with an
environment. However, these algorithms only offer full quadratic speed-ups in
sample complexity over their classical analogs when the trained policies
satisfy some regularity conditions. Interestingly, we find that reinforcement
learning policies derived from parametrized quantum circuits are well-behaved
with respect to these conditions, which showcases the benefit of a
fully-quantum reinforcement learning framework.",0,0,0,0,0,0,0.0653597,7.0,0.385188,46
97ce66ad-f0e6-4900-ab9b-67c6f3405658,Who's the Expert? On Multi-source Belief Change,4,0.0531594,0.546985,"Consider the following belief change/merging scenario. A group of information
sources gives a sequence of reports about the state of the world at various
instances (e.g. different points in time). The true states at these instances
are unknown to us. The sources have varying levels of expertise, also unknown
to us, and may be knowledgeable on some topics but not others. This may cause
sources to report false statements in areas they lack expertise. What should we
believe on the basis of these reports? We provide a framework in which to
explore this problem, based on an extension of propositional logic with
expertise formulas. This extended language allows us to express beliefs about
the state of the world at each instance, as well as beliefs about the expertise
of each source. We propose several postulates, provide a couple of families of
concrete operators, and analyse these operators with respect to the postulates.",0,0,0,0,0,0,2.92955e-07,40.0,0.583683,26
469dd034-345f-4ea4-aedc-9b89b0361c02,GROOT: Corrective Reward Optimization for Generative Sequential Labeling,1,0.00443001,0.0703775,"Sequential labeling is a fundamental NLP task, forming the backbone of many
applications. Supervised learning of Seq2Seq models has shown great success on
these problems. However, the training objectives are still significantly
disconnected with the metrics and desiderata we care about in practice. For
example, a practical sequence tagging application may want to optimize for a
certain precision-recall trade-off (of the top-k predictions) which is quite
different from the standard objective of maximizing the likelihood of the gold
labeled sequence. Thus to bridge this gap, we propose GROOT -- a simple yet
effective framework for Generative Reward Optimization Of Text sequences. GROOT
works by training a generative sequential labeling model to match the decoder
output distribution with that of the (black-box) reward function. Using an
iterative training regime, we first generate prediction candidates, then
correct errors in them, and finally contrast those candidates (based on their
reward values). As demonstrated via extensive experiments on four public
benchmarks, GROOT significantly improves all reward metrics. Furthermore, GROOT
leads to improvements of the overall decoder distribution as evidenced by the
quality gains of the top-$k$ candidates.",0,1,0,0,0,1,0.303847,10.0,0.737496,34
2f502e17-093b-47ca-85aa-84fc47c24028,Diversity Over Size: On the Effect of Sample and Topic Sizes for Argument Mining Datasets,1,0.0211735,0.0660777,"The task of Argument Mining, that is extracting argumentative sentences for a
specific topic from large document sources, is an inherently difficult task for
machine learning models and humans alike, as large Argument Mining datasets are
rare and recognition of argumentative sentences requires expert knowledge. The
task becomes even more difficult if it also involves stance detection of
retrieved arguments. Given the cost and complexity of creating suitably large
Argument Mining datasets, we ask whether it is necessary for acceptable
performance to have datasets growing in size. Our findings show that, when
using carefully composed training samples and a model pretrained on related
tasks, we can reach 95% of the maximum performance while reducing the training
sample size by at least 85%. This gain is consistent across three Argument
Mining tasks on three different datasets. We also publish a new dataset for
future benchmarking.",0,1,0,1,0,0,0.724794,7.0,0.806484,47
90c92e73-6573-445a-9fe4-0cdf4e048983,A Novel Semi-supervised Meta Learning Method for Subject-transfer Brain-computer Interface,12,0.202933,0.933001,"Brain-computer interface (BCI) provides a direct communication pathway
between human brain and external devices. Before a new subject could use BCI, a
calibration procedure is usually required. Because the inter- and intra-subject
variances are so large that the models trained by the existing subjects perform
poorly on new subjects. Therefore, effective subject-transfer and calibration
method is essential. In this paper, we propose a semi-supervised meta learning
(SSML) method for subject-transfer learning in BCIs. The proposed SSML learns a
meta model with the existing subjects first, then fine-tunes the model in a
semi-supervised learning manner, i.e. using few labeled and many unlabeled
samples of target subject for calibration. It is significant for BCI
applications where the labeled data are scarce or expensive while unlabeled
data are readily available. To verify the SSML method, three different BCI
paradigms are tested: 1) event-related potential detection; 2) emotion
recognition; and 3) sleep staging. The SSML achieved significant improvements
of over 15% on the first two paradigms and 4.9% on the third. The experimental
results demonstrated the effectiveness and potential of the SSML method in BCI
applications.",0,0,1,0,0,0,0.434721,6.0,0.638201,60
0a456794-0a3c-4fd4-9ba2-eb2c667ddb69,"For Learning in Symmetric Teams, Local Optima are Global Nash Equilibria",3,0.0256521,0.418476,"Although it has been known since the 1970s that a globally optimal strategy
profile in a common-payoff game is a Nash equilibrium, global optimality is a
strict requirement that limits the result's applicability. In this work, we
show that any locally optimal symmetric strategy profile is also a (global)
Nash equilibrium. Furthermore, we show that this result is robust to
perturbations to the common payoff and to the local optimum. Applied to machine
learning, our result provides a global guarantee for any gradient method that
finds a local optimum in symmetric strategy space. While this result indicates
stability to unilateral deviation, we nevertheless identify broad classes of
games where mixed local optima are unstable under joint, asymmetric deviations.
We analyze the prevalence of instability by running learning algorithms in a
suite of symmetric games, and we conclude by discussing the applicability of
our results to multi-agent RL, cooperative inverse RL, and decentralized
POMDPs.",1,0,0,0,0,0,0.0223411,16.0,0.662532,55
311e3dea-db3d-4e4c-82cd-a5c9caad02d1,Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns,11,0.269258,0.862106,"There is a growing interest in the combined use of NLP and machine learning
methods to predict gaze patterns during naturalistic reading. While promising
results have been obtained through the use of transformer-based language
models, little work has been undertaken to relate the performance of such
models to general text characteristics. In this paper we report on experiments
with two eye-tracking corpora of naturalistic reading and two language models
(BERT and GPT-2). In all experiments, we test effects of a broad spectrum of
features for predicting human reading behavior that fall into five categories
(syntactic complexity, lexical richness, register-based multiword combinations,
readability and psycholinguistic word properties). Our experiments show that
both the features included and the architecture of the transformer-based
language models play a role in predicting multiple eye-tracking measures during
naturalistic reading. We also report the results of experiments aimed at
determining the relative importance of features from different groups using
SP-LIME.",0,0,0,0,0,0,0.549131,11.0,0.833012,47
8442736f-d690-47fb-8a7c-454529ad178b,Generalizable Implicit Neural Representations via Instance Pattern Composers,8,0.0396769,0.365723,"Despite recent advances in implicit neural representations (INRs), it remains
challenging for a coordinate-based multi-layer perceptron (MLP) of INRs to
learn a common representation across data instances and generalize it for
unseen instances. In this work, we introduce a simple yet effective framework
for generalizable INRs that enables a coordinate-based MLP to represent complex
data instances by modulating only a small set of weights in an early MLP layer
as an instance pattern composer; the remaining MLP weights learn pattern
composition rules for common representations across instances. Our
generalizable INR framework is fully compatible with existing meta-learning and
hypernetworks in learning to predict the modulated weight for unseen instances.
Extensive experiments demonstrate that our method achieves high performance on
a wide range of domains such as an audio, image, and 3D object, while the
ablation study validates our weight modulation.",0,0,0,0,0,1,0.587947,7.0,0.752886,34
9d7a6653-67d3-4777-998f-2c79893b95f7,DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis,18,0.373781,0.98213,"The rapid development of aspect-based sentiment analysis (ABSA) within recent
decades shows great potential for real-world society. The current ABSA works,
however, are mostly limited to the scenario of a single text piece, leaving the
study in dialogue contexts unexplored. To bridge the gap between fine-grained
sentiment analysis and conversational opinion mining, in this work, we
introduce a novel task of conversational aspect-based sentiment quadruple
analysis, namely DiaASQ, aiming to detect the quadruple of
target-aspect-opinion-sentiment in a dialogue. We manually construct a
large-scale high-quality DiaASQ dataset in both Chinese and English languages.
We deliberately develop a neural model to benchmark the task, which advances in
effectively performing end-to-end quadruple prediction, and manages to
incorporate rich dialogue-specific and discourse feature representations for
better cross-utterance quadruple extraction. We hope the new benchmark will
spur more advancements in the sentiment analysis community.",1,1,1,1,0,0,0.808276,4.0,0.723092,68
4e390e5e-ec1b-4618-a68c-fcf793be98e3,Contrastive Learning for Object Detection,1,0.00212706,0.0297614,"Contrastive learning is commonly used as a method of self-supervised learning
with the ""anchor"" and ""positive"" being two random augmentations of a given
input image, and the ""negative"" is the set of all other images. However, the
requirement of large batch sizes and memory banks has made it difficult and
slow to train. This has motivated the rise of Supervised Contrasative
approaches that overcome these problems by using annotated data. We look to
further improve supervised contrastive learning by ranking classes based on
their similarity, and observe the impact of human bias (in the form of ranking)
on the learned representations. We feel this is an important question to
address, as learning good feature embeddings has been a long sought after
problem in computer vision.",1,1,0,0,0,0,0.8551,6.0,0.841495,7
799a28be-65e9-44e9-8f5c-df448aea949f,Understanding the Covariance Structure of Convolutional Filters,8,0.542788,0.523239,"Neural network weights are typically initialized at random from univariate
distributions, controlling just the variance of individual weights even in
highly-structured operations like convolutions. Recent ViT-inspired
convolutional networks such as ConvMixer and ConvNeXt use large-kernel
depthwise convolutions whose learned filters have notable structure; this
presents an opportunity to study their empirical covariances. In this work, we
first observe that such learned filters have highly-structured covariance
matrices, and moreover, we find that covariances calculated from small networks
may be used to effectively initialize a variety of larger networks of different
depths, widths, patch sizes, and kernel sizes, indicating a degree of
model-independence to the covariance structure. Motivated by these findings, we
then propose a learning-free multivariate initialization scheme for
convolutional filters using a simple, closed-form construction of their
covariance. Models using our initialization outperform those using traditional
univariate initializations, and typically meet or exceed the performance of
those initialized from the covariances of learned filters; in some cases, this
improvement can be achieved without training the depthwise convolutional
filters at all.",1,0,0,0,0,0,0.992834,3.0,0.995867,19
adaa5058-d491-4c9f-99bf-1182a10fb44e,Deep Learning Computer Vision Algorithms for Real-time UAVs On-board Camera Image Processing,1,0.0481925,0.125374,"This paper describes how advanced deep learning based computer vision
algorithms are applied to enable real-time on-board sensor processing for small
UAVs. Four use cases are considered: target detection, classification and
localization, road segmentation for autonomous navigation in GNSS-denied zones,
human body segmentation, and human action recognition. All algorithms have been
developed using state-of-the-art image processing methods based on deep neural
networks. Acquisition campaigns have been carried out to collect custom
datasets reflecting typical operational scenarios, where the peculiar point of
view of a multi-rotor UAV is replicated. Algorithms architectures and trained
models performances are reported, showing high levels of both accuracy and
inference speed. Output examples and on-field videos are presented,
demonstrating models operation when deployed on a GPU-powered commercial
embedded device (NVIDIA Jetson Xavier) mounted on board of a custom quad-rotor,
paving the way to enabling high level autonomy.",0,1,0,0,0,0,0.952406,12.0,0.958673,59
18dd46a2-bdda-4061-944d-0392f264398e,Correcting the Sub-optimal Bit Allocation,1,0.011438,0.0675683,"In this paper, we investigate the problem of bit allocation in Neural Video
Compression (NVC). First, we reveal that a recent bit allocation approach
claimed to be optimal is, in fact, sub-optimal due to its implementation.
Specifically, we find that its sub-optimality lies in the improper application
of semi-amortized variational inference (SAVI) on latent with non-factorized
variational posterior. Then, we show that the corrected version of SAVI on
non-factorized latent requires recursively applying back-propagating through
gradient ascent, based on which we derive the corrected optimal bit allocation
algorithm. Due to the computational in-feasibility of the corrected bit
allocation, we design an efficient approximation to make it practical.
Empirical results show that our proposed correction significantly improves the
incorrect bit allocation in terms of R-D performance and bitrate error, and
outperforms all other bit allocation methods by a large margin. The source code
is provided in the supplementary material.",0,1,0,0,0,0,0.463992,8.0,0.739791,31
6997bec1-094c-40bf-8595-366fa3b63c08,TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters,14,0.150477,0.361784,"Topic taxonomies, which represent the latent topic (or category) structure of
document collections, provide valuable knowledge of contents in many
applications such as web search and information filtering. Recently, several
unsupervised methods have been developed to automatically construct the topic
taxonomy from a text corpus, but it is challenging to generate the desired
taxonomy without any prior knowledge. In this paper, we study how to leverage
the partial (or incomplete) information about the topic structure as guidance
to find out the complete topic taxonomy. We propose a novel framework for topic
taxonomy completion, named TaxoCom, which recursively expands the topic
taxonomy by discovering novel sub-topic clusters of terms and documents. To
effectively identify novel topics within a hierarchical topic structure,
TaxoCom devises its embedding and clustering techniques to be closely-linked
with each other: (i) locally discriminative embedding optimizes the text
embedding space to be discriminative among known (i.e., given) sub-topics, and
(ii) novelty adaptive clustering assigns terms into either one of the known
sub-topics or novel sub-topics. Our comprehensive experiments on two real-world
datasets demonstrate that TaxoCom not only generates the high-quality topic
taxonomy in terms of term coherency and topic coverage but also outperforms all
other baselines for a downstream task.",0,0,1,0,0,0,0.200743,8.0,0.611847,48
c8754536-c09b-4728-8a0b-9abcb3f8a39d,Learning Quantum Entanglement Distillation with Noisy Classical Communications,5,0.0817754,0.559702,"Quantum networking relies on the management and exploitation of entanglement.
Practical sources of entangled qubits are imperfect, producing mixed quantum
state with reduced fidelity with respect to ideal Bell pairs. Therefore, an
important primitive for quantum networking is entanglement distillation, whose
goal is to enhance the fidelity of entangled qubits through local operations
and classical communication (LOCC). Existing distillation protocols assume the
availability of ideal, noiseless, communication channels. In this paper, we
study the case in which communication takes place over noisy binary symmetric
channels. We propose to implement local processing through parameterized
quantum circuits (PQCs) that are optimized to maximize the average fidelity,
while accounting for communication errors. The introduced approach, Noise
Aware-LOCCNet (NA-LOCCNet), is shown to have significant advantages over
existing protocols designed for noiseless communications.",1,1,0,0,0,0,0.37494,9.0,0.737261,22
4bf45bdb-f455-4152-b5ce-803fb1d5cfd1,"CounterGeDi: A controllable approach to generate polite, detoxified and emotional counterspeech",18,0.0797447,0.787466,"Recently, many studies have tried to create generation models to assist
counter speakers by providing counterspeech suggestions for combating the
explosive proliferation of online hate. However, since these suggestions are
from a vanilla generation model, they might not include the appropriate
properties required to counter a particular hate speech instance. In this
paper, we propose CounterGeDi - an ensemble of generative discriminators (GeDi)
to guide the generation of a DialoGPT model toward more polite, detoxified, and
emotionally laden counterspeech. We generate counterspeech using three datasets
and observe significant improvement across different attribute scores. The
politeness and detoxification scores increased by around 15% and 6%
respectively, while the emotion in the counterspeech increased by at least 10%
across all the datasets. We also experiment with triple-attribute control and
observe significant improvement over single attribute results when combining
complementing attributes, e.g., politeness, joyfulness and detoxification. In
all these experiments, the relevancy of the generated text does not deteriorate
due to the application of these controls",0,1,0,0,0,0,0.226461,6.0,0.505171,29
288b31f6-9253-438a-b12b-06e4f7fc3ef4,Learning Instrumental Variable from Data Fusion for Treatment Effect Estimation,2,0.0163332,0.10693,"The advent of the big data era brought new opportunities and challenges to
draw treatment effect in data fusion, that is, a mixed dataset collected from
multiple sources (each source with an independent treatment assignment
mechanism). Due to possibly omitted source labels and unmeasured confounders,
traditional methods cannot estimate individual treatment assignment probability
and infer treatment effect effectively. Therefore, we propose to reconstruct
the source label and model it as a Group Instrumental Variable (GIV) to
implement IV-based Regression for treatment effect estimation. In this paper,
we conceptualize this line of thought and develop a unified framework (Meta-EM)
to (1) map the raw data into a representation space to construct Linear Mixed
Models for the assigned treatment variable; (2) estimate the distribution
differences and model the GIV for the different treatment assignment
mechanisms; and (3) adopt an alternating training strategy to iteratively
optimize the representations and the joint distribution to model GIV for IV
regression. Empirical results demonstrate the advantages of our Meta-EM
compared with state-of-the-art methods.",1,0,0,0,1,0,0.141922,6.0,0.41894,44
ba50e50a-0373-4a60-90d3-933903de6c1a,ImLiDAR: Cross-Sensor Dynamic Message Propagation Network for 3D Object Detection,3,0.0139808,0.248871,"LiDAR and camera, as two different sensors, supply geometric (point clouds)
and semantic (RGB images) information of 3D scenes. However, it is still
challenging for existing methods to fuse data from the two cross sensors,
making them complementary for quality 3D object detection (3OD). We propose
ImLiDAR, a new 3OD paradigm to narrow the cross-sensor discrepancies by
progressively fusing the multi-scale features of camera Images and LiDAR point
clouds. ImLiDAR enables to provide the detection head with cross-sensor yet
robustly fused features. To achieve this, two core designs exist in ImLiDAR.
First, we propose a cross-sensor dynamic message propagation module to combine
the best of the multi-scale image and point features. Second, we raise a direct
set prediction problem that allows designing an effective set-based detector to
tackle the inconsistency of the classification and localization confidences,
and the sensitivity of hand-tuned hyperparameters. Besides, the novel set-based
detector can be detachable and easily integrated into various detection
networks. Comparisons on both the KITTI and SUN-RGBD datasets show clear visual
and numerical improvements of our ImLiDAR over twenty-three state-of-the-art
3OD methods.",0,1,0,0,1,0,0.399034,7.0,0.673669,94
29de8a5b-b176-41af-8e42-c1a46ed9e96f,Word-Level Fine-Grained Story Visualization,14,0.339518,0.681272,"Story visualization aims to generate a sequence of images to narrate each
sentence in a multi-sentence story with a global consistency across dynamic
scenes and characters. Current works still struggle with output images' quality
and consistency, and rely on additional semantic information or auxiliary
captioning networks. To address these challenges, we first introduce a new
sentence representation, which incorporates word information from all story
sentences to mitigate the inconsistency problem. Then, we propose a new
discriminator with fusion features and further extend the spatial attention to
improve image quality and story consistency. Extensive experiments on different
datasets and human evaluation demonstrate the superior performance of our
approach, compared to state-of-the-art methods, neither using segmentation
masks nor auxiliary captioning networks.",1,0,0,0,1,0,0.87592,7.0,0.875173,32
a1702cce-fe28-432a-ac04-3fbd5881ee75,Sequential Causal Imitation Learning with Unobserved Confounders,28,0.263758,0.903047,"""Monkey see monkey do"" is an age-old adage, referring to na\""ive imitation
without a deep understanding of a system's underlying mechanics. Indeed, if a
demonstrator has access to information unavailable to the imitator (monkey),
such as a different set of sensors, then no matter how perfectly the imitator
models its perceived environment (See), attempting to reproduce the
demonstrator's behavior (Do) can lead to poor outcomes. Imitation learning in
the presence of a mismatch between demonstrator and imitator has been studied
in the literature under the rubric of causal imitation learning (Zhang et al.,
2020), but existing solutions are limited to single-stage decision-making. This
paper investigates the problem of causal imitation learning in sequential
settings, where the imitator must make multiple decisions per episode. We
develop a graphical criterion that is necessary and sufficient for determining
the feasibility of causal imitation, providing conditions when an imitator can
match a demonstrator's performance despite differing capabilities. Finally, we
provide an efficient algorithm for determining imitability and corroborate our
theory with simulations.",0,0,0,0,0,0,0.0274348,29.0,0.820983,29
825c28be-3800-4770-a8e4-92cc6027c61d,SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields,22,0.186906,0.93809,"3D reconstruction from a single 2D image was extensively covered in the
literature but relies on depth supervision at training time, which limits its
applicability. To relax the dependence to depth we propose SceneRF, a
self-supervised monocular scene reconstruction method using only posed image
sequences for training. Fueled by the recent progress in neural radiance fields
(NeRF) we optimize a radiance field though with explicit depth optimization and
a novel probabilistic sampling strategy to efficiently handle large scenes. At
inference, a single input image suffices to hallucinate novel depth views which
are fused together to obtain 3D scene reconstruction. Thorough experiments
demonstrate that we outperform all baselines for novel depth views synthesis
and scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI.
Code is available at https://astra-vision.github.io/SceneRF .",1,1,0,0,1,0,0.563253,6.0,0.700384,102
9037400b-f953-4341-8dde-3d85e977bdc8,DDIPNet and DDIPNet+: Discriminant Deep Image Prior Networks for Remote Sensing Image Classification,4,0.0249295,0.176068,"Research on remote sensing image classification significantly impacts
essential human routine tasks such as urban planning and agriculture. Nowadays,
the rapid advance in technology and the availability of many high-quality
remote sensing images create a demand for reliable automation methods. The
current paper proposes two novel deep learning-based architectures for image
classification purposes, i.e., the Discriminant Deep Image Prior Network and
the Discriminant Deep Image Prior Network+, which combine Deep Image Prior and
Triplet Networks learning strategies. Experiments conducted over three
well-known public remote sensing image datasets achieved state-of-the-art
results, evidencing the effectiveness of using deep image priors for remote
sensing image classification.",0,1,0,0,1,0,0.390596,9.0,0.743102,16
0d644702-2c43-4219-bdeb-fa360d45ecdc,Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian,2,0.00776153,0.110233,"We propose a framework that can deform an object in a 2D image as it exists
in 3D space. Most existing methods for 3D-aware image manipulation are limited
to (1) only changing the global scene information or depth, or (2) manipulating
an object of specific categories. In this paper, we present a 3D-aware image
deformation method with minimal restrictions on shape category and deformation
type. While our framework leverages 2D-to-3D reconstruction, we argue that
reconstruction is not sufficient for realistic deformations due to the
vulnerability to topological errors. Thus, we propose to take a supervised
learning-based approach to predict the shape Laplacian of the underlying volume
of a 3D reconstruction represented as a point cloud. Given the deformation
energy calculated using the predicted shape Laplacian and user-defined
deformation handles (e.g., keypoints), we obtain bounded biharmonic weights to
model plausible handle-based image deformation. In the experiments, we present
our results of deforming 2D character and clothed human images. We also
quantitatively show that our approach can produce more accurate deformation
weights compared to alternative methods (i.e., mesh reconstruction and point
cloud Laplacian methods).",0,0,0,0,0,0,0.225524,7.0,0.575185,58
714f8f51-3b62-4ac3-91f7-7b84b9c0b484,Towards Federated Long-Tailed Learning,6,0.162614,0.466454,"Data privacy and class imbalance are the norm rather than the exception in
many machine learning tasks. Recent attempts have been launched to, on one
side, address the problem of learning from pervasive private data, and on the
other side, learn from long-tailed data. However, both assumptions might hold
in practical applications, while an effective method to simultaneously
alleviate both issues is yet under development. In this paper, we focus on
learning with long-tailed (LT) data distributions under the context of the
popular privacy-preserved federated learning (FL) framework. We characterize
three scenarios with different local or global long-tailed data distributions
in the FL framework, and highlight the corresponding challenges. The
preliminary results under different scenarios reveal that substantial future
work are of high necessity to better resolve the characterized federated
long-tailed learning tasks.",0,0,0,0,0,0,0.986576,6.0,0.975278,35
5cc75719-54ae-42be-a984-73644f114ffc,Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images,9,0.156733,0.580896,"Developing an AI-assisted gland segmentation method from histology images is
critical for automatic cancer diagnosis and prognosis; however, the high cost
of pixel-level annotations hinders its applications to broader diseases.
Existing weakly-supervised semantic segmentation methods in computer vision
achieve degenerative results for gland segmentation, since the characteristics
and problems of glandular datasets are different from general object datasets.
We observe that, unlike natural images, the key problem with histology images
is the confusion of classes owning to morphological homogeneity and low color
contrast among different tissues. To this end, we propose a novel method Online
Easy Example Mining (OEEM) that encourages the network to focus on credible
supervision signals rather than noisy signals, therefore mitigating the
influence of inevitable false predictions in pseudo-masks. According to the
characteristics of glandular datasets, we design a strong framework for gland
segmentation. Our results exceed many fully-supervised methods and
weakly-supervised methods for gland segmentation over 4.4% and 6.04% at mIoU,
respectively. Code is available at https://github.com/xmed-lab/OEEM.",1,1,1,0,1,0,0.96614,9.0,0.956674,23
862b9d41-4b8c-4dbc-bb74-3eecbe864fab,CITRIS: Causal Identifiability from Temporal Intervened Sequences,61,0.374327,0.804722,"Understanding the latent causal factors of a dynamical system from visual
observations is considered a crucial step towards agents reasoning in complex
environments. In this paper, we propose CITRIS, a variational autoencoder
framework that learns causal representations from temporal sequences of images
in which underlying causal factors have possibly been intervened upon. In
contrast to the recent literature, CITRIS exploits temporality and observing
intervention targets to identify scalar and multidimensional causal factors,
such as 3D rotation angles. Furthermore, by introducing a normalizing flow,
CITRIS can be easily extended to leverage and disentangle representations
obtained by already pretrained autoencoders. Extending previous results on
scalar causal factors, we prove identifiability in a more general setting, in
which only some components of a causal factor are affected by interventions. In
experiments on 3D rendered image sequences, CITRIS outperforms previous methods
on recovering the underlying causal variables. Moreover, using pretrained
autoencoders, CITRIS can even generalize to unseen instantiations of causal
factors, opening future research areas in sim-to-real generalization for causal
representation learning.",1,0,1,0,0,0,0.360619,11.0,0.78054,69
973973d9-3b0c-450e-acd6-ec34d613e3ce,FaceDancer: Pose- and Occlusion-Aware High Fidelity Face Swapping,11,0.185335,0.546002,"In this work, we present a new single-stage method for subject agnostic face
swapping and identity transfer, named FaceDancer. We have two major
contributions: Adaptive Feature Fusion Attention (AFFA) and Interpreted Feature
Similarity Regularization (IFSR). The AFFA module is embedded in the decoder
and adaptively learns to fuse attribute features and features conditioned on
identity information without requiring any additional facial segmentation
process. In IFSR, we leverage the intermediate features in an identity encoder
to preserve important attributes such as head pose, facial expression,
lighting, and occlusion in the target face, while still transferring the
identity of the source face with high fidelity. We conduct extensive
quantitative and qualitative experiments on various datasets and show that the
proposed FaceDancer outperforms other state-of-the-art networks in terms of
identityn transfer, while having significantly better pose preservation than
most of the previous methods.",1,1,0,0,1,0,0.894265,10.0,0.920008,46
327c911e-bdc9-42da-aee8-9748ac121490,Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation,1,0.0131127,0.138044,"Recently, Synthetic data-based Instance Segmentation has become an
exceedingly favorable optimization paradigm since it leverages simulation
rendering and physics to generate high-quality image-annotation pairs. In this
paper, we propose a Parallel Pre-trained Transformers (PPT) framework to
accomplish the synthetic data-based Instance Segmentation task. Specifically,
we leverage the off-the-shelf pre-trained vision Transformers to alleviate the
gap between natural and synthetic data, which helps to provide good
generalization in the downstream synthetic data scene with few samples.
Swin-B-based CBNet V2, SwinL-based CBNet V2 and Swin-L-based Uniformer are
employed for parallel feature learning, and the results of these three models
are fused by pixel-level Non-maximum Suppression (NMS) algorithm to obtain more
robust results. The experimental results reveal that PPT ranks first in the
CVPR2022 AVA Accessibility Vision and Autonomy Challenge, with a 65.155% mAP.",0,1,0,0,1,0,0.931996,6.0,0.896572,18
26b6ac59-0dcd-4a0e-af68-d4f9a8118180,Generalized Probabilistic U-Net for medical image segementation,4,0.0229528,0.242389,"We propose the Generalized Probabilistic U-Net, which extends the
Probabilistic U-Net by allowing more general forms of the Gaussian distribution
as the latent space distribution that can better approximate the uncertainty in
the reference segmentations. We study the effect the choice of latent space
distribution has on capturing the uncertainty in the reference segmentations
using the LIDC-IDRI dataset. We show that the choice of distribution affects
the sample diversity of the predictions and their overlap with respect to the
reference segmentations. For the LIDC-IDRI dataset, we show that using a
mixture of Gaussians results in a statistically significant improvement in the
generalized energy distance (GED) metric with respect to the standard
Probabilistic U-Net. We have made our implementation available at
https://github.com/ishaanb92/GeneralizedProbabilisticUNet",1,0,0,0,0,0,0.144106,12.0,0.710846,28
9bef7aa0-a9ae-4267-868c-5711f9635592,How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image,4,0.119938,0.406163,"This study investigates how fake news uses a thumbnail for a news article
with a focus on whether a news article's thumbnail represents the news content
correctly. A news article shared with an irrelevant thumbnail can mislead
readers into having a wrong impression of the issue, especially in social media
environments where users are less likely to click the link and consume the
entire content. We propose to capture the degree of semantic incongruity in the
multimodal relation by using the pretrained CLIP representation. From a
source-level analysis, we found that fake news employs a more incongruous image
to the main content than general news. Going further, we attempted to detect
news articles with image-text incongruity. Evaluation experiments suggest that
CLIP-based methods can successfully detect news articles in which the thumbnail
is semantically irrelevant to news text. This study contributes to the research
by providing a novel view on tackling online fake news and misinformation. Code
and datasets are available at
https://github.com/ssu-humane/fake-news-thumbnail.",0,1,0,0,0,0,0.929096,6.0,0.893962,29
954af74c-2d8a-42d8-8345-5f506187fc75,Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs,27,0.0554356,0.529341,"Question answering over temporal knowledge graphs (KGs) efficiently uses
facts contained in a temporal KG, which records entity relations and when they
occur in time, to answer natural language questions (e.g., ""Who was the
president of the US before Obama?""). These questions often involve three
time-related challenges that previous work fail to adequately address: 1)
questions often do not specify exact timestamps of interest (e.g., ""Obama""
instead of 2000); 2) subtle lexical differences in time relations (e.g.,
""before"" vs ""after""); 3) off-the-shelf temporal KG embeddings that previous
work builds on ignore the temporal order of timestamps, which is crucial for
answering temporal-order related questions. In this paper, we propose a
time-sensitive question answering (TSQA) framework to tackle these problems.
TSQA features a timestamp estimation module to infer the unwritten timestamp
from the question. We also employ a time-sensitive KG encoder to inject
ordering information into the temporal KG embeddings that TSQA is based on.
With the help of techniques to reduce the search space for potential answers,
TSQA significantly outperforms the previous state of the art on a new benchmark
for question answering over temporal KGs, especially achieving a 32% (absolute)
error reduction on complex questions that require multiple steps of reasoning
over facts in the temporal KG.",0,1,0,0,1,0,0.0924251,7.0,0.436758,29
6643eaa7-e186-4d49-b71f-add0109eb229,MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid,18,0.681276,0.77728,"Multi-modal entity alignment (MMEA) aims to discover identical entities
across different knowledge graphs (KGs) whose entities are associated with
relevant images. However, current MMEA algorithms rely on KG-level modality
fusion strategies for multi-modal entity representation, which ignores the
variations of modality preferences of different entities, thus compromising
robustness against noise in modalities such as blurry images and relations.
This paper introduces MEAformer, a multi-modal entity alignment transformer
approach for meta modality hybrid, which dynamically predicts the mutual
correlation coefficients among modalities for more fine-grained entity-level
modality fusion and alignment. Experimental results demonstrate that our model
not only achieves SOTA performance in multiple training scenarios, including
supervised, unsupervised, iterative, and low-resource settings, but also has a
limited number of parameters, efficient runtime, and interpretability. Our code
is available at https://github.com/zjukg/MEAformer.",0,0,0,0,1,0,0.936339,6.0,0.900614,69
90452690-cec6-4261-9ee9-cd3354238d3d,A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction,21,0.765925,0.985451,"The recent advances of deep learning have dramatically changed how machine
learning, especially in the domain of natural language processing, can be
applied to legal domain. However, this shift to the data-driven approaches
calls for larger and more diverse datasets, which are nevertheless still small
in number, especially in non-English languages. Here we present the first
large-scale benchmark of Korean legal AI datasets, LBOX OPEN, that consists of
one legal corpus, two classification tasks, two legal judgement prediction
(LJP) tasks, and one summarization task. The legal corpus consists of 147k
Korean precedents (259M tokens), of which 63k are sentenced in last 4 years and
96k are from the first and the second level courts in which factual issues are
reviewed. The two classification tasks are case names (11.3k) and statutes
(2.8k) prediction from the factual description of individual cases. The LJP
tasks consist of (1) 10.5k criminal examples where the model is asked to
predict fine amount, imprisonment with labor, and imprisonment without labor
ranges for the given facts, and (2) 4.7k civil examples where the inputs are
facts and claim for relief and outputs are the degrees of claim acceptance. The
summarization task consists of the Supreme Court precedents and the
corresponding summaries (20k). We also release realistic variants of the
datasets by extending the domain (1) to infrequent case categories in case name
(31k examples) and statute (17.7k) classification tasks, and (2) to long input
sequences in the summarization task (51k). Finally, we release LCUBE, the first
Korean legal language model trained on the legal corpus from this study. Given
the uniqueness of the Law of South Korea and the diversity of the legal tasks
covered in this work, we believe that LBOX OPEN contributes to the
multilinguality of global legal research. LBOX OPEN and LCUBE will be publicly
available.",1,1,0,1,0,0,0.968405,3.0,0.87677,60
6932aa1a-5ff3-4113-abd8-c577ca893adc,Monant Medical Misinformation Dataset: Mapping Articles to Fact-Checked Claims,10,0.0438831,0.604618,"False information has a significant negative influence on individuals as well
as on the whole society. Especially in the current COVID-19 era, we witness an
unprecedented growth of medical misinformation. To help tackle this problem
with machine learning approaches, we are publishing a feature-rich dataset of
approx. 317k medical news articles/blogs and 3.5k fact-checked claims. It also
contains 573 manually and more than 51k automatically labelled mappings between
claims and articles. Mappings consist of claim presence, i.e., whether a claim
is contained in a given article, and article stance towards the claim. We
provide several baselines for these two tasks and evaluate them on the manually
labelled part of the dataset. The dataset enables a number of additional tasks
related to medical misinformation, such as misinformation characterisation
studies or studies of misinformation diffusion between sources.",1,1,1,1,0,0,0.257442,5.0,0.435748,53
7a1209b5-b398-4aba-926d-2fe67a819a6d,Phantom -- A RL-driven multi-agent framework to model complex systems,5,0.0185159,0.300345,"Agent based modelling (ABM) is a computational approach to modelling complex
systems by specifying the behaviour of autonomous decision-making components or
agents in the system and allowing the system dynamics to emerge from their
interactions. Recent advances in the field of Multi-agent reinforcement
learning (MARL) have made it feasible to study the equilibrium of complex
environments where multiple agents learn simultaneously. However, most ABM
frameworks are not RL-native, in that they do not offer concepts and interfaces
that are compatible with the use of MARL to learn agent behaviours. In this
paper, we introduce a new open-source framework, Phantom, to bridge the gap
between ABM and MARL. Phantom is an RL-driven framework for agent-based
modelling of complex multi-agent systems including, but not limited to economic
systems and markets. The framework aims to provide the tools to simplify the
ABM specification in a MARL-compatible way - including features to encode
dynamic partial observability, agent utility functions, heterogeneity in agent
preferences or types, and constraints on the order in which agents can act
(e.g. Stackelberg games, or more complex turn-taking environments). In this
paper, we present these features, their design rationale and present two new
environments leveraging the framework.",0,1,0,0,0,0,0.0793652,8.0,0.487241,31
6188c9b1-2a5f-48b8-a17e-f3e10358fed0,FusionVAE: A Deep Hierarchical Variational Autoencoder for RGB Image Fusion,1,0.0,0.069257,"Sensor fusion can significantly improve the performance of many computer
vision tasks. However, traditional fusion approaches are either not data-driven
and cannot exploit prior knowledge nor find regularities in a given dataset or
they are restricted to a single application. We overcome this shortcoming by
presenting a novel deep hierarchical variational autoencoder called FusionVAE
that can serve as a basis for many fusion tasks. Our approach is able to
generate diverse image samples that are conditioned on multiple noisy,
occluded, or only partially visible input images. We derive and optimize a
variational lower bound for the conditional log-likelihood of FusionVAE. In
order to assess the fusion capabilities of our model thoroughly, we created
three novel datasets for image fusion based on popular computer vision
datasets. In our experiments, we show that FusionVAE learns a representation of
aggregated information that is relevant to fusion tasks. The results
demonstrate that our approach outperforms traditional methods significantly.
Furthermore, we present the advantages and disadvantages of different design
choices.",0,0,0,1,0,0,0.912388,11.0,0.934586,73
45abe536-ba12-4cf8-8793-51a74c150ba3,Forming Effective Human-AI Teams: Building Machine Learning Models that Complement the Capabilities of Multiple Experts,19,0.557479,0.603157,"Machine learning (ML) models are increasingly being used in application
domains that often involve working together with human experts. In this
context, it can be advantageous to defer certain instances to a single human
expert when they are difficult to predict for the ML model. While previous work
has focused on scenarios with one distinct human expert, in many real-world
situations several human experts with varying capabilities may be available. In
this work, we propose an approach that trains a classification model to
complement the capabilities of multiple human experts. By jointly training the
classifier together with an allocation system, the classifier learns to
accurately predict those instances that are difficult for the human experts,
while the allocation system learns to pass each instance to the most suitable
team member -- either the classifier or one of the human experts. We evaluate
our proposed approach in multiple experiments on public datasets with
""synthetic"" experts and a real-world medical dataset annotated by multiple
radiologists. Our approach outperforms prior work and is more accurate than the
best human expert or a classifier. Furthermore, it is flexibly adaptable to
teams of varying sizes and different levels of expert diversity.",1,1,0,0,1,0,0.981406,7.0,0.967583,30
2693b2f3-9fd4-42fe-a57f-fc858b1d3ba7,Style-ERD: Responsive and Coherent Online Motion Style Transfer,7,0.0781874,0.501096,"Motion style transfer is a common method for enriching character animation.
Motion style transfer algorithms are often designed for offline settings where
motions are processed in segments. However, for online animation applications,
such as realtime avatar animation from motion capture, motions need to be
processed as a stream with minimal latency. In this work, we realize a
flexible, high-quality motion style transfer method for this setting. We
propose a novel style transfer model, Style-ERD, to stylize motions in an
online manner with an Encoder-Recurrent-Decoder structure, along with a novel
discriminator that combines feature attention and temporal attention. Our
method stylizes motions into multiple target styles with a unified model.
Although our method targets online settings, it outperforms previous offline
methods in motion realism and style expressiveness and provides significant
gains in runtime efficiency",0,0,0,0,0,0,0.28416,10.0,0.729484,64
807f3311-42b0-439d-a522-4f09bb1a851c,Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation,8,0.0724021,0.241894,"We introduce Action-GPT, a plug-and-play framework for incorporating Large
Language Models (LLMs) into text-based action generation models. Action phrases
in current motion capture datasets contain minimal and to-the-point
information. By carefully crafting prompts for LLMs, we generate richer and
fine-grained descriptions of the action. We show that utilizing these detailed
descriptions instead of the original action phrases leads to better alignment
of text and motion spaces. We introduce a generic approach compatible with
stochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion
models. In addition, the approach enables multiple text descriptions to be
utilized. Our experiments show (i) noticeable qualitative and quantitative
improvement in the quality of synthesized motions, (ii) benefits of utilizing
multiple LLM-generated descriptions, (iii) suitability of the prompt function,
and (iv) zero-shot generation capabilities of the proposed approach. Project
page: https://actiongpt.github.io",1,1,0,0,0,0,0.804957,4.0,0.72048,18
0fb8c7dc-1299-490b-a1ec-da4b859e0bad,Exhaustivity and anti-exhaustivity in the RSA framework: Testing the effect of prior beliefs,4,0.0110394,0.318134,"During communication, the interpretation of utterances is sensitive to a
listener's probabilistic prior beliefs, something which is captured by one
currently influential model of pragmatics, the Rational Speech Act (RSA)
framework. In this paper we focus on cases when this sensitivity to priors
leads to counterintuitive predictions of the framework. Our domain of interest
is exhaustivity effects, whereby a sentence such as ""Mary came"" is understood
to mean that only Mary came. We show that in the baseline RSA model, under
certain conditions, anti-exhaustive readings are predicted (e.g., ""Mary came""
would be used to convey that both Mary and Peter came). The specific question
we ask is the following: should exhaustive interpretations be derived as purely
pragmatic inferences (as in the classical Gricean view, endorsed in the
baseline RSA model), or should they rather be generated by an encapsulated
semantic mechanism (as argued in some of the recent formal literature)? To
answer this question, we provide a detailed theoretical analysis of different
RSA models and evaluate them against data obtained in a new study which tested
the effects of prior beliefs on both production and comprehension, improving on
previous empirical work. We found no anti-exhaustivity effects, but observed
that message choice is sensitive to priors, as predicted by the RSA framework
overall. The best models turn out to be those which include an encapsulated
exhaustivity mechanism (as other studies concluded on the basis of very
different data). We conclude that, on the one hand, in the division of labor
between semantics and pragmatics, semantics plays a larger role than is often
thought, but, on the other hand, the tradeoff between informativity and cost
which characterizes all RSA models does play a central role for genuine
pragmatic effects.",1,0,0,0,0,0,0.000236479,14.0,0.288645,56
59ddfbae-f7d6-4dcb-b6ea-7537f728a153,Narrowing the Gap: Improved Detector Training with Noisy Location Annotations,7,0.0470648,0.627523,"Deep learning methods require massive of annotated data for optimizing
parameters. For example, datasets attached with accurate bounding box
annotations are essential for modern object detection tasks. However, labeling
with such pixel-wise accuracy is laborious and time-consuming, and elaborate
labeling procedures are indispensable for reducing man-made noise, involving
annotation review and acceptance testing. In this paper, we focus on the impact
of noisy location annotations on the performance of object detection approaches
and aim to, on the user side, reduce the adverse effect of the noise. First,
noticeable performance degradation is experimentally observed for both
one-stage and two-stage detectors when noise is introduced to the bounding box
annotations. For instance, our synthesized noise results in performance
decrease from 38.9% AP to 33.6% AP for FCOS detector on COCO test split, and
37.8%AP to 33.7%AP for Faster R-CNN. Second, a self-correction technique based
on a Bayesian filter for prediction ensemble is proposed to better exploit the
noisy location annotations following a Teacher-Student learning paradigm.
Experiments for both synthesized and real-world scenarios consistently
demonstrate the effectiveness of our approach, e.g., our method increases the
degraded performance of the FCOS detector from 33.6% AP to 35.6% AP on COCO.",1,1,0,0,0,0,0.850667,10.0,0.903325,58
224bf1ac-8197-461c-9981-793d9573f086,Learning to Express in Knowledge-Grounded Conversation,5,0.0183781,0.340851,"Grounding dialogue generation by extra knowledge has shown great potentials
towards building a system capable of replying with knowledgeable and engaging
responses. Existing studies focus on how to synthesize a response with proper
knowledge, yet neglect that the same knowledge could be expressed differently
by speakers even under the same context. In this work, we mainly consider two
aspects of knowledge expression, namely the structure of the response and style
of the content in each part. We therefore introduce two sequential latent
variables to represent the structure and the content style respectively. We
propose a segmentation-based generation model and optimize the model by a
variational approach to discover the underlying pattern of knowledge expression
in a response. Evaluation results on two benchmarks indicate that our model can
learn the structure style defined by a few examples and generate responses in
desired content style.",0,0,0,0,1,0,0.270794,8.0,0.654739,58
18ebf9d7-e340-4938-adb5-aba6bcec09ad,Tools and Practices for Responsible AI Engineering,9,0.303676,0.612201,"Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs.",0,1,0,0,0,0,0.969364,5.0,0.927839,33
961c2344-e38e-4f2e-a534-d382d172c0b5,Query-based Industrial Analytics over Knowledge Graphs with Ontology Reshaping,12,0.121469,0.706386,"Industrial analytics that includes among others equipment diagnosis and
anomaly detection heavily relies on integration of heterogeneous production
data. Knowledge Graphs (KGs) as the data format and ontologies as the unified
data schemata are a prominent solution that offers high quality data
integration and a convenient and standardised way to exchange data and to layer
analytical applications over it. However, poor design of ontologies of high
degree of mismatch between them and industrial data naturally lead to KGs of
low quality that impede the adoption and scalability of industrial analytics.
Indeed, such KGs substantially increase the training time of writing queries
for users, consume high volume of storage for redundant information, and are
hard to maintain and update. To address this problem we propose an ontology
reshaping approach to transform ontologies into KG schemata that better reflect
the underlying data and thus help to construct better KGs. In this poster we
present a preliminary discussion of our on-going research, evaluate our
approach with a rich set of SPARQL queries on real-world industry data at Bosch
and discuss our findings.",0,1,0,0,0,0,0.127015,6.0,0.399043,18
3cfd3f3e-f284-4a51-84dd-4c70b776ca31,Informal Persian Universal Dependency Treebank,1,0.016788,0.0297252,"This paper presents the phonological, morphological, and syntactic
distinctions between formal and informal Persian, showing that these two
variants have fundamental differences that cannot be attributed solely to
pronunciation discrepancies. Given that informal Persian exhibits particular
characteristics, any computational model trained on formal Persian is unlikely
to transfer well to informal Persian, necessitating the creation of dedicated
treebanks for this variety. We thus detail the development of the open-source
Informal Persian Universal Dependency Treebank, a new treebank annotated within
the Universal Dependencies scheme. We then investigate the parsing of informal
Persian by training two dependency parsers on existing formal treebanks and
evaluating them on out-of-domain data, i.e. the development set of our informal
treebank. Our results show that parsers experience a substantial performance
drop when we move across the two domains, as they face more unknown tokens and
structures and fail to generalize well. Furthermore, the dependency relations
whose performance deteriorates the most represent the unique properties of the
informal variant. The ultimate goal of this study that demonstrates a broader
impact is to provide a stepping-stone to reveal the significance of informal
variants of languages, which have been widely overlooked in natural language
processing tools across languages.",0,1,1,1,0,0,0.0127728,15.0,0.602436,39
36e17ce4-5171-41b7-bd72-a75614ccad1b,PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch,11,0.100312,0.820822,"Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a local region. Patch attacks can be highly effective in a
variety of tasks and physically realizable via attachment (e.g. a sticker) to
the real-world objects. Despite the diversity in attack patterns, adversarial
patches tend to be highly textured and different in appearance from natural
images. We exploit this property and present PatchZero, a general defense
pipeline against white-box adversarial patches without retraining the
downstream classifier or detector. Specifically, our defense detects
adversaries at the pixel-level and ""zeros out"" the patch region by repainting
with mean pixel values. We further design a two-stage adversarial training
scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA
defense performance on the image classification (ImageNet, RESISC45), object
detection (PASCAL VOC), and video classification (UCF101) tasks with little
degradation in benign performance. In addition, PatchZero transfers to
different patch shapes and attack types.",0,1,0,0,1,1,0.360803,7.0,0.655226,49
093dccfa-083e-4a9b-b155-d43e2ad339b5,Hierarchical Phrase-based Sequence-to-Sequence Learning,7,0.105139,0.675982,"We describe a neural transducer that maintains the flexibility of standard
sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases
as a source of inductive bias during training and as explicit constraints
during inference. Our approach trains two models: a discriminative parser based
on a bracketing transduction grammar whose derivation tree hierarchically
aligns source and target phrases, and a neural seq2seq model that learns to
translate the aligned phrases one-by-one. We use the same seq2seq model to
translate at all phrase scales, which results in two inference modes: one mode
in which the parser is discarded and only the seq2seq component is used at the
sequence-level, and another in which the parser is combined with the seq2seq
model. Decoding in the latter mode is done with the cube-pruned CKY algorithm,
which is more involved but can make use of new translation rules during
inference. We formalize our model as a source-conditioned synchronous grammar
and develop an efficient variational inference algorithm for training. When
applied on top of both randomly initialized and pretrained seq2seq models, we
find that both inference modes performs well compared to baselines on small
scale machine translation benchmarks.",1,0,0,0,0,0,0.265591,11.0,0.746831,73
bbd6ba64-dad1-4a8d-9622-0a752b511b57,Uncertainty-aware deep learning methods for robust diabetic retinopathy classification,18,0.0741023,0.766631,"Automatic classification of diabetic retinopathy from retinal images has been
widely studied using deep neural networks with impressive results. However,
there is a clinical need for estimation of the uncertainty in the
classifications, a shortcoming of modern neural networks. Recently, approximate
Bayesian deep learning methods have been proposed for the task but the studies
have only considered the binary referable/non-referable diabetic retinopathy
classification applied to benchmark datasets. We present novel results by
systematically investigating a clinical dataset and a clinically relevant
5-class classification scheme, in addition to benchmark datasets and the binary
classification scheme. Moreover, we derive a connection between uncertainty
measures and classifier risk, from which we develop a new uncertainty measure.
We observe that the previously proposed entropy-based uncertainty measure
generalizes to the clinical dataset on the binary classification scheme but not
on the 5-class scheme, whereas our new uncertainty measure generalizes to the
latter case.",0,1,0,0,0,0,0.311712,11.0,0.764169,39
be5c75d8-3d71-493d-b66c-e2081ff2734c,FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales,7,0.0732435,0.578565,"Following how humans communicate, free-text rationales aim to use natural
language to explain neural language model (LM) behavior. However, free-text
rationales' unconstrained nature makes them prone to hallucination, so it is
important to have metrics for free-text rationale quality. Existing free-text
rationale metrics measure how consistent the rationale is with the LM's
predicted label, but there is no protocol for assessing such metrics'
reliability. Thus, we propose FRAME, a framework for evaluating rationale-label
consistency (RLC) metrics for free-text rationales. FRAME is based on three
axioms: (1) good metrics should yield highest scores for reference rationales,
which maximize RLC by construction; (2) good metrics should be appropriately
sensitive to semantic perturbation of rationales; and (3) good metrics should
be robust to variation in the LM's task performance. Across three text
classification datasets, we show that existing RLC metrics cannot satisfy all
three FRAME axioms, since they are implemented via model pretraining which
muddles the metric's signal. Then, we introduce a non-pretraining RLC metric
that greatly outperforms baselines on (1) and (3), while performing
competitively on (2). Finally, we discuss the limitations of using RLC to
evaluate free-text rationales.",0,0,0,0,0,0,0.72668,6.0,0.775117,32
c9d6c739-0ffd-4ab1-8449-4c499b0329a2,Improving Fuzzy-Logic based Map-Matching Method with Trajectory Stay-Point Detection,2,0.0103773,0.215948,"The requirement to trace and process moving objects in the contemporary era
gradually increases since numerous applications quickly demand precise moving
object locations. The Map-matching method is employed as a preprocessing
technique, which matches a moving object point on a corresponding road.
However, most of the GPS trajectory datasets include stay-points irregularity,
which makes map-matching algorithms mismatch trajectories to irrelevant
streets. Therefore, determining the stay-point region in GPS trajectory
datasets results in better accurate matching and more rapid approaches. In this
work, we cluster stay-points in a trajectory dataset with DBSCAN and eliminate
redundant data to improve the efficiency of the map-matching algorithm by
lowering processing time. We reckoned our proposed method's performance and
exactness with a ground truth dataset compared to a fuzzy-logic based
map-matching algorithm. Fortunately, our approach yields 27.39% data size
reduction and 8.9% processing time reduction with the same accurate results as
the previous fuzzy-logic based map-matching approach.",0,1,0,0,0,0,7.99916e-05,21.0,0.474143,34
4fec8d23-c2a8-4cf2-918e-43bb06f824e3,DALL-E 2 Fails to Reliably Capture Common Syntactic Processes,22,0.0367884,0.457871,"Machine intelligence is increasingly being linked to claims about sentience,
language processing, and an ability to comprehend and transform natural
language into a range of stimuli. We systematically analyze the ability of
DALL-E 2 to capture 8 grammatical phenomena pertaining to compositionality that
are widely discussed in linguistics and pervasive in human language: binding
principles and coreference, passives, word order, coordination, comparatives,
negation, ellipsis, and structural ambiguity. Whereas young children routinely
master these phenomena, learning systematic mappings between syntax and
semantics, DALL-E 2 is unable to reliably infer meanings that are consistent
with the syntax. These results challenge recent claims concerning the capacity
of such systems to understand of human language. We make available the full set
of test materials as a benchmark for future testing.",0,0,0,0,0,0,0.0145362,7.0,0.16668,41
4da1de8b-9c8e-466b-a6a7-75016ee64165,Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering,6,0.0391367,0.664556,"Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a
two-stage framework that first retrieves external knowledge given the visual
question and then predicts the answer based on the retrieved content. However,
the retrieved knowledge is often inadequate. Retrievals are frequently too
general and fail to cover specific knowledge needed to answer the question.
Also, the naturally available supervision (whether the passage contains the
correct answer) is weak and does not guarantee question relevancy. To address
these issues, we propose an Entity-Focused Retrieval (EnFoRe) model that
provides stronger supervision during training and recognizes question-relevant
entities to help retrieve more specific knowledge. Experiments show that our
EnFoRe model achieves superior retrieval performance on OK-VQA, the currently
largest outside-knowledge VQA dataset. We also combine the retrieved knowledge
with state-of-the-art VQA models, and achieve a new state-of-the-art
performance on OK-VQA.",1,1,0,0,1,0,0.366371,7.0,0.657991,49
2afc570d-1c86-41dc-8733-d2516880ab29,Guided Depth Super-Resolution by Deep Anisotropic Diffusion,12,0.24442,0.892782,"Performing super-resolution of a depth image using the guidance from an RGB
image is a problem that concerns several fields, such as robotics, medical
imaging, and remote sensing. While deep learning methods have achieved good
results in this problem, recent work highlighted the value of combining modern
methods with more formal frameworks. In this work, we propose a novel approach
which combines guided anisotropic diffusion with a deep convolutional network
and advances the state of the art for guided depth super-resolution. The edge
transferring/enhancing properties of the diffusion are boosted by the
contextual reasoning capabilities of modern networks, and a strict adjustment
step guarantees perfect adherence to the source image. We achieve unprecedented
results in three commonly used benchmarks for guided depth super-resolution.
The performance gain compared to other methods is the largest at larger scales,
such as x32 scaling. Code
(https://github.com/prs-eth/Diffusion-Super-Resolution) for the proposed method
is available to promote reproducibility of our results.",0,1,0,0,1,0,0.109649,13.0,0.710584,53
42a26473-6c36-4b3b-acd1-c7db7d71ba58,Motion Policy Networks,21,0.409204,0.754901,"Collision-free motion generation in unknown environments is a core building
block for robot manipulation. Generating such motions is challenging due to
multiple objectives; not only should the solutions be optimal, the motion
generator itself must be fast enough for real-time performance and reliable
enough for practical deployment. A wide variety of methods have been proposed
ranging from local controllers to global planners, often being combined to
offset their shortcomings. We present an end-to-end neural model called Motion
Policy Networks (M$\pi$Nets) to generate collision-free, smooth motion from
just a single depth camera observation. M$\pi$Nets are trained on over 3
million motion planning problems in over 500,000 environments. Our experiments
show that M$\pi$Nets are significantly faster than global planners while
exhibiting the reactivity needed to deal with dynamic scenes. They are 46%
better than prior neural planners and more robust than local control policies.
Despite being only trained in simulation, M$\pi$Nets transfer well to the real
robot with noisy partial point clouds. Code and data are publicly available at
https://mpinets.github.io.",1,1,0,0,1,0,0.702359,11.0,0.871157,66
de7f2809-91d2-473b-8a8a-0221266ac00e,Deformable Butterfly: A Highly Structured and Sparse Linear Transform,13,0.0852236,0.715186,"We introduce a new kind of linear transform named Deformable Butterfly
(DeBut) that generalizes the conventional butterfly matrices and can be adapted
to various input-output dimensions. It inherits the fine-to-coarse-grained
learnable hierarchy of traditional butterflies and when deployed to neural
networks, the prominent structures and sparsity in a DeBut layer constitutes a
new way for network compression. We apply DeBut as a drop-in replacement of
standard fully connected and convolutional layers, and demonstrate its
superiority in homogenizing a neural network and rendering it favorable
properties such as light weight and low inference complexity, without
compromising accuracy. The natural complexity-accuracy tradeoff arising from
the myriad deformations of a DeBut layer also opens up new rooms for analytical
and practical research. The codes and Appendix are publicly available at:
https://github.com/ruilin0212/DeBut.",1,0,0,0,0,0,0.561662,14.0,0.871279,44
10535b95-ec1c-465e-8ff4-0a61ecc88f34,AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,9,0.386723,0.69306,"This paper proposes AdaTest, a novel adaptive test pattern generation
framework for efficient and reliable Hardware Trojan (HT) detection. HT is a
backdoor attack that tampers with the design of victim integrated circuits
(ICs). AdaTest improves the existing HT detection techniques in terms of
scalability and accuracy of detecting smaller Trojans in the presence of noise
and variations. To achieve high trigger coverage, AdaTest leverages
Reinforcement Learning (RL) to produce a diverse set of test inputs.
Particularly, we progressively generate test vectors with high reward values in
an iterative manner. In each iteration, the test set is evaluated and
adaptively expanded as needed. Furthermore, AdaTest integrates adaptive
sampling to prioritize test samples that provide more information for HT
detection, thus reducing the number of samples while improving the sample
quality for faster exploration. We develop AdaTest with a Software/Hardware
co-design principle and provide an optimized on-chip architecture solution.
AdaTest's architecture minimizes the hardware overhead in two ways:(i)
Deploying circuit emulation on programmable hardware to accelerate reward
evaluation of the test input; (ii) Pipelining each computation stage in AdaTest
by automatically constructing auxiliary circuit for test input generation,
reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on
various HT benchmarks and compare it with two prior works that use logic
testing for HT detection. Experimental results show that AdaTest engenders up
to two orders of test generation speedup and two orders of test set size
reduction compared to the prior works while achieving the same level or higher
Trojan detection rate.",0,1,0,0,1,0,0.65899,14.0,0.890262,49
7088039f-676b-4fba-be8d-d8f87a7d4b46,"How ""Multi"" is Multi-Document Summarization?",10,0.203923,0.69657,"The task of multi-document summarization (MDS) aims at models that, given
multiple documents as input, are able to generate a summary that combines
disperse information, originally spread across these documents. Accordingly, it
is expected that both reference summaries in MDS datasets, as well as system
summaries, would indeed be based on such dispersed information. In this paper,
we argue for quantifying and assessing this expectation. To that end, we
propose an automated measure for evaluating the degree to which a summary is
``disperse'', in the sense of the number of source documents needed to cover
its content. We apply our measure to empirically analyze several popular MDS
datasets, with respect to their reference summaries, as well as the output of
state-of-the-art systems. Our results show that certain MDS datasets barely
require combining information from multiple documents, where a single document
often covers the full summary content. Overall, we advocate using our metric
for assessing and improving the degree to which summarization datasets require
combining multi-document information, and similarly how summarization models
actually meet this challenge. Our code is available in
https://github.com/ariecattan/multi_mds.",0,0,0,0,0,0,0.700332,7.0,0.79673,25
ea81c571-4437-4e9e-89df-14209afff4c2,Error Correction Code Transformer,23,0.14872,0.469642,"Error correction code is a major part of the communication physical layer,
ensuring the reliable transfer of data over noisy channels. Recently, neural
decoders were shown to outperform classical decoding techniques. However, the
existing neural approaches present strong overfitting due to the exponential
training complexity, or a restrictive inductive bias due to reliance on Belief
Propagation. Recently, Transformers have become methods of choice in many
applications thanks to their ability to represent complex interactions between
elements. In this work, we propose to extend for the first time the Transformer
architecture to the soft decoding of linear codes at arbitrary block lengths.
We encode each channel's output dimension to high dimension for better
representation of the bits information to be processed separately. The
element-wise processing allows the analysis of the channel output reliability,
while the algebraic code and the interaction between the bits are inserted into
the model via an adapted masked self-attention module. The proposed approach
demonstrates the extreme power and flexibility of Transformers and outperforms
existing state-of-the-art neural decoders by large margins at a fraction of
their time complexity.",1,1,0,0,1,0,0.278279,10.0,0.727006,33
26caa5c9-b50a-4647-8575-4e1713ef7a15,Motion Sensitive Contrastive Learning for Self-supervised Video Representation,9,0.0326387,0.459931,"Contrastive learning has shown great potential in video representation
learning. However, existing approaches fail to sufficiently exploit short-term
motion dynamics, which are crucial to various down-stream video understanding
tasks. In this paper, we propose Motion Sensitive Contrastive Learning (MSCL)
that injects the motion information captured by optical flows into RGB frames
to strengthen feature learning. To achieve this, in addition to clip-level
global contrastive learning, we develop Local Motion Contrastive Learning
(LMCL) with frame-level contrastive objectives across the two modalities.
Moreover, we introduce Flow Rotation Augmentation (FRA) to generate extra
motion-shuffled negative samples and Motion Differential Sampling (MDS) to
accurately screen training samples. Extensive experiments on standard
benchmarks validate the effectiveness of the proposed method. With the
commonly-used 3D ResNet-18 as the backbone, we achieve the top-1 accuracies of
91.5\% on UCF101 and 50.3\% on Something-Something v2 for video classification,
and a 65.6\% Top-1 Recall on UCF101 for video retrieval, notably improving the
state-of-the-art.",0,1,0,0,1,0,0.51716,5.0,0.614641,57
d334d860-54c3-48a4-a2ad-3d11118c06cb,Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines,22,0.187101,0.615549,"Strong inductive biases give humans the ability to quickly learn to perform a
variety of tasks. Although meta-learning is a method to endow neural networks
with useful inductive biases, agents trained by meta-learning may sometimes
acquire very different strategies from humans. We show that co-training these
agents on predicting representations from natural language task descriptions
and programs induced to generate such tasks guides them toward more human-like
inductive biases. Human-generated language descriptions and program induction
models that add new learned primitives both contain abstract concepts that can
compress description length. Co-training on these representations result in
more human-like behavior in downstream meta-reinforcement learning agents than
less abstract controls (synthetic language descriptions, program induction
without learned primitives), suggesting that the abstraction supported by these
representations is key.",0,0,0,0,0,0,0.384204,7.0,0.666661,63
c216dcd5-d50c-4cc0-af0e-8038961cb84d,On the Role of Parallel Data in Cross-lingual Transfer Learning,6,0.125248,0.543287,"While prior work has established that the use of parallel data is conducive
for cross-lingual learning, it is unclear if the improvements come from the
data itself, or if it is the modeling of parallel interactions that matters.
Exploring this, we examine the usage of unsupervised machine translation to
generate synthetic parallel data, and compare it to supervised machine
translation and gold parallel data. We find that even model generated parallel
data can be useful for downstream tasks, in both a general setting (continued
pretraining) as well as the task-specific setting (translate-train), although
our best results are still obtained using real parallel data. Our findings
suggest that existing multilingual models do not exploit the full potential of
monolingual data, and prompt the community to reconsider the traditional
categorization of cross-lingual learning approaches.",0,1,0,0,0,1,0.749919,7.0,0.816709,21
3ebe264a-ea45-4489-b770-858918ec581f,Data-driven End-to-end Learning of Pole Placement Control for Nonlinear Dynamics via Koopman Invariant Subspaces,2,0.0720034,0.358377,"We propose a data-driven method for controlling the frequency and convergence
rate of black-box nonlinear dynamical systems based on the Koopman operator
theory. With the proposed method, a policy network is trained such that the
eigenvalues of a Koopman operator of controlled dynamics are close to the
target eigenvalues. The policy network consists of a neural network to find a
Koopman invariant subspace, and a pole placement module to adjust the
eigenvalues of the Koopman operator. Since the policy network is
differentiable, we can train it in an end-to-end fashion using reinforcement
learning. We demonstrate that the proposed method achieves better performance
than model-free reinforcement learning and model-based control with system
identification.",0,0,0,0,0,0,0.702948,14.0,0.898883,35
4bd3a7b6-abff-44d6-856c-eb982ebf95a5,"Artificial Intelligence for Cybersecurity: Threats, Attacks and Mitigation",8,0.197732,0.835668,"With the advent of the digital era, every day-to-day task is automated due to
technological advances. However, technology has yet to provide people with
enough tools and safeguards. As the internet connects more-and-more devices
around the globe, the question of securing the connected devices grows at an
even spiral rate. Data thefts, identity thefts, fraudulent transactions,
password compromises, and system breaches are becoming regular everyday news.
The surging menace of cyber-attacks got a jolt from the recent advancements in
Artificial Intelligence. AI is being applied in almost every field of different
sciences and engineering. The intervention of AI not only automates a
particular task but also improves efficiency by many folds. So it is evident
that such a scrumptious spread would be very appetizing to cybercriminals. Thus
the conventional cyber threats and attacks are now ``intelligent"" threats. This
article discusses cybersecurity and cyber threats along with both conventional
and intelligent ways of defense against cyber-attacks. Furthermore finally, end
the discussion with the potential prospects of the future of AI in
cybersecurity.",0,1,0,0,0,1,0.31718,10.0,0.742698,42
5d9fbf1c-c5db-4b8d-9925-3f45ca4be55d,Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents,8,0.0742597,0.402222,"We argue that disentangling content selection from the budget used to cover
salient content improves the performance and applicability of abstractive
summarizers. Our method, FactorSum, does this disentanglement by factorizing
summarization into two steps through an energy function: (1) generation of
abstractive summary views; (2) combination of these views into a final summary,
following a budget and content guidance. This guidance may come from different
sources, including from an advisor model such as BART or BigBird, or in oracle
mode -- from the reference. This factorization achieves significantly higher
ROUGE scores on multiple benchmarks for long document summarization, namely
PubMed, arXiv, and GovReport. Most notably, our model is effective for domain
adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1
score on arXiv, which indicates a strong performance due to more flexible
budget adaptation and content selection less dependent on domain-specific
textual structure.",0,0,0,0,1,0,0.484484,6.0,0.66316,36
65a59ec1-dd40-4ce9-8395-a22a1044818f,CorrLoss: Integrating Co-Occurrence Domain Knowledge for Affect Recognition,1,0.00664997,0.0675455,"Neural networks are widely adopted, yet the integration of domain knowledge
is still underutilized. We propose to integrate domain knowledge about
co-occurring facial movements as a constraint in the loss function to enhance
the training of neural networks for affect recognition. As the co-ccurrence
patterns tend to be similar across datasets, applying our method can lead to a
higher generalizability of models and a lower risk of overfitting. We
demonstrate this by showing performance increases in cross-dataset testing for
various datasets. We also show the applicability of our method for calibrating
neural networks to different facial expressions.",0,1,0,0,0,0,0.158359,8.0,0.579079,37
35f7fd3e-15b7-40fc-8469-5bdf790b5e82,Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model,12,0.222128,0.649663,"The recently released NLLB-200 is a set of multilingual Neural Machine
Translation models that cover 202 languages. The largest model is based on a
Mixture of Experts architecture and achieves SoTA results across many language
pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just
for inference. In this work, we propose a pruning method that enables the
removal of up to 80% of experts without further finetuning and with a
negligible loss in translation quality, which makes it feasible to run the
model on a single 32GB GPU. Further analysis suggests that our pruning metrics
can identify language-specific experts.",0,1,0,0,0,0,0.885513,5.0,0.832808,41
7bdb9a0e-397b-4dc7-a6ec-005d56635413,Concordance based Survival Cobra with regression type weak learners,2,0.03601,0.351477,"In this paper, we predict conditional survival functions through a combined
regression strategy. We take weak learners as different random survival trees.
We propose to maximize concordance in the right-censored set up to find the
optimal parameters. We explore two approaches, a usual survival cobra and a
novel weighted predictor based on the concordance index. Our proposed
formulations use two different norms, say, Max-norm and Frobenius norm, to find
a proximity set of predictions from query points in the test dataset. We
illustrate our algorithms through three different real-life dataset
implementations.",0,0,0,0,0,0,0.00161115,29.0,0.722778,11
8fc20250-9d58-4224-b273-e3caa0d7e528,MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image Translation,19,0.28354,0.933722,"We present a novel method for exemplar-based image translation, called
matching interleaved diffusion models (MIDMs). Most existing methods for this
task were formulated as GAN-based matching-then-generation framework. However,
in this framework, matching errors induced by the difficulty of semantic
matching across cross-domain, e.g., sketch and photo, can be easily propagated
to the generation step, which in turn leads to degenerated results. Motivated
by the recent success of diffusion models overcoming the shortcomings of GANs,
we incorporate the diffusion models to overcome these limitations.
Specifically, we formulate a diffusion-based matching-and-generation framework
that interleaves cross-domain matching and diffusion steps in the latent space
by iteratively feeding the intermediate warp into the noising process and
denoising it to generate a translated image. In addition, to improve the
reliability of the diffusion process, we design a confidence-aware process
using cycle-consistency to consider only confident regions during translation.
Experimental results show that our MIDMs generate more plausible images than
state-of-the-art methods.",1,0,1,0,1,0,0.886437,9.0,0.90753,74
196d14bb-5d41-4cea-8c0f-275431c83cba,QuadTree Attention for Vision Transformers,105,0.781749,0.766959,"Transformers have been successful in many vision tasks, thanks to their
capability of capturing long-range dependency. However, their quadratic
computational complexity poses a major obstacle for applying them to vision
tasks requiring dense predictions, such as object detection, feature matching,
stereo, etc. We introduce QuadTree Attention, which reduces the computational
complexity from quadratic to linear. Our quadtree transformer builds token
pyramids and computes attention in a coarse-to-fine manner. At each level, the
top K patches with the highest attention scores are selected, such that at the
next level, attention is only evaluated within the relevant regions
corresponding to these top K patches. We demonstrate that quadtree attention
achieves state-of-the-art performance in various vision tasks, e.g. with 4.0%
improvement in feature matching on ScanNet, about 50% flops reduction in stereo
matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification,
1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on
semantic segmentation over previous state-of-the-art transformers. The codes
are available at https://github.com/Tangshitao/QuadtreeAttention.",1,1,0,0,1,0,0.967176,4.0,0.904802,47
3021f713-6064-4f7c-a52c-8c8e244dd783,kogito: A Commonsense Knowledge Inference Toolkit,6,0.081662,0.387911,"In this paper, we present kogito, an open-source tool for generating
commonsense inferences about situations described in text. kogito provides an
intuitive and extensible interface to interact with natural language generation
models that can be used for hypothesizing commonsense knowledge inference from
a textual input. In particular, kogito offers several features for targeted,
multi-granularity knowledge generation. These include a standardized API for
training and evaluating knowledge models, and generating and filtering
inferences from them. We also include helper functions for converting natural
language texts into a format ingestible by knowledge models - intermediate
pipeline stages such as knowledge head extraction from text, heuristic and
model-based knowledge head-relation matching, and an ability to define and use
custom knowledge relations. We make the code for kogito available at
https://github.com/epfl-nlp/kogito along with thorough documentation at
https://kogito.readthedocs.io.",1,1,0,0,0,0,0.882657,6.0,0.858768,37
713274a2-f282-4f12-b22d-f1a3e7f9428d,EfficientNeRF: Efficient Neural Radiance Fields,77,0.759059,0.894623,"Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications.",1,1,0,0,0,0,0.992736,4.0,0.996215,37
4e1ea5ee-4655-4036-8cee-87e83396e51c,Multimodal Token Fusion for Vision Transformers,80,0.640492,0.998234,"Many adaptations of transformers have emerged to address the single-modal
vision tasks, where self-attention modules are stacked to handle input sources
like images. Intuitively, feeding multiple modalities of data to vision
transformers could improve the performance, yet the inner-modal attentive
weights may also be diluted, which could thus undermine the final performance.
In this paper, we propose a multimodal token fusion method (TokenFusion),
tailored for transformer-based vision tasks. To effectively fuse multiple
modalities, TokenFusion dynamically detects uninformative tokens and
substitutes these tokens with projected and aggregated inter-modal features.
Residual positional alignment is also adopted to enable explicit utilization of
the inter-modal alignments after fusion. The design of TokenFusion allows the
transformer to learn correlations among multimodal features, while the
single-modal transformer architecture remains largely intact. Extensive
experiments are conducted on a variety of homogeneous and heterogeneous
modalities and demonstrate that TokenFusion surpasses state-of-the-art methods
in three typical vision tasks: multimodal image-to-image translation, RGB-depth
semantic segmentation, and 3D object detection with point cloud and images. Our
code is available at https://github.com/yikaiw/TokenFusion.",1,1,0,0,1,0,0.926941,5.0,0.870479,46
3d758b61-e6c7-466b-a327-379d1fd50e37,Self-Supervised Equivariant Learning for Oriented Keypoint Detection,20,0.131918,0.811833,"Detecting robust keypoints from an image is an integral part of many computer
vision problems, and the characteristic orientation and scale of keypoints play
an important role for keypoint description and matching. Existing
learning-based methods for keypoint detection rely on standard
translation-equivariant CNNs but often fail to detect reliable keypoints
against geometric variations. To learn to detect robust oriented keypoints, we
introduce a self-supervised learning framework using rotation-equivariant CNNs.
We propose a dense orientation alignment loss by an image pair generated by
synthetic transformations for training a histogram-based orientation map. Our
method outperforms the previous methods on an image matching benchmark and a
camera pose estimation benchmark.",0,1,0,0,0,0,0.364569,10.0,0.75997,79
756d35e9-d0cb-4c43-bfc0-9518950aae52,Probabilistic Compositional Embeddings for Multimodal Image Retrieval,15,0.299287,0.38195,"Existing works in image retrieval often consider retrieving images with one
or two query inputs, which do not generalize to multiple queries. In this work,
we investigate a more challenging scenario for composing multiple multimodal
queries in image retrieval. Given an arbitrary number of query images and (or)
texts, our goal is to retrieve target images containing the semantic concepts
specified in multiple multimodal queries. To learn an informative embedding
that can flexibly encode the semantics of various queries, we propose a novel
multimodal probabilistic composer (MPC). Specifically, we model input images
and texts as probabilistic embeddings, which can be further composed by a
probabilistic composition rule to facilitate image retrieval with multiple
multimodal queries. We propose a new benchmark based on the MS-COCO dataset and
evaluate our model on various setups that compose multiple images and (or) text
queries for multimodal image retrieval. Without bells and whistles, we show
that our probabilistic model formulation significantly outperforms existing
related methods on multimodal image retrieval while generalizing well to query
with different amounts of inputs given in arbitrary visual and (or) textual
modalities. Code is available here: https://github.com/andreineculai/MPC.",1,0,0,0,1,0,0.916103,8.0,0.91226,71
92ecb9e7-dad0-4d02-87eb-2b7e56a87a6c,On the Transferability of Adversarial Examples between Encrypted Models,4,0.0235463,0.415071,"Deep neural networks (DNNs) are well known to be vulnerable to adversarial
examples (AEs). In addition, AEs have adversarial transferability, namely, AEs
generated for a source model fool other (target) models. In this paper, we
investigate the transferability of models encrypted for adversarially robust
defense for the first time. To objectively verify the property of
transferability, the robustness of models is evaluated by using a benchmark
attack method, called AutoAttack. In an image-classification experiment, the
use of encrypted models is confirmed not only to be robust against AEs but to
also reduce the influence of AEs in terms of the transferability of models.",0,1,0,0,0,0,0.382125,7.0,0.665664,24
016511d1-9f07-44ba-93f8-4660b5db5db9,How Well Do Vision Transformers (VTs) Transfer To The Non-Natural Image Domain? An Empirical Study Involving Art Classification,1,0.00637808,0.0260904,"Vision Transformers (VTs) are becoming a valuable alternative to
Convolutional Neural Networks (CNNs) when it comes to problems involving
high-dimensional and spatially organized inputs such as images. However, their
Transfer Learning (TL) properties are not yet well studied, and it is not fully
known whether these neural architectures can transfer across different domains
as well as CNNs. In this paper we study whether VTs that are pre-trained on the
popular ImageNet dataset learn representations that are transferable to the
non-natural image domain. To do so we consider three well-studied art
classification problems and use them as a surrogate for studying the TL
potential of four popular VTs. Their performance is extensively compared
against that of four common CNNs across several TL experiments. Our results
show that VTs exhibit strong generalization properties and that these networks
are more powerful feature extractors than CNNs.",1,1,0,0,0,0,0.139876,7.0,0.499708,46
859d0b23-d0fd-4c32-9f84-c8416864fbd2,Word Order Matters when you Increase Masking,5,0.0356956,0.523553,"Word order, an essential property of natural languages, is injected in
Transformer-based neural language models using position encoding. However,
recent experiments have shown that explicit position encoding is not always
useful, since some models without such feature managed to achieve state-of-the
art performance on some tasks. To understand better this phenomenon, we examine
the effect of removing position encodings on the pre-training objective itself
(i.e., masked language modelling), to test whether models can reconstruct
position information from co-occurrences alone. We do so by controlling the
amount of masked tokens in the input sentence, as a proxy to affect the
importance of position information for the task. We find that the necessity of
position information increases with the amount of masking, and that masked
language models without position encodings are not able to reconstruct this
information on the task. These findings point towards a direct relationship
between the amount of masking and the ability of Transformers to capture
order-sensitive aspects of language using position encoding.",0,0,0,0,0,1,0.621551,4.0,0.590457,20
769e9773-b7ef-4597-aa18-2f5af363700a,Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion,56,0.596439,0.80512,"We present a framework for modeling interactional communication in dyadic
conversations: given multimodal inputs of a speaker, we autoregressively output
multiple possibilities of corresponding listener motion. We combine the motion
and speech audio of the speaker using a motion-audio cross attention
transformer. Furthermore, we enable non-deterministic prediction by learning a
discrete latent representation of realistic listener motion with a novel
motion-encoding VQ-VAE. Our method organically captures the multimodal and
non-deterministic nature of nonverbal dyadic interactions. Moreover, it
produces realistic 3D listener facial motion synchronous with the speaker (see
video). We demonstrate that our method outperforms baselines qualitatively and
quantitatively via a rich suite of experiments. To facilitate this line of
research, we introduce a novel and large in-the-wild dataset of dyadic
conversations. Code, data, and videos available at
https://evonneng.github.io/learning2listen/.",0,0,0,1,0,0,0.370217,10.0,0.761919,70
9244ef98-899c-47be-9056-95f777a91080,Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information,14,0.203894,0.671592,"We describe a simple and effective method (Spectral Attribute removaL; SAL)
to remove private or guarded information from neural representations. Our
method uses matrix decomposition to project the input representations into
directions with reduced covariance with the guarded information rather than
maximal covariance as factorization methods normally use. We begin with linear
information removal and proceed to generalize our algorithm to the case of
nonlinear information removal using kernels. Our experiments demonstrate that
our algorithm retains better main task performance after removing the guarded
information compared to previous work. In addition, our experiments demonstrate
that we need a relatively small amount of guarded attribute data to remove
information about these attributes, which lowers the exposure to sensitive data
and is more suitable for low-resource scenarios. Code is available at
https://github.com/jasonshaoshun/SAL.",1,1,0,0,0,0,0.560022,8.0,0.774171,32
1ea5beda-11e5-4d86-83e7-b1d285b952a5,Twitter Dataset for 2022 Russo-Ukrainian Crisis,26,0.222065,0.912974,"Online Social Networks (OSNs) play a significant role in information sharing
during a crisis. The data collected during such a crisis can reflect the large
scale public opinions and sentiment. In addition, OSN data can also be used to
study different campaigns that are employed by various entities to engineer
public opinions. Such information sharing campaigns can range from spreading
factual information to propaganda and misinformation. We provide a Twitter
dataset of the 2022 Russo-Ukrainian conflict. In the first release, we share
over 1.6 million tweets shared during the 1st week of the crisis.",1,1,0,1,0,0,0.00942684,9.0,0.303456,11
2b98354a-99a9-4019-9ea8-078090a3fbca,The Need for a Meta-Architecture for Robot Autonomy,1,0.012512,0.0970449,"Long-term autonomy of robotic systems implicitly requires dependable
platforms that are able to naturally handle hardware and software faults,
problems in behaviors, or lack of knowledge. Model-based dependable platforms
additionally require the application of rigorous methodologies during the
system development, including the use of correct-by-construction techniques to
implement robot behaviors. As the level of autonomy in robots increases, so do
the cost of offering guarantees about the dependability of the system.
Certifiable dependability of autonomous robots, we argue, can benefit from
formal models of the integration of several cognitive functions, knowledge
processing, reasoning, and meta-reasoning. Here we put forward the case for a
generative model of cognitive architectures for autonomous robotic agents that
subscribes to the principles of model-based engineering and certifiable
dependability, autonomic computing, and knowledge-enabled robotics.",0,0,0,0,0,0,0.00171367,24.0,0.667596,58
266effc5-c2bf-4c23-b381-edb9e14f672b,DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation,21,0.358473,0.952576,"In many real-world scenarios, we often deal with streaming data that is
sequentially collected over time. Due to the non-stationary nature of the
environment, the streaming data distribution may change in unpredictable ways,
which is known as concept drift. To handle concept drift, previous methods
first detect when/where the concept drift happens and then adapt models to fit
the distribution of the latest data. However, there are still many cases that
some underlying factors of environment evolution are predictable, making it
possible to model the future concept drift trend of the streaming data, while
such cases are not fully explored in previous work.
  In this paper, we propose a novel method DDG-DA, that can effectively
forecast the evolution of data distribution and improve the performance of
models. Specifically, we first train a predictor to estimate the future data
distribution, then leverage it to generate training samples, and finally train
models on the generated data. We conduct experiments on three real-world tasks
(forecasting on stock price trend, electricity load and solar irradiance) and
obtain significant improvement on multiple widely-used models.",1,1,1,0,0,0,0.437615,13.0,0.833705,53
57238be8-bb4c-4f02-9b0d-a4b66abba651,Multi-model Ensemble Learning Method for Human Expression Recognition,6,0.0150694,0.208826,"Analysis of human affect plays a vital role in human-computer interaction
(HCI) systems. Due to the difficulty in capturing large amounts of real-life
data, most of the current methods have mainly focused on controlled
environments, which limit their application scenarios. To tackle this problem,
we propose our solution based on the ensemble learning method. Specifically, we
formulate the problem as a classification task, and then train several
expression classification models with different types of backbones--ResNet,
EfficientNet and InceptionNet. After that, the outputs of several models are
fused via model ensemble method to predict the final results. Moreover, we
introduce the multi-fold ensemble method to train and ensemble several models
with the same architecture but different data distributions to enhance the
performance of our solution. We conduct many experiments on the AffWild2
dataset of the ABAW2022 Challenge, and the results demonstrate the
effectiveness of our solution.",0,1,0,0,0,0,0.0913593,10.0,0.604513,28
c3b85146-5e5a-4542-ac67-9d9035107f82,Rethinking Super-Resolution as Text-Guided Details Generation,2,0.0446522,0.101533,"Deep neural networks have greatly promoted the performance of single image
super-resolution (SISR). Conventional methods still resort to restoring the
single high-resolution (HR) solution only based on the input of image modality.
However, the image-level information is insufficient to predict adequate
details and photo-realistic visual quality facing large upscaling factors (x8,
x16). In this paper, we propose a new perspective that regards the SISR as a
semantic image detail enhancement problem to generate semantically reasonable
HR image that are faithful to the ground truth. To enhance the semantic
accuracy and the visual quality of the reconstructed image, we explore the
multi-modal fusion learning in SISR by proposing a Text-Guided Super-Resolution
(TGSR) framework, which can effectively utilize the information from the text
and image modalities. Different from existing methods, the proposed TGSR could
generate HR image details that match the text descriptions through a
coarse-to-fine process. Extensive experiments and ablation studies demonstrate
the effect of the TGSR, which exploits the text reference to recover realistic
images.",0,1,1,0,0,0,0.939293,9.0,0.935643,38
5f1d231f-f7ae-457b-b7fb-24bd691e6ef7,BlobGAN: Spatially Disentangled Scene Representations,32,0.0974769,0.800611,"We propose an unsupervised, mid-level representation for a generative model
of scenes. The representation is mid-level in that it is neither per-pixel nor
per-image; rather, scenes are modeled as a collection of spatial, depth-ordered
""blobs"" of features. Blobs are differentiably placed onto a feature grid that
is decoded into an image by a generative adversarial network. Due to the
spatial uniformity of blobs and the locality inherent to convolution, our
network learns to associate different blobs with different entities in a scene
and to arrange these blobs to capture scene layout. We demonstrate this
emergent behavior by showing that, despite training without any supervision,
our method enables applications such as easy manipulation of objects within a
scene (e.g., moving, removing, and restyling furniture), creation of feasible
scenes given constraints (e.g., plausible rooms with drawers at a particular
location), and parsing of real-world images into constituent parts. On a
challenging multi-category dataset of indoor scenes, BlobGAN outperforms
StyleGAN2 in image quality as measured by FID. See our project page for video
results and interactive demo: https://www.dave.ml/blobgan",0,0,0,0,0,0,0.393821,7.0,0.671225,106
ddefa8d8-956d-4421-92cf-907f7e2fec5f,Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection,32,0.398754,0.815495,"Nowadays, most methods in end-to-end contextual speech recognition bias the
recognition process towards contextual knowledge. Since all-neural contextual
biasing methods rely on phrase-level contextual modeling and attention-based
relevance modeling, they may encounter confusion between similar
context-specific phrases, which hurts predictions at the token level. In this
work, we focus on mitigating confusion problems with fine-grained contextual
knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge
to reduce the uncertainty of token predictions. Specifically, we first apply
phrase selection to narrow the range of phrase candidates, and then conduct
token attention on the tokens in the selected phrase candidates. Moreover, we
re-normalize the attention weights of most relevant phrases in inference to
obtain more focused phrase-level contextual representations, and inject
position information to better discriminate phrases or tokens. On LibriSpeech
and an in-house 160,000-hour dataset, we explore the proposed methods based on
a controllable all-neural biasing method, collaborative decoding (ColDec). The
proposed methods provide at most 6.1% relative word error rate reduction on
LibriSpeech and 16.4% relative character error rate reduction on the in-house
dataset over ColDec.",0,1,0,0,0,0,0.714725,7.0,0.802449,30
346efdbf-8b34-4ad2-ab2e-988277de99f6,Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification,6,0.195229,0.547919,"Transformer has been widely used in histopathology whole slide image (WSI)
classification for the purpose of tumor grading, prognosis analysis, etc.
However, the design of token-wise self-attention and positional embedding
strategy in the common Transformer limits the effectiveness and efficiency in
the application to gigapixel histopathology images. In this paper, we propose a
kernel attention Transformer (KAT) for histopathology WSI classification. The
information transmission of the tokens is achieved by cross-attention between
the tokens and a set of kernels related to a set of positional anchors on the
WSI. Compared to the common Transformer structure, the proposed KAT can better
describe the hierarchical context information of the local regions of the WSI
and meanwhile maintains a lower computational complexity. The proposed method
was evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset
with 2560 WSIs, and was compared with 6 state-of-the-art methods. The
experimental results have demonstrated the proposed KAT is effective and
efficient in the task of histopathology WSI classification and is superior to
the state-of-the-art methods. The code is available at
https://github.com/zhengyushan/kat.",1,1,0,0,1,0,0.926967,5.0,0.870506,18
31cf8f20-0551-4787-9c1d-af396ae3aee4,Entity Type Prediction Leveraging Graph Walks and Entity Descriptions,3,0.0166717,0.398018,"The entity type information in Knowledge Graphs (KGs) such as DBpedia,
Freebase, etc. is often incomplete due to automated generation or human
curation. Entity typing is the task of assigning or inferring the semantic type
of an entity in a KG. This paper presents \textit{GRAND}, a novel approach for
entity typing leveraging different graph walk strategies in RDF2vec together
with textual entity descriptions. RDF2vec first generates graph walks and then
uses a language model to obtain embeddings for each node in the graph. This
study shows that the walk generation strategy and the embedding model have a
significant effect on the performance of the entity typing task. The proposed
approach outperforms the baseline approaches on the benchmark datasets DBpedia
and FIGER for entity typing in KGs for both fine-grained and coarse-grained
classes. The results show that the combination of order-aware RDF2vec variants
together with the contextual embeddings of the textual entity descriptions
achieve the best results.",0,1,0,0,1,0,0.0544087,9.0,0.500797,43
e0dd337e-9e83-417c-bbf9-ea0462dba3fc,Does Corpus Quality Really Matter for Low-Resource Languages?,11,0.0802143,0.326311,"The vast majority of non-English corpora are derived from automatically
filtered versions of CommonCrawl. While prior work has identified major issues
on the quality of these datasets (Kreutzer et al., 2021), it is not clear how
this impacts downstream performance. Taking representation learning in Basque
as a case study, we explore tailored crawling (manually identifying and
scraping websites with high-quality content) as an alternative to filtering
CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque
portion of popular multilingual corpora like CC100 and mC4, yet it has a much
higher quality according to native annotators. For instance, 66% of documents
are rated as high-quality for EusCrawl, in contrast with <33% for both mC4 and
CC100. Nevertheless, we obtain similar results on downstream NLU tasks
regardless of the corpus used for pre-training. Our work suggests that NLU
performance in low-resource languages is not primarily constrained by the
quality of the data, and other factors like corpus size and domain coverage can
play a more important role.",0,1,0,1,0,0,0.720675,5.0,0.726761,18
a822d14e-482b-4da4-9668-185e3848c5a7,Outliers Dimensions that Disrupt Transformers Are Driven by Frequency,22,0.148853,0.768856,"While Transformer-based language models are generally very robust to pruning,
there is the recently discovered outlier phenomenon: disabling only 48 out of
110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We
replicate the original evidence for the outlier phenomenon and we link it to
the geometry of the embedding space. We find that in both BERT and RoBERTa the
magnitude of hidden state coefficients corresponding to outlier dimensions
correlates with the frequency of encoded tokens in pre-training data, and it
also contributes to the ""vertical"" self-attention pattern enabling the model to
focus on the special tokens. This explains the drop in performance from
disabling the outliers, and it suggests that to decrease anisotropicity in
future models we need pre-training schemas that would better take into account
the skewed token distributions.",1,0,0,0,0,0,0.74062,5.0,0.738054,43
1cd849f2-37d7-422e-ad82-7962a0778ebc,A Marker-based Neural Network System for Extracting Social Determinants of Health,1,0.0526912,0.228252,"Objective. The impact of social determinants of health (SDoH) on patients'
healthcare quality and the disparity is well-known. Many SDoH items are not
coded in structured forms in electronic health records. These items are often
captured in free-text clinical notes, but there are limited methods for
automatically extracting them. We explore a multi-stage pipeline involving
named entity recognition (NER), relation classification (RC), and text
classification methods to extract SDoH information from clinical notes
automatically.
  Materials and Methods. The study uses the N2C2 Shared Task data, which was
collected from two sources of clinical notes: MIMIC-III and University of
Washington Harborview Medical Centers. It contains 4480 social history sections
with full annotation for twelve SDoHs. In order to handle the issue of
overlapping entities, we developed a novel marker-based NER model. We used it
in a multi-stage pipeline to extract SDoH information from clinical notes.
  Results. Our marker-based system outperformed the state-of-the-art span-based
models at handling overlapping entities based on the overall Micro-F1 score
performance. It also achieved state-of-the-art performance compared to the
shared task methods.
  Conclusion. The major finding of this study is that the multi-stage pipeline
effectively extracts SDoH information from clinical notes. This approach can
potentially improve the understanding and tracking of SDoHs in clinical
settings. However, error propagation may be an issue, and further research is
needed to improve the extraction of entities with complex semantic meanings and
low-resource entities using external knowledge.",0,1,0,0,1,0,0.889898,7.0,0.883129,44
2469f777-e71f-4242-8ee6-bd8ece8c244e,Beyond Value: CHECKLIST for Testing Inferences in Planning-Based RL,1,0.0362594,0.0572511,"Reinforcement learning (RL) agents are commonly evaluated via their expected
value over a distribution of test scenarios. Unfortunately, this evaluation
approach provides limited evidence for post-deployment generalization beyond
the test distribution. In this paper, we address this limitation by extending
the recent CheckList testing methodology from natural language processing to
planning-based RL. Specifically, we consider testing RL agents that make
decisions via online tree search using a learned transition model and value
function. The key idea is to improve the assessment of future performance via a
CheckList approach for exploring and assessing the agent's inferences during
tree search. The approach provides the user with an interface and general
query-rule mechanism for identifying potential inference flaws and validating
expected inference invariances. We present a user study involving knowledgeable
AI researchers using the approach to evaluate an agent trained to play a
complex real-time strategy game. The results show the approach is effective in
allowing users to identify previously-unknown flaws in the agent's reasoning.
In addition, our analysis provides insight into how AI experts use this type of
testing approach, which may help improve future instantiations.",0,1,0,0,0,0,0.800736,7.0,0.838391,29
4cbb2e31-18e6-465b-99bd-bb76ae41d213,Featurized Query R-CNN,4,0.0784927,0.116821,"The query mechanism introduced in the DETR method is changing the paradigm of
object detection and recently there are many query-based methods have obtained
strong object detection performance. However, the current query-based detection
pipelines suffer from the following two issues. Firstly, multi-stage decoders
are required to optimize the randomly initialized object queries, incurring a
large computation burden. Secondly, the queries are fixed after training,
leading to unsatisfying generalization capability. To remedy the above issues,
we present featurized object queries predicted by a query generation network in
the well-established Faster R-CNN framework and develop a Featurized Query
R-CNN. Extensive experiments on the COCO dataset show that our Featurized Query
R-CNN obtains the best speed-accuracy trade-off among all R-CNN detectors,
including the recent state-of-the-art Sparse R-CNN detector. The code is
available at {https://github.com/hustvl/Featurized-QueryRCNN.",1,1,0,0,1,0,0.990863,7.0,0.991023,39
63331e21-886b-4069-b856-8f58a7dcf472,Deep Learning in Business Analytics: A Clash of Expectations and Reality,32,0.196408,0.976897,"Our fast-paced digital economy shaped by global competition requires
increased data-driven decision-making based on artificial intelligence (AI) and
machine learning (ML). The benefits of deep learning (DL) are manifold, but it
comes with limitations that have - so far - interfered with widespread industry
adoption. This paper explains why DL - despite its popularity - has
difficulties speeding up its adoption within business analytics. It is shown -
by a mixture of content analysis and empirical study - that the adoption of
deep learning is not only affected by computational complexity, lacking big
data architecture, lack of transparency (black-box), and skill shortage, but
also by the fact that DL does not outperform traditional ML models in the case
of structured datasets with fixed-length feature vectors. Deep learning should
be regarded as a powerful addition to the existing body of ML models instead of
a one size fits all solution.",0,0,0,0,0,0,0.423659,6.0,0.63244,61
edc423e1-c6b3-4179-82c9-6cd3fbf99dad,The Free Energy Principle for Perception and Action: A Deep Learning Perspective,18,0.230177,0.372725,"The free energy principle, and its corollary active inference, constitute a
bio-inspired theory that assumes biological agents act to remain in a
restricted set of preferred states of the world, i.e., they minimize their free
energy. Under this principle, biological agents learn a generative model of the
world and plan actions in the future that will maintain the agent in an
homeostatic state that satisfies its preferences. This framework lends itself
to being realized in silico, as it comprehends important aspects that make it
computationally affordable, such as variational inference and amortized
planning. In this work, we investigate the tool of deep learning to design and
realize artificial agents based on active inference, presenting a deep-learning
oriented presentation of the free energy principle, surveying works that are
relevant in both machine learning and active inference areas, and discussing
the design choices that are involved in the implementation process. This
manuscript probes newer perspectives for the active inference framework,
grounding its theoretical aspects into more pragmatic affairs, offering a
practical guide to active inference newcomers and a starting point for deep
learning practitioners that would like to investigate implementations of the
free energy principle.",0,0,0,0,0,0,0.716052,7.0,0.802979,166
8445e0c3-e5ec-4456-bec7-cc6caf4e1535,A Mask Attention Interaction and Scale Enhancement Network for SAR Ship Instance Segmentation,33,0.874083,0.991808,"Most of existing synthetic aperture radar (SAR) ship in-stance segmentation
models do not achieve mask interac-tion or offer limited interaction
performance. Besides, their multi-scale ship instance segmentation performance
is moderate especially for small ships. To solve these problems, we propose a
mask attention interaction and scale enhancement network (MAI-SE-Net) for SAR
ship instance segmentation. MAI uses an atrous spatial pyra-mid pooling (ASPP)
to gain multi-resolution feature re-sponses, a non-local block (NLB) to model
long-range spa-tial dependencies, and a concatenation shuffle attention block
(CSAB) to improve interaction benefits. SE uses a content-aware reassembly of
features block (CARAFEB) to generate an extra pyramid bottom-level to boost
small ship performance, a feature balance operation (FBO) to improve scale
feature description, and a global context block (GCB) to refine features.
Experimental results on two public SSDD and HRSID datasets reveal that
MAI-SE-Net outperforms the other nine competitive models, better than the
suboptimal model by 4.7% detec-tion AP and 3.4% segmentation AP on SSDD and by
3.0% detection AP and 2.4% segmentation AP on HRSID.",0,1,0,0,1,0,0.943512,7.0,0.920882,43
d4a930e3-f1af-4d09-91a0-81803b97887a,Rhino: Deep Causal Temporal Relationship Learning With History-dependent Noise,14,0.187259,0.949108,"Discovering causal relationships between different variables from time series
data has been a long-standing challenge for many domains such as climate
science, finance, and healthcare. Given the complexity of real-world
relationships and the nature of observations in discrete time, causal discovery
methods need to consider non-linear relations between variables, instantaneous
effects and history-dependent noise (the change of noise distribution due to
past actions). However, previous works do not offer a solution addressing all
these problems together. In this paper, we propose a novel causal relationship
learning framework for time-series data, called Rhino, which combines vector
auto-regression, deep learning and variational inference to model non-linear
relationships with instantaneous effects while allowing the noise distribution
to be modulated by historical observations. Theoretically, we prove the
structural identifiability of Rhino. Our empirical results from extensive
synthetic experiments and two real-world benchmarks demonstrate better
discovery performance compared to relevant baselines, with ablation studies
revealing its robustness under model misspecification.",1,0,1,0,0,0,0.441422,10.0,0.78499,53
172a7406-3fdf-4e3b-8d8b-65f329bd472f,On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,61,0.124145,0.810268,"Many recent studies on large-scale language models have reported successful
in-context zero- and few-shot learning ability. However, the in-depth analysis
of when in-context learning occurs is still lacking. For example, it is unknown
how in-context learning performance changes as the training corpus varies.
Here, we investigate the effects of the source and size of the pretraining
corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From
our in-depth investigation, we introduce the following observations: (1)
in-context learning performance heavily depends on the corpus domain source,
and the size of the pretraining corpus does not necessarily determine the
emergence of in-context learning, (2) in-context learning ability can emerge
when a language model is trained on a combination of multiple corpora, even
when each corpus does not result in in-context learning on its own, (3)
pretraining with a corpus related to a downstream task does not always
guarantee the competitive in-context learning performance of the downstream
task, especially in the few-shot setting, and (4) the relationship between
language modeling (measured in perplexity) and in-context learning does not
always correlate: e.g., low perplexity does not always imply high in-context
few-shot learning performance.",0,1,0,0,0,0,0.883245,3.0,0.718316,31
bef1366a-3bc9-4e06-974b-3df7335aea3e,Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe,31,0.155246,0.45696,"Privacy concerns have attracted increasing attention in data-driven products
due to the tendency of machine learning models to memorize sensitive training
data. Generating synthetic versions of such data with a formal privacy
guarantee, such as differential privacy (DP), provides a promising path to
mitigating these privacy concerns, but previous approaches in this direction
have typically failed to produce synthetic data of high quality. In this work,
we show that a simple and practical recipe in the text domain is effective:
simply fine-tuning a pretrained generative language model with DP enables the
model to generate useful synthetic text with strong privacy protection. Through
extensive empirical analyses on both benchmark and private customer data, we
demonstrate that our method produces synthetic text that is competitive in
terms of utility with its non-private counterpart, meanwhile providing strong
protection against potential privacy leakages.",1,1,0,0,0,0,0.540158,4.0,0.534521,74
93b70b2f-2ba6-4370-92ef-36a5399ee992,Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding,7,0.159373,0.397495,"Unsupervised pre-training on millions of digital-born or scanned documents
has shown promising advances in visual document understanding~(VDU). While
various vision-language pre-training objectives are studied in existing
solutions, the document textline, as an intrinsic granularity in VDU, has
seldom been explored so far. A document textline usually contains words that
are spatially and semantically correlated, which can be easily obtained from
OCR engines. In this paper, we propose Wukong-Reader, trained with new
pre-training objectives to leverage the structural knowledge nested in document
textlines. We introduce textline-region contrastive learning to achieve
fine-grained alignment between the visual regions and texts of document
textlines. Furthermore, masked region modeling and textline-grid matching are
also designed to enhance the visual and layout representations of textlines.
Experiments show that our Wukong-Reader has superior performance on various VDU
tasks such as information extraction. The fine-grained alignment over textlines
also empowers Wukong-Reader with promising localization ability.",0,1,0,0,1,0,0.972317,4.0,0.916965,46
5d790d40-dd96-4727-9a8c-b2d76350859f,Convolutional Neural Networks for Image Spam Detection,22,0.15965,0.797376,"Spam can be defined as unsolicited bulk email. In an effort to evade
text-based filters, spammers sometimes embed spam text in an image, which is
referred to as image spam. In this research, we consider the problem of image
spam detection, based on image analysis. We apply convolutional neural networks
(CNN) to this problem, we compare the results obtained using CNNs to other
machine learning techniques, and we compare our results to previous related
work. We consider both real-world image spam and challenging image spam-like
datasets. Our results improve on previous work by employing CNNs based on a
novel feature set consisting of a combination of the raw image and Canny edges.",0,1,0,0,0,0,0.00672269,11.0,0.399242,26
57cf2d48-83e3-4781-88e4-e7aa5eb1b063,What Does DALL-E 2 Know About Radiology?,21,0.360077,0.335674,"Generative models such as DALL-E 2 could represent a promising future tool
for image generation, augmentation, and manipulation for artificial
intelligence research in radiology provided that these models have sufficient
medical domain knowledge. Here we show that DALL-E 2 has learned relevant
representations of X-ray images with promising capabilities in terms of
zero-shot text-to-image generation of new images, continuation of an image
beyond its original boundaries, or removal of elements, while pathology
generation or CT, MRI, and ultrasound images are still limited. The use of
generative models for augmenting and generating radiological data thus seems
feasible, even if further fine-tuning and adaptation of these models to the
respective domain is required beforehand.",0,1,0,0,0,0,0.981486,1.0,0.774156,11
0228fee8-73af-4412-b903-7ad035f7ad79,Dynamic Point Cloud Compression with Cross-Sectional Approach,2,0.167167,0.0847754,"The recent development of dynamic point clouds has introduced the possibility
of mimicking natural reality, and greatly assisting quality of life. However,
to broadcast successfully, the dynamic point clouds require higher compression
due to their huge volume of data compared to the traditional video. Recently,
MPEG finalized a Video-based Point Cloud Compression standard known as V-PCC.
However, V-PCC requires huge computational time due to expensive normal
calculation and segmentation, sacrifices some points to limit the number of 2D
patches, and cannot occupy all spaces in the 2D frame. The proposed method
addresses these limitations by using a novel cross-sectional approach. This
approach reduces expensive normal estimation and segmentation, retains more
points, and utilizes more spaces for 2D frame generation compared to the VPCC.
The experimental results using standard video sequences show that the proposed
technique can achieve better compression in both geometric and texture data
compared to the V-PCC standard.",0,1,0,0,0,0,0.761568,3.0,0.5836,21
44e843e9-fab1-41e4-a31f-8c846c73bb3e,Learning State-Aware Visual Representations from Audible Interactions,15,0.468152,0.387151,"We propose a self-supervised algorithm to learn representations from
egocentric video data. Recently, significant efforts have been made to capture
humans interacting with their own environments as they go about their daily
activities. In result, several large egocentric datasets of interaction-rich
multi-modal data have emerged. However, learning representations from videos
can be challenging. First, given the uncurated nature of long-form continuous
videos, learning effective representations require focusing on moments in time
when interactions take place. Second, visual representations of daily
activities should be sensitive to changes in the state of the environment.
However, current successful multi-modal learning frameworks encourage
representation invariance over time. To address these challenges, we leverage
audio signals to identify moments of likely interactions which are conducive to
better learning. We also propose a novel self-supervised objective that learns
from audible state changes caused by interactions. We validate these
contributions extensively on two large-scale egocentric datasets,
EPIC-Kitchens-100 and the recently released Ego4D, and show improvements on
several downstream tasks, including action recognition, long-term action
anticipation, and object state change classification.",1,1,0,0,0,0,0.960201,7.0,0.937308,73
19e31adf-30be-4f59-8a83-471acf2b76d2,Features Fusion Framework for Multimodal Irregular Time-series Events,2,0.0445329,0.292714,"Some data from multiple sources can be modeled as multimodal time-series
events which have different sampling frequencies, data compositions, temporal
relations and characteristics. Different types of events have complex nonlinear
relationships, and the time of each event is irregular. Neither the classical
Recurrent Neural Network (RNN) model nor the current state-of-the-art
Transformer model can deal with these features well. In this paper, a features
fusion framework for multimodal irregular time-series events is proposed based
on the Long Short-Term Memory networks (LSTM). Firstly, the complex features
are extracted according to the irregular patterns of different events.
Secondly, the nonlinear correlation and complex temporal dependencies
relationship between complex features are captured and fused into a tensor.
Finally, a feature gate are used to control the access frequency of different
tensors. Extensive experiments on MIMIC-III dataset demonstrate that the
proposed framework significantly outperforms to the existing methods in terms
of AUC (the area under Receiver Operating Characteristic curve) and AP (Average
Precision).",0,1,0,0,1,0,0.898215,11.0,0.928808,30
a1f531f2-3453-46ef-a42f-9d21d308a77a,HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition,11,0.0200478,0.456608,"Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT)
and wav2vec 2.0, has brought significant improvements in automatic speech
recognition (ASR). However, these models usually require an expensive
computational cost to achieve outstanding performance, slowing down the
inference speed. To improve the model efficiency, we propose an early exit
scheme for ASR, namely HuBERT-EE, that allows the model to stop the inference
dynamically. In HuBERT-EE, multiple early exit branches are added at the
intermediate layers, and each branch is used to decide whether a prediction can
be exited early. Experimental results on the LibriSpeech dataset show that
HuBERT-EE can accelerate the inference of a large-scale HuBERT model while
simultaneously balancing the trade-off between the word error rate (WER)
performance and the latency.",0,1,0,0,0,0,0.489388,4.0,0.49832,24
94bb2f18-52d6-4657-a709-afd1bfd49023,Introspective Learning : A Two-Stage Approach for Inference in Neural Networks,8,0.00887284,0.183427,"In this paper, we advocate for two stages in a neural network's decision
making process. The first is the existing feed-forward inference framework
where patterns in given data are sensed and associated with previously learned
patterns. The second stage is a slower reflection stage where we ask the
network to reflect on its feed-forward decision by considering and evaluating
all available choices. Together, we term the two stages as introspective
learning. We use gradients of trained neural networks as a measurement of this
reflection. A simple three-layered Multi Layer Perceptron is used as the second
stage that predicts based on all extracted gradient features. We perceptually
visualize the post-hoc explanations from both stages to provide a visual
grounding to introspection. For the application of recognition, we show that an
introspective network is 4% more robust and 42% less prone to calibration
errors when generalizing to noisy data. We also illustrate the value of
introspective networks in downstream tasks that require generalizability and
calibration including active learning, out-of-distribution detection, and
uncertainty estimation. Finally, we ground the proposed machine introspection
to human introspection for the application of image quality assessment.",1,0,1,0,0,0,0.00238593,13.0,0.411815,81
ef4076ae-3d88-433a-a47a-985e58cbdd60,RMGN: A Regional Mask Guided Network for Parser-free Virtual Try-on,5,0.0828075,0.693199,"Virtual try-on(VTON) aims at fitting target clothes to reference person
images, which is widely adopted in e-commerce.Existing VTON approaches can be
narrowly categorized into Parser-Based(PB) and Parser-Free(PF) by whether
relying on the parser information to mask the persons' clothes and synthesize
try-on images. Although abandoning parser information has improved the
applicability of PF methods, the ability of detail synthesizing has also been
sacrificed. As a result, the distraction from original cloth may persistin
synthesized images, especially in complicated postures and high resolution
applications. To address the aforementioned issue, we propose a novel PF method
named Regional Mask Guided Network(RMGN). More specifically, a regional mask is
proposed to explicitly fuse the features of target clothes and reference
persons so that the persisted distraction can be eliminated. A posture
awareness loss and a multi-level feature extractor are further proposed to
handle the complicated postures and synthesize high resolution images.
Extensive experiments demonstrate that our proposed RMGN outperforms both
state-of-the-art PB and PF methods.Ablation studies further verify the
effectiveness ofmodules in RMGN.",1,1,0,0,1,0,0.919743,7.0,0.90226,23
5dca96e9-7aff-43c0-ae2e-72cd611db47b,Magpie: Automatically Tuning Static Parameters for Distributed File Systems using Deep Reinforcement Learning,3,0.0376593,0.597459,"Distributed file systems are widely used nowadays, yet using their default
configurations is often not optimal. At the same time, tuning configuration
parameters is typically challenging and time-consuming. It demands expertise
and tuning operations can also be expensive. This is especially the case for
static parameters, where changes take effect only after a restart of the system
or workloads. We propose a novel approach, Magpie, which utilizes deep
reinforcement learning to tune static parameters by strategically exploring and
exploiting configuration parameter spaces. To boost the tuning of the static
parameters, our method employs both server and client metrics of distributed
file systems to understand the relationship between static parameters and
performance. Our empirical evaluation results show that Magpie can noticeably
improve the performance of the distributed file system Lustre, where our
approach on average achieves 91.8% throughput gains against default
configuration after tuning towards single performance indicator optimization,
while it reaches 39.7% more throughput gains against the baseline.",0,1,0,0,0,0,0.0779177,10.0,0.587875,39
84df5505-bb87-408e-b79e-99d7ac92bf40,Generative Multiplane Images: Making a 2D GAN 3D-Aware,59,0.123343,0.814304,"What is really needed to make an existing 2D GAN 3D-aware? To answer this
question, we modify a classical GAN, i.e., StyleGANv2, as little as possible.
We find that only two modifications are absolutely necessary: 1) a multiplane
image style generator branch which produces a set of alpha maps conditioned on
their depth; 2) a pose-conditioned discriminator. We refer to the generated
output as a 'generative multiplane image' (GMPI) and emphasize that its
renderings are not only high-quality but also guaranteed to be view-consistent,
which makes GMPIs different from many prior works. Importantly, the number of
alpha maps can be dynamically adjusted and can differ between training and
inference, alleviating memory concerns and enabling fast training of GMPIs in
less than half a day at a resolution of $1024^2$. Our findings are consistent
across three challenging and common high-resolution datasets, including FFHQ,
AFHQv2, and MetFaces.",1,1,0,0,0,0,0.205525,8.0,0.615151,66
18c56192-4725-40ff-871a-d4315e442014,Active Self-Training for Weakly Supervised 3D Scene Semantic Segmentation,5,0.110065,0.353525,"Since the preparation of labeled data for training semantic segmentation
networks of point clouds is a time-consuming process, weakly supervised
approaches have been introduced to learn from only a small fraction of data.
These methods are typically based on learning with contrastive losses while
automatically deriving per-point pseudo-labels from a sparse set of
user-annotated labels. In this paper, our key observation is that the selection
of what samples to annotate is as important as how these samples are used for
training. Thus, we introduce a method for weakly supervised segmentation of 3D
scenes that combines self-training with active learning. The active learning
selects points for annotation that likely result in performance improvements to
the trained model, while the self-training makes efficient use of the
user-provided labels for learning the model. We demonstrate that our approach
leads to an effective method that provides improvements in scene segmentation
over previous works and baselines, while requiring only a small number of user
annotations.",0,1,0,0,0,0,0.877765,7.0,0.876194,40
dea930b6-4bb9-47bd-aac9-b4b3cc964204,Neuro-Symbolic Verification of Deep Neural Networks,11,0.181224,0.435713,"Formal verification has emerged as a powerful approach to ensure the safety
and reliability of deep neural networks. However, current verification tools
are limited to only a handful of properties that can be expressed as
first-order constraints over the inputs and output of a network. While
adversarial robustness and fairness fall under this category, many real-world
properties (e.g., ""an autonomous vehicle has to stop in front of a stop sign"")
remain outside the scope of existing verification technology. To mitigate this
severe practical restriction, we introduce a novel framework for verifying
neural networks, named neuro-symbolic verification. The key idea is to use
neural networks as part of the otherwise logical specification, enabling the
verification of a wide variety of complex, real-world properties, including the
one above. Moreover, we demonstrate how neuro-symbolic verification can be
implemented on top of existing verification infrastructure for neural networks,
making our framework easily accessible to researchers and practitioners alike.",1,0,1,0,0,0,0.692657,6.0,0.759317,47
0dc3429e-a395-4072-972b-14cfe60887e4,Cross Attention Based Style Distribution for Controllable Person Image Synthesis,38,0.260371,0.900514,"Controllable person image synthesis task enables a wide range of applications
through explicit control over body pose and appearance. In this paper, we
propose a cross attention based style distribution module that computes between
the source semantic styles and target pose for pose transfer. The module
intentionally selects the style represented by each semantic and distributes
them according to the target pose. The attention matrix in cross attention
expresses the dynamic similarities between the target pose and the source
styles for all semantics. Therefore, it can be utilized to route the color and
texture from the source image, and is further constrained by the target parsing
map to achieve a clearer objective. At the same time, to encode the source
appearance accurately, the self attention among different semantic styles is
also added. The effectiveness of our model is validated quantitatively and
qualitatively on pose transfer and virtual try-on tasks.",1,1,0,0,0,0,0.692287,8.0,0.81936,50
4af18046-723f-49cc-8ca7-b6dba2929302,Learning to Execute Actions or Ask Clarification Questions,29,0.230022,0.82288,"Collaborative tasks are ubiquitous activities where a form of communication
is required in order to reach a joint goal. Collaborative building is one of
such tasks. We wish to develop an intelligent builder agent in a simulated
building environment (Minecraft) that can build whatever users wish to build by
just talking to the agent. In order to achieve this goal, such agents need to
be able to take the initiative by asking clarification questions when further
information is needed. Existing works on Minecraft Corpus Dataset only learn to
execute instructions neglecting the importance of asking for clarifications. In
this paper, we extend the Minecraft Corpus Dataset by annotating all builder
utterances into eight types, including clarification questions, and propose a
new builder agent model capable of determining when to ask or execute
instructions. Experimental results show that our model achieves
state-of-the-art performance on the collaborative building task with a
substantial improvement. We also define two new tasks, the learning to ask task
and the joint learning task. The latter consists of solving both collaborating
building and learning to ask tasks jointly.",1,1,0,1,1,0,0.219396,6.0,0.499163,45
fd6cc292-9c7d-42c9-bbd7-8f1233fc4b8d,CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command Recognition,9,0.755374,0.638463,"With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.",1,1,1,1,0,0,0.987519,5.0,0.973684,37
9f43a72c-685c-4f21-881f-28fce64552f5,ViewNeRF: Unsupervised Viewpoint Estimation Using Category-Level Neural Radiance Fields,1,0.0222169,0.0426664,"We introduce ViewNeRF, a Neural Radiance Field-based viewpoint estimation
method that learns to predict category-level viewpoints directly from images
during training. While NeRF is usually trained with ground-truth camera poses,
multiple extensions have been proposed to reduce the need for this expensive
supervision. Nonetheless, most of these methods still struggle in complex
settings with large camera movements, and are restricted to single scenes, i.e.
they cannot be trained on a collection of scenes depicting the same object
category. To address these issues, our method uses an analysis by synthesis
approach, combining a conditional NeRF with a viewpoint predictor and a scene
encoder in order to produce self-supervised reconstructions for whole object
categories. Rather than focusing on high fidelity reconstruction, we target
efficient and accurate viewpoint prediction in complex scenarios, e.g.
360{\deg} rotation on real data. Our model shows competitive results on
synthetic and real datasets, both for single scenes and multi-instance
collections.",0,1,0,0,0,0,0.798415,8.0,0.857691,55
27c09d40-1d10-425a-92ea-0751cdbbaefb,Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control,8,0.426155,0.385174,"We present Free-HeadGAN, a person-generic neural talking head synthesis
system. We show that modeling faces with sparse 3D facial landmarks are
sufficient for achieving state-of-the-art generative performance, without
relying on strong statistical priors of the face, such as 3D Morphable Models.
Apart from 3D pose and facial expressions, our method is capable of fully
transferring the eye gaze, from a driving actor to a source identity. Our
complete pipeline consists of three components: a canonical 3D key-point
estimator that regresses 3D pose and expression-related deformations, a gaze
estimation network and a generator that is built upon the architecture of
HeadGAN. We further experiment with an extension of our generator to
accommodate few-shot learning using an attention mechanism, in case more than
one source images are available. Compared to the latest models for reenactment
and motion transfer, our system achieves higher photo-realism combined with
superior identity preservation, while offering explicit gaze control.",0,1,0,0,0,0,0.98398,9.0,0.978866,80
bfb8c7c8-367c-40fb-bf72-7c6169626612,Visualizing Global Explanations of Point Cloud DNNs,4,0.0515103,0.152549,"In the field of autonomous driving and robotics, point clouds are showing
their excellent real-time performance as raw data from most of the mainstream
3D sensors. Therefore, point cloud neural networks have become a popular
research direction in recent years. So far, however, there has been little
discussion about the explainability of deep neural networks for point clouds.
In this paper, we propose a point cloud-applicable explainability approach
based on a local surrogate model-based method to show which components
contribute to the classification. Moreover, we propose quantitative fidelity
validations for generated explanations that enhance the persuasive power of
explainability and compare the plausibility of different existing point
cloud-applicable explainability methods. Our new explainability approach
provides a fairly accurate, more semantically coherent and widely applicable
explanation for point cloud classification tasks. Our code is available at
https://github.com/Explain3D/LIME-3D",1,0,0,0,0,0,0.606056,10.0,0.831965,45
ab368ef8-1870-4bc3-90f2-060b9098e26e,E2E Refined Dataset,1,0.00724965,0.0229432,"Although the well-known MR-to-text E2E dataset has been used by many
researchers, its MR-text pairs include many deletion/insertion/substitution
errors. Since such errors affect the quality of MR-to-text systems, they must
be fixed as much as possible. Therefore, we developed a refined dataset and
some python programs that convert the original E2E dataset into a refined
dataset.",0,1,0,0,0,0,0.208454,14.0,0.781224,16
b56e075a-1f78-447a-be3d-b5a678a6fe32,"""This is Fake! Shared it by Mistake"": Assessing the Intent of Fake News Spreaders",18,0.370389,0.887701,"Individuals can be misled by fake news and spread it unintentionally without
knowing it is false. This phenomenon has been frequently observed but has not
been investigated. Our aim in this work is to assess the intent of fake news
spreaders. To distinguish between intentional versus unintentional spreading,
we study the psychological explanations of unintentional spreading. With this
foundation, we then propose an influence graph, using which we assess the
intent of fake news spreaders. Our extensive experiments show that the assessed
intent can help significantly differentiate between intentional and
unintentional fake news spreaders. Furthermore, the estimated intent can
significantly improve the current techniques that detect fake news. To our best
knowledge, this is the first work to model individuals' intent in fake news
spreading.",0,1,0,0,0,0,0.943958,7.0,0.921275,46
ddb92509-ed64-476e-9acc-0989d81f4db5,MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering,43,0.194625,0.783259,"Visual language data such as plots, charts, and infographics are ubiquitous
in the human world. However, state-of-the-art vision-language models do not
perform well on these data. We propose MatCha (Math reasoning and Chart
derendering pretraining) to enhance visual language models' capabilities in
jointly modeling charts/plots and language data. Specifically, we propose
several pretraining tasks that cover plot deconstruction and numerical
reasoning which are the key capabilities in visual language modeling.
  We perform the MatCha pretraining starting from Pix2Struct, a recently
proposed image-to-text visual language model. On standard benchmarks such as
PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as
much as nearly 20%. We also examine how well MatCha pretraining transfers to
domains such as screenshots, textbook diagrams, and document figures and
observe overall improvement, verifying the usefulness of MatCha pretraining on
broader visual language tasks.",0,1,1,0,1,0,0.488443,5.0,0.598105,48
d3dff66d-a546-4ab2-94fd-d23e31eddff4,CVAE-H: Conditionalizing Variational Autoencoders via Hypernetworks and Trajectory Forecasting for Autonomous Driving,8,0.0435799,0.253888,"The task of predicting stochastic behaviors of road agents in diverse
environments is a challenging problem for autonomous driving. To best
understand scene contexts and produce diverse possible future states of the
road agents adaptively in different environments, a prediction model should be
probabilistic, multi-modal, context-driven, and general. We present
Conditionalizing Variational AutoEncoders via Hypernetworks (CVAE-H); a
conditional VAE that extensively leverages hypernetwork and performs generative
tasks for high-dimensional problems like the prediction task. We first evaluate
CVAE-H on simple generative experiments to show that CVAE-H is probabilistic,
multi-modal, context-driven, and general. Then, we demonstrate that the
proposed model effectively solves a self-driving prediction problem by
producing accurate predictions of road agents in various environments.",0,1,0,0,0,0,0.657613,9.0,0.828879,47
00fb004d-d6b8-4aa6-8cd2-8a7bf88c6549,Training Language Models with Language Feedback,38,0.216883,0.746939,"Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level summarization
ability.",0,1,0,0,0,1,0.737535,4.0,0.670367,43
71ca7f3f-906b-442f-9024-e853b76db4a4,"Automated Clinical Coding: What, Why, and Where We Are?",28,0.254466,0.95329,"Clinical coding is the task of transforming medical information in a
patient's health records into structured codes so that they can be used for
statistical analysis. This is a cognitive and time-consuming task that follows
a standard process in order to achieve a high level of consistency. Clinical
coding could potentially be supported by an automated system to improve the
efficiency and accuracy of the process. We introduce the idea of automated
clinical coding and summarise its challenges from the perspective of Artificial
Intelligence (AI) and Natural Language Processing (NLP), based on the
literature, our project experience over the past two and half years (late 2019
- early 2022), and discussions with clinical coding experts in Scotland and the
UK. Our research reveals the gaps between the current deep learning-based
approach applied to clinical coding and the need for explainability and
consistency in real-world practice. Knowledge-based methods that represent and
reason the standard, explainable process of a task may need to be incorporated
into deep learning-based methods for clinical coding. Automated clinical coding
is a promising task for AI, despite the technical and organisational
challenges. Coders are needed to be involved in the development process. There
is much to achieve to develop and deploy an AI-based automated system to
support coding in the next five years and beyond.",0,0,0,0,0,0,0.357032,4.0,0.393337,107
9d1f9d7a-715f-40eb-b86d-31600375f6b0,Team FAL at CMCL 2022 Shared Task: Figuring out the correct recipe for predicting Eye-Tracking features using Pretrained Language Models,2,0.0306254,0.240295,"Eye-Tracking data is a very useful source of information to study cognition
and especially language comprehension in humans. In this paper, we describe our
systems for the CMCL 2022 shared task on predicting eye-tracking information.
We describe our experiments with pretrained models like BERT and XLM and the
different ways in which we used those representations to predict four
eye-tracking features. Along with analysing the effect of using two different
kinds of pretrained multilingual language models and different ways of pooling
the tokenlevel representations, we also explore how contextual information
affects the performance of the systems. Finally, we also explore if factors
like augmenting linguistic information affect the predictions. Our submissions
achieved an average MAE of 5.72 and ranked 5th in the shared task. The average
MAE showed further reduction to 5.25 in post task evaluation.",0,1,0,0,0,0,0.514171,6.0,0.677449,23
ce9fc796-c984-463d-9a91-ec358d18e25c,MMFN: Multi-Modal-Fusion-Net for End-to-End Driving,19,0.255604,0.708153,"Inspired by the fact that humans use diverse sensory organs to perceive the
world, sensors with different modalities are deployed in end-to-end driving to
obtain the global context of the 3D scene. In previous works, camera and LiDAR
inputs are fused through transformers for better driving performance. These
inputs are normally further interpreted as high-level map information to assist
navigation tasks. Nevertheless, extracting useful information from the complex
map input is challenging, for redundant information may mislead the agent and
negatively affect driving performance. We propose a novel approach to
efficiently extract features from vectorized High-Definition (HD) maps and
utilize them in the end-to-end driving tasks. In addition, we design a new
expert to further enhance the model performance by considering multi-road
rules. Experimental results prove that both of the proposed improvements enable
our agent to achieve superior performance compared with other methods.",1,1,0,0,0,0,0.680874,5.0,0.704698,32
5562c574-5013-4cc6-a9e9-e8ef1ba7869b,IDEAL: Query-Efficient Data-Free Learning from Black-box Models,7,0.0449713,0.382572,"Knowledge Distillation (KD) is a typical method for training a lightweight
student model with the help of a well-trained teacher model. However, most KD
methods require access to either the teacher's training data or model
parameters, which is unrealistic. To tackle this problem, recent works study KD
under data-free and black-box settings. Nevertheless, these works require a
large number of queries to the teacher model, which incurs significant monetary
and computational costs. To address these problems, we propose a novel method
called \emph{query-effIcient Data-free lEarning from blAck-box modeLs} (IDEAL),
which aims to query-efficiently learn from black-box model APIs to train a good
student without any real data. In detail, IDEAL trains the student model in two
stages: data generation and model distillation. Note that IDEAL does not
require any query in the data generation stage and queries the teacher only
once for each sample in the distillation stage. Extensive experiments on
various real-world datasets show the effectiveness of the proposed IDEAL. For
instance, IDEAL can improve the performance of the best baseline method DFME by
5.83% on CIFAR10 dataset with only 0.02x the query budget of DFME.",0,1,0,0,0,0,0.377606,5.0,0.528879,44
43a4882c-2830-484e-aa2d-ffb46151eae4,Optimizing Multiple Simultaneous Objectives for Voting and Facility Location,3,0.0264264,0.230901,"We study the classic facility location setting, where we are given $n$
clients and $m$ possible facility locations in some arbitrary metric space, and
want to choose a location to build a facility. The exact same setting also
arises in spatial social choice, where voters are the clients and the goal is
to choose a candidate or outcome, with the distance from a voter to an outcome
representing the cost of this outcome for the voter (e.g., based on their
ideological differences). Unlike most previous work, we do not focus on a
single objective to optimize (e.g., the total distance from clients to the
facility, or the maximum distance, etc.), but instead attempt to optimize
several different objectives simultaneously. More specifically, we consider the
$l$-centrum family of objectives, which includes the total distance, max
distance, and many others. We present tight bounds on how well any pair of such
objectives (e.g., max and sum) can be simultaneously approximated compared to
their optimum outcomes. In particular, we show that for any such pair of
objectives, it is always possible to choose an outcome which simultaneously
approximates both objectives within a factor of $1+\sqrt{2}$, and give a
precise characterization of how this factor improves as the two objectives
being optimized become more similar. For $q>2$ different centrum objectives, we
show that it is always possible to approximate all $q$ of these objectives
within a small constant, and that this constant approaches 3 as $q\rightarrow
\infty$. Our results show that when optimizing only a few simultaneous
objectives, it is always possible to form an outcome which is a significantly
better than 3 approximation for all of these objectives.",0,0,0,0,0,0,1.0052e-06,26.0,0.406932,31
1c843553-a694-426c-aca1-486c04b3536f,Arabic Word-level Readability Visualization for Assisted Text Simplification,3,0.096784,0.457826,"This demo paper presents a Google Docs add-on for automatic Arabic word-level
readability visualization. The add-on includes a lemmatization component that
is connected to a five-level readability lexicon and Arabic WordNet-based
substitution suggestions. The add-on can be used for assessing the reading
difficulty of a text and identifying difficult words as part of the task of
manual text simplification. We make our add-on and its code publicly available.",0,1,0,0,0,0,0.161988,9.0,0.628599,39
887c2ce3-ef72-49b9-9f29-ecba23b05a34,SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory,10,0.150576,0.351716,"Neural Radiance Fields (NeRF) have demonstrated superior novel view synthesis
performance but are slow at rendering. To speed up the volume rendering
process, many acceleration methods have been proposed at the cost of large
memory consumption. To push the frontier of the efficiency-memory trade-off, we
explore a new perspective to accelerate NeRF rendering, leveraging a key fact
that the viewpoint change is usually smooth and continuous in interactive
viewpoint control. This allows us to leverage the information of preceding
viewpoints to reduce the number of rendered pixels as well as the number of
sampled points along the ray of the remaining pixels. In our pipeline, a
low-resolution feature map is rendered first by volume rendering, then a
lightweight 2D neural renderer is applied to generate the output image at
target resolution leveraging the features of preceding and current frames. We
show that the proposed method can achieve competitive rendering quality while
reducing the rendering time with little memory overhead, enabling 30FPS at
1080P image resolution with a low memory footprint.",0,1,0,0,0,0,0.943893,4.0,0.862131,65
91016dbd-841f-4c59-bef3-8722ded91e26,Trustworthy Autonomous Systems (TAS): Engaging TAS experts in curriculum design,6,0.40815,0.624536,"Recent advances in artificial intelligence, specifically machine learning,
contributed positively to enhancing the autonomous systems industry, along with
introducing social, technical, legal and ethical challenges to make them
trustworthy. Although Trustworthy Autonomous Systems (TAS) is an established
and growing research direction that has been discussed in multiple disciplines,
e.g., Artificial Intelligence, Human-Computer Interaction, Law, and Psychology.
The impact of TAS on education curricula and required skills for future TAS
engineers has rarely been discussed in the literature. This study brings
together the collective insights from a number of TAS leading experts to
highlight significant challenges for curriculum design and potential TAS
required skills posed by the rapid emergence of TAS. Our analysis is of
interest not only to the TAS education community but also to other researchers,
as it offers ways to guide future research toward operationalising TAS
education.",0,1,0,0,0,0,0.940501,5.0,0.885588,24
261a2577-9592-4a67-985d-178b206feeaa,3D Scene Inference from Transient Histograms,4,0.0116151,0.465629,"Time-resolved image sensors that capture light at pico-to-nanosecond
timescales were once limited to niche applications but are now rapidly becoming
mainstream in consumer devices. We propose low-cost and low-power imaging
modalities that capture scene information from minimal time-resolved image
sensors with as few as one pixel. The key idea is to flood illuminate large
scene patches (or the entire scene) with a pulsed light source and measure the
time-resolved reflected light by integrating over the entire illuminated area.
The one-dimensional measured temporal waveform, called \emph{transient},
encodes both distances and albedoes at all visible scene points and as such is
an aggregate proxy for the scene's 3D geometry. We explore the viability and
limitations of the transient waveforms by themselves for recovering scene
information, and also when combined with traditional RGB cameras. We show that
plane estimation can be performed from a single transient and that using only a
few more it is possible to recover a depth map of the whole scene. We also show
two proof-of-concept hardware prototypes that demonstrate the feasibility of
our approach for compact, mobile, and budget-limited applications.",0,1,0,0,0,0,0.0686196,8.0,0.46834,42
bddd40b6-8a0b-483a-8426-f602ad785921,DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation,23,0.078729,0.691153,"Task-oriented dialogue generation is challenging since the underlying
knowledge is often dynamic and effectively incorporating knowledge into the
learning process is hard. It is particularly challenging to generate both
human-like and informative responses in this setting. Recent research primarily
focused on various knowledge distillation methods where the underlying
relationship between the facts in a knowledge base is not effectively captured.
In this paper, we go one step further and demonstrate how the structural
information of a knowledge graph can improve the system's inference
capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue
system that effectively incorporates knowledge into a language model. Our
proposed system views relational knowledge as a knowledge graph and introduces
(1) a structure-aware knowledge embedding technique, and (2) a knowledge
graph-weighted attention masking strategy to facilitate the system selecting
relevant information during the dialogue generation. An empirical evaluation
demonstrates the effectiveness of DialoKG over state-of-the-art methods on
several standard benchmark datasets.",1,1,0,0,1,0,0.171782,7.0,0.531685,49
c38fb8bd-d862-4620-b398-db9fe2469d14,DuAT: Dual-Aggregation Transformer Network for Medical Image Segmentation,19,0.0734575,0.802303,"Transformer-based models have been widely demonstrated to be successful in
computer vision tasks by modelling long-range dependencies and capturing global
representations. However, they are often dominated by features of large
patterns leading to the loss of local details (e.g., boundaries and small
objects), which are critical in medical image segmentation. To alleviate this
problem, we propose a Dual-Aggregation Transformer Network called DuAT, which
is characterized by two innovative designs, namely, the Global-to-Local Spatial
Aggregation (GLSA) and Selective Boundary Aggregation (SBA) modules. The GLSA
has the ability to aggregate and represent both global and local spatial
features, which are beneficial for locating large and small objects,
respectively. The SBA module is used to aggregate the boundary characteristic
from low-level features and semantic information from high-level features for
better preserving boundary details and locating the re-calibration objects.
Extensive experiments in six benchmark datasets demonstrate that our proposed
model outperforms state-of-the-art methods in the segmentation of skin lesion
images, and polyps in colonoscopy images. In addition, our approach is more
robust than existing methods in various challenging situations such as small
object segmentation and ambiguous object boundaries.",0,1,0,0,1,0,0.610521,7.0,0.761688,59
9fed0e45-06aa-47e8-8283-704f35c7865c,Robot Vitals and Robot Health: Towards Systematically Quantifying Runtime Performance Degradation in Robots Under Adverse Conditions,10,0.365849,0.594093,"This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.",0,1,0,0,0,0,0.11093,17.0,0.779407,29
6f3b4ae3-6688-4205-a681-495d14fbc8aa,TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition,56,0.926913,0.531379,"Creation of 3D content by stylization is a promising yet challenging problem
in computer vision and graphics research. In this work, we focus on stylizing
photorealistic appearance renderings of a given surface mesh of arbitrary
topology. Motivated by the recent surge of cross-modal supervision of the
Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which
transfers the appearance style of a given 3D shape according to a text prompt
in a photorealistic manner. Technically, we propose to disentangle the
appearance style as the spatially varying bidirectional reflectance
distribution function, the local geometric variation, and the lighting
condition, which are jointly optimized, via supervision of the CLIP loss, by a
spherical Gaussians based differentiable renderer. As such, TANGO enables
photorealistic 3D style transfer by automatically predicting reflectance
effects even for bare, low-quality meshes, without training on a task-specific
dataset. Extensive experiments show that TANGO outperforms existing methods of
text-driven 3D style transfer in terms of photorealistic quality, consistency
of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and
results are available at our project webpage https://cyw-3d.github.io/tango/.",0,0,0,0,0,0,0.971521,6.0,0.943321,64
a40d3ce3-ad42-4846-923c-4c4ba4a281d1,TEMPERA: Test-Time Prompting via Reinforcement Learning,27,0.20986,0.726416,"Careful prompt design is critical to the use of large language models in
zero-shot or few-shot learning. As a consequence, there is a growing interest
in automated methods to design optimal prompts. In this work, we propose
Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to
prior prompt generation methods, TEMPERA can efficiently leverage prior
knowledge, is adaptive to different queries and provides an interpretable
prompt for every query. To achieve this, we design a novel action space that
allows flexible editing of the initial prompts covering a wide set of
commonly-used components like instructions, few-shot exemplars, and
verbalizers. The proposed method achieves significant gains compared with
recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a
variety of tasks including sentiment analysis, topic classification, natural
language inference, and reading comprehension. Our method achieves 5.33x on
average improvement in sample efficiency when compared to the traditional
fine-tuning methods.",1,1,0,0,1,1,0.949688,4.0,0.871419,34
3a92a602-168f-46b7-a14c-038cadd1e3ad,GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering,6,0.0363045,0.552016,"Content-based collaborative filtering (CCF) predicts user-item interactions
based on both users' interaction history and items' content information.
Recently, pre-trained language models (PLM) have been used to extract
high-quality item encodings for CCF. However, it is resource-intensive to train
a PLM-based CCF model in an end-to-end (E2E) manner, since optimization
involves back-propagating through every content encoding within a given user
interaction sequence. To tackle this issue, we propose GRAM (GRadient
Accumulation for Multi-modality in CCF), which exploits the fact that a given
item often appears multiple times within a batch of interaction histories.
Specifically, Single-step GRAM aggregates each item encoding's gradients for
back-propagation, with theoretic equivalence to the standard E2E training. As
an extension of Single-step GRAM, we propose Multi-step GRAM, which increases
the gradient update latency, achieving a further speedup with drastically less
GPU memory. GRAM significantly improves training efficiency (up to 146x) on
five datasets from two task domains of Knowledge Tracing and News
Recommendation. Our code is available at https://github.com/yoonseok312/GRAM.",1,1,0,0,0,0,0.561959,8.0,0.774841,35
acddf5d2-5905-43f8-a36d-0ac24d4755a6,A Double-Graph Based Framework for Frame Semantic Parsing,10,0.086337,0.739206,"Frame semantic parsing is a fundamental NLP task, which consists of three
subtasks: frame identification, argument identification and role
classification. Most previous studies tend to neglect relations between
different subtasks and arguments and pay little attention to ontological frame
knowledge defined in FrameNet. In this paper, we propose a Knowledge-guided
Incremental semantic parser with Double-graph (KID). We first introduce Frame
Knowledge Graph (FKG), a heterogeneous graph containing both frames and FEs
(Frame Elements) built on the frame knowledge so that we can derive
knowledge-enhanced representations for frames and FEs. Besides, we propose
Frame Semantic Graph (FSG) to represent frame semantic structures extracted
from the text with graph structures. In this way, we can transform frame
semantic parsing into an incremental graph construction problem to strengthen
interactions between subtasks and relations between arguments. Our experiments
show that KID outperforms the previous state-of-the-art method by up to 1.7
F1-score on two FrameNet datasets. Our code is availavle at
https://github.com/PKUnlp-icler/KID.",1,0,0,0,1,0,0.077876,11.0,0.62529,49
18b1e68f-7b13-4b34-951f-f9a9485145aa,Mobile Robot Manipulation using Pure Object Detection,1,0.0613399,0.0499145,"This paper addresses the problem of mobile robot manipulation using object
detection. Our approach uses detection and control as complimentary functions
that learn from real-world interactions. We develop an end-to-end manipulation
method based solely on detection and introduce Task-focused Few-shot Object
Detection (TFOD) to learn new objects and settings. Our robot collects its own
training data and automatically determines when to retrain detection to improve
performance across various subtasks (e.g., grasping). Notably, detection
training is low-cost, and our robot learns to manipulate new objects using as
few as four clicks of annotation. In physical experiments, our robot learns
visual control from a single click of annotation and a novel update
formulation, manipulates new objects in clutter and other mobile settings, and
achieves state-of-the-art results on an existing visual servo control and depth
estimation benchmark. Finally, we develop a TFOD Benchmark to support future
object detection research for robotics: https://github.com/griffbr/tfod.",1,1,0,0,1,0,0.838188,7.0,0.855732,71
3369d323-1ceb-43c2-93be-21f249bc73ea,"Explainable Artificial Intelligence in Construction: The Content, Context, Process, Outcome Evaluation Framework",1,0.0461959,0.096465,"Explainable artificial intelligence is an emerging and evolving concept. Its
impact on construction, though yet to be realised, will be profound in the
foreseeable future. Still, XAI has received limited attention in construction.
As a result, no evaluation frameworks have been propagated to enable
construction organisations to understand the what, why, how, and when of XAI.
Our paper aims to fill this void by developing a content, context, process, and
outcome evaluation framework that can be used to justify the adoption and
effective management of XAI. After introducing and describing this novel
framework, we discuss its implications for future research. While our novel
framework is conceptual, it provides a frame of reference for construction
organisations to make headway toward realising XAI business value and benefits.",0,0,0,0,0,0,0.663214,8.0,0.809398,81
5f73878e-48e9-4231-a0af-1e5b3b7e172d,Human-centric Image Cropping with Partition-aware and Content-preserving Features,5,0.0887584,0.511555,"Image cropping aims to find visually appealing crops in an image, which is an
important yet challenging task. In this paper, we consider a specific and
practical application: human-centric image cropping, which focuses on the
depiction of a person. To this end, we propose a human-centric image cropping
method with two novel feature designs for the candidate crop: partition-aware
feature and content-preserving feature. For partition-aware feature, we divide
the whole image into nine partitions based on the human bounding box and treat
different partitions in a candidate crop differently conditioned on the human
information. For content-preserving feature, we predict a heatmap indicating
the important content to be included in a good crop, and extract the geometric
relation between the heatmap and a candidate crop. Extensive experiments
demonstrate that our method can perform favorably against state-of-the-art
image cropping methods on human-centric image cropping task. Code is available
at https://github.com/bcmi/Human-Centric-Image-Cropping.",1,0,0,0,0,0,0.154514,11.0,0.69144,53
956461ef-03e5-4357-b941-1b3e67e348f1,Data Augmentation to Address Out-of-Vocabulary Problem in Low-Resource Sinhala-English Neural Machine Translation,5,0.0171715,0.186166,"Out-of-Vocabulary (OOV) is a problem for Neural Machine Translation (NMT).
OOV refers to words with a low occurrence in the training data, or to those
that are absent from the training data. To alleviate this, word or phrase-based
Data Augmentation (DA) techniques have been used. However, existing DA
techniques have addressed only one of these OOV types and limit to considering
either syntactic constraints or semantic constraints. We present a word and
phrase replacement-based DA technique that consider both types of OOV, by
augmenting (1) rare words in the existing parallel corpus, and (2) new words
from a bilingual dictionary. During augmentation, we consider both syntactic
and semantic properties of the words to guarantee fluency in the synthetic
sentences. This technique was experimented with low resource Sinhala-English
language pair. We observe with only semantic constraints in the DA, the results
are comparable with the scores obtained considering syntactic constraints, and
is favourable for low-resourced languages that lacks linguistic tool support.
Additionally, results can be further improved by considering both syntactic and
semantic constraints.",0,1,0,0,0,0,0.029699,8.0,0.361119,37
ad2a2cfa-25af-4bff-8af4-2d623655130d,Depth Map Decomposition for Monocular Depth Estimation,12,0.195117,0.657514,"We propose a novel algorithm for monocular depth estimation that decomposes a
metric depth map into a normalized depth map and scale features. The proposed
network is composed of a shared encoder and three decoders, called G-Net,
N-Net, and M-Net, which estimate gradient maps, a normalized depth map, and a
metric depth map, respectively. M-Net learns to estimate metric depths more
accurately using relative depth features extracted by G-Net and N-Net. The
proposed algorithm has the advantage that it can use datasets without metric
depth labels to improve the performance of metric depth estimation.
Experimental results on various datasets demonstrate that the proposed
algorithm not only provides competitive performance to state-of-the-art
algorithms but also yields acceptable results even when only a small amount of
metric depth data is available for its training.",0,0,0,0,0,0,0.867656,9.0,0.899425,58
aa88c883-5c6a-42eb-a5c9-2cb089a14a07,Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors,15,0.236576,0.778976,"We present a method for inferring diverse 3D models of human-object
interactions from images. Reasoning about how humans interact with objects in
complex scenes from a single 2D image is a challenging task given ambiguities
arising from the loss of information through projection. In addition, modeling
3D interactions requires the generalization ability towards diverse object
categories and interaction types. We propose an action-conditioned modeling of
interactions that allows us to infer diverse 3D arrangements of humans and
objects without supervision on contact regions or 3D scene geometry. Our method
extracts high-level commonsense knowledge from large language models (such as
GPT-3), and applies them to perform 3D reasoning of human-object interactions.
Our key insight is priors extracted from large language models can help in
reasoning about human-object contacts from textural prompts only. We
quantitatively evaluate the inferred 3D models on a large human-object
interaction dataset and show how our method leads to better 3D reconstructions.
We further qualitatively evaluate the effectiveness of our method on real
images and demonstrate its generalizability towards interaction types and
object categories.",0,0,0,0,0,0,0.842183,6.0,0.833959,43
30004d39-3172-4311-b318-8b6a396f7823,CATrans: Context and Affinity Transformer for Few-Shot Segmentation,14,0.119492,0.260079,"Few-shot segmentation (FSS) aims to segment novel categories given scarce
annotated support images. The crux of FSS is how to aggregate dense
correlations between support and query images for query segmentation while
being robust to the large variations in appearance and context. To this end,
previous Transformer-based methods explore global consensus either on context
similarity or affinity map between support-query pairs. In this work, we
effectively integrate the context and affinity information via the proposed
novel Context and Affinity Transformer (CATrans) in a hierarchical
architecture. Specifically, the Relation-guided Context Transformer (RCT)
propagates context information from support to query images conditioned on more
informative support features. Based on the observation that a huge feature
distinction between support and query pairs brings barriers for context
knowledge transfer, the Relation-guided Affinity Transformer (RAT) measures
attention-aware affinity as auxiliary information for FSS, in which the
self-affinity is responsible for more reliable cross-affinity. We conduct
experiments to demonstrate the effectiveness of the proposed model,
outperforming the state-of-the-art methods.",0,0,0,0,1,0,0.885261,6.0,0.860504,24
2bee3bd7-dfc1-498d-90e4-0297a9335ec2,Cost-Effective Online Contextual Model Selection,5,0.0425242,0.583542,"How can we collect the most useful labels to learn a model selection policy,
when presented with arbitrary heterogeneous data streams? In this paper, we
formulate this task as an online contextual active model selection problem,
where at each round the learner receives an unlabeled data point along with a
context. The goal is to output the best model for any given context without
obtaining an excessive amount of labels. In particular, we focus on the task of
selecting pre-trained classifiers, and propose a contextual active model
selection algorithm (CAMS), which relies on a novel uncertainty sampling query
criterion defined on a given policy class for adaptive model selection. In
comparison to prior art, our algorithm does not assume a globally optimal
model. We provide rigorous theoretical analysis for the regret and query
complexity under both adversarial and stochastic settings. Our experiments on
several benchmark classification datasets demonstrate the algorithm's
effectiveness in terms of both regret and query complexity. Notably, to achieve
the same accuracy, CAMS incurs less than 10% of the label cost when compared to
the best online model selection baselines on CIFAR10.",0,0,0,0,0,0,0.029202,19.0,0.730096,79
6d1fbc16-17b2-4ac0-9936-06d27f4822cb,BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model,74,0.859567,0.668824,"Pretrained language models have served as important backbones for natural
language processing. Recently, in-domain pretraining has been shown to benefit
various domain-specific downstream tasks. In the biomedical domain, natural
language generation (NLG) tasks are of critical importance, while understudied.
Approaching natural language understanding (NLU) tasks as NLG achieves
satisfying performance in the general domain through constrained language
generation or language prompting. We emphasize the lack of in-domain generative
language models and the unsystematic generative downstream benchmarks in the
biomedical domain, hindering the development of the research community. In this
work, we introduce the generative language model BioBART that adapts BART to
the biomedical domain. We collate various biomedical language generation tasks
including dialogue, summarization, entity linking, and named entity
recognition. BioBART pretrained on PubMed abstracts has enhanced performance
compared to BART and set strong baselines on several tasks. Furthermore, we
conduct ablation studies on the pretraining tasks for BioBART and find that
sentence permutation has negative effects on downstream tasks.",1,1,0,0,0,0,0.884034,6.0,0.859684,80
d5f69485-a9d0-4310-872c-68aefd2d9489,How to Understand Masked Autoencoders,36,0.228851,0.517066,"""Masked Autoencoders (MAE) Are Scalable Vision Learners"" revolutionizes the
self-supervised learning method in that it not only achieves the
state-of-the-art for image pre-training, but is also a milestone that bridges
the gap between visual and linguistic masked autoencoding (BERT-style)
pre-trainings. However, to our knowledge, to date there are no theoretical
perspectives to explain the powerful expressivity of MAE. In this paper, we,
for the first time, propose a unified theoretical framework that provides a
mathematical understanding for MAE. Specifically, we explain the patch-based
attention approaches of MAE using an integral kernel under a non-overlapping
domain decomposition setting. To help the research community to further
comprehend the main reasons of the great success of MAE, based on our
framework, we pose five questions and answer them with mathematical rigor using
insights from operator theory.",1,0,0,0,0,0,0.880965,6.0,0.857651,55
8c92f3fe-0adf-489b-b4b4-9a43a20d5cc9,Variational Reasoning over Incomplete Knowledge Graphs for Conversational Recommendation,14,0.254679,0.629163,"Conversational recommender systems (CRSs) often utilize external knowledge
graphs (KGs) to introduce rich semantic information and recommend relevant
items through natural language dialogues. However, original KGs employed in
existing CRSs are often incomplete and sparse, which limits the reasoning
capability in recommendation. Moreover, only few of existing studies exploit
the dialogue context to dynamically refine knowledge from KGs for better
recommendation. To address the above issues, we propose the Variational
Reasoning over Incomplete KGs Conversational Recommender (VRICR). Our key idea
is to incorporate the large dialogue corpus naturally accompanied with CRSs to
enhance the incomplete KGs; and perform dynamic knowledge reasoning conditioned
on the dialogue context. Specifically, we denote the dialogue-specific
subgraphs of KGs as latent variables with categorical priors for adaptive
knowledge graphs refactor. We propose a variational Bayesian method to
approximate posterior distributions over dialogue-specific subgraphs, which not
only leverages the dialogue corpus for restructuring missing entity relations
but also dynamically selects knowledge based on the dialogue context. Finally,
we infuse the dialogue-specific subgraphs to decode the recommendation and
responses. We conduct experiments on two benchmark CRSs datasets. Experimental
results confirm the effectiveness of our proposed method.",1,0,0,0,0,0,0.70142,8.0,0.822515,40
4c3dc8c2-bb75-4891-9a01-79f2a27f6ad1,Variable Bitrate Neural Fields,77,0.735546,0.999999,"Neural approximations of scalar and vector fields, such as signed distance
functions and radiance fields, have emerged as accurate, high-quality
representations. State-of-the-art results are obtained by conditioning a neural
approximation with a lookup from trainable feature grids that take on part of
the learning task and allow for smaller, more efficient neural networks.
Unfortunately, these feature grids usually come at the cost of significantly
increased memory consumption compared to stand-alone neural network models. We
present a dictionary method for compressing such feature grids, reducing their
memory consumption by up to 100x and permitting a multiresolution
representation which can be useful for out-of-core streaming. We formulate the
dictionary optimization as a vector-quantized auto-decoder problem which lets
us learn end-to-end discrete neural representations in a space where no direct
supervision is available and with dynamic topology and structure. Our source
code will be available at https://github.com/nv-tlabs/vqad.",0,1,0,0,0,0,0.752122,6.0,0.787221,63
1d756b38-b6b3-4f9e-8ab6-fe8a257e8aca,DiGamma: Domain-aware Genetic Algorithm for HW-Mapping Co-optimization for DNN Accelerators,14,0.767173,0.887689,"The design of DNN accelerators includes two key parts: HW resource
configuration and mapping strategy. Intensive research has been conducted to
optimize each of them independently. Unfortunately, optimizing for both
together is extremely challenging due to the extremely large cross-coupled
search space. To address this, in this paper, we propose a HW-Mapping
co-optimization framework, an efficient encoding of the immense design space
constructed by HW and Mapping, and a domain-aware genetic algorithm, named
DiGamma, with specialized operators for improving search efficiency. We
evaluate DiGamma with seven popular DNNs models with different properties. Our
evaluations show DiGamma can achieve (geomean) 3.0x and 10.0x speedup,
comparing to the best-performing baseline optimization algorithms, in edge and
cloud settings.",1,1,0,0,1,0,0.993219,8.0,0.999841,11
aa55b467-3f60-4b32-aa54-e853f97f868c,Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery,8,0.0189815,0.248232,"Discovering out-of-domain (OOD) intent is important for developing new skills
in task-oriented dialogue systems. The key challenges lie in how to transfer
prior in-domain (IND) knowledge to OOD clustering, as well as jointly learn OOD
representations and cluster assignments. Previous methods suffer from in-domain
overfitting problem, and there is a natural gap between representation learning
and clustering objectives. In this paper, we propose a unified K-nearest
neighbor contrastive learning framework to discover OOD intents. Specifically,
for IND pre-training stage, we propose a KCL objective to learn inter-class
discriminative features, while maintaining intra-class diversity, which
alleviates the in-domain overfitting problem. For OOD clustering stage, we
propose a KCC method to form compact clusters by mining true hard negative
samples, which bridges the gap between clustering and representation learning.
Extensive experiments on three benchmark datasets show that our method achieves
substantial improvements over the state-of-the-art methods.",1,1,0,0,1,0,0.232176,6.0,0.509916,35
06a1324e-180e-41b1-b730-0f047e404950,Integrating Policy Summaries with Reward Decomposition for Explaining Reinforcement Learning Agents,4,0.0348766,0.187615,"Explaining the behavior of reinforcement learning agents operating in
sequential decision-making settings is challenging, as their behavior is
affected by a dynamic environment and delayed rewards. Methods that help users
understand the behavior of such agents can roughly be divided into local
explanations that analyze specific decisions of the agents and global
explanations that convey the general strategy of the agents. In this work, we
study a novel combination of local and global explanations for reinforcement
learning agents. Specifically, we combine reward decomposition, a local
explanation method that exposes which components of the reward function
influenced a specific decision, and HIGHLIGHTS, a global explanation method
that shows a summary of the agent's behavior in decisive states. We conducted
two user studies to evaluate the integration of these explanation methods and
their respective benefits. Our results show significant benefits for both
methods. In general, we found that the local reward decomposition was more
useful for identifying the agents' priorities. However, when there was only a
minor difference between the agents' preferences, then the global information
provided by HIGHLIGHTS additionally improved participants' understanding.",1,1,0,0,0,0,0.137271,8.0,0.559711,21
71e1fa56-9c51-4eaa-bcc5-304f77601b89,NFLAT: Non-Flat-Lattice Transformer for Chinese Named Entity Recognition,12,0.32472,0.864484,"Recently, Flat-LAttice Transformer (FLAT) has achieved great success in
Chinese Named Entity Recognition (NER). FLAT performs lexical enhancement by
constructing flat lattices, which mitigates the difficulties posed by blurred
word boundaries and the lack of word semantics. In FLAT, the positions of
starting and ending characters are used to connect a matching word. However,
this method is likely to match more words when dealing with long texts,
resulting in long input sequences. Therefore, it significantly increases the
memory and computational costs of the self-attention module. To deal with this
issue, we advocate a novel lexical enhancement method, InterFormer, that
effectively reduces the amount of computational and memory costs by
constructing non-flat lattices. Furthermore, with InterFormer as the backbone,
we implement NFLAT for Chinese NER. NFLAT decouples lexicon fusion and context
feature encoding. Compared with FLAT, it reduces unnecessary attention
calculations in ""word-character"" and ""word-word"". This reduces the memory usage
by about 50% and can use more extensive lexicons or higher batches for network
training. The experimental results obtained on several well-known benchmarks
demonstrate the superiority of the proposed method over the state-of-the-art
hybrid (character-word) models.",0,1,0,0,1,0,0.934058,10.0,0.939082,51
877e9d25-7aa4-48ca-b067-fa7eaafde023,Accelerating DETR Convergence via Semantic-Aligned Matching,72,0.778902,0.993459,"The recently developed DEtection TRansformer (DETR) establishes a new object
detection paradigm by eliminating a series of hand-crafted components. However,
DETR suffers from extremely slow convergence, which increases the training cost
significantly. We observe that the slow convergence is largely attributed to
the complication in matching object queries with target features in different
feature embedding spaces. This paper presents SAM-DETR, a
Semantic-Aligned-Matching DETR that greatly accelerates DETR's convergence
without sacrificing its accuracy. SAM-DETR addresses the convergence issue from
two perspectives. First, it projects object queries into the same embedding
space as encoded image features, where the matching can be accomplished
efficiently with aligned semantics. Second, it explicitly searches salient
points with the most discriminative features for semantic-aligned matching,
which further speeds up the convergence and boosts detection accuracy as well.
Being like a plug and play, SAM-DETR complements existing convergence solutions
well yet only introduces slight computational overhead. Extensive experiments
show that the proposed SAM-DETR achieves superior convergence as well as
competitive detection accuracy. The implementation codes are available at
https://github.com/ZhangGongjie/SAM-DETR.",1,1,0,0,0,0,0.971349,8.0,0.957279,64
a9ac386b-687a-4098-af67-f9617ddf2d29,Covariance Matrix Adaptation MAP-Annealing,12,0.0696248,0.43636,"Single-objective optimization algorithms search for the single
highest-quality solution with respect to an objective. Quality diversity (QD)
optimization algorithms, such as Covariance Matrix Adaptation MAP-Elites
(CMA-ME), search for a collection of solutions that are both high-quality with
respect to an objective and diverse with respect to specified measure
functions. However, CMA-ME suffers from three major limitations highlighted by
the QD community: prematurely abandoning the objective in favor of exploration,
struggling to explore flat objectives, and having poor performance for
low-resolution archives. We propose a new quality diversity algorithm,
Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), that addresses all three
limitations. We provide theoretical justifications for the new algorithm with
respect to each limitation. Our theory informs our experiments, which support
the theory and show that CMA-MAE achieves state-of-the-art performance and
robustness.",0,0,0,0,1,0,0.231559,5.0,0.41129,92
36510ad8-5c10-46ad-880c-061df655b9dd,Towards Open Set Video Anomaly Detection,7,0.204898,0.463687,"Open Set Video Anomaly Detection (OpenVAD) aims to identify abnormal events
from video data where both known anomalies and novel ones exist in testing.
Unsupervised models learned solely from normal videos are applicable to any
testing anomalies but suffer from a high false positive rate. In contrast,
weakly supervised methods are effective in detecting known anomalies but could
fail in an open world. We develop a novel weakly supervised method for the
OpenVAD problem by integrating evidential deep learning (EDL) and normalizing
flows (NFs) into a multiple instance learning (MIL) framework. Specifically, we
propose to use graph neural networks and triplet loss to learn discriminative
features for training the EDL classifier, where the EDL is capable of
identifying the unknown anomalies by quantifying the uncertainty. Moreover, we
develop an uncertainty-aware selection strategy to obtain clean anomaly
instances and a NFs module to generate the pseudo anomalies. Our method is
superior to existing approaches by inheriting the advantages of both the
unsupervised NFs and the weakly-supervised MIL framework. Experimental results
on multiple real-world video datasets show the effectiveness of our method.",0,1,1,0,0,0,0.877614,9.0,0.903641,62
0f5b6867-291d-49c6-be11-eaa26a3fa321,Semi-supervised detection of structural damage using Variational Autoencoder and a One-Class Support Vector Machine,5,0.135767,0.393621,"In recent years, Artificial Neural Networks (ANNs) have been introduced in
Structural Health Monitoring (SHM) systems. A semi-supervised method with a
data-driven approach allows the ANN training on data acquired from an undamaged
structural condition to detect structural damages. In standard approaches,
after the training stage, a decision rule is manually defined to detect
anomalous data. However, this process could be made automatic using machine
learning methods, whom performances are maximised using hyperparameter
optimization techniques. The paper proposes a semi-supervised method with a
data-driven approach to detect structural anomalies. The methodology consists
of: (i) a Variational Autoencoder (VAE) to approximate undamaged data
distribution and (ii) a One-Class Support Vector Machine (OC-SVM) to
discriminate different health conditions using damage sensitive features
extracted from VAE's signal reconstruction. The method is applied to a scale
steel structure that was tested in nine damage's scenarios by IASC-ASCE
Structural Health Monitoring Task Group.",0,1,0,0,0,0,0.5448,9.0,0.794562,91
0127ef36-d1d9-4074-a68e-c29c1630dc4f,Globally Optimal Event-Based Divergence Estimation for Ventral Landing,5,0.0548415,0.661295,"Event sensing is a major component in bio-inspired flight guidance and
control systems. We explore the usage of event cameras for predicting
time-to-contact (TTC) with the surface during ventral landing. This is achieved
by estimating divergence (inverse TTC), which is the rate of radial optic flow,
from the event stream generated during landing. Our core contributions are a
novel contrast maximisation formulation for event-based divergence estimation,
and a branch-and-bound algorithm to exactly maximise contrast and find the
optimal divergence value. GPU acceleration is conducted to speed up the global
algorithm. Another contribution is a new dataset containing real event streams
from ventral landing that was employed to test and benchmark our method. Owing
to global optimisation, our algorithm is much more capable at recovering the
true divergence, compared to other heuristic divergence estimators or
event-based optic flow methods. With GPU acceleration, our method also achieves
competitive runtimes.",0,0,1,0,1,1,0.184742,8.0,0.60027,60
2fdcd238-cf4f-4031-b5e8-ec331614ddb1,Computable Artificial General Intelligence,3,0.0454775,0.166708,"Artificial general intelligence (AGI) may herald our extinction, according to
AI safety research. Yet claims regarding AGI must rely upon mathematical
formalisms -- theoretical agents we may analyse or attempt to build. AIXI
appears to be the only such formalism supported by proof that its behaviour is
optimal, a consequence of its use of compression as a proxy for intelligence.
Unfortunately, AIXI is incomputable and claims regarding its behaviour highly
subjective. We argue that this is because AIXI formalises cognition as taking
place in isolation from the environment in which goals are pursued (Cartesian
dualism). We propose an alternative, supported by proof and experiment, which
overcomes these problems. Integrating research from cognitive science with AI,
we formalise an enactive model of learning and reasoning to address the problem
of subjectivity. This allows us to formulate a different proxy for
intelligence, called weakness, which addresses the problem of incomputability.
We prove optimal behaviour is attained when weakness is maximised. This proof
is supplemented by experimental results comparing weakness and description
length (the closest analogue to compression possible without reintroducing
subjectivity). Weakness outperforms description length, suggesting it is a
better proxy. Furthermore we show that, if cognition is enactive, then
minimisation of description length is neither necessary nor sufficient to
attain optimal performance, undermining the notion that compression is closely
related to intelligence. However, there remain open questions regarding the
implementation of scale-able AGI. In the short term, these results may be best
utilised to improve the performance of existing systems. For example, our
results explain why Deepmind's Apperception Engine is able to generalise
effectively, and how to replicate that performance by maximising weakness.",0,0,0,0,0,0,0.0824168,15.0,0.729153,80
5c347b9a-4a75-436f-b03c-a31fc575c2f1,Evaluating the Knowledge Dependency of Questions,3,0.0396961,0.0391349,"The automatic generation of Multiple Choice Questions (MCQ) has the potential
to reduce the time educators spend on student assessment significantly.
However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE,
and METEOR, focus on the n-gram based similarity of the generated MCQ to the
gold sample in the dataset and disregard their educational value. They fail to
evaluate the MCQ's ability to assess the student's knowledge of the
corresponding target fact. To tackle this issue, we propose a novel automatic
evaluation metric, coined Knowledge Dependent Answerability (KDA), which
measures the MCQ's answerability given knowledge of the target fact.
Specifically, we first show how to measure KDA based on student responses from
a human survey. Then, we propose two automatic evaluation metrics, KDA_disc and
KDA_cont, that approximate KDA by leveraging pre-trained language models to
imitate students' problem-solving behavior. Through our human studies, we show
that KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2)
usability in an actual classroom setting, labeled by experts. Furthermore, when
combined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown
to have a strong predictive power for various expert-labeled MCQ quality
measures.",0,0,1,0,0,0,0.460371,8.0,0.738434,39
7e7b4d4d-8a51-4620-b765-3257b4fad63f,USLN: A statistically guided lightweight network for underwater image enhancement via dual-statistic white balance and multi-color space stretch,6,0.212057,0.501694,"Underwater images are inevitably affected by color distortion and reduced
contrast. Traditional statistic-based methods such as white balance and
histogram stretching attempted to adjust the imbalance of color channels and
narrow distribution of intensities a priori thus with limited performance.
Recently, deep-learning-based methods have achieved encouraging results.
However, the involved complicate architecture and high computational costs may
hinder their deployment in practical constrained platforms. Inspired by above
works, we propose a statistically guided lightweight underwater image
enhancement network (USLN). Concretely, we first develop a dual-statistic white
balance module which can learn to use both average and maximum of images to
compensate the color distortion for each specific pixel. Then this is followed
by a multi-color space stretch module to adjust the histogram distribution in
RGB, HSI, and Lab color spaces adaptively. Extensive experiments show that,
with the guidance of statistics, USLN significantly reduces the required
network capacity (over98%) and achieves state-of-the-art performance. The code
and relevant resources are available at https://github.com/deepxzy/USLN.",1,1,0,0,1,0,0.971886,12.0,0.971962,39
01cb360b-00a0-483f-b46b-230a1fe746dc,An Attention-based Method for Action Unit Detection at the 3rd ABAW Competition,6,0.104404,0.636091,"Facial Action Coding System is an approach for modeling the complexity of
human emotional expression. Automatic action unit (AU) detection is a crucial
research area in human-computer interaction. This paper describes our
submission to the third Affective Behavior Analysis in-the-wild (ABAW)
competition 2022. We proposed a method for detecting facial action units in the
video. At the first stage, a lightweight CNN-based feature extractor is
employed to extract the feature map from each video frame. Then, an attention
module is applied to refine the attention map. The attention encoded vector is
derived using a weighted sum of the feature map and the attention scores later.
Finally, the sigmoid function is used at the output layer to make the
prediction suitable for multi-label AUs detection. We achieved a macro F1 score
of 0.48 on the ABAW challenge validation set compared to 0.39 from the baseline
model.",0,1,0,0,1,0,0.724253,5.0,0.728772,32
91e39678-e2d2-46cb-908d-ebc717b68b83,"Massively Multilingual ASR on 70 Languages: Tokenization, Architecture, and Generalization Capabilities",3,0.0847319,0.190166,"End-to-end multilingual ASR has become more appealing because of several
reasons such as simplifying the training and deployment process and positive
performance transfer from high-resource to low-resource languages. However,
scaling up the number of languages, total hours, and number of unique tokens is
not a trivial task. This paper explores large-scale multilingual ASR models on
70 languages. We inspect two architectures: (1) Shared embedding and output and
(2) Multiple embedding and output model. In the shared model experiments, we
show the importance of tokenization strategy across different languages. Later,
we use our optimal tokenization strategy to train multiple embedding and output
model to further improve our result. Our multilingual ASR achieves 13.9%-15.6%
average WER relative improvement compared to monolingual models. We show that
our multilingual ASR generalizes well on an unseen dataset and domain,
achieving 9.5% and 7.5% WER on Multilingual Librispeech (MLS) with zero-shot
and finetuning, respectively.",0,1,0,0,1,0,0.520272,10.0,0.808205,36
4182f15a-465b-41d2-8ae8-e34a7f009c1c,Synthetic Question Value Estimation for Domain Adaptation of Question Answering,12,0.0384946,0.424084,"Synthesizing QA pairs with a question generator (QG) on the target domain has
become a popular approach for domain adaptation of question answering (QA)
models. Since synthetic questions are often noisy in practice, existing work
adapts scores from a pretrained QA (or QG) model as criteria to select
high-quality questions. However, these scores do not directly serve the
ultimate goal of improving QA performance on the target domain. In this paper,
we introduce a novel idea of training a question value estimator (QVE) that
directly estimates the usefulness of synthetic questions for improving the
target-domain QA performance. By conducting comprehensive experiments, we show
that the synthetic questions selected by QVE can help achieve better
target-domain QA performance, in comparison with existing techniques. We
additionally show that by using such questions and only around 15% of the human
annotations on the target domain, we can achieve comparable performance to the
fully-supervised baselines.",1,1,0,0,0,0,0.184542,6.0,0.466826,47
2d2b45cd-bc65-41b6-862e-8598036ccf7e,Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for the Diagnosis of Eosinophilic Esophagitis,11,0.201863,0.271492,"Eosinophilic esophagitis (EoE) is a chronic allergic inflammatory condition
of the esophagus associated with elevated esophageal eosinophils. Second only
to gastroesophageal reflux disease, EoE is one of the leading causes of chronic
refractory dysphagia in adults and children. EoE diagnosis requires enumerating
the density of esophageal eosinophils in esophageal biopsies, a somewhat
subjective task that is time-consuming, thus reducing the ability to process
the complex tissue structure. Previous artificial intelligence (AI) approaches
that aimed to improve histology-based diagnosis focused on recapitulating
identification and quantification of the area of maximal eosinophil density.
However, this metric does not account for the distribution of eosinophils or
other histological features, over the whole slide image. Here, we developed an
artificial intelligence platform that infers local and spatial biomarkers based
on semantic segmentation of intact eosinophils and basal zone distributions.
Besides the maximal density of eosinophils (referred to as Peak Eosinophil
Count [PEC]) and a maximal basal zone fraction, we identify two additional
metrics that reflect the distribution of eosinophils and basal zone fractions.
This approach enables a decision support system that predicts EoE activity and
classifies the histological severity of EoE patients. We utilized a cohort that
includes 1066 biopsy slides from 400 subjects to validate the system's
performance and achieved a histological severity classification accuracy of
86.70%, sensitivity of 84.50%, and specificity of 90.09%. Our approach
highlights the importance of systematically analyzing the distribution of
biopsy features over the entire slide and paves the way towards a personalized
decision support system that will assist not only in counting cells but can
also potentially improve diagnosis and provide treatment prediction.",0,1,0,0,0,0,0.480943,8.0,0.746072,31
cc1739ae-b485-415d-b9c8-b44b0dccd46b,PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection,12,0.060941,0.796125,"Recent years have witnessed a trend of applying context frames to boost the
performance of object detection as video object detection. Existing methods
usually aggregate features at one stroke to enhance the feature. These methods,
however, usually lack spatial information from neighboring frames and suffer
from insufficient feature aggregation. To address the issues, we perform a
progressive way to introduce both temporal information and spatial information
for an integrated enhancement. The temporal information is introduced by the
temporal feature aggregation model (TFAM), by conducting an attention mechanism
between the context frames and the target frame (i.e., the frame to be
detected). Meanwhile, we employ a Spatial Transition Awareness Model (STAM) to
convey the location transition information between each context frame and
target frame. Built upon a transformer-based detector DETR, our PTSEFormer also
follows an end-to-end fashion to avoid heavy post-processing procedures while
achieving 88.1% mAP on the ImageNet VID dataset. Codes are available at
https://github.com/Hon-Wong/PTSEFormer.",1,1,0,0,1,0,0.351113,9.0,0.728042,35
cc272ca0-c47a-4e6f-a926-91d72dc78fb1,LingYi: Medical Conversational Question Answering System based on Multi-modal Knowledge Graphs,10,0.203812,0.741724,"The medical conversational system can relieve the burden of doctors and
improve the efficiency of healthcare, especially during the pandemic. This
paper presents a medical conversational question answering (CQA) system based
on the multi-modal knowledge graph, namely ""LingYi"", which is designed as a
pipeline framework to maintain high flexibility. Our system utilizes automated
medical procedures including medical triage, consultation, image-text drug
recommendation and record. To conduct knowledge-grounded dialogues with
patients, we first construct a Chinese Medical Multi-Modal Knowledge Graph
(CM3KG) and collect a large-scale Chinese Medical CQA (CMCQA) dataset. Compared
with the other existing medical question-answering systems, our system adopts
several state-of-the-art technologies including medical entity disambiguation
and medical dialogue generation, which is more friendly to provide medical
services to patients. In addition, we have open-sourced our codes which contain
back-end models and front-end web pages at https://github.com/WENGSYX/LingYi.
The datasets including CM3KG at https://github.com/WENGSYX/CM3KG and CMCQA at
https://github.com/WENGSYX/CMCQA are also released to further promote future
research.",1,1,0,1,0,0,0.774101,4.0,0.696943,41
bdcf963e-3f76-401f-9e47-d74b3dc3efce,Hyperbolic Vision Transformers: Combining Improvements in Metric Learning,55,0.130777,0.819661,"Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric.",1,0,0,0,0,0,0.274892,9.0,0.695067,69
7da45cc3-5885-4eb6-adf2-afd7fab74cfe,Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication,9,0.196835,0.444694,"Social media has become extremely influential when it comes to policy making
in modern societies, especially in the western world, where platforms such as
Twitter allow users to follow politicians, thus making citizens more involved
in political discussion. In the same vein, politicians use Twitter to express
their opinions, debate among others on current topics and promote their
political agendas aiming to influence voter behaviour. In this paper, we
attempt to analyse tweets of politicians from three European countries and
explore the virality of their tweets. Previous studies have shown that tweets
conveying negative sentiment are likely to be retweeted more frequently. By
utilising state-of-the-art pre-trained language models, we performed sentiment
analysis on hundreds of thousands of tweets collected from members of
parliament in Greece, Spain and the United Kingdom, including devolved
administrations. We achieved this by systematically exploring and analysing the
differences between influential and less popular tweets. Our analysis indicates
that politicians' negatively charged tweets spread more widely, especially in
more recent times, and highlights interesting differences between political
parties as well as between politicians and the general population.",0,1,0,1,0,0,0.302313,9.0,0.707652,87
55bc8842-ce56-49e7-ab8a-3ff3908eff47,Fairly Accurate: Learning Optimal Accuracy vs. Fairness Tradeoffs for Hate Speech Detection,2,0.0459656,0.0911082,"Recent work has emphasized the importance of balancing competing objectives
in model training (e.g., accuracy vs. fairness, or competing measures of
fairness). Such trade-offs reflect a broader class of multi-objective
optimization (MOO) problems in which optimization methods seek Pareto optimal
trade-offs between competing goals. In this work, we first introduce a
differentiable measure that enables direct optimization of group fairness
(specifically, balancing accuracy across groups) in model training. Next, we
demonstrate two model-agnostic MOO frameworks for learning Pareto optimal
parameterizations over different groups of neural classification models. We
evaluate our methods on the specific task of hate speech detection, in which
prior work has shown lack of group fairness across speakers of different
English dialects. Empirical results across convolutional, sequential, and
transformer-based neural architectures show superior empirical accuracy vs.
fairness trade-offs over prior work. More significantly, our measure enables
the Pareto machinery to ensure that each architecture achieves the best
possible trade-off between fairness and accuracy w.r.t. the dataset, given
user-prescribed error tolerance bounds.",0,0,1,0,0,0,0.873684,6.0,0.852935,39
d20a1f56-d851-4377-9fc3-86fcb9ef7210,Re-Examining Calibration: The Case of Question Answering,19,0.0,0.701852,"For users to trust model predictions, they need to understand model outputs,
particularly their confidence - calibration aims to adjust (calibrate) models'
confidence to match expected accuracy. We argue that the traditional
calibration evaluation does not promote effective calibrations: for example, it
can encourage always assigning a mediocre confidence score to all predictions,
which does not help users distinguish correct predictions from wrong ones.
Building on those observations, we propose a new calibration metric, MacroCE,
that better captures whether the model assigns low confidence to wrong
predictions and high confidence to correct predictions. Focusing on the
practical application of open-domain question answering, we examine
conventional calibration methods applied on the widely-used retriever-reader
pipeline, all of which do not bring significant gains under our new MacroCE
metric. Toward better calibration, we propose a new calibration method
(ConsCal) that uses not just final model predictions but whether multiple model
checkpoints make consistent predictions. Altogether, we provide an alternative
view of calibration along with a new metric, re-evaluation of existing
calibration methods on our metric, and proposal of a more effective calibration
method.",1,1,0,0,0,0,0.504779,6.0,0.672968,33
8a1a53da-9c6f-40ff-90a3-84bc16f7d458,Heterogeneous-Agent Mirror Learning: A Continuum of Solutions to Cooperative MARL,10,0.243488,0.674645,"The necessity for cooperation among intelligent machines has popularised
cooperative multi-agent reinforcement learning (MARL) in the artificial
intelligence (AI) research community. However, many research endeavors have
been focused on developing practical MARL algorithms whose effectiveness has
been studied only empirically, thereby lacking theoretical guarantees. As
recent studies have revealed, MARL methods often achieve performance that is
unstable in terms of reward monotonicity or suboptimal at convergence. To
resolve these issues, in this paper, we introduce a novel framework named
Heterogeneous-Agent Mirror Learning (HAML) that provides a general template for
MARL algorithmic designs. We prove that algorithms derived from the HAML
template satisfy the desired properties of the monotonic improvement of the
joint reward and the convergence to Nash equilibrium. We verify the
practicality of HAML by proving that the current state-of-the-art cooperative
MARL algorithms, HATRPO and HAPPO, are in fact HAML instances. Next, as a
natural outcome of our theory, we propose HAML extensions of two well-known RL
algorithms, HAA2C (for A2C) and HADDPG (for DDPG), and demonstrate their
effectiveness against strong baselines on StarCraftII and Multi-Agent MuJoCo
tasks.",1,0,0,0,0,0,0.927095,7.0,0.9076,34
85124db8-ee63-46f0-9772-ea08e2b2ef56,Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks,23,0.251162,0.462945,"In the pursuit of explaining implicit regularization in deep learning,
prominent focus was given to matrix and tensor factorizations, which correspond
to simplified neural networks. It was shown that these models exhibit an
implicit tendency towards low matrix and tensor ranks, respectively. Drawing
closer to practical deep learning, the current paper theoretically analyzes the
implicit regularization in hierarchical tensor factorization, a model
equivalent to certain deep convolutional neural networks. Through a dynamical
systems lens, we overcome challenges associated with hierarchy, and establish
implicit regularization towards low hierarchical tensor rank. This translates
to an implicit regularization towards locality for the associated convolutional
networks. Inspired by our theory, we design explicit regularization
discouraging locality, and demonstrate its ability to improve the performance
of modern convolutional networks on non-local tasks, in defiance of
conventional wisdom by which architectural changes are needed. Our work
highlights the potential of enhancing neural networks via theoretical analysis
of their implicit regularization.",1,0,0,0,0,0,0.30486,7.0,0.625568,106
444f38dd-1957-41ed-a35f-5b2ead66c534,When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture,25,0.0559657,0.809908,"Vision Transformers (ViTs) have recently achieved competitive performance in
broad vision tasks. Unfortunately, on popular threat models, naturally trained
ViTs are shown to provide no more adversarial robustness than convolutional
neural networks (CNNs). Adversarial training is still required for ViTs to
defend against such adversarial attacks. In this paper, we provide the first
and comprehensive study on the adversarial training recipe of ViTs via
extensive evaluation of various training techniques across benchmark datasets.
We find that pre-training and SGD optimizer are necessary for ViTs' adversarial
training. Further considering ViT as a new type of model architecture, we
investigate its adversarial robustness from the perspective of its unique
architectural components. We find, when randomly masking gradients from some
attention blocks or masking perturbations on some patches during adversarial
training, the adversarial robustness of ViTs can be remarkably improved, which
may potentially open up a line of work to explore the architectural information
inside the newly designed models like ViTs. Our code is available at
https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers.",1,1,0,0,0,0,0.695879,6.0,0.760799,54
e43bc829-0b81-487c-8728-4ac12d94f61a,DEER: Descriptive Knowledge Graph for Explaining Entity Relationships,8,0.17994,0.628506,"We propose DEER (Descriptive Knowledge Graph for Explaining Entity
Relationships) - an open and informative form of modeling entity relationships.
In DEER, relationships between entities are represented by free-text relation
descriptions. For instance, the relationship between entities of machine
learning and algorithm can be represented as ``Machine learning explores the
study and construction of algorithms that can learn from and make predictions
on data.'' To construct DEER, we propose a self-supervised learning method to
extract relation descriptions with the analysis of dependency patterns and
generate relation descriptions with a transformer-based relation description
synthesizing model, where no human labeling is required. Experiments
demonstrate that our system can extract and generate high-quality relation
descriptions for explaining entity relationships. The results suggest that we
can build an open and informative knowledge graph without human annotation.",0,0,0,0,0,1,0.580638,9.0,0.805575,34
0a9e7569-1d8c-4687-88d4-7c76828232d9,Will Large-scale Generative Models Corrupt Future Datasets?,19,0.324605,0.298103,"Recently proposed large-scale text-to-image generative models such as
DALL$\cdot$E 2, Midjourney, and StableDiffusion can generate high-quality and
realistic images from users' prompts. Not limited to the research community,
ordinary Internet users enjoy these generative models, and consequently, a
tremendous amount of generated images have been shared on the Internet.
Meanwhile, today's success of deep learning in the computer vision field owes a
lot to images collected from the Internet. These trends lead us to a research
question: ""\textbf{will such generated images impact the quality of future
datasets and the performance of computer vision models positively or
negatively?}"" This paper empirically answers this question by simulating
contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using
a state-of-the-art generative model and evaluate models trained with
""contaminated"" datasets on various tasks, including image classification and
image generation. Throughout experiments, we conclude that generated images
negatively affect downstream performance, while the significance depends on
tasks and the amount of generated images. The generated datasets and the codes
for experiments will be publicly released for future research. Generated
datasets and source codes are available from
\url{https://github.com/moskomule/dataset-contamination}.",1,1,0,1,0,0,0.968823,6.0,0.939028,70
75e36924-b686-4e38-a68a-5c1ef3256ea9,Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric Approach for Explainability Requirements,10,0.0646886,0.508468,"With the recent proliferation of artificial intelligence systems, there has
been a surge in the demand for explainability of these systems. Explanations
help to reduce system opacity, support transparency, and increase stakeholder
trust. In this position paper, we discuss synergies between requirements
engineering (RE) and Explainable AI (XAI). We highlight challenges in the field
of XAI, and propose a framework and research directions on how RE practices can
help to mitigate these challenges.",0,0,0,0,0,0,0.395732,7.0,0.672123,36
0beab9c1-7bad-45d4-a488-087a43de0f02,STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding,3,0.0221774,0.225042,"In this technical report, we introduce our solution to human-centric
spatio-temporal video grounding task. We propose a concise and effective
framework named STVGFormer, which models spatiotemporal visual-linguistic
dependencies with a static branch and a dynamic branch. The static branch
performs cross-modal understanding in a single frame and learns to localize the
target object spatially according to intra-frame visual cues like object
appearances. The dynamic branch performs cross-modal understanding across
multiple frames. It learns to predict the starting and ending time of the
target moment according to dynamic visual cues like motions. Both the static
and dynamic branches are designed as cross-modal transformers. We further
design a novel static-dynamic interaction block to enable the static and
dynamic branches to transfer useful and complementary information from each
other, which is shown to be effective to improve the prediction on hard cases.
Our proposed method achieved 39.6% vIoU and won the first place in the HC-STVG
track of the 4th Person in Context Challenge.",0,0,0,0,1,0,0.717066,4.0,0.655922,16
481537be-afff-446f-922b-690bf36849b3,PP-YOLOE: An evolved version of YOLO,130,0.130264,0.87966,"In this report, we present PP-YOLOE, an industrial state-of-the-art object
detector with high performance and friendly deployment. We optimize on the
basis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful
backbone and neck equipped with CSPRepResStage, ET-head and dynamic label
assignment algorithm TAL. We provide s/m/l/x models for different practice
scenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1
FPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35% speed
up) and (+1.3 AP, +24.96% speed up), compared to the previous state-of-the-art
industrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference
speed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct
extensive experiments to verify the effectiveness of our designs. Source code
and pre-trained models are available at
https://github.com/PaddlePaddle/PaddleDetection.",1,1,0,0,1,0,0.485801,5.0,0.596563,35
458d909b-e591-4dbd-88ff-ea867c120495,Transformer-based SAR Image Despeckling,21,0.469536,0.922288,"Synthetic Aperture Radar (SAR) images are usually degraded by a
multiplicative noise known as speckle which makes processing and interpretation
of SAR images difficult. In this paper, we introduce a transformer-based
network for SAR image despeckling. The proposed despeckling network comprises
of a transformer-based encoder which allows the network to learn global
dependencies between different image regions - aiding in better despeckling.
The network is trained end-to-end with synthetically generated speckled images
using a composite loss function. Experiments show that the proposed method
achieves significant improvements over traditional and convolutional neural
network-based despeckling methods on both synthetic and real SAR images.",1,1,0,0,1,0,0.897779,10.0,0.921501,15
d7666ed0-11f1-41bc-b911-717d4a678fa7,Hybrid Multimodal Fusion for Humor Detection,11,0.111402,0.675339,"In this paper, we present our solution to the MuSe-Humor sub-challenge of the
Multimodal Emotional Challenge (MuSe) 2022. The goal of the MuSe-Humor
sub-challenge is to detect humor and calculate AUC from audiovisual recordings
of German football Bundesliga press conferences. It is annotated for humor
displayed by the coaches. For this sub-challenge, we first build a discriminant
model using the transformer module and BiLSTM module, and then propose a hybrid
fusion strategy to use the prediction results of each modality to improve the
performance of the model. Our experiments demonstrate the effectiveness of our
proposed model and hybrid fusion strategy on multimodal fusion, and the AUC of
our proposed model on the test set is 0.8972.",0,1,0,0,0,0,0.246832,10.0,0.71299,46
6b2a6f2e-f6bf-461a-81c4-64026684cfcc,Finite Entailment of UCRPQs over ALC Ontologies,2,0.0643209,0.110555,"We investigate the problem of finite entailment of ontology-mediated queries.
We consider the expressive query language, unions of conjunctive regular path
queries (UCRPQs), extending the well-known class of union of conjunctive
queries, with regular expressions over roles. We look at ontologies formulated
using the description logic ALC, and show a tight 2EXPTIME upper bound for
entailment of UCRPQs. At the core of our decision procedure, there is a novel
automata-based technique introducing a stratification of interpretations
induced by the deterministic finite automaton underlying the input UCRPQ",0,0,0,0,0,0,0.0117753,14.0,0.568195,51
940df1d4-f479-473d-8937-304b62243e7f,Hardware-aware mobile building block evaluation for computer vision,1,0.00779097,0.0646389,"In this work we propose a methodology to accurately evaluate and compare the
performance of efficient neural network building blocks for computer vision in
a hardware-aware manner. Our comparison uses pareto fronts based on randomly
sampled networks from a design space to capture the underlying
accuracy/complexity trade-offs. We show that our approach allows to match the
information obtained by previous comparison paradigms, but provides more
insights in the relationship between hardware cost and accuracy. We use our
methodology to analyze different building blocks and evaluate their performance
on a range of embedded hardware platforms. This highlights the importance of
benchmarking building blocks as a preselection step in the design process of a
neural network. We show that choosing the right building block can speed up
inference by up to a factor of 2x on specific hardware ML accelerators.",0,1,0,0,0,0,0.750805,8.0,0.83994,31
2d29d91f-6299-4c06-8c83-6e15a700030f,Pyramid Frequency Network with Spatial Attention Residual Refinement Module for Monocular Depth Estimation,8,0.122111,0.449916,"Deep-learning-based approaches to depth estimation are rapidly advancing,
offering superior performance over existing methods. To estimate the depth in
real-world scenarios, depth estimation models require the robustness of various
noise environments. In this work, a Pyramid Frequency Network(PFN) with Spatial
Attention Residual Refinement Module(SARRM) is proposed to deal with the weak
robustness of existing deep-learning methods. To reconstruct depth maps with
accurate details, the SARRM constructs a residual fusion method with an
attention mechanism to refine the blur depth. The frequency division strategy
is designed, and the frequency pyramid network is developed to extract features
from multiple frequency bands. With the frequency strategy, PFN achieves better
visual accuracy than state-of-the-art methods in both indoor and outdoor scenes
on Make3D, KITTI depth, and NYUv2 datasets. Additional experiments on the noisy
NYUv2 dataset demonstrate that PFN is more reliable than existing deep-learning
methods in high-noise scenes.",0,1,0,0,1,0,0.87488,10.0,0.91222,56
2afdd477-61e4-42c6-a212-32e072c85331,An Online Semantic Mapping System for Extending and Enhancing Visual SLAM,14,0.0973208,0.449646,"We present a real-time semantic mapping approach for mobile vision systems
with a 2D to 3D object detection pipeline and rapid data association for
generated landmarks. Besides the semantic map enrichment the associated
detections are further introduced as semantic constraints into a simultaneous
localization and mapping (SLAM) system for pose correction purposes. This way,
we are able generate additional meaningful information that allows to achieve
higher-level tasks, while simultaneously leveraging the view-invariance of
object detections to improve the accuracy and the robustness of the odometry
estimation. We propose tracklets of locally associated object observations to
handle ambiguous and false predictions and an uncertainty-based greedy
association scheme for an accelerated processing time. Our system reaches
real-time capabilities with an average iteration duration of 65~ms and is able
to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a
public dataset. Additionally, we implemented our approach as a modular ROS
package that makes it straightforward for integration in arbitrary graph-based
SLAM methods.",0,1,0,0,1,0,0.171378,10.0,0.67192,44
9f7f6770-13dd-4c44-bb88-fdf7ad25cd1c,Machine Learning Methods in Solving the Boolean Satisfiability Problem,12,0.194373,0.849792,"This paper reviews the recent literature on solving the Boolean
satisfiability problem (SAT), an archetypal NP-complete problem, with the help
of machine learning techniques. Despite the great success of modern SAT solvers
to solve large industrial instances, the design of handcrafted heuristics is
time-consuming and empirical. Under the circumstances, the flexible and
expressive machine learning methods provide a proper alternative to solve this
long-standing problem. We examine the evolving ML-SAT solvers from naive
classifiers with handcrafted features to the emerging end-to-end SAT solvers
such as NeuroSAT, as well as recent progress on combinations of existing CDCL
and local search solvers with machine learning methods. Overall, solving SAT
with machine learning is a promising yet challenging research topic. We
conclude the limitations of current works and suggest possible future
directions.",0,0,0,0,0,0,0.0686569,10.0,0.574728,114
4bf1033a-2116-4e14-8728-49d90a26e9d1,MAGVIT: Masked Generative Video Transformer,72,0.598397,0.903484,"We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle
various video synthesis tasks with a single model. We introduce a 3D tokenizer
to quantize a video into spatial-temporal visual tokens and propose an
embedding method for masked video token modeling to facilitate multi-task
learning. We conduct extensive experiments to demonstrate the quality,
efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT
performs favorably against state-of-the-art approaches and establishes the
best-published FVD on three video generation benchmarks, including the
challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference
time by two orders of magnitude against diffusion models and by 60x against
autoregressive models. (iii) A single MAGVIT model supports ten diverse
generation tasks and generalizes across videos from different visual domains.
The source code and trained models will be released to the public at
https://magvit.cs.cmu.edu.",1,0,0,0,0,0,0.899985,5.0,0.844906,86
dcb0b077-996a-481a-a037-f1905876ece7,Finding Diverse and Predictable Subgraphs for Graph Domain Generalization,6,0.0857231,0.244766,"This paper focuses on out-of-distribution generalization on graphs where
performance drops due to the unseen distribution shift. Previous graph domain
generalization works always resort to learning an invariant predictor among
different source domains. However, they assume sufficient source domains are
available during training, posing huge challenges for realistic applications.
By contrast, we propose a new graph domain generalization framework, dubbed as
DPS, by constructing multiple populations from the source domains.
Specifically, DPS aims to discover multiple \textbf{D}iverse and
\textbf{P}redictable \textbf{S}ubgraphs with a set of generators, namely,
subgraphs are different from each other but all the them share the same
semantics with the input graph. These generated source domains are exploited to
learn an \textit{equi-predictive} graph neural network (GNN) across domains,
which is expected to generalize well to unseen target domains. Generally, DPS
is model-agnostic that can be incorporated with various GNN backbones.
Extensive experiments on both node-level and graph-level benchmarks shows that
the proposed DPS achieves impressive performance for various graph domain
generalization tasks.",0,0,0,0,0,0,0.907772,6.0,0.876521,70
2ee0c17c-6565-4e34-ae78-776839364468,Event-based Monocular Dense Depth Estimation with Recurrent Transformers,8,0.115345,0.701452,"Event cameras, offering high temporal resolutions and high dynamic ranges,
have brought a new perspective to address common challenges (e.g., motion blur
and low light) in monocular depth estimation. However, how to effectively
exploit the sparse spatial information and rich temporal cues from asynchronous
events remains a challenging endeavor. To this end, we propose a novel
event-based monocular depth estimator with recurrent transformers, namely
EReFormer, which is the first pure transformer with a recursive mechanism to
process continuous event streams. Technically, for spatial modeling, a novel
transformer-based encoder-decoder with a spatial transformer fusion module is
presented, having better global context information modeling capabilities than
CNN-based methods. For temporal modeling, we design a gate recurrent vision
transformer unit that introduces a recursive mechanism into transformers,
improving temporal modeling capabilities while alleviating the expensive GPU
memory cost. The experimental results show that our EReFormer outperforms
state-of-the-art methods by a margin on both synthetic and real-world datasets.
We hope that our work will attract further research to develop stunning
transformers in the event-based vision community. Our open-source code can be
found in the supplemental material.",0,0,0,0,1,0,0.50009,5.0,0.604862,51
95285aa7-2607-487c-a8ec-92da69dd32fe,CAISE: Conversational Agent for Image Search and Editing,5,0.0704112,0.266723,"Demand for image editing has been increasing as users' desire for expression
is also increasing. However, for most users, image editing tools are not easy
to use since the tools require certain expertise in photo effects and have
complex interfaces. Hence, users might need someone to help edit their images,
but having a personal dedicated human assistant for every user is impossible to
scale. For that reason, an automated assistant system for image editing is
desirable. Additionally, users want more image sources for diverse image
editing works, and integrating an image search functionality into the editing
tool is a potential remedy for this demand. Thus, we propose a dataset of an
automated Conversational Agent for Image Search and Editing (CAISE). To our
knowledge, this is the first dataset that provides conversational image search
and editing annotations, where the agent holds a grounded conversation with
users and helps them to search and edit images according to their requests. To
build such a system, we first collect image search and editing conversations
between pairs of annotators. The assistant-annotators are equipped with a
customized image search and editing tool to address the requests from the
user-annotators. The functions that the assistant-annotators conduct with the
tool are recorded as executable commands, allowing the trained system to be
useful for real-world application execution. We also introduce a
generator-extractor baseline model for this task, which can adaptively select
the source of the next token (i.e., from the vocabulary or from textual/visual
contexts) for the executable command. This serves as a strong starting point
while still leaving a large human-machine performance gap for useful future
work. Our code and dataset are publicly available at:
https://github.com/hyounghk/CAISE",1,1,1,1,0,0,0.426746,9.0,0.756038,43
55dac7b5-ada0-4ac6-b170-dee84b1456e2,HCL-TAT: A Hybrid Contrastive Learning Method for Few-shot Event Detection with Task-Adaptive Threshold,8,0.158503,0.521845,"Conventional event detection models under supervised learning settings suffer
from the inability of transfer to newly-emerged event types owing to lack of
sufficient annotations. A commonly-adapted solution is to follow a
identify-then-classify manner, which first identifies the triggers and then
converts the classification task via a few-shot learning paradigm. However,
these methods still fall far short of expectations due to: (i) insufficient
learning of discriminative representations in low-resource scenarios, and (ii)
trigger misidentification caused by the overlap of the learned representations
of triggers and non-triggers. To address the problems, in this paper, we
propose a novel Hybrid Contrastive Learning method with a Task-Adaptive
Threshold (abbreviated as HCLTAT), which enables discriminative representation
learning with a two-view contrastive loss (support-support and
prototype-query), and devises a easily-adapted threshold to alleviate
misidentification of triggers. Extensive experiments on the benchmark dataset
FewEvent demonstrate the superiority of our method to achieve better results
compared to the state-of-the-arts. All the code and data of this paper will be
available for online public access.",1,0,0,0,1,0,0.780829,6.0,0.801315,36
238a3e35-7530-45ca-b858-f3b5f8e0939f,ADDMU: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation,3,0.0380659,0.249179,"Adversarial Examples Detection (AED) is a crucial defense technique against
adversarial attacks and has drawn increasing attention from the Natural
Language Processing (NLP) community. Despite the surge of new AED methods, our
studies show that existing methods heavily rely on a shortcut to achieve good
performance. In other words, current search-based adversarial attacks in NLP
stop once model predictions change, and thus most adversarial examples
generated by those attacks are located near model decision boundaries. To
surpass this shortcut and fairly evaluate AED methods, we propose to test AED
methods with \textbf{F}ar \textbf{B}oundary (\textbf{FB}) adversarial examples.
Existing methods show worse than random guess performance under this scenario.
To overcome this limitation, we propose a new technique, \textbf{ADDMU},
\textbf{a}dversary \textbf{d}etection with \textbf{d}ata and \textbf{m}odel
\textbf{u}ncertainty, which combines two types of uncertainty estimation for
both regular and FB adversarial example detection. Our new method outperforms
previous methods by 3.6 and 6.0 \emph{AUC} points under each scenario. Finally,
our analysis shows that the two types of uncertainty provided by \textbf{ADDMU}
can be leveraged to characterize adversarial examples and identify the ones
that contribute most to model's robustness in adversarial training.",1,1,0,0,1,0,0.625449,7.0,0.767489,55
385fb687-7c7e-426b-97b2-2f3e7c707e59,TALM: Tool Augmented Language Models,93,0.905499,0.99359,"Transformer based language models (LMs) demonstrate increasing performance
with scale across a wide variety of tasks. Scale alone however cannot enable
models to solve tasks that require access to ephemeral, changing, or private
data that was unavailable at training time. Many useful tasks may also benefit
from LMs being able to access APIs that read or modify state. In this work, we
present Tool Augmented Language Models (TALM), combining a text-only approach
to augment language models with non-differentiable tools, and an iterative
""self-play"" technique to bootstrap performance starting from few tool
demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA
task and a reasoning oriented math task with simple tools. At a given model
scale, TALM significantly outperforms non-augmented LMs. We further demonstrate
that TALM successfully performs out-of-distribution inferences on both QA and
math tasks, where non-augmented LMs fail. Our results suggest that Tool
Augmented Language Models are a promising direction to enrich LMs'
capabilities, with less dependence on scale.",0,1,0,0,0,0,0.989954,5.0,0.98335,21
2334ecf9-078a-4b1d-8424-2ab241af1075,Semiconductor Defect Detection by Hybrid Classical-Quantum Deep Learning,25,0.596777,0.934265,"With the rapid development of artificial intelligence and autonomous driving
technology, the demand for semiconductors is projected to rise substantially.
However, the massive expansion of semiconductor manufacturing and the
development of new technology will bring many defect wafers. If these defect
wafers have not been correctly inspected, the ineffective semiconductor
processing on these defect wafers will cause additional impact to our
environment, such as excessive carbon dioxide emission and energy consumption.
In this paper, we utilize the information processing advantages of quantum
computing to promote the defect learning defect review (DLDR). We propose a
classical-quantum hybrid algorithm for deep learning on near-term quantum
processors. By tuning parameters implemented on it, quantum circuit driven by
our framework learns a given DLDR task, include of wafer defect map
classification, defect pattern classification, and hotspot detection. In
addition, we explore parametrized quantum circuits with different
expressibility and entangling capacities. These results can be used to build a
future roadmap to develop circuit-based quantum deep learning for semiconductor
defect detection.",0,0,0,0,0,0,0.426664,6.0,0.634014,44
661853c9-c5b2-43dc-9fc2-a9bd04e6ab20,IGLU 2022: Interactive Grounded Language Understanding in a Collaborative Environment at NeurIPS 2022,15,0.198542,0.697186,"Human intelligence has the remarkable ability to adapt to new tasks and
environments quickly. Starting from a very young age, humans acquire new skills
and learn how to solve new tasks either by imitating the behavior of others or
by following provided natural language instructions. To facilitate research in
this direction, we propose IGLU: Interactive Grounded Language Understanding in
a Collaborative Environment. The primary goal of the competition is to approach
the problem of how to develop interactive embodied agents that learn to solve a
task while provided with grounded natural language instructions in a
collaborative environment. Understanding the complexity of the challenge, we
split it into sub-tasks to make it feasible for participants.
  This research challenge is naturally related, but not limited, to two fields
of study that are highly relevant to the NeurIPS community: Natural Language
Understanding and Generation (NLU/G) and Reinforcement Learning (RL).
Therefore, the suggested challenge can bring two communities together to
approach one of the crucial challenges in AI. Another critical aspect of the
challenge is the dedication to perform a human-in-the-loop evaluation as a
final evaluation for the agents developed by contestants.",0,0,1,0,0,0,0.487501,7.0,0.71254,59
e99a281f-a762-4ec2-a3d5-b3c13d810ac6,Towards Automated Polyp Segmentation Using Weakly- and Semi-Supervised Learning and Deformable Transformers,3,0.111489,0.091589,"Polyp segmentation is a crucial step towards computer-aided diagnosis of
colorectal cancer. However, most of the polyp segmentation methods require
pixel-wise annotated datasets. Annotated datasets are tedious and
time-consuming to produce, especially for physicians who must dedicate their
time to their patients. We tackle this issue by proposing a novel framework
that can be trained using only weakly annotated images along with exploiting
unlabeled images. To this end, we propose three ideas to address this problem,
more specifically our contributions are: 1) a novel sparse foreground loss that
suppresses false positives and improves weakly-supervised training, 2) a
batch-wise weighted consistency loss utilizing predicted segmentation maps from
identical networks trained using different initialization during
semi-supervised training, 3) a deformable transformer encoder neck for feature
enhancement by fusing information across levels and flexible spatial locations.
  Extensive experimental results demonstrate the merits of our ideas on five
challenging datasets outperforming some state-of-the-art fully supervised
models. Also, our framework can be utilized to fine-tune models trained on
natural image segmentation datasets drastically improving their performance for
polyp segmentation and impressively demonstrating superior performance to fully
supervised fine-tuning.",0,1,1,1,1,0,0.924578,8.0,0.917519,48
889286c8-9a04-42a4-ba02-bff11bae6b2c,OpenTAL: Towards Open Set Temporal Action Localization,17,0.486503,0.842421,"Temporal Action Localization (TAL) has experienced remarkable success under
the supervised learning paradigm. However, existing TAL methods are rooted in
the closed set assumption, which cannot handle the inevitable unknown actions
in open-world scenarios. In this paper, we, for the first time, step toward the
Open Set TAL (OSTAL) problem and propose a general framework OpenTAL based on
Evidential Deep Learning (EDL). Specifically, the OpenTAL consists of
uncertainty-aware action classification, actionness prediction, and temporal
location regression. With the proposed importance-balanced EDL method,
classification uncertainty is learned by collecting categorical evidence
majorly from important samples. To distinguish the unknown actions from
background video frames, the actionness is learned by the positive-unlabeled
learning. The classification uncertainty is further calibrated by leveraging
the guidance from the temporal localization quality. The OpenTAL is general to
enable existing TAL models for open set scenarios, and experimental results on
THUMOS14 and ActivityNet1.3 benchmarks show the effectiveness of our method.
The code and pre-trained models are released at
https://www.rit.edu/actionlab/opental.",1,0,1,0,0,0,0.954064,6.0,0.919276,75
5db04216-a1c2-497c-9c69-6cc25f7c3be1,Grafting Pre-trained Models for Multimodal Headline Generation,3,0.0460704,0.155471,"Multimodal headline utilizes both video frames and transcripts to generate
the natural language title of the videos. Due to a lack of large-scale,
manually annotated data, the task of annotating grounded headlines for video is
labor intensive and impractical. Previous researches on pre-trained language
models and video-language models have achieved significant progress in related
downstream tasks. However, none of them can be directly applied to multimodal
headline architecture where we need both multimodal encoder and sentence
decoder. A major challenge in simply gluing language model and video-language
model is the modality balance, which is aimed at combining visual-language
complementary abilities. In this paper, we propose a novel approach to graft
the video encoder from the pre-trained video-language model on the generative
pre-trained language model. We also present a consensus fusion mechanism for
the integration of different components, via inter/intra modality relation.
Empirically, experiments show that the grafted model achieves strong results on
a brand-new dataset collected from real-world applications.",0,1,0,1,0,0,0.784597,5.0,0.76385,43
10c0f8d8-9879-4ae6-9af9-d75215dcd57e,DeepStruct: Pretraining of Language Models for Structure Prediction,42,0.57847,0.471784,"We introduce a method for improving the structural understanding abilities of
language models. Unlike previous approaches that finetune the models with
task-specific augmentation, we pretrain language models on a collection of
task-agnostic corpora to generate structures from text. Our structure
pretraining enables zero-shot transfer of the learned knowledge that models
have about the structure tasks. We study the performance of this approach on 28
datasets, spanning 10 structure prediction tasks including open information
extraction, joint entity and relation extraction, named entity recognition,
relation classification, semantic role labeling, event extraction, coreference
resolution, factual probe, intent detection, and dialogue state tracking. We
further enhance the pretraining with the task-specific training sets. We show
that a 10B parameter language model transfers non-trivially to most tasks and
obtains state-of-the-art performance on 21 of 28 datasets that we evaluate.",1,0,0,0,1,0,0.92565,7.0,0.906526,104
52640978-6dfc-4adf-97f5-d5ab45d2e5b5,Local Structure Matters Most in Most Languages,1,0.0022741,0.045992,"Many recent perturbation studies have found unintuitive results on what does
and does not matter when performing Natural Language Understanding (NLU) tasks
in English. Coding properties, such as the order of words, can often be removed
through shuffling without impacting downstream performances. Such insight may
be used to direct future research into English NLP models. As many improvements
in multilingual settings consist of wholesale adaptation of English approaches,
it is important to verify whether those studies replicate or not in
multilingual settings. In this work, we replicate a study on the importance of
local structure, and the relative unimportance of global structure, in a
multilingual setting. We find that the phenomenon observed on the English
language broadly translates to over 120 languages, with a few caveats.",0,0,0,0,0,0,0.19602,6.0,0.478022,37
b0c58dfa-3881-49d3-b8b8-56d0cd9f3388,NTULM: Enriching Social Media Text Representations with Non-Textual Units,3,0.0230374,0.408569,"On social media, additional context is often present in the form of
annotations and meta-data such as the post's author, mentions, Hashtags, and
hyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit
that NTUs provide social context beyond their textual semantics and leveraging
these units can enrich social media text representations. In this work we
construct an NTU-centric social heterogeneous network to co-embed NTUs. We then
principally integrate these NTU embeddings into a large pretrained language
model by fine-tuning with these additional units. This adds context to noisy
short-text social media. Experiments show that utilizing NTU-augmented text
representations significantly outperforms existing text-only baselines by 2-5\%
relative points on many downstream tasks highlighting the importance of context
to social media NLP. We also highlight that including NTU context into the
initial layers of language model alongside text is better than using it after
the text embedding is generated. Our work leads to the generation of holistic
general purpose social media content embedding.",0,1,0,0,0,0,0.278437,5.0,0.454147,30
1964cf85-6b0d-4333-830f-9b74a6b2f4d8,Style Matters! Investigating Linguistic Style in Online Communities,19,0.273308,0.2038,"Content has historically been the primary lens used to study language in
online communities. This paper instead focuses on the linguistic style of
communities. While we know that individuals have distinguishable styles, here
we ask whether communities have distinguishable styles. Additionally, while
prior work has relied on a narrow definition of style, we employ a broad
definition involving 262 features to analyze the linguistic style of 9 online
communities from 3 social media platforms discussing politics, television and
travel. We find that communities indeed have distinct styles. Also, style is an
excellent predictor of group membership (F-score 0.952 and Accuracy 96.09%).
While on average it is statistically equivalent to predictions using content
alone, it is more resilient to reductions in training data.",0,0,0,0,0,0,0.14156,19.0,0.816362,24
56335260-5663-487c-bf78-5c594f8cb63a,How can NLP Help Revitalize Endangered Languages? A Case Study and Roadmap for the Cherokee Language,21,0.411774,0.136946,"More than 43% of the languages spoken in the world are endangered, and
language loss currently occurs at an accelerated rate because of globalization
and neocolonialism. Saving and revitalizing endangered languages has become
very important for maintaining the cultural diversity on our planet. In this
work, we focus on discussing how NLP can help revitalize endangered languages.
We first suggest three principles that may help NLP practitioners to foster
mutual understanding and collaboration with language communities, and we
discuss three ways in which NLP can potentially assist in language education.
We then take Cherokee, a severely-endangered Native American language, as a
case study. After reviewing the language's history, linguistic features, and
existing resources, we (in collaboration with Cherokee community members)
arrive at a few meaningful ways NLP practitioners can collaborate with
community partners. We suggest two approaches to enrich the Cherokee language's
resources with machine-in-the-loop processing, and discuss several NLP tools
that people from the Cherokee community have shown interest in. We hope that
our work serves not only to inform the NLP community about Cherokee, but also
to provide inspiration for future work on endangered languages in general. Our
code and data will be open-sourced at
https://github.com/ZhangShiyue/RevitalizeCherokee",1,1,0,0,0,0,0.248504,12.0,0.761476,71
90774a69-84a8-476f-b363-7057b864a328,Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning,29,0.207871,0.64335,"Contrastive learning has been demonstrated to be effective in enhancing
pre-trained language models (PLMs) to derive superior universal sentence
embeddings. However, existing contrastive methods still have two limitations.
Firstly, previous works may acquire poor performance under domain shift
settings, thus hindering the application of sentence representations in
practice. We attribute this low performance to the over-parameterization of
PLMs with millions of parameters. To alleviate it, we propose PromCSE
(Prompt-based Contrastive Learning for Sentence Embeddings), which only trains
small-scale \emph{Soft Prompt} (i.e., a set of trainable vectors) while keeping
PLMs fixed. Secondly, the commonly used NT-Xent loss function of contrastive
learning does not fully exploit hard negatives in supervised learning settings.
To this end, we propose to integrate an Energy-based Hinge loss to enhance the
pairwise discriminative power, inspired by the connection between the NT-Xent
loss and the Energy-based Learning paradigm. Empirical results on seven
standard semantic textual similarity (STS) tasks and a domain-shifted STS task
both show the effectiveness of our method compared with the current
state-of-the-art sentence embedding models. Our code is publicly avaliable at
https://github.com/YJiangcm/PromCSE",1,1,0,0,1,1,0.887567,7.0,0.881765,74
d99e72aa-09a7-4c18-a128-392c3c8a178f,Memory-free Online Change-point Detection: A Novel Neural Network Approach,6,0.0930036,0.747131,"Change-point detection (CPD), which detects abrupt changes in the data
distribution, is recognized as one of the most significant tasks in time series
analysis. Despite the extensive literature on offline CPD, unsupervised online
CPD still suffers from major challenges, including scalability, hyperparameter
tuning, and learning constraints. To mitigate some of these challenges, in this
paper, we propose a novel deep learning approach for unsupervised online CPD
from multi-dimensional time series, named Adaptive LSTM-Autoencoder
Change-Point Detection (ALACPD). ALACPD exploits an LSTM-autoencoder-based
neural network to perform unsupervised online CPD. It continuously adapts to
the incoming samples without keeping the previously received input, thus being
memory-free. We perform an extensive evaluation on several real-world time
series CPD benchmarks. We show that ALACPD, on average, ranks first among
state-of-the-art CPD algorithms in terms of quality of the time series
segmentation, and it is on par with the best performer in terms of the accuracy
of the estimated change-points. The implementation of ALACPD is available
online on Github\footnote{\url{https://github.com/zahraatashgahi/ALACPD}}.",1,1,0,0,1,0,0.306917,10.0,0.738709,72
d6237d87-93c7-4e90-99f2-950687c7dfc7,A Two-Stage Efficient 3-D CNN Framework for EEG Based Emotion Recognition,4,0.0870505,0.345428,"This paper proposes a novel two-stage framework for emotion recognition using
EEG data that outperforms state-of-the-art models while keeping the model size
small and computationally efficient. The framework consists of two stages; the
first stage involves constructing efficient models named EEGNet, which is
inspired by the state-of-the-art efficient architecture and employs
inverted-residual blocks that contain depthwise separable convolutional layers.
The EEGNet models on both valence and arousal labels achieve the average
classification accuracy of 90%, 96.6%, and 99.5% with only 6.4k, 14k, and 25k
parameters, respectively. In terms of accuracy and storage cost, these models
outperform the previous state-of-the-art result by up to 9%. In the second
stage, we binarize these models to further compress them and deploy them easily
on edge devices. Binary Neural Networks (BNNs) typically degrade model
accuracy. We improve the EEGNet binarized models in this paper by introducing
three novel methods and achieving a 20\% improvement over the baseline binary
models. The proposed binarized EEGNet models achieve accuracies of 81%, 95%,
and 99% with storage costs of 0.11Mbits, 0.28Mbits, and 0.46Mbits,
respectively. Those models help deploy a precise human emotion recognition
system on the edge environment.",0,1,0,0,1,0,0.569692,12.0,0.851673,37
036b758f-c6c1-40a7-9275-1c41c8d3e75d,Socially Intelligent Genetic Agents for the Emergence of Explicit Norms,5,0.176315,0.320399,"Norms help regulate a society. Norms may be explicit (represented in
structured form) or implicit. We address the emergence of explicit norms by
developing agents who provide and reason about explanations for norm violations
in deciding sanctions and identifying alternative norms. These agents use a
genetic algorithm to produce norms and reinforcement learning to learn the
values of these norms. We find that applying explanations leads to norms that
provide better cohesion and goal satisfaction for the agents. Our results are
stable for societies with differing attitudes of generosity.",1,0,0,0,0,0,0.0857245,13.0,0.690648,37
01fc01e3-16e9-4d07-bf96-b087ed5e01cc,Human Interpretation of Saliency-based Explanation Over Text,27,0.144335,0.796287,"While a lot of research in explainable AI focuses on producing effective
explanations, less work is devoted to the question of how people understand and
interpret the explanation. In this work, we focus on this question through a
study of saliency-based explanations over textual data. Feature-attribution
explanations of text models aim to communicate which parts of the input text
were more influential than others towards the model decision. Many current
explanation methods, such as gradient-based or Shapley value-based methods,
provide measures of importance which are well-understood mathematically. But
how does a person receiving the explanation (the explainee) comprehend it? And
does their understanding match what the explanation attempted to communicate?
We empirically investigate the effect of various factors of the input, the
feature-attribution explanation, and visualization procedure, on laypeople's
interpretation of the explanation. We query crowdworkers for their
interpretation on tasks in English and German, and fit a GAMM model to their
responses considering the factors of interest. We find that people often
mis-interpret the explanations: superficial and unrelated factors, such as word
length, influence the explainees' importance assignment despite the explanation
communicating importance directly. We then show that some of this distortion
can be attenuated: we propose a method to adjust saliencies based on model
estimates of over- and under-perception, and explore bar charts as an
alternative to heatmap saliency visualization. We find that both approaches can
attenuate the distorting effect of specific factors, leading to
better-calibrated understanding of the explanation.",0,0,0,0,0,0,0.322351,7.0,0.635244,60
9bc00e0f-451c-4786-a7f4-b88d0749cf21,Humans disagree with the IoU for measuring object detector localization error,2,0.0056404,0.169392,"The localization quality of automatic object detectors is typically evaluated
by the Intersection over Union (IoU) score. In this work, we show that humans
have a different view on localization quality. To evaluate this, we conduct a
survey with more than 70 participants. Results show that for localization
errors with the exact same IoU score, humans might not consider that these
errors are equal, and express a preference. Our work is the first to evaluate
IoU with humans and makes it clear that relying on IoU scores alone to evaluate
localization errors might not be sufficient.",0,1,0,0,0,0,0.0535775,15.0,0.699423,30
e8c040fa-7afb-41b2-84f3-24e0f4105331,SemFormer: Semantic Guided Activation Transformer for Weakly Supervised Semantic Segmentation,1,0.0460502,0.0927033,"Recent mainstream weakly supervised semantic segmentation (WSSS) approaches
are mainly based on Class Activation Map (CAM) generated by a CNN
(Convolutional Neural Network) based image classifier. In this paper, we
propose a novel transformer-based framework, named Semantic Guided Activation
Transformer (SemFormer), for WSSS. We design a transformer-based Class-Aware
AutoEncoder (CAAE) to extract the class embeddings for the input image and
learn class semantics for all classes of the dataset. The class embeddings and
learned class semantics are then used to guide the generation of activation
maps with four losses, i.e., class-foreground, class-background, activation
suppression, and activation complementation loss. Experimental results show
that our SemFormer achieves \textbf{74.3}\% mIoU and surpasses many recent
mainstream WSSS approaches by a large margin on PASCAL VOC 2012 dataset. Code
will be available at \url{https://github.com/JLChen-C/SemFormer}.",1,0,0,0,1,0,0.980488,6.0,0.960152,47
1870a56a-401f-471d-9c43-41915e7d4bf2,Lightweight Monocular Depth Estimation with an Edge Guided Network,3,0.0424828,0.254361,"Monocular depth estimation is an important task that can be applied to many
robotic applications. Existing methods focus on improving depth estimation
accuracy via training increasingly deeper and wider networks, however these
suffer from large computational complexity. Recent studies found that edge
information are important cues for convolutional neural networks (CNNs) to
estimate depth. Inspired by the above observations, we present a novel
lightweight Edge Guided Depth Estimation Network (EGD-Net) in this study. In
particular, we start out with a lightweight encoder-decoder architecture and
embed an edge guidance branch which takes as input image gradients and
multi-scale feature maps from the backbone to learn the edge attention
features. In order to aggregate the context information and edge attention
features, we design a transformer-based feature aggregation module (TRFA). TRFA
captures the long-range dependencies between the context information and edge
attention features through cross-attention mechanism. We perform extensive
experiments on the NYU depth v2 dataset. Experimental results show that the
proposed method runs about 96 fps on a Nvidia GTX 1080 GPU whilst achieving the
state-of-the-art performance in terms of accuracy.",0,1,0,0,1,0,0.547144,8.0,0.769699,26
421c4ee6-9221-4eed-9bbb-eeed03b422d6,Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation,3,0.0331523,0.421889,"With rising male infertility, sperm head morphology classification becomes
critical for accurate and timely clinical diagnosis. Recent deep learning (DL)
morphology analysis methods achieve promising benchmark results, but leave
performance and robustness on the table by relying on limited and possibly
noisy class labels. To address this, we introduce a new DL training framework
that leverages anatomical and image priors from human sperm microscopy crops to
extract useful features without additional labeling cost. Our core idea is to
distill sperm head information with reliably-generated pseudo-masks and
unsupervised spatial prediction tasks. The predicted foreground masks from this
distillation step are then leveraged to regularize and reduce image and label
noise in the tuning stage. We evaluate our new approach on two public sperm
datasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy
and 96.5% HuSHeM accuracy).",0,1,0,0,1,0,0.154628,9.0,0.62296,18
983263ed-8a83-4ae4-9d9e-5e73eca3c3f6,Creativity in translation: machine translation as a constraint for literary texts,21,0.0364166,0.730876,"This article presents the results of a study involving the translation of a
short story by Kurt Vonnegut from English to Catalan and Dutch using three
modalities: machine-translation (MT), post-editing (PE) and translation without
aid (HT). Our aim is to explore creativity, understood to involve novelty and
acceptability, from a quantitative perspective. The results show that HT has
the highest creativity score, followed by PE, and lastly, MT, and this is
unanimous from all reviewers. A neural MT system trained on literary data does
not currently have the necessary capabilities for a creative translation; it
renders literal solutions to translation problems. More importantly, using MT
to post-edit raw output constrains the creativity of translators, resulting in
a poorer translation often not fit for publication, according to experts.",0,0,0,0,0,0,0.00189372,10.0,0.21223,52
6e64a6bd-069a-43a7-97f1-77199b4060bd,Test-Time Adaptation for Visual Document Understanding,5,0.116624,0.273317,"For visual document understanding (VDU), self-supervised pretraining has been
shown to successfully generate transferable representations, yet, effective
adaptation of such representations to distribution shifts at test-time remains
to be an unexplored area. We propose DocTTA, a novel test-time adaptation
method for documents, that does source-free domain adaptation using unlabeled
target document data. DocTTA leverages cross-modality self-supervised learning
via masked visual language modeling, as well as pseudo labeling to adapt models
learned on a \textit{source} domain to an unlabeled \textit{target} domain at
test time. We introduce new benchmarks using existing public datasets for
various VDU tasks, including entity recognition, key-value extraction, and
document visual question answering. DocTTA shows significant improvements on
these compared to the source model performance, up to 1.89\% in (F1 score),
3.43\% (F1 score), and 17.68\% (ANLS score), respectively. Our benchmark
datasets are available at \url{https://saynaebrahimi.github.io/DocTTA.html}.",0,1,0,0,1,0,0.984445,6.0,0.969481,62
8c22e898-500d-460d-a13b-aedb890b533d,What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation,18,0.276799,0.280826,"Accurate automatic evaluation metrics for open-domain dialogs are in high
demand. Existing model-based metrics for system response evaluation are trained
on human annotated data, which is cumbersome to collect. In this work, we
propose to use information that can be automatically extracted from the next
user utterance, such as its sentiment or whether the user explicitly ends the
conversation, as a proxy to measure the quality of the previous system
response. This allows us to train on a massive set of dialogs with weak
supervision, without requiring manual system turn quality annotations.
Experiments show that our model is comparable to models trained on human
annotated data. Furthermore, our model generalizes across both spoken and
written open-domain dialog corpora collected from real and paid users.",1,1,0,0,0,0,0.762351,4.0,0.688273,34
c01b4f42-f79c-4035-a1e4-263bda180195,CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds,50,0.220214,0.987915,"We present a novel two-stage fully sparse convolutional 3D object detection
framework, named CAGroup3D. Our proposed method first generates some
high-quality 3D proposals by leveraging the class-aware local group strategy on
the object surface voxels with the same semantic predictions, which considers
semantic consistency and diverse locality abandoned in previous bottom-up
approaches. Then, to recover the features of missed voxels due to incorrect
voxel-wise segmentation, we build a fully sparse convolutional RoI pooling
module to directly aggregate fine-grained spatial information from backbone for
further proposal refinement. It is memory-and-computation efficient and can
better encode the geometry-specific features of each 3D proposal. Our model
achieves state-of-the-art 3D detection performance with remarkable gains of
+\textit{3.6\%} on ScanNet V2 and +\textit{2.6}\% on SUN RGB-D in term of
mAP@0.25. Code will be available at https://github.com/Haiyang-W/CAGroup3D.",1,1,0,0,1,0,0.423197,7.0,0.684741,65
8d94e121-269e-47f1-b5d2-03e97c3635fd,Robust Monocular Localization of Drones by Adapting Domain Maps to Depth Prediction Inaccuracies,3,0.0620392,0.171273,"We present a novel monocular localization framework by jointly training deep
learning-based depth prediction and Bayesian filtering-based pose reasoning.
The proposed cross-modal framework significantly outperforms deep learning-only
predictions with respect to model scalability and tolerance to environmental
variations. Specifically, we show little-to-no degradation of pose accuracy
even with extremely poor depth estimates from a lightweight depth predictor.
Our framework also maintains high pose accuracy in extreme lighting variations
compared to standard deep learning, even without explicit domain adaptation. By
openly representing the map and intermediate feature maps (such as depth
estimates), our framework also allows for faster updates and reusing
intermediate predictions for other tasks, such as obstacle avoidance, resulting
in much higher resource efficiency.",0,1,0,0,0,0,0.606113,10.0,0.831981,24
055781b6-a489-4a94-9581-e811e475915f,Learnable Graph Convolutional Network and Feature Fusion for Multi-view Learning,45,0.288272,0.983776,"In practical applications, multi-view data depicting objectives from assorted
perspectives can facilitate the accuracy increase of learning algorithms.
However, given multi-view data, there is limited work for learning
discriminative node relationships and graph information simultaneously via
graph convolutional network that has drawn the attention from considerable
researchers in recent years. Most of existing methods only consider the
weighted sum of adjacency matrices, yet a joint neural network of both feature
and graph fusion is still under-explored. To cope with these issues, this paper
proposes a joint deep learning framework called Learnable Graph Convolutional
Network and Feature Fusion (LGCN-FF), consisting of two stages: feature fusion
network and learnable graph convolutional network. The former aims to learn an
underlying feature representation from heterogeneous views, while the latter
explores a more discriminative graph fusion via learnable weights and a
parametric activation function dubbed Differentiable Shrinkage Activation (DSA)
function. The proposed LGCN-FF is validated to be superior to various
state-of-the-art methods in multi-view semi-supervised classification.",0,0,0,0,1,0,0.226134,6.0,0.504896,58
72c22cba-2ca6-4e38-8f06-4db3b8ba1f99,Random Rank: The One and Only Strategyproof and Proportionally Fair Randomized Facility Location Mechanism,3,0.0264264,0.178221,"Proportionality is an attractive fairness concept that has been applied to a
range of problems including the facility location problem, a classic problem in
social choice. In our work, we propose a concept called Strong Proportionality,
which ensures that when there are two groups of agents at different locations,
both groups incur the same total cost. We show that although Strong
Proportionality is a well-motivated and basic axiom, there is no deterministic
strategyproof mechanism satisfying the property. We then identify a randomized
mechanism called Random Rank (which uniformly selects a number $k$ between $1$
to $n$ and locates the facility at the $k$'th highest agent location) which
satisfies Strong Proportionality in expectation. Our main theorem characterizes
Random Rank as the unique mechanism that achieves universal truthfulness,
universal anonymity, and Strong Proportionality in expectation among all
randomized mechanisms. Finally, we show via the AverageOrRandomRank mechanism
that even stronger ex-post fairness guarantees can be achieved by weakening
universal truthfulness to strategyproofness in expectation.",0,0,0,0,0,0,0.00242062,16.0,0.523003,49
643b0f08-c9f9-4a68-8546-6089ee60abe1,Finding patterns in Knowledge Attribution for Transformers,1,0.00626816,0.115831,"We analyze the Knowledge Neurons framework for the attribution of factual and
relational knowledge to particular neurons in the transformer network. We use a
12-layer multi-lingual BERT model for our experiments. Our study reveals
various interesting phenomena. We observe that mostly factual knowledge can be
attributed to middle and higher layers of the network($\ge 6$). Further
analysis reveals that the middle layers($6-9$) are mostly responsible for
relational information, which is further refined into actual factual knowledge
or the ""correct answer"" in the last few layers($10-12$). Our experiments also
show that the model handles prompts in different languages, but representing
the same fact, similarly, providing further evidence for effectiveness of
multi-lingual pre-training. Applying the attribution scheme for grammatical
knowledge, we find that grammatical knowledge is far more dispersed among the
neurons than factual knowledge.",0,0,0,0,0,0,0.549853,10.0,0.816514,34
35711953-ecd6-4d3a-8351-7b5ae9758d6d,Efficient yet Competitive Speech Translation: FBK@IWSLT2022,13,0.10968,0.688462,"The primary goal of this FBK's systems submission to the IWSLT 2022 offline
and simultaneous speech translation tasks is to reduce model training costs
without sacrificing translation quality. As such, we first question the need of
ASR pre-training, showing that it is not essential to achieve competitive
results. Second, we focus on data filtering, showing that a simple method that
looks at the ratio between source and target characters yields a quality
improvement of 1 BLEU. Third, we compare different methods to reduce the
detrimental effect of the audio segmentation mismatch between training data
manually segmented at sentence level and inference data that is automatically
segmented. Towards the same goal of training cost reduction, we participate in
the simultaneous task with the same model trained for offline ST. The
effectiveness of our lightweight training strategy is shown by the high score
obtained on the MuST-C en-de corpus (26.7 BLEU) and is confirmed in
high-resource data conditions by a 1.6 BLEU improvement on the IWSLT2020 test
set over last year's winning system.",1,1,0,0,0,0,0.547363,4.0,0.539551,50
cf0cf048-a5f8-4386-969a-594b207b2ab7,Leveraging Causal Inference for Explainable Automatic Program Repair,2,0.00893473,0.0709842,"Deep learning models have made significant progress in automatic program
repair. However, the black-box nature of these methods has restricted their
practical applications. To address this challenge, this paper presents an
interpretable approach for program repair based on sequence-to-sequence models
with causal inference and our method is called CPR, short for causal program
repair. Our CPR can generate explanations in the process of decision making,
which consists of groups of causally related input-output tokens. Firstly, our
method infers these relations by querying the model with inputs disturbed by
data augmentation. Secondly, it generates a graph over tokens from the
responses and solves a partitioning problem to select the most relevant
components. The experiments on four programming languages (Java, C, Python, and
JavaScript) show that CPR can generate causal graphs for reasonable
interpretations and boost the performance of bug fixing in automatic program
repair.",0,1,0,0,0,0,0.147568,9.0,0.617318,45
446124e5-8ddd-46ca-a183-f97c3364a6d4,Meta-Learning for Unsupervised Outlier Detection with Optimal Transport,1,0.0178325,0.0977081,"Automated machine learning has been widely researched and adopted in the
field of supervised classification and regression, but progress in unsupervised
settings has been limited. We propose a novel approach to automate outlier
detection based on meta-learning from previous datasets with outliers. Our
premise is that the selection of the optimal outlier detection technique
depends on the inherent properties of the data distribution. We leverage
optimal transport in particular, to find the dataset with the most similar
underlying distribution, and then apply the outlier detection techniques that
proved to work best for that data distribution. We evaluate the robustness of
our approach and find that it outperforms the state of the art methods in
unsupervised outlier detection. This approach can also be easily generalized to
automate other unsupervised settings.",0,0,0,0,1,0,0.207335,13.0,0.763928,56
e13d3cfb-ea21-4588-9d9e-727d968f7bc8,Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database,15,0.0948225,0.516673,"Parsing natural language questions into executable logical forms is a useful
and interpretable way to perform question answering on structured data such as
knowledge bases (KB) or databases (DB). However, existing approaches on
semantic parsing cannot adapt to both modalities, as they suffer from the
exponential growth of the logical form candidates and can hardly generalize to
unseen data. In this work, we propose Uni-Parser, a unified semantic parser for
question answering (QA) on both KB and DB. We introduce the primitive (relation
and entity in KB, and table name, column name and cell value in DB) as an
essential element in our framework. The number of primitives grows linearly
with the number of retrieved relations in KB and DB, preventing us from dealing
with exponential logic form candidates. We leverage the generator to predict
final logical forms by altering and composing topranked primitives with
different operations (e.g. select, where, count). With sufficiently pruned
search space by a contrastive primitive ranker, the generator is empowered to
capture the composition of primitives enhancing its generalization ability. We
achieve competitive results on multiple KB and DB QA benchmarks more
efficiently, especially in the compositional and zero-shot settings.",0,1,0,0,0,0,0.248778,6.0,0.523165,54
92bd1308-5018-473f-a9ec-5df62f80c577,Patch-NetVLAD+: Learned patch descriptor and weighted matching strategy for place recognition,5,0.0110554,0.204239,"Visual Place Recognition (VPR) in areas with similar scenes such as urban or
indoor scenarios is a major challenge. Existing VPR methods using global
descriptors have difficulty capturing local specific regions (LSR) in the scene
and are therefore prone to localization confusion in such scenarios. As a
result, finding the LSR that are critical for location recognition becomes key.
To address this challenge, we introduced Patch-NetVLAD+, which was inspired by
patch-based VPR researches. Our method proposed a fine-tuning strategy with
triplet loss to make NetVLAD suitable for extracting patch-level descriptors.
Moreover, unlike existing methods that treat all patches in an image equally,
our method extracts patches of LSR, which present less frequently throughout
the dataset, and makes them play an important role in VPR by assigning proper
weights to them. Experiments on Pittsburgh30k and Tokyo247 datasets show that
our approach achieved up to 6.35\% performance improvement than existing
patch-based methods.",0,1,0,0,0,0,0.0311892,9.0,0.437631,39
a6056969-4c84-421f-a344-f5c3147b5f7a,Differentiable Self-Adaptive Learning Rate,3,0.0697908,0.214954,"Learning rate adaptation is a popular topic in machine learning. Gradient
Descent trains neural nerwork with a fixed learning rate. Learning rate
adaptation is proposed to accelerate the training process through adjusting the
step size in the training session. Famous works include Momentum, Adam and
Hypergradient. Hypergradient is the most special one. Hypergradient achieved
adaptation by calculating the derivative of learning rate with respect to cost
function and utilizing gradient descent for learning rate. However,
Hypergradient is still not perfect. In practice, Hypergradient fail to decrease
training loss after learning rate adaptation with a large probability. Apart
from that, evidence has been found that Hypergradient are not suitable for
dealing with large datesets in the form of minibatch training. Most
unfortunately, Hypergradient always fails to get a good accuracy on the
validation dataset although it could reduce training loss to a very tiny value.
To solve Hypergradient's problems, we propose a novel adaptation algorithm,
where learning rate is parameter specific and internal structured. We conduct
extensive experiments on multiple network models and datasets compared with
various benchmark optimizers. It is shown that our algorithm can achieve faster
and higher qualified convergence than those state-of-art optimizers.",0,1,0,0,1,1,0.945806,16.0,0.966281,30
67ab042f-7542-4ba4-a639-b836e557ba47,SARAS-Net: Scale and Relation Aware Siamese Network for Change Detection,10,0.0474658,0.563476,"Change detection (CD) aims to find the difference between two images at
different times and outputs a change map to represent whether the region has
changed or not. To achieve a better result in generating the change map, many
State-of-The-Art (SoTA) methods design a deep learning model that has a
powerful discriminative ability. However, these methods still get lower
performance because they ignore spatial information and scaling changes between
objects, giving rise to blurry or wrong boundaries. In addition to these, they
also neglect the interactive information of two different images. To alleviate
these problems, we propose our network, the Scale and Relation-Aware Siamese
Network (SARAS-Net) to deal with this issue. In this paper, three modules are
proposed that include relation-aware, scale-aware, and cross-transformer to
tackle the problem of scene change detection more effectively. To verify our
model, we tested three public datasets, including LEVIR-CD, WHU-CD, and DSFIN,
and obtained SoTA accuracy. Our code is available at
https://github.com/f64051041/SARAS-Net.",1,1,0,0,1,0,0.209146,8.0,0.617608,31
31c1b0cc-4147-4ced-b092-4db142725521,$\mathcal{X}$-Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing,15,0.059962,0.976658,"This paper presents a generic probabilistic framework for estimating the
statistical dependency and finding the anatomical correspondences among an
arbitrary number of medical images. The method builds on a novel formulation of
the $N$-dimensional joint intensity distribution by representing the common
anatomy as latent variables and estimating the appearance model with
nonparametric estimators. Through connection to maximum likelihood and the
expectation-maximization algorithm, an information\hyp{}theoretic metric called
$\mathcal{X}$-metric and a co-registration algorithm named $\mathcal{X}$-CoReg
are induced, allowing groupwise registration of the $N$ observed images with
computational complexity of $\mathcal{O}(N)$. Moreover, the method naturally
extends for a weakly-supervised scenario where anatomical labels of certain
images are provided. This leads to a combined\hyp{}computing framework
implemented with deep learning, which performs registration and segmentation
simultaneously and collaboratively in an end-to-end fashion. Extensive
experiments were conducted to demonstrate the versatility and applicability of
our model, including multimodal groupwise registration, motion correction for
dynamic contrast enhanced magnetic resonance images, and deep combined
computing for multimodal medical images. Results show the superiority of our
method in various applications in terms of both accuracy and efficiency,
highlighting the advantage of the proposed representation of the imaging
process.",1,0,0,0,0,0,8.54582e-06,28.0,0.525732,59
05f4d825-0879-46a9-82df-dc7b378f9830,On the Evaluation of Answer-Agnostic Paragraph-level Multi-Question Generation,1,0.0298271,0.0283878,"We study the task of predicting a set of salient questions from a given
paragraph without any prior knowledge of the precise answer. We make two main
contributions. First, we propose a new method to evaluate a set of predicted
questions against the set of references by using the Hungarian algorithm to
assign predicted questions to references before scoring the assigned pairs. We
show that our proposed evaluation strategy has better theoretical and practical
properties compared to prior methods because it can properly account for the
coverage of references. Second, we compare different strategies to utilize a
pre-trained seq2seq model to generate and select a set of questions related to
a given paragraph. The code is available.",0,0,0,0,0,0,0.771262,8.0,0.847418,60
89274b28-cbb8-4f97-837c-4349cfab7d9e,LibertyMFD: A Lexicon to Assess the Moral Foundation of Liberty,6,0.0412975,0.239324,"Quantifying the moral narratives expressed in the user-generated text, news,
or public discourses is fundamental for understanding individuals' concerns and
viewpoints and preventing violent protests and social polarisation. The Moral
Foundation Theory (MFT) was developed to operationalise morality in a
five-dimensional scale system. Recent developments of the theory urged for the
introduction of a new foundation, the Liberty Foundation. Being only recently
added to the theory, there are no available linguistic resources to assess
whether liberty is present in text corpora. Given its importance to current
social issues such as the vaccination debate, we propose two data-driven
approaches, deriving two candidate lexicons generated based on aligned
documents from online news sources with different worldviews. After extensive
experimentation, we contribute to the research community a novel lexicon that
assesses the liberty moral foundation in the way individuals with contrasting
viewpoints express themselves through written text. The LibertyMFD dictionary
can be a valuable tool for policymakers to understand diverse viewpoints on
controversial social issues such as vaccination, abortion, or even uprisings,
as they happen and on a large scale.",0,1,0,1,0,0,0.0233343,12.0,0.553709,34
26171536-7f7c-4a86-9caf-951b23e71f3a,Traceable and Authenticable Image Tagging for Fake News Detection,1,0.0425484,0.255494,"To prevent fake news images from misleading the public, it is desirable not
only to verify the authenticity of news images but also to trace the source of
fake news, so as to provide a complete forensic chain for reliable fake news
detection. To simultaneously achieve the goals of authenticity verification and
source tracing, we propose a traceable and authenticable image tagging approach
that is based on a design of Decoupled Invertible Neural Network (DINN). The
designed DINN can simultaneously embed the dual-tags, \textit{i.e.},
authenticable tag and traceable tag, into each news image before publishing,
and then separately extract them for authenticity verification and source
tracing. Moreover, to improve the accuracy of dual-tags extraction, we design a
parallel Feature Aware Projection Model (FAPM) to help the DINN preserve
essential tag information. In addition, we define a Distance Metric-Guided
Module (DMGM) that learns asymmetric one-class representations to enable the
dual-tags to achieve different robustness performances under malicious
manipulations. Extensive experiments, on diverse datasets and unseen
manipulations, demonstrate that the proposed tagging approach achieves
excellent performance in the aspects of both authenticity verification and
source tracing for reliable fake news detection and outperforms the prior
works.",0,1,0,0,0,0,0.390419,9.0,0.743037,41
3c95727d-35d7-44ee-9af1-140cc8eb2438,Region-Aware Face Swapping,33,0.409678,0.731105,"This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to
achieve identity-consistent harmonious high-resolution face generation in a
local-global manner: \textbf{1)} Local Facial Region-Aware (FRA) branch
augments local identity-relevant features by introducing the Transformer to
effectively model misaligned cross-scale semantic interaction. \textbf{2)}
Global Source Feature-Adaptive (SFA) branch further complements global
identity-relevant cues for generating identity-consistent swapped faces.
Besides, we propose a \textit{Face Mask Predictor} (FMP) module incorporated
with StyleGAN2 to predict identity-relevant soft facial masks in an
unsupervised manner that is more practical for generating harmonious
high-resolution faces. Abundant experiments qualitatively and quantitatively
demonstrate the superiority of our method for generating more
identity-consistent high-resolution swapped faces over SOTA methods, \eg,
obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\uparrow$.",0,1,0,0,1,0,0.883388,6.0,0.859253,59
173e7c7a-45f1-4668-afa7-3cf34e411172,Patents Phrase to Phrase Semantic Matching Dataset,5,0.0580126,0.531412,"There are many general purpose benchmark datasets for Semantic Textual
Similarity but none of them are focused on technical concepts found in patents
and scientific publications. This work aims to fill this gap by presenting a
new human rated contextual phrase to phrase matching dataset. The entire
dataset contains close to $50,000$ rated phrase pairs, each with a CPC
(Cooperative Patent Classification) class as a context. This paper describes
the dataset and some baseline models.",0,1,1,1,0,0,0.6596,15.0,0.897689,14
ac6e2222-e7f1-4985-9ce1-16665fcb1360,Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis,19,0.216059,0.962557,"We use a deep learning based approach to predict whether a selected element
in a mobile UI screenshot will be perceived by users as tappable, based on
pixels only instead of view hierarchies required by previous work. To help
designers better understand model predictions and to provide more actionable
design feedback than predictions alone, we additionally use ML interpretability
techniques to help explain the output of our model. We use XRAI to highlight
areas in the input screenshot that most strongly influence the tappability
prediction for the selected region, and use k-Nearest Neighbors to present the
most similar mobile UIs from the dataset with opposing influences on
tappability perception.",1,1,0,1,0,0,0.322829,7.0,0.635503,47
a7a61e5e-ea34-465e-8d90-3ab3bec08412,Zero-shot Cross-lingual Transfer is Under-specified Optimization,6,0.123095,0.31641,"Pretrained multilingual encoders enable zero-shot cross-lingual transfer, but
often produce unreliable models that exhibit high performance variance on the
target language. We postulate that this high variance results from zero-shot
cross-lingual transfer solving an under-specified optimization problem. We show
that any linear-interpolated model between the source language monolingual
model and source + target bilingual model has equally low source language
generalization error, yet the target language generalization error reduces
smoothly and linearly as we move from the monolingual to bilingual model,
suggesting that the model struggles to identify good solutions for both source
and target languages using the source language alone. Additionally, we show
that zero-shot solution lies in non-flat region of target language error
generalization surface, causing the high variance.",0,0,0,0,0,0,0.930211,7.0,0.909963,31
0344e761-a907-46ee-a3bc-b403bee150fa,Winning the CVPR'2022 AQTC Challenge: A Two-stage Function-centric Approach,2,0.0196894,0.0552861,"Affordance-centric Question-driven Task Completion for Egocentric
Assistant(AQTC) is a novel task which helps AI assistant learn from
instructional videos and scripts and guide the user step-by-step. In this
paper, we deal with the AQTC via a two-stage Function-centric approach, which
consists of Question2Function Module to ground the question with the related
function and Function2Answer Module to predict the action based on the
historical steps. We evaluated several possible solutions in each module and
obtained significant gains compared to the given baselines. Our code is
available at \url{https://github.com/starsholic/LOVEU-CVPR22-AQTC}.",1,1,1,0,0,0,0.75682,14.0,0.909782,22
75a2bc5d-e545-4a1d-be33-d2426fecd16b,MIRROR: Differentiable Deep Social Projection for Assistive Human-Robot Communication,8,0.230835,0.68292,"Communication is a hallmark of intelligence. In this work, we present MIRROR,
an approach to (i) quickly learn human models from human demonstrations, and
(ii) use the models for subsequent communication planning in assistive
shared-control settings. MIRROR is inspired by social projection theory, which
hypothesizes that humans use self-models to understand others. Likewise, MIRROR
leverages self-models learned using reinforcement learning to bootstrap human
modeling. Experiments with simulated humans show that this approach leads to
rapid learning and more robust models compared to existing behavioral cloning
and state-of-the-art imitation learning methods. We also present a
human-subject study using the CARLA simulator which shows that (i) MIRROR is
able to scale to complex domains with high-dimensional observations and
complicated world physics and (ii) provides effective assistive communication
that enabled participants to drive more safely in adverse weather conditions.",0,1,0,0,0,0,0.86075,7.0,0.86705,53
7b268b2a-69d6-4ee6-8931-119b86267ed3,Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring,21,0.109561,0.559861,"Attributes skew hinders the current federated learning (FL) frameworks from
consistent optimization directions among the clients, which inevitably leads to
performance reduction and unstable convergence. The core problems lie in that:
1) Domain-specific attributes, which are non-causal and only locally valid, are
indeliberately mixed into global aggregation. 2) The one-stage optimizations of
entangled attributes cannot simultaneously satisfy two conflicting objectives,
i.e., generalization and personalization. To cope with these, we proposed
disentangled federated learning (DFL) to disentangle the domain-specific and
cross-invariant attributes into two complementary branches, which are trained
by the proposed alternating local-global optimization independently.
Importantly, convergence analysis proves that the FL system can be stably
converged even if incomplete client models participate in the global
aggregation, which greatly expands the application scope of FL. Extensive
experiments verify that DFL facilitates FL with higher performance, better
interpretability, and faster convergence rate, compared with SOTA FL methods on
both manually synthesized and realistic attributes skew datasets.",0,0,0,0,1,0,0.683187,7.0,0.789977,36
e1806389-d28d-4fe2-a67f-fdd9da8bfb24,Combining Lipschitz and RBF Surrogate Models for High-dimensional Computationally Expensive Problems,14,0.126317,0.747803,"Standard evolutionary optimization algorithms assume that the evaluation of
the objective and constraint functions is straightforward and computationally
cheap. However, in many real-world optimization problems, these evaluations
involve computationally expensive numerical simulations or physical
experiments. Surrogate-assisted evolutionary algorithms (SAEAs) have recently
gained increased attention for their performance in solving these types of
problems. The main idea of SAEAs is the integration of an evolutionary
algorithm with a selected surrogate model that approximates the computationally
expensive function. In this paper, we propose a surrogate model based on a
Lipschitz underestimation and use it to develop a differential evolution-based
algorithm. The algorithm, called Lipschitz Surrogate-assisted Differential
Evolution (LSADE), utilizes the Lipschitz-based surrogate model, along with a
standard radial basis function surrogate model and a local search procedure.
The experimental results on seven benchmark functions of dimensions 30, 50,
100, and 200 show that the proposed LSADE algorithm is competitive compared
with the state-of-the-art algorithms under a limited computational budget,
being especially effective for the very complicated benchmark functions in high
dimensions.",0,1,0,0,1,0,0.0374856,14.0,0.651843,50
7858b2e4-8800-48e1-b230-4aaeb32bf9fb,Entailment Semantics Can Be Extracted from an Ideal Language Model,10,0.201135,0.401227,"Language models are often trained on text alone, without additional
grounding. There is debate as to how much of natural language semantics can be
inferred from such a procedure. We prove that entailment judgments between
sentences can be extracted from an ideal language model that has perfectly
learned its target distribution, assuming the training sentences are generated
by Gricean agents, i.e., agents who follow fundamental principles of
communication from the linguistic theory of pragmatics. We also show entailment
judgments can be decoded from the predictions of a language model trained on
such Gricean data. Our results reveal a pathway for understanding the semantic
information encoded in unlabeled linguistic data and a potential framework for
extracting semantics from language models.",0,0,0,0,0,0,0.648652,10.0,0.843551,29
2affec14-cf16-4861-af60-2e54e56b50cf,Self-Supervised Domain Calibration and Uncertainty Estimation for Place Recognition,5,0.0608308,0.253216,"Visual place recognition techniques based on deep learning, which have
imposed themselves as the state-of-the-art in recent years, do not generalize
well to environments visually different from the training set. Thus, to achieve
top performance, it is sometimes necessary to fine-tune the networks to the
target environment. To this end, we propose a self-supervised domain
calibration procedure based on robust pose graph optimization from Simultaneous
Localization and Mapping (SLAM) as the supervision signal without requiring GPS
or manual labeling. Moreover, we leverage the procedure to improve uncertainty
estimation for place recognition matches which is important in safety critical
applications. We show that our approach can improve the performance of a
state-of-the-art technique on a target environment dissimilar from its training
set and that we can obtain uncertainty estimates. We believe that this approach
will help practitioners to deploy robust place recognition solutions in
real-world applications. Our code is available publicly:
https://github.com/MISTLab/vpr-calibration-and-uncertainty",1,1,0,0,0,0,0.517864,6.0,0.679201,51
5dc48c31-26b3-4857-93d6-97c7e32f9c4b,SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners,14,0.157988,0.539665,"Recently, self-supervised Masked Autoencoders (MAE) have attracted
unprecedented attention for their impressive representation learning ability.
However, the pretext task, Masked Image Modeling (MIM), reconstructs the
missing local patches, lacking the global understanding of the image. This
paper extends MAE to a fully supervised setting by adding a supervised
classification branch, thereby enabling MAE to learn global features from
golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a
visible subset of image patches for classification, unlike the standard
supervised pre-training where all image patches are used. Through experiments,
we demonstrate that SupMAE is not only more training efficient but it also
learns more robust and transferable features. Specifically, SupMAE achieves
comparable performance with MAE using only 30% of compute when evaluated on
ImageNet with the ViT-B/16 model. SupMAE's robustness on ImageNet variants and
transfer learning performance outperforms MAE and standard supervised
pre-training counterparts. Codes are available at
https://github.com/enyac-group/supmae.",1,1,0,0,1,0,0.964752,9.0,0.955348,67
34413a03-cda8-4c8c-9ffa-fb10eb7d8f5c,Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models,5,0.0368591,0.438925,"Our native language influences the way we perceive speech sounds, affecting
our ability to discriminate non-native sounds. We compare two ideas about the
influence of the native language on speech perception: the Perceptual
Assimilation Model, which appeals to a mental classification of sounds into
native phoneme categories, versus the idea that rich, fine-grained phonetic
representations tuned to the statistics of the native language, are sufficient.
We operationalize this idea using representations from two state-of-the-art
speech models, a Dirichlet process Gaussian mixture model and the more recent
wav2vec 2.0 model. We present a new, open dataset of French- and
English-speaking participants' speech perception behaviour for 61 vowel sounds
from six languages. We show that phoneme assimilation is a better predictor
than fine-grained phonetic modelling, both for the discrimination behaviour as
a whole, and for predicting differences in discriminability associated with
differences in native language background. We also show that wav2vec 2.0, while
not good at capturing the effects of native language on speech perception, is
complementary to information about native phoneme assimilation, and provides a
good model of low-level phonetic representations, supporting the idea that both
categorical and fine-grained perception are used during speech perception.",0,0,0,1,0,0,0.00416784,17.0,0.583076,27
fcbb8e1d-0b36-431e-83db-f4a20f9cbf1b,Model Agnostic Interpretability for Multiple Instance Learning,6,0.0,0.112071,"In Multiple Instance Learning (MIL), models are trained using bags of
instances, where only a single label is provided for each bag. A bag label is
often only determined by a handful of key instances within a bag, making it
difficult to interpret what information a classifier is using to make
decisions. In this work, we establish the key requirements for interpreting MIL
models. We then go on to develop several model-agnostic approaches that meet
these requirements. Our methods are compared against existing inherently
interpretable MIL models on several datasets, and achieve an increase in
interpretability accuracy of up to 30%. We also examine the ability of the
methods to identify interactions between instances and scale to larger
datasets, improving their applicability to real-world problems.",1,1,0,0,0,0,0.00364377,13.0,0.444435,30
96e2c774-2480-4836-ac2e-71d8e42bda0a,Interactive Concept Bottleneck Models,21,0.16749,0.98592,"Concept bottleneck models (CBMs) are interpretable neural networks that first
predict labels for human-interpretable concepts relevant to the prediction
task, and then predict the final label based on the concept label predictions.
We extend CBMs to interactive prediction settings where the model can query a
human collaborator for the label to some concepts. We develop an interaction
policy that, at prediction time, chooses which concepts to request a label for
so as to maximally improve the final prediction. We demonstrate that a simple
policy combining concept prediction uncertainty and influence of the concept on
the final prediction achieves strong performance and outperforms static
approaches as well as active feature acquisition methods proposed in the
literature. We show that the interactive CBM can achieve accuracy gains of
5-10% with only 5 interactions over competitive baselines on the Caltech-UCSD
Birds, CheXpert and OAI datasets.",1,1,0,0,0,0,0.106708,8.0,0.526099,15
3ff6da70-d4c8-4a18-a326-7f2b3d73449d,Visually-Augmented Language Modeling,16,0.224465,0.70736,"Human language is grounded on multimodal knowledge including visual knowledge
like colors, sizes, and shapes. However, current large-scale pre-trained
language models rely on text-only self-supervised training with massive text
data, which precludes them from utilizing relevant visual information when
necessary. To address this, we propose a novel pre-training framework, named
VaLM, to Visually-augment text tokens with retrieved relevant images for
Language Modeling. Specifically, VaLM builds on a novel latent text-image
alignment method via an image retrieval module to fetch corresponding images
given a textual context. With the visually-augmented context, VaLM uses a
visual knowledge fusion layer to enable multimodal grounded language modeling
by attending to both text context and visual knowledge in images. We evaluate
VaLM on various visual knowledge-intensive commonsense reasoning tasks, which
require visual information to excel. The experimental results illustrate that
VaLM outperforms all strong language-only and vision-language baselines with
substantial gains in reasoning object commonsense including color, size, and
shape. Our code is available at https://github.com/Victorwz/VaLM.",1,0,0,0,0,0,0.934251,6.0,0.89865,56
1ab33d5c-ce9c-4767-a48d-81511c21a4b4,Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation,15,0.100231,0.67135,"After the great success of Vision Transformer variants (ViTs) in computer
vision, it has also demonstrated great potential in domain adaptive semantic
segmentation. Unfortunately, straightforwardly applying local ViTs in domain
adaptive semantic segmentation does not bring in expected improvement. We find
that the pitfall of local ViTs is due to the severe high-frequency components
generated during both the pseudo-label construction and features alignment for
target domains. These high-frequency components make the training of local ViTs
very unsmooth and hurt their transferability. In this paper, we introduce a
low-pass filtering mechanism, momentum network, to smooth the learning dynamics
of target domain features and pseudo labels. Furthermore, we propose a dynamic
of discrepancy measurement to align the distributions in the source and target
domains via dynamic weights to evaluate the importance of the samples. After
tackling the above issues, extensive experiments on sim2real benchmarks show
that the proposed method outperforms the state-of-the-art methods. Our codes
are available at https://github.com/alpc91/TransDA",1,1,0,0,1,0,0.889961,7.0,0.883166,57
5bf8001e-4c5b-4953-9425-dbcf6962fc81,PromptBERT: Improving BERT Sentence Embeddings with Prompts,72,0.451315,0.932574,"We propose PromptBERT, a novel contrastive learning method for learning
better sentence representation. We firstly analyze the drawback of current
sentence embedding from original BERT and find that it is mainly due to the
static token embedding bias and ineffective BERT layers. Then we propose the
first prompt-based sentence embeddings method and discuss two prompt
representing methods and three prompt searching methods to make BERT achieve
better sentence embeddings. Moreover, we propose a novel unsupervised training
objective by the technology of template denoising, which substantially shortens
the performance gap between the supervised and unsupervised settings. Extensive
experiments show the effectiveness of our method. Compared to SimCSE,
PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and
RoBERTa in the unsupervised setting.",1,0,0,0,1,0,0.948244,7.0,0.925166,29
fce9bf18-62ac-478f-98a3-aaa1cf19e6ee,Spontaneous Emerging Preference in Two-tower Language Model,1,0.0118803,0.209694,"The ever-growing size of the foundation language model has brought
significant performance gains in various types of downstream tasks. With the
existence of side-effects brought about by the large size of the foundation
language model such as deployment cost, availability issues, and environmental
cost, there is some interest in exploring other possible directions, such as a
divide-and-conquer scheme. In this paper, we are asking a basic question: are
language processes naturally dividable? We study this problem with a simple
two-tower language model setting, where two language models with identical
configurations are trained side-by-side cooperatively. With this setting, we
discover the spontaneous emerging preference phenomenon, where some of the
tokens are consistently better predicted by one tower while others by another
tower. This phenomenon is qualitatively stable, regardless of model
configuration and type, suggesting this as an intrinsic property of natural
language. This study suggests that interesting properties of natural language
are still waiting to be discovered, which may aid the future development of
natural language processing techniques.",0,0,0,0,0,0,0.957564,7.0,0.934437,41
02adf522-7f92-474c-9a82-ba46a2acf5b3,Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data,69,0.247894,0.995833,"Retrieval-based methods have been shown to be effective in NLP tasks via
introducing external knowledge. However, the indexing and retrieving of
large-scale corpora bring considerable computational cost. Surprisingly, we
found that REtrieving from the traINing datA (REINA) only can lead to
significant gains on multiple NLG and NLU tasks. We retrieve the labeled
training instances most similar to the input text and then concatenate them
with the input to feed into the model to generate the output. Experimental
results show that this simple method can achieve significantly better
performance on a variety of NLU and NLG tasks, including summarization, machine
translation, language modeling, and question answering tasks. For instance, our
proposed method achieved state-of-the-art results on XSum, BigPatent, and
CommonsenseQA. Our code is released, https://github.com/microsoft/REINA .",0,1,0,0,1,1,0.693131,6.0,0.759535,48
7203aa2b-4bed-4800-9ecb-e3084e93fba5,Arabic Fake News Detection Based on Deep Contextualized Embedding Models,25,0.422273,0.928891,"Social media is becoming a source of news for many people due to its ease and
freedom of use. As a result, fake news has been spreading quickly and easily
regardless of its credibility, especially in the last decade. Fake news
publishers take advantage of critical situations such as the Covid-19 pandemic
and the American presidential elections to affect societies negatively. Fake
news can seriously impact society in many fields including politics, finance,
sports, etc. Many studies have been conducted to help detect fake news in
English, but research conducted on fake news detection in the Arabic language
is scarce. Our contribution is twofold: first, we have constructed a large and
diverse Arabic fake news dataset. Second, we have developed and evaluated
transformer-based classifiers to identify fake news while utilizing eight
state-of-the-art Arabic contextualized embedding models. The majority of these
models had not been previously used for Arabic fake news detection. We conduct
a thorough analysis of the state-of-the-art Arabic contextualized embedding
models as well as comparison with similar fake news detection systems.
Experimental results confirm that these state-of-the-art models are robust,
with accuracy exceeding 98%.",0,1,1,1,1,0,0.40204,5.0,0.545096,40
abc0010d-f133-48b8-bba3-a697c1a27910,On-Board Pedestrian Trajectory Prediction Using Behavioral Features,7,0.15274,0.562441,"This paper presents a novel approach to pedestrian trajectory prediction for
on-board camera systems, which utilizes behavioral features of pedestrians that
can be inferred from visual observations. Our proposed method, called
Behavior-Aware Pedestrian Trajectory Prediction (BA-PTP), processes multiple
input modalities, i.e. bounding boxes, body and head orientation of pedestrians
as well as their pose, with independent encoding streams. The encodings of each
stream are fused using a modality attention mechanism, resulting in a final
embedding that is used to predict future bounding boxes in the image.
  In experiments on two datasets for pedestrian behavior prediction, we
demonstrate the benefit of using behavioral features for pedestrian trajectory
prediction and evaluate the effectiveness of the proposed encoding strategy.
Additionally, we investigate the relevance of different behavioral features on
the prediction performance based on an ablation study.",0,1,0,0,0,0,0.563672,6.0,0.700577,37
547b018c-263c-43b1-ae1e-a450ccdf3678,Surgical Skill Assessment via Video Semantic Aggregation,7,0.338347,0.827443,"Automated video-based assessment of surgical skills is a promising task in
assisting young surgical trainees, especially in poor-resource areas. Existing
works often resort to a CNN-LSTM joint framework that models long-term
relationships by LSTMs on spatially pooled short-term CNN features. However,
this practice would inevitably neglect the difference among semantic concepts
such as tools, tissues, and background in the spatial dimension, impeding the
subsequent temporal relationship modeling. In this paper, we propose a novel
skill assessment framework, Video Semantic Aggregation (ViSA), which discovers
different semantic parts and aggregates them across spatiotemporal dimensions.
The explicit discovery of semantic parts provides an explanatory visualization
that helps understand the neural network's decisions. It also enables us to
further incorporate auxiliary information such as the kinematic data to improve
representation learning and performance. The experiments on two datasets show
the competitiveness of ViSA compared to state-of-the-art methods. Source code
is available at: bit.ly/MICCAI2022ViSA.",0,0,0,0,0,0,0.915135,8.0,0.91168,27
6c7b6f8e-d84d-487e-af23-dd616f17f5a9,Private Quantiles Estimation in the Presence of Atoms,6,0.138897,0.597973,"We consider the differentially private estimation of multiple quantiles (MQ)
of a distribution from a dataset, a key building block in modern data analysis.
We apply the recent non-smoothed Inverse Sensitivity (IS) mechanism to this
specific problem. We establish that the resulting method is closely related to
the recently published ad hoc algorithm JointExp. In particular, they share the
same computational complexity and a similar efficiency. We prove the
statistical consistency of these two algorithms for continuous distributions.
Furthermore, we demonstrate both theoretically and empirically that this method
suffers from an important lack of performance in the case of peaked
distributions, which can degrade up to a potentially catastrophic impact in the
presence of atoms. Its smoothed version (i.e. by applying a max kernel to its
output density) would solve this problem, but remains an open challenge to
implement. As a proxy, we propose a simple and numerically efficient method
called Heuristically Smoothed JointExp (HSJointExp), which is endowed with
performance guarantees for a broad class of distributions and achieves results
that are orders of magnitude better on problematic datasets.",1,1,0,0,0,0,0.352128,16.0,0.84725,48
884f0717-a08f-4a69-b7b8-eccec7f22a09,ProPaLL: Probabilistic Partial Label Learning,1,0.00955458,0.140387,"Partial label learning is a type of weakly supervised learning, where each
training instance corresponds to a set of candidate labels, among which only
one is true. In this paper, we introduce ProPaLL, a novel probabilistic
approach to this problem, which has at least three advantages compared to the
existing approaches: it simplifies the training process, improves performance,
and can be applied to any deep architecture. Experiments conducted on
artificial and real-world datasets indicate that ProPaLL outperforms the
existing approaches.",1,1,1,0,1,0,0.267263,11.0,0.7475,43
1b02ab64-e9cb-41fb-a27b-6429e39013bf,Batch-efficient EigenDecomposition for Small and Medium Matrices,2,0.0544776,0.270007,"EigenDecomposition (ED) is at the heart of many computer vision algorithms
and applications. One crucial bottleneck limiting its usage is the expensive
computation cost, particularly for a mini-batch of matrices in the deep neural
networks. In this paper, we propose a QR-based ED method dedicated to the
application scenarios of computer vision. Our proposed method performs the ED
entirely by batched matrix/vector multiplication, which processes all the
matrices simultaneously and thus fully utilizes the power of GPUs. Our
technique is based on the explicit QR iterations by Givens rotation with double
Wilkinson shifts. With several acceleration techniques, the time complexity of
QR iterations is reduced from $O{(}n^5{)}$ to $O{(}n^3{)}$. The numerical test
shows that for small and medium batched matrices (\emph{e.g.,} $dim{<}32$) our
method can be much faster than the Pytorch SVD function. Experimental results
on visual recognition and image generation demonstrate that our methods also
achieve competitive performances.",0,1,0,0,0,0,0.558997,10.0,0.819053,49
ec6e56c4-16c6-4bb2-9f85-27bfe947f58c,Self-conditioned Embedding Diffusion for Text Generation,55,0.208554,0.65196,"Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete
nature of text data, we can simply project tokens in a continuous space of
embeddings, as is standard in language modeling. We propose Self-conditioned
Embedding Diffusion, a continuous diffusion mechanism that operates on token
embeddings and allows to learn flexible and scalable diffusion models for both
conditional and unconditional text generation. Through qualitative and
quantitative evaluation, we show that our text diffusion models generate
samples comparable with those produced by standard autoregressive language
models - while being in theory more efficient on accelerator hardware at
inference time. Our work paves the way for scaling up diffusion models for
text, similarly to autoregressive models, and for improving performance with
recent refinements to continuous diffusion.",0,0,0,0,0,0,0.938484,4.0,0.854012,35
a38a5781-3be8-47c9-b2bb-f97ca900011f,Semantically Proportional Patchmix for Few-Shot Learning,1,0.00459273,0.0229563,"Few-shot learning aims to classify unseen classes with only a limited number
of labeled data. Recent works have demonstrated that training models with a
simple transfer learning strategy can achieve competitive results in few-shot
classification. Although excelling at distinguishing training data, these
models are not well generalized to unseen data, probably due to insufficient
feature representations on evaluation. To tackle this issue, we propose
Semantically Proportional Patchmix (SePPMix), in which patches are cut and
pasted among training images and the ground truth labels are mixed
proportionally to the semantic information of the patches. In this way, we can
improve the generalization ability of the model by regional dropout effect
without introducing severe label noise. To learn more robust representations of
data, we further take rotate transformation on the mixed images and predict
rotations as a rule-based regularizer. Extensive experiments on prevalent
few-shot benchmarks have shown the effectiveness of our proposed method.",0,1,0,0,0,0,0.853229,8.0,0.880288,29
b7a290e5-a5bc-429e-8703-c7d0f8325095,Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations,3,0.0934331,0.149377,"We suggest a simple practical method to combine the human and artificial
intelligence to both learn best investment practices of fund managers, and
provide recommendations to improve them. Our approach is based on a combination
of Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns
the intent of fund managers as suggested by their trading history, and recovers
their implied reward function. At the second step, this reward function is used
by a direct RL algorithm to optimize asset allocation decisions. We show that
our method is able to improve over the performance of individual fund managers.",0,1,0,0,0,0,0.959949,13.0,0.966092,8
c5cd5a66-e654-4e72-adef-0b51aa2ebae7,Panchromatic and Multispectral Image Fusion via Alternating Reverse Filtering Network,8,0.0362978,0.343064,"Panchromatic (PAN) and multi-spectral (MS) image fusion, named
Pan-sharpening, refers to super-resolve the low-resolution (LR) multi-spectral
(MS) images in the spatial domain to generate the expected high-resolution (HR)
MS images, conditioning on the corresponding high-resolution PAN images. In
this paper, we present a simple yet effective \textit{alternating reverse
filtering network} for pan-sharpening. Inspired by the classical reverse
filtering that reverses images to the status before filtering, we formulate
pan-sharpening as an alternately iterative reverse filtering process, which
fuses LR MS and HR MS in an interpretable manner. Different from existing
model-driven methods that require well-designed priors and degradation
assumptions, the reverse filtering process avoids the dependency on pre-defined
exact priors. To guarantee the stability and convergence of the iterative
process via contraction mapping on a metric space, we develop the learnable
multi-scale Gaussian kernel module, instead of using specific filters. We
demonstrate the theoretical feasibility of such formulations. Extensive
experiments on diverse scenes to thoroughly verify the performance of our
method, significantly outperforming the state of the arts.",0,1,1,0,1,0,0.0821399,8.0,0.491722,79
603203f6-ccc1-4945-bdfb-6d21ecc3d272,Continual VQA for Disaster Response Systems,1,0.0143203,0.0580357,"Visual Question Answering (VQA) is a multi-modal task that involves answering
questions from an input image, semantically understanding the contents of the
image and answering it in natural language. Using VQA for disaster management
is an important line of research due to the scope of problems that are answered
by the VQA system. However, the main challenge is the delay caused by the
generation of labels in the assessment of the affected areas. To tackle this,
we deployed pre-trained CLIP model, which is trained on visual-image pairs.
however, we empirically see that the model has poor zero-shot performance.
Thus, we instead use pre-trained embeddings of text and image from this model
for our supervised training and surpass previous state-of-the-art results on
the FloodNet dataset. We expand this to a continual setting, which is a more
real-life scenario. We tackle the problem of catastrophic forgetting using
various experience replay methods. Our training runs are available at:
https://wandb.ai/compyle/continual_vqa_final. Our code is available at
https://github.com/AdityaKane2001/continual_vqa.",1,1,0,0,0,0,0.756854,4.0,0.684263,11
61df4f62-58b1-47b7-92a8-5ae048d1c15f,PromptBoosting: Black-Box Text Classification with Ten Forward Passes,23,0.0485509,0.715876,"We describe PromptBoosting, a query-efficient procedure for building a text
classifier from a neural language model (LM) without access to the LM's
parameters, gradients, or hidden representations. This form of ""black-box""
classifier training has become increasingly important as the cost of training
and inference in large-scale LMs grows. But existing black-box LM classifier
learning approaches are themselves computationally inefficient, typically
specializing LMs to the target task by searching in a large space of (discrete
or continuous) prompts using zeroth-order optimization methods. Instead of
directly optimizing in prompt space, PromptBoosting obtains a small pool of
prompts via a gradient-free approach and then constructs a large pool of weak
learners by pairing these prompts with different elements of the LM's output
distribution. These weak learners are then ensembled using the AdaBoost
algorithm. The entire learning process requires only a small number of forward
passes and no backward pass. Experiments show that PromptBoosting achieves
state-of-the-art performance in multiple black-box few-shot classification
tasks, and matches or outperforms full fine-tuning in both few-shot and
standard learning paradigms, while training 10x faster than existing black-box
methods.",1,1,0,0,1,0,0.58717,7.0,0.752582,51
4a582e62-05b0-43a2-93e7-86199c2aa459,Estimating Social Influence from Observational Data,3,0.00724489,0.172184,"We consider the problem of estimating social influence, the effect that a
person's behavior has on the future behavior of their peers. The key challenge
is that shared behavior between friends could be equally explained by influence
or by two other confounding factors: 1) latent traits that caused people to
both become friends and engage in the behavior, and 2) latent preferences for
the behavior. This paper addresses the challenges of estimating social
influence with three contributions. First, we formalize social influence as a
causal effect, one which requires inferences about hypothetical interventions.
Second, we develop Poisson Influence Factorization (PIF), a method for
estimating social influence from observational data. PIF fits probabilistic
factor models to networks and behavior data to infer variables that serve as
substitutes for the confounding latent traits. Third, we develop assumptions
under which PIF recovers estimates of social influence. We empirically study
PIF with semi-synthetic and real data from Last.fm, and conduct a sensitivity
analysis. We find that PIF estimates social influence most accurately compared
to related methods and remains robust under some violations of its assumptions.",1,0,0,0,0,0,0.00223549,13.0,0.406799,42
26d65a95-28ec-4194-b650-b7b1a9b50336,"A Simple, Yet Effective Approach to Finding Biases in Code Generation",5,0.0832761,0.129749,"Recently, high-performing code generation systems based on large language
models have surfaced. They are trained on massive corpora containing much more
natural text than actual executable computer code. This work shows that current
code generation systems exhibit undesired biases inherited from their large
language model backbones, which can reduce the quality of the generated code
under specific circumstances.
  To investigate the effect, we propose the ""block of influence"" concept, which
enables a modular decomposition and analysis of the coding challenges. We
introduce an automated intervention mechanism reminiscent of adversarial
testing that exposes undesired biases through the failure modes of the models
under test. Finally, we demonstrate how our framework can be used as a data
transformation technique during fine-tuning, acting as a mitigation strategy
for these biases.",0,0,0,0,0,0,0.95725,4.0,0.884681,53
52bbf16c-086f-4702-9625-5dec01403f58,Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking,19,0.535909,0.998752,"Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling
errors. Recent researches start from the pretrained knowledge of language
models and take multimodal information into CSC models to improve the
performance. However, they overlook the rich knowledge in the dictionary, the
reference book where one can learn how one character should be pronounced,
written, and used. In this paper, we propose the LEAD framework, which renders
the CSC model to learn heterogeneous knowledge from the dictionary in terms of
phonetics, vision, and meaning. LEAD first constructs positive and negative
samples according to the knowledge of character phonetics, glyphs, and
definitions in the dictionary. Then a unified contrastive learning-based
training scheme is employed to refine the representations of the CSC models.
Extensive experiments and detailed analyses on the SIGHAN benchmark datasets
demonstrate the effectiveness of our proposed methods.",1,1,0,0,0,0,0.920329,6.0,0.886454,39
df305a25-6145-419d-b079-627817c4bbce,Dual-Geometric Space Embedding Model for Two-View Knowledge Graphs,12,0.218048,0.692489,"Two-view knowledge graphs (KGs) jointly represent two components: an ontology
view for abstract and commonsense concepts, and an instance view for specific
entities that are instantiated from ontological concepts. As such, these KGs
contain heterogeneous structures that are hierarchical, from the ontology-view,
and cyclical, from the instance-view. Despite these various structures in KGs,
most recent works on embedding KGs assume that the entire KG belongs to only
one of the two views but not both simultaneously. For works that seek to put
both views of the KG together, the instance and ontology views are assumed to
belong to the same geometric space, such as all nodes embedded in the same
Euclidean space or non-Euclidean product space, an assumption no longer
reasonable for two-view KGs where different portions of the graph exhibit
different structures. To address this issue, we define and construct a
dual-geometric space embedding model (DGS) that models two-view KGs using a
complex non-Euclidean geometric space, by embedding different portions of the
KG in different geometric spaces. DGS utilizes the spherical space, hyperbolic
space, and their intersecting space in a unified framework for learning
embeddings. Furthermore, for the spherical space, we propose novel closed
spherical space operators that directly operate in the spherical space without
the need for mapping to an approximate tangent space. Experiments on public
datasets show that DGS significantly outperforms previous state-of-the-art
baseline models on KG completion tasks, demonstrating its ability to better
model heterogeneous structures in KGs.",1,0,0,0,1,0,0.757568,8.0,0.842391,32
6742bd4a-72d4-4865-a6e6-95fc4b97039f,DocEnTr: An End-to-End Document Image Enhancement Transformer,31,0.670265,0.795889,"Document images can be affected by many degradation scenarios, which cause
recognition and processing difficulties. In this age of digitization, it is
important to denoise them for proper usage. To address this challenge, we
present a new encoder-decoder architecture based on vision transformers to
enhance both machine-printed and handwritten document images, in an end-to-end
fashion. The encoder operates directly on the pixel patches with their
positional information without the use of any convolutional layers, while the
decoder reconstructs a clean image from the encoded patches. Conducted
experiments show a superiority of the proposed model compared to the state-of
the-art methods on several DIBCO benchmarks. Code and models will be publicly
available at: \url{https://github.com/dali92002/DocEnTR}.",1,1,0,0,1,0,0.751959,7.0,0.817551,44
7beb17cc-bdcd-41f2-b7a4-f954b863b409,Risk-aware Stochastic Shortest Path,7,0.203916,0.410512,"We treat the problem of risk-aware control for stochastic shortest path (SSP)
on Markov decision processes (MDP). Typically, expectation is considered for
SSP, which however is oblivious to the incurred risk. We present an alternative
view, instead optimizing conditional value-at-risk (CVaR), an established risk
measure. We treat both Markov chains as well as MDP and introduce, through
novel insights, two algorithms, based on linear programming and value
iteration, respectively. Both algorithms offer precise and provably correct
solutions. Evaluation of our prototype implementation shows that risk-aware
control is feasible on several moderately sized models.",0,0,0,0,0,0,0.16767,29.0,0.88604,42
54a62b35-00ae-4f07-8e2d-bf067140bcb0,On Improving Summarization Factual Consistency from Natural Language Feedback,22,0.4179,0.804394,"Despite the recent progress in language generation models, their outputs may
not always meet user expectations. In this work, we study whether informational
feedback in natural language can be leveraged to improve generation quality and
user preference alignment. To this end, we consider factual consistency in
summarization, the quality that the summary should only contain information
supported by the input documents, as the user-expected preference. We collect a
high-quality dataset, DeFacto, containing human demonstrations and
informational natural language feedback consisting of corrective instructions,
edited summaries, and explanations with respect to the factual consistency of
the summary. Using our dataset, we study three natural language generation
tasks: (1) editing a summary by following the human feedback, (2) generating
human feedback for editing the original summary, and (3) revising the initial
summary to correct factual errors by generating both the human feedback and
edited summary. We show that DeFacto can provide factually consistent
human-edited summaries and further insights into summarization factual
consistency thanks to its informational natural language feedback. We further
demonstrate that fine-tuned language models can leverage our dataset to improve
the summary factual consistency, while large language models lack the zero-shot
learning ability in our proposed tasks that require controllable text
generation.",0,1,1,1,0,0,0.887209,4.0,0.792726,69
3828431c-f7de-41df-a6a8-86ea62e284b5,Continual Predictive Learning from Videos,6,0.0166109,0.311971,"Predictive learning ideally builds the world model of physical processes in
one or more given environments. Typical setups assume that we can collect data
from all environments at all times. In practice, however, different prediction
tasks may arrive sequentially so that the environments may change persistently
throughout the training procedure. Can we develop predictive learning
algorithms that can deal with more realistic, non-stationary physical
environments? In this paper, we study a new continual learning problem in the
context of video prediction, and observe that most existing methods suffer from
severe catastrophic forgetting in this setup. To tackle this problem, we
propose the continual predictive learning (CPL) approach, which learns a
mixture world model via predictive experience replay and performs test-time
adaptation with non-parametric task inference. We construct two new benchmarks
based on RoboNet and KTH, in which different tasks correspond to different
physical robotic environments or human actions. Our approach is shown to
effectively mitigate forgetting and remarkably outperform the na\""ive
combinations of previous art in video prediction and continual learning.",0,0,1,0,1,0,0.222823,10.0,0.701258,50
5549a5ee-832f-49be-922d-985e0c3c507d,A Memory Transformer Network for Incremental Learning,12,0.0473467,0.679776,"We study class-incremental learning, a training setup in which new classes of
data are observed over time for the model to learn from. Despite the
straightforward problem formulation, the naive application of classification
models to class-incremental learning results in the ""catastrophic forgetting""
of previously seen classes. One of the most successful existing methods has
been the use of a memory of exemplars, which overcomes the issue of
catastrophic forgetting by saving a subset of past data into a memory bank and
utilizing it to prevent forgetting when training future tasks. In our paper, we
propose to enhance the utilization of this memory bank: we not only use it as a
source of additional training data like existing works but also integrate it in
the prediction process explicitly.Our method, the Memory Transformer Network
(MTN), learns how to combine and aggregate the information from the nearest
neighbors in the memory with a transformer to make more accurate predictions.
We conduct extensive experiments and ablations to evaluate our approach. We
show that MTN achieves state-of-the-art performance on the challenging
ImageNet-1k and Google-Landmarks-1k incremental learning benchmarks.",0,1,0,0,1,0,0.500203,7.0,0.717805,42
0b7ae3a8-cc36-4813-8e7d-e5a701c73614,longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks,8,0.0317409,0.457757,"Developing methods to adversarially challenge NLP systems is a promising
avenue for improving both model performance and interpretability. Here, we
describe the approach of the team ""longhorns"" on Task 1 of the The First
Workshop on Dynamic Adversarial Data Collection (DADC), which asked teams to
manually fool a model on an Extractive Question Answering task. Our team
finished first, with a model error rate of 62%. We advocate for a systematic,
linguistically informed approach to formulating adversarial questions, and we
describe the results of our pilot experiments, as well as our official
submission.",0,1,0,0,0,0,0.21684,7.0,0.568812,47
a672b220-b1d0-4057-b96e-b452106418da,Robust Anytime Learning of Markov Decision Processes,8,0.0745652,0.579167,"Markov decision processes (MDPs) are formal models commonly used in
sequential decision-making. MDPs capture the stochasticity that may arise, for
instance, from imprecise actuators via probabilities in the transition
function. However, in data-driven applications, deriving precise probabilities
from (limited) data introduces statistical errors that may lead to unexpected
or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise
probabilities but instead use so-called uncertainty sets in the transitions,
accounting for such limited data. Tools from the formal verification community
efficiently compute robust policies that provably adhere to formal
specifications, like safety constraints, under the worst-case instance in the
uncertainty set. We continuously learn the transition probabilities of an MDP
in a robust anytime-learning approach that combines a dedicated Bayesian
inference scheme with the computation of robust policies. In particular, our
method (1) approximates probabilities as intervals, (2) adapts to new data that
may be inconsistent with an intermediate model, and (3) may be stopped at any
time to compute a robust policy on the uMDP that faithfully captures the data
so far. Furthermore, our method is capable of adapting to changes in the
environment. We show the effectiveness of our approach and compare it to robust
policies computed on uMDPs learned by the UCRL2 reinforcement learning
algorithm in an experimental evaluation on several benchmarks.",1,1,0,0,0,0,0.0270193,17.0,0.693707,64
8d7096b5-dc96-484a-aed7-2cf6abd707c2,Topological Structure Learning for Weakly-Supervised Out-of-Distribution Detection,3,0.151631,0.156674,"Out-of-distribution (OOD) detection is the key to deploying models safely in
the open world. For OOD detection, collecting sufficient in-distribution (ID)
labeled data is usually more time-consuming and costly than unlabeled data.
When ID labeled data is limited, the previous OOD detection methods are no
longer superior due to their high dependence on the amount of ID labeled data.
Based on limited ID labeled data and sufficient unlabeled data, we define a new
setting called Weakly-Supervised Out-of-Distribution Detection (WSOOD). To
solve the new problem, we propose an effective method called Topological
Structure Learning (TSL). Firstly, TSL uses a contrastive learning method to
build the initial topological structure space for ID and OOD data. Secondly,
TSL mines effective topological connections in the initial topological space.
Finally, based on limited ID labeled data and mined topological connections,
TSL reconstructs the topological structure in a new topological space to
increase the separability of ID and OOD instances. Extensive studies on several
representative datasets show that TSL remarkably outperforms the
state-of-the-art, verifying the validity and robustness of our method in the
new setting of WSOOD.",0,0,1,0,1,0,0.86472,6.0,0.847319,59
b4e37f5e-e6ea-4e3a-9cb9-0e4542ee85ca,A2: Efficient Automated Attacker for Boosting Adversarial Training,9,0.0183417,0.448265,"Based on the significant improvement of model robustness by AT (Adversarial
Training), various variants have been proposed to further boost the
performance. Well-recognized methods have focused on different components of AT
(e.g., designing loss functions and leveraging additional unlabeled data). It
is generally accepted that stronger perturbations yield more robust models.
However, how to generate stronger perturbations efficiently is still missed. In
this paper, we propose an efficient automated attacker called A2 to boost AT by
generating the optimal perturbations on-the-fly during training. A2 is a
parameterized automated attacker to search in the attacker space for the best
attacker against the defense model and examples. Extensive experiments across
different datasets demonstrate that A2 generates stronger perturbations with
low extra cost and reliably improves the robustness of various AT methods
against different attacks.",1,1,0,0,0,0,0.64091,11.0,0.85586,41
83ed3f04-4d21-42fd-a13d-325f769c1c27,Continual Spatio-Temporal Graph Convolutional Networks,6,0.095428,0.472025,"Graph-based reasoning over skeleton data has emerged as a promising approach
for human action recognition. However, the application of prior graph-based
methods, which predominantly employ whole temporal sequences as their input, to
the setting of online inference entails considerable computational redundancy.
In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph
Convolutional Neural Network as a Continual Inference Network, which can
perform step-by-step predictions in time without repeat frame processing. To
evaluate our method, we create a continual version of ST-GCN, CoST-GCN,
alongside two derived methods with different self-attention mechanisms, CoAGCN
and CoS-TR. We investigate weight transfer strategies and architectural
modifications for inference acceleration, and perform experiments on the NTU
RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar
predictive accuracy, we observe up to 109x reduction in time complexity,
on-hardware accelerations of 26x, and reductions in maximum allocated memory of
52% during online inference.",1,1,0,0,0,0,0.733849,6.0,0.778498,38
161d002b-68a2-48f8-a24c-746869efaf47,Matching Tweets With Applicable Fact-Checks Across Languages,11,0.167775,0.362923,"An important challenge for news fact-checking is the effective dissemination
of existing fact-checks. This in turn brings the need for reliable methods to
detect previously fact-checked claims. In this paper, we focus on automatically
finding existing fact-checks for claims made in social media posts (tweets). We
conduct both classification and retrieval experiments, in monolingual (English
only), multilingual (Spanish, Portuguese), and cross-lingual (Hindi-English)
settings using multilingual transformer models such as XLM-RoBERTa and
multilingual embeddings such as LaBSE and SBERT. We present promising results
for ""match"" classification (86% average accuracy) in four language pairs. We
also find that a BM25 baseline outperforms or is on par with state-of-the-art
multilingual embedding models for the retrieval task during our monolingual
experiments. We highlight and discuss NLP challenges while addressing this
problem in different languages, and we introduce a novel curated dataset of
fact-checks and corresponding tweets for future research.",0,1,0,1,0,0,0.900457,4.0,0.806645,31
c226ea3b-9302-493f-becc-ac139eea0174,Learning Object Placement via Dual-path Graph Completion,12,0.185563,0.213493,"Object placement aims to place a foreground object over a background image
with a suitable location and size. In this work, we treat object placement as a
graph completion problem and propose a novel graph completion module (GCM). The
background scene is represented by a graph with multiple nodes at different
spatial locations with various receptive fields. The foreground object is
encoded as a special node that should be inserted at a reasonable place in this
graph. We also design a dual-path framework upon the structure of GCM to fully
exploit annotated composite images. With extensive experiments on OPA dataset,
our method proves to significantly outperform existing methods in generating
plausible object placement without loss of diversity.",1,0,0,0,0,0,0.595717,9.0,0.810161,44
42e9aa9b-f1ed-432a-846e-c3c142786a71,Event knowledge in large language models: the gap between the impossible and the unlikely,24,0.187273,0.927765,"Word co-occurrence patterns in language corpora contain a surprising amount
of conceptual knowledge. Large language models (LLMs), trained to predict words
in context, leverage these patterns to achieve impressive performance on
diverse semantic tasks requiring world knowledge. An important but understudied
question about LLMs' semantic abilities is whether they acquire generalized
knowledge of common events. Here, we test whether five pre-trained LLMs (from
2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions
of agent-patient interactions than to minimally different implausible versions
of the same event. Using three curated sets of minimal sentence pairs (total
n=1,215), we found that pre-trained LLMs possess substantial event knowledge,
outperforming other distributional language models. In particular, they almost
always assign higher likelihood to possible vs. impossible events (The teacher
bought the laptop vs. The laptop bought the teacher). However, LLMs show less
consistent preferences for likely vs. unlikely events (The nanny tutored the
boy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM
scores are driven by both plausibility and surface-level sentence features,
(ii) LLM scores generalize well across syntactic variants (active vs. passive
constructions) but less well across semantic variants (synonymous sentences),
(iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence
plausibility serves as an organizing dimension in internal LLM representations.
Overall, our results show that important aspects of event knowledge naturally
emerge from distributional linguistic patterns, but also highlight a gap
between representations of possible/impossible and likely/unlikely events.",0,0,0,0,0,1,0.147097,8.0,0.569049,204
71975146-ffea-4efc-bff5-7a3cf65e493f,Subsampling for Knowledge Graph Embedding Explained,1,0.0186698,0.125139,"In this article, we explain the recent advance of subsampling methods in
knowledge graph embedding (KGE) starting from the original one used in
word2vec.",0,1,0,0,0,0,0.271579,13.0,0.787794,12
6bad2ebc-7182-4d87-b0b0-33650e1d7892,Swapping Semantic Contents for Mixing Images,2,0.0240226,0.039898,"Deep architecture have proven capable of solving many tasks provided a
sufficient amount of labeled data. In fact, the amount of available labeled
data has become the principal bottleneck in low label settings such as
Semi-Supervised Learning. Mixing Data Augmentations do not typically yield new
labeled samples, as indiscriminately mixing contents creates between-class
samples. In this work, we introduce the SciMix framework that can learn to
generator to embed a semantic style code into image backgrounds, we obtain new
mixing scheme for data augmentation. We then demonstrate that SciMix yields
novel mixed samples that inherit many characteristics from their non-semantic
parents. Afterwards, we verify those samples can be used to improve the
performance semi-supervised frameworks like Mean Teacher or Fixmatch, and even
fully supervised learning on a small labeled dataset.",0,1,0,0,0,0,0.948356,9.0,0.941877,40
a35ab0ec-72c5-456c-885c-6b1303f65c55,Graph-Text Multi-Modal Pre-training for Medical Representation Learning,8,0.208928,0.35139,"As the volume of Electronic Health Records (EHR) sharply grows, there has
been emerging interest in learning the representation of EHR for healthcare
applications. Representation learning of EHR requires appropriate modeling of
the two dominant modalities in EHR: structured data and unstructured text. In
this paper, we present MedGTX, a pre-trained model for multi-modal
representation learning of the structured and textual EHR data. MedGTX uses a
novel graph encoder to exploit the graphical nature of structured EHR data, and
a text encoder to handle unstructured text, and a cross-modal encoder to learn
a joint representation space. We pre-train our model through four proxy tasks
on MIMIC-III, an open-source EHR data, and evaluate our model on two clinical
benchmarks and three novel downstream tasks which tackle real-world problems in
EHR data. The results consistently show the effectiveness of pre-training the
model for joint representation of both structured and unstructured information
from EHR. Given the promising performance of MedGTX, we believe this work opens
a new door to jointly understanding the two fundamental modalities of EHR data.",1,1,0,0,0,0,0.90505,7.0,0.892407,51
09a87fe8-afff-4621-9fcb-0031b9cbfd43,End-to-End 3D Hand Pose Estimation from Stereo Cameras,11,0.162834,0.692917,"This work proposes an end-to-end approach to estimate full 3D hand pose from
stereo cameras. Most existing methods of estimating hand pose from stereo
cameras apply stereo matching to obtain depth map and use depth-based solution
to estimate hand pose. In contrast, we propose to bypass the stereo matching
and directly estimate the 3D hand pose from the stereo image pairs. The
proposed neural network architecture extends from any keypoint predictor to
estimate the sparse disparity of the hand joints. In order to effectively train
the model, we propose a large scale synthetic dataset that is composed of
stereo image pairs and ground truth 3D hand pose annotations. Experiments show
that the proposed approach outperforms the existing methods based on the stereo
depth.",0,1,0,1,0,0,0.786719,11.0,0.893243,35
5fc4b9cd-02b0-44db-90dc-0481a2312e14,Regularizing Neural Network Training via Identity-wise Discriminative Feature Suppression,1,0.0248225,0.0264514,"It is well-known that a deep neural network has a strong fitting capability
and can easily achieve a low training error even with randomly assigned class
labels. When the number of training samples is small, or the class labels are
noisy, networks tend to memorize patterns specific to individual instances to
minimize the training error. This leads to the issue of overfitting and poor
generalisation performance. This paper explores a remedy by suppressing the
network's tendency to rely on instance-specific patterns for empirical error
minimisation. The proposed method is based on an adversarial training
framework. It suppresses features that can be utilized to identify individual
instances among samples within each class. This leads to classifiers only using
features that are both discriminative across classes and common within each
class. We call our method Adversarial Suppression of Identity Features (ASIF),
and demonstrate the usefulness of this technique in boosting generalisation
accuracy when faced with small datasets or noisy labels. Our source code is
available.",0,0,0,0,0,1,0.883218,12.0,0.92957,24
728794c5-14b3-4957-9aa1-5cacef11c4d7,Clickbait Spoiling via Question Answering and Passage Retrieval,31,0.548168,0.998871,"We introduce and study the task of clickbait spoiling: generating a short
text that satisfies the curiosity induced by a clickbait post. Clickbait links
to a web page and advertises its contents by arousing curiosity instead of
providing an informative summary. Our contributions are approaches to classify
the type of spoiler needed (i.e., a phrase or a passage), and to generate
appropriate spoilers. A large-scale evaluation and error analysis on a new
corpus of 5,000 manually spoiled clickbait posts -- the Webis Clickbait
Spoiling Corpus 2022 -- shows that our spoiler type classifier achieves an
accuracy of 80%, while the question answering model DeBERTa-large outperforms
all others in generating spoilers for both types.",0,1,1,1,0,0,0.913897,7.0,0.898222,48
60134d1c-ce16-4809-8cd6-2570ddb87df8,Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping,4,0.0337588,0.0829001,"Fine-tuning over large pretrained language models (PLMs) has established many
state-of-the-art results. Despite its superior performance, such fine-tuning
can be unstable, resulting in significant variance in performance and potential
risks for practical applications. Previous works have attributed such
instability to the catastrophic forgetting problem in the top layers of PLMs,
which indicates iteratively that fine-tuning layers in a top-down manner is a
promising solution. In this paper, we first point out that this method does not
always work out due to the different convergence speeds of different
layers/modules. Inspired by this observation, we propose a simple
component-wise gradient norm clipping method to adjust the convergence speed
for different components. Experiment results demonstrate that our method
achieves consistent improvements in terms of generalization performance,
convergence speed, and training stability. The codebase can be found at
https://github.com/yangalan123/FineTuningStability.",1,1,0,0,0,0,0.976502,7.0,0.958935,11
c88cf6a4-93c3-4750-ae5e-7ecbc51de368,Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization,16,0.0710453,0.775762,"Neural architecture search (NAS) and network pruning are widely studied
efficient AI techniques, but not yet perfect. NAS performs exhaustive candidate
architecture search, incurring tremendous search cost. Though (structured)
pruning can simply shrink model dimension, it remains unclear how to decide the
per-layer sparsity automatically and optimally. In this work, we revisit the
problem of layer-width optimization and propose Pruning-as-Search (PaS), an
end-to-end channel pruning method to search out desired sub-network
automatically and efficiently. Specifically, we add a depth-wise binary
convolution to learn pruning policies directly through gradient descent. By
combining the structural reparameterization and PaS, we successfully searched
out a new family of VGG-like and lightweight networks, which enable the
flexibility of arbitrary width with respect to each layer instead of each
stage. Experimental results show that our proposed architecture outperforms
prior arts by around $1.0\%$ top-1 accuracy under similar inference speed on
ImageNet-1000 classification task. Furthermore, we demonstrate the
effectiveness of our width search on complex tasks including instance
segmentation and image translation. Code and models are released.",0,1,0,0,1,0,0.658795,9.0,0.829237,20
3e6f33d4-d399-4963-afaa-a429b195cc11,On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,5,0.0426657,0.465233,"Most works on modeling the conversation history in Conversational Question
Answering (CQA) report a single main result on a common CQA benchmark. While
existing models show impressive results on CQA leaderboards, it remains unclear
whether they are robust to shifts in setting (sometimes to more realistic
ones), training data size (e.g. from large to small sets) and domain. In this
work, we design and conduct the first large-scale robustness study of history
modeling approaches for CQA. We find that high benchmark scores do not
necessarily translate to strong robustness, and that various methods can
perform extremely differently under different settings. Equipped with the
insights from our study, we design a novel prompt-based history modeling
approach, and demonstrate its strong robustness across various settings. Our
approach is inspired by existing methods that highlight historic answers in the
passage. However, instead of highlighting by modifying the passage token
embeddings, we add textual prompts directly in the passage text. Our approach
is simple, easy-to-plug into practically any model, and highly effective, thus
we recommend it as a starting point for future model developers. We also hope
that our study and insights will raise awareness to the importance of
robustness-focused evaluation, in addition to obtaining high leaderboard
scores, leading to better CQA systems.",1,1,0,0,0,0,0.572022,6.0,0.704416,44
94ba2c87-9d76-4df1-a67a-e8f013b27628,Accurate Polygonal Mapping of Buildings in Satellite Imagery,10,0.225475,0.454484,"This paper studies the problem of polygonal mapping of buildings by tackling
the issue of mask reversibility that leads to a notable performance gap between
the predicted masks and polygons from the learning-based methods. We addressed
such an issue by exploiting the hierarchical supervision (of bottom-level
vertices, mid-level line segments and the high-level regional masks) and
proposed a novel interaction mechanism of feature embedding sourced from
different levels of supervision signals to obtain reversible building masks for
polygonal mapping of buildings. As a result, we show that the learned
reversible building masks take all the merits of the advances of deep
convolutional neural networks for high-performing polygonal mapping of
buildings. In the experiments, we evaluated our method on the two public
benchmarks of AICrowd and Inria. On the AICrowd dataset, our proposed method
obtains unanimous improvements on the metrics of AP, APboundary and PoLiS. For
the Inria dataset, our proposed method also obtains very competitive results on
the metrics of IoU and Accuracy. The models and source code are available at
https://github.com/SarahwXU.",1,1,0,0,0,0,0.667467,9.0,0.831866,57
1d16f599-f576-43ca-a036-e612dc2234d8,"Computational Inference in Cognitive Science: Operational, Societal and Ethical Considerations",8,0.118483,0.388046,"Emerging research frontiers and computational advances have gradually
transformed cognitive science into a multidisciplinary and data-driven field.
As a result, there is a proliferation of cognitive theories investigated and
interpreted from different academic lens and in different levels of
abstraction. We formulate this applied aspect of this challenge as the
computational cognitive inference, and describe the major routes of
computational approaches. To balance the potential optimism alongside the speed
and scale of the data-driven era of cognitive science, we propose to inspect
this trend in more empirical terms by identifying the operational challenges,
societal impacts and ethical guidelines in conducting research and interpreting
results from the computational inference in cognitive science.",0,0,0,0,0,0,0.308834,10.0,0.739462,48
6b78f405-1d4a-467a-a02f-0727ca735272,CORL: Research-oriented Deep Offline Reinforcement Learning Library,32,0.0645679,0.652544,"CORL is an open-source library that provides thoroughly benchmarked
single-file implementations of both deep offline and offline-to-online
reinforcement learning algorithms. It emphasizes a simple developing experience
with a straightforward codebase and a modern analysis tracking tool. In CORL,
we isolate methods implementation into separate single files, making
performance-relevant details easier to recognize. Additionally, an experiment
tracking feature is available to help log metrics, hyperparameters,
dependencies, and more to the cloud. Finally, we have ensured the reliability
of the implementations by benchmarking commonly employed D4RL datasets
providing a transparent source of results that can be reused for robust
evaluation tools such as performance profiles, probability of improvement, or
expected online performance.",1,1,0,0,0,0,0.197828,5.0,0.375679,43
e5bd22e8-57ae-47a2-a71a-488a90ee2c9c,DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization,8,0.115484,0.525208,"LiDAR mapping is important yet challenging in self-driving and mobile
robotics. To tackle such a global point cloud registration problem, DeepMapping
converts the complex map estimation into a self-supervised training of simple
deep networks. Despite its broad convergence range on small datasets,
DeepMapping still cannot produce satisfactory results on large-scale datasets
with thousands of frames. This is due to the lack of loop closures and exact
cross-frame point correspondences, and the slow convergence of its global
localization network. We propose DeepMapping2 by adding two novel techniques to
address these issues: (1) organization of training batch based on map topology
from loop closing, and (2) self-supervised local-to-global point consistency
loss leveraging pairwise registration. Our experiments and ablation studies on
public datasets (KITTI, NCLT, and Nebula) demonstrate the effectiveness of our
method.",0,1,0,0,1,0,0.761334,10.0,0.875012,63
b0c1c895-d59b-46a2-93bd-4538abd6f4ab,"A Distant Supervision Corpus for Extracting Biomedical Relationships Between Chemicals, Diseases and Genes",5,0.140912,0.451174,"We introduce ChemDisGene, a new dataset for training and evaluating
multi-class multi-label document-level biomedical relation extraction models.
Our dataset contains 80k biomedical research abstracts labeled with mentions of
chemicals, diseases, and genes, portions of which human experts labeled with 18
types of biomedical relationships between these entities (intended for
evaluation), and the remainder of which (intended for training) has been
distantly labeled via the CTD database with approximately 78\% accuracy. In
comparison to similar preexisting datasets, ours is both substantially larger
and cleaner; it also includes annotations linking mentions to their entities.
We also provide three baseline deep neural network relation extraction models
trained and evaluated on our new dataset.",0,1,0,1,0,0,0.770799,7.0,0.825424,41
b63f4ba4-b7f4-4d59-9079-5633570b8c65,Mitigating Artifacts in Real-World Video Super-Resolution Models,10,0.165653,0.568923,"The recurrent structure is a prevalent framework for the task of video
super-resolution, which models the temporal dependency between frames via
hidden states. When applied to real-world scenarios with unknown and complex
degradations, hidden states tend to contain unpleasant artifacts and propagate
them to restored frames. In this circumstance, our analyses show that such
artifacts can be largely alleviated when the hidden state is replaced with a
cleaner counterpart. Based on the observations, we propose a Hidden State
Attention (HSA) module to mitigate artifacts in real-world video
super-resolution. Specifically, we first adopt various cheap filters to produce
a hidden state pool. For example, Gaussian blur filters are for smoothing
artifacts while sharpening filters are for enhancing details. To aggregate a
new hidden state that contains fewer artifacts from the hidden state pool, we
devise a Selective Cross Attention (SCA) module, in which the attention between
input features and each hidden state is calculated. Equipped with HSA, our
proposed method, namely FastRealVSR, is able to achieve 2x speedup while
obtaining better performance than Real-BasicVSR. Codes will be available at
https://github.com/TencentARC/FastRealVSR",1,1,0,0,0,0,0.903542,7.0,0.891448,43
d6f2667d-bc5b-43c3-b949-d138aab1d8bf,NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,65,0.743995,0.849796,"Given the ubiquitous nature of numbers in text, reasoning with numbers to
perform simple calculations is an important skill of AI systems. While many
datasets and models have been developed to this end, state-of-the-art AI
systems are brittle; failing to perform the underlying mathematical reasoning
when they appear in a slightly different scenario. Drawing inspiration from
GLUE that was proposed in the context of natural language understanding, we
propose NumGLUE, a multi-task benchmark that evaluates the performance of AI
systems on eight different tasks, that at their core require simple arithmetic
understanding. We show that this benchmark is far from being solved with neural
models including state-of-the-art large-scale language models performing
significantly worse than humans (lower by 46.4%). Further, NumGLUE promotes
sharing knowledge across tasks, especially those with limited training data as
evidenced by the superior performance (average gain of 3.4% on each task) when
a model is jointly trained on all the tasks as opposed to task-specific
modeling. Finally, we hope that NumGLUE will encourage systems that perform
robust and general arithmetic reasoning within language, a first step towards
being able to perform more complex mathematical reasoning.",0,1,0,1,0,0,0.768205,7.0,0.824329,40
bbcd76ea-75c2-491f-8c99-0cc6a214d661,Proper Reuse of Image Classification Features Improves Object Detection,22,0.181826,0.391092,"A common practice in transfer learning is to initialize the downstream model
weights by pre-training on a data-abundant upstream task. In object detection
specifically, the feature backbone is typically initialized with Imagenet
classifier weights and fine-tuned on the object detection task. Recent works
show this is not strictly necessary under longer training regimes and provide
recipes for training the backbone from scratch. We investigate the opposite
direction of this end-to-end training trend: we show that an extreme form of
knowledge preservation -- freezing the classifier-initialized backbone --
consistently improves many different detection models, and leads to
considerable resource savings. We hypothesize and corroborate experimentally
that the remaining detector components capacity and structure is a crucial
factor in leveraging the frozen backbone. Immediate applications of our
findings include performance improvements on hard cases like detection of
long-tail object classes and computational and memory resource savings that
contribute to making the field more accessible to researchers with access to
fewer computational resources.",0,1,0,0,0,0,0.659621,8.0,0.808173,48
2e6741c2-b9b4-4c47-8d62-3cbd627cf4b3,MotionBERT: A Unified Perspective on Learning Human Motion Representations,29,0.535356,0.809985,"We present a unified perspective on tackling various human-centric video
tasks by learning human motion representations from large-scale and
heterogeneous data resources. Specifically, we propose a pretraining stage in
which a motion encoder is trained to recover the underlying 3D motion from
noisy partial 2D observations. The motion representations acquired in this way
incorporate geometric, kinematic, and physical knowledge about human motion,
which can be easily transferred to multiple downstream tasks. We implement the
motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)
neural network. It could capture long-range spatio-temporal relationships among
the skeletal joints comprehensively and adaptively, exemplified by the lowest
3D pose estimation error so far when trained from scratch. Furthermore, our
proposed framework achieves state-of-the-art performance on all three
downstream tasks by simply finetuning the pretrained motion encoder with a
simple regression head (1-2 layers), which demonstrates the versatility of the
learned motion representations. Code and models are available at
https://motionbert.github.io/",1,1,0,0,1,0,0.866847,6.0,0.848634,145
a45b2ff4-cb1d-41a5-8ca4-cac8d79da30d,"Unsupervised Learning of Depth, Camera Pose and Optical Flow from Monocular Video",1,0.00117547,0.0406232,"We propose DFPNet -- an unsupervised, joint learning system for monocular
Depth, Optical Flow and egomotion (Camera Pose) estimation from monocular image
sequences. Due to the nature of 3D scene geometry these three components are
coupled. We leverage this fact to jointly train all the three components in an
end-to-end manner. A single composite loss function -- which involves image
reconstruction-based loss for depth & optical flow, bidirectional consistency
checks and smoothness loss components -- is used to train the network. Using
hyperparameter tuning, we are able to reduce the model size to less than 5%
(8.4M parameters) of state-of-the-art DFP models. Evaluation on KITTI and
Cityscapes driving datasets reveals that our model achieves results comparable
to state-of-the-art in all of the three tasks, even with the significantly
smaller model size.",0,1,0,0,1,0,0.0391494,11.0,0.560918,37
2ed8f7ac-b0e0-4d21-b788-20fb1d73be6e,Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models,2,0.0300338,0.120566,"We perform knowledge distillation (KD) benchmark from task-specific BERT-base
teacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini,
and BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text
classification and sequence labeling in the Indonesian language. We also
compare various aspects of distillations including the usage of word embeddings
and unlabeled data augmentation. Our experiments show that, despite the rising
popularity of Transformer-based models, using BiLSTM and CNN student models
provide the best trade-off between performance and computational resource (CPU,
RAM, and storage) compared to pruned BERT models. We further propose some quick
wins on performing KD to produce small NLP models via efficient KD training
mechanisms involving simple choices of loss functions, word embeddings, and
unlabeled data preparation.",0,1,0,0,0,0,0.795915,6.0,0.808967,58
272c5fd1-66e4-435b-a064-a4f1d6ad694a,Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series,1,0.108114,0.0702589,"Deep Learning and transfer learning models are being used to generate time
series forecasts; however, there is scarce evidence about their performance
prediction that it is more evident for monthly time series. The purpose of this
paper is to compare Deep Learning models with transfer learning and without
transfer learning and other traditional methods used for monthly forecasts to
answer three questions about the suitability of Deep Learning and Transfer
Learning to generate predictions of time series. Time series of M4 and M3
competitions were used for the experiments. The results suggest that deep
learning models based on TCN, LSTM, and CNN with transfer learning tend to
surpass the performance prediction of other traditional methods. On the other
hand, TCN and LSTM, trained directly on the target time series, got similar or
better performance than traditional methods for some forecast horizons.",1,1,0,0,0,0,0.647752,5.0,0.686613,45
caf63646-54e7-407b-a8f3-c5df257fd7d1,Emergent Communication for Understanding Human Language Evolution: What's Missing?,14,0.197732,0.942802,"Emergent communication protocols among humans and artificial neural network
agents do not yet share the same properties and show some critical mismatches
in results. We describe three important phenomena with respect to the emergence
and benefits of compositionality: ease-of-learning, generalization, and group
size effects (i.e., larger groups create more systematic languages). The latter
two are not fully replicated with neural agents, which hinders the use of
neural emergent communication for language evolution research. We argue that
one possible reason for these mismatches is that key cognitive and
communicative constraints of humans are not yet integrated. Specifically, in
humans, memory constraints and the alternation between the roles of speaker and
listener underlie the emergence of linguistic structure, yet these constraints
are typically absent in neural simulations. We suggest that introducing such
communicative and cognitive constraints would promote more linguistically
plausible behaviors with neural agents.",0,0,0,0,0,0,0.148654,11.0,0.68762,52
c3a7f587-5d09-45a1-a896-1923001ee4a4,Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,9,0.144887,0.305748,"This paper studies multi-task training of retrieval-augmented generation
models for knowledge-intensive tasks. We propose to clean the training set by
utilizing a distinct property of knowledge-intensive generation: The connection
of query-answer pairs to items in the knowledge base. We filter training
examples via a threshold of confidence on the relevance labels, whether a pair
is answerable by the knowledge base or not. We train a single Fusion-in-Decoder
(FiD) generator on seven combined tasks of the KILT benchmark. The experimental
results suggest that our simple yet effective approach substantially improves
competitive baselines on two strongly imbalanced tasks; and shows either
smaller improvements or no significant regression on the remaining tasks.
Furthermore, we demonstrate our multi-task training with relevance label
sampling scales well with increased model capacity and achieves
state-of-the-art results in five out of seven KILT tasks.",0,1,0,0,1,0,0.877762,5.0,0.82667,34
d9effd32-ad8e-4441-ac9c-853bbefe9a0b,Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study,25,0.256633,0.957513,"In this work, we present an extensive study on the use of pre-trained
language models for the task of automatic Counter Narrative (CN) generation to
fight online hate speech in English. We first present a comparative study to
determine whether there is a particular Language Model (or class of LMs) and a
particular decoding mechanism that are the most appropriate to generate CNs.
Findings show that autoregressive models combined with stochastic decodings are
the most promising. We then investigate how an LM performs in generating a CN
with regard to an unseen target of hate. We find out that a key element for
successful `out of target' experiments is not an overall similarity with the
training data but the presence of a specific subset of training data, i.e. a
target that shares some commonalities with the test target that can be defined
a-priori. We finally introduce the idea of a pipeline based on the addition of
an automatic post-editing step to refine generated CNs.",0,1,0,0,0,0,0.642273,7.0,0.774023,64
51adfcd5-cd09-4cc6-bd29-79374aa7a529,Explaining Chest X-ray Pathologies in Natural Language,15,0.0352445,0.431648,"Most deep learning algorithms lack explanations for their predictions, which
limits their deployment in clinical practice. Approaches to improve
explainability, especially in medical imaging, have often been shown to convey
limited information, be overly reassuring, or lack robustness. In this work, we
introduce the task of generating natural language explanations (NLEs) to
justify predictions made on medical images. NLEs are human-friendly and
comprehensive, and enable the training of intrinsically explainable models. To
this goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging
dataset with NLEs. It contains over 38,000 NLEs, which explain the presence of
various thoracic pathologies and chest X-ray findings. We propose a general
approach to solve the task and evaluate several architectures on this dataset,
including via clinician assessment.",1,0,1,1,0,0,0.145513,7.0,0.50581,40
f3ea3b0c-b346-45cc-9b64-57492aad986f,Attentive Task Interaction Network for Multi-Task Learning,1,0.00455575,0.0207436,"Multitask learning (MTL) has recently gained a lot of popularity as a
learning paradigm that can lead to improved per-task performance while also
using fewer per-task model parameters compared to single task learning. One of
the biggest challenges regarding MTL networks involves how to share features
across tasks. To address this challenge, we propose the Attentive Task
Interaction Network (ATI-Net). ATI-Net employs knowledge distillation of the
latent features for each task, then combines the feature maps to provide
improved contextualized information to the decoder. This novel approach to
introducing knowledge distillation into an attention based multitask network
outperforms state of the art MTL baselines such as the standalone MTAN and
PAD-Net, with roughly the same number of model parameters.",1,1,0,0,0,0,0.44934,10.0,0.787412,22
e49a0361-7074-45a9-99ae-0a33011170a7,Geometric Digital Twinning of Industrial Facilities: Retrieval of Industrial Shapes,4,0.0313928,0.132624,"This paper devises, implements and benchmarks a novel shape retrieval method
that can accurately match individual labelled point clusters (instances) of
existing industrial facilities with their respective CAD models. It employs a
combination of image and point cloud deep learning networks to classify and
match instances to their geometrically similar CAD model. It extends our
previous research on geometric digital twin generation from point cloud data,
which currently is a tedious, manual process. Experiments with our joint
network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The
proposed research is a fundamental framework to enable the geometric Digital
Twin (gDT) pipeline and incorporate the real geometric configuration into the
Digital Twin.",0,1,0,0,0,0,0.0190603,15.0,0.629335,130
4cab9935-bc3e-4ddc-99c1-b006c46c0e35,Adapting BigScience Multilingual Model to Unseen Languages,5,0.042731,0.2418,"We benchmark different strategies of adding new languages (German and Korean)
into the BigScience's pretrained multilingual language model with 1.3 billion
parameters that currently supports 13 languages. We investigate the factors
that affect the language adaptability of the model and the trade-offs between
computational costs and expected performance.",0,1,0,0,0,0,0.744604,4.0,0.675418,32
6d223789-417d-4c47-b935-231bc5117eab,Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation,3,0.0783527,0.136694,"We study video-grounded dialogue generation, where a response is generated
based on the dialogue context and the associated video. The primary challenges
of this task lie in (1) the difficulty of integrating video data into
pre-trained language models (PLMs) which presents obstacles to exploiting the
power of large-scale pre-training; and (2) the necessity of taking into account
the complementarity of various modalities throughout the reasoning process.
Although having made remarkable progress in video-grounded dialogue generation,
existing methods still fall short when it comes to integrating with PLMs in a
way that allows information from different modalities to complement each other.
To alleviate these issues, we first propose extracting pertinent information
from videos and turning it into reasoning paths that are acceptable to PLMs.
Additionally, we propose a multi-agent reinforcement learning method to
collaboratively perform reasoning on different modalities (i.e., video and
dialogue context). Empirical experiment results on two public datasets indicate
that the proposed model can significantly outperform state-of-the-art models by
large margins on both automatic and human evaluations.",0,0,0,0,1,0,0.602698,9.0,0.812278,49
409e164c-e8ac-4e85-ae22-2c68e0b752a4,Infrared and visible image fusion based on Multi-State Contextual Hidden Markov Model,5,0.020331,0.155693,"The traditional two-state hidden Markov model divides the high frequency
coefficients only into two states (large and small states). Such scheme is
prone to produce an inaccurate statistical model for the high frequency subband
and reduces the quality of fusion result. In this paper, a fine-grained
multi-state contextual hidden Markov model (MCHMM) is proposed for infrared and
visible image fusion in the non-subsampled Shearlet domain, which takes full
consideration of the strong correlations and level of details of NSST
coefficients. To this end, an accurate soft context variable is designed
correspondingly from the perspective of context correlation. Then, the
statistical features provided by MCHMM are utilized for the fusion of high
frequency subbands. To ensure the visual quality, a fusion strategy based on
the difference in regional energy is proposed as well for lowfrequency
subbands. Experimental results demonstrate that the proposed method can achieve
a superior performance compared with other fusion methods in both subjective
and objective aspects.",0,1,0,0,0,0,0.00822097,13.0,0.507201,37
e4ef1efc-4a5b-4f14-baee-b1bec5f0efb5,Filler Word Detection and Classification: A Dataset and Benchmark,5,0.0659652,0.555862,"Filler words such as `uh' or `um' are sounds or words people use to signal
they are pausing to think. Finding and removing filler words from recordings is
a common and tedious task in media editing. Automatically detecting and
classifying filler words could greatly aid in this task, but few studies have
been published on this problem to date. A key reason is the absence of a
dataset with annotated filler words for model training and evaluation. In this
work, we present a novel speech dataset, PodcastFillers, with 35K annotated
filler words and 50K annotations of other sounds that commonly occur in
podcasts such as breaths, laughter, and word repetitions. We propose a pipeline
that leverages VAD and ASR to detect filler candidates and a classifier to
distinguish between filler word types. We evaluate our proposed pipeline on
PodcastFillers, compare to several baselines, and present a detailed ablation
study. In particular, we evaluate the importance of using ASR and how it
compares to a transcription-free approach resembling keyword spotting. We show
that our pipeline obtains state-of-the-art results, and that leveraging ASR
strongly outperforms a keyword spotting approach. We make PodcastFillers
publicly available, in the hope that our work serves as a benchmark for future
research.",0,1,1,1,1,0,0.0879365,10.0,0.600509,33
3a3120fc-4127-487b-9d08-de8db351ae86,Self-Supervised Pre-training of Vision Transformers for Dense Prediction Tasks,2,0.024642,0.0609561,"We present a new self-supervised pre-training of Vision Transformers for
dense prediction tasks. It is based on a contrastive loss across views that
compares pixel-level representations to global image representations. This
strategy produces better local features suitable for dense prediction tasks as
opposed to contrastive pre-training based on global image representation only.
Furthermore, our approach does not suffer from a reduced batch size since the
number of negative examples needed in the contrastive loss is in the order of
the number of local features. We demonstrate the effectiveness of our
pre-training strategy on two dense prediction tasks: semantic segmentation and
monocular depth estimation.",0,1,0,0,0,0,0.989354,4.0,0.976011,34
689f9cca-effa-49a2-9a9e-b0f7a70b4dbe,Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models,29,0.105866,0.754409,"Recently the prompt-tuning paradigm has attracted significant attention. By
only tuning continuous prompts with a frozen pre-trained language model (PLM),
prompt-tuning takes a step towards deploying a shared frozen PLM to serve
numerous downstream tasks. Although prompt-tuning shows good performance on
certain natural language understanding (NLU) tasks, its effectiveness on
natural language generation (NLG) tasks is still under-explored. In this paper,
we argue that one of the factors hindering the development of prompt-tuning on
NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different
from the pretraining corpus). For example, our preliminary exploration reveals
a large performance gap between prompt-tuning and fine-tuning when unfamiliar
inputs occur frequently in NLG tasks. This motivates us to propose
input-tuning, which fine-tunes both the continuous prompts and the input
representations, leading to a more effective way to adapt unfamiliar inputs to
frozen PLMs. Our proposed input-tuning is conceptually simple and empirically
powerful. Experimental results on seven NLG tasks demonstrate that input-tuning
is significantly and consistently better than prompt-tuning. Furthermore, on
three of these tasks, input-tuning can achieve a comparable or even better
performance than fine-tuning.",0,1,0,0,0,0,0.900857,4.0,0.807082,51
11faab3b-904d-456a-8852-e151240f9f5c,Compressing Transformer-based self-supervised models for speech processing,3,0.0295787,0.0569188,"Despite the success of Transformers in self- supervised learning with
applications to various downstream tasks, the computational cost of training
and inference remains a major challenge for applying these models to a wide
spectrum of devices. Several isolated attempts have been made to compress
Transformers, but the settings and metrics are different across studies.
Trade-off at various compression rates are also largely missing in prior work,
making it difficult to compare compression techniques. In this work, we aim to
provide context for the isolated results, studying several commonly used
compression techniques, including weight pruning, head pruning, low-rank
approximation, and knowledge distillation. We report trade- off at various
compression rate, including wall-clock time, the number of parameters, and the
number of multiply-accumulate operations. Our results show that compared to
recent approaches, basic compression techniques are strong baselines. We
further present several applications of our results, revealing properties of
Transformers, such as the significance of diagonal attention heads. In
addition, our results lead to a simple combination of compression techniques
that improves trade-off over recent approaches. We hope the results would
promote more diverse comparisons among model compression techniques and promote
the use of model compression as a tool for analyzing models. Our code of
compressing speech self-supervised model is available at
https://github.com/nervjack2/Speech-SSL-Compression/.",0,1,0,0,0,0,0.516759,5.0,0.614413,57
5f324795-99b0-4296-bea1-49dd0e6835f2,Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations,39,0.534594,0.726289,"In this paper, we show that recent advances in self-supervised feature
learning enable unsupervised object discovery and semantic segmentation with a
performance that matches the state of the field on supervised semantic
segmentation 10 years ago. We propose a methodology based on unsupervised
saliency masks and self-supervised feature clustering to kickstart object
discovery followed by training a semantic segmentation network on pseudo-labels
to bootstrap the system on images with multiple objects. We present results on
PASCAL VOC that go far beyond the current state of the art (50.0 mIoU), and we
report for the first time results on MS COCO for the whole set of 81 classes:
our method discovers 34 categories with more than $20\%$ IoU, while obtaining
an average IoU of 19.6 for all 81 categories.",0,1,0,0,1,0,0.928409,6.0,0.893353,77
a9b1dcf2-7140-4827-a28d-e5691eeeca01,Revisiting Discrete Soft Actor-Critic,8,0.25998,0.815925,"We study the adaption of soft actor-critic (SAC) from continuous action space
to discrete action space. We revisit vanilla SAC and provide an in-depth
understanding of its Q value underestimation and performance instability issues
when applied to discrete settings. We thereby propose entropy-penalty and
double average Q-learning with Q-clip to address these issues. Extensive
experiments on typical benchmarks with discrete action space, including Atari
games and a large-scale MOBA game, show the efficacy of our proposed method.
Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.",1,1,0,0,0,0,0.641218,10.0,0.841529,34
4cc5a86b-1d38-4db1-ba2c-266dfdc58e2c,Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech Translation,8,0.275564,0.633786,"Simultaneous speech translation (SimulST) is a challenging task aiming to
translate streaming speech before the complete input is observed. A SimulST
system generally includes two components: the pre-decision that aggregates the
speech information and the policy that decides to read or write. While recent
works had proposed various strategies to improve the pre-decision, they mainly
adopt the fixed wait-k policy, leaving the adaptive policies rarely explored.
This paper proposes to model the adaptive policy by adapting the Continuous
Integrate-and-Fire (CIF). Compared with monotonic multihead attention (MMA),
our method has the advantage of simpler computation, superior quality at low
latency, and better generalization to long utterances. We conduct experiments
on the MuST-C V2 dataset and show the effectiveness of our approach.",1,0,0,0,0,0,0.869499,6.0,0.850289,39
73ff77cc-5177-4607-a568-61106ac15e45,Optimizing Test-Time Query Representations for Dense Retrieval,3,0.0704767,0.092549,"Recent developments of dense retrieval rely on quality representations of
queries and contexts from pre-trained query and context encoders. In this
paper, we introduce TOUR (Test-Time Optimization of Query Representations),
which further optimizes instance-level query representations guided by signals
from test-time retrieval results. We leverage a cross-encoder re-ranker to
provide fine-grained pseudo labels over retrieval results and iteratively
optimize query representations with gradient descent. Our theoretical analysis
reveals that TOUR can be viewed as a generalization of the classical Rocchio
algorithm for pseudo relevance feedback, and we present two variants that
leverage pseudo-labels as hard binary or soft continuous labels. We first apply
TOUR on phrase retrieval with our proposed phrase re-ranker, and also evaluate
its effectiveness on passage retrieval with an off-the-shelf re-ranker. TOUR
greatly improves end-to-end open-domain question answering accuracy, as well as
passage retrieval performance. TOUR also consistently improves direct
re-ranking by up to 2.0% while running 1.3-2.4x faster with an efficient
implementation.",0,1,0,0,0,0,0.916452,4.0,0.82494,46
52ecb704-7e02-4d1e-9538-062dca061b9f,End-to-End Image-Based Fashion Recommendation,9,0.115361,0.703351,"In fashion-based recommendation settings, incorporating the item image
features is considered a crucial factor, and it has shown significant
improvements to many traditional models, including but not limited to matrix
factorization, auto-encoders, and nearest neighbor models. While there are
numerous image-based recommender approaches that utilize dedicated deep neural
networks, comparisons to attribute-aware models are often disregarded despite
their ability to be easily extended to leverage items' image features. In this
paper, we propose a simple yet effective attribute-aware model that
incorporates image features for better item representation learning in item
recommendation tasks. The proposed model utilizes items' image features
extracted by a calibrated ResNet50 component. We present an ablation study to
compare incorporating the image features using three different techniques into
the recommender system component that can seamlessly leverage any available
items' attributes. Experiments on two image-based real-world recommender
systems datasets show that the proposed model significantly outperforms all
state-of-the-art image-based models.",1,1,0,0,1,0,0.575036,13.0,0.864214,16
192dcb18-1f2f-43bd-b6f8-01ec214e0ebe,Retrieval Based Time Series Forecasting,3,0.0158277,0.245293,"Time series data appears in a variety of applications such as smart
transportation and environmental monitoring. One of the fundamental problems
for time series analysis is time series forecasting. Despite the success of
recent deep time series forecasting methods, they require sufficient
observation of historical values to make accurate forecasting. In other words,
the ratio of the output length (or forecasting horizon) to the sum of the input
and output lengths should be low enough (e.g., 0.3). As the ratio increases
(e.g., to 0.8), the uncertainty for the forecasting accuracy increases
significantly. In this paper, we show both theoretically and empirically that
the uncertainty could be effectively reduced by retrieving relevant time series
as references. In the theoretical analysis, we first quantify the uncertainty
and show its connections to the Mean Squared Error (MSE). Then we prove that
models with references are easier to learn than models without references since
the retrieved references could reduce the uncertainty. To empirically
demonstrate the effectiveness of the retrieval based time series forecasting
models, we introduce a simple yet effective two-stage method, called ReTime
consisting of a relational retrieval and a content synthesis. We also show that
ReTime can be easily adapted to the spatial-temporal time series and time
series imputation settings. Finally, we evaluate ReTime on real-world datasets
to demonstrate its effectiveness.",0,0,0,0,0,1,0.180293,9.0,0.641683,46
d3c55138-6b34-4017-a5c1-2bd7a0f61050,Towards Accurate Open-Set Recognition via Background-Class Regularization,12,0.123343,0.450777,"In open-set recognition (OSR), classifiers should be able to reject
unknown-class samples while maintaining high closed-set classification
accuracy. To effectively solve the OSR problem, previous studies attempted to
limit latent feature space and reject data located outside the limited space
via offline analyses, e.g., distance-based feature analyses, or complicated
network architectures. To conduct OSR via a simple inference process (without
offline analyses) in standard classifier architectures, we use distance-based
classifiers instead of conventional Softmax classifiers. Afterwards, we design
a background-class regularization strategy, which uses background-class data as
surrogates of unknown-class ones during training phase. Specifically, we
formulate a novel regularization loss suitable for distance-based classifiers,
which reserves sufficiently large class-wise latent feature spaces for known
classes and forces background-class samples to be located far away from the
limited spaces. Through our extensive experiments, we show that the proposed
method provides robust OSR results, while maintaining high closed-set
classification accuracy.",1,1,0,0,0,0,0.657445,11.0,0.859951,46
a7410371-6c2b-4fc4-918e-02f51f7b8a09,Concept Graph Neural Networks for Surgical Video Understanding,7,0.0254293,0.585931,"We constantly integrate our knowledge and understanding of the world to
enhance our interpretation of what we see.
  This ability is crucial in application domains which entail reasoning about
multiple entities and concepts, such as AI-augmented surgery. In this paper, we
propose a novel way of integrating conceptual knowledge into temporal analysis
tasks via temporal concept graph networks. In the proposed networks, a global
knowledge graph is incorporated into the temporal analysis of surgical
instances, learning the meaning of concepts and relations as they apply to the
data. We demonstrate our results in surgical video data for tasks such as
verification of critical view of safety, as well as estimation of Parkland
grading scale. The results show that our method improves the recognition and
detection of complex benchmarks as well as enables other analytic applications
of interest.",1,0,0,0,0,0,0.295291,7.0,0.620094,97
c195ea1b-96b9-4bfe-99ca-e8de8c6d768a,Local Contrastive Feature learning for Tabular Data,2,0.00747361,0.230875,"Contrastive self-supervised learning has been successfully used in many
domains, such as images, texts, graphs, etc., to learn features without
requiring label information. In this paper, we propose a new local contrastive
feature learning (LoCL) framework, and our theme is to learn local
patterns/features from tabular data. In order to create a niche for local
learning, we use feature correlations to create a maximum-spanning tree, and
break the tree into feature subsets, with strongly correlated features being
assigned next to each other. Convolutional learning of the features is used to
learn latent feature space, regulated by contrastive and reconstruction losses.
Experiments on public tabular datasets show the effectiveness of the proposed
method versus state-of-the-art baseline methods.",0,1,0,0,0,0,0.609985,4.0,0.582589,25
f0db1084-6000-4b2e-8d35-602b103da021,Large Language Models Struggle to Learn Long-Tail Knowledge,145,0.976944,0.999999,"The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail.",0,0,0,0,0,1,0.94625,5.0,0.892662,42
eeb14ce7-06e0-4d9a-97a6-3a69c57a2852,Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification,10,0.0502361,0.247929,"Prompt-based learning (i.e., prompting) is an emerging paradigm for
exploiting knowledge learned by a pretrained language model. In this paper, we
propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method
to automatically select label mappings for few-shot text classification with
prompting. Our method exploits one-to-many label mappings and a
statistics-based algorithm to select label mappings given a prompt template.
Our experiments demonstrate that AMuLaP achieves competitive performance on the
GLUE benchmark without human effort or external resources.",0,1,0,0,0,0,0.949406,5.0,0.89676,33
5d09ecce-ce21-415c-8ef1-b2e6013acbd5,Coordinated Topic Modeling,1,0.00991701,0.0543311,"We propose a new problem called coordinated topic modeling that imitates
human behavior while describing a text corpus. It considers a set of
well-defined topics like the axes of a semantic space with a reference
representation. It then uses the axes to model a corpus for easily
understandable representation. This new task helps represent a corpus more
interpretably by reusing existing knowledge and benefits the corpora comparison
task. We design ECTM, an embedding-based coordinated topic model that
effectively uses the reference representation to capture the target
corpus-specific aspects while maintaining each topic's global semantics. In
ECTM, we introduce the topic- and document-level supervision with a
self-training mechanism to solve the problem. Finally, extensive experiments on
multiple domains show the superiority of our model over other baselines.",0,0,1,0,1,0,0.486581,14.0,0.856078,32
87dcb339-659a-4c1d-98d7-979bebb2df8c,MetricBERT: Text Representation Learning via Self-Supervised Triplet Training,5,0.0228547,0.215452,"We present MetricBERT, a BERT-based model that learns to embed text under a
well-defined similarity metric while simultaneously adhering to the
``traditional'' masked-language task. We focus on downstream tasks of learning
similarities for recommendations where we show that MetricBERT outperforms
state-of-the-art alternatives, sometimes by a substantial margin. We conduct
extensive evaluations of our method and its different variants, showing that
our training objective is highly beneficial over a traditional contrastive
loss, a standard cosine similarity objective, and six other baselines. As an
additional contribution, we publish a dataset of video games descriptions along
with a test set of similarity annotations crafted by a domain expert.",0,1,0,1,1,0,0.171,5.0,0.343354,29
ebbc6dfa-47f7-4977-b611-b83fd0b498c6,Knowledge Distillation with the Reused Teacher Classifier,87,0.641788,0.997165,"Knowledge distillation aims to compress a powerful yet cumbersome teacher
model into a lightweight student model without much sacrifice of performance.
For this purpose, various approaches have been proposed over the past few
years, generally with elaborately designed knowledge representations, which in
turn increase the difficulty of model development and interpretation. In
contrast, we empirically show that a simple knowledge distillation technique is
enough to significantly narrow down the teacher-student performance gap. We
directly reuse the discriminative classifier from the pre-trained teacher model
for student inference and train a student encoder through feature alignment
with a single $\ell_2$ loss. In this way, the student model is able to achieve
exactly the same performance as the teacher model provided that their extracted
features are perfectly aligned. An additional projector is developed to help
the student encoder match with the teacher classifier, which renders our
technique applicable to various teacher and student architectures. Extensive
experiments demonstrate that our technique achieves state-of-the-art results at
the modest cost of compression ratio due to the added projector.",0,1,0,0,1,1,0.893331,7.0,0.885165,61
d92f02be-1c11-431d-af78-91f6646cf2bc,Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment,9,0.212172,0.478116,"Despite recent progress towards scaling up multimodal vision-language models,
these models are still known to struggle on compositional generalization
benchmarks such as Winoground. We find that a critical component lacking from
current vision-language models is relation-level alignment: the ability to
match directional semantic relations in text (e.g., ""mug in grass"") with
spatial relationships in the image (e.g., the position of the mug relative to
the grass). To tackle this problem, we show that relation alignment can be
enforced by encouraging the directed language attention from 'mug' to 'grass'
(capturing the semantic relation 'in') to match the directed visual attention
from the mug to the grass. Tokens and their corresponding objects are softly
identified using the cross-modal attention. We prove that this notion of soft
relation alignment is equivalent to enforcing congruence between vision and
language attention matrices under a 'change of basis' provided by the
cross-modal attention matrix. Intuitively, our approach projects visual
attention into the language attention space to calculate its divergence from
the actual language attention, and vice versa. We apply our Cross-modal
Attention Congruence Regularization (CACR) loss to UNITER and improve on the
state-of-the-art approach to Winoground.",0,0,0,0,0,0,0.76141,4.0,0.687585,41
ba0334f4-3afd-4940-ad97-bdce865037b7,LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation,9,0.23917,0.41785,"The rapid growth of biomedical literature poses a significant challenge for
curation and interpretation. This has become more evident during the COVID-19
pandemic. LitCovid, a literature database of COVID-19 related papers in PubMed,
has accumulated over 180,000 articles with millions of accesses. Approximately
10,000 new articles are added to LitCovid every month. A main curation task in
LitCovid is topic annotation where an article is assigned with up to eight
topics, e.g., Treatment and Diagnosis. The annotated topics have been widely
used both in LitCovid (e.g., accounting for ~18% of total uses) and downstream
studies such as network generation. However, it has been a primary curation
bottleneck due to the nature of the task and the rapid literature growth. This
study proposes LITMC-BERT, a transformer-based multi-label classification
method in biomedical literature. It uses a shared transformer backbone for all
the labels while also captures label-specific features and the correlations
between label pairs. We compare LITMC-BERT with three baseline models on two
datasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the
current best results, respectively, and only requires ~18% of the inference
time than the Binary BERT baseline. The related datasets and models are
available via https://github.com/ncbi/ml-transformer.",1,1,0,0,1,0,0.633748,6.0,0.732498,51
54c8d718-f653-49da-a5f2-845db360bc5e,CATs are Fuzzy PETs: A Corpus and Analysis of Potentially Euphemistic Terms,15,0.190404,0.408779,"Euphemisms have not received much attention in natural language processing,
despite being an important element of polite and figurative language.
Euphemisms prove to be a difficult topic, not only because they are subject to
language change, but also because humans may not agree on what is a euphemism
and what is not. Nevertheless, the first step to tackling the issue is to
collect and analyze examples of euphemisms. We present a corpus of potentially
euphemistic terms (PETs) along with example texts from the GloWbE corpus.
Additionally, we present a subcorpus of texts where these PETs are not being
used euphemistically, which may be useful for future applications. We also
discuss the results of multiple analyses run on the corpus. Firstly, we find
that sentiment analysis on the euphemistic texts supports that PETs generally
decrease negative and offensive sentiment. Secondly, we observe cases of
disagreement in an annotation task, where humans are asked to label PETs as
euphemistic or not in a subset of our corpus text examples. We attribute the
disagreement to a variety of potential reasons, including if the PET was a
commonly accepted term (CAT).",0,0,0,1,0,0,0.0107545,18.0,0.659085,41
f465dcef-b998-4dc9-8908-3270d4d618b5,SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval,23,0.378141,0.673324,"Sampling proper negatives from a large document pool is vital to effectively
train a dense retrieval model. However, existing negative sampling strategies
suffer from the uninformative or false negative problem. In this work, we
empirically show that according to the measured relevance scores, the negatives
ranked around the positives are generally more informative and less likely to
be false negatives. Intuitively, these negatives are not too hard (\emph{may be
false negatives}) or too easy (\emph{uninformative}). They are the ambiguous
negatives and need more attention during training. Thus, we propose a simple
ambiguous negatives sampling method, SimANS, which incorporates a new sampling
probability distribution to sample more ambiguous negatives. Extensive
experiments on four public and one industry datasets show the effectiveness of
our approach. We made the code and models publicly available in
\url{https://github.com/microsoft/SimXNS}.",1,1,0,0,0,0,0.877312,5.0,0.82632,53
0e62bd34-f2d8-4fe8-a66a-aa6e894e2de9,SQuId: Measuring Speech Naturalness in Many Languages,9,0.210664,0.798448,"Much of text-to-speech research relies on human evaluation, which incurs
heavy costs and slows down the development process. The problem is particularly
acute in heavily multilingual applications, where recruiting and polling judges
can take weeks. We introduce SQuId (Speech Quality Identification), a
multilingual naturalness prediction model trained on over a million ratings and
tested in 65 locales-the largest effort of this type to date. The main insight
is that training one model on many locales consistently outperforms mono-locale
baselines. We present our task, the model, and show that it outperforms a
competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then
demonstrate the effectiveness of cross-locale transfer during fine-tuning and
highlight its effect on zero-shot locales, i.e., locales for which there is no
fine-tuning data. Through a series of analyses, we highlight the role of
non-linguistic effects such as sound artifacts in cross-locale transfer.
Finally, we present the effect of our design decision, e.g., model size,
pre-training diversity, and language rebalancing with several ablation
experiments.",0,1,0,1,1,0,0.766434,5.0,0.753016,47
398bc730-b745-4fd7-901c-b1a53af085a7,A Capability and Skill Model for Heterogeneous Autonomous Robots,6,0.0213716,0.204726,"Teams of heterogeneous autonomous robots become increasingly important due to
their facilitation of various complex tasks. For such heterogeneous robots,
there is currently no consistent way of describing the functions that each
robot provides. In the field of manufacturing, capability modeling is
considered a promising approach to semantically model functions provided by
different machines. This contribution investigates how to apply and extend
capability models from manufacturing to the field of autonomous robots and
presents an approach for such a capability model.",0,0,0,0,0,0,0.00247471,11.0,0.308197,31
a5adbb9a-da5a-4bbc-b628-7499c2dc1733,DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation,9,0.0326703,0.348123,"Deep learning approaches achieve prominent success in 3D semantic
segmentation. However, collecting densely annotated real-world 3D datasets is
extremely time-consuming and expensive. Training models on synthetic data and
generalizing on real-world scenarios becomes an appealing alternative, but
unfortunately suffers from notorious domain shifts. In this work, we propose a
Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and
context gaps caused by different sensing mechanisms and layout placements
across domains. Our DODA encompasses virtual scan simulation to imitate
real-world point cloud patterns and tail-aware cuboid mixing to alleviate the
interior context gap with a cuboid-based intermediate domain. The first
unsupervised sim-to-real adaptation benchmark on 3D indoor semantic
segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular
Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA
approaches by over 13% on both 3D-FRONT -> ScanNet and 3D-FRONT -> S3DIS. Code
is available at https://github.com/CVMI-Lab/DODA.",1,1,0,0,0,0,0.391607,7.0,0.670181,75
2e8260ca-3e0e-4127-978f-87da222e47da,Fine-grained Semantic Alignment Network for Weakly Supervised Temporal Language Grounding,10,0.144719,0.573411,"Temporal language grounding (TLG) aims to localize a video segment in an
untrimmed video based on a natural language description. To alleviate the
expensive cost of manual annotations for temporal boundary labels, we are
dedicated to the weakly supervised setting, where only video-level descriptions
are provided for training. Most of the existing weakly supervised methods
generate a candidate segment set and learn cross-modal alignment through a
MIL-based framework. However, the temporal structure of the video as well as
the complicated semantics in the sentence are lost during the learning. In this
work, we propose a novel candidate-free framework: Fine-grained Semantic
Alignment Network (FSAN), for weakly supervised TLG. Instead of view the
sentence and candidate moments as a whole, FSAN learns token-by-clip
cross-modal semantic alignment by an iterative cross-modal interaction module,
generates a fine-grained cross-modal semantic alignment map, and performs
grounding directly on top of the map. Extensive experiments are conducted on
two widely-used benchmarks: ActivityNet-Captions, and DiDeMo, where our FSAN
achieves state-of-the-art performance.",0,1,0,0,1,0,0.854104,7.0,0.863631,49
647a9f1d-f494-4025-b2f9-7a6dae25ce84,Data Splits and Metrics for Method Benchmarking on Surgical Action Triplet Datasets,11,0.304246,0.846434,"In addition to generating data and annotations, devising sensible data
splitting strategies and evaluation metrics is essential for the creation of a
benchmark dataset. This practice ensures consensus on the usage of the data,
homogeneous assessment, and uniform comparison of research methods on the
dataset. This study focuses on CholecT50, which is a 50 video surgical dataset
that formalizes surgical activities as triplets of <instrument, verb, target>.
In this paper, we introduce the standard splits for the CholecT50 and CholecT45
datasets and show how they compare with existing use of the dataset. CholecT45
is the first public release of 45 videos of CholecT50 dataset. We also develop
a metrics library, ivtmetrics, for model evaluation on surgical triplets.
Furthermore, we conduct a benchmark study by reproducing baseline methods in
the most predominantly used deep learning frameworks (PyTorch and TensorFlow)
to evaluate them using the proposed data splits and metrics and release them
publicly to support future research. The proposed data splits and evaluation
metrics will enable global tracking of research progress on the dataset and
facilitate optimal model selection for further deployment.",1,1,0,0,0,0,0.712332,7.0,0.801494,21
510bde2c-fdc4-474a-bfab-2225f7258bbf,Learning MAX-SAT from Contextual Examples for Combinatorial Optimisation,12,0.0282676,0.411003,"Combinatorial optimisation problems are ubiquitous in artificial
intelligence. Designing the underlying models, however, requires substantial
expertise, which is a limiting factor in practice. The models typically consist
of hard and soft constraints, or combine hard constraints with an objective
function. We introduce a novel setting for learning combinatorial optimisation
problems from contextual examples. These positive and negative examples show -
in a particular context - whether the solutions are good enough or not. We
develop our framework using the MAX-SAT formalism as it is simple yet powerful
setting having these features. We study the learnability of MAX-SAT models. Our
theoretical results show that high-quality MAX-SAT models can be learned from
contextual examples in the realisable and agnostic settings, as long as the
data satisfies an intuitive ""representativeness"" condition. We also contribute
two implementations based on our theoretical results: one leverages ideas from
syntax-guided synthesis while the other makes use of stochastic local search
techniques. The two implementations are evaluated by recovering synthetic and
benchmark models from contextual examples. The experimental results support our
theoretical analysis, showing that MAX-SAT models can be learned from
contextual examples. Among the two implementations, the stochastic local search
learner scales much better than the syntax-guided implementation while
providing comparable or better models.",1,0,1,0,0,0,4.10595e-05,20.0,0.414505,56
b709658b-9107-488e-a7e5-bac3038156d2,Local Relighting of Real Scenes,2,0.0337003,0.0529478,"We introduce the task of local relighting, which changes a photograph of a
scene by switching on and off the light sources that are visible within the
image. This new task differs from the traditional image relighting problem, as
it introduces the challenge of detecting light sources and inferring the
pattern of light that emanates from them. We propose an approach for local
relighting that trains a model without supervision of any novel image dataset
by using synthetically generated image pairs from another model. Concretely, we
collect paired training images from a stylespace-manipulated GAN; then we use
these images to train a conditional image-to-image model. To benchmark local
relighting, we introduce Lonoff, a collection of 306 precisely aligned images
taken in indoor spaces with different combinations of lights switched on. We
show that our method significantly outperforms baseline methods based on GAN
inversion. Finally, we demonstrate extensions of our method that control
different light sources separately. We invite the community to tackle this new
task of local relighting.",0,0,1,1,0,0,0.79608,7.0,0.83633,33
9bf13d1d-3fcd-45a1-a794-d537b3065dab,CXTrack: Improving 3D Point Cloud Tracking with Contextual Information,17,0.145286,0.532579,"3D single object tracking plays an essential role in many applications, such
as autonomous driving. It remains a challenging problem due to the large
appearance variation and the sparsity of points caused by occlusion and limited
sensor capabilities. Therefore, contextual information across two consecutive
frames is crucial for effective object tracking. However, points containing
such useful information are often overlooked and cropped out in existing
methods, leading to insufficient use of important contextual knowledge. To
address this issue, we propose CXTrack, a novel transformer-based network for
3D object tracking, which exploits ConteXtual information to improve the
tracking results. Specifically, we design a target-centric transformer network
that directly takes point features from two consecutive frames and the previous
bounding box as input to explore contextual information and implicitly
propagate target cues. To achieve accurate localization for objects of all
sizes, we propose a transformer-based localization head with a novel center
embedding module to distinguish the target from distractors. Extensive
experiments on three large-scale datasets, KITTI, nuScenes and Waymo Open
Dataset, show that CXTrack achieves state-of-the-art tracking performance while
running at 34 FPS.",0,1,0,0,1,0,0.726019,5.0,0.729768,38
12cfd474-daa8-481c-8069-e500548529c9,A.I. Robustness: a Human-Centered Perspective on Technological Challenges and Opportunities,3,0.0585359,0.262764,"Despite the impressive performance of Artificial Intelligence (AI) systems,
their robustness remains elusive and constitutes a key issue that impedes
large-scale adoption. Robustness has been studied in many domains of AI, yet
with different interpretations across domains and contexts. In this work, we
systematically survey the recent progress to provide a reconciled terminology
of concepts around AI robustness. We introduce three taxonomies to organize and
describe the literature both from a fundamental and applied point of view: 1)
robustness by methods and approaches in different phases of the machine
learning pipeline; 2) robustness for specific model architectures, tasks, and
systems; and in addition, 3) robustness assessment methodologies and insights,
particularly the trade-offs with other trustworthiness properties. Finally, we
identify and discuss research gaps and opportunities and give an outlook on the
field. We highlight the central role of humans in evaluating and enhancing AI
robustness, considering the necessary knowledge humans can provide, and discuss
the need for better understanding practices and developing supportive tools in
the future.",1,0,0,0,0,0,0.390319,5.0,0.5374,367
7b0e14ee-d390-442e-8dcf-63b66083b64f,Vision-Based American Sign Language Classification Approach via Deep Learning,2,0.0668103,0.0682273,"Hearing-impaired is the disability of partial or total hearing loss that
causes a significant problem for communication with other people in society.
American Sign Language (ASL) is one of the sign languages that most commonly
used language used by Hearing impaired communities to communicate with each
other. In this paper, we proposed a simple deep learning model that aims to
classify the American Sign Language letters as a step in a path for removing
communication barriers that are related to disabilities.",0,1,0,0,0,0,0.768328,11.0,0.888242,29
e5cd4c2e-91a7-4189-998a-9e93130a8092,Streaming Intended Query Detection using E2E Modeling for Continued Conversation,3,0.0702145,0.0566598,"In voice-enabled applications, a predetermined hotword isusually used to
activate a device in order to attend to the query.However, speaking queries
followed by a hotword each timeintroduces a cognitive burden in continued
conversations. Toavoid repeating a hotword, we propose a streaming
end-to-end(E2E) intended query detector that identifies the utterancesdirected
towards the device and filters out other utterancesnot directed towards device.
The proposed approach incor-porates the intended query detector into the E2E
model thatalready folds different components of the speech recognitionpipeline
into one neural network.The E2E modeling onspeech decoding and intended query
detection also allows us todeclare a quick intended query detection based on
early partialrecognition result, which is important to decrease latencyand make
the system responsive. We demonstrate that theproposed E2E approach yields a
22% relative improvement onequal error rate (EER) for the detection accuracy
and 600 mslatency improvement compared with an independent intendedquery
detector. In our experiment, the proposed model detectswhether the user is
talking to the device with a 8.7% EERwithin 1.4 seconds of median latency after
user starts speaking.",0,1,0,0,0,0,0.588128,8.0,0.783837,25
9a74cccf-1e5d-4f6c-97a2-7b7438c333e9,Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries,37,0.288607,0.805202,"Knowledge graph (KG) embeddings have been a mainstream approach for reasoning
over incomplete KGs. However, limited by their inherently shallow and static
architectures, they can hardly deal with the rising focus on complex logical
queries, which comprise logical operators, imputed edges, multiple source
entities, and unknown intermediate entities. In this work, we present the
Knowledge Graph Transformer (kgTransformer) with masked pre-training and
fine-tuning strategies. We design a KG triple transformation method to enable
Transformer to handle KGs, which is further strengthened by the
Mixture-of-Experts (MoE) sparse activation. We then formulate the complex
logical queries as masked prediction and introduce a two-stage masked
pre-training strategy to improve transferability and generalizability.
Extensive experiments on two benchmarks demonstrate that kgTransformer can
consistently outperform both KG embedding-based baselines and advanced encoders
on nine in-domain and out-of-domain reasoning tasks. Additionally,
kgTransformer can reason with explainability via providing the full reasoning
paths to interpret given answers.",0,0,1,0,1,0,0.792068,6.0,0.806997,50
933bb0c2-0b57-4e80-9133-e05d0392635d,Why Deep Surgical Models Fail?: Revisiting Surgical Action Triplet Recognition through the Lens of Robustness,3,0.122341,0.418379,"Surgical action triplet recognition provides a better understanding of the
surgical scene. This task is of high relevance as it provides the surgeon with
context-aware support and safety. The current go-to strategy for improving
performance is the development of new network mechanisms. However, the
performance of current state-of-the-art techniques is substantially lower than
other surgical tasks. Why is this happening? This is the question that we
address in this work. We present the first study to understand the failure of
existing deep learning models through the lens of robustness and
explainability. Firstly, we study current existing models under weak and strong
$\delta-$perturbations via an adversarial optimisation scheme. We then analyse
the failure modes via feature based explanations. Our study reveals that the
key to improving performance and increasing reliability is in the core and
spurious attributes. Our work opens the door to more trustworthy and reliable
deep learning models in surgical data science.",0,0,0,0,0,0,0.867357,11.0,0.91761,36
d5ba15bc-6dc3-4434-a3d9-e2323de3935b,VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models,7,0.173963,0.411219,"VAST, the Valence-Assessing Semantics Test, is a novel intrinsic evaluation
task for contextualized word embeddings (CWEs). VAST uses valence, the
association of a word with pleasantness, to measure the correspondence of
word-level LM semantics with widely used human judgments, and examines the
effects of contextualization, tokenization, and LM-specific geometry. Because
prior research has found that CWEs from GPT-2 perform poorly on other intrinsic
evaluations, we select GPT-2 as our primary subject, and include results
showing that VAST is useful for 7 other LMs, and can be used in 7 languages.
GPT-2 results show that the semantics of a word incorporate the semantics of
context in layers closer to model output, such that VAST scores diverge between
our contextual settings, ranging from Pearson's rho of .55 to .77 in layer 11.
We also show that multiply tokenized words are not semantically encoded until
layer 8, where they achieve Pearson's rho of .46, indicating the presence of an
encoding process for multiply tokenized words which differs from that of singly
tokenized words, for which rho is highest in layer 0. We find that a few
neurons with values having greater magnitude than the rest mask word-level
semantics in GPT-2's top layer, but that word-level semantics can be recovered
by nullifying non-semantic principal components: Pearson's rho in the top layer
improves from .32 to .76. After isolating semantics, we show the utility of
VAST for understanding LM semantics via improvements over related work on four
word similarity tasks, with a score of .50 on SimLex-999, better than the
previous best of .45 for GPT-2. Finally, we show that 8 of 10 WEAT bias tests,
which compare differences in word embedding associations between groups of
words, exhibit more stereotype-congruent biases after isolating semantics,
indicating that non-semantic structures in LMs also mask biases.",1,0,1,0,0,0,0.933326,10.0,0.938675,72
5d1f3de3-1fda-49d4-9339-0b0d789ce492,Face recognition with small and large size databases,15,0.0287589,0.642743,"This paper presents experimental results using the ORL (40 people) and FERET
(994 people) databases. The ORL database can be useful for securing
applications where few users attempting to access are expected. This is the
case, for instance, of a PDA or PC where the password is the face of the user.
On the other hand, the FERET database is useful for studying those situations
where the number of authorized users is around a thousand people.",0,1,0,0,0,1,2.36964e-10,38.0,0.374407,15
8ca7daee-a019-4554-945f-695e016d028c,Spiking Approximations of the MaxPooling Operation in Deep SNNs,6,0.0361567,0.454638,"Spiking Neural Networks (SNNs) are an emerging domain of biologically
inspired neural networks that have shown promise for low-power AI. A number of
methods exist for building deep SNNs, with Artificial Neural Network
(ANN)-to-SNN conversion being highly successful. MaxPooling layers in
Convolutional Neural Networks (CNNs) are an integral component to downsample
the intermediate feature maps and introduce translational invariance, but the
absence of their hardware-friendly spiking equivalents limits such CNNs'
conversion to deep SNNs. In this paper, we present two hardware-friendly
methods to implement Max-Pooling in deep SNNs, thus facilitating easy
conversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute
SNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware
(with MNIST, FMNIST, & CIFAR10 dataset); thus, showing the feasibility of our
approach.",0,1,0,0,0,0,0.154406,7.0,0.515011,34
9820dafa-863f-42be-bd1b-1d34ea945234,Hierarchical Instance Mixing across Domains in Aerial Segmentation,7,0.053075,0.648687,"We investigate the task of unsupervised domain adaptation in aerial semantic
segmentation and discover that the current state-of-the-art algorithms designed
for autonomous driving based on domain mixing do not translate well to the
aerial setting. This is due to two factors: (i) a large disparity in the
extension of the semantic categories, which causes a domain imbalance in the
mixed image, and (ii) a weaker structural consistency in aerial scenes than in
driving scenes since the same scene might be viewed from different perspectives
and there is no well-defined and repeatable structure of the semantic elements
in the images. Our solution to these problems is composed of: (i) a new mixing
strategy for aerial segmentation across domains called Hierarchical Instance
Mixing (HIMix), which extracts a set of connected components from each semantic
mask and mixes them according to a semantic hierarchy and, (ii) a twin-head
architecture in which two separate segmentation heads are fed with variations
of the same images in a contrastive fashion to produce finer segmentation maps.
We conduct extensive experiments on the LoveDA benchmark, where our solution
outperforms the current state-of-the-art.",0,1,0,0,1,0,0.652496,7.0,0.777997,50
1f275255-093e-46f8-af12-174a2a3de448,Learning Local Displacements for Point Cloud Completion,27,0.220561,0.684097,"We propose a novel approach aimed at object and semantic scene completion
from a partial scan represented as a 3D point cloud. Our architecture relies on
three novel layers that are used successively within an encoder-decoder
structure and specifically developed for the task at hand. The first one
carries out feature extraction by matching the point features to a set of
pre-trained local descriptors. Then, to avoid losing individual descriptors as
part of standard operations such as max-pooling, we propose an alternative
neighbor-pooling operation that relies on adopting the feature vectors with the
highest activations. Finally, up-sampling in the decoder modifies our feature
extraction in order to increase the output dimension. While this model is
already able to achieve competitive results with the state of the art, we
further propose a way to increase the versatility of our approach to process
point clouds. To this aim, we introduce a second model that assembles our
layers within a transformer architecture. We evaluate both architectures on
object and indoor scene completion tasks, achieving state-of-the-art
performance.",0,1,0,0,1,0,0.629147,7.0,0.768926,52
ca8b1921-d49d-48f8-b9b7-32f3d133a676,Points2NeRF: Generating Neural Radiance Fields from 3D point cloud,12,0.208067,0.294958,"Contemporary registration devices for 3D visual information, such as LIDARs
and various depth cameras, capture data as 3D point clouds. In turn, such
clouds are challenging to be processed due to their size and complexity.
Existing methods address this problem by fitting a mesh to the point cloud and
rendering it instead. This approach, however, leads to the reduced fidelity of
the resulting visualization and misses color information of the objects crucial
in computer graphics applications. In this work, we propose to mitigate this
challenge by representing 3D objects as Neural Radiance Fields (NeRFs). We
leverage a hypernetwork paradigm and train the model to take a 3D point cloud
with the associated color values and return a NeRF network's weights that
reconstruct 3D objects from input 2D images. Our method provides efficient 3D
object representation and offers several advantages over the existing
approaches, including the ability to condition NeRFs and improved
generalization beyond objects seen in training. The latter we also confirmed in
the results of our empirical evaluation.",1,1,0,0,1,0,0.678028,8.0,0.814461,66
fe54dbe4-78bf-4b9c-a9e8-0de8c6b7aca0,"Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report",64,0.368025,0.971254,"In September 2021, the ""One Hundred Year Study on Artificial Intelligence""
project (AI100) issued the second report of its planned long-term periodic
assessment of artificial intelligence (AI) and its impact on society. It was
written by a panel of 17 study authors, each of whom is deeply rooted in AI
research, chaired by Michael Littman of Brown University. The report, entitled
""Gathering Strength, Gathering Storms,"" answers a set of 14 questions probing
critical areas of AI development addressing the major risks and dangers of AI,
its effects on society, its public perception and the future of the field. The
report concludes that AI has made a major leap from the lab to people's lives
in recent years, which increases the urgency to understand its potential
negative effects. The questions were developed by the AI100 Standing Committee,
chaired by Peter Stone of the University of Texas at Austin, consisting of a
group of AI leaders with expertise in computer science, sociology, ethics,
economics, and other disciplines.",0,0,0,0,0,1,0.130508,8.0,0.552919,54
4e13b173-0742-4db9-8a44-9db4d1f5c011,Evaluating Feature Attribution Methods in the Image Domain,15,0.364519,0.225253,"Feature attribution maps are a popular approach to highlight the most
important pixels in an image for a given prediction of a model. Despite a
recent growth in popularity and available methods, little attention is given to
the objective evaluation of such attribution maps. Building on previous work in
this domain, we investigate existing metrics and propose new variants of
metrics for the evaluation of attribution maps. We confirm a recent finding
that different attribution metrics seem to measure different underlying
concepts of attribution maps, and extend this finding to a larger selection of
attribution metrics. We also find that metric results on one dataset do not
necessarily generalize to other datasets, and methods with desirable
theoretical properties such as DeepSHAP do not necessarily outperform
computationally cheaper alternatives. Based on these findings, we propose a
general benchmarking approach to identify the ideal feature attribution method
for a given use case. Implementations of attribution metrics and our
experiments are available online.",1,0,0,0,0,0,0.969307,9.0,0.959852,46
d8da0d36-7db5-4f54-bd30-bb312e5861e5,FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation,12,0.201766,0.361167,"The speaker-follower models have proven to be effective in
vision-and-language navigation, where a speaker model is used to synthesize new
instructions to augment the training data for a follower navigation model.
However, in many of the previous methods, the generated instructions are not
directly trained to optimize the performance of the follower. In this paper, we
present \textsc{foam}, a \textsc{Fo}llower-\textsc{a}ware speaker
\textsc{M}odel that is constantly updated given the follower feedback, so that
the generated instructions can be more suitable to the current learning state
of the follower. Specifically, we optimize the speaker using a bi-level
optimization framework and obtain its training signals by evaluating the
follower on labeled data. Experimental results on the Room-to-Room and
Room-across-Room datasets demonstrate that our methods can outperform strong
baseline models across settings. Analyses also reveal that our generated
instructions are of higher quality than the baselines.",1,1,0,0,0,0,0.863632,6.0,0.84665,39
3fdc6efd-84b7-4f62-bdff-a77694a18188,Training Language Models with Memory Augmentation,87,0.85729,0.935083,"Recent work has improved language models (LMs) remarkably by equipping them
with a non-parametric memory component. However, most existing approaches only
introduce mem-ories at testing time or represent them using a separately
trained encoder, resulting in suboptimal training of the language model. In
this work, we present TRIME, a novel yet simple training approach designed for
training LMs with memory augmentation. Our approach uses a training objective
that directly takes in-batch examples as accessible memory. We also present new
methods for memory construction and data batching, which are used for adapting
to different sets of memories--local, long-term, and external memory--at
testing time. We evaluate TRIME on multiple language modeling and machine
translation benchmarks and show that it is able to achieve significant
improvements across all the settings. Concretely, TRIME reduces the perplexity
from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory
set from the training corpus. Compared to standard LM training, TRIME adds
negligible computational overhead and is compatible with different neural
architectures, making it a versatile solution for training memory-augmented
LMs.",1,1,0,0,0,0,0.949837,5.0,0.897333,61
90613832-0ea1-4b55-9747-62840bd7f82f,Meta Policy Learning for Cold-Start Conversational Recommendation,17,0.520624,0.513264,"Conversational recommender systems (CRS) explicitly solicit users'
preferences for improved recommendations on the fly. Most existing CRS
solutions count on a single policy trained by reinforcement learning for a
population of users. However, for users new to the system, such a global policy
becomes ineffective to satisfy them, i.e., the cold-start challenge. In this
paper, we study CRS policy learning for cold-start users via meta-reinforcement
learning. We propose to learn a meta policy and adapt it to new users with only
a few trials of conversational recommendations. To facilitate fast policy
adaptation, we design three synergetic components. Firstly, we design a
meta-exploration policy dedicated to identifying user preferences via a few
exploratory conversations, which accelerates personalized policy adaptation
from the meta policy. Secondly, we adapt the item recommendation module for
each user to maximize the recommendation quality based on the collected
conversation states during conversations. Thirdly, we propose a
Transformer-based state encoder as the backbone to connect the previous two
components. It provides comprehensive state representations by modeling
complicated relations between positive and negative feedback during the
conversation. Extensive experiments on three datasets demonstrate the advantage
of our solution in serving new users, compared with a rich set of
state-of-the-art CRS solutions.",1,1,1,0,1,0,0.823115,7.0,0.84857,59
9e336d9a-91f7-4f73-84cb-493f66022311,A description of Turkish Discourse Bank 1.2 and an examination of common dependencies in Turkish discourse,2,0.0272581,0.345296,"We describe Turkish Discourse Bank 1.2, the latest version of a discourse
corpus annotated for explicitly or implicitly conveyed discourse relations,
their constitutive units, and senses in the Penn Discourse Treebank style. We
present an evaluation of the recently added tokens and examine three commonly
occurring dependency patterns that hold among the constitutive units of a pair
of adjacent discourse relations, namely, shared arguments, full embedding and
partial containment of a discourse relation. We present three major findings:
(a) implicitly conveyed relations occur more often than explicitly conveyed
relations in the data; (b) it is much more common for two adjacent implicit
discourse relations to share an argument than for two adjacent explicit
relations to do so; (c) both full embedding and partial containment of
discourse relations are pervasive in the corpus, which can be partly due to
subordinator connectives whose preposed subordinate clause tends to be selected
together with the matrix clause rather than being selected alone. Finally, we
briefly discuss the implications of our findings for Turkish discourse parsing.",0,0,0,0,0,0,1.52258e-05,29.0,0.562002,18
7246123d-666a-4a5f-8879-52ccc42f7ee2,Meta-Learning Parameterized Skills,2,0.0124243,0.102912,"We propose a novel parameterized skill-learning algorithm that aims to learn
transferable parameterized skills and synthesize them into a new action space
that supports efficient learning in long-horizon tasks. We propose to leverage
off-policy Meta-RL combined with a trajectory-centric smoothness term to learn
a set of parameterized skills. Our agent can use these learned skills to
construct a three-level hierarchical framework that models a
Temporally-extended Parameterized Action Markov Decision Process. We
empirically demonstrate that the proposed algorithms enable an agent to solve a
set of difficult long-horizon (obstacle-course and robot manipulation) tasks.",1,0,0,0,0,0,0.144008,7.0,0.504202,69
bb00425f-e537-4dce-a7a0-494b2dfb8c29,Accurate Action Recommendation for Smart Home via Two-Level Encoders and Commonsense Knowledge,6,0.0628945,0.222751,"How can we accurately recommend actions for users to control their devices at
home? Action recommendation for smart home has attracted increasing attention
due to its potential impact on the markets of virtual assistants and Internet
of Things (IoT). However, designing an effective action recommender system for
smart home is challenging because it requires handling context correlations,
considering both queried contexts and previous histories of users, and dealing
with capricious intentions in history. In this work, we propose SmartSense, an
accurate action recommendation method for smart home. For individual action,
SmartSense summarizes its device control and its temporal contexts in a
self-attentive manner, to reflect the importance of the correlation between
them. SmartSense then summarizes sequences of users considering queried
contexts in a query-attentive manner to extract the query-related patterns from
the sequential actions. SmartSense also transfers the commonsense knowledge
from routine data to better handle intentions in action sequences. As a result,
SmartSense addresses all three main challenges of action recommendation for
smart home, and achieves the state-of-the-art performance giving up to 9.8%
higher mAP@1 than the best competitor.",1,1,0,0,1,0,0.362728,9.0,0.732588,54
164f2b5a-9952-4991-b19c-853fb5e3a93e,Bridging POMDPs and Bayesian decision making for robust maintenance planning under model uncertainty: An application to railway systems,8,0.142336,0.734882,"Structural Health Monitoring (SHM) describes a process for inferring
quantifiable metrics of structural condition, which can serve as input to
support decisions on the operation and maintenance of infrastructure assets.
Given the long lifespan of critical structures, this problem can be cast as a
sequential decision making problem over prescribed horizons. Partially
Observable Markov Decision Processes (POMDPs) offer a formal framework to solve
the underlying optimal planning task. However, two issues can undermine the
POMDP solutions. Firstly, the need for a model that can adequately describe the
evolution of the structural condition under deterioration or corrective actions
and, secondly, the non-trivial task of recovery of the observation process
parameters from available monitoring data. Despite these potential challenges,
the adopted POMDP models do not typically account for uncertainty on model
parameters, leading to solutions which can be unrealistically confident. In
this work, we address both key issues. We present a framework to estimate POMDP
transition and observation model parameters directly from available data, via
Markov Chain Monte Carlo (MCMC) sampling of a Hidden Markov Model (HMM)
conditioned on actions. The MCMC inference estimates distributions of the
involved model parameters. We then form and solve the POMDP problem by
exploiting the inferred distributions, to derive solutions that are robust to
model uncertainty. We successfully apply our approach on maintenance planning
for railway track assets on the basis of a ""fractal value"" indicator, which is
computed from actual railway monitoring data.",0,1,0,0,0,0,0.0198943,14.0,0.605948,62
a8a4886d-a69c-4d8a-9cd8-3528b92b884a,Ask Me Anything: A simple strategy for prompting language models,119,0.447286,0.798629,"Large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt that demonstrates how to perform the task and
no additional training. Prompting is a brittle process wherein small
modifications to the prompt can cause large variations in the model
predictions, and therefore significant effort is dedicated towards designing a
painstakingly ""perfect prompt"" for a task. To mitigate the high degree of
effort involved in prompt-design, we instead ask whether producing multiple
effective, yet imperfect, prompts and aggregating them can lead to a high
quality prompting strategy. Our observations motivate our proposed prompting
method, ASK ME ANYTHING (AMA). We first develop an understanding of the
effective prompt formats, finding that question-answering (QA) prompts, which
encourage open-ended generation (""Who went to the park?"") tend to outperform
those that restrict the model outputs (""John went to the park. Output True or
False.""). Our approach recursively uses the LLM itself to transform task inputs
to the effective QA format. We apply the collected prompts to obtain several
noisy votes for the input's true label. We find that the prompts can have very
different accuracies and complex dependencies and thus propose to use weak
supervision, a procedure for combining the noisy predictions, to produce the
final predictions for the inputs. We evaluate AMA across open-source model
families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B
parameters), demonstrating an average performance lift of 10.2% over the
few-shot baseline. This simple strategy enables the open-source GPT-J-6B model
to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular
benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms
few-shot GPT3-175B. We release our code here:
https://github.com/HazyResearch/ama_prompting",1,1,0,0,0,1,0.839826,5.0,0.799139,71
22289cac-cd43-49e3-be96-245b6cc7b4c9,Semantic Unfolding of StyleGAN Latent Space,3,0.0636046,0.122756,"Generative adversarial networks (GANs) have proven to be surprisingly
efficient for image editing by inverting and manipulating the latent code
corresponding to an input real image. This editing property emerges from the
disentangled nature of the latent space. In this paper, we identify that the
facial attribute disentanglement is not optimal, thus facial editing relying on
linear attribute separation is flawed. We thus propose to improve semantic
disentanglement with supervision. Our method consists in learning a proxy
latent representation using normalizing flows, and we show that this leads to a
more efficient space for face image editing.",0,0,0,0,0,0,0.979643,6.0,0.958348,22
137ed7f4-55ee-4946-ab5d-416541d7bc1a,Pretraining a Neural Network before Knowing Its Architecture,1,0.00657201,0.112449,"Training large neural networks is possible by training a smaller hypernetwork
that predicts parameters for the large ones. A recently released Graph
HyperNetwork (GHN) trained this way on one million smaller ImageNet
architectures is able to predict parameters for large unseen networks such as
ResNet-50. While networks with predicted parameters lose performance on the
source task, the predicted parameters have been found useful for fine-tuning on
other tasks. We study if fine-tuning based on the same GHN is still useful on
novel strong architectures that were published after the GHN had been trained.
We found that for recent architectures such as ConvNeXt, GHN initialization
becomes less useful than for ResNet-50. One potential reason is the increased
distribution shift of novel architectures from those used to train the GHN. We
also found that the predicted parameters lack the diversity necessary to
successfully fine-tune parameters with gradient descent. We alleviate this
limitation by applying simple post-processing techniques to predicted
parameters before fine-tuning them on a target task and improve fine-tuning of
ResNet-50 and ConvNeXt.",1,1,0,0,0,0,0.575667,8.0,0.779565,28
1f8d4153-213d-4bea-9f16-94651ec273b3,Cyberbullying detection across social media platforms via platform-aware adversarial encoding,13,0.279942,0.808821,"Despite the increasing interest in cyberbullying detection, existing efforts
have largely been limited to experiments on a single platform and their
generalisability across different social media platforms have received less
attention. We propose XP-CB, a novel cross-platform framework based on
Transformers and adversarial learning. XP-CB can enhance a Transformer
leveraging unlabelled data from the source and target platforms to come up with
a common representation while preventing platform-specific training. To
validate our proposed framework, we experiment on cyberbullying datasets from
three different platforms through six cross-platform configurations, showing
its effectiveness with both BERT and RoBERTa as the underlying Transformer
models.",0,1,1,0,1,0,0.903771,11.0,0.931014,26
352024c3-1a18-4893-882e-6bbade80fddf,Dataset Distillation by Matching Training Trajectories,198,0.972454,0.992013,"Dataset distillation is the task of synthesizing a small dataset such that a
model trained on the synthetic set will match the test accuracy of the model
trained on the full dataset. In this paper, we propose a new formulation that
optimizes our distilled data to guide networks to a similar state as those
trained on real data across many training steps. Given a network, we train it
for several iterations on our distilled data and optimize the distilled data
with respect to the distance between the synthetically trained parameters and
the parameters trained on real data. To efficiently obtain the initial and
target network parameters for large-scale datasets, we pre-compute and store
training trajectories of expert networks trained on the real dataset. Our
method handily outperforms existing methods and also allows us to distill
higher-resolution visual data.",0,0,0,0,1,0,0.881868,7.0,0.878496,49
570734d0-6b8e-458a-93cc-c8e5f1bfca18,Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks,19,0.165854,0.173341,"Before entering the neural network, a token is generally converted to the
corresponding one-hot representation, which is a discrete distribution of the
vocabulary. Smoothed representation is the probability of candidate tokens
obtained from a pre-trained masked language model, which can be seen as a more
informative substitution to the one-hot representation. We propose an efficient
data augmentation method, termed text smoothing, by converting a sentence from
its one-hot representation to a controllable smoothed representation. We
evaluate text smoothing on different benchmarks in a low-resource regime.
Experimental results show that text smoothing outperforms various mainstream
data augmentation methods by a substantial margin. Moreover, text smoothing can
be combined with those data augmentation methods to achieve better performance.",1,1,0,0,0,0,0.907672,7.0,0.894096,15
84f56ab1-63d9-48f3-81e9-24b14b1d2d26,ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps,24,0.119776,0.363496,"Pre-trained models (PTMs) have become a fundamental backbone for downstream
tasks in natural language processing and computer vision. Despite initial gains
that were obtained by applying generic PTMs to geo-related tasks at Baidu Maps,
a clear performance plateau over time was observed. One of the main reasons for
this plateau is the lack of readily available geographic knowledge in generic
PTMs. To address this problem, in this paper, we present ERNIE-GeoL, which is a
geography-and-language pre-trained model designed and developed for improving
the geo-related tasks at Baidu Maps. ERNIE-GeoL is elaborately designed to
learn a universal representation of geography-language by pre-training on
large-scale data generated from a heterogeneous graph that contains abundant
geographic knowledge. Extensive quantitative and qualitative experiments
conducted on large-scale real-world datasets demonstrate the superiority and
effectiveness of ERNIE-GeoL. ERNIE-GeoL has already been deployed in production
at Baidu Maps since April 2021, which significantly benefits the performance of
various downstream tasks. This demonstrates that ERNIE-GeoL can serve as a
fundamental backbone for a wide range of geo-related tasks.",0,1,0,0,0,0,0.427274,6.0,0.634332,38
09e9b421-0777-4714-813a-81c49c6f9c36,Hierarchical Optimal Transport for Comparing Histopathology Datasets,7,0.0378982,0.242987,"Scarcity of labeled histopathology data limits the applicability of deep
learning methods to under-profiled cancer types and labels. Transfer learning
allows researchers to overcome the limitations of small datasets by
pre-training machine learning models on larger datasets similar to the small
target dataset. However, similarity between datasets is often determined
heuristically. In this paper, we propose a principled notion of distance
between histopathology datasets based on a hierarchical generalization of
optimal transport distances. Our method does not require any training, is
agnostic to model type, and preserves much of the hierarchical structure in
histopathology datasets imposed by tiling. We apply our method to H&E stained
slides from The Cancer Genome Atlas from six different cancer types. We show
that our method outperforms a baseline distance in a cancer-type prediction
task. Our results also show that our optimal transport distance predicts
difficulty of transferability in a tumor vs.normal prediction setting.",0,0,0,0,0,0,0.113164,9.0,0.585677,28
ee317e08-ef11-4090-bfa1-ccacfbc33fe6,Improving Policy Learning via Language Dynamics Distillation,11,0.0251469,0.397633,"Recent work has shown that augmenting environments with language descriptions
improves policy learning. However, for environments with complex language
abstractions, learning how to ground language to observations is difficult due
to sparse, delayed rewards. We propose Language Dynamics Distillation (LDD),
which pretrains a model to predict environment dynamics given demonstrations
with language descriptions, and then fine-tunes these language-aware pretrained
representations via reinforcement learning (RL). In this way, the model is
trained to both maximize expected reward and retain knowledge about how
language relates to environment dynamics. On SILG, a benchmark of five tasks
with language descriptions that evaluate distinct generalization challenges on
unseen environments (NetHack, ALFWorld, RTFM, Messenger, and Touchdown), LDD
outperforms tabula-rasa RL, VAE pretraining, and methods that learn from
unlabeled demonstrations in inverse RL and reward shaping with pretrained
experts. In our analyses, we show that language descriptions in demonstrations
improve sample-efficiency and generalization across environments, and that
dynamics modelling with expert demonstrations is more effective than with
non-experts.",1,1,0,0,0,0,0.0271389,9.0,0.421944,31
5864bb41-0222-4c96-9778-a1c7917eb805,HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising,29,0.185105,0.989692,"The paper presents a novel approach for vector-floorplan generation via a
diffusion model, which denoises 2D coordinates of room/door corners with two
inference objectives: 1) a single-step noise as the continuous quantity to
precisely invert the continuous forward process; and 2) the final 2D coordinate
as the discrete quantity to establish geometric incident relationships such as
parallelism, orthogonality, and corner-sharing. Our task is graph-conditioned
floorplan generation, a common workflow in floorplan design. We represent a
floorplan as 1D polygonal loops, each of which corresponds to a room or a door.
Our diffusion model employs a Transformer architecture at the core, which
controls the attention masks based on the input graph-constraint and directly
generates vector-graphics floorplans via a discrete and continuous denoising
process. We have evaluated our approach on RPLAN dataset. The proposed approach
makes significant improvements in all the metrics against the state-of-the-art
with significant margins, while being capable of generating non-Manhattan
structures and controlling the exact number of corners per room. A project
website with supplementary video and document is here
https://aminshabani.github.io/housediffusion.",1,1,0,0,1,0,0.77758,4.0,0.699538,52
9dc49c8d-b0c7-473f-847a-d5b7abd11f5f,Experiencer-Specific Emotion and Appraisal Prediction,4,0.157098,0.685414,"Emotion classification in NLP assigns emotions to texts, such as sentences or
paragraphs. With texts like ""I felt guilty when he cried"", focusing on the
sentence level disregards the standpoint of each participant in the situation:
the writer (""I"") and the other entity (""he"") could in fact have different
affective states. The emotions of different entities have been considered only
partially in emotion semantic role labeling, a task that relates semantic roles
to emotion cue words. Proposing a related task, we narrow the focus on the
experiencers of events, and assign an emotion (if any holds) to each of them.
To this end, we represent each emotion both categorically and with appraisal
variables, as a psychological access to explaining why a person develops a
particular emotion. On an event description corpus, our experiencer-aware
models of emotions and appraisals outperform the experiencer-agnostic
baselines, showing that disregarding event participants is an
oversimplification for the emotion detection task.",1,0,1,0,0,0,0.374475,16.0,0.85211,43
be6801a7-98eb-4448-8bc1-eef5ca0027b7,Distance-Aware Occlusion Detection with Focused Attention,3,0.111373,0.218159,"For humans, understanding the relationships between objects using visual
signals is intuitive. For artificial intelligence, however, this task remains
challenging. Researchers have made significant progress studying semantic
relationship detection, such as human-object interaction detection and visual
relationship detection. We take the study of visual relationships a step
further from semantic to geometric. In specific, we predict relative occlusion
and relative distance relationships. However, detecting these relationships
from a single image is challenging. Enforcing focused attention to
task-specific regions plays a critical role in successfully detecting these
relationships. In this work, (1) we propose a novel three-decoder architecture
as the infrastructure for focused attention; 2) we use the generalized
intersection box prediction task to effectively guide our model to focus on
occlusion-specific regions; 3) our model achieves a new state-of-the-art
performance on distance-aware relationship detection. Specifically, our model
increases the distance F1-score from 33.8% to 38.6% and boosts the occlusion
F1-score from 34.4% to 41.2%. Our code is publicly available.",1,1,1,0,1,0,0.955605,8.0,0.940834,33
e3448b12-57ea-47fb-ae5b-08727bda927a,Semi-Supervised Single-View 3D Reconstruction via Prototype Shape Priors,12,0.171473,0.722299,"The performance of existing single-view 3D reconstruction methods heavily
relies on large-scale 3D annotations. However, such annotations are tedious and
expensive to collect. Semi-supervised learning serves as an alternative way to
mitigate the need for manual labels, but remains unexplored in 3D
reconstruction. Inspired by the recent success of semi-supervised image
classification tasks, we propose SSP3D, a semi-supervised framework for 3D
reconstruction. In particular, we introduce an attention-guided prototype shape
prior module for guiding realistic object reconstruction. We further introduce
a discriminator-guided module to incentivize better shape generation, as well
as a regularizer to tolerate noisy training samples. On the ShapeNet benchmark,
the proposed approach outperforms previous supervised methods by clear margins
under various labeling ratios, (i.e., 1%, 5% , 10% and 20%). Moreover, our
approach also performs well when transferring to real-world Pix3D datasets
under labeling ratios of 10%. We also demonstrate our method could transfer to
novel categories with few novel supervised data. Experiments on the popular
ShapeNet dataset show that our method outperforms the zero-shot baseline by
over 12% and we also perform rigorous ablations and analysis to validate our
approach.",1,1,1,0,1,0,0.61999,9.0,0.817509,47
8c3c8d4a-c29b-4bb7-a8d6-52b3ffdeb6aa,ASR Error Correction with Constrained Decoding on Operation Prediction,6,0.0498047,0.510625,"Error correction techniques remain effective to refine outputs from automatic
speech recognition (ASR) models. Existing end-to-end error correction methods
based on an encoder-decoder architecture process all tokens in the decoding
phase, creating undesirable latency. In this paper, we propose an ASR error
correction method utilizing the predictions of correction operations. More
specifically, we construct a predictor between the encoder and the decoder to
learn if a token should be kept (""K""), deleted (""D""), or changed (""C"") to
restrict decoding to only part of the input sequence embeddings (the ""C""
tokens) for fast inference. Experiments on three public datasets demonstrate
the effectiveness of the proposed approach in reducing the latency of the
decoding process in ASR correction. It enhances the inference speed by at least
three times (3.4 and 5.7 times) while maintaining the same level of accuracy
(with WER reductions of 0.53% and 1.69% respectively) for our two proposed
models compared to a solid encoder-decoder baseline. In the meantime, we
produce and release a benchmark dataset contributing to the ASR error
correction community to foster research along this line.",1,1,0,1,0,0,0.287112,6.0,0.551188,27
139c1a2d-8408-4a36-b4fe-b0d9ec0cff14,VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting,34,0.508577,0.839875,"We propose VoLux-GAN, a generative framework to synthesize 3D-aware faces
with convincing relighting. Our main contribution is a volumetric HDRI
relighting method that can efficiently accumulate albedo, diffuse and specular
lighting contributions along each 3D ray for any desired HDR environmental map.
Additionally, we show the importance of supervising the image decomposition
process using multiple discriminators. In particular, we propose a data
augmentation technique that leverages recent advances in single image portrait
relighting to enforce consistent geometry, albedo, diffuse and specular
components. Multiple experiments and comparisons with other generative
frameworks show how our model is a step forward towards photorealistic
relightable 3D generative models.",0,1,0,0,0,0,0.917713,5.0,0.861174,80
066d7de0-afac-46db-aa67-20b361be152f,Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities,14,0.875808,0.639195,"As for other forms of AI, speech recognition has recently been examined with
respect to performance disparities across different user cohorts. One approach
to achieve fairness in speech recognition is to (1) identify speaker cohorts
that suffer from subpar performance and (2) apply fairness mitigation measures
targeting the cohorts discovered. In this paper, we report on initial findings
with both discovery and mitigation of performance disparities using data from a
product-scale AI assistant speech recognition system. We compare cohort
discovery based on geographic and demographic information to a more scalable
method that groups speakers without human labels, using speaker embedding
technology. For fairness mitigation, we find that oversampling of
underrepresented cohorts, as well as modeling speaker cohort membership by
additional input variables, reduces the gap between top- and bottom-performing
cohorts, without deteriorating overall recognition accuracy.",0,1,0,0,0,0,0.984037,7.0,0.97295,25
5f3ef8ff-dd06-4c16-90b8-545d0de0c37f,Semeval-2022 Task 1: CODWOE -- Comparing Dictionaries and Word Embeddings,24,0.140828,0.908236,"Word embeddings have advanced the state of the art in NLP across numerous
tasks. Understanding the contents of dense neural representations is of utmost
interest to the computational semantics community. We propose to focus on
relating these opaque word vectors with human-readable definitions, as found in
dictionaries. This problem naturally divides into two subtasks: converting
definitions into embeddings, and converting embeddings into definitions. This
task was conducted in a multilingual setting, using comparable sets of
embeddings trained homogeneously.",1,0,1,1,0,0,0.03316,8.0,0.375121,66
8c711137-50fa-4e4d-ba02-5e6b66af0cf9,Soft-Labeled Contrastive Pre-training for Function-level Code Representation,10,0.023647,0.315772,"Code contrastive pre-training has recently achieved significant progress on
code-related tasks. In this paper, we present \textbf{SCodeR}, a
\textbf{S}oft-labeled contrastive pre-training framework with two positive
sample construction methods to learn functional-level \textbf{Code}
\textbf{R}epresentation. Considering the relevance between codes in a
large-scale code corpus, the soft-labeled contrastive pre-training can obtain
fine-grained soft-labels through an iterative adversarial manner and use them
to learn better code representation. The positive sample construction is
another key for contrastive pre-training. Previous works use
transformation-based methods like variable renaming to generate semantically
equal positive codes. However, they usually result in the generated code with a
highly similar surface form, and thus mislead the model to focus on superficial
code structure instead of code semantics. To encourage SCodeR to capture
semantic information from the code, we utilize code comments and abstract
syntax sub-trees of the code to build positive samples. We conduct experiments
on four code-related tasks over seven datasets. Extensive experimental results
show that SCodeR achieves new state-of-the-art performance on all of them,
which illustrates the effectiveness of the proposed pre-training method.",1,1,0,0,1,0,0.403422,5.0,0.545994,45
5a840b8c-7d81-4364-9b62-0d9cd14606e7,New wrapper method based on normalized mutual information for dimension reduction and classification of hyperspectral images,7,0.0507043,0.179298,"Feature selection is one of the most important problems in hyperspectral
images classification. It consists to choose the most informative bands from
the entire set of input datasets and discard the noisy, redundant and
irrelevant ones. In this context, we propose a new wrapper method based on
normalized mutual information (NMI) and error probability (PE) using support
vector machine (SVM) to reduce the dimensionality of the used hyperspectral
images and increase the classification efficiency. The experiments have been
performed on two challenging hyperspectral benchmarks datasets captured by the
NASA's Airborne Visible/Infrared Imaging Spectrometer Sensor (AVIRIS). Several
metrics had been calculated to evaluate the performance of the proposed
algorithm. The obtained results prove that our method can increase the
classification performance and provide an accurate thematic map in comparison
with other reproduced algorithms. This method may be improved for more
classification efficiency. Keywords-Feature selection, hyperspectral images,
classification, wrapper, normalized mutual information, support vector machine.",0,1,0,0,0,0,0.00276185,11.0,0.31819,20
6a673a4a-8e05-4077-99de-39e3f01f39a9,PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization,21,0.564512,0.821903,"Few-shot abstractive summarization has become a challenging task in natural
language generation. To support it, we designed a novel soft prompts
architecture coupled with a prompt pre-training plus fine-tuning paradigm that
is effective and tunes only extremely light parameters. The soft prompts
include continuous input embeddings across an encoder and a decoder to fit the
structure of the generation models. Importantly, a novel inner-prompt placed in
the text is introduced to capture document-level information. The aim is to
devote attention to understanding the document that better prompts the model to
generate document-related content. The first step in the summarization
procedure is to conduct prompt pre-training with self-supervised pseudo-data.
This teaches the model basic summarizing capabilities. The model is then
fine-tuned with few-shot examples. Experimental results on the CNN/DailyMail
and XSum datasets show that our method, with only 0.1% of the parameters,
outperforms full-model tuning where all model parameters are tuned. It also
surpasses Prompt Tuning by a large margin and delivers competitive results
against Prefix-Tuning with 3% of the parameters.",0,1,0,0,1,0,0.982284,5.0,0.957028,40
37694b8b-8985-480e-8e0e-7b3810662312,Policy Optimization over General State and Action Spaces,7,0.075312,0.397804,"Reinforcement learning (RL) problems over general state and action spaces are
notoriously challenging. In contrast to the tableau setting, one can not
enumerate all the states and then iteratively update the policies for each
state. This prevents the application of many well-studied RL methods especially
those with provable convergence guarantees. In this paper, we first present a
substantial generalization of the recently developed policy mirror descent
method to deal with general state and action spaces. We introduce new
approaches to incorporate function approximation into this method, so that we
do not need to use explicit policy parameterization at all. Moreover, we
present a novel policy dual averaging method for which possibly simpler
function approximation techniques can be applied. We establish linear
convergence rate to global optimality or sublinear convergence to stationarity
for these methods applied to solve different classes of RL problems under exact
policy evaluation. We then define proper notions of the approximation errors
for policy evaluation and investigate their impact on the convergence of these
methods applied to general-state RL problems with either finite-action or
continuous-action spaces. To the best of our knowledge, the development of
these algorithmic frameworks as well as their convergence analysis appear to be
new in the literature.",0,0,0,0,0,1,0.514439,7.0,0.723637,33
04f7291b-2b99-4b87-a775-6dfca5be10c1,On the Paradox of Learning to Reason from Data,58,0.414073,0.787685,"Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be
trained end-to-end to solve logical reasoning problems presented in natural
language? We attempt to answer this question in a confined problem space where
there exists a set of parameters that perfectly simulates logical reasoning. We
make observations that seem to contradict each other: BERT attains near-perfect
accuracy on in-distribution test examples while failing to generalize to other
data distributions over the exact same problem space. Our study provides an
explanation for this paradox: instead of learning to emulate the correct
reasoning function, BERT has in fact learned statistical features that
inherently exist in logical reasoning problems. We also show that it is
infeasible to jointly remove statistical features from data, illustrating the
difficulty of learning to reason in general. Our result naturally extends to
other neural models and unveils the fundamental difference between learning to
reason and learning to achieve high performance on NLP benchmarks using
statistical features.",1,0,0,0,0,0,0.734474,6.0,0.778793,54
2fcd7ee0-f16d-424a-81e1-6845b16b2551,Feeding What You Need by Understanding What You Learned,4,0.0686223,0.493873,"Machine Reading Comprehension (MRC) reveals the ability to understand a given
text passage and answer questions based on it. Existing research works in MRC
rely heavily on large-size models and corpus to improve the performance
evaluated by metrics such as Exact Match ($EM$) and $F_1$. However, such a
paradigm lacks sufficient interpretation to model capability and can not
efficiently train a model with a large corpus. In this paper, we argue that a
deep understanding of model capabilities and data properties can help us feed a
model with appropriate training data based on its learning status.
Specifically, we design an MRC capability assessment framework that assesses
model capabilities in an explainable and multi-dimensional manner. Based on it,
we further uncover and disentangle the connections between various data
properties and model performance. Finally, to verify the effectiveness of the
proposed MRC capability assessment framework, we incorporate it into a
curriculum learning pipeline and devise a Capability Boundary Breakthrough
Curriculum (CBBC) strategy, which performs a model capability-based training to
maximize the data value and improve training efficiency. Extensive experiments
demonstrate that our approach significantly improves performance, achieving up
to an 11.22% / 8.71% improvement of $EM$ / $F_1$ on MRC tasks.",0,0,0,0,0,0,0.830125,9.0,0.884785,58
2428cee7-49bd-4c32-ac7b-c1545d6a768d,Debiased Contrastive Learning of Unsupervised Sentence Representations,69,0.229144,0.996586,"Recently, contrastive learning has been shown to be effective in improving
pre-trained language models (PLM) to derive high-quality sentence
representations. It aims to pull close positive examples to enhance the
alignment while push apart irrelevant negatives for the uniformity of the whole
representation space. However, previous works mostly adopt in-batch negatives
or sample from training data at random. Such a way may cause the sampling bias
that improper negatives (e.g. false negatives and anisotropy representations)
are used to learn sentence representations, which will hurt the uniformity of
the representation space. To address it, we present a new framework
\textbf{DCLR} (\underline{D}ebiased \underline{C}ontrastive
\underline{L}earning of unsupervised sentence \underline{R}epresentations) to
alleviate the influence of these improper negatives. In DCLR, we design an
instance weighting method to punish false negatives and generate noise-based
negatives to guarantee the uniformity of the representation space. Experiments
on seven semantic textual similarity tasks show that our approach is more
effective than competitive baselines. Our code and data are publicly available
at the link: \textcolor{blue}{\url{https://github.com/RUCAIBox/DCLR}}.",1,0,1,0,0,0,0.474847,9.0,0.77229,56
a4f4ae41-16e2-4fac-aad8-fee4d98cd2b1,Multi-class Token Transformer for Weakly Supervised Semantic Segmentation,131,0.965638,0.998657,"This paper proposes a new transformer-based framework to learn class-specific
object localization maps as pseudo labels for weakly supervised semantic
segmentation (WSSS). Inspired by the fact that the attended regions of the
one-class token in the standard vision transformer can be leveraged to form a
class-agnostic localization map, we investigate if the transformer model can
also effectively capture class-specific attention for more discriminative
object localization by learning multiple class tokens within the transformer.
To this end, we propose a Multi-class Token Transformer, termed as MCTformer,
which uses multiple class tokens to learn interactions between the class tokens
and the patch tokens. The proposed MCTformer can successfully produce
class-discriminative object localization maps from class-to-patch attentions
corresponding to different class tokens. We also propose to use a patch-level
pairwise affinity, which is extracted from the patch-to-patch transformer
attention, to further refine the localization maps. Moreover, the proposed
framework is shown to fully complement the Class Activation Mapping (CAM)
method, leading to remarkably superior WSSS results on the PASCAL VOC and MS
COCO datasets. These results underline the importance of the class token for
WSSS.",0,1,0,0,0,0,0.981354,4.0,0.943096,52
1e39df4a-1f77-46ed-8076-17392fb7df47,Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics,31,0.0,0.635434,"Recent work incorporates pre-trained word embeddings such as BERT embeddings
into Neural Topic Models (NTMs), generating highly coherent topics. However,
with high-quality contextualized document representations, do we really need
sophisticated neural models to obtain coherent and interpretable topics? In
this paper, we conduct thorough experiments showing that directly clustering
high-quality sentence embeddings with an appropriate word selecting method can
generate more coherent and diverse topics than NTMs, achieving also higher
efficiency and simplicity.",1,1,0,0,0,0,0.12048,6.0,0.389632,28
482e1c70-f2c1-44a6-8720-e7d808229474,Prototypical Verbalizer for Prompt-based Few-shot Tuning,68,0.477935,0.683766,"Prompt-based tuning for pre-trained language models (PLMs) has shown its
effectiveness in few-shot learning. Typically, prompt-based tuning wraps the
input text into a cloze question. To make predictions, the model maps the
output words to labels via a verbalizer, which is either manually designed or
automatically built. However, manual verbalizers heavily depend on
domain-specific prior knowledge and human efforts, while finding appropriate
label words automatically still remains challenging.In this work, we propose
the prototypical verbalizer (ProtoVerb) which is built directly from training
data. Specifically, ProtoVerb learns prototype vectors as verbalizers by
contrastive learning. In this way, the prototypes summarize training instances
and are able to enclose rich class-level semantics. We conduct experiments on
both topic classification and entity typing tasks, and the results demonstrate
that ProtoVerb significantly outperforms current automatic verbalizers,
especially when training data is extremely scarce. More surprisingly, ProtoVerb
consistently boosts prompt-based tuning even on untuned PLMs, indicating an
elegant non-tuning way to utilize PLMs. Our codes are avaliable at
https://github.com/thunlp/OpenPrompt.",1,1,0,0,0,1,0.957409,5.0,0.907981,46
c7acec7c-0024-4f6f-9424-9cce792342b0,Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective,22,0.529702,0.959124,"Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called ""neural texts""), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.",0,0,0,0,0,0,0.794135,5.0,0.769665,133
ccf7c426-14f3-4402-890c-640ba653abd8,Deep Learning Architecture for Automatic Essay Scoring,2,0.0409417,0.169064,"Automatic evaluation of essay (AES) and also called automatic essay scoring
has become a severe problem due to the rise of online learning and evaluation
platforms such as Coursera, Udemy, Khan academy, and so on. Researchers have
recently proposed many techniques for automatic evaluation. However, many of
these techniques use hand-crafted features and thus are limited from the
feature representation point of view. Deep learning has emerged as a new
paradigm in machine learning which can exploit the vast data and identify the
features useful for essay evaluation. To this end, we propose a novel
architecture based on recurrent networks (RNN) and convolution neural network
(CNN). In the proposed architecture, the multichannel convolutional layer
learns and captures the contextual features of the word n-gram from the word
embedding vectors and the essential semantic concepts to form the feature
vector at essay level using max-pooling operation. A variant of RNN called
Bi-gated recurrent unit (BGRU) is used to access both previous and subsequent
contextual representations. The experiment was carried out on eight data sets
available on Kaggle for the task of AES. The experimental results show that our
proposed system achieves significantly higher grading accuracy than other deep
learning-based AES systems and also other state-of-the-art AES systems.",0,1,0,0,1,0,0.0429869,16.0,0.704099,64
09083b19-34bf-4db9-8f21-b21134e71225,Fine-Grained Object Classification via Self-Supervised Pose Alignment,26,0.278983,0.823612,"Semantic patterns of fine-grained objects are determined by subtle appearance
difference of local parts, which thus inspires a number of part-based methods.
However, due to uncontrollable object poses in images, distinctive details
carried by local regions can be spatially distributed or even self-occluded,
leading to a large variation on object representation. For discounting pose
variations, this paper proposes to learn a novel graph based object
representation to reveal a global configuration of local parts for
self-supervised pose alignment across classes, which is employed as an
auxiliary feature regularization on a deep representation learning
network.Moreover, a coarse-to-fine supervision together with the proposed
pose-insensitive constraint on shallow-to-deep sub-networks encourages
discriminative features in a curriculum learning manner. We evaluate our method
on three popular fine-grained object classification benchmarks, consistently
achieving the state-of-the-art performance. Source codes are available at
https://github.com/yangxh11/P2P-Net.",1,1,0,0,1,0,0.742746,8.0,0.837044,49
9a94c941-02a5-4634-b42a-1b07171538f5,Penalized Proximal Policy Optimization for Safe Reinforcement Learning,33,0.534066,0.774289,"Safe reinforcement learning aims to learn the optimal policy while satisfying
safety constraints, which is essential in real-world applications. However,
current algorithms still struggle for efficient policy updates with hard
constraint satisfaction. In this paper, we propose Penalized Proximal Policy
Optimization (P3O), which solves the cumbersome constrained policy iteration
via a single minimization of an equivalent unconstrained problem. Specifically,
P3O utilizes a simple-yet-effective penalty function to eliminate cost
constraints and removes the trust-region constraint by the clipped surrogate
objective. We theoretically prove the exactness of the proposed method with a
finite penalty factor and provide a worst-case analysis for approximate error
when evaluated on sample trajectories. Moreover, we extend P3O to more
challenging multi-constraint and multi-agent scenarios which are less studied
in previous work. Extensive experiments show that P3O outperforms
state-of-the-art algorithms with respect to both reward improvement and
constraint satisfaction on a set of constrained locomotive tasks.",1,0,0,0,1,0,0.981229,10.0,0.97707,28
1399643b-ebd0-4466-875d-ae6c1f58c9ad,That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory,3,0.158874,0.335971,"The evolution of language follows the rule of gradual change. Grammar,
vocabulary, and lexical semantic shifts take place over time, resulting in a
diachronic linguistic gap. As such, a considerable amount of texts are written
in languages of different eras, which creates obstacles for natural language
processing tasks, such as word segmentation and machine translation. Although
the Chinese language has a long history, previous Chinese natural language
processing research has primarily focused on tasks within a specific era.
Therefore, we propose a cross-era learning framework for Chinese word
segmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to
incorporate era-specific linguistic knowledge. Experiments on four corpora from
different eras show that the performance of each corpus significantly improves.
Further analyses also demonstrate that the SM can effectively integrate the
knowledge of the eras into the neural network.",1,1,1,0,0,0,0.527636,9.0,0.78921,33
829586c3-736a-4172-9bd1-24ac7fb187f0,Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent Recognition and Question Answering Architecture,4,0.172169,0.137616,"As artificial intelligence (AI) becomes a prominent part of modern life, AI
literacy is becoming important for all citizens, not just those in technology
careers. Previous research in AI education materials has largely focused on the
introduction of terminology as well as AI use cases and ethics, but few allow
students to learn by creating their own machine learning models. Therefore,
there is a need for enriching AI educational tools with more adaptable and
flexible platforms for interested educators with any level of technical
experience to utilize within their teaching material. As such, we propose the
development of an open-source tool (Build-a-Bot) for students and teachers to
not only create their own transformer-based chatbots based on their own course
material, but also learn the fundamentals of AI through the model creation
process. The primary concern of this paper is the creation of an interface for
students to learn the principles of artificial intelligence by using a natural
language pipeline to train a customized model to answer questions based on
their own school curriculums. The model uses contexts given by their
instructor, such as chapters of a textbook, to answer questions and is deployed
on an interactive chatbot/voice agent. The pipeline teaches students data
collection, data augmentation, intent recognition, and question answering by
having them work through each of these processes while creating their AI agent,
diverging from previous chatbot work where students and teachers use the bots
as black-boxes with no abilities for customization or the bots lack AI
capabilities, with the majority of dialogue scripts being rule-based. In
addition, our tool is designed to make each step of this pipeline intuitive for
students at a middle-school level. Further work primarily lies in providing our
tool to schools and seeking student and teacher evaluations.",0,1,0,0,0,0,0.834883,7.0,0.854137,30
7ff13cd3-97ee-465a-b23d-1731d636fa1c,Cross-Domain Cross-Set Few-Shot Learning via Learning Compact and Aligned Representations,5,0.0822358,0.362147,"Few-shot learning (FSL) aims to recognize novel queries with only a few
support samples through leveraging prior knowledge from a base dataset. In this
paper, we consider the domain shift problem in FSL and aim to address the
domain gap between the support set and the query set. Different from previous
cross-domain FSL work (CD-FSL) that considers the domain shift between base and
novel classes, the new problem, termed cross-domain cross-set FSL (CDSC-FSL),
requires few-shot learners not only to adapt to the new domain, but also to be
consistent between different domains within each novel class. To this end, we
propose a novel approach, namely stabPA, to learn prototypical compact and
cross-domain aligned representations, so that the domain shift and few-shot
learning can be addressed simultaneously. We evaluate our approach on two new
CDCS-FSL benchmarks built from the DomainNet and Office-Home datasets
respectively. Remarkably, our approach outperforms multiple elaborated
baselines by a large margin, e.g., improving 5-shot accuracy by 6.0 points on
average on DomainNet. Code is available at
https://github.com/WentaoChen0813/CDCS-FSL",1,1,1,0,1,0,0.925807,8.0,0.918311,53
1d2d0ed6-6878-4cae-bcf5-831bcab7fb24,Moving Window Regression: A Novel Approach to Ordinal Regression,30,0.205802,0.624344,"A novel ordinal regression algorithm, called moving window regression (MWR),
is proposed in this paper. First, we propose the notion of relative rank
($\rho$-rank), which is a new order representation scheme for input and
reference instances. Second, we develop global and local relative regressors
($\rho$-regressors) to predict $\rho$-ranks within entire and specific rank
ranges, respectively. Third, we refine an initial rank estimate iteratively by
selecting two reference instances to form a search window and then estimating
the $\rho$-rank within the window. Extensive experiments results show that the
proposed algorithm achieves the state-of-the-art performances on various
benchmark datasets for facial age estimation and historical color image
classification. The codes are available at https://github.com/nhshin-mcl/MWR.",0,0,0,0,1,0,0.0914076,11.0,0.640517,50
35f0103e-8db3-4139-b407-c83393a082fc,Bubble identification from images with machine learning methods,22,0.0187493,0.424227,"An automated and reliable processing of bubbly flow images is highly needed
to analyse large data sets of comprehensive experimental series. A particular
difficulty arises due to overlapping bubble projections in recorded images,
which highly complicates the identification of individual bubbles. Recent
approaches focus on the use of deep learning algorithms for this task and have
already proven the high potential of such techniques. The main difficulties are
the capability to handle different image conditions, higher gas volume
fractions and a proper reconstruction of the hidden segment of a partly
occluded bubble. In the present work, we try to tackle these points by testing
three different methods based on Convolutional Neural Networks (CNNs) for the
two former and two individual approaches that can be used subsequently to
address the latter. To validate our methodology, we created test data sets with
synthetic images that further demonstrate the capabilities as well as
limitations of our combined approach. The generated data, code and trained
models are made accessible to facilitate the use as well as further
developments in the research field of bubble recognition in experimental
images.",0,1,0,0,0,0,0.0187377,7.0,0.203255,29
a6c7e885-bc4f-4f98-83bb-c7f931025213,Transformers are Sample-Efficient World Models,78,0.0927048,0.792866,"Deep reinforcement learning agents are notoriously sample inefficient, which
considerably limits their application to real-world problems. Recently, many
model-based methods have been designed to address this issue, with learning in
the imagination of a world model being one of the most prominent approaches.
However, while virtually unlimited interaction with a simulated environment
sounds appealing, the world model has to be accurate over extended periods of
time. Motivated by the success of Transformers in sequence modeling tasks, we
introduce IRIS, a data-efficient agent that learns in a world model composed of
a discrete autoencoder and an autoregressive Transformer. With the equivalent
of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean
human normalized score of 1.046, and outperforms humans on 10 out of 26 games,
setting a new state of the art for methods without lookahead search. To foster
future research on Transformers and world models for sample-efficient
reinforcement learning, we release our code and models at
https://github.com/eloialonso/iris.",1,1,0,0,1,0,0.432766,6.0,0.63719,64
f893b09c-f526-4d28-8508-4572a4e72def,Transformer-based Approaches for Legal Text Processing,10,0.295022,0.234913,"In this paper, we introduce our approaches using Transformer-based models for
different problems of the COLIEE 2021 automatic legal text processing
competition. Automated processing of legal documents is a challenging task
because of the characteristics of legal documents as well as the limitation of
the amount of data. With our detailed experiments, we found that
Transformer-based pretrained language models can perform well with automated
legal text processing problems with appropriate approaches. We describe in
detail the processing steps for each task such as problem formulation, data
processing and augmentation, pretraining, finetuning. In addition, we introduce
to the community two pretrained models that take advantage of parallel
translations in legal domain, NFSP and NMSP. In which, NFSP achieves the
state-of-the-art result in Task 5 of the competition. Although the paper
focuses on technical reporting, the novelty of its approaches can also be an
useful reference in automated legal document processing using Transformer-based
models.",0,1,0,0,1,0,0.742945,6.0,0.78282,30
a5a0b082-becc-42e1-a5d6-588bb62f38ea,PrivHAR: Recognizing Human Actions From Privacy-preserving Lens,11,0.0534364,0.656398,"The accelerated use of digital cameras prompts an increasing concern about
privacy and security, particularly in applications such as action recognition.
In this paper, we propose an optimizing framework to provide robust visual
privacy protection along the human action recognition pipeline. Our framework
parameterizes the camera lens to successfully degrade the quality of the videos
to inhibit privacy attributes and protect against adversarial attacks while
maintaining relevant features for activity recognition. We validate our
approach with extensive simulations and hardware experiments.",0,1,0,0,0,0,0.160483,9.0,0.627465,61
9fce4f9f-87b1-4896-9e07-76b92666aa69,A Generalist Framework for Panoptic Segmentation of Images and Videos,60,0.140766,0.820693,"Panoptic segmentation assigns semantic and instance ID labels to every pixel
of an image. As permutations of instance IDs are also valid solutions, the task
requires learning of high-dimensional one-to-many mapping. As a result,
state-of-the-art approaches use customized architectures and task-specific loss
functions. We formulate panoptic segmentation as a discrete data generation
problem, without relying on inductive bias of the task. A diffusion model is
proposed to model panoptic masks, with a simple architecture and generic loss
function. By simply adding past predictions as a conditioning signal, our
method is capable of modeling video (in a streaming setting) and thereby learns
to track object instances automatically. With extensive experiments, we
demonstrate that our simple approach can perform competitively to
state-of-the-art specialist methods in similar settings.",0,0,0,0,0,0,0.613272,4.0,0.584826,72
12fd5811-0aee-4ac0-958f-da4e44719496,SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning with Human Object Interactions,1,0.00950527,0.0547102,"Detecting suspicious activities in surveillance videos is a longstanding
problem in real-time surveillance that leads to difficulties in detecting
crimes. Hence, we propose a novel approach for detecting and summarizing
suspicious activities in surveillance videos. We have also created ground truth
summaries for the UCF-Crime video dataset. We modify a pre-existing approach
for this task by leveraging the Human-Object Interaction (HOI) model for the
Visual features in the Bi-Modal Transformer. Further, we validate our approach
against the existing state-of-the-art algorithms for the Dense Video Captioning
task for the ActivityNet Captions dataset. We observe that this formulation for
Dense Captioning performs significantly better than other discussed BMT-based
approaches for BLEU@1, BLEU@2, BLEU@3, BLEU@4, and METEOR. We further perform a
comparative analysis of the dataset and the model to report the findings based
on different NMS thresholds (searched using Genetic Algorithms). Here, our
formulation outperforms all the models for BLEU@1, BLEU@2, BLEU@3, and most
models for BLEU@4 and METEOR falling short of only ADV-INF Global by 25% and
0.5%, respectively.",0,1,0,0,1,0,0.623662,5.0,0.673514,52
261da98e-e62b-4e1f-b74a-57b44e57d99d,Learning Perception-Aware Agile Flight in Cluttered Environments,10,0.600993,0.77599,"Recently, neural control policies have outperformed existing model-based
planning-and-control methods for autonomously navigating quadrotors through
cluttered environments in minimum time. However, they are not perception aware,
a crucial requirement in vision-based navigation due to the camera's limited
field of view and the underactuated nature of a quadrotor. We propose a
learning-based system that achieves perception-aware, agile flight in cluttered
environments. Our method combines imitation learning with reinforcement
learning (RL) by leveraging a privileged learning-by-cheating framework. Using
RL, we first train a perception-aware teacher policy with full-state
information to fly in minimum time through cluttered environments. Then, we use
imitation learning to distill its knowledge into a vision-based student policy
that only perceives the environment via a camera. Our approach tightly couples
perception and control, showing a significant advantage in computation speed
(10 times faster) and success rate. We demonstrate the closed-loop control
performance using hardware-in-the-loop simulation.",0,1,0,0,0,0,0.985424,8.0,0.979047,36
3c24eff8-4b91-47a7-ac69-e6489f3f68b4,Multielement polynomial chaos Kriging-based metamodelling for Bayesian inference of non-smooth systems,4,0.245231,0.695532,"This paper presents a surrogate modelling technique based on domain
partitioning for Bayesian parameter inference of highly nonlinear engineering
models. In order to alleviate the computational burden typically involved in
Bayesian inference applications, a multielement Polynomial Chaos Expansion
based Kriging metamodel is proposed. The developed surrogate model combines in
a piecewise function an array of local Polynomial Chaos based Kriging
metamodels constructed on a finite set of non-overlapping subdomains of the
stochastic input space. Therewith, the presence of non-smoothness in the
response of the forward model (e.g.~ nonlinearities and sparseness) can be
reproduced by the proposed metamodel with minimum computational costs owing to
its local adaptation capabilities. The model parameter inference is conducted
through a Markov chain Monte Carlo approach comprising adaptive exploration and
delayed rejection. The efficiency and accuracy of the proposed approach are
validated through two case studies, including an analytical benchmark and a
numerical case study. The latter relates the partial differential equation
governing the hydrogen diffusion phenomenon of metallic materials in Thermal
Desorption Spectroscopy tests.",0,1,0,0,0,0,0.61188,10.0,0.833552,80
8e7af36a-bcf9-4ffe-a3c3-36659d85b2ce,LEVEN: A Large-Scale Chinese Legal Event Detection Dataset,28,0.343925,0.907219,"Recognizing facts is the most fundamental step in making judgments, hence
detecting events in the legal documents is important to legal case analysis
tasks. However, existing Legal Event Detection (LED) datasets only concern
incomprehensive event types and have limited annotated data, which restricts
the development of LED methods and their downstream applications. To alleviate
these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection
dataset, with 8,116 legal documents and 150,977 human-annotated event mentions
in 108 event types. Not only charge-related events, LEVEN also covers general
events, which are critical for legal case understanding but neglected in
existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and
has dozens of times the data scale of others, which shall significantly promote
the training and evaluation of LED methods. The results of extensive
experiments indicate that LED is challenging and needs further effort.
Moreover, we simply utilize legal events as side information to promote
downstream applications. The method achieves improvements of average 2.2 points
precision in low-resource judgment prediction, and 1.5 points mean average
precision in unsupervised case retrieval, which suggests the fundamentality of
LED. The source code and dataset can be obtained from
https://github.com/thunlp/LEVEN.",1,1,1,1,0,0,0.462436,6.0,0.652278,56
74161517-26ef-4fdb-ab18-10b9e758de33,Continual Inference: A Library for Efficient Online Inference with Deep Neural Networks in PyTorch,3,0.015658,0.347091,"We present Continual Inference, a Python library for implementing Continual
Inference Networks (CINs) in PyTorch, a class of Neural Networks designed
specifically for efficient inference in both online and batch processing
scenarios. We offer a comprehensive introduction and guide to CINs and their
implementation in practice, and provide best-practices and code examples for
composing complex modules for modern Deep Learning. Continual Inference is
readily downloadable via the Python Package Index and at
\url{www.github.com/lukashedegaard/continual-inference}.",1,1,0,1,0,0,0.268894,8.0,0.653705,21
53a960c3-b469-484b-bb52-4a14a1794de9,DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation,39,0.480251,0.987203,"Dialog response generation in open domain is an important research topic
where the main challenge is to generate relevant and diverse responses. In this
paper, we propose a new dialog pre-training framework called DialogVED, which
introduces continuous latent variables into the enhanced encoder-decoder
pre-training framework to increase the relevance and diversity of responses.
With the help of a large dialog corpus (Reddit), we pre-train the model using
the following 4 tasks adopted in language models (LMs) and variational
autoencoders (VAEs): 1) masked language model; 2) response generation; 3)
bag-of-words prediction; and 4) KL divergence reduction. We also add additional
parameters to model the turn structure in dialogs to improve the performance of
the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and
DSTC7-AVSD benchmarks for response generation. Experimental results show that
our model achieves the new state-of-the-art results on all these datasets.",0,1,0,0,1,0,0.828551,8.0,0.869731,54
f60ec051-3a89-4563-a420-84ecfa6ef085,MGFN: Magnitude-Contrastive Glance-and-Focus Network for Weakly-Supervised Video Anomaly Detection,36,0.474251,0.999221,"Weakly supervised detection of anomalies in surveillance videos is a
challenging task. Going beyond existing works that have deficient capabilities
to localize anomalies in long videos, we propose a novel glance and focus
network to effectively integrate spatial-temporal information for accurate
anomaly detection. In addition, we empirically found that existing approaches
that use feature magnitudes to represent the degree of anomalies typically
ignore the effects of scene variations, and hence result in sub-optimal
performance due to the inconsistency of feature magnitudes across scenes. To
address this issue, we propose the Feature Amplification Mechanism and a
Magnitude Contrastive Loss to enhance the discriminativeness of feature
magnitudes for detecting anomalies. Experimental results on two large-scale
benchmarks UCF-Crime and XD-Violence manifest that our method outperforms
state-of-the-art approaches.",1,1,0,0,1,0,0.87825,6.0,0.855875,49
61717a42-3519-4cc0-a81e-3036af985eec,TaSPM: Targeted Sequential Pattern Mining,10,0.178835,0.671764,"Sequential pattern mining (SPM) is an important technique of pattern mining,
which has many applications in reality. Although many efficient sequential
pattern mining algorithms have been proposed, there are few studies can focus
on target sequences. Targeted querying sequential patterns can not only reduce
the number of sequences generated by SPM, but also improve the efficiency of
users in performing pattern analysis. The current algorithms available on
targeted sequence querying are based on specific scenarios and cannot be
generalized to other applications. In this paper, we formulate the problem of
targeted sequential pattern mining and propose a generic framework namely
TaSPM, based on the fast CM-SPAM algorithm. What's more, to improve the
efficiency of TaSPM on large-scale datasets and multiple-items-based sequence
datasets, we propose several pruning strategies to reduce meaningless
operations in mining processes. Totally four pruning strategies are designed in
TaSPM, and hence it can terminate unnecessary pattern extensions quickly and
achieve better performance. Finally, we conduct extensive experiments on
different datasets to compare the existing SPM algorithms with TaSPM.
Experiments show that the novel targeted mining algorithm TaSPM can achieve
faster running time and less memory consumption.",0,1,1,0,0,0,0.00978918,17.0,0.633471,51
a37af22e-315f-4815-93be-ef731b5dc6fa,Integrating Human-in-the-loop into Swarm Learning for Decentralized Fake News Detection,6,0.445094,0.77687,"Social media has become an effective platform to generate and spread fake
news that can mislead people and even distort public opinion. Centralized
methods for fake news detection, however, cannot effectively protect user
privacy during the process of centralized data collection for training models.
Moreover, it cannot fully involve user feedback in the loop of learning
detection models for further enhancing fake news detection. To overcome these
challenges, this paper proposed a novel decentralized method, Human-in-the-loop
Based Swarm Learning (HBSL), to integrate user feedback into the loop of
learning and inference for recognizing fake news without violating user privacy
in a decentralized manner. It consists of distributed nodes that are able to
independently learn and detect fake news on local data. Furthermore, detection
models trained on these nodes can be enhanced through decentralized model
merging. Experimental results demonstrate that the proposed method outperforms
the state-of-the-art decentralized method in regard of detecting fake news on a
benchmark dataset.",0,1,0,0,1,0,0.981605,7.0,0.967967,41
7241c99e-dd2b-482b-bce7-05f6ad984e7b,Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning,16,0.137285,0.529104,"Indiscriminate data poisoning attacks are quite effective against supervised
learning. However, not much is known about their impact on unsupervised
contrastive learning (CL). This paper is the first to consider indiscriminate
poisoning attacks of contrastive learning. We propose Contrastive Poisoning
(CP), the first effective such attack on CL. We empirically show that
Contrastive Poisoning, not only drastically reduces the performance of CL
algorithms, but also attacks supervised learning models, making it the most
generalizable indiscriminate poisoning attack. We also show that CL algorithms
with a momentum encoder are more robust to indiscriminate poisoning, and
propose a new countermeasure based on matrix completion. Code is available at:
https://github.com/kaiwenzha/contrastive-poisoning.",1,0,1,0,0,0,0.809689,5.0,0.779367,59
6851a42f-93e1-4717-93f5-e7c524ce0e09,PanoFormer: Panorama Transformer for Indoor 360 Depth Estimation,36,0.35782,0.943347,"Existing panoramic depth estimation methods based on convolutional neural
networks (CNNs) focus on removing panoramic distortions, failing to perceive
panoramic structures efficiently due to the fixed receptive field in CNNs. This
paper proposes the panorama transformer (named PanoFormer) to estimate the
depth in panorama images, with tangent patches from spherical domain, learnable
token flows, and panorama specific metrics. In particular, we divide patches on
the spherical tangent domain into tokens to reduce the negative effect of
panoramic distortions. Since the geometric structures are essential for depth
estimation, a self-attention module is redesigned with an additional learnable
token flow. In addition, considering the characteristic of the spherical
domain, we present two panorama-specific metrics to comprehensively evaluate
the panoramic depth estimation models' performance. Extensive experiments
demonstrate that our approach significantly outperforms the state-of-the-art
(SOTA) methods. Furthermore, the proposed method can be effectively extended to
solve semantic panorama segmentation, a similar pixel2pixel task. Code will be
available.",1,1,0,0,1,0,0.418997,7.0,0.682845,44
885c1ae5-8a16-4dc8-920c-e276f6f84e26,Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search,27,0.573246,0.596257,"Nonlinear dynamics is ubiquitous in nature and commonly seen in various
science and engineering disciplines. Distilling analytical expressions that
govern nonlinear dynamics from limited data remains vital but challenging. To
tackle this fundamental issue, we propose a novel Symbolic Physics Learner
(SPL) machine to discover the mathematical structure of nonlinear dynamics. The
key concept is to interpret mathematical operations and system state variables
by computational rules and symbols, establish symbolic reasoning of
mathematical formulas via expression trees, and employ a Monte Carlo tree
search (MCTS) agent to explore optimal expression trees based on measurement
data. The MCTS agent obtains an optimistic selection policy through the
traversal of expression trees, featuring the one that maps to the arithmetic
expression of underlying physics. Salient features of the proposed framework
include search flexibility and enforcement of parsimony for discovered
equations. The efficacy and superiority of the SPL machine are demonstrated by
numerical examples, compared with state-of-the-art baselines.",1,0,0,0,1,0,0.40107,12.0,0.810194,55
8804dc3a-163a-439c-a574-dc671cb883c4,Diverse Plausible 360-Degree Image Outpainting for Efficient 3DCG Background Creation,23,0.562523,0.76693,"We address the problem of generating a 360-degree image from a single image
with a narrow field of view by estimating its surroundings. Previous methods
suffered from overfitting to the training resolution and deterministic
generation. This paper proposes a completion method using a transformer for
scene modeling and novel methods to improve the properties of a 360-degree
image on the output image. Specifically, we use CompletionNets with a
transformer to perform diverse completions and AdjustmentNet to match color,
stitching, and resolution with an input image, enabling inference at any
resolution. To improve the properties of a 360-degree image on an output image,
we also propose WS-perceptual loss and circular inference. Thorough experiments
show that our method outperforms state-of-the-art (SOTA) methods both
qualitatively and quantitatively. For example, compared to SOTA methods, our
method completes images 16 times larger in resolution and achieves 1.7 times
lower Frechet inception distance (FID). Furthermore, we propose a pipeline that
uses the completion results for lighting and background of 3DCG scenes. Our
plausible background completion enables perceptually natural results in the
application of inserting virtual objects with specular surfaces.",0,1,0,0,1,0,0.975061,6.0,0.949424,41
eb6c70b0-083a-4770-8048-d62d6b527cdb,On the Importance of Data Size in Probing Fine-tuned Models,11,0.0113525,0.23669,"Several studies have investigated the reasons behind the effectiveness of
fine-tuning, usually through the lens of probing. However, these studies often
neglect the role of the size of the dataset on which the model is fine-tuned.
In this paper, we highlight the importance of this factor and its undeniable
role in probing performance. We show that the extent of encoded linguistic
knowledge depends on the number of fine-tuning samples. The analysis also
reveals that larger training data mainly affects higher layers, and that the
extent of this change is a factor of the number of iterations updating the
model during fine-tuning rather than the diversity of the training samples.
Finally, we show through a set of experiments that fine-tuning data size
affects the recoverability of the changes made to the model's linguistic
knowledge.",1,0,0,0,0,0,0.193017,6.0,0.475149,24
98b59aa0-dc5f-450a-b69c-e9cc43f1304e,Automated Identification of Eviction Status from Electronic Health Record Notes,8,0.0997199,0.928906,"Objective: Evictions are important social and behavioral determinants of
health. Evictions are associated with a cascade of negative events that can
lead to unemployment, housing insecurity/homelessness, long-term poverty, and
mental health problems. In this study, we developed a natural language
processing system to automatically detect eviction status from electronic
health record (EHR) notes.
  Materials and Methods: We first defined eviction status (eviction presence
and eviction period) and then annotated eviction status in 5000 EHR notes from
the Veterans Health Administration (VHA). We developed a novel model, KIRESH,
that has shown to substantially outperform other state-of-the-art models such
as fine-tuning pre-trained language models like BioBERT and BioClinicalBERT.
Moreover, we designed a novel prompt to further improve the model performance
by using the intrinsic connection between the two sub-tasks of eviction
presence and period prediction. Finally, we used the Temperature Scaling-based
Calibration on our KIRESH-Prompt method to avoid over-confidence issues arising
from the imbalance dataset.
  Results: KIRESH-Prompt substantially outperformed strong baseline models
including fine-tuning the BioClinicalBERT model to achieve 0.74672 MCC, 0.71153
Macro-F1, and 0.83396 Micro-F1 in predicting eviction period and 0.66827 MCC,
0.62734 Macro-F1, and 0.7863 Micro-F1 in predicting eviction presence. We also
conducted additional experiments on a benchmark social determinants of health
(SBDH) dataset to demonstrate the generalizability of our methods.
  Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction
status classification. We plan to deploy KIRESH-Prompt to the VHA EHRs as an
eviction surveillance system to help address the US Veterans' housing
insecurity.",0,1,0,1,1,0,0.18756,8.0,0.60237,63
4a2c18e8-bc97-4498-a786-1d0d3081e7ef,Adversarial Domain Adaptation for Action Recognition Around the Clock,1,0.00604067,0.100333,"Due to the numerous potential applications in visual surveillance and
nighttime driving, recognizing human action in low-light conditions remains a
difficult problem in computer vision. Existing methods separate action
recognition and dark enhancement into two distinct steps to accomplish this
task. However, isolating the recognition and enhancement impedes end-to-end
learning of the space-time representation for video action classification. This
paper presents a domain adaptation-based action recognition approach that uses
adversarial learning in cross-domain settings to learn cross-domain action
recognition. Supervised learning can train it on a large amount of labeled data
from the source domain (daytime action sequences). However, it uses deep domain
invariant features to perform unsupervised learning on many unlabelled data
from the target domain (night-time action sequences). The resulting augmented
model, named 3D-DiNet can be trained using standard backpropagation with an
additional layer. It achieves SOTA performance on InFAR and XD145 actions
datasets.",1,1,0,0,1,0,0.0309121,9.0,0.436624,31
378e4ced-2043-4f39-9b54-2fb5711318e6,Look to the Right: Mitigating Relative Position Bias in Extractive Question Answering,6,0.0431245,0.19095,"Extractive question answering (QA) models tend to exploit spurious
correlations to make predictions when a training set has unintended biases.
This tendency results in models not being generalizable to examples where the
correlations do not hold. Determining the spurious correlations QA models can
exploit is crucial in building generalizable QA models in real-world
applications; moreover, a method needs to be developed that prevents these
models from learning the spurious correlations even when a training set is
biased. In this study, we discovered that the relative position of an answer,
which is defined as the relative distance from an answer span to the closest
question-context overlap word, can be exploited by QA models as superficial
cues for making predictions. Specifically, we find that when the relative
positions in a training set are biased, the performance on examples with
relative positions unseen during training is significantly degraded. To
mitigate the performance degradation for unseen relative positions, we propose
an ensemble-based debiasing method that does not require prior knowledge about
the distribution of relative positions. We demonstrate that the proposed method
mitigates the models' reliance on relative positions using the biased and full
SQuAD dataset. We hope that this study can help enhance the generalization
ability of QA models in real-world applications.",1,1,0,0,0,0,0.552252,9.0,0.796868,19
c1835ea1-f54b-483f-9d9c-09f21d564ef9,Power-law Scaling to Assist with Key Challenges in Artificial Intelligence,12,0.0,0.0358813,"Power-law scaling, a central concept in critical phenomena, is found to be
useful in deep learning, where optimized test errors on handwritten digit
examples converge as a power-law to zero with database size. For rapid decision
making with one training epoch, each example is presented only once to the
trained network, the power-law exponent increased with the number of hidden
layers. For the largest dataset, the obtained test error was estimated to be in
the proximity of state-of-the-art algorithms for large epoch numbers. Power-law
scaling assists with key challenges found in current artificial intelligence
applications and facilitates an a priori dataset size estimation to achieve a
desired test accuracy. It establishes a benchmark for measuring training
complexity and a quantitative hierarchy of machine learning tasks and
algorithms.",0,0,0,0,0,0,0.00653963,10.0,0.336397,35
8f09970d-1bc0-4e72-8a85-616b0947e97e,YOLO-FaceV2: A Scale and Occlusion Aware Face Detector,19,0.0813296,0.711309,"In recent years, face detection algorithms based on deep learning have made
great progress. These algorithms can be generally divided into two categories,
i.e. two-stage detector like Faster R-CNN and one-stage detector like YOLO.
Because of the better balance between accuracy and speed, one-stage detectors
have been widely used in many applications. In this paper, we propose a
real-time face detector based on the one-stage detector YOLOv5, named
YOLO-FaceV2. We design a Receptive Field Enhancement module called RFE to
enhance receptive field of small face, and use NWD Loss to make up for the
sensitivity of IoU to the location deviation of tiny objects. For face
occlusion, we present an attention module named SEAM and introduce Repulsion
Loss to solve it. Moreover, we use a weight function Slide to solve the
imbalance between easy and hard samples and use the information of the
effective receptive field to design the anchor. The experimental results on
WiderFace dataset show that our face detector outperforms YOLO and its variants
can be find in all easy, medium and hard subsets. Source code in
https://github.com/Krasjet-Yu/YOLO-FaceV2",1,1,0,0,1,0,0.409644,9.0,0.750007,46
2bc7e5d6-ca2d-452b-812b-8dfa35f33326,Multiscale Analysis for Improving Texture Classification,5,0.0314564,0.562578,"Information from an image occurs over multiple and distinct spatial scales.
Image pyramid multiresolution representations are a useful data structure for
image analysis and manipulation over a spectrum of spatial scales. This paper
employs the Gaussian-Laplacian pyramid to treat different spatial frequency
bands of a texture separately. First, we generate three images corresponding to
three levels of the Gaussian-Laplacian pyramid for an input image to capture
intrinsic details. Then we aggregate features extracted from gray and color
texture images using bio-inspired texture descriptors, information-theoretic
measures, gray-level co-occurrence matrix features, and Haralick statistical
features into a single feature vector. Such an aggregation aims at producing
features that characterize textures to their maximum extent, unlike employing
each descriptor separately, which may lose some relevant textural information
and reduce the classification performance. The experimental results on texture
and histopathologic image datasets have shown the advantages of the proposed
method compared to state-of-the-art approaches. Such findings emphasize the
importance of multiscale image analysis and corroborate that the descriptors
mentioned above are complementary.",0,1,0,0,1,0,0.0346307,11.0,0.549556,46
6843514b-73f2-4dd1-81ab-820ec934a605,AlphaDesign: A graph protein design method and benchmark on AlphaFoldDB,39,0.0969995,0.91636,"While DeepMind has tentatively solved protein folding, its inverse problem --
protein design which predicts protein sequences from their 3D structures --
still faces significant challenges. Particularly, the lack of large-scale
standardized benchmark and poor accuray hinder the research progress. In order
to standardize comparisons and draw more research interest, we use AlphaFold
DB, one of the world's largest protein structure databases, to establish a new
graph-based benchmark -- AlphaDesign. Based on AlphaDesign, we propose a new
method called ADesign to improve accuracy by introducing protein angles as new
features, using a simplified graph transformer encoder (SGT), and proposing a
confidence-aware protein decoder (CPD). Meanwhile, SGT and CPD also improve
model efficiency by simplifying the training and testing procedures.
Experiments show that ADesign significantly outperforms previous graph models,
e.g., the average accuracy is improved by 8\%, and the inference speed is 40+
times faster than before.",0,1,1,1,1,0,0.341995,6.0,0.586598,28
522b5895-8730-49fc-ba67-77f8c966dac1,Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification,9,0.843882,0.675348,"To learn camera-view invariant features for person Re-IDentification (Re-ID),
the cross-camera image pairs of each person play an important role. However,
such cross-view training samples could be unavailable under the ISolated Camera
Supervised (ISCS) setting, e.g., a surveillance system deployed across distant
scenes. To handle this challenging problem, a new pipeline is introduced by
synthesizing the cross-camera samples in the feature space for model training.
Specifically, the feature encoder and generator are end-to-end optimized under
a novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint
learning procedure raises concern on the stability of generative model
training. Therefore, a new feature generator, $\sigma$-Regularized Conditional
Variational Autoencoder ($\sigma$-Reg.~CVAE), is proposed with theoretical and
experimental analysis on its robustness. Extensive experiments on two ISCS
person Re-ID datasets demonstrate the superiority of our CCSFG to the
competitors.",1,1,0,0,1,0,0.966343,8.0,0.951481,58
83eb8c81-ac99-4e66-9d71-6106c3fde023,SciFact-Open: Towards open-domain scientific claim verification,27,0.941222,0.99634,"While research on scientific claim verification has led to the development of
powerful systems that appear to approach human performance, these approaches
have yet to be tested in a realistic setting against large corpora of
scientific literature. Moving to this open-domain evaluation setting, however,
poses unique challenges; in particular, it is infeasible to exhaustively
annotate all evidence documents. In this work, we present SciFact-Open, a new
test collection designed to evaluate the performance of scientific claim
verification systems on a corpus of 500K research abstracts. Drawing upon
pooling techniques from information retrieval, we collect evidence for
scientific claims by pooling and annotating the top predictions of four
state-of-the-art scientific claim verification models. We find that systems
developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting
performance drops of at least 15 F1. In addition, analysis of the evidence in
SciFact-Open reveals interesting phenomena likely to appear when claim
verification systems are deployed in practice, e.g., cases where the evidence
supports only a special case of the claim. Our dataset is available at
https://github.com/dwadden/scifact-open.",1,1,0,1,0,0,0.981222,6.0,0.961768,51
226d764b-efc9-4997-9adb-7f6851d12dfe,A Classical-Quantum Convolutional Neural Network for Detecting Pneumonia from Chest Radiographs,3,0.00673841,0.221785,"While many quantum computing techniques for machine learning have been
proposed, their performance on real-world datasets remains to be studied. In
this paper, we explore how a variational quantum circuit could be integrated
into a classical neural network for the problem of detecting pneumonia from
chest radiographs. We substitute one layer of a classical convolutional neural
network with a variational quantum circuit to create a hybrid neural network.
We train both networks on an image dataset containing chest radiographs and
benchmark their performance. To mitigate the influence of different sources of
randomness in network training, we sample the results over multiple rounds. We
show that the hybrid network outperforms the classical network on different
performance measures, and that these improvements are statistically
significant. Our work serves as an experimental demonstration of the potential
of quantum computing to significantly improve neural network performance for
real-world, non-trivial problems relevant to society and industry.",0,0,0,0,0,0,0.238105,9.0,0.67649,52
905b9c8d-787e-422c-a34a-c8178b4dd9be,Keys to Better Image Inpainting: Structure and Texture Go Hand in Hand,19,0.15704,0.709273,"Deep image inpainting has made impressive progress with recent advances in
image generation and processing algorithms. We claim that the performance of
inpainting algorithms can be better judged by the generated structures and
textures. Structures refer to the generated object boundary or novel geometric
structures within the hole, while texture refers to high-frequency details,
especially man-made repeating patterns filled inside the structural regions. We
believe that better structures are usually obtained from a coarse-to-fine
GAN-based generator network while repeating patterns nowadays can be better
modeled using state-of-the-art high-frequency fast fourier convolutional
layers. In this paper, we propose a novel inpainting network combining the
advantages of the two designs. Therefore, our model achieves a remarkable
visual quality to match state-of-the-art performance in both structure
generation and repeating texture synthesis using a single network. Extensive
experiments demonstrate the effectiveness of the method, and our conclusions
further highlight the two critical factors of image inpainting quality,
structures, and textures, as the future design directions of inpainting
networks.",1,1,0,0,1,0,0.667259,9.0,0.831803,60
0eee77ee-1937-45f5-a1ed-a23be37569c4,MiniViT: Compressing Vision Transformers with Weight Multiplexing,78,0.348614,0.777443,"Vision Transformer (ViT) models have recently drawn much attention in
computer vision due to their high model capability. However, ViT models suffer
from huge number of parameters, restricting their applicability on devices with
limited memory. To alleviate this problem, we propose MiniViT, a new
compression framework, which achieves parameter reduction in vision
transformers while retaining the same performance. The central idea of MiniViT
is to multiplex the weights of consecutive transformer blocks. More
specifically, we make the weights shared across layers, while imposing a
transformation on the weights to increase diversity. Weight distillation over
self-attention is also applied to transfer knowledge from large-scale ViT
models to weight-multiplexed compact models. Comprehensive experiments
demonstrate the efficacy of MiniViT, showing that it can reduce the size of the
pre-trained Swin-B transformer by 48\%, while achieving an increase of 1.0\% in
Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters,
MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters,
without seriously compromising the performance. Finally, we verify the
transferability of MiniViT by reporting its performance on downstream
benchmarks. Code and models are available at here.",0,1,0,0,1,0,0.850407,6.0,0.838721,66
62fe11e9-ce80-4223-a335-1df0c81dd372,FixMatchSeg: Fixing FixMatch for Semi-Supervised Semantic Segmentation,6,0.0849445,0.340588,"Supervised deep learning methods for semantic medical image segmentation are
getting increasingly popular in the past few years.However, in resource
constrained settings, getting large number of annotated images is very
difficult as it mostly requires experts, is expensive and
time-consuming.Semi-supervised segmentation can be an attractive solution where
a very few labeled images are used along with a large number of unlabeled ones.
While the gap between supervised and semi-supervised methods have been
dramatically reduced for classification problems in the past couple of years,
there still remains a larger gap in segmentation methods. In this work, we
adapt a state-of-the-art semi-supervised classification method FixMatch to
semantic segmentation task, introducing FixMatchSeg. FixMatchSeg is evaluated
in four different publicly available datasets of different anatomy and
different modality: cardiac ultrasound, chest X-ray, retinal fundus image, and
skin images. When there are few labels, we show that FixMatchSeg performs on
par with strong supervised baselines.",0,1,0,0,0,0,0.760547,8.0,0.843477,30
6f218b1c-9c47-4627-ac37-d476ded36aaf,Do Charge Prediction Models Learn Legal Theory?,3,0.0385021,0.140431,"The charge prediction task aims to predict the charge for a case given its
fact description. Recent models have already achieved impressive accuracy in
this task, however, little is understood about the mechanisms they use to
perform the judgment.For practical applications, a charge prediction model
should conform to the certain legal theory in civil law countries, as under the
framework of civil law, all cases are judged according to certain local legal
theories. In China, for example, nearly all criminal judges make decisions
based on the Four Elements Theory (FET).In this paper, we argue that
trustworthy charge prediction models should take legal theories into
consideration, and standing on prior studies in model interpretation, we
propose three principles for trustworthy models should follow in this task,
which are sensitive, selective, and presumption of innocence.We further design
a new framework to evaluate whether existing charge prediction models learn
legal theories. Our findings indicate that, while existing charge prediction
models meet the selective principle on a benchmark dataset, most of them are
still not sensitive enough and do not satisfy the presumption of innocence. Our
code and dataset are released at https://github.com/ZhenweiAn/EXP_LJP.",1,0,0,0,0,0,0.446965,8.0,0.73336,39
25f6634f-86bb-47ea-829c-763e2e39ae37,PIC4rl-gym: a ROS2 modular framework for Robots Autonomous Navigation with Deep Reinforcement Learning,9,0.172703,0.80804,"Learning agents can optimize standard autonomous navigation improving
flexibility, efficiency, and computational cost of the system by adopting a
wide variety of approaches. This work introduces the \textit{PIC4rl-gym}, a
fundamental modular framework to enhance navigation and learning research by
mixing ROS2 and Gazebo, the standard tools of the robotics community, with Deep
Reinforcement Learning (DRL). The paper describes the whole structure of the
PIC4rl-gym, which fully integrates DRL agent's training and testing in several
indoor and outdoor navigation scenarios and tasks. A modular approach is
adopted to easily customize the simulation by selecting new platforms, sensors,
or models. We demonstrate the potential of our novel gym by benchmarking the
resulting policies, trained for different navigation tasks, with a complete set
of metrics.",1,1,0,0,0,0,0.427276,5.0,0.561201,19
ff3dc86f-cad7-47ef-ab63-ae6ad65bf479,Less is More: Task-aware Layer-wise Distillation for Language Model Compression,20,0.125729,0.675151,"Layer-wise distillation is a powerful tool to compress large models (i.e.
teacher models) into small ones (i.e., student models). The student distills
knowledge from the teacher by mimicking the hidden representations of the
teacher at every intermediate layer. However, layer-wise distillation is
difficult. Since the student has a smaller model capacity than the teacher, it
is often under-fitted. Furthermore, the hidden representations of the teacher
contain redundant information that the student does not necessarily need for
the target task's learning. To address these challenges, we propose a novel
Task-aware layEr-wise Distillation (TED). TED designs task-aware filters to
align the hidden representations of the student and the teacher at each layer.
The filters select the knowledge that is useful for the target task from the
hidden representations. As such, TED reduces the knowledge gap between the two
models and helps the student to fit better on the target task. We evaluate TED
in two scenarios: continual pre-training and fine-tuning. TED demonstrates
significant and consistent improvements over existing distillation methods in
both scenarios. Code is available at
https://github.com/cliang1453/task-aware-distillation.",1,1,0,0,1,0,0.826088,6.0,0.824954,57
fa3e13ef-799b-4a88-a9fa-91b49ac70ae3,Interpretable Hidden Markov Model-Based Deep Reinforcement Learning Hierarchical Framework for Predictive Maintenance of Turbofan Engines,6,0.0695273,0.676536,"An open research question in deep reinforcement learning is how to focus the
policy learning of key decisions within a sparse domain. This paper emphasizes
combining the advantages of inputoutput hidden Markov models and reinforcement
learning towards interpretable maintenance decisions. We propose a novel
hierarchical-modeling methodology that, at a high level, detects and interprets
the root cause of a failure as well as the health degradation of the turbofan
engine, while, at a low level, it provides the optimal replacement policy. It
outperforms the baseline performance of deep reinforcement learning methods
applied directly to the raw data or when using a hidden Markov model without
such a specialized hierarchy. It also provides comparable performance to prior
work, however, with the additional benefit of interpretability.",1,1,0,0,0,0,0.415891,5.0,0.55401,32
41f9d4a2-9ae7-481c-b1bf-33a10a66e34f,Human-Centered Concept Explanations for Neural Networks,20,0.195559,0.508133,"Understanding complex machine learning models such as deep neural networks
with explanations is crucial in various applications. Many explanations stem
from the model perspective, and may not necessarily effectively communicate why
the model is making its predictions at the right level of abstraction. For
example, providing importance weights to individual pixels in an image can only
express which parts of that particular image are important to the model, but
humans may prefer an explanation which explains the prediction by concept-based
thinking. In this work, we review the emerging area of concept based
explanations. We start by introducing concept explanations including the class
of Concept Activation Vectors (CAV) which characterize concepts using vectors
in appropriate spaces of neural activations, and discuss different properties
of useful concepts, and approaches to measure the usefulness of concept
vectors. We then discuss approaches to automatically extract concepts, and
approaches to address some of their caveats. Finally, we discuss some case
studies that showcase the utility of such concept-based explanations in
synthetic settings and real world applications.",0,0,0,0,0,0,0.595238,8.0,0.786267,88
b26488b7-daca-4885-b7ab-ab59e0ac8397,A Reference Model for Common Understanding of Capabilities and Skills in Manufacturing,19,0.684153,0.944496,"In manufacturing, many use cases of Industry 4.0 require vendor-neutral and
machine-readable information models to describe, implement and execute resource
functions. Such models have been researched under the terms capabilities and
skills. Standardization of such models is required, but currently not
available. This paper presents a reference model developed jointly by members
of various organizations in a working group of the Plattform Industrie 4.0.
This model covers definitions of most important aspects of capabilities and
skills. It can be seen as a basis for further standardization efforts.",0,1,0,0,0,0,0.244258,8.0,0.639724,26
75f4325a-9815-4640-b35c-69b92172cb7b,Clues Before Answers: Generation-Enhanced Multiple-Choice QA,16,0.791599,0.982885,"A trending paradigm for multiple-choice question answering (MCQA) is using a
text-to-text framework. By unifying data in different tasks into a single
text-to-text format, it trains a generative encoder-decoder model which is both
powerful and universal. However, a side effect of twisting a generation target
to fit the classification nature of MCQA is the under-utilization of the
decoder and the knowledge that can be decoded. To exploit the generation
capability and underlying knowledge of a pre-trained encoder-decoder model, in
this paper, we propose a generation-enhanced MCQA model named GenMC. It
generates a clue from the question and then leverages the clue to enhance a
reader for MCQA. It outperforms text-to-text models on multiple MCQA datasets.",0,0,0,0,0,1,0.96965,6.0,0.940313,47
e0bc21c9-e81c-44f3-8057-efbcc6e3a226,Fine Detailed Texture Learning for 3D Meshes with Generative Models,12,0.278571,0.301897,"This paper presents a method to reconstruct high-quality textured 3D models
from both multi-view and single-view images. The reconstruction is posed as an
adaptation problem and is done progressively where in the first stage, we focus
on learning accurate geometry, whereas in the second stage, we focus on
learning the texture with a generative adversarial network. In the generative
learning pipeline, we propose two improvements. First, since the learned
textures should be spatially aligned, we propose an attention mechanism that
relies on the learnable positions of pixels. Secondly, since discriminator
receives aligned texture maps, we augment its input with a learnable embedding
which improves the feedback to the generator. We achieve significant
improvements on multi-view sequences from Tripod dataset as well as on
single-view image datasets, Pascal 3D+ and CUB. We demonstrate that our method
achieves superior 3D textured models compared to the previous works. Please
visit our web-page for 3D visuals.",1,1,0,0,1,0,0.958452,7.0,0.93539,63
25acd395-b15e-45b1-9b3f-1739a6015d28,Learning Progressive Modality-shared Transformers for Effective Visible-Infrared Person Re-identification,25,0.775069,0.956954,"Visible-Infrared Person Re-Identification (VI-ReID) is a challenging
retrieval task under complex modality changes. Existing methods usually focus
on extracting discriminative visual features while ignoring the reliability and
commonality of visual features between different modalities. In this paper, we
propose a novel deep learning framework named Progressive Modality-shared
Transformer (PMT) for effective VI-ReID. To reduce the negative effect of
modality gaps, we first take the gray-scale images as an auxiliary modality and
propose a progressive learning strategy. Then, we propose a Modality-Shared
Enhancement Loss (MSEL) to guide the model to explore more reliable identity
information from modality-shared features. Finally, to cope with the problem of
large intra-class differences and small inter-class differences, we propose a
Discriminative Center Loss (DCL) combined with the MSEL to further improve the
discrimination of reliable features. Extensive experiments on SYSU-MM01 and
RegDB datasets show that our proposed framework performs better than most
state-of-the-art methods. For model reproduction, we release the source code at
https://github.com/hulu88/PMT.",1,1,0,0,1,0,0.974929,5.0,0.939023,36
951e4bd5-721d-48c3-8aa5-58083358b0aa,Leveraging Language for Accelerated Learning of Tool Manipulation,27,0.123205,0.59889,"Robust and generalized tool manipulation requires an understanding of the
properties and affordances of different tools. We investigate whether
linguistic information about a tool (e.g., its geometry, common uses) can help
control policies adapt faster to new tools for a given task. We obtain diverse
descriptions of various tools in natural language and use pre-trained language
models to generate their feature representations. We then perform
language-conditioned meta-learning to learn policies that can efficiently adapt
to new tools given their corresponding text descriptions. Our results
demonstrate that combining linguistic information and meta-learning
significantly accelerates tool learning in several manipulation tasks including
pushing, lifting, sweeping, and hammering.",0,1,0,0,0,0,0.116107,9.0,0.58871,47
f29f7727-b399-405d-aaf0-6605940b783f,Lightweight Long-Range Generative Adversarial Networks,1,0.0171602,0.153562,"In this paper, we introduce novel lightweight generative adversarial
networks, which can effectively capture long-range dependencies in the image
generation process, and produce high-quality results with a much simpler
architecture. To achieve this, we first introduce a long-range module, allowing
the network to dynamically adjust the number of focused sampling pixels and to
also augment sampling locations. Thus, it can break the limitation of the fixed
geometric structure of the convolution operator, and capture long-range
dependencies in both spatial and channel-wise directions. Also, the proposed
long-range module can highlight negative relations between pixels, working as a
regularization to stabilize training. Furthermore, we propose a new generation
strategy through which we introduce metadata into the image generation process
to provide basic information about target images, which can stabilize and speed
up the training process. Our novel long-range module only introduces few
additional parameters and is easily inserted into existing models to capture
long-range dependencies. Extensive experiments demonstrate the competitive
performance of our method with a lightweight architecture.",0,0,0,0,0,0,0.956689,10.0,0.953458,45
872fbeef-cfb5-4ce3-8691-932fb05b1492,Frequency-Aware Self-Supervised Monocular Depth Estimation,5,0.137455,0.515564,"We present two versatile methods to generally enhance self-supervised
monocular depth estimation (MDE) models. The high generalizability of our
methods is achieved by solving the fundamental and ubiquitous problems in
photometric loss function. In particular, from the perspective of spatial
frequency, we first propose Ambiguity-Masking to suppress the incorrect
supervision under photometric loss at specific object boundaries, the cause of
which could be traced to pixel-level ambiguity. Second, we present a novel
frequency-adaptive Gaussian low-pass filter, designed to robustify the
photometric loss in high-frequency regions. We are the first to propose
blurring images to improve depth estimators with an interpretable analysis.
Both modules are lightweight, adding no parameters and no need to manually
change the network structures. Experiments show that our methods provide
performance boosts to a large number of existing models, including those who
claimed state-of-the-art, while introducing no extra inference computation at
all.",1,1,0,0,1,0,0.926305,9.0,0.927675,40
ee2fc9c6-e28c-49af-86ae-2027d6ddea43,A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation,14,0.122683,0.760263,"We propose a simple, yet powerful approach for unsupervised object
segmentation in videos. We introduce an objective function whose minimum
represents the mask of the main salient object over the input sequence. It only
relies on independent image features and optical flows, which can be obtained
using off-the-shelf self-supervised methods. It scales with the length of the
sequence with no need for superpixels or sparsification, and it generalizes to
different datasets without any specific training. This objective function can
actually be derived from a form of spectral clustering applied to the entire
video. Our method achieves on-par performance with the state of the art on
standard benchmarks (DAVIS2016, SegTrack-v2, FBMS59), while being conceptually
and practically much simpler. Code is available at
https://ponimatkin.github.io/ssl-vos.",0,1,0,0,1,0,0.640585,8.0,0.801697,95
eccad1e9-23ad-4f20-aaa6-4942edf942f1,TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,7,0.0537176,0.719008,"Recently, the structural reading comprehension (SRC) task on web pages has
attracted increasing research interests. Although previous SRC work has
leveraged extra information such as HTML tags or XPaths, the informative
topology of web pages is not effectively exploited. In this work, we propose a
Topological Information Enhanced model (TIE), which transforms the token-level
task into a tag-level task by introducing a two-stage process (i.e. node
locating and answer refining). Based on that, TIE integrates Graph Attention
Network (GAT) and Pre-trained Language Model (PLM) to leverage the topological
information of both logical structures and spatial structures. Experimental
results demonstrate that our model outperforms strong baselines and achieves
state-of-the-art performances on the web-based SRC benchmark WebSRC at the time
of writing. The code of TIE will be publicly available at
https://github.com/X-LANCE/TIE.",1,1,0,0,1,0,0.399169,6.0,0.619354,52
4be4c714-b6a8-48b4-b5a0-5140b50de298,Choose qualified instructor for university based on rule-based weighted expert system,1,0.0271228,0.281407,"Near the entire university faculty directors must select some qualified
professors for respected courses in each academic semester. In this sense,
factors such as teaching experience, academic training, competition, etc. are
considered. This work is usually done by experts, such as faculty directors,
which is time consuming. Up to now, several semi-automatic systems have been
proposed to assist heads. In this article, a fully automatic rule-based expert
system is developed. The proposed expert system consists of three main stages.
First, the knowledge of human experts is entered and designed as a decision
tree. In the second step, an expert system is designed based on the provided
rules of the generated decision tree. In the third step, an algorithm is
proposed to weight the results of the tree based on the quality of the experts.
To improve the performance of the expert system, a majority voting algorithm is
developed as a post-process step to select the qualified trainer who satisfies
the most expert decision tree for each course. The quality of the proposed
expert system is evaluated using real data from Iranian universities. The
calculated accuracy rate is 85.55, demonstrating the robustness and accuracy of
the proposed system. The proposed system has little computational complexity
compared to related efficient works. Also, simple implementation and
transparent box are other features of the proposed system.",0,1,0,0,0,0,0.00388514,17.0,0.578936,32
1a7a4a11-955a-4a76-8bed-fcc083699044,How to Fine-Tune Vision Models with SGD,21,0.147246,0.414509,"SGD and AdamW are the two most used optimizers for fine-tuning large neural
networks in computer vision. When the two methods perform the same, SGD is
preferable because it uses less memory (12 bytes/parameter with momentum and 8
bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite
of downstream tasks, especially those with distribution shifts, we find that
fine-tuning with AdamW performs substantially better than SGD on modern Vision
Transformer and ConvNeXt models. We find that large gaps in performance between
SGD and AdamW occur when the fine-tuning gradients in the first ""embedding""
layer are much larger than in the rest of the model. Our analysis suggests an
easy fix that works consistently across datasets and models: freezing the
embedding layer (less than 1% of the parameters) leads to SGD with or without
momentum performing slightly better than AdamW while using less memory (e.g.,
on ViT-L, SGD uses 33% less GPU memory). Our insights result in
state-of-the-art accuracies on five popular distribution shift benchmarks:
WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.",0,1,0,0,1,0,0.827661,7.0,0.850701,50
bb782ef2-22d9-457b-887a-7dc6b701c50c,Shift-tolerant Perceptual Similarity Metric,13,0.45142,0.96691,"Existing perceptual similarity metrics assume an image and its reference are
well aligned. As a result, these metrics are often sensitive to a small
alignment error that is imperceptible to the human eyes. This paper studies the
effect of small misalignment, specifically a small shift between the input and
reference image, on existing metrics, and accordingly develops a shift-tolerant
similarity metric. This paper builds upon LPIPS, a widely used learned
perceptual similarity metric, and explores architectural design considerations
to make it robust against imperceptible misalignment. Specifically, we study a
wide spectrum of neural network elements, such as anti-aliasing filtering,
pooling, striding, padding, and skip connection, and discuss their roles in
making a robust metric. Based on our studies, we develop a new deep neural
network-based perceptual similarity metric. Our experiments show that our
metric is tolerant to imperceptible shifts while being consistent with the
human similarity judgment.",1,1,0,0,0,0,0.830687,12.0,0.913744,40
21ac1db5-f895-4c64-90d9-5d0ccb6ee612,Panoramic Human Activity Recognition,11,0.110384,0.687657,"To obtain a more comprehensive activity understanding for a crowded scene, in
this paper, we propose a new problem of panoramic human activity recognition
(PAR), which aims to simultaneous achieve the individual action, social group
activity, and global activity recognition. This is a challenging yet practical
problem in real-world applications. For this problem, we develop a novel
hierarchical graph neural network to progressively represent and model the
multi-granularity human activities and mutual social relations for a crowd of
people. We further build a benchmark to evaluate the proposed method and other
existing related methods. Experimental results verify the rationality of the
proposed PAR problem, the effectiveness of our method and the usefulness of the
benchmark. We will release the source code and benchmark to the public for
promoting the study on this problem.",1,1,1,0,0,0,0.288513,8.0,0.664115,57
47e52a54-02c1-48af-93dd-bb05be0a3bdd,Rethinking and Refining the Distinct Metric,5,0.576199,0.864665,"Distinct-$n$ score\cite{Li2016} is a widely used automatic metric for
evaluating diversity in language generation tasks. However, we observed that
the original approach for calculating distinct scores has evident biases that
tend to assign higher penalties to longer sequences. We refine the calculation
of distinct scores by scaling the number of distinct tokens based on their
expectations. We provide both empirical and theoretical evidence to show that
our method effectively removes the biases existing in the original distinct
score. Our experiments show that our proposed metric,
\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human
judgment in evaluating response diversity. To foster future research, we
provide an example implementation at
\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.",1,0,0,0,0,1,0.984296,7.0,0.973513,60
11c2760a-fae9-4dfb-b835-82b3b37a8bee,4D-StOP: Panoptic Segmentation of 4D LiDAR using Spatio-temporal Object Proposal Generation and Aggregation,20,0.527837,0.79649,"In this work, we present a new paradigm, called 4D-StOP, to tackle the task
of 4D Panoptic LiDAR Segmentation. 4D-StOP first generates spatio-temporal
proposals using voting-based center predictions, where each point in the 4D
volume votes for a corresponding center. These tracklet proposals are further
aggregated using learned geometric features. The tracklet aggregation method
effectively generates a video-level 4D scene representation over the entire
space-time volume. This is in contrast to existing end-to-end trainable
state-of-the-art approaches which use spatio-temporal embeddings that are
represented by Gaussian probability distributions. Our voting-based tracklet
generation method followed by geometric feature-based aggregation generates
significantly improved panoptic LiDAR segmentation quality when compared to
modeling the entire 4D volume using Gaussian probability distributions. 4D-StOP
achieves a new state-of-the-art when applied to the SemanticKITTI test dataset
with a score of 63.9 LSTQ, which is a large (+7%) improvement compared to
current best-performing end-to-end trainable methods. The code and pre-trained
models are available at: https://github.com/LarsKreuzberg/4D-StOP.",1,1,1,0,1,0,0.980185,6.0,0.959498,61
d58e3449-abd0-4b76-81b4-382b8dc0558a,Real-time Gesture Animation Generation from Speech for Virtual Human Interaction,12,0.304115,0.308295,"We propose a real-time system for synthesizing gestures directly from speech.
Our data-driven approach is based on Generative Adversarial Neural Networks to
model the speech-gesture relationship. We utilize the large amount of speaker
video data available online to train our 3D gesture model. Our model generates
speaker-specific gestures by taking consecutive audio input chunks of two
seconds in length. We animate the predicted gestures on a virtual avatar. We
achieve a delay below three seconds between the time of audio input and gesture
animation. Code and videos are available at
https://github.com/mrebol/Gestures-From-Speech",0,1,0,0,0,0,0.94963,9.0,0.942809,9
8666778c-e195-4fc8-9fca-7bf75649a730,RBC: Rectifying the Biased Context in Continual Semantic Segmentation,16,0.0592367,0.323235,"Recent years have witnessed a great development of Convolutional Neural
Networks in semantic segmentation, where all classes of training images are
simultaneously available. In practice, new images are usually made available in
a consecutive manner, leading to a problem called Continual Semantic
Segmentation (CSS). Typically, CSS faces the forgetting problem since previous
training images are unavailable, and the semantic shift problem of the
background class. Considering the semantic segmentation as a context-dependent
pixel-level classification task, we explore CSS from a new perspective of
context analysis in this paper. We observe that the context of old-class pixels
in the new images is much more biased on new classes than that in the old
images, which can sharply aggravate the old-class forgetting and new-class
overfitting. To tackle the obstacle, we propose a biased-context-rectified CSS
framework with a context-rectified image-duplet learning scheme and a
biased-context-insensitive consistency loss. Furthermore, we propose an
adaptive re-weighting class-balanced learning strategy for the biased class
distribution. Our approach outperforms state-of-the-art methods by a large
margin in existing CSS scenarios.",1,0,0,0,1,0,0.340082,6.0,0.585437,59
c3ab0602-98c6-41f8-b6c4-35f568eab858,LeNSE: Learning To Navigate Subgraph Embeddings for Large-Scale Combinatorial Optimisation,4,0.022097,0.169861,"Combinatorial Optimisation problems arise in several application domains and
are often formulated in terms of graphs. Many of these problems are NP-hard,
but exact solutions are not always needed. Several heuristics have been
developed to provide near-optimal solutions; however, they do not typically
scale well with the size of the graph. We propose a low-complexity approach for
identifying a (possibly much smaller) subgraph of the original graph where the
heuristics can be run in reasonable time and with a high likelihood of finding
a global near-optimal solution. The core component of our approach is LeNSE, a
reinforcement learning algorithm that learns how to navigate the space of
possible subgraphs using an Euclidean subgraph embedding as its map. To solve
CO problems, LeNSE is provided with a discriminative embedding trained using
any existing heuristics using only on a small portion of the original graph.
When tested on three problems (vertex cover, max-cut and influence
maximisation) using real graphs with up to $10$ million edges, LeNSE identifies
small subgraphs yielding solutions comparable to those found by running the
heuristics on the entire graph, but at a fraction of the total run time.",1,1,0,0,0,0,0.0876042,11.0,0.636466,83
28ed08fb-6b67-4ea6-be79-aa0b5e1cf713,Phrase-level Textual Adversarial Attack with Label Preservation,12,0.0877469,0.571138,"Generating high-quality textual adversarial examples is critical for
investigating the pitfalls of natural language processing (NLP) models and
further promoting their robustness. Existing attacks are usually realized
through word-level or sentence-level perturbations, which either limit the
perturbation space or sacrifice fluency and textual quality, both affecting the
attack effectiveness. In this paper, we propose Phrase-Level Textual
Adversarial aTtack (PLAT) that generates adversarial samples through
phrase-level perturbations. PLAT first extracts the vulnerable phrases as
attack targets by a syntactic parser, and then perturbs them by a pre-trained
blank-infilling model. Such flexible perturbation design substantially expands
the search space for more effective attacks without introducing too many
modifications, and meanwhile maintaining the textual fluency and grammaticality
via contextualized generation using surrounding texts. Moreover, we develop a
label-preservation filter leveraging the likelihoods of language models
fine-tuned on each class, rather than textual similarity, to rule out those
perturbations that potentially alter the original class label for humans.
Extensive experiments and human evaluation demonstrate that PLAT has a superior
attack effectiveness as well as a better label consistency than strong
baselines.",1,1,0,0,0,0,0.670083,6.0,0.748991,51
b597e61f-4e3f-4af2-a826-2745e5a3c1ab,REx: Data-Free Residual Quantization Error Expansion,3,0.0,0.107764,"Deep neural networks (DNNs) are ubiquitous in computer vision and natural
language processing, but suffer from high inference cost. This problem can be
addressed by quantization, which consists in converting floating point
operations into a lower bit-width format. With the growing concerns on privacy
rights, we focus our efforts on data-free methods. However, such techniques
suffer from their lack of adaptability to the target devices, as a hardware
typically only support specific bit widths. Thus, to adapt to a variety of
devices, a quantization method shall be flexible enough to find good accuracy
v.s. speed trade-offs for every bit width and target device. To achieve this,
we propose REx, a quantization method that leverages residual error expansion,
along with group sparsity and an ensemble approximation for better
parallelization. REx is backed off by strong theoretical guarantees and
achieves superior performance on every benchmarked application (from vision to
NLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 to
ternary quantization).",0,1,0,0,1,1,0.146171,8.0,0.568195,76
f61b012b-310e-4abd-aa3c-c9ea545fa957,Exemplar Free Class Agnostic Counting,21,0.457027,0.819298,"We tackle the task of Class Agnostic Counting, which aims to count objects in
a novel object category at test time without any access to labeled training
data for that category. All previous class agnostic counting methods cannot
work in a fully automated setting, and require computationally expensive test
time adaptation. To address these challenges, we propose a visual counter which
operates in a fully automated setting and does not require any test time
adaptation. Our proposed approach first identifies exemplars from repeating
objects in an image, and then counts the repeating objects. We propose a novel
region proposal network for identifying the exemplars. After identifying the
exemplars, we obtain the corresponding count by using a density estimation
based Visual Counter. We evaluate our proposed approach on FSC-147 dataset, and
show that it achieves superior performance compared to the existing approaches.",0,1,1,0,1,0,0.956055,9.0,0.947771,47
500581b2-9ada-4a9e-8b83-d1b976b41627,Domain-Specific Text Generation for Machine Translation,6,0.191801,0.271638,"Preservation of domain knowledge from the source to target is crucial in any
translation workflow. It is common in the translation industry to receive
highly specialized projects, where there is hardly any parallel in-domain data.
In such scenarios where there is insufficient in-domain data to fine-tune
Machine Translation (MT) models, producing translations that are consistent
with the relevant context is challenging. In this work, we propose a novel
approach to domain adaptation leveraging state-of-the-art pretrained language
models (LMs) for domain-specific data augmentation for MT, simulating the
domain characteristics of either (a) a small bilingual dataset, or (b) the
monolingual source text to be translated. Combining this idea with
back-translation, we can generate huge amounts of synthetic bilingual in-domain
data for both use cases. For our investigation, we use the state-of-the-art
Transformer architecture. We employ mixed fine-tuning to train models that
significantly improve translation of in-domain texts. More specifically, in
both scenarios, our proposed methods achieve improvements of approximately 5-6
BLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic
language pairs. Furthermore, the outcome of human evaluation corroborates the
automatic evaluation results.",1,1,0,0,0,0,0.883406,6.0,0.859265,75
6d34607f-8ed2-41b1-9df3-08ed4fe91d9d,Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation,27,0.269175,0.35071,"Attribute-based Controlled Text Generation (CTG) refers to generating
sentences that satisfy desirable attributes (e.g., emotions and topics).
Existing works often utilize fine-tuning or resort to extra attribute
classifiers, yet suffer from storage and inference time increases. To address
these concerns, we explore attribute-based CTG in a prompt-based manner. In
short, the proposed Tailor represents each attribute as a pre-trained
continuous vector (i.e., single-attribute prompt) and guides the generation of
a fixed PLM switch to a pre-specified attribute. We experimentally find that
these prompts can be simply concatenated as a whole to multi-attribute CTG
without any re-training, yet raises problems of fluency decrease and position
sensitivity. To this end, Tailor provides a multi-attribute prompt mask and a
re-indexing position-ids sequence to bridge the gap between the training (one
prompt for each task) and testing stage (concatenating more than one prompt).
To further enhance such single-attribute prompt combinations, Tailor also
introduces a trainable prompt connector, which can be concatenated with any two
single-attribute prompts to multi-attribute text generation. Experiments on 11
attribute-specific generation tasks demonstrate strong performances of Tailor
on both single-attribute and multi-attribute CTG, with 0.08\% training
parameters of a GPT-2.",0,1,0,0,0,0,0.929512,5.0,0.873199,35
42177f69-5ec8-4c25-81c9-8365df475146,Understanding Zero-Shot Adversarial Robustness for Large-Scale Models,24,0.444235,0.731581,"Pretrained large-scale vision-language models like CLIP have exhibited strong
generalization over unseen tasks. Yet imperceptible adversarial perturbations
can significantly reduce CLIP's performance on new tasks. In this work, we
identify and explore the problem of \emph{adapting large-scale models for
zero-shot adversarial robustness}. We first identify two key factors during
model adaption -- training losses and adaptation methods -- that affect the
model's zero-shot adversarial robustness. We then propose a text-guided
contrastive adversarial training loss, which aligns the text embeddings and the
adversarial visual features with contrastive learning on a small set of
training data. We apply this training loss to two adaption methods, model
finetuning and visual prompt tuning. We find that visual prompt tuning is more
effective in the absence of texts, while finetuning wins in the existence of
text guidance. Overall, our approach significantly improves the zero-shot
adversarial robustness over CLIP, seeing an average improvement of over 31
points over ImageNet and 15 zero-shot datasets. We hope this work can shed
light on understanding the zero-shot adversarial robustness of large-scale
models.",1,1,0,0,1,0,0.76887,8.0,0.846533,75
aeb05046-60e6-4336-a225-3e638d1035df,TabLLM: Few-shot Classification of Tabular Data with Large Language Models,88,0.603066,1.0,"We study the application of large language models to zero-shot and few-shot
classification of tabular data. We prompt the large language model with a
serialization of the tabular data to a natural-language string, together with a
short description of the classification problem. In the few-shot setting, we
fine-tune the large language model using some labeled examples. We evaluate
several serialization methods including templates, table-to-text models, and
large language models. Despite its simplicity, we find that this technique
outperforms prior deep-learning-based tabular classification methods on several
benchmark datasets. In most cases, even zero-shot classification obtains
non-trivial performance, illustrating the method's ability to exploit prior
knowledge encoded in large language models. Unlike many deep learning methods
for tabular datasets, this approach is also competitive with strong traditional
baselines like gradient-boosted trees, especially in the very-few-shot setting.",1,1,0,0,0,0,0.876327,4.0,0.781945,62
3c18f293-faa8-4c53-b238-3cc4084a36fc,PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for Pathology Domain,10,0.13059,0.690825,"Pathology text mining is a challenging task given the reporting variability
and constant new findings in cancer sub-type definitions. However, successful
text mining of a large pathology database can play a critical role to advance
'big data' cancer research like similarity-based treatment selection, case
identification, prognostication, surveillance, clinical trial screening, risk
stratification, and many others. While there is a growing interest in
developing language models for more specific clinical domains, no
pathology-specific language space exist to support the rapid data-mining
development in pathology space. In literature, a few approaches fine-tuned
general transformer models on specialized corpora while maintaining the
original tokenizer, but in fields requiring specialized terminology, these
models often fail to perform adequately. We propose PathologyBERT - a
pre-trained masked language model which was trained on 347,173 histopathology
specimen reports and publicly released in the Huggingface repository. Our
comprehensive experiments demonstrate that pre-training of transformer model on
pathology corpora yields performance improvements on Natural Language
Understanding (NLU) and Breast Cancer Diagnose Classification when compared to
nonspecific language models.",0,1,1,1,0,0,0.81503,9.0,0.879316,22
ad6640fc-cb11-49ac-be6c-8272a9c9d32b,TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction,12,0.0818215,0.701998,"This technical report presents an effective method for motion prediction in
autonomous driving. We develop a Transformer-based method for input encoding
and trajectory prediction. Besides, we propose the Temporal Flow Header to
enhance the trajectory encoding. In the end, an efficient K-means ensemble
method is used. Using our Transformer network and ensemble method, we win the
first place of Argoverse 2 Motion Forecasting Challenge with the
state-of-the-art brier-minFDE score of 1.90.",0,1,0,0,1,0,0.534182,12.0,0.843444,13
69759090-4da2-4110-ae61-9aa4cd913ad0,Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks,11,0.140658,0.291619,"Memorization presents a challenge for several constrained Natural Language
Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the
proclivity of neural models to memorize noisy and atypical samples reacts
adversely with the noisy (web crawled) datasets. However, previous studies of
memorization in constrained NLG tasks have only focused on counterfactual
memorization, linking it to the problem of hallucinations. In this work, we
propose a new, inexpensive algorithm for extractive memorization (exact
training data generation under insufficient context) in constrained sequence
generation tasks and use it to study extractive memorization and its effects in
NMT. We demonstrate that extractive memorization poses a serious threat to NMT
reliability by qualitatively and quantitatively characterizing the memorized
samples as well as the model behavior in their vicinity. Based on empirical
observations, we develop a simple algorithm which elicits non-memorized
translations of memorized samples from the same model, for a large fraction of
such samples. Finally, we show that the proposed algorithm could also be
leveraged to mitigate memorization in the model through finetuning. We have
released the code to reproduce our results at
https://github.com/vyraun/Finding-Memo.",1,0,0,0,0,0,0.868842,4.0,0.774816,43
3ab65a9a-1819-415f-b024-6a76be76b74c,TEMOS: Generating diverse human motions from textual descriptions,194,0.82574,0.979468,"We address the problem of generating diverse 3D human motions from textual
descriptions. This challenging task requires joint modeling of both modalities:
understanding and extracting useful human-centric information from the text,
and then generating plausible and realistic sequences of human poses. In
contrast to most previous work which focuses on generating a single,
deterministic, motion from a textual description, we design a variational
approach that can produce multiple diverse human motions. We propose TEMOS, a
text-conditioned generative model leveraging variational autoencoder (VAE)
training with human motion data, in combination with a text encoder that
produces distribution parameters compatible with the VAE latent space. We show
the TEMOS framework can produce both skeleton-based animations as in prior
work, as well more expressive SMPL body motions. We evaluate our approach on
the KIT Motion-Language benchmark and, despite being relatively
straightforward, demonstrate significant improvements over the state of the
art. Code and models are available on our webpage.",1,0,0,0,1,0,0.69151,6.0,0.75879,66
4667869f-92b5-4344-8cc3-c1ac937580a8,3D Equivariant Graph Implicit Functions,16,0.717117,0.393469,"In recent years, neural implicit representations have made remarkable
progress in modeling of 3D shapes with arbitrary topology. In this work, we
address two key limitations of such representations, in failing to capture
local 3D geometric fine details, and to learn from and generalize to shapes
with unseen 3D transformations. To this end, we introduce a novel family of
graph implicit functions with equivariant layers that facilitates modeling fine
local details and guaranteed robustness to various groups of geometric
transformations, through local $k$-NN graph embeddings with sparse point set
observations at multiple resolutions. Our method improves over the existing
rotation-equivariant implicit function from 0.69 to 0.89 (IoU) on the ShapeNet
reconstruction task. We also show that our equivariant implicit function can be
extended to other types of similarity transformations and generalizes to unseen
translations and scaling.",0,0,0,0,1,0,0.983995,5.0,0.962002,58
f5773c30-f0e7-4543-87f3-8506af84eec8,A Novel Underwater Image Enhancement and Improved Underwater Biological Detection Pipeline,4,0.112059,0.323108,"For aquaculture resource evaluation and ecological environment monitoring,
automatic detection and identification of marine organisms is critical.
However, due to the low quality of underwater images and the characteristics of
underwater biological, a lack of abundant features may impede traditional
hand-designed feature extraction approaches or CNN-based object detection
algorithms, particularly in complex underwater environment. Therefore, the goal
of this paper is to perform object detection in the underwater environment.
This paper proposed a novel method for capturing feature information, which
adds the convolutional block attention module (CBAM) to the YOLOv5 backbone.
The interference of underwater creature characteristics on object
characteristics is decreased, and the output of the backbone network to object
information is enhanced. In addition, the self-adaptive global histogram
stretching algorithm (SAGHS) is designed to eliminate the degradation problems
such as low contrast and color loss caused by underwater environmental
information to better restore image quality. Extensive experiments and
comprehensive evaluation on the URPC2021 benchmark dataset demonstrate the
effectiveness and adaptivity of our methods. Beyond that, this paper conducts
an exhaustive analysis of the role of training data on performance.",0,1,0,0,0,0,0.961941,10.0,0.957493,38
9be675ea-6d0f-4618-a5fe-900d28df1d13,CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers,10,0.426669,0.539693,"Context-dependent text-to-SQL is the task of translating multi-turn questions
into database-related SQL queries. Existing methods typically focus on making
full use of history context or previously predicted SQL for currently SQL
parsing, while neglecting to explicitly comprehend the schema and
conversational dependency, such as co-reference, ellipsis and user focus
change. In this paper, we propose CQR-SQL, which uses auxiliary Conversational
Question Reformulation (CQR) learning to explicitly exploit schema and decouple
contextual dependency for SQL parsing. Specifically, we first present a schema
enhanced recursive CQR method to produce domain-relevant self-contained
questions. Secondly, we train CQR-SQL models to map the semantics of multi-turn
questions and auxiliary self-contained questions into the same latent space
through schema grounding consistency task and tree-structured SQL parsing
consistency task, which enhances the abilities of SQL parsing by adequately
contextual understanding. At the time of writing, our CQR-SQL achieves new
state-of-the-art results on two context-dependent text-to-SQL benchmarks SParC
and CoSQL.",0,0,0,0,1,0,0.961292,5.0,0.913949,46
b5d12435-c327-4fe7-a780-b495f8a8a2df,HSGNet: Object Re-identification with Hierarchical Similarity Graph Network,7,0.169765,0.562526,"Object re-identification method is made up of backbone network, feature
aggregation, and loss function. However, most backbone networks lack a special
mechanism to handle rich scale variations and mine discriminative feature
representations. In this paper, we firstly design a hierarchical similarity
graph module (HSGM) to reduce the conflict of backbone and re-identification
networks. The designed HSGM builds a rich hierarchical graph to mine the
mapping relationships between global-local and local-local. Secondly, we divide
the feature map along with the spatial and channel directions in each
hierarchical graph. The HSGM applies the spatial features and channel features
extracted from different locations as nodes, respectively, and utilizes the
similarity scores between nodes to construct spatial and channel similarity
graphs. During the learning process of HSGM, we utilize a learnable parameter
to re-optimize the importance of each position, as well as evaluate the
correlation between different nodes. Thirdly, we develop a novel hierarchical
similarity graph network (HSGNet) by embedding the HSGM in the backbone
network. Furthermore, HSGM can be easily embedded into backbone networks of any
depth to improve object re-identification ability. Finally, extensive
experiments on three large-scale object datasets demonstrate that the proposed
HSGNet is superior to state-of-the-art object re-identification approaches.",0,1,0,0,1,0,0.723158,8.0,0.830098,65
40ffbb58-4ee9-491c-9f2d-0cd8d5e03eda,Standby-Based Deadlock Avoidance Method for Multi-Agent Pickup and Delivery Tasks,7,0.222079,0.504809,"The multi-agent pickup and delivery (MAPD) problem, in which multiple agents
iteratively carry materials without collisions, has received significant
attention. However, many conventional MAPD algorithms assume a specifically
designed grid-like environment, such as an automated warehouse. Therefore, they
have many pickup and delivery locations where agents can stay for a lengthy
period, as well as plentiful detours to avoid collisions owing to the freedom
of movement in a grid. By contrast, because a maze-like environment such as a
search-and-rescue or construction site has fewer pickup/delivery locations and
their numbers may be unbalanced, many agents concentrate on such locations
resulting in inefficient operations, often becoming stuck or deadlocked. Thus,
to improve the transportation efficiency even in a maze-like restricted
environment, we propose a deadlock avoidance method, called standby-based
deadlock avoidance (SBDA). SBDA uses standby nodes determined in real-time
using the articulation-point-finding algorithm, and the agent is guaranteed to
stay there for a finite amount of time. We demonstrated that our proposed
method outperforms a conventional approach. We also analyzed how the parameters
used for selecting standby nodes affect the performance.",0,1,0,0,0,0,0.507256,7.0,0.720703,37
07b9e0b0-05a8-4461-91b6-0d7307c37795,Improving Multilingual Neural Machine Translation System for Indic Languages,9,0.0477744,0.741898,"Machine Translation System (MTS) serves as an effective tool for
communication by translating text or speech from one language to another
language. The need of an efficient translation system becomes obvious in a
large multilingual environment like India, where English and a set of Indian
Languages (ILs) are officially used. In contrast with English, ILs are still
entreated as low-resource languages due to unavailability of corpora. In order
to address such asymmetric nature, multilingual neural machine translation
(MNMT) system evolves as an ideal approach in this direction. In this paper, we
propose a MNMT system to address the issues related to low-resource language
translation. Our model comprises of two MNMT systems i.e. for English-Indic
(one-to-many) and the other for Indic-English (many-to-one) with a shared
encoder-decoder containing 15 language pairs (30 translation directions). Since
most of IL pairs have scanty amount of parallel corpora, not sufficient for
training any machine translation model. We explore various augmentation
strategies to improve overall translation quality through the proposed model. A
state-of-the-art transformer architecture is used to realize the proposed
model. Trials over a good amount of data reveal its superiority over the
conventional models. In addition, the paper addresses the use of language
relationships (in terms of dialect, script, etc.), particularly about the role
of high-resource languages of the same family in boosting the performance of
low-resource languages. Moreover, the experimental results also show the
advantage of backtranslation and domain adaptation for ILs to enhance the
translation quality of both source and target languages. Using all these key
approaches, our proposed model emerges to be more efficient than the baseline
model in terms of evaluation metrics i.e BLEU (BiLingual Evaluation Understudy)
score for a set of ILs.",0,1,0,0,1,0,0.0655165,9.0,0.522089,78
ae5bb618-35ea-4357-8f14-0b15350c278b,VPAIR -- Aerial Visual Place Recognition and Localization in Large-scale Outdoor Environments,6,0.0223563,0.624076,"Visual Place Recognition and Visual Localization are essential components in
navigation and mapping for autonomous vehicles especially in GNSS-denied
navigation scenarios. Recent work has focused on ground or close to ground
applications such as self-driving cars or indoor-scenarios and low-altitude
drone flights. However, applications such as Urban Air Mobility require
operations in large-scale outdoor environments at medium to high altitudes. We
present a new dataset named VPAIR. The dataset was recorded on board a light
aircraft flying at an altitude of more than 300 meters above ground capturing
images with a downwardfacing camera. Each image is paired with a high
resolution reference render including dense depth information and 6-DoF
reference poses. The dataset covers a more than one hundred kilometers long
trajectory over various types of challenging landscapes, e.g. urban, farmland
and forests. Experiments on this dataset illustrate the challenges introduced
by the change in perspective to a bird's eye view such as in-plane rotations.",1,1,0,1,0,0,0.068525,8.0,0.468161,24
6a26e791-b26e-477f-a1b3-b3e84b7086f7,Deformable Sprites for Unsupervised Video Decomposition,48,0.409192,0.590244,"We describe a method to extract persistent elements of a dynamic scene from
an input video. We represent each scene element as a \emph{Deformable Sprite}
consisting of three components: 1) a 2D texture image for the entire video, 2)
per-frame masks for the element, and 3) non-rigid deformations that map the
texture image into each video frame. The resulting decomposition allows for
applications such as consistent video editing. Deformable Sprites are a type of
video auto-encoder model that is optimized on individual videos, and does not
require training on a large dataset, nor does it rely on pre-trained models.
Moreover, our method does not require object masks or other user input, and
discovers moving objects of a wider variety than previous work. We evaluate our
approach on standard video datasets and show qualitative results on a diverse
array of Internet videos. Code and video results can be found at
https://deformable-sprites.github.io",0,0,0,0,0,0,0.288816,12.0,0.776181,41
77b19317-aa5d-449d-9655-aaa6ab8fee09,A Variational Hierarchical Model for Neural Cross-Lingual Summarization,27,0.298116,0.895683,"The goal of the cross-lingual summarization (CLS) is to convert a document in
one language (e.g., English) to a summary in another one (e.g., Chinese).
Essentially, the CLS task is the combination of machine translation (MT) and
monolingual summarization (MS), and thus there exists the hierarchical
relationship between MT\&MS and CLS. Existing studies on CLS mainly focus on
utilizing pipeline methods or jointly training an end-to-end model through an
auxiliary MT or MS objective. However, it is very challenging for the model to
directly conduct CLS as it requires both the abilities to translate and
summarize. To address this issue, we propose a hierarchical model for the CLS
task, based on the conditional variational auto-encoder. The hierarchical model
contains two kinds of latent variables at the local and global levels,
respectively. At the local level, there are two latent variables, one for
translation and the other for summarization. As for the global level, there is
another latent variable for cross-lingual summarization conditioned on the two
local-level variables. Experiments on two language directions (English-Chinese)
verify the effectiveness and superiority of the proposed approach. In addition,
we show that our model is able to generate better cross-lingual summaries than
comparison models in the few-shot setting.",1,0,0,0,0,0,0.245241,7.0,0.588919,63
2c3eba00-a9a6-49b9-a861-a1ea38fb246b,Keep Me Updated! Memory Management in Long-term Conversations,20,0.581138,0.662878,"Remembering important information from the past and continuing to talk about
it in the present are crucial in long-term conversations. However, previous
literature does not deal with cases where the memorized information is
outdated, which may cause confusion in later conversations. To address this
issue, we present a novel task and a corresponding dataset of memory management
in long-term conversations, in which bots keep track of and bring up the latest
information about users while conversing through multiple sessions. In order to
support more precise and interpretable memory, we represent memory as
unstructured text descriptions of key information and propose a new mechanism
of memory management that selectively eliminates invalidated or redundant
information. Experimental results show that our approach outperforms the
baselines that leave the stored memory unchanged in terms of engagingness and
humanness, with larger performance gap especially in the later sessions.",1,1,1,1,0,0,0.956953,5.0,0.907305,53
1a19abb3-9f0e-4fe3-a4f1-03b8ea456b5e,Zero-Label Prompt Selection,3,0.152924,0.0983039,"Natural language prompts have been shown to facilitate cross-task
generalization for large language models. However, with no or limited labeled
examples, the cross-task performance is highly sensitive to the choice of
prompts, while selecting a high-performing prompt is challenging given the
scarcity of labels. To address the issue, we propose a Zero-Label Prompt
Selection (ZPS) method that selects prompts without any labeled data or
gradient update. Specifically, given the candidate human-written prompts for a
task, ZPS labels a set of unlabeled data with a prompt ensemble and uses the
pseudo-labels for prompt selection. Experiments show that ZPS improves over
prior methods by a sizeable margin in zero-label performance. We also extend
ZPS to a few-shot setting and show its advantages over strong baselines such as
prompt tuning and model tuning.",0,0,1,0,0,0,0.989029,4.0,0.974353,51
32d216be-81d8-4cef-aa22-579ec362079b,PPCD-GAN: Progressive Pruning and Class-Aware Distillation for Large-Scale Conditional GANs Compression,5,0.0433462,0.131906,"We push forward neural network compression research by exploiting a novel
challenging task of large-scale conditional generative adversarial networks
(GANs) compression. To this end, we propose a gradually shrinking GAN
(PPCD-GAN) by introducing progressive pruning residual block (PP-Res) and
class-aware distillation. The PP-Res is an extension of the conventional
residual block where each convolutional layer is followed by a learnable mask
layer to progressively prune network parameters as training proceeds. The
class-aware distillation, on the other hand, enhances the stability of training
by transferring immense knowledge from a well-trained teacher model through
instructive attention maps. We train the pruning and distillation processes
simultaneously on a well-known GAN architecture in an end-to-end manner. After
training, all redundant parameters as well as the mask layers are discarded,
yielding a lighter network while retaining the performance. We comprehensively
illustrate, on ImageNet 128x128 dataset, PPCD-GAN reduces up to 5.2x (81%)
parameters against state-of-the-arts while keeping better performance.",0,1,1,0,1,0,0.695016,9.0,0.840268,26
72adcbdb-dc81-43cc-b9bb-e5967090718f,Discovering Salient Neurons in Deep NLP Models,13,0.152864,0.393679,"While a lot of work has been done in understanding representations learned
within deep NLP models and what knowledge they capture, little attention has
been paid towards individual neurons. We present a technique called as
Linguistic Correlation Analysis to extract salient neurons in the model, with
respect to any extrinsic property - with the goal of understanding how such a
knowledge is preserved within neurons. We carry out a fine-grained analysis to
answer the following questions: (i) can we identify subsets of neurons in the
network that capture specific linguistic properties? (ii) how localized or
distributed neurons are across the network? iii) how redundantly is the
information preserved? iv) how fine-tuning pre-trained models towards
downstream NLP tasks, impacts the learned linguistic knowledge? iv) how do
architectures vary in learning different linguistic properties? Our
data-driven, quantitative analysis illuminates interesting findings: (i) we
found small subsets of neurons that can predict different linguistic tasks, ii)
with neurons capturing basic lexical information (such as suffixation)
localized in lower most layers, iii) while those learning complex concepts
(such as syntactic role) predominantly in middle and higher layers, iii) that
salient linguistic neurons are relocated from higher to lower layers during
transfer learning, as the network preserve the higher layers for task specific
information, iv) we found interesting differences across pre-trained models,
with respect to how linguistic information is preserved within, and v) we found
that concept exhibit similar neuron distribution across different languages in
the multilingual transformer models. Our code is publicly available as part of
the NeuroX toolkit.",0,0,0,0,0,0,0.433757,8.0,0.728277,88
f19cc3a6-bd2f-45f4-b530-3d9f8794d09e,Multifidelity Reinforcement Learning with Control Variates,4,0.0112403,0.124354,"In many computational science and engineering applications, the output of a
system of interest corresponding to a given input can be queried at different
levels of fidelity with different costs. Typically, low-fidelity data is cheap
and abundant, while high-fidelity data is expensive and scarce. In this work we
study the reinforcement learning (RL) problem in the presence of multiple
environments with different levels of fidelity for a given control task. We
focus on improving the RL agent's performance with multifidelity data.
Specifically, a multifidelity estimator that exploits the cross-correlations
between the low- and high-fidelity returns is proposed to reduce the variance
in the estimation of the state-action value function. The proposed estimator,
which is based on the method of control variates, is used to design a
multifidelity Monte Carlo RL (MFMCRL) algorithm that improves the learning of
the agent in the high-fidelity environment. The impacts of variance reduction
on policy evaluation and policy improvement are theoretically analyzed by using
probability bounds. Our theoretical analysis and numerical experiments
demonstrate that for a finite budget of high-fidelity data samples, our
proposed MFMCRL agent attains superior performance compared with that of a
standard RL agent that uses only the high-fidelity environment data for
learning the optimal policy.",0,0,0,0,0,0,0.0839182,11.0,0.632378,43
e1c5f2e9-1c95-48d5-8872-ce01e68d1ff3,A Unified Framework of Medical Information Annotation and Extraction for Chinese Clinical Text,3,0.0409269,0.289872,"Medical information extraction consists of a group of natural language
processing (NLP) tasks, which collaboratively convert clinical text to
pre-defined structured formats. Current state-of-the-art (SOTA) NLP models are
highly integrated with deep learning techniques and thus require massive
annotated linguistic data. This study presents an engineering framework of
medical entity recognition, relation extraction and attribute extraction, which
are unified in annotation, modeling and evaluation. Specifically, the
annotation scheme is comprehensive, and compatible between tasks, especially
for the medical relations. The resulted annotated corpus includes 1,200 full
medical records (or 18,039 broken-down documents), and achieves inter-annotator
agreements (IAAs) of 94.53%, 73.73% and 91.98% F 1 scores for the three tasks.
Three task-specific neural network models are developed within a shared
structure, and enhanced by SOTA NLP techniques, i.e., pre-trained language
models. Experimental results show that the system can retrieve medical
entities, relations and attributes with F 1 scores of 93.47%, 67.14% and
90.89%, respectively. This study, in addition to our publicly released
annotation scheme and code, provides solid and practical engineering experience
of developing an integrated medical information extraction system.",1,1,0,1,0,0,0.650766,11.0,0.858297,56
217a3480-b47d-4919-af55-21ce2b2398aa,Domain-Adaptive Text Classification with Structured Knowledge from Unlabeled Data,5,0.028662,0.361962,"Domain adaptive text classification is a challenging problem for the
large-scale pretrained language models because they often require expensive
additional labeled data to adapt to new domains. Existing works usually fails
to leverage the implicit relationships among words across domains. In this
paper, we propose a novel method, called Domain Adaptation with Structured
Knowledge (DASK), to enhance domain adaptation by exploiting word-level
semantic relationships. DASK first builds a knowledge graph to capture the
relationship between pivot terms (domain-independent words) and non-pivot terms
in the target domain. Then during training, DASK injects pivot-related
knowledge graph information into source domain texts. For the downstream task,
these knowledge-injected texts are fed into a BERT variant capable of
processing knowledge-injected textual data. Thanks to the knowledge injection,
our model learns domain-invariant features for non-pivots according to their
relationships with pivots. DASK ensures the pivots to have domain-invariant
behaviors by dynamically inferring via the polarity scores of candidate pivots
during training with pseudo-labels. We validate DASK on a wide range of
cross-domain sentiment classification tasks and observe up to 2.9% absolute
performance improvement over baselines for 20 different domain pairs. Code will
be made available at https://github.com/hikaru-nara/DASK.",0,1,0,0,0,0,0.571501,8.0,0.778132,30
227bff44-d9d9-458c-9912-8432953cd5a4,Decomposing Counterfactual Explanations for Consequential Decision Making,1,0.026028,0.0200974,"The goal of algorithmic recourse is to reverse unfavorable decisions (e.g.,
from loan denial to approval) under automated decision making by suggesting
actionable feature changes (e.g., reduce the number of credit cards). To
generate low-cost recourse the majority of methods work under the assumption
that the features are independently manipulable (IMF). To address the feature
dependency issue the recourse problem is usually studied through the causal
recourse paradigm. However, it is well known that strong assumptions, as
encoded in causal models and structural equations, hinder the applicability of
these methods in complex domains where causal dependency structures are
ambiguous. In this work, we develop \texttt{DEAR} (DisEntangling Algorithmic
Recourse), a novel and practical recourse framework that bridges the gap
between the IMF and the strong causal assumptions. \texttt{DEAR} generates
recourses by disentangling the latent representation of co-varying features
from a subset of promising recourse features to capture the main practical
recourse desiderata. Our experiments on real-world data corroborate our
theoretically motivated recourse model and highlight our framework's ability to
provide reliable, low-cost recourse in the presence of feature dependencies.",0,1,0,0,0,0,0.892479,7.0,0.884656,46
080298ec-8d2c-49ad-8f6f-892d5d3fa6dc,Rewarding Episodic Visitation Discrepancy for Exploration in Reinforcement Learning,5,0.0,0.167784,"Exploration is critical for deep reinforcement learning in complex
environments with high-dimensional observations and sparse rewards. To address
this problem, recent approaches proposed to leverage intrinsic rewards to
improve exploration, such as novelty-based exploration and prediction-based
exploration. However, many intrinsic reward modules require sophisticated
structures and representation learning, resulting in prohibitive computational
complexity and unstable performance. In this paper, we propose Rewarding
Episodic Visitation Discrepancy (REVD), a computation-efficient and quantified
exploration method. More specifically, REVD provides intrinsic rewards by
evaluating the R\'enyi divergence-based visitation discrepancy between
episodes. To make efficient divergence estimation, a k-nearest neighbor
estimator is utilized with a randomly-initialized state encoder. Finally, the
REVD is tested on Atari games and PyBullet Robotics Environments. Extensive
experiments demonstrate that REVD can significantly improves the sample
efficiency of reinforcement learning algorithms and outperforms the
benchmarking methods.",1,1,0,0,0,1,0.0304005,12.0,0.576055,40
60ac230a-2767-4210-a796-604e95158f02,Multi-modal Contrastive Representation Learning for Entity Alignment,28,0.355412,0.90175,"Multi-modal entity alignment aims to identify equivalent entities between two
different multi-modal knowledge graphs, which consist of structural triples and
images associated with entities. Most previous works focus on how to utilize
and encode information from different modalities, while it is not trivial to
leverage multi-modal knowledge in entity alignment because of the modality
heterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive
Learning based Entity Alignment model, to obtain effective joint
representations for multi-modal entity alignment. Different from previous
works, MCLEA considers task-oriented modality and models the inter-modal
relationships for each entity representation. In particular, MCLEA firstly
learns multiple individual representations from multiple modalities, and then
performs contrastive learning to jointly model intra-modal and inter-modal
interactions. Extensive experimental results show that MCLEA outperforms
state-of-the-art baselines on public datasets under both supervised and
unsupervised settings.",1,1,0,0,1,0,0.698469,6.0,0.761992,40
81fbe560-e352-4f56-b959-e98c2f5ae5fc,BSAL: A Framework of Bi-component Structure and Attribute Learning for Link Prediction,5,0.0293539,0.378012,"Given the ubiquitous existence of graph-structured data, learning the
representations of nodes for the downstream tasks ranging from node
classification, link prediction to graph classification is of crucial
importance. Regarding missing link inference of diverse networks, we revisit
the link prediction techniques and identify the importance of both the
structural and attribute information. However, the available techniques either
heavily count on the network topology which is spurious in practice or cannot
integrate graph topology and features properly. To bridge the gap, we propose a
bicomponent structural and attribute learning framework (BSAL) that is designed
to adaptively leverage information from topology and feature spaces.
Specifically, BSAL constructs a semantic topology via the node attributes and
then gets the embeddings regarding the semantic view, which provides a flexible
and easy-to-implement solution to adaptively incorporate the information
carried by the node attributes. Then the semantic embedding together with
topology embedding is fused together using an attention mechanism for the final
prediction. Extensive experiments show the superior performance of our proposal
and it significantly outperforms baselines on diverse research benchmarks.",1,0,0,0,1,0,0.428875,6.0,0.635168,47
e4c21488-d246-44a2-8c7b-57f926b4dee3,Preserving Fine-Grain Feature Information in Classification via Entropic Regularization,3,0.0392757,0.151416,"Labeling a classification dataset implies to define classes and associated
coarse labels, that may approximate a smoother and more complicated ground
truth. For example, natural images may contain multiple objects, only one of
which is labeled in many vision datasets, or classes may result from the
discretization of a regression problem. Using cross-entropy to train
classification models on such coarse labels is likely to roughly cut through
the feature space, potentially disregarding the most meaningful such features,
in particular losing information on the underlying fine-grain task. In this
paper we are interested in the problem of solving fine-grain classification or
regression, using a model trained on coarse-grain labels only. We show that
standard cross-entropy can lead to overfitting to coarse-related features. We
introduce an entropy-based regularization to promote more diversity in the
feature space of trained models, and empirically demonstrate the efficacy of
this methodology to reach better performance on the fine-grain problems. Our
results are supported through theoretical developments and empirical
validation.",1,0,0,0,0,0,0.505491,12.0,0.836655,38
6463ba60-5cc2-4bf1-abca-6b1cd18eaedb,"Statistical Dataset Evaluation: Reliability, Difficulty, and Validity",2,0.00173429,0.0361859,"Datasets serve as crucial training resources and model performance trackers.
However, existing datasets have exposed a plethora of problems, inducing biased
models and unreliable evaluation results. In this paper, we propose a
model-agnostic dataset evaluation framework for automatic dataset quality
evaluation. We seek the statistical properties of the datasets and address
three fundamental dimensions: reliability, difficulty, and validity, following
a classical testing theory. Taking the Named Entity Recognition (NER) datasets
as a case study, we introduce $9$ statistical metrics for a statistical dataset
evaluation framework. Experimental results and human evaluation validate that
our evaluation framework effectively assesses various aspects of the dataset
quality. Furthermore, we study how the dataset scores on our statistical
metrics affect the model performance, and appeal for dataset quality evaluation
or targeted dataset improvement before training or testing models.",0,1,0,0,0,0,0.0576313,7.0,0.366629,45
57560b93-b5bf-45e5-a85d-2714bc5cd4bb,Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval,2,0.0102867,0.105521,"Contrastive learning (CL) has become a ubiquitous approach for several
natural language processing (NLP) downstream tasks, especially for question
answering (QA). However, the major challenge, how to efficiently train the
knowledge retrieval model in an unsupervised manner, is still unresolved.
Recently the commonly used methods are composed of CL and masked language model
(MLM). Unexpectedly, MLM ignores the sentence-level training, and CL also
neglects extraction of the internal info from the query. To optimize the CL
hardly obtain internal information from the original query, we introduce a
joint training method by combining CL and Auto-MLM for self-supervised
multi-lingual knowledge retrieval. First, we acquire the fixed dimensional
sentence vector. Then, mask some words among the original sentences with random
strategy. Finally, we generate a new token representation for predicting the
masked tokens. Experimental results show that our proposed approach
consistently outperforms all the previous SOTA methods on both AliExpress $\&$
LAZADA service corpus and openly available corpora in 8 languages.",0,1,0,0,1,0,0.804842,6.0,0.813594,26
743f9007-e57e-46c0-b383-db0fb565108c,Practical Phase Retrieval Using Double Deep Image Priors,9,0.103442,0.414824,"Phase retrieval (PR) concerns the recovery of complex phases from complex
magnitudes. We identify the connection between the difficulty level and the
number and variety of symmetries in PR problems. We focus on the most difficult
far-field PR (FFPR), and propose a novel method using double deep image priors.
In realistic evaluation, our method outperforms all competing methods by large
margins. As a single-instance method, our method requires no training data and
minimal hyperparameter tuning, and hence enjoys good practicality.",0,1,0,0,1,0,0.0696894,10.0,0.576276,56
5477b618-abc5-4232-8cb9-b3f60aa8eb4f,Detector-Free Weakly Supervised Group Activity Recognition,23,0.392427,0.814712,"Group activity recognition is the task of understanding the activity
conducted by a group of people as a whole in a multi-person video. Existing
models for this task are often impractical in that they demand ground-truth
bounding box labels of actors even in testing or rely on off-the-shelf object
detectors. Motivated by this, we propose a novel model for group activity
recognition that depends neither on bounding box labels nor on object detector.
Our model based on Transformer localizes and encodes partial contexts of a
group activity by leveraging the attention mechanism, and represents a video
clip as a set of partial context embeddings. The embedding vectors are then
aggregated to form a single group representation that reflects the entire
context of an activity while capturing temporal evolution of each partial
context. Our method achieves outstanding performance on two benchmarks,
Volleyball and NBA datasets, surpassing not only the state of the art trained
with the same level of supervision, but also some of existing models relying on
stronger supervision.",1,1,0,0,1,0,0.75199,10.0,0.872295,65
34ecfb86-a82b-4d84-81d4-125aab93f62a,Direct Speech Translation for Automatic Subtitling,3,0.0898647,0.444886,"Automatic subtitling is the task of automatically translating the speech of
audiovisual content into short pieces of timed text, i.e. subtitles and their
corresponding timestamps. The generated subtitles need to conform to space and
time requirements, while being synchronised with the speech and segmented in a
way that facilitates comprehension. Given its considerable complexity, the task
has so far been addressed through a pipeline of components that separately deal
with transcribing, translating, and segmenting text into subtitles, as well as
predicting timestamps. In this paper, we propose the first direct ST model for
automatic subtitling that generates subtitles in the target language along with
their timestamps with a single model. Our experiments on 7 language pairs show
that our approach outperforms a cascade system in the same data condition, also
being competitive with production tools on both in-domain and newly-released
out-domain benchmarks covering new scenarios.",1,1,0,0,0,0,0.469415,6.0,0.65575,125
7caf0eba-e000-4298-8a65-8037d738b438,An Empirical Study on Cross-X Transfer for Legal Judgment Prediction,15,0.249298,0.749591,"Cross-lingual transfer learning has proven useful in a variety of Natural
Language Processing (NLP) tasks, but it is understudied in the context of legal
NLP, and not at all in Legal Judgment Prediction (LJP). We explore transfer
learning techniques on LJP using the trilingual Swiss-Judgment-Prediction
dataset, including cases written in three languages. We find that cross-lingual
transfer improves the overall results across languages, especially when we use
adapter-based fine-tuning. Finally, we further improve the model's performance
by augmenting the training dataset with machine-translated versions of the
original documents, using a 3x larger training corpus. Further on, we perform
an analysis exploring the effect of cross-domain and cross-regional transfer,
i.e., train a model across domains (legal areas), or regions. We find that in
both settings (legal areas, origin regions), models trained across all groups
perform overall better, while they also have improved results in the worst-case
scenarios. Finally, we report improved results when we ambitiously apply
cross-jurisdiction transfer, where we further augment our dataset with Indian
legal cases.",1,1,0,0,0,0,0.77381,5.0,0.757381,49
99d99c3e-7b40-451c-8d2f-8ce9e980012a,Exploring Visual Prompts for Adapting Large-Scale Models,154,0.981281,0.827194,"We investigate the efficacy of visual prompting to adapt large-scale models
in vision. Following the recent approach from prompt tuning and adversarial
reprogramming, we learn a single image perturbation such that a frozen model
prompted with this perturbation performs a new task. Through comprehensive
experiments, we demonstrate that visual prompting is particularly effective for
CLIP and robust to distribution shift, achieving performance competitive with
standard linear probes. We further analyze properties of the downstream
dataset, prompt design, and output transformation in regard to adaptation
performance. The surprising effectiveness of visual prompting provides a new
perspective on adapting pre-trained models in vision. Code is available at
http://hjbahng.github.io/visual_prompting .",1,0,1,0,0,1,0.977284,4.0,0.930381,51
2498e99d-3cc5-4893-95c1-61447c57abb8,Detection of Fights in Videos: A Comparison Study of Anomaly Detection and Action Recognition,7,0.0258098,0.308443,"Detection of fights is an important surveillance application in videos. Most
existing methods use supervised binary action recognition. Since frame-level
annotations are very hard to get for anomaly detection, weakly supervised
learning using multiple instance learning is widely used. This paper explores
the detection of fights in videos as one special type of anomaly detection and
as binary action recognition. We use the UBI-Fight and NTU-CCTV-Fight datasets
for most of the study since they have frame-level annotations. We find that the
anomaly detection has similar or even better performance than the action
recognition. Furthermore, we study to use anomaly detection as a toolbox to
generate training datasets for action recognition in an iterative way
conditioned on the performance of the anomaly detection. Experiment results
should show that we achieve state-of-the-art performance on three fight
detection datasets.",0,1,0,0,1,0,0.168169,7.0,0.528347,30
5ec8a3de-2d04-4e46-9f8a-5dae9dbb81ba,Localizing Visual Sounds the Easy Way,44,0.516551,0.991228,"Unsupervised audio-visual source localization aims at localizing visible
sound sources in a video without relying on ground-truth localization for
training. Previous works often seek high audio-visual similarities for likely
positive (sounding) regions and low similarities for likely negative regions.
However, accurately distinguishing between sounding and non-sounding regions is
challenging without manual annotations. In this work, we propose a simple yet
effective approach for Easy Visual Sound Localization, namely EZ-VSL, without
relying on the construction of positive and/or negative regions during
training. Instead, we align audio and visual spaces by seeking audio-visual
representations that are aligned in, at least, one location of the associated
image, while not matching other images, at any location. We also introduce a
novel object guided localization scheme at inference time for improved
precision. Our simple and effective framework achieves state-of-the-art
performance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In
particular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to
83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is
available at https://github.com/stoneMo/EZ-VSL.",1,1,0,0,1,0,0.893997,8.0,0.899869,36
8aa0fbb3-5504-485a-8616-43e94daad554,Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling,5,0.042214,0.162729,"Generative Spoken Language Modeling research focuses on optimizing speech
Language Models (LMs) using raw audio recordings without accessing any textual
supervision. Such speech LMs usually operate over discrete units obtained from
quantizing internal representations of self-supervised models. Although such
units show impressive modeling results, their robustness capabilities have not
been extensively investigated. This work focuses on improving the robustness of
discrete input representations for generative spoken language modeling. First,
we formally define how to measure the robustness of such representations to
various signal variations that do not alter the spoken information (e.g.,
time-stretch). Next, we empirically demonstrate how current state-of-the-art
representation models lack robustness to such variations. To overcome this, we
propose an effective and efficient method to learn robust discrete speech
representation for generative spoken language modeling. The proposed approach
is based on applying a set of signal transformations to the speech signal and
optimizing the model using an iterative pseudo-labeling scheme. Our method
significantly improves over the evaluated baselines when considering encoding
and modeling metrics. We additionally evaluate our method on the
speech-to-speech translation task, considering Spanish-English and
French-English translations, and show the proposed approach outperforms the
evaluated baselines.",0,0,0,0,0,0,0.768568,5.0,0.754274,39
76da099d-c212-447f-a143-ec9270d19f43,Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks,5,0.0679962,0.428203,"Facial expression in-the-wild is essential for various interactive computing
domains. Especially, ""Learning from Synthetic Data"" (LSD) is an important topic
in the facial expression recognition task. In this paper, we propose a
multi-task learning-based facial expression recognition approach which consists
of emotion and appearance learning branches that can share all face
information, and present preliminary results for the LSD challenge introduced
in the 4th affective behavior analysis in-the-wild (ABAW) competition. Our
method achieved the mean F1 score of 0.71.",0,1,0,0,0,0,0.546659,8.0,0.769531,34
7331276b-7e1c-440b-9e5f-83da33c9b7b3,Calibrated Interpretation: Confidence Estimation in Semantic Parsing,12,0.0836827,0.513746,"Sequence generation models are increasingly being used to translate natural
language into programs, i.e. to perform executable semantic parsing. The fact
that semantic parsing aims to predict programs that can lead to executed
actions in the real world motivates developing safe systems. This in turn makes
measuring calibration -- a central component to safety -- particularly
important. We investigate the calibration of popular generation models across
four popular semantic parsing datasets, finding that it varies across models
and datasets. We then analyze factors associated with calibration error and
release new confidence-based challenge splits of two parsing datasets. To
facilitate the inclusion of calibration in semantic parsing evaluations, we
release a library for computing calibration metrics.",0,1,0,0,0,0,0.270186,7.0,0.605039,102
151ea524-5fc3-445a-a554-5d76bf2ea776,DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks,2,0.0381218,0.0451114,"Since 2017, the Transformer-based models play critical roles in various
downstream Natural Language Processing tasks. However, a common limitation of
the attention mechanism utilized in Transformer Encoder is that it cannot
automatically capture the information of word order, so explicit position
embeddings are generally required to be fed into the target model. In contrast,
Transformer Decoder with the causal attention masks is naturally sensitive to
the word order. In this work, we focus on improving the position encoding
ability of BERT with the causal attention masks. Furthermore, we propose a new
pre-trained language model DecBERT and evaluate it on the GLUE benchmark.
Experimental results show that (1) the causal attention mask is effective for
BERT on the language understanding tasks; (2) our DecBERT model without
position embeddings achieve comparable performance on the GLUE benchmark; and
(3) our modification accelerates the pre-training process and DecBERT w/ PE
achieves better overall performance than the baseline systems when pre-training
with the same amount of computational resources.",0,1,0,0,0,1,0.925347,7.0,0.906301,58
cc267365-56c6-439d-b5b7-84e972ea30ff,Image-free Domain Generalization via CLIP for 3D Hand Pose Estimation,4,0.0172592,0.275158,"RGB-based 3D hand pose estimation has been successful for decades thanks to
large-scale databases and deep learning. However, the hand pose estimation
network does not operate well for hand pose images whose characteristics are
far different from the training data. This is caused by various factors such as
illuminations, camera angles, diverse backgrounds in the input images, etc.
Many existing methods tried to solve it by supplying additional large-scale
unconstrained/target domain images to augment data space; however collecting
such large-scale images takes a lot of labors. In this paper, we present a
simple image-free domain generalization approach for the hand pose estimation
framework that uses only source domain data. We try to manipulate the image
features of the hand pose estimation network by adding the features from text
descriptions using the CLIP (Contrastive Language-Image Pre-training) model.
The manipulated image features are then exploited to train the hand pose
estimation network via the contrastive learning framework. In experiments with
STB and RHD datasets, our algorithm shows improved performance over the
state-of-the-art domain generalization approaches.",0,1,0,0,1,0,0.161631,7.0,0.52214,65
48bdb1ff-dfc5-40dd-8dbc-952c9c61a59e,CLEAR: Causal Explanations from Attention in Neural Recommenders,3,0.0577327,0.168822,"We present CLEAR, a method for learning session-specific causal graphs, in
the possible presence of latent confounders, from attention in pre-trained
attention-based recommenders. These causal graphs describe user behavior,
within the context captured by attention, and can provide a counterfactual
explanation for a recommendation. In essence, these causal graphs allow
answering ""why"" questions uniquely for any specific session. Using empirical
evaluations we show that, compared to naively using attention weights to
explain input-output relations, counterfactual explanations found by CLEAR are
shorter and an alternative recommendation is ranked higher in the original
top-k recommendations.",0,0,0,0,0,0,0.722183,12.0,0.886503,33
0d9e1551-71ea-49ec-88b4-c544d9783839,WinoDict: Probing language models for in-context word acquisition,6,0.200043,0.71724,"We introduce a new in-context learning paradigm to measure Large Language
Models' (LLMs) ability to learn novel words during inference. In particular, we
rewrite Winograd-style co-reference resolution problems by replacing the key
concept word with a synthetic but plausible word that the model must understand
to complete the task. Solving this task requires the model to make use of the
dictionary definition of the new word given in the prompt. This benchmark
addresses word acquisition, one important aspect of the diachronic degradation
known to afflict LLMs. As LLMs are frozen in time at the moment they are
trained, they are normally unable to reflect the way language changes over
time. We show that the accuracy of LLMs compared to the original Winograd tasks
decreases radically in our benchmark, thus identifying a limitation of current
models and providing a benchmark to measure future improvements in LLMs ability
to do in-context learning.",1,0,1,1,0,0,0.964757,6.0,0.93303,30
a36f896c-4dbd-416f-b841-6818b3a3182f,Towards Responsible AI for Financial Transactions,13,0.301678,0.230077,"The application of AI in finance is increasingly dependent on the principles
of responsible AI. These principles - explainability, fairness, privacy,
accountability, transparency and soundness form the basis for trust in future
AI systems. In this study, we address the first principle by providing an
explanation for a deep neural network that is trained on a mixture of
numerical, categorical and textual inputs for financial transaction
classification. The explanation is achieved through (1) a feature importance
analysis using Shapley additive explanations (SHAP) and (2) a hybrid approach
of text clustering and decision tree classifiers. We then test the robustness
of the model by exposing it to a targeted evasion attack, leveraging the
knowledge we gained about the model through the extracted explanation.",0,1,0,0,0,0,0.75844,7.0,0.820238,22
1a59b49d-8c9e-4e4b-a2ab-83db2aed79ee,InterpretTime: a new approach for the systematic evaluation of neural-network interpretability in time series classification,5,0.0,0.0876221,"We present a novel approach to evaluate the performance of interpretability
methods for time series classification, and propose a new strategy to assess
the similarity between domain experts and machine data interpretation. The
novel approach leverages a new family of synthetic datasets and introduces new
interpretability evaluation metrics. The approach addresses several common
issues encountered in the literature, and clearly depicts how well an
interpretability method is capturing neural network's data usage, providing a
systematic interpretability evaluation framework. The new methodology
highlights the superiority of Shapley Value Sampling and Integrated Gradients
for interpretability in time-series classification tasks.",0,1,0,1,1,0,0.0414555,9.0,0.469836,35
5cb0b583-e902-4df8-93bf-836083589a89,ARMOR: A Model-based Framework for Improving Arbitrary Baseline Policies with Offline Data,5,0.123507,0.328462,"We propose a new model-based offline RL framework, called Adversarial Models
for Offline Reinforcement Learning (ARMOR), which can robustly learn policies
to improve upon an arbitrary baseline policy regardless of data coverage. Based
on the concept of relative pessimism, ARMOR is designed to optimize for the
worst-case relative performance when facing uncertainty. In theory, we prove
that the learned policy of ARMOR never degrades the performance of the baseline
policy with any admissible hyperparameter, and can learn to compete with the
best policy within data coverage when the hyperparameter is well tuned, and the
baseline policy is supported by the data. Such a robust policy improvement
property makes ARMOR especially suitable for building real-world learning
systems, because in practice ensuring no performance degradation is imperative
before considering any benefit learning can bring.",0,0,1,0,0,0,0.968716,5.0,0.926634,21
f14acc71-04d1-46dd-973f-181f3be02e5a,Teacher Forcing Recovers Reward Functions for Text Generation,4,0.018989,0.0765689,"Reinforcement learning (RL) has been widely used in text generation to
alleviate the exposure bias issue or to utilize non-parallel datasets. The
reward function plays an important role in making RL training successful.
However, previous reward functions are typically task-specific and sparse,
restricting the use of RL. In our work, we propose a task-agnostic approach
that derives a step-wise reward function directly from a model trained with
teacher forcing. We additionally propose a simple modification to stabilize the
RL training on non-parallel datasets with our induced reward function.
Empirical results show that our method outperforms self-training and reward
regression methods on several text generation tasks, confirming the
effectiveness of our reward function.",1,1,0,0,0,1,0.077026,12.0,0.655563,70
d6c8b6ad-ba33-4035-8fb9-6fa32c691911,Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation,48,0.0812858,0.561553,"In reinforcement learning for visual navigation, it is common to develop a
model for each new task, and train that model from scratch with task-specific
interactions in 3D environments. However, this process is expensive; massive
amounts of interactions are needed for the model to generalize well. Moreover,
this process is repeated whenever there is a change in the task type or the
goal modality. We present a unified approach to visual navigation using a novel
modular transfer learning model. Our model can effectively leverage its
experience from one source task and apply it to multiple target tasks (e.g.,
ObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch,
audio, label). Furthermore, our model enables zero-shot experience learning,
whereby it can solve the target tasks without receiving any task-specific
interactive training. Our experiments on multiple photorealistic datasets and
challenging tasks show that our approach learns faster, generalizes better, and
outperforms SoTA models by a significant margin.",1,1,0,0,1,0,0.174414,6.0,0.456423,87
3c237491-8413-4514-a07b-2103e410bd0c,Optimal estimation of Gaussian DAG models,9,0.0735383,0.761446,"We study the optimal sample complexity of learning a Gaussian directed
acyclic graph (DAG) from observational data. Our main results establish the
minimax optimal sample complexity for learning the structure of a linear
Gaussian DAG model in two settings of interest: 1) Under equal variances
without knowledge of the true ordering, and 2) For general linear models given
knowledge of the ordering. In both cases the sample complexity is $n\asymp
q\log(d/q)$, where $q$ is the maximum number of parents and $d$ is the number
of nodes. We further make comparisons with the classical problem of learning
(undirected) Gaussian graphical models, showing that under the equal variance
assumption, these two problems share the same optimal sample complexity. In
other words, at least for Gaussian models with equal error variances, learning
a directed graphical model is statistically no more difficult than learning an
undirected graphical model. Our results also extend to more general
identification assumptions as well as subgaussian errors.",0,0,0,0,0,0,0.00882253,17.0,0.627326,50
b63c18f2-0441-49d9-96c7-4891fb70500e,Detecting Emerging Technologies and their Evolution using Deep Learning and Weak Signal Analysis,8,0.0646372,0.506184,"Emerging technologies can have major economic impacts and affect strategic
stability. Yet, early identification of emerging technologies remains
challenging. In order to identify emerging technologies in a timely and
reliable manner, a comprehensive examination of relevant scientific and
technological (S&T) trends and their related references is required. This
examination is generally done by domain experts and requires significant
amounts of time and effort to gain insights. The use of domain experts to
identify emerging technologies from S&T trends may limit the capacity to
analyse large volumes of information and introduce subjectivity in the
assessments. Decision support systems are required to provide accurate and
reliable evidence-based indicators through constant and continuous monitoring
of the environment and help identify signals of emerging technologies that
could alter security and economic prosperity. For example, the research field
of hypersonics has recently witnessed several advancements having profound
technological, commercial, and national security implications. In this work, we
present a multi-layer quantitative approach able to identify future signs from
scientific publications on hypersonics by leveraging deep learning and weak
signal analysis. The proposed framework can help strategic planners and domain
experts better identify and monitor emerging technology trends.",0,1,0,0,0,0,0.000177623,21.0,0.512133,80
886ab7ac-2dcc-4dd3-a951-ae7755820d7b,Learn what matters: cross-domain imitation learning with task-relevant embeddings,11,0.122254,0.613829,"We study how an autonomous agent learns to perform a task from demonstrations
in a different domain, such as a different environment or different agent. Such
cross-domain imitation learning is required to, for example, train an
artificial agent from demonstrations of a human expert. We propose a scalable
framework that enables cross-domain imitation learning without access to
additional demonstrations or further domain knowledge. We jointly train the
learner agent's policy and learn a mapping between the learner and expert
domains with adversarial training. We effect this by using a mutual information
criterion to find an embedding of the expert's state space that contains
task-relevant information and is invariant to domain specifics. This step
significantly simplifies estimating the mapping between the learner and expert
domains and hence facilitates end-to-end learning. We demonstrate successful
transfer of policies between considerably different domains, without extra
supervision such as additional demonstrations, and in situations where other
methods fail.",1,0,0,0,0,0,0.521798,7.0,0.726625,57
2a87a284-2a97-4c6c-a825-fc566ac0e319,Boilerplate Detection via Semantic Classification of TextBlocks,3,0.0036595,0.0243924,"We present a hierarchical neural network model called SemText to detect HTML
boilerplate based on a novel semantic representation of HTML tags, class names,
and text blocks. We train SemText on three published datasets of news webpages
and fine-tune it using a small number of development data in CleanEval and
GoogleTrends-2017. We show that SemText achieves the state-of-the-art accuracy
on these datasets. We then demonstrate the robustness of SemText by showing
that it also detects boilerplate effectively on out-of-domain community-based
question-answer webpages.",1,1,0,0,1,0,3.70401e-05,17.0,0.305122,29
4f779920-588b-4c09-a7f4-c9561747ef0b,Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning,42,0.250344,0.767272,"Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions
formed from seen state and object during training. Since the same state may be
various in the visual appearance while entangled with different objects, CZSL
is still a challenging task. Some methods recognize state and object with two
trained classifiers, ignoring the impact of the interaction between object and
state; the other methods try to learn the joint representation of the
state-object compositions, leading to the domain gap between seen and unseen
composition sets. In this paper, we propose a novel Siamese Contrastive
Embedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for
unseen composition recognition. Considering the entanglement between state and
object, we embed the visual feature into a Siamese Contrastive Space to capture
prototypes of them separately, alleviating the interaction between state and
object. In addition, we design a State Transition Module (STM) to increase the
diversity of training compositions, improving the robustness of the recognition
model. Extensive experiments indicate that our method significantly outperforms
the state-of-the-art approaches on three challenging benchmark datasets,
including the recent proposed C-QGA dataset.",1,1,0,0,1,0,0.41863,7.0,0.682679,34
9ea7f630-66e4-4387-b3f2-cba04a0f024f,Psychiatric Scale Guided Risky Post Screening for Early Detection of Depression,14,0.393798,0.214362,"Depression is a prominent health challenge to the world, and early risk
detection (ERD) of depression from online posts can be a promising technique
for combating the threat. Early depression detection faces the challenge of
efficiently tackling streaming data, balancing the tradeoff between timeliness,
accuracy and explainability. To tackle these challenges, we propose a
psychiatric scale guided risky post screening method that can capture risky
posts related to the dimensions defined in clinical depression scales, and
providing interpretable diagnostic basis. A Hierarchical Attentional Network
equipped with BERT (HAN-BERT) is proposed to further advance explainable
predictions. For ERD, we propose an online algorithm based on an evolving queue
of risky posts that can significantly reduce the number of model inferences to
boost efficiency. Experiments show that our method outperforms the competitive
feature-based and neural models under conventional depression detection
settings, and achieves simultaneous improvement in both efficacy and efficiency
for ERD.",0,0,0,0,0,0,0.960693,10.0,0.956501,29
1a063aff-3e43-44c9-8561-fc3fd5557407,Neural Cloth Simulation,28,0.223767,0.887304,"We present a general framework for the garment animation problem through
unsupervised deep learning inspired in physically based simulation. Existing
trends in the literature already explore this possibility. Nonetheless, these
approaches do not handle cloth dynamics. Here, we propose the first methodology
able to learn realistic cloth dynamics unsupervisedly, and henceforth, a
general formulation for neural cloth simulation. The key to achieve this is to
adapt an existing optimization scheme for motion from simulation based
methodologies to deep learning. Then, analyzing the nature of the problem, we
devise an architecture able to automatically disentangle static and dynamic
cloth subspaces by design. We will show how this improves model performance.
Additionally, this opens the possibility of a novel motion augmentation
technique that greatly improves generalization. Finally, we show it also allows
to control the level of motion in the predictions. This is a useful, never seen
before, tool for artists. We provide of detailed analysis of the problem to
establish the bases of neural cloth simulation and guide future research into
the specifics of this domain.",0,0,1,0,0,0,0.0562299,13.0,0.657004,49
097ccbb8-d8e6-4fa0-b857-79fa7cda91f1,Where did you tweet from? Inferring the origin locations of tweets based on contextual information,5,0.179546,0.617902,"Public conversations on Twitter comprise many pertinent topics including
disasters, protests, politics, propaganda, sports, climate change,
epidemics/pandemic outbreaks, etc., that can have both regional and global
aspects. Spatial discourse analysis rely on geographical data. However, today
less than 1% of tweets are geotagged; in both cases--point location or bounding
place information. A major issue with tweets is that Twitter users can be at
location A and exchange conversations specific to location B, which we call the
Location A/B problem. The problem is considered solved if location entities can
be classified as either origin locations (Location As) or non-origin locations
(Location Bs). In this work, we propose a simple yet effective framework--the
True Origin Model--to address the problem that uses machine-level natural
language understanding to identify tweets that conceivably contain their origin
location information. The model achieves promising accuracy at country (80%),
state (67%), city (58%), county (56%) and district (64%) levels with support
from a Location Extraction Model as basic as the CoNLL-2003-based RoBERTa. We
employ a tweet contexualizer (locBERT) which is one of the core components of
the proposed model, to investigate multiple tweets' distributions for
understanding Twitter users' tweeting behavior in terms of mentioning origin
and non-origin locations. We also highlight a major concern with the currently
regarded gold standard test set (ground truth) methodology, introduce a new
data set, and identify further research avenues for advancing the area.",0,1,0,0,0,0,0.31805,14.0,0.816451,36
996e4b89-85ad-49bf-9ee3-d8679cbfd8ea,Academic Resource Text Level Multi-label Classification based on Attention,1,0.0110462,0.0364364,"Hierarchical multi-label academic text classification (HMTC) is to assign
academic texts into a hierarchically structured labeling system. We propose an
attention-based hierarchical multi-label classification algorithm of academic
texts (AHMCA) by integrating features such as text, keywords, and hierarchical
structure, the academic documents are classified into the most relevant
categories. We utilize word2vec and BiLSTM to obtain embedding and latent
vector representations of text, keywords, and hierarchies. We use hierarchical
attention mechanism to capture the associations between keywords, label
hierarchies, and text word vectors to generate hierarchical-specific document
embedding vectors to replace the original text embeddings in HMCN-F. The
experimental results on the academic text dataset demonstrate the effectiveness
of the AHMCA algorithm.",0,1,0,0,0,0,0.0901184,13.0,0.694675,37
9aa32590-9c26-4d60-93c7-1f239280ddb4,TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval,69,0.618997,0.919095,"Text-Video retrieval is a task of great practical value and has received
increasing attention, among which learning spatial-temporal video
representation is one of the research hotspots. The video encoders in the
state-of-the-art video retrieval models usually directly adopt the pre-trained
vision backbones with the network structure fixed, they therefore can not be
further improved to produce the fine-grained spatial-temporal video
representation. In this paper, we propose Token Shift and Selection Network
(TS2-Net), a novel token shift and selection transformer architecture, which
dynamically adjusts the token sequence and selects informative tokens in both
temporal and spatial dimensions from input video samples. The token shift
module temporally shifts the whole token features back-and-forth across
adjacent frames, to preserve the complete token representation and capture
subtle movements. Then the token selection module selects tokens that
contribute most to local spatial semantics. Based on thorough experiments, the
proposed TS2-Net achieves state-of-the-art performance on major text-video
retrieval benchmarks, including new records on MSRVTT, VATEX, LSMDC,
ActivityNet, and DiDeMo.",1,1,0,0,1,0,0.950156,5.0,0.897759,52
58bdb750-54e5-473e-9f0d-2fa19abc0612,"Dynamic Sparse Network for Time Series Classification: Learning What to ""see''",12,0.0852565,0.872512,"The receptive field (RF), which determines the region of time series to be
``seen'' and used, is critical to improve the performance for time series
classification (TSC). However, the variation of signal scales across and within
time series data, makes it challenging to decide on proper RF sizes for TSC. In
this paper, we propose a dynamic sparse network (DSN) with sparse connections
for TSC, which can learn to cover various RF without cumbersome
hyper-parameters tuning. The kernels in each sparse layer are sparse and can be
explored under the constraint regions by dynamic sparse training, which makes
it possible to reduce the resource cost. The experimental results show that the
proposed DSN model can achieve state-of-art performance on both univariate and
multivariate TSC datasets with less than 50\% computational cost compared with
recent baseline methods, opening the path towards more accurate resource-aware
methods for time series analyses. Our code is publicly available at:
https://github.com/QiaoXiao7282/DSN.",1,1,0,0,1,0,0.248737,7.0,0.591257,61
571b8a31-da07-499f-8250-7dd597d74b32,Making Heads or Tails: Towards Semantically Consistent Visual Counterfactuals,19,0.262118,0.294748,"A visual counterfactual explanation replaces image regions in a query image
with regions from a distractor image such that the system's decision on the
transformed image changes to the distractor class. In this work, we present a
novel framework for computing visual counterfactual explanations based on two
key ideas. First, we enforce that the replaced and replacer regions contain the
same semantic part, resulting in more semantically consistent explanations.
Second, we use multiple distractor images in a computationally efficient way
and obtain more discriminative explanations with fewer region replacements. Our
approach is 27 % more semantically consistent and an order of magnitude faster
than a competing method on three fine-grained image recognition datasets. We
highlight the utility of our counterfactuals over existing works through
machine teaching experiments where we teach humans to classify different bird
species. We also complement our explanations with the vocabulary of parts and
attributes that contributed the most to the system's decision. In this task as
well, we obtain state-of-the-art results when using our counterfactual
explanations relative to existing works, reinforcing the importance of
semantically consistent explanations. Source code is available at
https://github.com/facebookresearch/visual-counterfactuals.",1,1,0,0,0,0,0.880925,8.0,0.893218,64
24d3d2b9-42bf-47bc-99aa-660ed981f2f2,Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings,13,0.0730849,0.80125,"Although contextualized embeddings generated from large-scale pre-trained
models perform well in many tasks, traditional static embeddings (e.g.,
Skip-gram, Word2Vec) still play an important role in low-resource and
lightweight settings due to their low computational cost, ease of deployment,
and stability. In this paper, we aim to improve word embeddings by 1)
incorporating more contextual information from existing pre-trained models into
the Skip-gram framework, which we call Context-to-Vec; 2) proposing a
post-processing retrofitting method for static embeddings independent of
training by employing priori synonym knowledge and weighted vector
distribution. Through extrinsic and intrinsic tasks, our methods are well
proven to outperform the baselines by a large margin.",1,1,0,0,1,0,0.093352,13.0,0.697522,33
290bedcd-a1b1-4f9f-be83-8f50a9c7e3ff,Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition,13,0.278836,0.796594,"This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech
Recognition (AVSR) system. To this end, we propose Visual Context-driven Audio
Feature Enhancement module (V-CAFE) to enhance the input noisy audio speech
with a help of audio-visual correspondence. The proposed V-CAFE is designed to
capture the transition of lip movements, namely visual context and to generate
a noise reduction mask by considering the obtained visual context. Through
context-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be
refined for mask generation. The noisy representations are masked out with the
noise reduction mask resulting in enhanced audio features. The enhanced audio
features are fused with the visual features and taken to an encoder-decoder
model composed of Conformer and Transformer for speech recognition. We show the
proposed end-to-end AVSR with the V-CAFE can further improve the
noise-robustness of AVSR. The effectiveness of the proposed method is evaluated
in noisy speech recognition and overlapped speech recognition experiments using
the two largest audio-visual datasets, LRS2 and LRS3.",0,1,0,0,0,0,0.819994,8.0,0.86623,37
0f362834-1f48-45dc-b48e-5c1af6e0677a,"The Conversational Short-phrase Speaker Diarization (CSSD) Task: Dataset, Evaluation Metric and Baselines",6,0.219823,0.986236,"The conversation scenario is one of the most important and most challenging
scenarios for speech processing technologies because people in conversation
respond to each other in a casual style. Detecting the speech activities of
each person in a conversation is vital to downstream tasks, like natural
language processing, machine translation, etc. People refer to the detection
technology of ""who speak when"" as speaker diarization (SD). Traditionally,
diarization error rate (DER) has been used as the standard evaluation metric of
SD systems for a long time. However, DER fails to give enough importance to
short conversational phrases, which are short but important on the semantic
level. Also, a carefully and accurately manually-annotated testing dataset
suitable for evaluating the conversational SD technologies is still unavailable
in the speech community. In this paper, we design and describe the
Conversational Short-phrases Speaker Diarization (CSSD) task, which consists of
training and testing datasets, evaluation metric and baselines. In the dataset
aspect, despite the previously open-sourced 180-hour conversational
MagicData-RAMC dataset, we prepare an individual 20-hour conversational speech
test dataset with carefully and artificially verified speakers timestamps
annotations for the CSSD task. In the metric aspect, we design the new
conversational DER (CDER) evaluation metric, which calculates the SD accuracy
at the utterance level. In the baseline aspect, we adopt a commonly used
method: Variational Bayes HMM x-vector system, as the baseline of the CSSD
task. Our evaluation metric is publicly available at
https://github.com/SpeechClub/CDER_Metric.",1,1,1,1,0,0,0.845489,6.0,0.83586,34
864fbb92-0798-4a36-a7f8-d4bcf7796320,Volume Rendering Digest (for NeRF),22,0.316566,0.561598,"Neural Radiance Fields employ simple volume rendering as a way to overcome
the challenges of differentiating through ray-triangle intersections by
leveraging a probabilistic notion of visibility. This is achieved by assuming
the scene is composed by a cloud of light-emitting particles whose density
changes in space. This technical report summarizes the derivations for
differentiable volume rendering. It is a condensed version of previous reports,
but rewritten in the context of NeRF, and adopting its commonly used notation.",0,0,0,0,0,0,0.00241982,37.0,0.793722,3
1fb102f0-aa2a-4d18-86b2-e85f226d2cd9,Applied monocular reconstruction of parametric faces with domain engineering,2,0.014166,0.283763,"Many modern online 3D applications and videogames rely on parametric models
of human faces for creating believable avatars. However, manual reproduction of
someone's facial likeness with a parametric model is difficult and
time-consuming. Machine Learning solution for that task is highly desirable but
is also challenging. The paper proposes a novel approach to the so-called
Face-to-Parameters problem (F2P for short), aiming to reconstruct a parametric
face from a single image. The proposed method utilizes synthetic data, domain
decomposition, and domain adaptation for addressing multifaceted challenges in
solving the F2P. The open-sourced codebase illustrates our key observations and
provides means for quantitative evaluation. The presented approach proves
practical in an industrial application; it improves accuracy and allows for
more efficient models training. The techniques have the potential to extend to
other types of parametric models.",0,1,1,0,0,0,0.376883,10.0,0.764195,38
27e786b8-10d6-4744-ba01-4af4ba4d1712,Adaptation of domain-specific transformer models with text oversampling for sentiment analysis of social media posts on Covid-19 vaccines,3,0.0105982,0.222998,"Covid-19 has spread across the world and several vaccines have been developed
to counter its surge. To identify the correct sentiments associated with the
vaccines from social media posts, we fine-tune various state-of-the-art
pre-trained transformer models on tweets associated with Covid-19 vaccines.
Specifically, we use the recently introduced state-of-the-art pre-trained
transformer models RoBERTa, XLNet and BERT, and the domain-specific transformer
models CT-BERT and BERTweet that are pre-trained on Covid-19 tweets. We further
explore the option of text augmentation by oversampling using Language Model
based Oversampling Technique (LMOTE) to improve the accuracies of these models,
specifically, for small sample datasets where there is an imbalanced class
distribution among the positive, negative and neutral sentiment classes. Our
results summarize our findings on the suitability of text oversampling for
imbalanced small sample datasets that are used to fine-tune state-of-the-art
pre-trained transformer models, and the utility of domain-specific transformer
models for the classification task.",0,1,0,0,0,0,0.198669,6.0,0.480524,47
7b164279-aaed-4a50-9117-155c2fc09d70,"FaceOcc: A Diverse, High-quality Face Occlusion Dataset for Human Face Extraction",6,0.0505855,0.210983,"Occlusions often occur in face images in the wild, troubling face-related
tasks such as landmark detection, 3D reconstruction, and face recognition. It
is beneficial to extract face regions from unconstrained face images
accurately. However, current face segmentation datasets suffer from small data
volumes, few occlusion types, low resolution, and imprecise annotation,
limiting the performance of data-driven-based algorithms. This paper proposes a
novel face occlusion dataset with manually labeled face occlusions from the
CelebA-HQ and the internet. The occlusion types cover sunglasses, spectacles,
hands, masks, scarfs, microphones, etc. To the best of our knowledge, it is by
far the largest and most comprehensive face occlusion dataset. Combining it
with the attribute mask in CelebAMask-HQ, we trained a straightforward face
segmentation model but obtained SOTA performance, convincingly demonstrating
the effectiveness of the proposed dataset.",0,1,1,1,1,0,0.125785,12.0,0.698653,35
1a2ddaed-43a9-430c-b76a-9a628467c270,CLIPascene: Scene Sketching with Different Types and Levels of Abstraction,25,0.32769,0.579664,"In this paper, we present a method for converting a given scene image into a
sketch using different types and multiple levels of abstraction. We distinguish
between two types of abstraction. The first considers the fidelity of the
sketch, varying its representation from a more precise portrayal of the input
to a looser depiction. The second is defined by the visual simplicity of the
sketch, moving from a detailed depiction to a sparse sketch. Using an explicit
disentanglement into two abstraction axes -- and multiple levels for each one
-- provides users additional control over selecting the desired sketch based on
their personal goals and preferences. To form a sketch at a given level of
fidelity and simplification, we train two MLP networks. The first network
learns the desired placement of strokes, while the second network learns to
gradually remove strokes from the sketch without harming its recognizability
and semantics. Our approach is able to generate sketches of complex scenes
including those with complex backgrounds (e.g., natural and urban settings) and
subjects (e.g., animals and people) while depicting gradual abstractions of the
input scene in terms of fidelity and simplicity.",0,1,0,0,0,0,0.797777,5.0,0.771911,54
158182a8-3ffb-4fbe-bf5a-944237ff9afe,AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension,14,0.260882,0.653461,"Recent machine reading comprehension datasets such as ReClor and LogiQA
require performing logical reasoning over text. Conventional neural models are
insufficient for logical reasoning, while symbolic reasoners cannot directly
apply to text. To meet the challenge, we present a neural-symbolic approach
which, to predict an answer, passes messages over a graph representing logical
relations between text units. It incorporates an adaptive logic graph network
(AdaLoGN) which adaptively infers logical relations to extend the graph and,
essentially, realizes mutual and iterative reinforcement between neural and
symbolic reasoning. We also implement a novel subgraph-to-node message passing
mechanism to enhance context-option interaction for answering multiple-choice
questions. Our approach shows promising results on ReClor and LogiQA.",1,0,0,0,0,0,0.806697,6.0,0.814565,50
1232aadd-1e6f-4556-896d-0c5b0c2f2ea2,Towards Unbiased Label Distribution Learning for Facial Pose Estimation Using Anisotropic Spherical Gaussian,9,0.036847,0.37848,"Facial pose estimation refers to the task of predicting face orientation from
a single RGB image. It is an important research topic with a wide range of
applications in computer vision. Label distribution learning (LDL) based
methods have been recently proposed for facial pose estimation, which achieve
promising results. However, there are two major issues in existing LDL methods.
First, the expectations of label distributions are biased, leading to a biased
pose estimation. Second, fixed distribution parameters are applied for all
learning samples, severely limiting the model capability. In this paper, we
propose an Anisotropic Spherical Gaussian (ASG)-based LDL approach for facial
pose estimation. In particular, our approach adopts the spherical Gaussian
distribution on a unit sphere which constantly generates unbiased expectation.
Meanwhile, we introduce a new loss function that allows the network to learn
the distribution parameter for each learning sample flexibly. Extensive
experimental results show that our method sets new state-of-the-art records on
AFLW2000 and BIWI datasets.",0,1,0,0,1,0,0.00853694,13.0,0.510115,52
3f459a2a-130b-493a-9ebb-8837b3241260,PInKS: Preconditioned Commonsense Inference with Minimal Supervision,5,0.073023,0.11952,"Reasoning with preconditions such as ""glass can be used for drinking water
unless the glass is shattered"" remains an open problem for language models. The
main challenge lies in the scarcity of preconditions data and the model's lack
of support for such reasoning. We present PInKS, Preconditioned Commonsense
Inference with WeaK Supervision, an improved model for reasoning with
preconditions through minimum supervision. We show, both empirically and
theoretically, that PInKS improves the results on benchmarks focused on
reasoning with the preconditions of commonsense knowledge (up to 40% Macro-F1
scores). We further investigate PInKS through PAC-Bayesian informativeness
analysis, precision measures, and ablation study.",1,0,0,0,0,0,0.262498,10.0,0.720143,73
f104536c-87cf-4bb1-8057-6c26bbdc6364,Online Segmentation of LiDAR Sequences: Dataset and Algorithm,8,0.0977997,0.537489,"Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles.
However, most semantic datasets and algorithms used for LiDAR sequence
segmentation operate on $360^\circ$ frames, causing an acquisition latency
incompatible with real-time applications. To address this issue, we first
introduce HelixNet, a $10$ billion point dataset with fine-grained labels,
timestamps, and sensor rotation information necessary to accurately assess the
real-time readiness of segmentation algorithms. Second, we propose Helix4D, a
compact and efficient spatio-temporal transformer architecture specifically
designed for rotating LiDAR sequences. Helix4D operates on acquisition slices
corresponding to a fraction of a full sensor rotation, significantly reducing
the total latency. Helix4D reaches accuracy on par with the best segmentation
algorithms on HelixNet and SemanticKITTI with a reduction of over $5\times$ in
terms of latency and $50\times$ in model size. The code and data are available
at: https://romainloiseau.fr/helixnet",1,1,0,1,1,0,0.955986,4.0,0.882359,57
b9a94be4-f9c6-46ef-9ebb-d914f21f5ab6,Machine Learning-Powered Course Allocation,8,0.243665,0.441048,"We study the course allocation problem, where universities assign course
schedules to students. The current state-of-the-art mechanism, Course Match,
has one major shortcoming: students make significant mistakes when reporting
their preferences, which negatively affects welfare and fairness. To address
this issue, we introduce a new mechanism, Machine Learning-powered Course Match
(MLCM). At the core of MLCM is a machine learning-powered preference
elicitation module that iteratively asks personalized pairwise comparison
queries to alleviate students' reporting mistakes. Extensive computational
experiments, grounded in real-world data, demonstrate that MLCM, with only ten
comparison queries, significantly increases both average and minimum student
utility by 7%-11% and 17%-29%, respectively. Finally, we highlight MLCM's
robustness to changes in the environment and show how our design minimizes the
risk of upgrading to MLCM while making the upgrade process simple for
universities and seamless for their students.",0,1,1,0,0,0,0.015796,22.0,0.73866,47
de5afe43-15e3-42cc-b0c5-1a0404f4aa58,RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering,29,0.271891,0.500266,"Finding accurate correspondences among different views is the Achilles' heel
of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the
assumption that corresponding pixels share similar photometric features.
However, multi-view images in real scenarios observe non-Lambertian surfaces
and experience occlusions. In this work, we propose a novel approach with
neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences
among views. Specifically, we impose a depth rendering consistency loss to
constrain the geometry features close to the object surface to alleviate
occlusions. Concurrently, we introduce a reference view synthesis loss to
generate consistent supervision, even for non-Lambertian surfaces. Extensive
experiments on DTU and Tanks\&Temples benchmarks demonstrate that our RC-MVSNet
approach achieves state-of-the-art performance over unsupervised MVS frameworks
and competitive performance to many supervised methods.The code is released at
https://github.com/Boese0601/RC-MVSNet",1,1,0,0,1,0,0.864582,4.0,0.770851,38
1b0e9ccb-8c1f-4102-8fbc-3569abaaa4ef,Self-Supervised Speech Representations Preserve Speech Characteristics while Anonymizing Voices,1,0.0104646,0.0843146,"Collecting speech data is an important step in training speech recognition
systems and other speech-based machine learning models. However, the issue of
privacy protection is an increasing concern that must be addressed. The current
study investigates the use of voice conversion as a method for anonymizing
voices. In particular, we train several voice conversion models using
self-supervised speech representations including Wav2Vec2.0, Hubert and
UniSpeech. Converted voices retain a low word error rate within 1% of the
original voice. Equal error rate increases from 1.52% to 46.24% on the
LibriSpeech test set and from 3.75% to 45.84% on speakers from the VCTK corpus
which signifies degraded performance on speaker verification. Lastly, we
conduct experiments on dysarthric speech data to show that speech features
relevant to articulation, prosody, phonation and phonology can be extracted
from anonymized voices for discriminating between healthy and pathological
speech.",0,1,0,0,0,0,0.554056,4.0,0.544207,33
28c593b8-d071-4a0c-8211-82ee1e5bdefa,Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models,22,0.0367175,0.580953,"We investigate what kind of structural knowledge learned in neural network
encoders is transferable to processing natural language. We design artificial
languages with structural properties that mimic natural language, pretrain
encoders on the data, and see how much performance the encoder exhibits on
downstream tasks in natural language. Our experimental results show that
pretraining with an artificial language with a nesting dependency structure
provides some knowledge transferable to natural language. A follow-up probing
analysis indicates that its success in the transfer is related to the amount of
encoded contextual information and what is transferred is the knowledge of
position-aware context dependence of language. Our results provide insights
into how neural network encoders process human languages and the source of
cross-lingual transferability of recent multilingual language models.",0,0,0,0,0,0,0.261688,6.0,0.532971,50
1343bddc-7d30-4649-b97d-5084f326bf86,Towards a Cleaner Document-Oriented Multilingual Crawled Corpus,94,0.995915,0.99851,"The need for raw large raw corpora has dramatically increased in recent years
with the introduction of transfer learning and semi-supervised learning methods
to Natural Language Processing. And while there have been some recent attempts
to manually curate the amount of data necessary to train large language models,
the main way to obtain this data is still through automatic web crawling. In
this paper we take the existing multilingual web corpus OSCAR and its pipeline
Ungoliant that extracts and classifies data from Common Crawl at the line
level, and propose a set of improvements and automatic annotations in order to
produce a new document-oriented version of OSCAR that could prove more suitable
to pre-train large generative language models as well as hopefully other
applications in Natural Language Processing and Digital Humanities.",0,1,0,1,0,0,0.992334,6.0,0.995643,49
9203c9bb-eb49-4554-a47d-aae9a4726009,Salience Allocation as Guidance for Abstractive Summarization,16,0.0850859,0.703719,"Abstractive summarization models typically learn to capture the salient
information from scratch implicitly. Recent literature adds extractive
summaries as guidance for abstractive summarization models to provide hints of
salient content and achieves better performance. However, extractive summaries
as guidance could be over strict, leading to information loss or noisy signals.
Furthermore, it cannot easily adapt to documents with various abstractiveness.
As the number and allocation of salience content pieces vary, it is hard to
find a fixed threshold deciding which content should be included in the
guidance. In this paper, we propose a novel summarization approach with a
flexible and reliable salience guidance, namely SEASON (SaliencE Allocation as
Guidance for Abstractive SummarizatiON). SEASON utilizes the allocation of
salience expectation to guide abstractive summarization and adapts well to
articles in different abstractiveness. Automatic and human evaluations on two
benchmark datasets show that the proposed method is effective and reliable.
Empirical results on more than one million news articles demonstrate a natural
fifteen-fifty salience split for news article sentences, providing a useful
insight for composing news articles.",1,1,0,0,0,0,0.206755,7.0,0.561132,52
ff827454-c92e-461d-9334-bcb419141cc2,Defending Person Detection Against Adversarial Patch Attack by using Universal Defensive Frame,4,0.0284138,0.23342,"Person detection has attracted great attention in the computer vision area
and is an imperative element in human-centric computer vision. Although the
predictive performances of person detection networks have been improved
dramatically, they are vulnerable to adversarial patch attacks. Changing the
pixels in a restricted region can easily fool the person detection network in
safety-critical applications such as autonomous driving and security systems.
Despite the necessity of countering adversarial patch attacks, very few efforts
have been dedicated to defending person detection against adversarial patch
attack. In this paper, we propose a novel defense strategy that defends against
an adversarial patch attack by optimizing a defensive frame for person
detection. The defensive frame alleviates the effect of the adversarial patch
while maintaining person detection performance with clean person. The proposed
defensive frame in the person detection is generated with a competitive
learning algorithm which makes an iterative competition between detection
threatening module and detection shielding module in person detection.
Comprehensive experimental results demonstrate that the proposed method
effectively defends person detection against adversarial patch attacks.",1,1,0,0,0,0,0.311569,7.0,0.629328,68
ca4288ad-9bee-4a50-990f-885d501fdd86,Are High-Resolution Event Cameras Really Needed?,15,0.187954,0.51533,"Due to their outstanding properties in challenging conditions, event cameras
have become indispensable in a wide range of applications, ranging from
automotive, computational photography, and SLAM. However, as further
improvements are made to the sensor design, modern event cameras are trending
toward higher and higher sensor resolutions, which result in higher bandwidth
and computational requirements on downstream tasks. Despite this trend, the
benefits of using high-resolution event cameras to solve standard computer
vision tasks are still not clear. In this work, we report the surprising
discovery that, in low-illumination conditions and at high speeds,
low-resolution cameras can outperform high-resolution ones, while requiring a
significantly lower bandwidth. We provide both empirical and theoretical
evidence for this claim, which indicates that high-resolution event cameras
exhibit higher per-pixel event rates, leading to higher temporal noise in
low-illumination conditions and at high speeds. As a result, in most cases,
high-resolution event cameras show a lower task performance, compared to lower
resolution sensors in these conditions. We empirically validate our findings
across several tasks, namely image reconstruction, optical flow estimation, and
camera pose tracking, both on synthetic and real data. We believe that these
findings will provide important guidelines for future trends in event camera
development.",0,0,0,0,0,0,0.426966,6.0,0.634172,46
6b8de2f1-1a4e-48b9-bd28-76c0d93e8021,HM: Hybrid Masking for Few-Shot Segmentation,3,0.0496288,0.062629,"We study few-shot semantic segmentation that aims to segment a target object
from a query image when provided with a few annotated support images of the
target class. Several recent methods resort to a feature masking (FM) technique
to discard irrelevant feature activations which eventually facilitates the
reliable prediction of segmentation mask. A fundamental limitation of FM is the
inability to preserve the fine-grained spatial details that affect the accuracy
of segmentation mask, especially for small target objects. In this paper, we
develop a simple, effective, and efficient approach to enhance feature masking
(FM). We dub the enhanced FM as hybrid masking (HM). Specifically, we
compensate for the loss of fine-grained spatial details in FM technique by
investigating and leveraging a complementary basic input masking method.
Experiments have been conducted on three publicly available benchmarks with
strong few-shot segmentation (FSS) baselines. We empirically show improved
performance against the current state-of-the-art methods by visible margins
across different benchmarks. Our code and trained models are available at:
https://github.com/moonsh/HM-Hybrid-Masking",1,1,0,0,1,0,0.794735,8.0,0.856271,42
fb4d40a0-7875-47de-9d67-d1e0cb0925d6,Layout Aware Inpainting for Automated Furniture Removal in Indoor Scenes,5,0.045409,0.593599,"We address the problem of detecting and erasing furniture from a wide angle
photograph of a room. Inpainting large regions of an indoor scene often results
in geometric inconsistencies of background elements within the inpaint mask. To
address this problem, we utilize perceptual information (e.g. instance
segmentation, and room layout) to produce a geometrically consistent empty
version of a room. We share important details to make this system viable, such
as per-plane inpainting, automatic rectification, and texture refinement. We
provide detailed ablation along with qualitative examples, justifying our
design choices. We show an application of our system by removing real furniture
from a room and redecorating it with virtual furniture.",0,1,0,0,0,0,0.680595,8.0,0.815341,47
0a6c6b80-ce63-43be-b0ac-276e35ba5603,Agile Maneuvers in Legged Robots: a Predictive Control Approach,32,0.100565,0.826171,"Planning and execution of agile locomotion maneuvers have been a longstanding
challenge in legged robotics. It requires to derive motion plans and local
feedback policies in real-time to handle the nonholonomy of the kinetic
momenta. To achieve so, we propose a hybrid predictive controller that
considers the robot's actuation limits and full-body dynamics. It combines the
feedback policies with tactile information to locally predict future actions.
It converges within a few milliseconds thanks to a feasibility-driven approach.
Our predictive controller enables ANYmal robots to generate agile maneuvers in
realistic scenarios. A crucial element is to track the local feedback policies
as, in contrast to whole-body control, they achieve the desired angular
momentum. To the best of our knowledge, our predictive controller is the first
to handle actuation limits, generate agile locomotion maneuvers, and execute
optimal feedback policies for low level torque control without the use of a
separate whole-body controller.",0,0,0,0,0,0,0.0167667,10.0,0.431064,89
02fbc889-b510-4674-a13c-8e706ca25941,Incorporating Multi-armed Bandit with Local Search for MaxSAT,1,0.0117189,0.284802,"Partial MaxSAT (PMS) and Weighted PMS (WPMS) are two practical
generalizations of the MaxSAT problem. In this paper, we propose a local search
algorithm for these problems, called BandHS, which applies two multi-armed
bandits to guide the search directions when escaping local optima. One bandit
is combined with all the soft clauses to help the algorithm select to satisfy
appropriate soft clauses, and the other bandit with all the literals in hard
clauses to help the algorithm select appropriate literals to satisfy the hard
clauses. These two bandits can improve the algorithm's search ability in both
feasible and infeasible solution spaces. We further propose an initialization
method for (W)PMS that prioritizes both unit and binary clauses when producing
the initial solutions. Extensive experiments demonstrate the excellent
performance and generalization capability of our proposed methods, that greatly
boost the state-of-the-art local search algorithm, SATLike3.0, and the
state-of-the-art SAT-based incomplete solver, NuWLS-c.",1,1,0,0,1,0,0.0186133,11.0,0.492369,42
603efffb-2e0f-4961-99e6-3d75e1325362,PointSCNet: Point Cloud Structure and Correlation Learning Based on Space Filling Curve-Guided Sampling,6,0.112838,0.422746,"Geometrical structures and the internal local region relationship, such as
symmetry, regular array, junction, etc., are essential for understanding a 3D
shape. This paper proposes a point cloud feature extraction network named
PointSCNet, to capture the geometrical structure information and local region
correlation information of a point cloud. The PointSCNet consists of three main
modules: the space-filling curve-guided sampling module, the information fusion
module, and the channel-spatial attention module. The space-filling
curve-guided sampling module uses Z-order curve coding to sample points that
contain geometrical correlation. The information fusion module uses a
correlation tensor and a set of skip connections to fuse the structure and
correlation information. The channel-spatial attention module enhances the
representation of key points and crucial feature channels to refine the
network. The proposed PointSCNet is evaluated on shape classification and part
segmentation tasks. The experimental results demonstrate that the PointSCNet
outperforms or is on par with state-of-the-art methods by learning the
structure and correlation of point clouds effectively.",1,1,0,0,1,0,0.730279,8.0,0.832609,57
0b1cc74b-6861-498c-ab17-3d83e6c7471e,Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts,16,0.143994,0.951533,"Recent progress in language model pre-training has led to important
improvements in Named Entity Recognition (NER). Nonetheless, this progress has
been mainly tested in well-formatted documents such as news, Wikipedia, or
scientific articles. In social media the landscape is different, in which it
adds another layer of complexity due to its noisy and dynamic nature. In this
paper, we focus on NER in Twitter, one of the largest social media platforms,
and construct a new NER dataset, TweetNER7, which contains seven entity types
annotated over 11,382 tweets from September 2019 to August 2021. The dataset
was constructed by carefully distributing the tweets over time and taking
representative trends as a basis. Along with the dataset, we provide a set of
language model baselines and perform an analysis on the language model
performance on the task, especially analyzing the impact of different time
periods. In particular, we focus on three important temporal aspects in our
analysis: short-term degradation of NER models over time, strategies to
fine-tune a language model over different periods, and self-labeling as an
alternative to lack of recently-labeled data. TweetNER7 is released publicly
(https://huggingface.co/datasets/tner/tweetner7) along with the models
fine-tuned on it.",0,1,1,1,0,0,0.55272,7.0,0.739017,36
91b93f41-74b6-47a3-a42d-7a407cd3d157,Robustness to corruption in pre-trained Bayesian neural networks,2,0.0132633,0.196091,"We develop ShiftMatch, a new training-data-dependent likelihood for
robustness to corruption in Bayesian neural networks (BNNs). ShiftMatch is
inspired by the training-data-dependent ""EmpCov"" priors from Izmailov et al.
(2021a), and efficiently matches test-time spatial correlations to those at
training time. Critically, ShiftMatch is designed to leave the neural network's
training time likelihood unchanged, allowing it to use publicly available
samples from pre-trained BNNs. Using pre-trained HMC samples, ShiftMatch gives
strong performance improvements on CIFAR-10-C, outperforms EmpCov priors
(though ShiftMatch uses extra information from a minibatch of corrupted test
points), and is perhaps the first Bayesian method capable of convincingly
outperforming plain deep ensembles.",0,0,0,0,0,0,0.320901,7.0,0.634457,57
5d5bf0ad-142e-4fba-b34a-4cfcc6572a32,Emergent Quantized Communication,5,0.0679047,0.669995,"The field of emergent communication aims to understand the characteristics of
communication as it emerges from artificial agents solving tasks that require
information exchange. Communication with discrete messages is considered a
desired characteristic, for both scientific and applied reasons. However,
training a multi-agent system with discrete communication is not
straightforward, requiring either reinforcement learning algorithms or relaxing
the discreteness requirement via a continuous approximation such as the
Gumbel-softmax. Both these solutions result in poor performance compared to
fully continuous communication. In this work, we propose an alternative
approach to achieve discrete communication -- quantization of communicated
messages. Using message quantization allows us to train the model end-to-end,
achieving superior performance in multiple setups. Moreover, quantization is a
natural framework that runs the gamut from continuous to discrete
communication. Thus, it sets the ground for a broader view of multi-agent
communication in the deep learning era.",0,0,0,0,0,0,0.468948,7.0,0.70473,40
48f86cfd-0178-48ff-b9c7-a4edde57d61f,Knowledge Sharing via Domain Adaptation in Customs Fraud Detection,6,0.0838995,0.634039,"Knowledge of the changing traffic is critical in risk management. Customs
offices worldwide have traditionally relied on local resources to accumulate
knowledge and detect tax fraud. This naturally poses countries with weak
infrastructure to become tax havens of potentially illicit trades. The current
paper proposes DAS, a memory bank platform to facilitate knowledge sharing
across multi-national customs administrations to support each other. We propose
a domain adaptation method to share transferable knowledge of frauds as
prototypes while safeguarding the local trade information. Data encompassing
over 8 million import declarations have been used to test the feasibility of
this new system, which shows that participating countries may benefit up to
2-11 times in fraud detection with the help of shared knowledge. We discuss
implications for substantial tax revenue potential and strengthened policy
against illicit trades.",0,1,1,0,0,0,0.291751,7.0,0.618034,39
3db32823-5ed1-42f5-bd3d-3a0f080e0ca7,GloCAL: Glocalized Curriculum-Aided Learning of Multiple Tasks with Application to Robotic Grasping,1,0.00880284,0.0804053,"The domain of robotics is challenging to apply deep reinforcement learning
due to the need for large amounts of data and for ensuring safety during
learning. Curriculum learning has shown good performance in terms of sample-
efficient deep learning. In this paper, we propose an algorithm (named GloCAL)
that creates a curriculum for an agent to learn multiple discrete tasks, based
on clustering tasks according to their evaluation scores. From the
highest-performing cluster, a global task representative of the cluster is
identified for learning a global policy that transfers to subsequently formed
new clusters, while the remaining tasks in the cluster are learned as local
policies. The efficacy and efficiency of our GloCAL algorithm are compared with
other approaches in the domain of grasp learning for 49 objects with varied
object complexity and grasp difficulty from the EGAD! dataset. The results show
that GloCAL is able to learn to grasp 100% of the objects, whereas other
approaches achieve at most 86% despite being given 1.5 times longer training
time.",1,1,0,0,1,0,0.494244,7.0,0.715342,40
f1afa2a0-da6f-4560-8183-9b87dc73b14f,MorDeephy: Face Morphing Detection Via Fused Classification,8,0.0686657,0.435882,"Face morphing attack detection (MAD) is one of the most challenging tasks in
the field of face recognition nowadays. In this work, we introduce a novel deep
learning strategy for a single image face morphing detection, which implies the
discrimination of morphed face images along with a sophisticated face
recognition task in a complex classification scheme. It is directed onto
learning the deep facial features, which carry information about the
authenticity of these features. Our work also introduces several additional
contributions: the public and easy-to-use face morphing detection benchmark and
the results of our wild datasets filtering strategy. Our method, which we call
MorDeephy, achieved the state of the art performance and demonstrated a
prominent ability for generalising the task of morphing detection to unseen
scenarios.",0,1,0,1,1,0,0.132433,9.0,0.604342,75
aa78e095-ffee-44db-8612-9fdfdd282301,Disaster Tweets Classification using BERT-Based Language Model,1,0.0117994,0.143409,"Social networking services have became an important communication channel in
time of emergency. The aim of this study is to create a machine learning
language model that is able to investigate if a person or area was in danger or
not. The ubiquitousness of smartphones enables people to announce an emergency
they are observing in real-time. Because of this, more agencies are interested
in programmatically monitoring Twitter (i.e. disaster relief organizations and
news agencies). Design a language model that is able to understand and
acknowledge when a disaster is happening based on the social network posts will
become more and more necessary over time.",0,1,0,0,0,0,0.934537,10.0,0.93935,12
13e51601-9728-4202-9f09-9c0a6ae99399,Meta-Learning a Cross-lingual Manifold for Semantic Parsing,10,0.160017,0.710179,"Localizing a semantic parser to support new languages requires effective
cross-lingual generalization. Recent work has found success with
machine-translation or zero-shot methods although these approaches can struggle
to model how native speakers ask questions. We consider how to effectively
leverage minimal annotated examples in new languages for few-shot cross-lingual
semantic parsing. We introduce a first-order meta-learning algorithm to train a
semantic parser with maximal sample efficiency during cross-lingual transfer.
Our algorithm uses high-resource languages to train the parser and
simultaneously optimizes for cross-lingual generalization for lower-resource
languages. Results across six languages on ATIS demonstrate that our
combination of generalization steps yields accurate semantic parsers sampling
$\le$10% of source training data in each new language. Our approach also trains
a competitive model on Spider using English with generalization to Chinese
similarly sampling $\le$10% of training data.",1,1,0,0,0,0,0.463941,7.0,0.702596,75
82fac479-a8f1-45c7-8b64-c353dc680f46,MorphTE: Injecting Morphology in Tensorized Embeddings,3,0.0312579,0.2987,"In the era of deep learning, word embeddings are essential when dealing with
text tasks. However, storing and accessing these embeddings requires a large
amount of space. This is not conducive to the deployment of these models on
resource-limited devices. Combining the powerful compression capability of
tensor products, we propose a word embedding compression method with
morphological augmentation, Morphologically-enhanced Tensorized Embeddings
(MorphTE). A word consists of one or more morphemes, the smallest units that
bear meaning or have a grammatical function. MorphTE represents a word
embedding as an entangled form of its morpheme vectors via the tensor product,
which injects prior semantic and grammatical knowledge into the learning of
embeddings. Furthermore, the dimensionality of the morpheme vector and the
number of morphemes are much smaller than those of words, which greatly reduces
the parameters of the word embeddings. We conduct experiments on tasks such as
machine translation and question answering. Experimental results on four
translation datasets of different languages show that MorphTE can compress word
embedding parameters by about 20 times without performance loss and
significantly outperforms related embedding compression methods.",0,1,0,0,1,0,0.101434,11.0,0.650474,40
8d076cd8-6687-4c43-a536-6922a2f62150,A general-purpose method for applying Explainable AI for Anomaly Detection,7,0.0258098,0.320443,"The need for explainable AI (XAI) is well established but relatively little
has been published outside of the supervised learning paradigm. This paper
focuses on a principled approach to applying explainability and
interpretability to the task of unsupervised anomaly detection. We argue that
explainability is principally an algorithmic task and interpretability is
principally a cognitive task, and draw on insights from the cognitive sciences
to propose a general-purpose method for practical diagnosis using explained
anomalies. We define Attribution Error, and demonstrate, using real-world
labeled datasets, that our method based on Integrated Gradients (IG) yields
significantly lower attribution errors than alternative methods.",0,0,0,0,0,0,0.297033,11.0,0.758882,38
4b74a587-79d0-4d34-8bfa-fa7069d06687,Some Reflections on Drawing Causal Inference using Textual Data: Parallels Between Human Subjects and Organized Texts,4,0.0177896,0.151625,"We examine the role of textual data as study units when conducting causal
inference by drawing parallels between human subjects and organized texts. %in
human population research. We elaborate on key causal concepts and principles,
and expose some ambiguity and sometimes fallacies. To facilitate better framing
a causal query, we discuss two strategies: (i) shifting from immutable traits
to perceptions of them, and (ii) shifting from some abstract concept/property
to its constituent parts, i.e., adopting a constructivist perspective of an
abstract concept. We hope this article would raise the awareness of the
importance of articulating and clarifying fundamental concepts before delving
into developing methodologies when drawing causal inference using textual data.",0,0,0,0,0,0,0.112622,10.0,0.626599,72
90263f55-a045-44a1-b82b-5a89c84b67e1,Learning Deep Time-index Models for Time Series Forecasting,9,0.382244,0.308243,"Deep learning has been actively applied to time series forecasting, leading
to a deluge of new methods, belonging to the class of historical-value models.
Yet, despite the attractive properties of time-index models, such as being able
to model the continuous nature of underlying time series dynamics, little
attention has been given to them. Indeed, while naive deep time-index models
are far more expressive than the manually predefined function representations
of classical time-index models, they are inadequate for forecasting, being
unable to generalize to unseen time steps due to the lack of inductive bias. In
this paper, we propose DeepTime, a meta-optimization framework to learn deep
time-index models which overcome these limitations, yielding an efficient and
accurate forecasting model. Extensive experiments on real world datasets in the
long sequence time-series forecasting setting demonstrate that our approach
achieves competitive results with state-of-the-art methods, and is highly
efficient. Code is available at https://github.com/salesforce/DeepTime.",1,1,0,0,1,0,0.945127,9.0,0.93958,65
a781c793-471c-48bf-a3ed-737f77890383,TokenFlow: Rethinking Fine-grained Cross-modal Alignment in Vision-Language Retrieval,2,0.0986908,0.0964232,"Most existing methods in vision-language retrieval match two modalities by
either comparing their global feature vectors which misses sufficient
information and lacks interpretability, detecting objects in images or videos
and aligning the text with fine-grained features which relies on complicated
model designs, or modeling fine-grained interaction via cross-attention upon
visual and textual tokens which suffers from inferior efficiency. To address
these limitations, some recent works simply aggregate the token-wise
similarities to achieve fine-grained alignment, but they lack intuitive
explanations as well as neglect the relationships between token-level features
and global representations with high-level semantics. In this work, we rethink
fine-grained cross-modal alignment and devise a new model-agnostic formulation
for it. We additionally demystify the recent popular works and subsume them
into our scheme. Furthermore, inspired by optimal transport theory, we
introduce TokenFlow, an instantiation of the proposed scheme. By modifying only
the similarity function, the performance of our method is comparable to the
SoTA algorithms with heavy model designs on major video-text retrieval
benchmarks. The visualization further indicates that TokenFlow successfully
leverages the fine-grained information and achieves better interpretability.",0,0,0,0,1,0,0.991412,5.0,0.990054,37
0b0901e6-9be5-4231-bf90-28b5afea46b1,Goal Recognition as a Deep Learning Task: the GRNet Approach,5,0.020821,0.315034,"In automated planning, recognising the goal of an agent from a trace of
observations is an important task with many applications. The state-of-the-art
approaches to goal recognition rely on the application of planning techniques,
which requires a model of the domain actions and of the initial domain state
(written, e.g., in PDDL). We study an alternative approach where goal
recognition is formulated as a classification task addressed by machine
learning. Our approach, called GRNet, is primarily aimed at making goal
recognition more accurate as well as faster by learning how to solve it in a
given domain. Given a planning domain specified by a set of propositions and a
set of action names, the goal classification instances in the domain are solved
by a Recurrent Neural Network (RNN). A run of the RNN processes a trace of
observed actions to compute how likely it is that each domain proposition is
part of the agent's goal, for the problem instance under considerations. These
predictions are then aggregated to choose one of the candidate goals. The only
information required as input of the trained RNN is a trace of action labels,
each one indicating just the name of an observed action. An experimental
analysis confirms that \our achieves good performance in terms of both goal
classification accuracy and runtime, obtaining better performance w.r.t. a
state-of-the-art goal recognition system over the considered benchmarks.",0,1,0,0,1,0,0.00692159,13.0,0.493917,48
2afec887-ee35-4512-b96d-5da1a87ff98b,Adapting to Latent Subgroup Shifts via Concepts and Proxies,5,0.0298213,0.454264,"We address the problem of unsupervised domain adaptation when the source
domain differs from the target domain because of a shift in the distribution of
a latent subgroup. When this subgroup confounds all observed data, neither
covariate shift nor label shift assumptions apply. We show that the optimal
target predictor can be non-parametrically identified with the help of concept
and proxy variables available only in the source domain, and unlabeled data
from the target. The identification results are constructive, immediately
suggesting an algorithm for estimating the optimal predictor in the target. For
continuous observations, when this algorithm becomes impractical, we propose a
latent variable model specific to the data generation process at hand. We show
how the approach degrades as the size of the shift changes, and verify that it
outperforms both covariate and label shift adjustment.",0,0,1,0,0,0,0.159064,13.0,0.741345,72
a300483d-1197-4af8-bae7-47089ef80aae,ASL Video Corpora & Sign Bank: Resources Available through the American Sign Language Linguistic Research Project (ASLLRP),6,0.130147,0.313826,"The American Sign Language Linguistic Research Project (ASLLRP) provides
Internet access to high-quality ASL video data, generally including front and
side views and a close-up of the face. The manual and non-manual components of
the signing have been linguistically annotated using SignStream(R). The
recently expanded video corpora can be browsed and searched through the Data
Access Interface (DAI 2) we have designed; it is possible to carry out complex
searches. The data from our corpora can also be downloaded; annotations are
available in an XML export format. We have also developed the ASLLRP Sign Bank,
which contains almost 6,000 sign entries for lexical signs, with distinct
English-based glosses, with a total of 41,830 examples of lexical signs (in
addition to about 300 gestures, over 1,000 fingerspelled signs, and 475
classifier examples). The Sign Bank is likewise accessible and searchable on
the Internet; it can also be accessed from within SignStream(R) (software to
facilitate linguistic annotation and analysis of visual language data) to make
annotations more accurate and efficient. Here we describe the available
resources. These data have been used for many types of research in linguistics
and in computer-based sign language recognition from video; examples of such
research are provided in the latter part of this article.",0,1,0,1,0,0,0.0253643,8.0,0.341121,79
0823e884-293b-440e-8b5f-de60dec11608,Exploration of Machine Learning Classification Models Used for Behavioral Biometrics Authentication,5,0.180104,0.707976,"Mobile devices have been manufactured and enhanced at growing rates in the
past decades. While this growth has significantly evolved the capability of
these devices, their security has been falling behind. This contrast in
development between capability and security of mobile devices is a significant
problem with the sensitive information of the public at risk. Continuing the
previous work in this field, this study identifies key Machine Learning
algorithms currently being used for behavioral biometric mobile authentication
schemes and aims to provide a comprehensive review of these algorithms when
used with touch dynamics and phone movement. Throughout this paper the
benefits, limitations, and recommendations for future work will be discussed.",0,1,0,0,0,0,0.681039,3.0,0.507981,25
f2e7c547-e386-4675-9afd-446cda5a10f1,DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models,22,0.118075,0.569239,"We introduce DoWhy-GCM, an extension of the DoWhy Python library, that
leverages graphical causal models. Unlike existing causality libraries, which
mainly focus on effect estimation questions, with DoWhy-GCM, users can ask a
wide range of additional causal questions, such as identifying the root causes
of outliers and distributional changes, causal structure learning, attributing
causal influences, and diagnosis of causal structures. To this end, DoWhy-GCM
users first model cause-effect relations between variables in a system under
study through a graphical causal model, fit the causal mechanisms of variables
next, and then ask the causal question. All these steps take only a few lines
of code in DoWhy-GCM.
  The library is available at https://github.com/py-why/dowhy.",0,1,0,0,0,0,0.420654,6.0,0.63086,22
0d712bb9-9c92-4efa-9637-94582e192837,A Deep Learning Approach for Automatic Detection of Qualitative Features of Lecturing,2,0.00462911,0.0568428,"Artificial Intelligence in higher education opens new possibilities for
improving the lecturing process, such as enriching didactic materials, helping
in assessing students' works or even providing directions to the teachers on
how to enhance the lectures. We follow this research path, and in this work, we
explore how an academic lecture can be assessed automatically by quantitative
features. First, we prepare a set of qualitative features based on teaching
practices and then annotate the dataset of academic lecture videos collected
for this purpose. We then show how these features could be detected
automatically using machine learning and computer vision techniques. Our
results show the potential usefulness of our work.",0,1,0,0,0,0,0.0737972,12.0,0.651851,22
64319bc7-3248-4f5d-bfef-4ff100de4bc4,A sequence-to-sequence approach for document-level relation extraction,32,0.498666,0.99205,"Motivated by the fact that many relations cross the sentence boundary, there
has been increasing interest in document-level relation extraction (DocRE).
DocRE requires integrating information within and across sentences, capturing
complex interactions between mentions of entities. Most existing methods are
pipeline-based, requiring entities as input. However, jointly learning to
extract entities and relations can improve performance and be more efficient
due to shared parameters and training steps. In this paper, we develop a
sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE
(entity extraction, coreference resolution and relation extraction) end-to-end,
replacing a pipeline of task-specific components. Using a simple strategy we
call entity hinting, we compare our approach to existing pipeline-based methods
on several popular biomedical datasets, in some cases exceeding their
performance. We also report the first end-to-end results on these datasets for
future comparison. Finally, we demonstrate that, under our model, an end-to-end
approach outperforms a pipeline-based approach. Our code, data and trained
models are available at {\url{https://github.com/johngiorgi/seq2rel}}. An
online demo is available at
{\url{https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py}}.",1,1,0,0,0,0,0.776215,8.0,0.849259,82
8ed01dc3-4b8c-466e-952c-fc6c080b17ea,MSANet: Multi-Similarity and Attention Guidance for Boosting Few-Shot Segmentation,21,0.183801,0.386243,"Few-shot segmentation aims to segment unseen-class objects given only a
handful of densely labeled samples. Prototype learning, where the support
feature yields a singleor several prototypes by averaging global and local
object information, has been widely used in FSS. However, utilizing only
prototype vectors may be insufficient to represent the features for all
training data. To extract abundant features and make more precise predictions,
we propose a Multi-Similarity and Attention Network (MSANet) including two
novel modules, a multi-similarity module and an attention module. The
multi-similarity module exploits multiple feature-maps of support images and
query images to estimate accurate semantic relationships. The attention module
instructs the network to concentrate on class-relevant information. The network
is tested on standard FSS datasets, PASCAL-5i 1-shot, PASCAL-5i 5-shot,
COCO-20i 1-shot, and COCO-20i 5-shot. The MSANet with the backbone of
ResNet-101 achieves the state-of-the-art performance for all 4-benchmark
datasets with mean intersection over union (mIoU) of 69.13%, 73.99%, 51.09%,
56.80%, respectively. Code is available at
https://github.com/AIVResearch/MSANet",1,1,0,0,1,0,0.763979,6.0,0.792978,67
9513c5a0-0076-4372-ad79-7ac38967b309,Democratizing Neural Machine Translation with OPUS-MT,1,0.00495389,0.11469,"This paper presents the OPUS ecosystem with a focus on the development of
open machine translation models and tools, and their integration into end-user
applications, development platforms and professional workflows. We discuss our
on-going mission of increasing language coverage and translation quality, and
also describe on-going work on the development of modular translation models
and speed-optimized compact solutions for real-time translation on regular
desktops and small devices.",1,1,0,0,0,1,0.0461448,9.0,0.482013,58
4e76cc0a-d55d-445f-81bb-2681bd0f9983,Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure,15,0.306557,0.86535,"Generative models have demonstrated impressive results on Aspect-based
Sentiment Analysis (ABSA) tasks, particularly for the emerging task of
extracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these
models struggle with implicit sentiment expressions, which are commonly
observed in opinionated content such as online reviews. In this work, we
introduce GEN-SCL-NAT, which consists of two techniques for improved structured
generation for ACOS quadruple extraction. First, we propose GEN-SCL, a
supervised contrastive learning objective that aids quadruple prediction by
encouraging the model to produce input representations that are discriminable
across key input attributes, such as sentiment polarity and the existence of
implicit opinions and aspects. Second, we introduce GEN-NAT, a new structured
generation format that better adapts autoregressive encoder-decoder models to
extract quadruples in a generative fashion. Experimental results show that
GEN-SCL-NAT achieves top performance across three ACOS datasets, averaging
1.48% F1 improvement, with a maximum 1.73% increase on the LAPTOP-L1 dataset.
Additionally, we see significant gains on implicit aspect and opinion splits
that have been shown as challenging for existing ACOS approaches.",1,1,0,0,1,0,0.846249,5.0,0.803559,13
3d5ef2ed-39e9-4d5e-ab19-e0691d96aff3,A new Hyper-heuristic based on Adaptive Simulated Annealing and Reinforcement Learning for the Capacitated Electric Vehicle Routing Problem,7,0.122281,0.724777,"Electric vehicles (EVs) have been adopted in urban areas to reduce
environmental pollution and global warming as a result of the increasing number
of freight vehicles. However, there are still deficiencies in routing the
trajectories of last-mile logistics that continue to impact social and economic
sustainability. For that reason, in this paper, a hyper-heuristic (HH) approach
called Hyper-heuristic Adaptive Simulated Annealing with Reinforcement Learning
(HHASA$_{RL}$) is proposed. It is composed of a multi-armed bandit method and
the self-adaptive Simulated Annealing (SA) metaheuristic algorithm for solving
the problem called Capacitated Electric Vehicle Routing Problem (CEVRP). Due to
the limited number of charging stations and the travel range of EVs, the EVs
must require battery recharging moments in advance and reduce travel times and
costs. The HH implemented improves multiple minimum best-known solutions and
obtains the best mean values for some high-dimensional instances for the
proposed benchmark for the IEEE WCCI2020 competition.",0,1,0,0,0,0,0.548152,5.0,0.632081,66
bac507c6-9f5b-432b-aeb8-abe15eb2f56a,PARSRec: Explainable Personalized Attention-fused Recurrent Sequential Recommendation Using Session Partial Actions,5,0.0720377,0.254194,"The emerging meta- and multi-verse landscape is yet another step towards the
more prevalent use of already ubiquitous online markets. In such markets,
recommender systems play critical roles by offering items of interest to the
users, thereby narrowing down a vast search space that comprises hundreds of
thousands of products. Recommender systems are usually designed to learn common
user behaviors and rely on them for inference. This approach, while effective,
is oblivious to subtle idiosyncrasies that differentiate humans from each
other. Focusing on this observation, we propose an architecture that relies on
common patterns as well as individual behaviors to tailor its recommendations
for each person. Simulations under a controlled environment show that our
proposed model learns interpretable personalized user behaviors. Our empirical
results on Nielsen Consumer Panel dataset indicate that the proposed approach
achieves up to 27.9% performance improvement compared to the state-of-the-art.",1,1,0,0,1,0,0.785719,10.0,0.882265,44
1319c8ed-e922-402d-9906-0f8fc2b63857,A Self-Guided Framework for Radiology Report Generation,10,0.402216,0.715289,"Automatic radiology report generation is essential to computer-aided
diagnosis. Through the success of image captioning, medical report generation
has been achievable. However, the lack of annotated disease labels is still the
bottleneck of this area. In addition, the image-text data bias problem and
complex sentences make it more difficult to generate accurate reports. To
address these gaps, we pre-sent a self-guided framework (SGF), a suite of
unsupervised and supervised deep learning methods to mimic the process of human
learning and writing. In detail, our framework obtains the domain knowledge
from medical reports with-out extra disease labels and guides itself to extract
fined-grain visual features as-sociated with the text. Moreover, SGF
successfully improves the accuracy and length of medical report generation by
incorporating a similarity comparison mechanism that imitates the process of
human self-improvement through compar-ative practice. Extensive experiments
demonstrate the utility of our SGF in the majority of cases, showing its
superior performance over state-of-the-art meth-ods. Our results highlight the
capacity of the proposed framework to distinguish fined-grained visual details
between words and verify its advantage in generating medical reports.",0,1,0,0,1,0,0.955816,11.0,0.957109,31
67736c89-c7b4-4dca-8cbb-89a18a548c26,HINT: Hierarchical Neuron Concept Explainer,11,0.138065,0.885059,"To interpret deep networks, one main approach is to associate neurons with
human-understandable concepts. However, existing methods often ignore the
inherent relationships of different concepts (e.g., dog and cat both belong to
animals), and thus lose the chance to explain neurons responsible for
higher-level concepts (e.g., animal). In this paper, we study hierarchical
concepts inspired by the hierarchical cognition process of human beings. To
this end, we propose HIerarchical Neuron concepT explainer (HINT) to
effectively build bidirectional associations between neurons and hierarchical
concepts in a low-cost and scalable manner. HINT enables us to systematically
and quantitatively study whether and how the implicit hierarchical
relationships of concepts are embedded into neurons, such as identifying
collaborative neurons responsible to one concept and multimodal neurons for
different concepts, at different semantic levels from concrete concepts (e.g.,
dog) to more abstract ones (e.g., animal). Finally, we verify the faithfulness
of the associations using Weakly Supervised Object Localization, and
demonstrate its applicability in various tasks such as discovering saliency
regions and explaining adversarial attacks. Code is available on
https://github.com/AntonotnaWang/HINT.",1,0,0,0,0,0,0.87555,9.0,0.902753,87
959932ef-a077-4188-aa89-e4eaf7aa0e61,"""Does it come in black?"" CLIP-like models are zero-shot recommenders",6,0.0297578,0.111777,"Product discovery is a crucial component for online shopping. However,
item-to-item recommendations today do not allow users to explore changes along
selected dimensions: given a query item, can a model suggest something similar
but in a different color? We consider item recommendations of the comparative
nature (e.g. ""something darker"") and show how CLIP-based models can support
this use case in a zero-shot manner. Leveraging a large model built for
fashion, we introduce GradREC and its industry potential, and offer a first
rounded assessment of its strength and weaknesses.",1,1,0,0,0,0,0.349377,4.0,0.386546,38
39cf20bc-bed7-4504-b38e-ac83ba63987a,Knowledge Base Question Answering: A Semantic Parsing Perspective,21,0.109728,0.56986,"Recent advances in deep learning have greatly propelled the research on
semantic parsing. Improvement has since been made in many downstream tasks,
including natural language interface to web APIs, text-to-SQL generation, among
others. However, despite the close connection shared with these tasks, research
on question answering over knowledge bases (KBQA) has comparatively been
progressing slowly. We identify and attribute this to two unique challenges of
KBQA, schema-level complexity and fact-level complexity. In this survey, we
situate KBQA in the broader literature of semantic parsing and give a
comprehensive account of how existing KBQA approaches attempt to address the
unique challenges. Regardless of the unique challenges, we argue that we can
still take much inspiration from the literature of semantic parsing, which has
been overlooked by existing research on KBQA. Based on our discussion, we can
better understand the bottleneck of current KBQA research and shed light on
promising directions for KBQA to keep up with the literature of semantic
parsing, particularly in the era of pre-trained language models.",0,0,0,0,0,0,0.245725,6.0,0.520785,84
41743d50-809c-49d1-ae4b-e3084a763964,Revisiting initial sets in abstract argumentation,5,0.0517073,0.248192,"We revisit the notion of initial sets by Xu and Cayrol, i.e., non-empty
minimal admissible sets in abstract argumentation frameworks. Initial sets are
a simple concept for analysing conflicts in an abstract argumentation framework
and to explain why certain arguments can be accepted. We contribute with new
insights on the structure of initial sets and devise a simple non-deterministic
construction principle for any admissible set, based on iterative selection of
initial sets of the original framework and its induced reducts. In particular,
we characterise many existing admissibility-based semantics via this
construction principle, thus providing a constructive explanation on the
structure of extensions. We also investigate certain problems related to
initial sets with respect to their computational complexity.",0,0,0,0,0,0,0.00158356,15.0,0.462885,42
9f23a75f-6f5c-44f1-ba13-38b6ae4ed5ba,Needs-aware Artificial Intelligence: AI that 'serves [human] needs',2,0.0871288,0.131956,"By defining the current limits (and thereby the frontiers), many boundaries
are shaping, and will continue to shape, the future of Artificial Intelligence
(AI). We push on these boundaries in order to make further progress into what
were yesterday's frontiers. They are both pliable and resilient - always
creating new boundaries of what AI can (or should) achieve. Among these are
technical boundaries (such as processing capacity), psychological boundaries
(such as human trust in AI systems), ethical boundaries (such as with AI
weapons), and conceptual boundaries (such as the AI people can imagine). It is
within this final category while it can play a fundamental role in all other
boundaries} that we find the construct of needs and the limitations that our
current concept of need places on the future AI.",0,0,0,0,0,1,0.185935,6.0,0.468216,14
8754d3b8-80d7-4ab0-8bf1-d4594480ef3a,A Learning-Based Trajectory Planning of Multiple UAVs for AoI Minimization in IoT Networks,5,0.315775,0.476605,"Many emerging Internet of Things (IoT) applications rely on information
collected by sensor nodes where the freshness of information is an important
criterion. \textit{Age of Information} (AoI) is a metric that quantifies
information timeliness, i.e., the freshness of the received information or
status update. This work considers a setup of deployed sensors in an IoT
network, where multiple unmanned aerial vehicles (UAVs) serve as mobile relay
nodes between the sensors and the base station. We formulate an optimization
problem to jointly plan the UAVs' trajectory, while minimizing the AoI of the
received messages. This ensures that the received information at the base
station is as fresh as possible. The complex optimization problem is
efficiently solved using a deep reinforcement learning (DRL) algorithm. In
particular, we propose a deep Q-network, which works as a function
approximation to estimate the state-action value function. The proposed scheme
is quick to converge and results in a lower AoI than the random walk scheme.
Our proposed algorithm reduces the average age by approximately $25\%$ and
requires down to $50\%$ less energy when compared to the baseline scheme.",0,1,0,0,0,0,0.990803,8.0,0.991969,14
63c94076-9488-45e7-88dc-b1d22ec31ef0,Spatio-Temporal Dynamic Graph Relation Learning for Urban Metro Flow Prediction,11,0.40307,0.847838,"Urban metro flow prediction is of great value for metro operation scheduling,
passenger flow management and personal travel planning. However, it faces two
main challenges. First, different metro stations, e.g. transfer stations and
non-transfer stations, have unique traffic patterns. Second, it is challenging
to model complex spatio-temporal dynamic relation of metro stations. To address
these challenges, we develop a spatio-temporal dynamic graph relational
learning model (STDGRL) to predict urban metro station flow. First, we propose
a spatio-temporal node embedding representation module to capture the traffic
patterns of different stations. Second, we employ a dynamic graph relationship
learning module to learn dynamic spatial relationships between metro stations
without a predefined graph adjacency matrix. Finally, we provide a
transformer-based long-term relationship prediction module for long-term metro
flow prediction. Extensive experiments are conducted based on metro data in
Beijing, Shanghai, Chongqing and Hangzhou. Experimental results show the
advantages of our method beyond 11 baselines for urban metro flow prediction.",0,1,0,0,1,0,0.966691,6.0,0.935818,61
9424d949-575c-4577-9e5d-c187067131ad,A World-Self Model Towards Understanding Intelligence,1,0.0,0.0704122,"The symbolism, connectionism and behaviorism approaches of artificial
intelligence have achieved a lot of successes in various tasks, while we still
do not have a clear definition of ""intelligence"" with enough consensus in the
community (although there are over 70 different ""versions"" of definitions). The
nature of intelligence is still in darkness. In this work we do not take any of
these three traditional approaches, instead we try to identify certain
fundamental aspects of the nature of intelligence, and construct a mathematical
model to represent and potentially reproduce these fundamental aspects. We
first stress the importance of defining the scope of discussion and granularity
of investigation. We carefully compare human and artificial intelligence, and
qualitatively demonstrate an information abstraction process, which we propose
to be the key to connect perception and cognition. We then present the broader
idea of ""concept"", separate the idea of self model out of the world model, and
construct a new model called world-self model (WSM). We show the mechanisms of
creating and connecting concepts, and the flow of how the WSM receives,
processes and outputs information with respect to an arbitrary type of problem
to solve. We also consider and discuss the potential computer implementation
issues of the proposed theoretical framework, and finally we propose a unified
general framework of intelligence based on WSM.",0,0,0,0,0,1,0.121725,10.0,0.634877,63
83798049-7a4d-4457-b1ba-31c1bc621163,Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations,11,0.00625524,0.342612,"Recent work on explainable NLP has shown that few-shot prompting can enable
large pretrained language models (LLMs) to generate grammatical and factual
natural language explanations for data labels. In this work, we study the
connection between explainability and sample hardness by investigating the
following research question - ""Are LLMs and humans equally good at explaining
data labels for both easy and hard samples?"" We answer this question by first
collecting human-written explanations in the form of generalizable commonsense
rules on the task of Winograd Schema Challenge (Winogrande dataset). We compare
these explanations with those generated by GPT-3 while varying the hardness of
the test samples as well as the in-context samples. We observe that (1) GPT-3
explanations are as grammatical as human explanations regardless of the
hardness of the test samples, (2) for easy examples, GPT-3 generates highly
supportive explanations but human explanations are more generalizable, and (3)
for hard examples, human explanations are significantly better than GPT-3
explanations both in terms of label-supportiveness and generalizability
judgements. We also find that hardness of the in-context examples impacts the
quality of GPT-3 explanations. Finally, we show that the supportiveness and
generalizability aspects of human explanations are also impacted by sample
hardness, although by a much smaller margin than models. Supporting code and
data are available at https://github.com/swarnaHub/ExplanationHardness",0,0,0,0,0,0,0.302469,5.0,0.473898,45
1aab3484-017c-4e20-8044-525d9d4c373f,Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers,10,0.151236,0.588761,"Existing Math Word Problem (MWP) solvers have achieved high accuracy on
benchmark datasets. However, prior works have shown that such solvers do not
generalize well and rely on superficial cues to achieve high performance. In
this paper, we first conduct experiments to showcase that this behaviour is
mainly associated with the limited size and diversity present in existing MWP
datasets. Next, we propose several data augmentation techniques broadly
categorized into Substitution and Paraphrasing based methods. By deploying
these methods we increase the size of existing datasets by five folds.
Extensive experiments on two benchmark datasets across three state-of-the-art
MWP solvers show that proposed methods increase the generalization and
robustness of existing solvers. On average, proposed methods significantly
increase the state-of-the-art results by over five percentage points on
benchmark datasets. Further, the solvers trained on the augmented dataset
perform comparatively better on the challenge test set. We also show the
effectiveness of proposed techniques through ablation studies and verify the
quality of augmented samples through human evaluation.",1,1,0,1,1,0,0.896942,7.0,0.887347,37
7d628e75-f61f-4ca1-a810-9e9f4ecc547e,Legal Case Document Summarization: Extractive and Abstractive Methods and their Evaluation,19,0.500889,0.99341,"Summarization of legal case judgement documents is a challenging problem in
Legal NLP. However, not much analyses exist on how different families of
summarization models (e.g., extractive vs. abstractive) perform when applied to
legal case documents. This question is particularly important since many recent
transformer-based abstractive summarization models have restrictions on the
number of input tokens, and legal documents are known to be very long. Also, it
is an open question on how best to evaluate legal case document summarization
systems. In this paper, we carry out extensive experiments with several
extractive and abstractive summarization methods (both supervised and
unsupervised) over three legal summarization datasets that we have developed.
Our analyses, that includes evaluation by law practitioners, lead to several
interesting insights on legal summarization in specific and long document
summarization in general.",1,1,0,1,0,0,0.793976,7.0,0.835405,39
213e01ae-455f-42a7-b9cc-c71120a82383,Classification of Distraction Levels Using Hybrid Deep Neural Networks From EEG Signals,1,0.00878551,0.20264,"Non-invasive brain-computer interface technology has been developed for
detecting human mental states with high performances. Detection of the pilots'
mental states is particularly critical because their abnormal mental states
could cause catastrophic accidents. In this study, we presented the feasibility
of classifying distraction levels (namely, normal state, low distraction, and
high distraction) by applying the deep learning method. To the best of our
knowledge, this study is the first attempt to classify distraction levels under
a flight environment. We proposed a model for classifying distraction levels. A
total of ten pilots conducted the experiment in a simulated flight environment.
The grand-average accuracy was 0.8437 for classifying distraction levels across
all subjects. Hence, we believe that it will contribute significantly to
autonomous driving or flight based on artificial intelligence technology in the
future.",0,1,1,0,0,0,0.0815299,8.0,0.49075,32
60139d80-a3ef-4879-8205-03fde1c20401,CZU-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and 10 wearable inertial sensors,10,0.0819015,0.400692,"Human action recognition has been widely used in many fields of life, and
many human action datasets have been published at the same time. However, most
of the multi-modal databases have some shortcomings in the layout and number of
sensors, which cannot fully represent the action features. Regarding the
problems, this paper proposes a freely available dataset, named CZU-MHAD
(Changzhou University: a comprehensive multi-modal human action dataset). It
consists of 22 actions and three modals temporal synchronized data. These
modals include depth videos and skeleton positions from a kinect v2 camera, and
inertial signals from 10 wearable sensors. Compared with single modal sensors,
multi-modal sensors can collect different modal data, so the use of multi-modal
sensors can describe actions more accurately. Moreover, CZU-MHAD obtains the
3-axis acceleration and 3-axis angular velocity of 10 main motion joints by
binding inertial sensors to them, and these data were captured at the same
time. Experimental results are provided to show that this dataset can be used
to study structural relationships between different parts of the human body
when performing actions and fusion approaches that involve multi-modal sensor
data.",1,1,1,1,0,0,0.0859999,15.0,0.732119,46
0d2520db-67e3-44c7-b570-5e5ce90aa648,Towards Self-Supervised Category-Level Object Pose and Size Estimation,12,0.302638,0.532729,"In this work, we tackle the challenging problem of category-level object pose
and size estimation from a single depth image. Although previous
fully-supervised works have demonstrated promising performance, collecting
ground-truth pose labels is generally time-consuming and labor-intensive.
Instead, we propose a label-free method that learns to enforce the geometric
consistency between category template mesh and observed object point cloud
under a self-supervision manner. Specifically, our method consists of three key
components: differentiable shape deformation, registration, and rendering. In
particular, shape deformation and registration are applied to the template mesh
to eliminate the differences in shape, pose and scale. A differentiable
renderer is then deployed to enforce geometric consistency between point clouds
lifted from the rendered depth and the observed scene for self-supervision. We
evaluate our approach on real-world datasets and find that our approach
outperforms the simple traditional baseline by large margins while being
competitive with some fully-supervised approaches.",0,1,0,0,0,0,0.968458,9.0,0.958978,47
1ed5d240-933c-40c0-9137-2d3e51e39388,Transfer Learning based Search Space Design for Hyperparameter Tuning,10,0.0710509,0.694246,"The tuning of hyperparameters becomes increasingly important as machine
learning (ML) models have been extensively applied in data mining applications.
Among various approaches, Bayesian optimization (BO) is a successful
methodology to tune hyper-parameters automatically. While traditional methods
optimize each tuning task in isolation, there has been recent interest in
speeding up BO by transferring knowledge across previous tasks. In this work,
we introduce an automatic method to design the BO search space with the aid of
tuning history from past tasks. This simple yet effective approach can be used
to endow many existing BO methods with transfer learning capabilities. In
addition, it enjoys the three advantages: universality, generality, and
safeness. The extensive experiments show that our approach considerably boosts
BO by designing a promising and compact search space instead of using the
entire space, and outperforms the state-of-the-arts on a wide range of
benchmarks, including machine learning and deep learning tuning tasks, and
neural architecture search.",0,1,0,0,1,0,0.0909396,12.0,0.670025,50
7ed528dd-d4a4-4dd3-8c15-954abe628551,Improving the Robustness of Summarization Models by Detecting and Removing Input Noise,2,0.0,0.0698957,"The evaluation of abstractive summarization models typically uses test data
that is identically distributed as training data. In real-world practice,
documents to be summarized may contain input noise caused by text extraction
artifacts or data pipeline bugs. The robustness of model performance under
distribution shift caused by such noise is relatively under-studied. We present
a large empirical study quantifying the sometimes severe loss in performance
(up to 12 ROUGE-1 points) from different types of input noise for a range of
datasets and model sizes. We then propose a light-weight method for detecting
and removing such noise in the input during model inference without requiring
any extra training, auxiliary models, or even prior knowledge of the type of
noise. Our proposed approach effectively mitigates the loss in performance,
recovering a large fraction of the performance drop, sometimes as large as 11
ROUGE-1 points.",0,1,0,0,0,0,0.239167,7.0,0.58479,19
00060343-e78e-4a36-b288-ee739b50c9ce,Normalization Perturbation: A Simple Domain Generalization Method for Real-World Domain Shifts,7,0.0300079,0.432195,"Improving model's generalizability against domain shifts is crucial,
especially for safety-critical applications such as autonomous driving.
Real-world domain styles can vary substantially due to environment changes and
sensor noises, but deep models only know the training domain style. Such domain
style gap impedes model generalization on diverse real-world domains. Our
proposed Normalization Perturbation (NP) can effectively overcome this domain
style overfitting problem. We observe that this problem is mainly caused by the
biased distribution of low-level features learned in shallow CNN layers. Thus,
we propose to perturb the channel statistics of source domain features to
synthesize various latent styles, so that the trained deep model can perceive
diverse potential domains and generalizes well even without observations of
target domain data in training. We further explore the style-sensitive channels
for effective style synthesis. Normalization Perturbation only relies on a
single source domain and is surprisingly effective and extremely easy to
implement. Extensive experiments verify the effectiveness of our method for
generalizing models under real-world domain shifts.",0,1,0,0,0,0,0.330885,7.0,0.639823,146
4be6cbaa-3ab3-48e9-9c26-bf1af1b215b7,Improving Covariance Conditioning of the SVD Meta-layer by Orthogonality,6,0.0736072,0.195817,"Inserting an SVD meta-layer into neural networks is prone to make the
covariance ill-conditioned, which could harm the model in the training
stability and generalization abilities. In this paper, we systematically study
how to improve the covariance conditioning by enforcing orthogonality to the
Pre-SVD layer. Existing orthogonal treatments on the weights are first
investigated. However, these techniques can improve the conditioning but would
hurt the performance. To avoid such a side effect, we propose the Nearest
Orthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of
our methods is validated in two applications: decorrelated Batch Normalization
(BN) and Global Covariance Pooling (GCP). Extensive experiments on visual
recognition demonstrate that our methods can simultaneously improve the
covariance conditioning and generalization. Moreover, the combinations with
orthogonal weight can further boost the performances.",1,1,0,0,0,0,0.539511,8.0,0.767034,63
27b78b16-bdf6-45f9-917c-953d56e1bdb6,Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model,1,0.00372856,0.0410551,"Recently, deep learning-based algorithms are widely adopted due to the
advantage of being able to establish anomaly detection models without or with
minimal domain knowledge of the task. Instead, to train the artificial neural
network more stable, it should be better to define the appropriate neural
network structure or the loss function. For the training anomaly detection
model, the mean squared error (MSE) function is adopted widely. On the other
hand, the novel loss function, logarithmic mean squared error (LMSE), is
proposed in this paper to train the neural network more stable. This study
covers a variety of comparisons from mathematical comparisons, visualization in
the differential domain for backpropagation, loss convergence in the training
process, and anomaly detection performance. In an overall view, LMSE is
superior to the existing MSE function in terms of strongness of loss
convergence, anomaly detection performance. The LMSE function is expected to be
applicable for training not only the anomaly detection model but also the
general generative neural network.",0,1,0,0,0,0,0.080284,9.0,0.545548,21
ae1e60a1-fd8e-40f3-83a4-9ab8ddcbaa0a,NamedMask: Distilling Segmenters from Complementary Foundation Models,16,0.0377975,0.45153,"The goal of this work is to segment and name regions of images without access
to pixel-level labels during training. To tackle this task, we construct
segmenters by distilling the complementary strengths of two foundation models.
The first, CLIP (Radford et al. 2021), exhibits the ability to assign names to
image content but lacks an accessible representation of object structure. The
second, DINO (Caron et al. 2021), captures the spatial extent of objects but
has no knowledge of object names. Our method, termed NamedMask, begins by using
CLIP to construct category-specific archives of images. These images are
pseudo-labelled with a category-agnostic salient object detector bootstrapped
from DINO, then refined by category-specific segmenters using the CLIP archive
labels. Thanks to the high quality of the refined masks, we show that a
standard segmentation architecture trained on these archives with appropriate
data augmentation achieves impressive semantic segmentation abilities for both
single-object and multi-object images. As a result, our proposed NamedMask
performs favourably against a range of prior work on five benchmarks including
the VOC2012, COCO and large-scale ImageNet-S datasets.",1,1,0,0,1,1,0.496082,5.0,0.602545,46
967ce2e7-8430-49c0-8c06-549219b226c8,Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning and Generation with Large Language Models,3,0.21634,0.253973,"Previous work has demonstrated the effectiveness of planning for story
generation exclusively in a monolingual setting focusing primarily on English.
We consider whether planning brings advantages to automatic story generation
across languages. We propose a new task of cross-lingual story generation with
planning and present a new dataset for this task. We conduct a comprehensive
study of different plans and generate stories in several languages, by
leveraging the creative and reasoning capabilities of large pre-trained
language models. Our results demonstrate that plans which structure stories
into three acts lead to more coherent and interesting narratives, while
allowing to explicitly control their content and structure.",0,1,1,1,0,0,0.955031,6.0,0.920423,68
17d62115-ef09-4da5-b71d-8163f582be64,Cross-Image Relational Knowledge Distillation for Semantic Segmentation,87,0.998059,0.99806,"Current Knowledge Distillation (KD) methods for semantic segmentation often
guide the student to mimic the teacher's structured information generated from
individual data samples. However, they ignore the global semantic relations
among pixels across various images that are valuable for KD. This paper
proposes a novel Cross-Image Relational KD (CIRKD), which focuses on
transferring structured pixel-to-pixel and pixel-to-region relations among the
whole images. The motivation is that a good teacher network could construct a
well-structured feature space in terms of global pixel dependencies. CIRKD
makes the student mimic better structured semantic relations from the teacher,
thus improving the segmentation performance. Experimental results over
Cityscapes, CamVid and Pascal VOC datasets demonstrate the effectiveness of our
proposed approach against state-of-the-art distillation methods. The code is
available at https://github.com/winycg/CIRKD.",1,1,0,0,1,0,0.986785,8.0,0.981911,50
e77924b6-9db9-435a-9dd2-1e0cc063d64d,Combating high variance in Data-Scarce Implicit Hate Speech Classification,1,0.044928,0.129885,"Hate speech classification has been a long-standing problem in natural
language processing. However, even though there are numerous hate speech
detection methods, they usually overlook a lot of hateful statements due to
them being implicit in nature. Developing datasets to aid in the task of
implicit hate speech classification comes with its own challenges; difficulties
are nuances in language, varying definitions of what constitutes hate speech,
and the labor-intensive process of annotating such data. This had led to a
scarcity of data available to train and test such systems, which gives rise to
high variance problems when parameter-heavy transformer-based models are used
to address the problem. In this paper, we explore various optimization and
regularization techniques and develop a novel RoBERTa-based model that achieves
state-of-the-art performance.",0,1,0,0,1,0,0.962114,7.0,0.939475,19
e6f3ad58-6e2c-4f2a-aee0-d1f8837fc127,Read before Generate! Faithful Long Form Question Answering with Machine Reading,36,0.0684052,0.633893,"Long-form question answering (LFQA) aims to generate a paragraph-length
answer for a given question. While current work on LFQA using large pre-trained
model for generation are effective at producing fluent and somewhat relevant
content, one primary challenge lies in how to generate a faithful answer that
has less hallucinated content. We propose a new end-to-end framework that
jointly models answer generation and machine reading. The key idea is to
augment the generation model with fine-grained, answer-related salient
information which can be viewed as an emphasis on faithful facts.
State-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate
the effectiveness of our method, in comparison with strong baselines on
automatic and human evaluation metrics. A detailed analysis further proves the
competency of our methods in generating fluent, relevant, and more faithful
answers.",0,1,0,0,1,0,0.324846,6.0,0.576024,66
d01e0379-dca5-4e7c-a590-0de83abd0fce,DSI++: Updating Transformer Memory with New Documents,18,0.235929,0.641119,"Differentiable Search Indices (DSIs) encode a corpus of documents in model
parameters and use the same model to answer user queries directly. Despite the
strong performance of DSI models, deploying them in situations where the corpus
changes over time is computationally expensive because reindexing the corpus
requires re-training the model. In this work, we introduce DSI++, a continual
learning challenge for DSI to incrementally index new documents while being
able to answer queries related to both previously and newly indexed documents.
Across different model scales and document identifier representations, we show
that continual indexing of new documents leads to considerable forgetting of
previously indexed documents. We also hypothesize and verify that the model
experiences forgetting events during training, leading to unstable learning. To
mitigate these issues, we investigate two approaches. The first focuses on
modifying the training dynamics. Flatter minima implicitly alleviate
forgetting, so we optimize for flatter loss basins and show that the model
stably memorizes more documents ($+12\%$). Next, we introduce a generative
memory to sample pseudo-queries for documents and supplement them during
continual indexing to prevent forgetting for the retrieval task. Extensive
experiments on novel continual indexing benchmarks based on Natural Questions
(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting
significantly. Concretely, it improves the average Hits@10 by $+21.1\%$ over
competitive baselines for NQ and requires $6$ times fewer model updates
compared to re-training the DSI model for incrementally indexing five corpora
in a sequence.",0,1,1,0,0,0,0.83856,6.0,0.831898,56
4308be46-0e95-4516-955f-702e8250d433,Distilling Style from Image Pairs for Global Forward and Inverse Tone Mapping,7,0.122461,0.214744,"Many image enhancement or editing operations, such as forward and inverse
tone mapping or color grading, do not have a unique solution, but instead a
range of solutions, each representing a different style. Despite this, existing
learning-based methods attempt to learn a unique mapping, disregarding this
style. In this work, we show that information about the style can be distilled
from collections of image pairs and encoded into a 2- or 3-dimensional vector.
This gives us not only an efficient representation but also an interpretable
latent space for editing the image style. We represent the global color mapping
between a pair of images as a custom normalizing flow, conditioned on a
polynomial basis of the pixel color. We show that such a network is more
effective than PCA or VAE at encoding image style in low-dimensional space and
lets us obtain an accuracy close to 40 dB, which is about 7-10 dB improvement
over the state-of-the-art methods.",0,1,0,0,1,0,0.662553,9.0,0.830376,50
50d7aae0-fa74-49a7-bf95-373f98e3a3bc,Readability Controllable Biomedical Document Summarization,30,0.325033,0.99587,"Different from general documents, it is recognised that the ease with which
people can understand a biomedical text is eminently varied, owing to the
highly technical nature of biomedical documents and the variance of readers'
domain knowledge. However, existing biomedical document summarization systems
have paid little attention to readability control, leaving users with summaries
that are incompatible with their levels of expertise. In recognition of this
urgent demand, we introduce a new task of readability controllable
summarization for biomedical documents, which aims to recognise users'
readability demands and generate summaries that better suit their needs:
technical summaries for experts and plain language summaries (PLS) for laymen.
To establish this task, we construct a corpus consisting of biomedical papers
with technical summaries and PLSs written by the authors, and benchmark
multiple advanced controllable abstractive and extractive summarization models
based on pre-trained language models (PLMs) with prevalent controlling and
generation techniques. Moreover, we propose a novel masked language model (MLM)
based metric and its variant to effectively evaluate the readability
discrepancy between lay and technical summaries. Experimental results from
automated and human evaluations show that though current control techniques
allow for a certain degree of readability adjustment during generation, the
performance of existing controllable summarization methods is far from
desirable in this task.",0,0,1,1,0,0,0.57283,6.0,0.704786,41
f069a3ce-5a1c-4f7a-988e-72599ab249be,Construction Repetition Reduces Information Rate in Dialogue,2,0.0645693,0.142535,"Speakers repeat constructions frequently in dialogue. Due to their peculiar
information-theoretic properties, repetitions can be thought of as a strategy
for cost-effective communication. In this study, we focus on the repetition of
lexicalised constructions -- i.e., recurring multi-word units -- in English
open-domain spoken dialogues. We hypothesise that speakers use construction
repetition to mitigate information rate, leading to an overall decrease in
utterance information content over the course of a dialogue. We conduct a
quantitative analysis, measuring the information content of constructions and
that of their containing utterances, estimating information content with an
adaptive neural language model. We observe that construction usage lowers the
information content of utterances. This facilitating effect (i) increases
throughout dialogues, (ii) is boosted by repetition, (iii) grows as a function
of repetition frequency and density, and (iv) is stronger for repetitions of
referential constructions.",0,0,0,0,0,0,0.0613899,24.0,0.817982,84
52927983-bf2c-4db0-a518-aca265ff4bde,Sentiment-Aware Automatic Speech Recognition pre-training for enhanced Speech Emotion Recognition,17,0.195103,0.748558,"We propose a novel multi-task pre-training method for Speech Emotion
Recognition (SER). We pre-train SER model simultaneously on Automatic Speech
Recognition (ASR) and sentiment classification tasks to make the acoustic ASR
model more ``emotion aware''. We generate targets for the sentiment
classification using text-to-sentiment model trained on publicly available
data. Finally, we fine-tune the acoustic ASR on emotion annotated speech data.
We evaluated the proposed approach on the MSP-Podcast dataset, where we
achieved the best reported concordance correlation coefficient (CCC) of 0.41
for valence prediction.",0,1,0,0,1,0,0.296388,7.0,0.620728,23
1e9c0d0e-c686-4666-8050-ccec343f5065,"Intelligent Computing: The Latest Advances, Challenges and Future",27,0.195899,0.923387,"Computing is a critical driving force in the development of human
civilization. In recent years, we have witnessed the emergence of intelligent
computing, a new computing paradigm that is reshaping traditional computing and
promoting digital revolution in the era of big data, artificial intelligence
and internet-of-things with new computing theories, architectures, methods,
systems, and applications. Intelligent computing has greatly broadened the
scope of computing, extending it from traditional computing on data to
increasingly diverse computing paradigms such as perceptual intelligence,
cognitive intelligence, autonomous intelligence, and human-computer fusion
intelligence. Intelligence and computing have undergone paths of different
evolution and development for a long time but have become increasingly
intertwined in recent years: intelligent computing is not only
intelligence-oriented but also intelligence-driven. Such cross-fertilization
has prompted the emergence and rapid advancement of intelligent computing.
Intelligent computing is still in its infancy and an abundance of innovations
in the theories, systems, and applications of intelligent computing are
expected to occur soon. We present the first comprehensive survey of literature
on intelligent computing, covering its theory fundamentals, the technological
fusion of intelligence and computing, important applications, challenges, and
future perspectives. We believe that this survey is highly timely and will
provide a comprehensive reference and cast valuable insights into intelligent
computing for academic and industrial researchers and practitioners.",0,0,0,0,0,1,0.115293,9.0,0.587878,374
4d194239-ba3f-473e-b64d-ff010572a74a,Meet-in-the-middle: Multi-scale upsampling and matching for cross-resolution face recognition,2,0.0440643,0.318799,"In this paper, we aim to address the large domain gap between high-resolution
face images, e.g., from professional portrait photography, and low-quality
surveillance images, e.g., from security cameras. Establishing an identity
match between disparate sources like this is a classical surveillance face
identification scenario, which continues to be a challenging problem for modern
face recognition techniques. To that end, we propose a method that combines
face super-resolution, resolution matching, and multi-scale template
accumulation to reliably recognize faces from long-range surveillance footage,
including from low quality sources. The proposed approach does not require
training or fine-tuning on the target dataset of real surveillance images.
Extensive experiments show that our proposed method is able to outperform even
existing methods fine-tuned to the SCFace dataset.",0,1,0,0,1,0,0.254822,10.0,0.716684,44
1b483764-9ba2-4f3b-b72c-fb5660c6cc90,Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning,1,0.0120354,0.0271398,"This paper presents Non-Axiomatic Term Logic (NATL) as a theoretical
computational framework of humanlike symbolic reasoning in artificial
intelligence. NATL unites a discrete syntactic system inspired from Aristotle's
term logic and a continuous semantic system based on the modern idea of
distributed representations, or embeddings. This paper positions the proposed
approach in the phylogeny and the literature of logic, and explains the
framework. As it is yet no more than a theory and it requires much further
elaboration to implement it, no quantitative evaluation is presented. Instead,
qualitative analyses of arguments using NATL, some applications to possible
cognitive science/robotics-related research, and remaining issues towards a
machinery implementation are discussed.",0,0,0,0,0,0,0.148843,16.0,0.785325,63
f72dc8d8-50f6-4149-a163-0fb5d579839d,A Universal Discriminator for Zero-Shot Generalization,7,0.0988687,0.377566,"Generative modeling has been the dominant approach for large-scale
pretraining and zero-shot generalization. In this work, we challenge this
convention by showing that discriminative approaches perform substantially
better than generative ones on a large number of NLP tasks. Technically, we
train a single discriminator to predict whether a text sample comes from the
true data distribution, similar to GANs. Since many NLP tasks can be formulated
as selecting from a few options, we use this discriminator to predict the
concatenation of input and which option has the highest probability of coming
from the true data distribution. This simple formulation achieves
state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by
16.0\%, 7.8\%, and 11.5\% respectively on different scales. In the finetuning
setting, our approach also achieves new state-of-the-art results on a wide
range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile,
our approach requires minimal prompting efforts, which largely improves
robustness and is essential for real-world applications. Furthermore, we also
jointly train a generalized UD in combination with generative tasks, which
maintains its advantage on discriminative tasks and simultaneously works on
generative tasks.",0,0,0,0,1,1,0.969097,5.0,0.927341,43
3604ce65-f2a7-4b74-9b66-abbc24682729,FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference,22,0.42836,0.632054,"Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the
architecture used for FiD was chosen by making minimal modifications to a
standard T5 model, which our analysis shows to be highly suboptimal for a
retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to
the encoder, while the majority of inference time results from memory bandwidth
constraints in the decoder. We propose two simple changes to the FiD
architecture to alleviate memory bandwidth constraints, and speed up inference
by 7x. This allows us to use a much larger decoder at modest cost. We denote
FiD with the above modifications as FiDO, and show that it strongly improves
performance over existing FiD models for a wide range of inference budgets. For
example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves
better performance than FiD-Large.",0,1,0,0,0,0,0.950128,6.0,0.914768,49
f5eeed21-818b-4443-a55e-f1c4fc89509b,Speeding Up Action Recognition Using Dynamic Accumulation of Residuals in Compressed Domain,2,0.00604067,0.182805,"With the widespread use of installed cameras, video-based monitoring
approaches have seized considerable attention for different purposes like
assisted living. Temporal redundancy and the sheer size of raw videos are the
two most common problematic issues related to video processing algorithms. Most
of the existing methods mainly focused on increasing accuracy by exploring
consecutive frames, which is laborious and cannot be considered for real-time
applications. Since videos are mostly stored and transmitted in compressed
format, these kinds of videos are available on many devices. Compressed videos
contain a multitude of beneficial information, such as motion vectors and
quantized coefficients. Proper use of this available information can greatly
improve the video understanding methods' performance. This paper presents an
approach for using residual data, available in compressed videos directly,
which can be obtained by a light partially decoding procedure. In addition, a
method for accumulating similar residuals is proposed, which dramatically
reduces the number of processed frames for action recognition. Applying neural
networks exclusively for accumulated residuals in the compressed domain
accelerates performance, while the classification results are highly
competitive with raw video approaches.",0,1,0,0,0,0,0.00599652,10.0,0.327699,33
8a376ddb-bd1c-4d96-a781-8cc43b0c3441,Designing Network Design Strategies Through Gradient Path Analysis,87,0.303875,0.997823,"Designing a high-efficiency and high-quality expressive network architecture
has always been the most important research topic in the field of deep
learning. Most of today's network design strategies focus on how to integrate
features extracted from different layers, and how to design computing units to
effectively extract these features, thereby enhancing the expressiveness of the
network. This paper proposes a new network design strategy, i.e., to design the
network architecture based on gradient path analysis. On the whole, most of
today's mainstream network design strategies are based on feed forward path,
that is, the network architecture is designed based on the data path. In this
paper, we hope to enhance the expressive ability of the trained model by
improving the network learning ability. Due to the mechanism driving the
network parameter learning is the backward propagation algorithm, we design
network design strategies based on back propagation path. We propose the
gradient path design strategies for the layer-level, the stage-level, and the
network-level, and the design strategies are proved to be superior and feasible
from theoretical analysis and experiments.",0,0,0,0,0,0,0.647914,13.0,0.8795,40
6121f91a-4fb5-4a3a-a41e-35370a0fe950,Worst Case Matters for Few-Shot Recognition,5,0.0668169,0.106395,"Few-shot recognition learns a recognition model with very few (e.g., 1 or 5)
images per category, and current few-shot learning methods focus on improving
the average accuracy over many episodes. We argue that in real-world
applications we may often only try one episode instead of many, and hence
maximizing the worst-case accuracy is more important than maximizing the
average accuracy. We empirically show that a high average accuracy not
necessarily means a high worst-case accuracy. Since this objective is not
accessible, we propose to reduce the standard deviation and increase the
average accuracy simultaneously. In turn, we devise two strategies from the
bias-variance tradeoff perspective to implicitly reach this goal: a simple yet
effective stability regularization (SR) loss together with model ensemble to
reduce variance during fine-tuning, and an adaptability calibration mechanism
to reduce the bias. Extensive experiments on benchmark datasets demonstrate the
effectiveness of the proposed strategies, which outperforms current
state-of-the-art methods with a significant margin in terms of not only
average, but also worst-case accuracy. Our code is available at
https://github.com/heekhero/ACSR.",1,1,0,0,1,0,0.93699,8.0,0.925926,39
b2ad9197-bb19-4fe3-879d-f8afd35d8848,"YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception",34,0.434738,0.969665,"Over the last decade, multi-tasking learning approaches have achieved
promising results in solving panoptic driving perception problems, providing
both high-precision and high-efficiency performance. It has become a popular
paradigm when designing networks for real-time practical autonomous driving
system, where computation resources are limited. This paper proposed an
effective and efficient multi-task learning network to simultaneously perform
the task of traffic object detection, drivable road area segmentation and lane
detection. Our model achieved the new state-of-the-art (SOTA) performance in
terms of accuracy and speed on the challenging BDD100K dataset. Especially, the
inference time is reduced by half compared to the previous SOTA model. Code
will be released in the near future.",0,1,0,0,1,0,0.919005,9.0,0.923576,30
a30acb43-eb85-4430-b762-b279e70553e6,Diverse Multiple Trajectory Prediction Using a Two-stage Prediction Network Trained with Lane Loss,10,0.0992374,0.630542,"Prior arts in the field of motion predictions for autonomous driving tend to
focus on finding a trajectory that is close to the ground truth trajectory.
Such problem formulations and approaches, however, frequently lead to loss of
diversity and biased trajectory predictions. Therefore, they are unsuitable for
real-world autonomous driving where diverse and road-dependent multimodal
trajectory predictions are critical for safety. To this end, this study
proposes a novel loss function, \textit{Lane Loss}, that ensures map-adaptive
diversity and accommodates geometric constraints. A two-stage trajectory
prediction architecture with a novel trajectory candidate proposal module,
\textit{Trajectory Prediction Attention (TPA)}, is trained with Lane Loss
encourages multiple trajectories to be diversely distributed, covering feasible
maneuvers in a map-aware manner. Furthermore, considering that the existing
trajectory performance metrics are focusing on evaluating the accuracy based on
the ground truth future trajectory, a quantitative evaluation metric is also
suggested to evaluate the diversity of predicted multiple trajectories. The
experiments performed on the Argoverse dataset show that the proposed method
significantly improves the diversity of the predicted trajectories without
sacrificing the prediction accuracy.",0,1,0,0,0,0,0.459979,6.0,0.651049,39
72d9c39d-89a1-4570-85f2-5bdf9a8bd021,When Is It Acceptable to Break the Rules? Knowledge Representation of Moral Judgement Based on Empirical Data,3,0.0993594,0.33924,"One of the most remarkable things about the human moral mind is its
flexibility. We can make moral judgments about cases we have never seen before.
We can decide that pre-established rules should be broken. We can invent novel
rules on the fly. Capturing this flexibility is one of the central challenges
in developing AI systems that can interpret and produce human-like moral
judgment. This paper details the results of a study of real-world decision
makers who judge whether it is acceptable to break a well-established norm:
``no cutting in line.'' We gather data on how human participants judge the
acceptability of line-cutting in a range of scenarios. Then, in order to
effectively embed these reasoning capabilities into a machine, we propose a
method for modeling them using a preference-based structure, which captures a
novel modification to standard ``dual process'' theories of moral judgment.",0,0,0,0,0,0,0.269944,12.0,0.769518,66
730b26a3-b026-48d6-9e0f-eb3a3a5454f1,Object Type Clustering using Markov Directly-Follow Multigraph in Object-Centric Process Mining,3,0.0147134,0.247241,"Object-centric process mining is a new paradigm with more realistic
assumptions about underlying data by considering several case notions, e.g., an
order handling process can be analyzed based on order, item, package, and route
case notions. Including many case notions can result in a very complex model.
To cope with such complexity, this paper introduces a new approach to cluster
similar case notions based on Markov Directly-Follow Multigraph, which is an
extended version of the well-known Directly-Follow Graph supported by many
industrial and academic process mining tools. This graph is used to calculate a
similarity matrix for discovering clusters of similar case notions based on a
threshold. A threshold tuning algorithm is also defined to identify sets of
different clusters that can be discovered based on different levels of
similarity. Thus, the cluster discovery will not rely on merely analysts'
assumptions. The approach is implemented and released as a part of a python
library, called processmining, and it is evaluated through a Purchase to Pay
(P2P) object-centric event log file. Some discovered clusters are evaluated by
discovering Directly Follow-Multigraph by flattening the log based on the
clusters. The similarity between identified clusters is also evaluated by
calculating the similarity between the behavior of the process models
discovered for each case notion using inductive miner based on footprints
conformance checking.",1,1,0,0,0,0,0.0360684,6.0,0.181089,28
04081bee-c18a-4174-8100-ca63d985e555,A two-step approach to leverage contextual data: speech recognition in air-traffic communications,13,0.0233499,0.469055,"Automatic Speech Recognition (ASR), as the assistance of speech communication
between pilots and air-traffic controllers, can significantly reduce the
complexity of the task and increase the reliability of transmitted information.
ASR application can lead to a lower number of incidents caused by
misunderstanding and improve air traffic management (ATM) efficiency.
Evidently, high accuracy predictions, especially, of key information, i.e.,
callsigns and commands, are required to minimize the risk of errors. We prove
that combining the benefits of ASR and Natural Language Processing (NLP)
methods to make use of surveillance data (i.e. additional modality) helps to
considerably improve the recognition of callsigns (named entity). In this
paper, we investigate a two-step callsign boosting approach: (1) at the 1 step
(ASR), weights of probable callsign n-grams are reduced in G.fst and/or in the
decoding FST (lattices), (2) at the 2 step (NLP), callsigns extracted from the
improved recognition outputs with Named Entity Recognition (NER) are correlated
with the surveillance data to select the most suitable one. Boosting callsign
n-grams with the combination of ASR and NLP methods eventually leads up to
53.7% of an absolute, or 60.4% of a relative, improvement in callsign
recognition.",1,1,0,0,0,0,0.125913,5.0,0.276987,21
48655f97-01e6-4d68-a2b7-9c64ca0aad2f,Reducing Exploitability with Population Based Training,6,0.0730301,0.468461,"Self-play reinforcement learning has achieved state-of-the-art, and often
superhuman, performance in a variety of zero-sum games. Yet prior work has
found that policies that are highly capable against regular opponents can fail
catastrophically against adversarial policies: an opponent trained explicitly
against the victim. Prior defenses using adversarial training were able to make
the victim robust to a specific adversary, but the victim remained vulnerable
to new ones. We conjecture this limitation was due to insufficient diversity of
adversaries seen during training. We analyze a defense using population based
training to pit the victim against a diverse set of opponents. We evaluate this
defense's robustness against new adversaries in two low-dimensional
environments. This defense increases robustness against adversaries, as
measured by the number of attacker training timesteps to exploit the victim.
Furthermore, we show that robustness is correlated with the size of the
opponent population.",1,1,0,0,0,0,0.813357,10.0,0.89085,29
6bb3ba21-9577-4586-8a9c-213724624a67,Similarity and Content-based Phonetic Self Attention for Speech Recognition,4,0.0201256,0.101172,"Transformer-based speech recognition models have achieved great success due
to the self-attention (SA) mechanism that utilizes every frame in the feature
extraction process. Especially, SA heads in lower layers capture various
phonetic characteristics by the query-key dot product, which is designed to
compute the pairwise relationship between frames. In this paper, we propose a
variant of SA to extract more representative phonetic features. The proposed
phonetic self-attention (phSA) is composed of two different types of phonetic
attention; one is similarity-based and the other is content-based. In short,
similarity-based attention captures the correlation between frames while
content-based attention only considers each frame without being affected by
other frames. We identify which parts of the original dot product equation are
related to two different attention patterns and improve each part with simple
modifications. Our experiments on phoneme classification and speech recognition
show that replacing SA with phSA for lower layers improves the recognition
performance without increasing the latency and the parameter size.",0,0,0,0,0,0,0.51182,4.0,0.514495,29
02c5c2a8-9096-46f9-9c0e-53d5d2d55452,Zero-shot Cross-lingual Conversational Semantic Role Labeling,3,0.143743,0.257708,"While conversational semantic role labeling (CSRL) has shown its usefulness
on Chinese conversational tasks, it is still under-explored in non-Chinese
languages due to the lack of multilingual CSRL annotations for the parser
training. To avoid expensive data collection and error-propagation of
translation-based methods, we present a simple but effective approach to
perform zero-shot cross-lingual CSRL. Our model implicitly learns
language-agnostic, conversational structure-aware and semantically rich
representations with the hierarchical encoders and elaborately designed
pre-training objectives. Experimental results show that our model outperforms
all baselines by large margins on two newly collected English CSRL test sets.
More importantly, we confirm the usefulness of CSRL to non-Chinese
conversational tasks such as the question-in-context rewriting task in English
and the multi-turn dialogue response generation tasks in English, German and
Japanese by incorporating the CSRL information into the downstream
conversation-based models. We believe this finding is significant and will
facilitate the research of non-Chinese dialogue tasks which suffer the problems
of ellipsis and anaphora.",1,1,1,1,1,0,0.930665,5.0,0.874439,52
63a77933-700f-4b62-9d2a-a0c9da5e3513,EMA-VIO: Deep Visual-Inertial Odometry with External Memory Attention,11,0.258285,0.913878,"Accurate and robust localization is a fundamental need for mobile agents.
Visual-inertial odometry (VIO) algorithms exploit the information from camera
and inertial sensors to estimate position and translation. Recent deep learning
based VIO models attract attentions as they provide pose information in a
data-driven way, without the need of designing hand-crafted algorithms.
Existing learning based VIO models rely on recurrent models to fuse multimodal
data and process sensor signal, which are hard to train and not efficient
enough. We propose a novel learning based VIO framework with external memory
attention that effectively and efficiently combines visual and inertial
features for states estimation. Our proposed model is able to estimate pose
accurately and robustly, even in challenging scenarios, e.g., on overcast days
and water-filled ground , which are difficult for traditional VIO algorithms to
extract visual features. Experiments validate that it outperforms both
traditional and learning based VIO baselines in different scenes.",0,1,0,0,0,0,0.64121,10.0,0.841527,31
260215ff-2d6a-46c4-a00b-afa50ff1ff92,Active Image Indexing,3,0.0345474,0.251712,"Image copy detection and retrieval from large databases leverage two
components. First, a neural network maps an image to a vector representation,
that is relatively robust to various transformations of the image. Second, an
efficient but approximate similarity search algorithm trades scalability (size
and speed) against quality of the search, thereby introducing a source of
error. This paper improves the robustness of image copy detection with active
indexing, that optimizes the interplay of these two components. We reduce the
quantization loss of a given image representation by making imperceptible
changes to the image before its release. The loss is back-propagated through
the deep neural network back to the image, under perceptual constraints. These
modifications make the image more retrievable. Our experiments show that the
retrieval and copy detection of activated images is significantly improved. For
instance, activation improves by $+40\%$ the Recall1@1 on various image
transformations, and for several popular indexing structures based on product
quantization and locality sensitivity hashing.",1,1,0,0,0,0,0.252456,11.0,0.741454,66
8950a2f1-f40f-41cc-abf1-3bf0cccb3cc2,Near-Infrared Depth-Independent Image Dehazing using Haar Wavelets,2,0.00787596,0.0773233,"We propose a fusion algorithm for haze removal that combines color
information from an RGB image and edge information extracted from its
corresponding NIR image using Haar wavelets. The proposed algorithm is based on
the key observation that NIR edge features are more prominent in the hazy
regions of the image than the RGB edge features in those same regions. To
combine the color and edge information, we introduce a haze-weight map which
proportionately distributes the color and edge information during the fusion
process. Because NIR images are, intrinsically, nearly haze-free, our work
makes no assumptions like existing works that rely on a scattering model and
essentially designing a depth-independent method. This helps in minimizing
artifacts and gives a more realistic sense to the restored haze-free image.
Extensive experiments show that the proposed algorithm is both qualitatively
and quantitatively better on several key metrics when compared to existing
state-of-the-art methods.",0,1,0,0,1,0,0.0274155,16.0,0.675486,32
ee934b78-18f0-432f-9f88-a4cdf933ca65,PG3: Policy-Guided Planning for Generalized Policy Generation,5,0.171851,0.375199,"A longstanding objective in classical planning is to synthesize policies that
generalize across multiple problems from the same domain. In this work, we
study generalized policy search-based methods with a focus on the score
function used to guide the search over policies. We demonstrate limitations of
two score functions and propose a new approach that overcomes these
limitations. The main idea behind our approach, Policy-Guided Planning for
Generalized Policy Generation (PG3), is that a candidate policy should be used
to guide planning on training problems as a mechanism for evaluating that
candidate. Theoretical results in a simplified setting give conditions under
which PG3 is optimal or admissible. We then study a specific instantiation of
policy search where planning problems are PDDL-based and policies are lifted
decision lists. Empirical results in six domains confirm that PG3 learns
generalized policies more efficiently and effectively than several baselines.
Code: https://github.com/ryangpeixu/pg3",1,0,0,0,0,0,0.302992,10.0,0.737157,44
d11a5a80-237b-4b37-a681-9754d6dc0c9d,Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild,10,0.501892,0.540495,"Talking face generation with great practical significance has attracted more
attention in recent audio-visual studies. How to achieve accurate lip
synchronization is a long-standing challenge to be further investigated.
Motivated by xxx, in this paper, an AttnWav2Lip model is proposed by
incorporating spatial attention module and channel attention module into
lip-syncing strategy. Rather than focusing on the unimportant regions of the
face image, the proposed AttnWav2Lip model is able to pay more attention on the
lip region reconstruction. To our limited knowledge, this is the first attempt
to introduce attention mechanism to the scheme of talking face generation. An
extensive experiments have been conducted to evaluate the effectiveness of the
proposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a
superior performance has been demonstrated on the benchmark lip synthesis
datasets, including LRW, LRS2 and LRS3.",0,1,0,0,0,0,0.993219,11.0,0.999885,37
e1618f89-f475-420a-8b13-eb052e7bdc1d,Region Aware Video Object Segmentation with Deep Motion Modeling,12,0.0767863,0.34186,"Current semi-supervised video object segmentation (VOS) methods usually
leverage the entire features of one frame to predict object masks and update
memory. This introduces significant redundant computations. To reduce
redundancy, we present a Region Aware Video Object Segmentation (RAVOS)
approach that predicts regions of interest (ROIs) for efficient object
segmentation and memory storage. RAVOS includes a fast object motion tracker to
predict their ROIs in the next frame. For efficient segmentation, object
features are extracted according to the ROIs, and an object decoder is designed
for object-level segmentation. For efficient memory storage, we propose motion
path memory to filter out redundant context by memorizing the features within
the motion path of objects between two frames. Besides RAVOS, we also propose a
large-scale dataset, dubbed OVOS, to benchmark the performance of VOS models
under occlusions. Evaluation on DAVIS and YouTube-VOS benchmarks and our new
OVOS dataset show that our method achieves state-of-the-art performance with
significantly faster inference time, e.g., 86.1 J&F at 42 FPS on DAVIS and 84.4
J&F at 23 FPS on YouTube-VOS.",1,1,1,1,1,0,0.530502,5.0,0.622194,91
0ee3aa76-c5fe-4899-9231-fe5313a20df2,Grounding Answers for Visual Questions Asked by Visually Impaired People,30,0.286187,0.503319,"Visual question answering is the task of answering questions about images. We
introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually
grounds answers to visual questions asked by people with visual impairments. We
analyze our dataset and compare it with five VQA-Grounding datasets to
demonstrate what makes it similar and different. We then evaluate the SOTA VQA
and VQA-Grounding models and demonstrate that current SOTA algorithms often
fail to identify the correct visual evidence where the answer is located. These
models regularly struggle when the visual evidence occupies a small fraction of
the image, for images that are higher quality, as well as for visual questions
that require skills in text recognition. The dataset, evaluation server, and
leaderboard all can be found at the following link:
https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.",0,1,1,1,0,0,0.362073,7.0,0.655859,42
d569a582-0ab6-4868-8949-911cb9e34a93,ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information,5,0.1247,0.291826,"ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10
billion web pages affiliated with rich information. Its design was influenced
by the need for a high quality, large scale web corpus to support a range of
academic and industry research, for example, in information systems,
retrieval-augmented AI systems, and model pretraining. Compared with earlier
ClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of
higher-quality, and aligned with the document distributions in commercial web
search. Besides raw HTML, ClueWeb22 includes rich information about the web
pages provided by industry-standard document understanding systems, including
the visual representation of pages rendered by a web browser, parsed HTML
structure information from a neural network parser, and pre-processed cleaned
document text to lower the barrier to entry. Many of these signals have been
widely used in industry but are available to the research community for the
first time at this scale.",0,1,0,1,0,1,0.933667,7.0,0.912663,36
64c2c606-a36e-4873-ae9e-fb7851e65e49,VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification,18,0.301214,0.398475,"Multimodal learning from document data has achieved great success lately as
it allows to pre-train semantically meaningful features as a prior into a
learnable downstream task. In this paper, we approach the document
classification problem by learning cross-modal representations through language
and vision cues, considering intra- and inter-modality relationships. Instead
of merging features from different modalities into a joint representation
space, the proposed method exploits high-level interactions and learns relevant
semantic information from effective attention flows within and across
modalities. The proposed learning objective is devised between intra- and
inter-modality alignment tasks, where the similarity distribution per task is
computed by contracting positive sample pairs while simultaneously contrasting
negative ones in the joint representation space}. Extensive experiments on
public document classification datasets demonstrate the effectiveness and the
generality of our model on low-scale and large-scale datasets.",0,0,0,0,0,0,0.718781,5.0,0.725699,55
b36e0158-e98c-4b8d-96fc-a346dad45aa9,Lipschitz-constrained Unsupervised Skill Discovery,21,0.259206,0.902285,"We study the problem of unsupervised skill discovery, whose goal is to learn
a set of diverse and useful skills with no external reward. There have been a
number of skill discovery methods based on maximizing the mutual information
(MI) between skills and states. However, we point out that their MI objectives
usually prefer static skills to dynamic ones, which may hinder the application
for downstream tasks. To address this issue, we propose Lipschitz-constrained
Skill Discovery (LSD), which encourages the agent to discover more diverse,
dynamic, and far-reaching skills. Another benefit of LSD is that its learned
representation function can be utilized for solving goal-following downstream
tasks even in a zero-shot manner - i.e., without further training or complex
planning. Through experiments on various MuJoCo robotic locomotion and
manipulation environments, we demonstrate that LSD outperforms previous
approaches in terms of skill diversity, state space coverage, and performance
on seven downstream tasks including the challenging task of following multiple
goals on Humanoid. Our code and videos are available at
https://shpark.me/projects/lsd/.",1,0,0,0,1,0,0.561889,8.0,0.774817,40
95eab57b-69b2-40fa-a222-c75f4f51ef37,Instance Shadow Detection with A Single-Stage Detector,10,0.0969548,0.52337,"This paper formulates a new problem, instance shadow detection, which aims to
detect shadow instance and the associated object instance that cast each shadow
in the input image. To approach this task, we first compile a new dataset with
the masks for shadow instances, object instances, and shadow-object
associations. We then design an evaluation metric for quantitative evaluation
of the performance of instance shadow detection. Further, we design a
single-stage detector to perform instance shadow detection in an end-to-end
manner, where the bidirectional relation learning module and the deformable
maskIoU head are proposed in the detector to directly learn the relation
between shadow instances and object instances and to improve the accuracy of
the predicted masks. Finally, we quantitatively and qualitatively evaluate our
method on the benchmark dataset of instance shadow detection and show the
applicability of our method on light direction estimation and photo editing.",1,1,1,1,0,0,0.386249,9.0,0.741496,79
6ca9c61d-a806-4204-b771-b6792b6560cb,SC-wLS: Towards Interpretable Feed-forward Camera Re-localization,12,0.0717723,0.859523,"Visual re-localization aims to recover camera poses in a known environment,
which is vital for applications like robotics or augmented reality.
Feed-forward absolute camera pose regression methods directly output poses by a
network, but suffer from low accuracy. Meanwhile, scene coordinate based
methods are accurate, but need iterative RANSAC post-processing, which brings
challenges to efficient end-to-end training and inference. In order to have the
best of both worlds, we propose a feed-forward method termed SC-wLS that
exploits all scene coordinate estimates for weighted least squares pose
regression. This differentiable formulation exploits a weight network imposed
on 2D-3D correspondences, and requires pose supervision only. Qualitative
results demonstrate the interpretability of learned weights. Evaluations on
7Scenes and Cambridge datasets show significantly promoted performance when
compared with former feed-forward counterparts. Moreover, our SC-wLS method
enables a new capability: self-supervised test-time adaptation on the weight
network. Codes and models are publicly available.",0,0,0,0,0,0,0.152529,11.0,0.690161,60
0345c5df-87de-4b8b-878e-87852a51e390,"""Covid vaccine is against Covid but Oxford vaccine is made at Oxford!"" Semantic Interpretation of Proper Noun Compounds",1,0.0238498,0.308608,"Proper noun compounds, e.g., ""Covid vaccine"", convey information in a
succinct manner (a ""Covid vaccine"" is a ""vaccine that immunizes against the
Covid disease""). These are commonly used in short-form domains, such as news
headlines, but are largely ignored in information-seeking applications. To
address this limitation, we release a new manually annotated dataset, ProNCI,
consisting of 22.5K proper noun compounds along with their free-form semantic
interpretations. ProNCI is 60 times larger than prior noun compound datasets
and also includes non-compositional examples, which have not been previously
explored. We experiment with various neural models for automatically generating
the semantic interpretations from proper noun compounds, ranging from few-shot
prompting to supervised learning, with varying degrees of knowledge about the
constituent nouns. We find that adding targeted knowledge, particularly about
the common noun, results in performance gains of upto 2.8%. Finally, we
integrate our model generated interpretations with an existing Open IE system
and observe an 7.5% increase in yield at a precision of 85%. The dataset and
code are available at https://github.com/dair-iitd/pronci.",1,0,1,1,0,0,0.13026,10.0,0.642131,43
53e73f76-b806-4c39-b9a9-d7eea79314ea,SC^2-PCR: A Second Order Spatial Compatibility for Efficient and Robust Point Cloud Registration,53,0.656698,0.923899,"In this paper, we present a second order spatial compatibility (SC^2) measure
based method for efficient and robust point cloud registration (PCR), called
SC^2-PCR. Firstly, we propose a second order spatial compatibility (SC^2)
measure to compute the similarity between correspondences. It considers the
global compatibility instead of local consistency, allowing for more
distinctive clustering between inliers and outliers at early stage. Based on
this measure, our registration pipeline employs a global spectral technique to
find some reliable seeds from the initial correspondences. Then we design a
two-stage strategy to expand each seed to a consensus set based on the SC^2
measure matrix. Finally, we feed each consensus set to a weighted SVD algorithm
to generate a candidate rigid transformation and select the best model as the
final result. Our method can guarantee to find a certain number of outlier-free
consensus sets using fewer samplings, making the model estimation more
efficient and robust. In addition, the proposed SC^2 measure is general and can
be easily plugged into deep learning based frameworks. Extensive experiments
are carried out to investigate the performance of our method. Code will be
available at \url{https://github.com/ZhiChen902/SC2-PCR}.",1,1,0,0,0,0,0.86346,9.0,0.897696,68
5fe923b6-b4a9-446e-8ce6-f11f01233d21,Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives,6,0.0848364,0.280449,"In this paper, we investigate the instability in the standard dense retrieval
training, which iterates between model training and hard negative selection
using the being-trained model. We show the catastrophic forgetting phenomena
behind the training instability, where models learn and forget different
negative groups during training iterations. We then propose ANCE-Tele, which
accumulates momentum negatives from past iterations and approximates future
iterations using lookahead negatives, as ""teleportations"" along the time axis
to smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms
previous state-of-the-art systems of similar size, eliminates the dependency on
sparse retrieval negatives, and is competitive among systems using
significantly more (50x) parameters. Our analysis demonstrates that
teleportation negatives reduce catastrophic forgetting and improve convergence
speed for dense retrieval training. Our code is available at
https://github.com/OpenMatch/ANCE-Tele.",1,1,0,0,1,0,0.933141,5.0,0.877145,47
738e9fda-507d-4023-bac3-89d93c2d3b6a,HOME: High-Order Mixed-Moment-based Embedding for Representation Learning,4,0.00458805,0.0924635,"Minimum redundancy among different elements of an embedding in a latent space
is a fundamental requirement or major preference in representation learning to
capture intrinsic informational structures. Current self-supervised learning
methods minimize a pair-wise covariance matrix to reduce the feature redundancy
and produce promising results. However, such representation features of
multiple variables may contain the redundancy among more than two feature
variables that cannot be minimized via the pairwise regularization. Here we
propose the High-Order Mixed-Moment-based Embedding (HOME) strategy to reduce
the redundancy between any sets of feature variables, which is to our best
knowledge the first attempt to utilize high-order statistics/information in
this context. Multivariate mutual information is minimum if and only if
multiple variables are mutually independent, which suggests the necessary
conditions of factorized mixed moments among multiple variables. Based on these
statistical and information theoretic principles, our general HOME framework is
presented for self-supervised representation learning. Our initial experiments
show that a simple version in the form of a three-order HOME scheme already
significantly outperforms the current two-order baseline method (i.e., Barlow
Twins) in terms of the linear evaluation on representation features.",0,0,0,0,0,1,0.505544,6.0,0.673335,32
6b8318f3-4265-4f34-8950-2fa166c2449a,On Calibrating Semantic Segmentation Models: Analyses and An Algorithm,11,0.0406539,0.468647,"We study the problem of semantic segmentation calibration. Lots of solutions
have been proposed to approach model miscalibration of confidence in image
classification. However, to date, confidence calibration research on semantic
segmentation is still limited. We provide a systematic study on the calibration
of semantic segmentation models and propose a simple yet effective approach.
First, we find that model capacity, crop size, multi-scale testing, and
prediction correctness have impact on calibration. Among them, prediction
correctness, especially misprediction, is more important to miscalibration due
to over-confidence. Next, we propose a simple, unifying, and effective
approach, namely selective scaling, by separating correct/incorrect prediction
for scaling and more focusing on misprediction logit smoothing. Then, we study
popular existing calibration methods and compare them with selective scaling on
semantic segmentation calibration. We conduct extensive experiments with a
variety of benchmarks on both in-domain and domain-shift calibration and show
that selective scaling consistently outperforms other methods.",1,0,0,0,0,0,0.2907,7.0,0.617418,60
4711b739-d681-48be-bedc-b0f074025b5e,Non-Markovian Reward Modelling from Trajectory Labels via Interpretable Multiple Instance Learning,9,0.0557364,0.519171,"We generalise the problem of reward modelling (RM) for reinforcement learning
(RL) to handle non-Markovian rewards. Existing work assumes that human
evaluators observe each step in a trajectory independently when providing
feedback on agent behaviour. In this work, we remove this assumption, extending
RM to capture temporal dependencies in human assessment of trajectories. We
show how RM can be approached as a multiple instance learning (MIL) problem,
where trajectories are treated as bags with return labels, and steps within the
trajectories are instances with unseen reward labels. We go on to develop new
MIL models that are able to capture the time dependencies in labelled
trajectories. We demonstrate on a range of RL tasks that our novel MIL models
can reconstruct reward functions to a high level of accuracy, and can be used
to train high-performing agent policies.",1,0,0,0,0,0,0.0629025,11.0,0.605155,51
432df95c-f39f-40bd-8dd9-199eaae25241,Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problems,14,0.0544935,0.26817,"We prove fast mixing and characterize the stationary distribution of the
Langevin Algorithm for inverting random weighted DNN generators. This result
extends the work of Hand and Voroninski from efficient inversion to efficient
posterior sampling. In practice, to allow for increased expressivity, we
propose to do posterior sampling in the latent space of a pre-trained
generative model. To achieve that, we train a score-based model in the latent
space of a StyleGAN-2 and we use it to solve inverse problems. Our framework,
Score-Guided Intermediate Layer Optimization (SGILO), extends prior work by
replacing the sparsity regularization with a generative prior in the
intermediate layer. Experimentally, we obtain significant improvements over the
previous state-of-the-art, especially in the low measurement regime.",0,0,0,0,0,0,0.51655,5.0,0.614294,74
6b96606d-6c96-4103-8abf-190571e1c87e,"Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges",4,0.00573177,0.125811,"Machine Learning algorithms have had a profound impact on the field of
computer science over the past few decades. These algorithms performance is
greatly influenced by the representations that are derived from the data in the
learning process. The representations learned in a successful learning process
should be concise, discrete, meaningful, and able to be applied across a
variety of tasks. A recent effort has been directed toward developing Deep
Learning models, which have proven to be particularly effective at capturing
high-dimensional, non-linear, and multi-modal characteristics. In this work, we
discuss the principles and developments that have been made in the process of
learning representations, and converting them into desirable applications. In
addition, for each framework or model, the key issues and open challenges, as
well as the advantages, are examined.",0,0,0,0,0,1,0.0337099,9.0,0.446411,482
0b39f0f0-5bdc-43f1-b47d-cdce09395d80,Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?,10,0.0561313,0.327602,"The use of Deep Learning and Computer Vision in the Cultural Heritage domain
is becoming highly relevant in the last few years with lots of applications
about audio smart guides, interactive museums and augmented reality. All these
technologies require lots of data to work effectively and be useful for the
user. In the context of artworks, such data is annotated by experts in an
expensive and time consuming process. In particular, for each artwork, an image
of the artwork and a description sheet have to be collected in order to perform
common tasks like Visual Question Answering. In this paper we propose a method
for Visual Question Answering that allows to generate at runtime a description
sheet that can be used for answering both visual and contextual questions about
the artwork, avoiding completely the image and the annotation process. For this
purpose, we investigate on the use of GPT-3 for generating descriptions for
artworks analyzing the quality of generated descriptions through captioning
metrics. Finally we evaluate the performance for Visual Question Answering and
captioning tasks.",0,1,0,0,0,0,0.589145,6.0,0.712247,39
0d918816-6c45-4ac9-9036-477a54614f03,SiNeRF: Sinusoidal Neural Radiance Fields for Joint Pose Estimation and Scene Reconstruction,26,0.402941,0.618825,"NeRFmm is the Neural Radiance Fields (NeRF) that deal with Joint Optimization
tasks, i.e., reconstructing real-world scenes and registering camera parameters
simultaneously. Despite NeRFmm producing precise scene synthesis and pose
estimations, it still struggles to outperform the full-annotated baseline on
challenging scenes. In this work, we identify that there exists a systematic
sub-optimality in joint optimization and further identify multiple potential
sources for it. To diminish the impacts of potential sources, we propose
Sinusoidal Neural Radiance Fields (SiNeRF) that leverage sinusoidal activations
for radiance mapping and a novel Mixed Region Sampling (MRS) for selecting ray
batch efficiently. Quantitative and qualitative results show that compared to
NeRFmm, SiNeRF achieves comprehensive significant improvements in image
synthesis quality and pose estimation accuracy. Codes are available at
https://github.com/yitongx/sinerf.",1,0,0,0,0,0,0.987158,6.0,0.976983,28
7ffc3841-347f-4e6c-9c04-b6fb568a9bfe,What are the best systems? New perspectives on NLP Benchmarking,23,0.185963,0.262474,"In Machine Learning, a benchmark refers to an ensemble of datasets associated
with one or multiple metrics together with a way to aggregate different systems
performances. They are instrumental in (i) assessing the progress of new
methods along different axes and (ii) selecting the best systems for practical
use. This is particularly the case for NLP with the development of large
pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a
variety of tasks. While the community mainly focused on developing new datasets
and metrics, there has been little interest in the aggregation procedure, which
is often reduced to a simple average over various performance measures.
However, this procedure can be problematic when the metrics are on a different
scale, which may lead to spurious conclusions. This paper proposes a new
procedure to rank systems based on their performance across different tasks.
Motivated by the social choice theory, the final system ordering is obtained
through aggregating the rankings induced by each task and is theoretically
grounded. We conduct extensive numerical experiments (on over 270k scores) to
assess the soundness of our approach both on synthetic and real scores (e.g.
GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method
yields different conclusions on state-of-the-art systems than the
mean-aggregation procedure while being both more reliable and robust.",0,0,0,0,0,0,0.474922,8.0,0.743854,121
1c7af9d5-6af1-437d-9b89-4c0e2e308213,A gentle introduction to Quantum Natural Language Processing,6,0.0311055,0.14976,"The main goal of this master's thesis is to introduce Quantum Natural
Language Processing (QNLP) in a way understandable by both the NLP engineer and
the quantum computing practitioner. QNLP is a recent application of quantum
computing that aims at representing sentences' meaning as vectors encoded into
quantum computers. To achieve this, the distributional meaning of words is
extended by the compositional meaning of sentences (DisCoCat model) : the
vectors representing words' meanings are composed through the syntactic
structure of the sentence. This is done using an algorithm based on tensor
products. We see that this algorithm is inefficient on classical computers but
scales well using quantum circuits. After exposing the practical details of its
implementation, we go through three use-cases.",0,0,0,0,0,0,0.0176199,14.0,0.597193,49
a32a30d2-d2cf-4237-8106-ad6af1ff04c0,Learning from Bootstrapping and Stepwise Reinforcement Reward: A Semi-Supervised Framework for Text Style Transfer,1,0.0147221,0.029388,"Text style transfer is an important task in controllable language generation.
Supervised approaches have pushed performance improvement on style-oriented
rewriting such as formality conversion. However, challenges remain due to the
scarcity of large-scale parallel data in many domains. While unsupervised
approaches do not rely on annotated sentence pairs for each style, they are
often plagued with instability issues such as mode collapse or quality
degradation. To take advantage of both supervised and unsupervised paradigms
and tackle the challenges, in this work, we propose a semi-supervised framework
for text style transfer. First, the learning process is bootstrapped with
supervision guided by automatically constructed pseudo-parallel pairs using
lexical and semantic-based methods. Then the model learns from unlabeled data
via reinforcement rewards. Specifically, we propose to improve the
sequence-to-sequence policy gradient via stepwise reward optimization,
providing fine-grained learning signals and stabilizing the reinforced learning
process. Experimental results show that the proposed approach achieves
state-of-the-art performance on multiple datasets, and produces effective
generation with as minimal as 10\% of training data.",1,1,0,0,1,1,0.644035,7.0,0.774708,49
7d41d2a9-794e-4eb2-9570-5ea99d17a150,AutoAttention: Automatic Field Pair Selection for Attention in User Behavior Modeling,2,0.0585662,0.358099,"In Click-through rate (CTR) prediction models, a user's interest is usually
represented as a fixed-length vector based on her history behaviors. Recently,
several methods are proposed to learn an attentive weight for each user
behavior and conduct weighted sum pooling. However, these methods only manually
select several fields from the target item side as the query to interact with
the behaviors, neglecting the other target item fields, as well as user and
context fields. Directly including all these fields in the attention may
introduce noise and deteriorate the performance. In this paper, we propose a
novel model named AutoAttention, which includes all item/user/context side
fields as the query, and assigns a learnable weight for each field pair between
behavior fields and query fields. Pruning on these field pairs via these
learnable weights lead to automatic field pair selection, so as to identify and
remove noisy field pairs. Though including more fields, the computation cost of
AutoAttention is still low due to using a simple attention function and field
pair selection. Extensive experiments on the public dataset and Tencent's
production dataset demonstrate the effectiveness of the proposed approach.",1,1,0,0,1,0,0.867651,9.0,0.899423,49
94be1ef0-3ad8-4e21-83e3-ce60ba182656,Automatic Chain of Thought Prompting in Large Language Models,301,1.0,0.952796,"Large language models (LLMs) can perform complex reasoning by generating
intermediate reasoning steps. Providing these steps for prompting
demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has
two major paradigms. One leverages a simple prompt like ""Let's think step by
step"" to facilitate step-by-step thinking before answering a question. The
other uses a few manual demonstrations one by one, each composed of a question
and a reasoning chain that leads to an answer. The superior performance of the
second paradigm hinges on the hand-crafting of task-specific demonstrations one
by one. We show that such manual efforts may be eliminated by leveraging LLMs
with the ""Let's think step by step"" prompt to generate reasoning chains for
demonstrations one by one, i.e., let's think not just step by step, but also
one by one. However, these generated chains often come with mistakes. To
mitigate the effect of such mistakes, we find that diversity matters for
automatically constructing demonstrations. We propose an automatic CoT
prompting method: Auto-CoT. It samples questions with diversity and generates
reasoning chains to construct demonstrations. On ten public benchmark reasoning
tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of
the CoT paradigm that requires manual designs of demonstrations. Code is
available at https://github.com/amazon-research/auto-cot",0,1,0,0,0,0,0.993261,3.0,0.99999,39
cf2cb427-b659-450c-9db4-ec31b1108b6e,Predicting Vegetation Stratum Occupancy from Airborne LiDAR Data with Deep Learning,5,0.0877851,0.440051,"We propose a new deep learning-based method for estimating the occupancy of
vegetation strata from airborne 3D LiDAR point clouds. Our model predicts
rasterized occupancy maps for three vegetation strata corresponding to lower,
medium, and higher cover. Our weakly-supervised training scheme allows our
network to only be supervised with vegetation occupancy values aggregated over
cylindrical plots containing thousands of points. Such ground truth is easier
to produce than pixel-wise or point-wise annotations. Our method outperforms
handcrafted and deep learning baselines in terms of precision by up to 30%,
while simultaneously providing visual and interpretable predictions. We provide
an open-source implementation along with a dataset of 199 agricultural plots to
train and evaluate weakly supervised occupancy regression algorithms.",1,1,0,1,1,0,0.417103,11.0,0.797628,80
5abaa79d-a496-4464-bd59-1b0a026e0f3e,Graph Modeling in Computer Assisted Automotive Development,2,0.0025015,0.0674454,"We consider graph modeling for a knowledge graph for vehicle development,
with a focus on crash safety. An organized schema that incorporates information
from various structured and unstructured data sources is provided, which
includes relevant concepts within the domain. In particular, we propose
semantics for crash computer aided engineering (CAE) data, which enables
searchability, filtering, recommendation, and prediction for crash CAE data
during the development process. This graph modeling considers the CAE data in
the context of the R\&D development process and vehicle safety. Consequently,
we connect CAE data to the protocols that are used to assess vehicle safety
performances. The R\&D process includes CAD engineering and safety attributes,
with a focus on multidisciplinary problem-solving. We describe previous efforts
in graph modeling in comparison to our proposal, discuss its strengths and
limitations, and identify areas for future work.",0,1,0,0,0,0,0.000315931,11.0,0.120976,34
cfe09e2f-4b83-4586-8d07-f0ce1d750d32,Space-based gravitational wave signal detection and extraction with deep neural network,6,0.178143,0.730964,"Space-based gravitational wave (GW) detectors will be able to observe signals
from sources that are otherwise nearly impossible from current ground-based
detection. Consequently, the well established signal detection method, matched
filtering, will require a complex template bank, leading to a computational
cost that is too expensive in practice. Here, we develop a high-accuracy GW
signal detection and extraction method for all space-based GW sources. As a
proof of concept, we show that a science-driven and uniform multi-stage
self-attention-based deep neural network can identify synthetic signals that
are submerged in Gaussian noise. Our method exhibits a detection rate exceeding
99% in identifying signals from various sources, with the signal-to-noise ratio
at 50, at a false alarm rate of 1%. while obtaining at least 95% similarity
compared with target signals. We further demonstrate the interpretability and
strong generalization behavior for several extended scenarios.",1,1,0,0,0,0,0.614398,9.0,0.815819,50
7ffe9b46-76d9-4fc3-894d-0ee6bf02eeb2,HLDC: Hindi Legal Documents Corpus,10,0.573799,0.758396,"Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: https://github.com/Exploration-Lab/HLDC",1,1,1,1,0,0,0.987989,6.0,0.979524,36
ac0059a1-c428-4ab7-b640-8460a395e2ad,CRUSH: Contextually Regularized and User anchored Self-supervised Hate speech Detection,6,0.13879,0.480087,"The last decade has witnessed a surge in the interaction of people through
social networking platforms. While there are several positive aspects of these
social platforms, the proliferation has led them to become the breeding ground
for cyber-bullying and hate speech. Recent advances in NLP have often been used
to mitigate the spread of such hateful content. Since the task of hate speech
detection is usually applicable in the context of social networks, we introduce
CRUSH, a framework for hate speech detection using user-anchored
self-supervision and contextual regularization. Our proposed approach secures ~
1-12% improvement in test set metrics over best performing previous approaches
on two types of tasks and multiple popular english social media datasets.",1,1,0,0,1,0,0.88459,6.0,0.860055,34
559d5786-8323-4067-9ee5-7edb7d2a581f,Positive Unlabeled Contrastive Learning,8,0.086887,0.531131,"Self-supervised pretraining on unlabeled data followed by supervised
fine-tuning on labeled data is a popular paradigm for learning from limited
labeled examples. We extend this paradigm to the classical positive unlabeled
(PU) setting, where the task is to learn a binary classifier given only a few
labeled positive samples, and (often) a large amount of unlabeled samples
(which could be positive or negative).
  We first propose a simple extension of standard infoNCE family of contrastive
losses, to the PU setting; and show that this learns superior representations,
as compared to existing unsupervised and supervised approaches. We then develop
a simple methodology to pseudo-label the unlabeled samples using a new
PU-specific clustering scheme; these pseudo-labels can then be used to train
the final (positive vs. negative) classifier. Our method handily outperforms
state-of-the-art PU methods over several standard PU benchmark datasets, while
not requiring a-priori knowledge of any class prior (which is a common
assumption in other PU methods). We also provide a simple theoretical analysis
that motivates our methods.",0,0,1,0,0,0,0.86665,7.0,0.870153,59
2b7b6efc-2deb-45f7-83e6-566d4a99ec9e,"When classifying grammatical role, BERT doesn't care about word order... except when it matters",20,0.104264,0.742317,"Because meaning can often be inferred from lexical semantics alone, word
order is often a redundant cue in natural language. For example, the words
chopped, chef, and onion are more likely used to convey ""The chef chopped the
onion,"" not ""The onion chopped the chef."" Recent work has shown large language
models to be surprisingly word order invariant, but crucially has largely
considered natural prototypical inputs, where compositional meaning mostly
matches lexical expectations. To overcome this confound, we probe grammatical
role representation in English BERT and GPT-2, on instances where lexical
expectations are not sufficient, and word order knowledge is necessary for
correct classification. Such non-prototypical instances are naturally occurring
English sentences with inanimate subjects or animate objects, or sentences
where we systematically swap the arguments to make sentences like ""The onion
chopped the chef"". We find that, while early layer embeddings are largely
lexical, word order is in fact crucial in defining the later-layer
representations of words in semantically non-prototypical positions. Our
experiments isolate the effect of word order on the contextualization process,
and highlight how models use context in the uncommon, but critical, instances
where it matters.",1,0,0,0,0,0,0.515606,4.0,0.517195,40
a9833f31-3a92-44f2-a406-50101ff51a49,Relative Pose from SIFT Features,5,0.12378,0.264289,"This paper proposes the geometric relationship of epipolar geometry and
orientation- and scale-covariant, e.g., SIFT, features. We derive a new linear
constraint relating the unknown elements of the fundamental matrix and the
orientation and scale. This equation can be used together with the well-known
epipolar constraint to, e.g., estimate the fundamental matrix from four SIFT
correspondences, essential matrix from three, and to solve the semi-calibrated
case from three correspondences. Requiring fewer correspondences than the
well-known point-based approaches (e.g., 5PT, 6PT and 7PT solvers) for epipolar
geometry estimation makes RANSAC-like randomized robust estimation
significantly faster. The proposed constraint is tested on a number of problems
in a synthetic environment and on publicly available real-world datasets on
more than 80000 image pairs. It is superior to the state-of-the-art in terms of
processing time while often leading to more accurate results.",0,1,0,0,1,0,0.0492143,12.0,0.61701,40
b2d6f63f-eac6-4609-8c4d-94fca893e131,MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds,4,0.0692886,0.261831,"Semantic segmentation of 3D point cloud is an essential task for autonomous
driving environment perception. The pipeline of most pointwise point cloud
semantic segmentation methods includes points sampling, neighbor searching,
feature aggregation, and classification. Neighbor searching method like
K-nearest neighbors algorithm, KNN, has been widely applied. However, the
complexity of KNN is always a bottleneck of efficiency. In this paper, we
propose an end-to-end neural architecture, Multiple View Pointwise Net,
MVP-Net, to efficiently and directly infer large-scale outdoor point cloud
without KNN or any complex pre/postprocessing. Instead, assumption-based space
filling curves and multi-rotation of point cloud methods are introduced to
point feature aggregation and receptive field expanding. Numerical experiments
show that the proposed MVP-Net is 11 times faster than the most efficient
pointwise semantic segmentation method RandLA-Net and achieves the same
accuracy on the large-scale benchmark SemanticKITTI dataset.",0,1,0,0,1,0,0.883427,7.0,0.879382,36
234eee63-bccf-4f73-9bb0-a0f77d320042,MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning,11,0.200406,0.849612,"The deep learning community has witnessed an exponentially growing interest
in self-supervised learning (SSL). However, it still remains unexplored how to
build a framework for learning useful representations of raw music waveforms in
a self-supervised manner. In this work, we design Music2Vec, a framework
exploring different SSL algorithmic components and tricks for music audio
recordings. Our model achieves comparable results to the state-of-the-art
(SOTA) music SSL model Jukebox, despite being significantly smaller with less
than 2% of parameters of the latter. The model will be released on
Huggingface(Please refer to: https://huggingface.co/m-a-p/music2vec-v1)",1,1,0,0,1,0,0.863481,6.0,0.846558,15
0ac14974-0044-4e9a-b5db-7de47c9a3e5e,SDS-200: A Swiss German Speech to Standard German Text Corpus,14,0.582057,0.928154,"We present SDS-200, a corpus of Swiss German dialectal speech with Standard
German text translations, annotated with dialect, age, and gender information
of the speakers. The dataset allows for training speech translation, dialect
recognition, and speech synthesis systems, among others. The data was collected
using a web recording tool that is open to the public. Each participant was
given a text in Standard German and asked to translate it to their Swiss German
dialect before recording it. To increase the corpus quality, recordings were
validated by other participants. The data consists of 200 hours of speech by
around 4000 different speakers and covers a large part of the Swiss-German
dialect landscape. We release SDS-200 alongside a baseline speech translation
model, which achieves a word error rate (WER) of 30.3 and a BLEU score of 53.1
on the SDS-200 test set. Furthermore, we use SDS-200 to fine-tune a pre-trained
XLS-R model, achieving 21.6 WER and 64.0 BLEU.",1,1,1,1,1,0,0.901805,6.0,0.872079,16
abb66d9b-4952-487d-a01b-36c1f5528140,Relational Message Passing for Fully Inductive Knowledge Graph Completion,16,0.31091,0.93954,"In knowledge graph completion (KGC), predicting triples involving emerging
entities and/or relations, which are unseen when the KG embeddings are learned,
has become a critical challenge. Subgraph reasoning with message passing is a
promising and popular solution. Some recent methods have achieved good
performance, but they (i) usually can only predict triples involving unseen
entities alone, failing to address more realistic fully inductive situations
with both unseen entities and unseen relations, and (ii) often conduct message
passing over the entities with the relation patterns not fully utilized. In
this study, we propose a new method named RMPI which uses a novel Relational
Message Passing network for fully Inductive KGC. It passes messages directly
between relations to make full use of the relation patterns for subgraph
reasoning with new techniques on graph transformation, graph pruning,
relation-aware neighborhood attention, addressing empty subgraphs, etc., and
can utilize the relation semantics defined in the ontological schema of KG.
Extensive evaluation on multiple benchmarks has shown the effectiveness of
techniques involved in RMPI and its better performance compared with the
existing methods that support fully inductive KGC. RMPI is also comparable to
the state-of-the-art partially inductive KGC methods with very promising
results achieved. Our codes and data are available at
https://github.com/zjukg/RMPI.",1,1,1,0,0,0,0.642366,9.0,0.824268,48
d86b49c3-d8ae-4c0d-9560-c14b1acd134e,Uncertainty-based Visual Question Answering: Estimating Semantic Inconsistency between Image and Knowledge Base,1,0.0112885,0.074985,"Knowledge-based visual question answering (KVQA) task aims to answer
questions that require additional external knowledge as well as an
understanding of images and questions. Recent studies on KVQA inject an
external knowledge in a multi-modal form, and as more knowledge is used,
irrelevant information may be added and can confuse the question answering. In
order to properly use the knowledge, this study proposes the following: 1) we
introduce a novel semantic inconsistency measure computed from caption
uncertainty and semantic similarity; 2) we suggest a new external knowledge
assimilation method based on the semantic inconsistency measure and apply it to
integrate explicit knowledge and implicit knowledge for KVQA; 3) the proposed
method is evaluated with the OK-VQA dataset and achieves the state-of-the-art
performance.",0,1,0,0,1,0,0.70648,9.0,0.843796,42
0d2768f1-4783-4e96-9f22-5c334c896281,Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation,28,0.299205,0.99495,"Although the problem of hallucinations in neural machine translation (NMT)
has received some attention, research on this highly pathological phenomenon
lacks solid ground. Previous work has been limited in several ways: it often
resorts to artificial settings where the problem is amplified, it disregards
some (common) types of hallucinations, and it does not validate adequacy of
detection heuristics. In this paper, we set foundations for the study of NMT
hallucinations. First, we work in a natural setting, i.e., in-domain data
without artificial noise neither in training nor in inference. Next, we
annotate a dataset of over 3.4k sentences indicating different kinds of
critical errors and hallucinations. Then, we turn to detection methods and both
revisit methods used previously and propose using glass-box uncertainty-based
detectors. Overall, we show that for preventive settings, (i) previously used
methods are largely inadequate, (ii) sequence log-probability works best and
performs on par with reference-based methods. Finally, we propose
DeHallucinator, a simple method for alleviating hallucinations at test time
that significantly reduces the hallucinatory rate. To ease future research, we
release our annotated dataset for WMT18 German-English data, along with the
model, training data, and code.",1,1,0,1,0,0,0.641876,6.0,0.73618,62
0161a5f9-d890-4a2a-a886-031babcd9735,Intelligent problem-solving as integrated hierarchical reinforcement learning,44,0.387754,0.899674,"According to cognitive psychology and related disciplines, the development of
complex problem-solving behaviour in biological agents depends on hierarchical
cognitive mechanisms. Hierarchical reinforcement learning is a promising
computational approach that may eventually yield comparable problem-solving
behaviour in artificial agents and robots. However, to date the problem-solving
abilities of many human and non-human animals are clearly superior to those of
artificial systems. Here, we propose steps to integrate biologically inspired
hierarchical mechanisms to enable advanced problem-solving skills in artificial
agents. Therefore, we first review the literature in cognitive psychology to
highlight the importance of compositional abstraction and predictive
processing. Then we relate the gained insights with contemporary hierarchical
reinforcement learning methods. Interestingly, our results suggest that all
identified cognitive mechanisms have been implemented individually in isolated
computational architectures, raising the question of why there exists no single
unifying architecture that integrates them. As our final contribution, we
address this question by providing an integrative perspective on the
computational challenges to develop such a unifying architecture. We expect our
results to guide the development of more sophisticated cognitively inspired
hierarchical machine learning architectures.",0,0,0,0,0,0,0.216313,10.0,0.697893,132
09f882c6-2389-4b9d-a65e-550f9a4868ec,Encoding Concepts in Graph Neural Networks,17,0.0916829,0.559215,"The opaque reasoning of Graph Neural Networks induces a lack of human trust.
Existing graph network explainers attempt to address this issue by providing
post-hoc explanations, however, they fail to make the model itself more
interpretable. To fill this gap, we introduce the Concept Encoder Module, the
first differentiable concept-discovery approach for graph networks. The
proposed approach makes graph networks explainable by design by first
discovering graph concepts and then using these to solve the task. Our results
demonstrate that this approach allows graph networks to: (i) attain model
accuracy comparable with their equivalent vanilla versions, (ii) discover
meaningful concepts that achieve high concept completeness and purity scores,
(iii) provide high-quality concept-based logic explanations for their
prediction, and (iv) support effective interventions at test time: these can
increase human trust as well as significantly improve model performance.",0,0,0,0,0,0,0.767957,6.0,0.794928,47
0bc973f9-79cc-4f59-96ae-5e31e878bb3d,Adapters for Enhanced Modeling of Multilingual Knowledge and Text,5,0.151001,0.692198,"Large language models appear to learn facts from the large text corpora they
are trained on. Such facts are encoded implicitly within their many parameters,
making it difficult to verify or manipulate what knowledge has been learned.
Language models have recently been extended to multilingual language models
(MLLMs), enabling knowledge to be learned across hundreds of languages.
Meanwhile, knowledge graphs contain facts in an explicit triple format, which
require careful and costly curation and are only available in a few
high-resource languages, restricting their research and application. To address
these issues, we propose to enhance MLLMs with knowledge from multilingual
knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks
across many languages, including low-resource ones. Specifically, we introduce
a lightweight adapter set to enhance MLLMs with cross-lingual entity alignment
and facts from MLKGs for many languages. Experiments on common benchmarks show
that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable
or improved performance for knowledge graph completion and entity alignment
relative to baselines, especially for low-resource languages (for which
knowledge graphs are unavailable); and (2) improved MLLM performance on
language understanding tasks that require multilingual factual knowledge; all
while maintaining performance on other general language tasks.",1,1,0,0,0,0,0.882251,7.0,0.878713,36
3439d365-4bfb-47d7-908f-ead36413d531,Turning Stocks into Memes: A Dataset for Understanding How Social Communities Can Drive Wall Street,1,0.0256337,0.148272,"Who actually expresses an intent to buy GameStop shares on Reddit? What
convinces people to buy stocks? Are people convinced to support a coordinated
plan to adversely impact Wall Street investors? Existing literature on
understanding intent has mainly relied on surveys and self reporting; however
there are limitations to these methodologies. Hence, in this paper, we develop
an annotated dataset of communications centered on the GameStop phenomenon to
analyze the subscriber intentions behaviors within the r/WallStreetBets
community to buy (or not buy) stocks. Likewise, we curate a dataset to better
understand how intent interacts with a user's general support towards the
coordinated actions of the community for GameStop. Overall, our dataset can
provide insight to social scientists on the persuasive power to buy into social
movements online by adopting common language and narrative. WARNING: This paper
contains offensive language that commonly appears on Reddit's r/WallStreetBets
subreddit.",0,0,0,1,0,0,0.468337,6.0,0.655216,35
2abf45d1-2f1b-4b57-97ec-1aeae74ce57d,GaIA: Graphical Information Gain based Attention Network for Weakly Supervised Point Cloud Semantic Segmentation,12,0.182134,0.735116,"While point cloud semantic segmentation is a significant task in 3D scene
understanding, this task demands a time-consuming process of fully annotating
labels. To address this problem, recent studies adopt a weakly supervised
learning approach under the sparse annotation. Different from the existing
studies, this study aims to reduce the epistemic uncertainty measured by the
entropy for a precise semantic segmentation. We propose the graphical
information gain based attention network called GaIA, which alleviates the
entropy of each point based on the reliable information. The graphical
information gain discriminates the reliable point by employing relative entropy
between target point and its neighborhoods. We further introduce anchor-based
additive angular margin loss, ArcPoint. The ArcPoint optimizes the unlabeled
points containing high entropy towards semantically similar classes of the
labeled points on hypersphere space. Experimental results on S3DIS and
ScanNet-v2 datasets demonstrate our framework outperforms the existing weakly
supervised methods. We have released GaIA at https://github.com/Karel911/GaIA.",1,1,0,0,1,0,0.786798,8.0,0.85324,50
7c0a6b6f-346b-4336-900a-f417bef3649c,Analogical Math Word Problems Solving with Enhanced Problem-Solution Association,7,0.358383,0.250567,"Math word problem (MWP) solving is an important task in question answering
which requires human-like reasoning ability. Analogical reasoning has long been
used in mathematical education, as it enables students to apply common
relational structures of mathematical situations to solve new problems. In this
paper, we propose to build a novel MWP solver by leveraging analogical MWPs,
which advance the solver's generalization ability across different kinds of
MWPs. The key idea, named analogy identification, is to associate the
analogical MWP pairs in a latent space, i.e., encoding an MWP close to another
analogical MWP, while moving away from the non-analogical ones. Moreover, a
solution discriminator is integrated into the MWP solver to enhance the
association between the representations of MWPs and their true solutions. The
evaluation results verify that our proposed analogical learning strategy
promotes the performance of MWP-BERT on Math23k over the state-of-the-art model
Generate2Rank, with 5 times fewer parameters in the encoder. We also find that
our model has a stronger generalization ability in solving difficult MWPs due
to the analogical learning from easy MWPs.",1,0,0,0,1,1,0.950294,5.0,0.897943,51
c6d04389-c8c5-4cfb-8cd5-72825cef2eea,Taxonomy Enrichment with Text and Graph Vector Representations,8,0.0246562,0.263362,"Knowledge graphs such as DBpedia, Freebase or Wikidata always contain a
taxonomic backbone that allows the arrangement and structuring of various
concepts in accordance with the hypo-hypernym (""class-subclass"") relationship.
With the rapid growth of lexical resources for specific domains, the problem of
automatic extension of the existing knowledge bases with new words is becoming
more and more widespread. In this paper, we address the problem of taxonomy
enrichment which aims at adding new words to the existing taxonomy.
  We present a new method that allows achieving high results on this task with
little effort. It uses the resources which exist for the majority of languages,
making the method universal. We extend our method by incorporating deep
representations of graph structures like node2vec, Poincar\'e embeddings, GCN
etc. that have recently demonstrated promising results on various NLP tasks.
Furthermore, combining these representations with word embeddings allows us to
beat the state of the art.
  We conduct a comprehensive study of the existing approaches to taxonomy
enrichment based on word and graph vector representations and their fusion
approaches. We also explore the ways of using deep learning architectures to
extend the taxonomic backbones of knowledge graphs. We create a number of
datasets for taxonomy extension for English and Russian. We achieve
state-of-the-art results across different datasets and provide an in-depth
error analysis of mistakes.",0,1,0,1,1,0,0.0263589,10.0,0.476794,125
aff5c6c7-d081-4e66-9bba-0796bd1106e1,Large region targets observation scheduling by multiple satellites using resampling particle swarm optimization,8,0.196421,0.831394,"The last decades have witnessed a rapid increase of Earth observation
satellites (EOSs), leading to the increasing complexity of EOSs scheduling. On
account of the widespread applications of large region observation, this paper
aims to address the EOSs observation scheduling problem for large region
targets. A rapid coverage calculation method employing a projection reference
plane and a polygon clipping technique is first developed. We then formulate a
nonlinear integer programming model for the scheduling problem, where the
objective function is calculated based on the developed coverage calculation
method. A greedy initialization-based resampling particle swarm optimization
(GI-RPSO) algorithm is proposed to solve the model. The adopted greedy
initialization strategy and particle resampling method contribute to generating
efficient and effective solutions during the evolution process. In the end,
extensive experiments are conducted to illustrate the effectiveness and
reliability of the proposed method. Compared to the traditional particle swarm
optimization and the widely used greedy algorithm, the proposed GI-RPSO can
improve the scheduling result by 5.42% and 15.86%, respectively.",0,1,0,0,1,0,0.0784296,9.0,0.542841,47
9b0c68e7-6a03-44ee-acd6-b86d4a0000f6,Unsupervised Neural Stylistic Text Generation using Transfer learning and Adapters,1,0.0113139,0.0715592,"Research has shown that personality is a key driver to improve engagement and
user experience in conversational systems. Conversational agents should also
maintain a consistent persona to have an engaging conversation with a user.
However, text generation datasets are often crowd sourced and thereby have an
averaging effect where the style of the generation model is an average style of
all the crowd workers that have contributed to the dataset. While one can
collect persona-specific datasets for each task, it would be an expensive and
time consuming annotation effort. In this work, we propose a novel transfer
learning framework which updates only $0.3\%$ of model parameters to learn
style specific attributes for response generation. For the purpose of this
study, we tackle the problem of stylistic story ending generation using the ROC
stories Corpus. We learn style specific attributes from the
PERSONALITY-CAPTIONS dataset. Through extensive experiments and evaluation
metrics we show that our novel training procedure can improve the style
generation by 200 over Encoder-Decoder baselines while maintaining on-par
content relevance metrics with",0,1,0,0,1,1,0.259645,10.0,0.718867,43
88e3b38e-14d3-4ff4-bab8-5a5ac9b2e8b6,Kratt: Developing an Automatic Subject Indexing Tool for The National Library of Estonia,3,0.102619,0.303119,"Manual subject indexing in libraries is a time-consuming and costly process
and the quality of the assigned subjects is affected by the cataloguer's
knowledge on the specific topics contained in the book. Trying to solve these
issues, we exploited the opportunities arising from artificial intelligence to
develop Kratt: a prototype of an automatic subject indexing tool. Kratt is able
to subject index a book independent of its extent and genre with a set of
keywords present in the Estonian Subject Thesaurus. It takes Kratt
approximately 1 minute to subject index a book, outperforming humans 10-15
times. Although the resulting keywords were not considered satisfactory by the
cataloguers, the ratings of a small sample of regular library users showed more
promise. We also argue that the results can be enhanced by including a bigger
corpus for training the model and applying more careful preprocessing
techniques.",0,1,0,0,0,0,2.15464e-05,16.0,0.227829,25
5fc2bebe-db90-4b40-a52b-dc4507e3adcc,Evolved Open-Endedness in Cultural Evolution: A New Dimension in Open-Ended Evolution Research,5,0.0412655,0.467344,"The goal of Artificial Life research, as articulated by Chris Langton, is ""to
contribute to theoretical biology by locating life-as-we-know-it within the
larger picture of life-as-it-could-be"" (1989, p.1). The study and pursuit of
open-ended evolution in artificial evolutionary systems exemplifies this goal.
However, open-ended evolution research is hampered by two fundamental issues;
the struggle to replicate open-endedness in an artificial evolutionary system,
and the fact that we only have one system (genetic evolution) from which to
draw inspiration. Here we argue that cultural evolution should be seen not only
as another real-world example of an open-ended evolutionary system, but that
the unique qualities seen in cultural evolution provide us with a new
perspective from which we can assess the fundamental properties of, and ask new
questions about, open-ended evolutionary systems, especially in regard to
evolved open-endedness and transitions from bounded to unbounded evolution.
Here we provide an overview of culture as an evolutionary system, highlight the
interesting case of human cultural evolution as an open-ended evolutionary
system, and contextualise cultural evolution under the framework of (evolved)
open-ended evolution. We go on to provide a set of new questions that can be
asked once we consider cultural evolution within the framework of open-ended
evolution, and introduce new insights that we may be able to gain about evolved
open-endedness as a result of asking these questions.",0,0,0,0,0,0,0.00800009,15.0,0.571085,117
a482421f-ac67-490f-9578-fb8cbe8e0162,The Phenomenon of Policy Churn,17,0.363964,0.480338,"We identify and study the phenomenon of policy churn, that is, the rapid
change of the greedy policy in value-based reinforcement learning. Policy churn
operates at a surprisingly rapid pace, changing the greedy action in a large
fraction of states within a handful of learning updates (in a typical deep RL
set-up such as DQN on Atari). We characterise the phenomenon empirically,
verifying that it is not limited to specific algorithm or environment
properties. A number of ablations help whittle down the plausible explanations
on why churn occurs to just a handful, all related to deep learning. Finally,
we hypothesise that policy churn is a beneficial but overlooked form of
implicit exploration that casts $\epsilon$-greedy exploration in a fresh light,
namely that $\epsilon$-noise plays a much smaller role than expected.",1,0,0,0,0,0,0.667165,10.0,0.848597,57
8703decd-0080-465a-8bd4-c98a0e1c22c4,Depth Field Networks for Generalizable Multi-view Scene Representation,10,0.169314,0.441336,"Modern 3D computer vision leverages learning to boost geometric reasoning,
mapping image data to classical structures such as cost volumes or epipolar
constraints to improve matching. These architectures are specialized according
to the particular problem, and thus require significant task-specific tuning,
often leading to poor domain generalization performance. Recently, generalist
Transformer architectures have achieved impressive results in tasks such as
optical flow and depth estimation by encoding geometric priors as inputs rather
than as enforced constraints. In this paper, we extend this idea and propose to
learn an implicit, multi-view consistent scene representation, introducing a
series of 3D data augmentation techniques as a geometric inductive prior to
increase view diversity. We also show that introducing view synthesis as an
auxiliary task further improves depth estimation. Our Depth Field Networks
(DeFiNe) achieve state-of-the-art results in stereo and video depth estimation
without explicit geometric constraints, and improve on zero-shot domain
generalization by a wide margin.",1,0,0,0,1,0,0.961189,5.0,0.913785,61
ae6da2a9-f7cf-4b3f-8fa7-d02ef589be02,NaturalProver: Grounded Mathematical Proof Generation with Language Models,40,0.627666,0.997389,"Theorem proving in natural mathematical language - the mixture of symbolic
and natural language used by humans - plays a central role in mathematical
advances and education, and tests aspects of reasoning that are core to
intelligence. Yet it has remained underexplored with modern generative models.
We study large-scale language models on two new generation tasks: suggesting
the next step in a mathematical proof, and full proof generation. We develop
NaturalProver, a language model that generates proofs by conditioning on
background references (e.g. theorems and definitions that are either retrieved
or human-provided), and optionally enforces their presence with constrained
decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves
the quality of next-step suggestions and generated proofs over fine-tuned
GPT-3, according to human evaluations from university-level mathematics
students. NaturalProver is capable of proving some theorems that require short
(2-6 step) proofs, and providing next-step suggestions that are rated as
correct and useful over 40% of the time, which is to our knowledge the first
demonstration of these capabilities using neural language models.",0,0,0,0,0,0,0.942017,4.0,0.859259,57
1e58b115-ec0d-4ecd-b4e6-63cb62aabcfb,A Benchmark Generator for Combinatorial Testing,1,0.0,0.0642775,"Combinatorial Testing (CT) tools are essential to test properly a wide range
of systems (train systems, Graphical User Interfaces (GUIs), autonomous driving
systems, etc). While there is an active research community working on
developing CT tools, paradoxically little attention has been paid to making
available enough resources to test the CT tools themselves. In particular, the
set of available benchmarks to asses their correctness, effectiveness and
efficiency is rather limited. In this paper, we introduce a new generator of CT
benchmarks that essentially borrows the structure contained in the plethora of
available Combinatorial Problems from other research communities in order to
create meaningful benchmarks. We additionally perform an extensive evaluation
of CT tools with these new benchmarks. Thanks to this study we provide some
insights on under which circumstances a particular CT tool should be used.",0,1,0,1,0,0,3.79855e-05,22.0,0.464194,37
8bfcc3b1-9625-4745-adc7-9fb98f982d0b,Using Linguistic Typology to Enrich Multilingual Lexicons: the Case of Lexical Gaps in Kinship,6,0.0383945,0.536722,"This paper describes a method to enrich lexical resources with content
relating to linguistic diversity, based on knowledge from the field of lexical
typology. We capture the phenomenon of diversity through the notions of lexical
gap and language-specific word and use a systematic method to infer gaps
semi-automatically on a large scale. As a first result obtained for the domain
of kinship terminology, known to be very diverse throughout the world, we
publish a lexico-semantic resource consisting of 198 domain concepts, 1,911
words, and 37,370 gaps covering 699 languages. We see potential in the use of
resources such as ours for the improvement of a variety of cross-lingual NLP
tasks, which we demonstrate through a downstream application for the evaluation
of machine translation systems.",1,0,1,1,0,0,0.00248028,12.0,0.366035,57
e39709c6-1808-4676-8e3a-cba64d43cbc6,DPFNet: A Dual-branch Dilated Network with Phase-aware Fourier Convolution for Low-light Image Enhancement,3,0.083666,0.242925,"Low-light image enhancement is a classical computer vision problem aiming to
recover normal-exposure images from low-light images. However, convolutional
neural networks commonly used in this field are good at sampling low-frequency
local structural features in the spatial domain, which leads to unclear texture
details of the reconstructed images. To alleviate this problem, we propose a
novel module using the Fourier coefficients, which can recover high-quality
texture details under the constraint of semantics in the frequency phase and
supplement the spatial domain. In addition, we design a simple and efficient
module for the image spatial domain using dilated convolutions with different
receptive fields to alleviate the loss of detail caused by frequent
downsampling. We integrate the above parts into an end-to-end dual branch
network and design a novel loss committee and an adaptive fusion module to
guide the network to flexibly combine spatial and frequency domain features to
generate more pleasing visual effects. Finally, we evaluate the proposed
network on public benchmarks. Extensive experimental results show that our
method outperforms many existing state-of-the-art ones, showing outstanding
performance and potential.",1,1,0,0,1,0,0.96111,6.0,0.92805,37
f5b1f20e-deda-4c5c-9135-1528ef77c818,MMMNA-Net for Overall Survival Time Prediction of Brain Tumor Patients,5,0.0586286,0.236337,"Overall survival (OS) time is one of the most important evaluation indices
for gliomas situations. Multimodal Magnetic Resonance Imaging (MRI) scans play
an important role in the study of glioma prognosis OS time. Several deep
learning-based methods are proposed for the OS time prediction on multi-modal
MRI problems. However, these methods usually fuse multi-modal information at
the beginning or at the end of the deep learning networks and lack the fusion
of features from different scales. In addition, the fusion at the end of
networks always adapts global with global (eg. fully connected after
concatenation of global average pooling output) or local with local (eg.
bilinear pooling), which loses the information of local with global. In this
paper, we propose a novel method for multi-modal OS time prediction of brain
tumor patients, which contains an improved nonlocal features fusion module
introduced on different scales. Our method obtains a relative 8.76% improvement
over the current state-of-art method (0.6989 vs. 0.6426 on accuracy). Extensive
testing demonstrates that our method could adapt to situations with missing
modalities. The code is available at
https://github.com/TangWen920812/mmmna-net.",1,1,0,0,1,0,0.949496,10.0,0.94844,13
acf1d438-6187-4fb3-a33f-d2b9ded560f8,Bi-directional Joint Neural Networks for Intent Classification and Slot Filling,10,0.232865,0.6883,"Intent classification and slot filling are two critical tasks for natural
language understanding. Traditionally the two tasks proceeded independently.
However, more recently joint models for intent classification and slot filling
have achieved state-of-the-art performance, and have proved that there exists a
strong relationship between the two tasks. In this paper, we propose a
bi-directional joint model for intent classification and slot filling, which
includes a multi-stage hierarchical process via BERT and bi-directional joint
natural language understanding mechanisms, including intent2slot and
slot2intent, to obtain mutual performance enhancement between intent
classification and slot filling. The evaluations show that our model achieves
state-of-the-art results on intent classification accuracy, slot filling F1,
and significantly improves sentence-level semantic frame accuracy when applied
to publicly available benchmark datasets, ATIS (88.6%) and SNIPS (92.8%).",0,1,0,0,1,0,0.894954,7.0,0.88614,14
276b3014-008e-42e4-b689-b1f3b1abdf9c,"Interventions, Where and How? Experimental Design for Causal Models at Scale",29,0.465474,0.738378,"Causal discovery from observational and interventional data is challenging
due to limited data and non-identifiability: factors that introduce uncertainty
in estimating the underlying structural causal model (SCM). Selecting
experiments (interventions) based on the uncertainty arising from both factors
can expedite the identification of the SCM. Existing methods in experimental
design for causal discovery from limited data either rely on linear assumptions
for the SCM or select only the intervention target. This work incorporates
recent advances in Bayesian causal discovery into the Bayesian optimal
experimental design framework, allowing for active causal discovery of large,
nonlinear SCMs while selecting both the interventional target and the value. We
demonstrate the performance of the proposed method on synthetic graphs
(Erdos-R\`enyi, Scale Free) for both linear and nonlinear SCMs as well as on
the \emph{in-silico} single-cell gene regulatory network dataset, DREAM.",1,0,0,0,0,0,0.505674,9.0,0.782265,69
3f50e6c2-18f3-405e-9a80-592edd6e072d,Unsupervised Change Detection Based on Image Reconstruction Loss,19,0.346895,0.683269,"To train the change detector, bi-temporal images taken at different times in
the same area are used. However, collecting labeled bi-temporal images is
expensive and time consuming. To solve this problem, various unsupervised
change detection methods have been proposed, but they still require unlabeled
bi-temporal images. In this paper, we propose unsupervised change detection
based on image reconstruction loss using only unlabeled single temporal single
image. The image reconstruction model is trained to reconstruct the original
source image by receiving the source image and the photometrically transformed
source image as a pair. During inference, the model receives bi-temporal images
as the input, and tries to reconstruct one of the inputs. The changed region
between bi-temporal images shows high reconstruction loss. Our change detector
showed significant performance in various change detection benchmark datasets
even though only a single temporal single source image was used. The code and
trained models will be publicly available for reproducibility.",0,1,0,0,1,0,0.90004,7.0,0.889253,34
20c901c6-649b-4ea1-89e6-1e07a2efbe2d,A Reinforcement Learning Approach for Electric Vehicle Routing Problem with Vehicle-to-Grid Supply,6,0.182611,0.346204,"The use of electric vehicles (EV) in the last mile is appealing from both
sustainability and operational cost perspectives. In addition to the inherent
cost efficiency of EVs, selling energy back to the grid during peak grid
demand, is a potential source of additional revenue to a fleet operator. To
achieve this, EVs have to be at specific locations (discharge points) during
specific points in time (peak period), even while meeting their core purpose of
delivering goods to customers. In this work, we consider the problem of EV
routing with constraints on loading capacity; time window; vehicle-to-grid
energy supply (CEVRPTW-D); which not only satisfy multiple system objectives,
but also scale efficiently to large problem sizes involving hundreds of
customers and discharge stations. We present QuikRouteFinder that uses
reinforcement learning (RL) for EV routing to overcome these challenges. Using
Solomon datasets, results from RL are compared against exact formulations based
on mixed-integer linear program (MILP) and genetic algorithm (GA)
metaheuristics. On an average, the results show that RL is 24 times faster than
MILP and GA, while being close in quality (within 20%) to the optimal.",0,1,0,0,0,0,0.461597,11.0,0.810105,19
cae98d09-5991-4769-8850-b16c911d449f,Dialogue Strategy Adaptation to New Action Sets Using Multi-dimensional Modelling,3,0.142616,0.316139,"A major bottleneck for building statistical spoken dialogue systems for new
domains and applications is the need for large amounts of training data. To
address this problem, we adopt the multi-dimensional approach to dialogue
management and evaluate its potential for transfer learning. Specifically, we
exploit pre-trained task-independent policies to speed up training for an
extended task-specific action set, in which the single summary action for
requesting a slot is replaced by multiple slot-specific request actions. Policy
optimisation and evaluation experiments using an agenda-based user simulator
show that with limited training data, much better performance levels can be
achieved when using the proposed multi-dimensional adaptation method. We
confirm this improvement in a crowd-sourced human user evaluation of our spoken
dialogue system, comparing partially trained policies. The multi-dimensional
system (with adaptation on limited training data in the target scenario)
outperforms the one-dimensional baseline (without adaptation on the same amount
of training data) by 7% perceived success rate.",0,1,0,0,0,0,0.207723,18.0,0.829621,22
8e3f86f2-6abe-4a90-9fc1-fb6304f74b38,Imitating Past Successes can be Very Suboptimal,15,0.12548,0.287571,"Prior work has proposed a simple strategy for reinforcement learning (RL):
label experience with the outcomes achieved in that experience, and then
imitate the relabeled experience. These outcome-conditioned imitation learning
methods are appealing because of their simplicity, strong performance, and
close ties with supervised learning. However, it remains unclear how these
methods relate to the standard RL objective, reward maximization. In this
paper, we formally relate outcome-conditioned imitation learning to reward
maximization, drawing a precise relationship between the learned policy and
Q-values and explaining the close connections between these methods and prior
EM-based policy search methods. This analysis shows that existing
outcome-conditioned imitation learning methods do not necessarily improve the
policy, but a simple modification results in a method that does guarantee
policy improvement, under some assumptions.",1,0,0,0,0,0,0.529506,7.0,0.729737,44
586edef8-718b-449e-acf3-7752810a8f4c,Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models,21,0.427013,0.991265,"Recent open-domain dialogue models have brought numerous breakthroughs.
However, building a chat system is not scalable since it often requires a
considerable volume of human-human dialogue data, especially when enforcing
features such as persona, style, or safety. In this work, we study the
challenge of imposing roles on open-domain dialogue systems, with the goal of
making the systems maintain consistent roles while conversing naturally with
humans. To accomplish this, the system must satisfy a role specification that
includes certain conditions on the stated features as well as a system policy
on whether or not certain types of utterances are allowed. For this, we propose
an efficient data collection framework leveraging in-context few-shot learning
of large-scale language models for building role-satisfying dialogue dataset
from scratch. We then compare various architectures for open-domain dialogue
systems in terms of meeting role specifications while maintaining
conversational abilities. Automatic and human evaluations show that our models
return few out-of-bounds utterances, keeping competitive performance on general
metrics. We release a Korean dialogue dataset we built for further research.",0,1,0,1,0,0,0.886289,6.0,0.861196,66
5bdf465f-27c0-497a-9400-8e0a5d7e280c,Box2Seg: Learning Semantics of 3D Point Clouds with Box-Level Supervision,9,0.138417,0.46582,"Learning dense point-wise semantics from unstructured 3D point clouds with
fewer labels, although a realistic problem, has been under-explored in
literature. While existing weakly supervised methods can effectively learn
semantics with only a small fraction of point-level annotations, we find that
the vanilla bounding box-level annotation is also informative for semantic
segmentation of large-scale 3D point clouds. In this paper, we introduce a
neural architecture, termed Box2Seg, to learn point-level semantics of 3D point
clouds with bounding box-level supervision. The key to our approach is to
generate accurate pseudo labels by exploring the geometric and topological
structure inside and outside each bounding box. Specifically, an
attention-based self-training (AST) technique and Point Class Activation
Mapping (PCAM) are utilized to estimate pseudo-labels. The network is further
trained and refined with pseudo labels. Experiments on two large-scale
benchmarks including S3DIS and ScanNet demonstrate the competitive performance
of the proposed method. In particular, the proposed network can be trained with
cheap, or even off-the-shelf bounding box-level annotations and subcloud-level
tags.",0,1,0,0,0,0,0.935525,7.0,0.914151,63
7607802b-7788-43e9-aa3f-32912ad45145,Automatic Recognition and Classification of Future Work Sentences from Academic Articles in a Specific Domain,4,0.183299,0.182514,"Future work sentences (FWS) are the particular sentences in academic papers
that contain the author's description of their proposed follow-up research
direction. This paper presents methods to automatically extract FWS from
academic papers and classify them according to the different future directions
embodied in the paper's content. FWS recognition methods will enable subsequent
researchers to locate future work sentences more accurately and quickly and
reduce the time and cost of acquiring the corpus. The current work on automatic
identification of future work sentences is relatively small, and the existing
research cannot accurately identify FWS from academic papers, and thus cannot
conduct data mining on a large scale. Furthermore, there are many aspects to
the content of future work, and the subdivision of the content is conducive to
the analysis of specific development directions. In this paper, Nature Language
Processing (NLP) is used as a case study, and FWS are extracted from academic
papers and classified into different types. We manually build an annotated
corpus with six different types of FWS. Then, automatic recognition and
classification of FWS are implemented using machine learning models, and the
performance of these models is compared based on the evaluation metrics. The
results show that the Bernoulli Bayesian model has the best performance in the
automatic recognition task, with the Macro F1 reaching 90.73%, and the SCIBERT
model has the best performance in the automatic classification task, with the
weighted average F1 reaching 72.63%. Finally, we extract keywords from FWS and
gain a deep understanding of the key content described in FWS, and we also
demonstrate that content determination in FWS will be reflected in the
subsequent research work by measuring the similarity between future work
sentences and the abstracts.",1,1,0,0,0,0,0.0668716,17.0,0.748235,42
3f64994f-99f0-449a-b078-5e3818534b78,NAN: Noise-Aware NeRFs for Burst-Denoising,39,0.117735,0.653369,"Burst denoising is now more relevant than ever, as computational photography
helps overcome sensitivity issues inherent in mobile phones and small cameras.
A major challenge in burst-denoising is in coping with pixel misalignment,
which was so far handled with rather simplistic assumptions of simple motion,
or the ability to align in pre-processing. Such assumptions are not realistic
in the presence of large motion and high levels of noise. We show that Neural
Radiance Fields (NeRFs), originally suggested for physics-based novel-view
rendering, can serve as a powerful framework for burst denoising. NeRFs have an
inherent capability of handling noise as they integrate information from
multiple images, but they are limited in doing so, mainly since they build on
pixel-wise operations which are suitable to ideal imaging conditions. Our
approach, termed NAN, leverages inter-view and spatial information in NeRFs to
better deal with noise. It achieves state-of-the-art results in burst denoising
and is especially successful in coping with large movement and occlusions,
under very high levels of noise. With the rapid advances in accelerating NeRFs,
it could provide a powerful platform for denoising in challenging environments.",1,0,0,0,1,0,0.758018,4.0,0.68511,34
9a768e34-75b3-43e0-b2f4-000df01a1c26,Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions,63,0.260125,0.915768,"Training a high-dimensional simulated agent with an under-specified reward
function often leads the agent to learn physically infeasible strategies that
are ineffective when deployed in the real world. To mitigate these unnatural
behaviors, reinforcement learning practitioners often utilize complex reward
functions that encourage physically plausible behaviors. However, a tedious
labor-intensive tuning process is often required to create hand-designed
rewards which might not easily generalize across platforms and tasks. We
propose substituting complex reward functions with ""style rewards"" learned from
a dataset of motion capture demonstrations. A learned style reward can be
combined with an arbitrary task reward to train policies that perform tasks
using naturalistic strategies. These natural strategies can also facilitate
transfer to the real world. We build upon Adversarial Motion Priors -- an
approach from the computer graphics domain that encodes a style reward from a
dataset of reference motions -- to demonstrate that an adversarial approach to
training policies can produce behaviors that transfer to a real quadrupedal
robot without requiring complex reward functions. We also demonstrate that an
effective style reward can be learned from a few seconds of motion capture data
gathered from a German Shepherd and leads to energy-efficient locomotion
strategies with natural gait transitions.",1,1,0,0,0,0,0.0826733,10.0,0.594054,57
66d1d767-a167-4df2-8485-8905be2da3c0,Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting,17,0.595972,0.802616,"In this paper, we propose a novel end-to-end user-defined keyword spotting
method that utilizes linguistically corresponding patterns between speech and
text sequences. Unlike previous approaches requiring speech keyword enrollment,
our method compares input queries with an enrolled text keyword sequence. To
place the audio and text representations within a common latent space, we adopt
an attention-based cross-modal matching approach that is trained in an
end-to-end manner with monotonic matching loss and keyword classification loss.
We also utilize a de-noising loss for the acoustic embedding network to improve
robustness in noisy environments. Additionally, we introduce the LibriPhrase
dataset, a new short-phrase dataset based on LibriSpeech for efficiently
training keyword spotting models. Our proposed method achieves competitive
results on various evaluation sets compared to other single-modal and
cross-modal baselines.",0,1,0,1,0,0,0.726175,8.0,0.83116,24
e64d2a2a-eccf-4205-903d-8b1624c7d971,Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning,5,0.0976814,0.0749317,"Intelligent virtual assistants are currently designed to perform tasks or
services explicitly mentioned by users, so multiple related domains or tasks
need to be performed one by one through a long conversation with many explicit
intents. Instead, human assistants are capable of reasoning (multiple) implicit
intents based on user utterances via commonsense knowledge, reducing complex
interactions and improving practicality. Therefore, this paper proposes a
framework of multi-domain dialogue systems, which can automatically infer
implicit intents based on user utterances and then perform zero-shot prompting
using a large pre-trained language model to trigger suitable single
task-oriented bots. The proposed framework is demonstrated effective to realize
implicit intents and recommend associated bots in a zero-shot manner.",1,1,0,0,0,0,0.779248,6.0,0.800524,17
05d8a73d-4867-400c-9e78-4d5c10cc46a1,Bipartite-play Dialogue Collection for Practical Automatic Evaluation of Dialogue Systems,2,0.0,0.120327,"Automation of dialogue system evaluation is a driving force for the efficient
development of dialogue systems. This paper introduces the bipartite-play
method, a dialogue collection method for automating dialogue system evaluation.
It addresses the limitations of existing dialogue collection methods: (i)
inability to compare with systems that are not publicly available, and (ii)
vulnerability to cheating by intentionally selecting systems to be compared.
Experimental results show that the automatic evaluation using the
bipartite-play method mitigates these two drawbacks and correlates as strongly
with human subjectivity as existing methods.",0,1,0,0,0,0,0.68151,7.0,0.789319,55
d26f83ac-ab6f-413c-b978-694d0cf00a18,Sar Ship Detection based on Swin Transformer and Feature Enhancement Feature Pyramid Network,8,0.310344,0.784527,"With the booming of Convolutional Neural Networks (CNNs), CNNs such as VGG-16
and ResNet-50 widely serve as backbone in SAR ship detection. However, CNN
based backbone is hard to model long-range dependencies, and causes the lack of
enough high-quality semantic information in feature maps of shallow layers,
which leads to poor detection performance in complicated background and
small-sized ships cases. To address these problems, we propose a SAR ship
detection method based on Swin Transformer and Feature Enhancement Feature
Pyramid Network (FEFPN). Swin Transformer serves as backbone to model
long-range dependencies and generates hierarchical features maps. FEFPN is
proposed to further improve the quality of feature maps by gradually enhancing
the semantic information of feature maps at all levels, especially feature maps
in shallow layers. Experiments conducted on SAR ship detection dataset (SSDD)
reveal the advantage of our proposed methods.",0,1,0,0,0,1,0.992859,9.0,0.9987,14
862f2196-158c-4e3d-8eb4-d592c8f73414,Maximum Likelihood Uncertainty Estimation: Robustness to Outliers,4,0.0404765,0.18043,"We benchmark the robustness of maximum likelihood based uncertainty
estimation methods to outliers in training data for regression tasks. Outliers
or noisy labels in training data results in degraded performances as well as
incorrect estimation of uncertainty. We propose the use of a heavy-tailed
distribution (Laplace distribution) to improve the robustness to outliers. This
property is evaluated using standard regression benchmarks and on a
high-dimensional regression task of monocular depth estimation, both containing
outliers. In particular, heavy-tailed distribution based maximum likelihood
provides better uncertainty estimates, better separation in uncertainty for
out-of-distribution data, as well as better detection of adversarial attacks in
the presence of outliers.",1,1,0,0,0,0,0.565544,9.0,0.800959,45
0d0af82f-3cf7-42eb-83c2-a1fc957b2d3a,Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task,5,0.0214073,0.617908,"The adoption of pre-trained language models in task-oriented dialogue systems
has resulted in significant enhancements of their text generation abilities.
However, these architectures are slow to use because of the large number of
trainable parameters and can sometimes fail to generate diverse responses. To
address these limitations, we propose two models with auxiliary tasks for
response selection - (1) distinguishing distractors from ground truth responses
and (2) distinguishing synthetic responses from ground truth labels. They
achieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined
scores of 107.5 and 108.3 and outperform a baseline with three times more
parameters. We publish reproducible code and checkpoints and discuss the
effects of applying auxiliary tasks to T5-based architectures.",1,1,0,0,1,0,0.47046,6.0,0.656268,31
990cb83b-3d0d-4a6a-a1b5-004733ea81c4,Realistic Defocus Blur for Multiplane Computer-Generated Holography,10,0.566882,0.778965,"This paper introduces a new multiplane CGH computation method to reconstruct
artefact-free high-quality holograms with natural-looking defocus blur. Our
method introduces a new targeting scheme and a new loss function. While the
targeting scheme accounts for defocused parts of the scene at each depth plane,
the new loss function analyzes focused and defocused parts separately in
reconstructed images. Our method support phase-only CGH calculations using
various iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative
(e.g., Double Phase) CGH techniques. We achieve our best image quality using a
modified gradient descent-based optimization recipe where we introduce a
constraint inspired by the double phase method. We validate our method
experimentally using our proof-of-concept holographic display, comparing
various algorithms, including multi-depth scenes with sparse and dense
contents.",0,1,0,0,0,0,0.915532,7.0,0.899334,48
75a5daf3-8c29-4f99-9594-6dbc2991df14,SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning,24,0.20735,0.880727,"We present the SUPERB challenge at SLT 2022, which aims at learning
self-supervised speech representation for better performance, generalization,
and efficiency. The challenge builds upon the SUPERB benchmark and implements
metrics to measure the computation requirements of self-supervised learning
(SSL) representation and to evaluate its generalizability and performance
across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive
coverage of popular speech processing tasks, from speech and speaker
recognition to audio generation and semantic understanding. As SSL has gained
interest in the speech community and showed promising outcomes, we envision the
challenge to uplevel the impact of SSL techniques by motivating more practical
designs of techniques beyond task performance. We summarize the results of 14
submitted models in this paper. We also discuss the main findings from those
submissions and the future directions of SSL research.",1,1,1,0,0,0,0.514201,5.0,0.612955,61
05a5fd7b-2e2d-4f8c-b2d0-08b809ede502,Self-Supervised Visual Place Recognition by Mining Temporal and Feature Neighborhoods,5,0.0689686,0.42838,"Visual place recognition (VPR) using deep networks has achieved
state-of-the-art performance. However, most of them require a training set with
ground truth sensor poses to obtain positive and negative samples of each
observation's spatial neighborhood for supervised learning. When such
information is unavailable, temporal neighborhoods from a sequentially
collected data stream could be exploited for self-supervised training, although
we find its performance suboptimal. Inspired by noisy label learning, we
propose a novel self-supervised framework named \textit{TF-VPR} that uses
temporal neighborhoods and learnable feature neighborhoods to discover unknown
spatial neighborhoods. Our method follows an iterative training paradigm which
alternates between: (1) representation learning with data augmentation, (2)
positive set expansion to include the current feature space neighbors, and (3)
positive set contraction via geometric verification. We conduct comprehensive
experiments on both simulated and real datasets, with either RGB images or
point clouds as inputs. The results show that our method outperforms our
baselines in recall rate, robustness, and heading diversity, a novel metric we
propose for VPR. Our code and datasets can be found at
https://ai4ce.github.io/TF-VPR/.",1,1,0,1,0,0,0.687515,11.0,0.86743,60
8ebd81c8-b8ef-4580-98e0-65eb621cb525,Quantification of emotions in decision making,6,0.2321,0.249611,"The problem of quantification of emotions in the choice between alternatives
is considered. The alternatives are evaluated in a dual manner. From one side,
they are characterized by rational features defining the utility of each
alternative. From the other side, the choice is affected by emotions labeling
the alternatives as attractive or repulsive, pleasant or unpleasant. A decision
maker needs to make a choice taking into account both these features, the
utility of alternatives and their attractiveness. The notion of utility is
based on rational grounds, while the notion of attractiveness is vague and
rather is based on irrational feelings. A general method, allowing for the
quantification of the choice combining rational and emotional features is
described. Despite that emotions seem to avoid precise quantification, their
quantitative evaluation is possible at the aggregate level. The analysis of a
series of empirical data demonstrates the efficiency of the approach, including
the realistic behavioral problems that cannot be treated by the standard
expected utility theory.",0,0,0,0,0,0,0.027883,22.0,0.764769,59
2181a143-f11f-4080-8cbe-a245aa36abbf,Over-the-Air Computation over Balanced Numerals,7,0.0138647,0.42164,"In this study, a digital over-the-air computation (OAC) scheme for achieving
continuous-valued gradient aggregation is proposed. It is shown that the
average of a set of real-valued parameters can be calculated approximately by
using the average of the corresponding numerals, where the numerals are
obtained based on a balanced number system. By using this property, the
proposed scheme encodes the local gradients into a set of numerals. It then
determines the positions of the activated orthogonal frequency division
multiplexing (OFDM) subcarriers by using the values of the numerals. To
eliminate the need for a precise sample-level time synchronization, channel
estimation overhead, and power instabilities due to the channel inversion, the
proposed scheme also uses a non-coherent receiver at the edge server (ES) and
does not utilize a pre-equalization at the edge devices (EDs). Finally, the
theoretical mean squared error (MSE) performance of the proposed scheme is
derived and its performance for federated edge learning (FEEL) is demonstrated.",0,0,0,0,0,0,0.0830588,7.0,0.420771,21
0e8d767d-fa46-4ed7-a49d-1827bf16e5e9,Better Quality Estimation for Low Resource Corpus Mining,7,0.0302265,0.392217,"Quality Estimation (QE) models have the potential to change how we evaluate
and maybe even train machine translation models. However, these models still
lack the robustness to achieve general adoption. We show that State-of-the-art
QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform
unexpectedly bad due to a lack of robustness to out-of-domain examples. We
propose a combination of multitask training, data augmentation and contrastive
learning to achieve better and more robust QE performance. We show that our
method improves QE performance significantly in the MLQE challenge and the
robustness of QE models when tested in the Parallel Corpus Mining setup. We
increase the accuracy in PCM by more than 0.80, making it on par with
state-of-the-art PCM methods that use millions of sentence pairs to train their
models. In comparison, we use a thousand times less data, 7K parallel sentences
in total, and propose a novel low resource PCM method.",0,1,0,0,0,0,0.421071,6.0,0.63108,33
8d1672cd-a5be-4933-81c1-b373f547dc69,Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient Classification,1,0.0193421,0.104263,"Language Models pretrained on large textual data have been shown to encode
different types of knowledge simultaneously. Traditionally, only the features
from the last layer are used when adapting to new tasks or data. We put forward
that, when using or finetuning deep pretrained models, intermediate layer
features that may be relevant to the downstream task are buried too deep to be
used efficiently in terms of needed samples or steps. To test this, we propose
a new layer fusion method: Depth-Wise Attention (DWAtt), to help re-surface
signals from non-final layers. We compare DWAtt to a basic concatenation-based
layer fusion method (Concat), and compare both to a deeper model baseline --
all kept within a similar parameter budget. Our findings show that DWAtt and
Concat are more step- and sample-efficient than the baseline, especially in the
few-shot setting. DWAtt outperforms Concat on larger data sizes. On CoNLL-03
NER, layer fusion shows 3.68--9.73% F1 gain at different few-shot sizes. The
layer fusion models presented significantly outperform the baseline in various
training scenarios with different data sizes, architectures, and training
constraints.",1,1,0,0,0,0,0.52769,9.0,0.789227,38
d22a3bc2-d10e-435d-be8e-d49f4cadcc79,MHMS: Multimodal Hierarchical Multimedia Summarization,11,0.53199,0.67204,"Multimedia summarization with multimodal output can play an essential role in
real-world applications, i.e., automatically generating cover images and titles
for news articles or providing introductions to online videos. In this work, we
propose a multimodal hierarchical multimedia summarization (MHMS) framework by
interacting visual and language domains to generate both video and textual
summaries. Our MHMS method contains video and textual segmentation and
summarization module, respectively. It formulates a cross-domain alignment
objective with optimal transport distance which leverages cross-domain
interaction to generate the representative keyframe and textual summary. We
evaluated MHMS on three recent multimodal datasets and demonstrated the
effectiveness of our method in producing high-quality multimodal summaries.",0,1,0,0,0,0,0.93259,9.0,0.93141,108
00399e41-564a-4889-a32b-478e66e5de59,Enhancing Local Feature Learning for 3D Point Cloud Processing using Unary-Pairwise Attention,5,0.0484047,0.242699,"We present a simple but effective attention named the unary-pairwise
attention (UPA) for modeling the relationship between 3D point clouds. Our idea
is motivated by the analysis that the standard self-attention (SA) that
operates globally tends to produce almost the same attention maps for different
query positions, revealing difficulties for learning query-independent and
query-dependent information jointly. Therefore, we reformulate the SA and
propose query-independent (Unary) and query-dependent (Pairwise) components to
facilitate the learning of both terms. In contrast to the SA, the UPA ensures
query dependence via operating locally. Extensive experiments show that the UPA
outperforms the SA consistently on various point cloud understanding tasks
including shape classification, part segmentation, and scene segmentation.
Moreover, simply equipping the popular PointNet++ method with the UPA even
outperforms or is on par with the state-of-the-art attention-based approaches.
In addition, the UPA systematically boosts the performance of both standard and
modern networks when it is integrated into them as a compositional module.",0,1,0,0,0,0,0.97937,7.0,0.963808,43
64646df3-d228-4919-b8bb-9ba9800112f6,Rank-N-Contrast: Learning Continuous Representations for Regression,10,0.0425705,0.708021,"Deep regression models typically learn in an end-to-end fashion without
explicitly emphasizing a regression-aware representation. Consequently, the
learned representations exhibit fragmentation and fail to capture the
continuous nature of sample orders, inducing suboptimal results across a wide
range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a
framework that learns continuous representations for regression by contrasting
samples against each other based on their rankings in the target space. We
demonstrate, theoretically and empirically, that RNC guarantees the desired
order of learned representations in accordance with the target orders, enjoying
not only better performance but also significantly improved robustness,
efficiency, and generalization. Extensive experiments using five real-world
regression datasets that span computer vision, human-computer interaction, and
healthcare verify that RNC achieves state-of-the-art performance, highlighting
its intriguing properties including better data efficiency, robustness to
spurious targets and data corruptions, and generalization to distribution
shifts. Code is available at: https://github.com/kaiwenzha/Rank-N-Contrast.",1,0,0,0,1,1,0.162911,8.0,0.582951,52
5fadd2dd-3c74-4c8f-b0df-41cde61b2b96,A Unified View of Masked Image Modeling,22,0.399996,0.728224,"Masked image modeling has demonstrated great potential to eliminate the
label-hungry problem of training large-scale vision Transformers, achieving
impressive performance on various downstream tasks. In this work, we propose a
unified view of masked image modeling after revisiting existing methods. Under
the unified view, we introduce a simple yet effective method, termed as
MaskDistill, which reconstructs normalized semantic features from teacher
models at the masked positions, conditioning on corrupted input images.
Experimental results on image classification and semantic segmentation show
that MaskDistill achieves comparable or superior performance than
state-of-the-art methods. When using the huge vision Transformer and
pretraining 300 epochs, MaskDistill obtains 88.3% fine-tuning top-1 accuracy on
ImageNet-1k (224 size) and 58.8% semantic segmentation mIoU metric on ADE20k
(512 size). The code and pretrained models will be available at
https://aka.ms/unimim.",1,1,0,0,1,0,0.96805,4.0,0.90677,60
5be1c56b-f6e5-4c0d-9715-bec8f4981a62,End-to-End Speech to Intent Prediction to improve E-commerce Customer Support Voicebot in Hindi and English,1,0.156941,0.2387,"Automation of on-call customer support relies heavily on accurate and
efficient speech-to-intent (S2I) systems. Building such systems using
multi-component pipelines can pose various challenges because they require
large annotated datasets, have higher latency, and have complex deployment.
These pipelines are also prone to compounding errors. To overcome these
challenges, we discuss an end-to-end (E2E) S2I model for customer support
voicebot task in a bilingual setting. We show how we can solve E2E intent
classification by leveraging a pre-trained automatic speech recognition (ASR)
model with slight modification and fine-tuning on small annotated datasets.
Experimental results show that our best E2E model outperforms a conventional
pipeline by a relative ~27% on the F1 score.",0,1,0,0,1,0,0.585108,9.0,0.806937,31
3beffa19-61c5-4e35-ba68-8cb97026a57b,Continual Sequence Generation with Adaptive Compositional Modules,23,0.0757667,0.704817,"Continual learning is essential for real-world deployment when there is a
need to quickly adapt the model to new tasks without forgetting knowledge of
old tasks. Existing work on continual sequence generation either always reuses
existing parameters to learn new tasks, which is vulnerable to catastrophic
forgetting on dissimilar tasks, or blindly adds new parameters for every new
task, which could prevent knowledge sharing between similar tasks. To get the
best of both worlds, in this work, we propose continual sequence generation
with adaptive compositional modules to adaptively add modules in transformer
architectures and compose both old and new modules for new tasks. We also
incorporate pseudo experience replay to facilitate knowledge transfer in those
shared modules. Experiment results on various sequences of generation tasks
show that our framework can adaptively add modules or reuse modules based on
task similarity, outperforming state-of-the-art baselines in terms of both
performance and parameter efficiency. We make our code public at
https://github.com/GT-SALT/Adaptive-Compositional-Modules.",1,0,0,0,1,0,0.259613,7.0,0.598361,57
81717a08-bcd2-4d35-93be-7f2de408122f,Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior,7,0.0121391,0.365189,"Model visualizations provide information that outputs alone might miss. But
can we trust that model visualizations reflect model behavior? For instance,
can they diagnose abnormal behavior such as planted backdoors or
overregularization? To evaluate visualization methods, we test whether they
assign different visualizations to anomalously trained models and normal
models. We find that while existing methods can detect models with starkly
anomalous behavior, they struggle to identify more subtle anomalies. Moreover,
they often fail to recognize the inputs that induce anomalous behavior, e.g.
images containing a spurious cue. These results reveal blind spots and
limitations of some popular model visualizations. By introducing a novel
evaluation framework for visualizations, our work paves the way for developing
more reliable model transparency methods in the future.",1,0,0,0,0,0,0.401078,5.0,0.54447,43
65b2ec8b-027a-4524-87a4-d813f2e0cdfb,Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection,11,0.232184,0.673167,"Empathy, which is widely used in psychological counselling, is a key trait of
everyday human conversations. Equipped with commonsense knowledge, current
approaches to empathetic response generation focus on capturing implicit
emotion within dialogue context, where the emotions are treated as a static
variable throughout the conversations. However, emotions change dynamically
between utterances, which makes previous works difficult to perceive the
emotion flow and predict the correct emotion of the target response, leading to
inappropriate response. Furthermore, simply importing commonsense knowledge
without harmonization may trigger the conflicts between knowledge and emotion,
which confuse the model to choose incorrect information to guide the generation
process. To address the above problems, we propose a Serial Encoding and
Emotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.
We use a fine-grained encoding strategy which is more sensitive to the emotion
dynamics (emotion flow) in the conversations to predict the emotion-intent
characteristic of response. Besides, we design a novel framework to model the
interaction between knowledge and emotion to generate more sensible response.
Extensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms
the strong baselines in both automatic and manual evaluations.",1,0,1,0,0,0,0.761138,6.0,0.79159,36
87c60df3-a4ab-40f7-a8f3-7fb998f764e7,Human-to-Robot Imitation in the Wild,81,0.568392,0.877126,"We approach the problem of learning by watching humans in the wild. While
traditional approaches in Imitation and Reinforcement Learning are promising
for learning in the real world, they are either sample inefficient or are
constrained to lab settings. Meanwhile, there has been a lot of success in
processing passive, unstructured human data. We propose tackling this problem
via an efficient one-shot robot learning algorithm, centered around learning
from a third-person perspective. We call our method WHIRL: In-the-Wild Human
Imitating Robot Learning. WHIRL extracts a prior over the intent of the human
demonstrator, using it to initialize our agent's policy. We introduce an
efficient real-world policy learning scheme that improves using interactions.
Our key contributions are a simple sampling-based policy optimization approach,
a novel objective function for aligning human and robot videos as well as an
exploration method to boost sample efficiency. We show one-shot generalization
and success in real-world settings, including 20 different manipulation tasks
in the wild. Videos and talk at https://human2robot.github.io",1,1,0,0,0,0,0.632779,9.0,0.821373,87
97f81bd5-e905-4233-b453-adf9695e440c,Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks,52,0.380675,0.775642,"Recently, contrastive learning-based image translation methods have been
proposed, which contrasts different spatial locations to enhance the spatial
correspondence. However, the methods often ignore the diverse semantic relation
within the images. To address this, here we propose a novel semantic relation
consistency (SRC) regularization along with the decoupled contrastive learning,
which utilize the diverse semantics by focusing on the heterogeneous semantics
between the image patches of a single image. To further improve the
performance, we present a hard negative mining by exploiting the semantic
relation. We verified our method for three tasks: single-modal and multi-modal
image translations, and GAN compression task for image translation.
Experimental results confirmed the state-of-art performance of our method in
all the three tasks.",1,1,0,0,1,0,0.838803,6.0,0.832035,41
eecf876d-4445-43b1-9749-a361c2ebad83,"Fast, Accurate and Memory-Efficient Partial Permutation Synchronization",4,0.0722384,0.465452,"Previous partial permutation synchronization (PPS) algorithms, which are
commonly used for multi-object matching, often involve computation-intensive
and memory-demanding matrix operations. These operations become intractable for
large scale structure-from-motion datasets. For pure permutation
synchronization, the recent Cycle-Edge Message Passing (CEMP) framework
suggests a memory-efficient and fast solution. Here we overcome the restriction
of CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for
estimating the corruption levels of the observed partial permutations. It
allows us to subsequently implement a nonconvex weighted projected power method
without the need of spectral initialization. The resulting new PPS algorithm,
MatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse
matrix operations, and thus enjoys lower time and space complexities in
comparison to previous PPS algorithms. We prove that under adversarial
corruption, though without additive noise and with certain assumptions,
CEMP-Partial is able to exactly classify corrupted and clean partial
permutations. We demonstrate the state-of-the-art accuracy, speed and memory
efficiency of our method on both synthetic and real datasets.",0,1,0,0,1,0,0.0527792,14.0,0.676851,30
81a59dbe-4404-4bab-b2ab-7b34f4cc4d85,Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection,13,0.15652,0.544389,"Counterfactually Augmented Data (CAD) aims to improve out-of-domain
generalizability, an indicator of model robustness. The improvement is credited
with promoting core features of the construct over spurious artifacts that
happen to correlate with it. Yet, over-relying on core features may lead to
unintended model bias. Especially, construct-driven CAD -- perturbations of
core features -- may induce models to ignore the context in which core features
are used. Here, we test models for sexism and hate speech detection on
challenging data: non-hateful and non-sexist usage of identity and gendered
terms. In these hard cases, models trained on CAD, especially construct-driven
CAD, show higher false-positive rates than models trained on the original,
unperturbed data. Using a diverse set of CAD -- construct-driven and
construct-agnostic -- reduces such unintended bias.",1,1,0,0,0,0,0.763165,5.0,0.751095,37
995229c9-7a36-442a-816c-ccc053b6f01b,Learning Appearance-motion Normality for Video Anomaly Detection,29,0.456015,0.849839,"Video anomaly detection is a challenging task in the computer vision
community. Most single task-based methods do not consider the independence of
unique spatial and temporal patterns, while two-stream structures lack the
exploration of the correlations. In this paper, we propose spatial-temporal
memories augmented two-stream auto-encoder framework, which learns the
appearance normality and motion normality independently and explores the
correlations via adversarial learning. Specifically, we first design two proxy
tasks to train the two-stream structure to extract appearance and motion
features in isolation. Then, the prototypical features are recorded in the
corresponding spatial and temporal memory pools. Finally, the encoding-decoding
network performs adversarial learning with the discriminator to explore the
correlations between spatial and temporal patterns. Experimental results show
that our framework outperforms the state-of-the-art methods, achieving AUCs of
98.1% and 89.8% on UCSD Ped2 and CUHK Avenue datasets.",0,1,0,0,1,0,0.901293,10.0,0.923023,21
a0d3f16c-d6df-4ec4-8822-31a1fdbbd663,Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation,3,0.0219217,0.0693017,"How to learn highly compact yet effective sentence representation?
Pre-trained language models have been effective in many NLP tasks. However,
these models are often huge and produce large sentence embeddings. Moreover,
there is a big performance gap between large and small models. In this paper,
we propose Homomorphic Projective Distillation (HPD) to learn compressed
sentence embeddings. Our method augments a small Transformer encoder model with
learnable projection layers to produce compact representations while mimicking
a large pre-trained language model to retain the sentence representation
quality. We evaluate our method with different model sizes on both semantic
textual similarity (STS) and semantic retrieval (SR) tasks. Experiments show
that our method achieves 2.7-4.5 points performance gain on STS tasks compared
with previous best representations of the same size. In SR tasks, our method
improves retrieval speed (8.2$\times$) and memory usage (8.0$\times$) compared
with state-of-the-art large models.",1,1,0,0,0,1,0.74602,9.0,0.856193,42
7a303569-e991-45e9-9d6b-2cf97fab5550,Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering,20,0.274337,0.733156,"Retrieval augmented language models have recently become the standard for
knowledge intensive tasks. Rather than relying purely on latent semantics
within the parameters of large neural models, these methods enlist a
semi-parametric memory to encode an index of knowledge for the model to
retrieve over. Most prior work has employed text passages as the unit of
knowledge, which has high coverage at the cost of interpretability,
controllability, and efficiency. The opposite properties arise in other methods
which have instead relied on knowledge base (KB) facts. At the same time, more
recent work has demonstrated the effectiveness of storing and retrieving from
an index of Q-A pairs derived from text \citep{lewis2021paq}. This approach
yields a high coverage knowledge representation that maintains KB-like
properties due to its representations being more atomic units of information.
In this work we push this line of research further by proposing a
question-answer augmented encoder-decoder model and accompanying pretraining
strategy. This yields an end-to-end system that not only outperforms prior QA
retrieval methods on single-hop QA tasks but also enables compositional
reasoning, as demonstrated by strong performance on two multi-hop QA datasets.
Together, these methods improve the ability to interpret and control the model
while narrowing the performance gap with passage retrieval systems.",0,1,0,0,0,0,0.912125,5.0,0.855842,49
a53804c9-1af7-4ad0-b07d-148a76886f7f,Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking,89,0.443584,0.962422,"Exploiting a general-purpose neural architecture to replace hand-wired
designs or inductive biases has recently drawn extensive interest. However,
existing tracking approaches rely on customized sub-modules and need prior
knowledge for architecture selection, hindering the tracking development in a
more general system. This paper presents a Simplified Tracking architecture
(SimTrack) by leveraging a transformer backbone for joint feature extraction
and interaction. Unlike existing Siamese trackers, we serialize the input
images and concatenate them directly before the one-branch backbone. Feature
interaction in the backbone helps to remove well-designed interaction modules
and produce a more efficient and effective framework. To reduce the information
loss from down-sampling in vision transformers, we further propose a foveal
window strategy, providing more diverse input patches with acceptable
computational costs. Our SimTrack improves the baseline with 2.5%/2.6% AUC
gains on LaSOT/TNL2K and gets results competitive with other specialized
tracking algorithms without bells and whistles.",1,1,0,0,0,0,0.935869,3.0,0.800337,63
0517b7fa-2e51-4891-9f32-254acdf1040b,Spiking Graph Convolutional Networks,27,0.111822,0.831234,"Graph Convolutional Networks (GCNs) achieve an impressive performance due to
the remarkable representation ability in learning the graph information.
However, GCNs, when implemented on a deep network, require expensive
computation power, making them difficult to be deployed on battery-powered
devices. In contrast, Spiking Neural Networks (SNNs), which perform a
bio-fidelity inference process, offer an energy-efficient neural architecture.
In this work, we propose SpikingGCN, an end-to-end framework that aims to
integrate the embedding of GCNs with the biofidelity characteristics of SNNs.
The original graph data are encoded into spike trains based on the
incorporation of graph convolution. We further model biological information
processing by utilizing a fully connected layer combined with neuron nodes. In
a wide range of scenarios (e.g. citation networks, image graph classification,
and recommender systems), our experimental results show that the proposed
method could gain competitive performance against state-of-the-art approaches.
Furthermore, we show that SpikingGCN on a neuromorphic chip can bring a clear
advantage of energy efficiency into graph data analysis, which demonstrates its
great potential to construct environment-friendly machine learning models.",1,1,1,0,1,0,0.297524,10.0,0.734968,67
513f756b-3407-4e69-9765-97c28b280c7d,1Cademy @ Causal News Corpus 2022: Enhance Causal Span Detection via Beam-Search-based Position Selector,5,0.0508168,0.622464,"In this paper, we present our approach and empirical observations for
Cause-Effect Signal Span Detection -- Subtask 2 of Shared task
3~\cite{tan-etal-2022-event} at CASE 2022. The shared task aims to extract the
cause, effect, and signal spans from a given causal sentence. We model the task
as a reading comprehension (RC) problem and apply a token-level RC-based span
prediction paradigm to the task as the baseline. We explore different training
objectives to fine-tune the model, as well as data augmentation (DA) tricks
based on the language model (LM) for performance improvement. Additionally, we
propose an efficient beam-search post-processing strategy to due with the
drawbacks of span detection to obtain a further performance gain. Our approach
achieves an average $F_1$ score of 54.15 and ranks \textbf{$1^{st}$} in the
CASE competition. Our code is available at
\url{https://github.com/Gzhang-umich/1CademyTeamOfCASE}.",1,1,1,1,1,0,0.452818,6.0,0.647447,16
37901a8d-f8e5-4e76-8e24-38b9d956c01a,On Gap-dependent Bounds for Offline Reinforcement Learning,9,0.171654,0.325363,"This paper presents a systematic study on gap-dependent sample complexity in
offline reinforcement learning. Prior work showed when the density ratio
between an optimal policy and the behavior policy is upper bounded (the optimal
policy coverage assumption), then the agent can achieve an
$O\left(\frac{1}{\epsilon^2}\right)$ rate, which is also minimax optimal. We
show under the optimal policy coverage assumption, the rate can be improved to
$O\left(\frac{1}{\epsilon}\right)$ when there is a positive sub-optimality gap
in the optimal $Q$-function. Furthermore, we show when the visitation
probabilities of the behavior policy are uniformly lower bounded for states
where an optimal policy's visitation probabilities are positive (the uniform
optimal policy coverage assumption), the sample complexity of identifying an
optimal policy is independent of $\frac{1}{\epsilon}$. Lastly, we present
nearly-matching lower bounds to complement our gap-dependent upper bounds.",0,0,0,0,0,0,0.908449,3.0,0.754072,40
1c531c47-4f91-4cb8-a1fc-51bf3dbab027,3SD: Self-Supervised Saliency Detection With No Labels,3,0.0488677,0.165983,"We present a conceptually simple self-supervised method for saliency
detection. Our method generates and uses pseudo-ground truth labels for
training. The generated pseudo-GT labels don't require any kind of human
annotations (e.g., pixel-wise labels or weak labels like scribbles). Recent
works show that features extracted from classification tasks provide important
saliency cues like structure and semantic information of salient objects in the
image. Our method, called 3SD, exploits this idea by adding a branch for a
self-supervised classification task in parallel with salient object detection,
to obtain class activation maps (CAM maps). These CAM maps along with the edges
of the input image are used to generate the pseudo-GT saliency maps to train
our 3SD network. Specifically, we propose a contrastive learning-based training
on multiple image patches for the classification task. We show the multi-patch
classification with contrastive loss improves the quality of the CAM maps
compared to naive classification on the entire image. Experiments on six
benchmark datasets demonstrate that without any labels, our 3SD method
outperforms all existing weakly supervised and unsupervised methods, and its
performance is on par with the fully-supervised methods. Code is available at
:https://github.com/rajeevyasarla/3SD",1,1,0,0,1,0,0.765818,8.0,0.845408,64
c740ef6a-b84b-455a-bdd1-8118bea62e96,Modelling non-reinforced preferences using selective attention,1,0.0069781,0.0343821,"How can artificial agents learn non-reinforced preferences to continuously
adapt their behaviour to a changing environment? We decompose this question
into two challenges: ($i$) encoding diverse memories and ($ii$) selectively
attending to these for preference formation. Our proposed
\emph{no}n-\emph{re}inforced preference learning mechanism using selective
attention, \textsc{Nore}, addresses both by leveraging the agent's world model
to collect a diverse set of experiences which are interleaved with imagined
roll-outs to encode memories. These memories are selectively attended to, using
attention and gating blocks, to update agent's preferences. We validate
\textsc{Nore} in a modified OpenAI Gym FrozenLake environment (without any
external signal) with and without volatility under a fixed model of the
environment -- and compare its behaviour to \textsc{Pepper}, a Hebbian
preference learning mechanism. We demonstrate that \textsc{Nore} provides a
straightforward framework to induce exploratory preferences in the absence of
external signals.",1,0,0,0,0,0,0.0651703,12.0,0.64111,44
4ae10f1d-e061-4c6a-a36f-41626fb7f3cd,Urban Scene Semantic Segmentation with Low-Cost Coarse Annotation,4,0.0114926,0.177902,"For best performance, today's semantic segmentation methods use large and
carefully labeled datasets, requiring expensive annotation budgets. In this
work, we show that coarse annotation is a low-cost but highly effective
alternative for training semantic segmentation models. Considering the urban
scene segmentation scenario, we leverage cheap coarse annotations for
real-world captured data, as well as synthetic data to train our model and show
competitive performance compared with finely annotated real-world data.
Specifically, we propose a coarse-to-fine self-training framework that
generates pseudo labels for unlabeled regions of the coarsely annotated data,
using synthetic data to improve predictions around the boundaries between
semantic classes, and using cross-domain data augmentation to increase
diversity. Our extensive experimental results on Cityscapes and BDD100k
datasets demonstrate that our method achieves a significantly better
performance vs annotation cost tradeoff, yielding a comparable performance to
fully annotated data with only a small fraction of the annotation budget. Also,
when used as pretraining, our framework performs better compared to the
standard fully supervised setting.",0,1,0,0,0,0,0.108138,9.0,0.580322,47
3fcff545-8435-403c-80ff-e7ab761c35e5,Depth-aware Neural Style Transfer using Instance Normalization,9,0.402152,0.454876,"Neural Style Transfer (NST) is concerned with the artistic stylization of
visual media. It can be described as the process of transferring the style of
an artistic image onto an ordinary photograph. Recently, a number of studies
have considered the enhancement of the depth-preserving capabilities of the NST
algorithms to address the undesired effects that occur when the input content
images include numerous objects at various depths. Our approach uses a deep
residual convolutional network with instance normalization layers that utilizes
an advanced depth prediction network to integrate depth preservation as an
additional loss function to content and style. We demonstrate results that are
effective in retaining the depth and global structure of content images. Three
different evaluation processes show that our system is capable of preserving
the structure of the stylized results while exhibiting style-capture
capabilities and aesthetic qualities comparable or superior to state-of-the-art
methods. Project page:
https://ioannoue.github.io/depth-aware-nst-using-in.html.",1,1,0,0,1,0,0.831719,11.0,0.906215,35
10487b90-4195-493e-adea-b8f246b23ce4,Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition,21,0.383355,0.77845,"The research and applications of multimodal emotion recognition have become
increasingly popular recently. However, multimodal emotion recognition faces
the challenge of lack of data. To solve this problem, we propose to use
transfer learning which leverages state-of-the-art pre-trained models including
wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including
coattention-based early fusion and late fusion with the models trained on both
embeddings are explored. Also, a multi-granularity framework which extracts not
only frame-level speech embeddings but also segment-level embeddings including
phone, syllable and word-level speech embeddings is proposed to further boost
the performance. By combining our coattention-based early fusion model and late
fusion model with the multi-granularity feature extraction framework, we obtain
result that outperforms best baseline approaches by 1.3% unweighted accuracy
(UA) on the IEMOCAP dataset.",0,1,0,0,0,0,0.865922,5.0,0.817673,37
d3d35e83-f1f4-46c3-bcff-896456b82e14,BFCAI at SemEval-2022 Task 6: Multi-Layer Perceptron for Sarcasm Detection in Arabic Texts,1,0.0316709,0.0143108,"This paper describes the systems submitted to iSarcasm shared task. The aim
of iSarcasm is to identify the sarcastic contents in Arabic and English text.
Our team participated in iSarcasm for the Arabic language. A multi-Layer
machine learning based model has been submitted for Arabic sarcasm detection.
In this model, a vector space TF-IDF has been used as for feature
representation. The submitted system is simple and does not need any external
resources. The test results show encouraging results.",0,1,0,0,0,0,0.0785254,19.0,0.783518,11
77b001aa-251e-4729-8822-b1f61f49c077,Privacy-Preserving Personalized Fitness Recommender System (P3FitRec): A Multi-level Deep Learning Approach,3,0.213092,0.305858,"Recommender systems have been successfully used in many domains with the help
of machine learning algorithms. However, such applications tend to use
multi-dimensional user data, which has raised widespread concerns about the
breach of users privacy. Meanwhile, wearable technologies have enabled users to
collect fitness-related data through embedded sensors to monitor their
conditions or achieve personalized fitness goals. In this paper, we propose a
novel privacy-aware personalized fitness recommender system. We introduce a
multi-level deep learning framework that learns important features from a
large-scale real fitness dataset that is collected from wearable IoT devices to
derive intelligent fitness recommendations. Unlike most existing approaches,
our approach achieves personalization by inferring the fitness characteristics
of users from sensory data and thus minimizing the need for explicitly
collecting user identity or biometric information, such as name, age, height,
weight. In particular, our proposed models and algorithms predict (a)
personalized exercise distance recommendations to help users to achieve target
calories, (b) personalized speed sequence recommendations to adjust exercise
speed given the nature of the exercise and the chosen route, and (c)
personalized heart rate sequence to guide the user of the potential health
status for future exercises. Our experimental evaluation on a real-world Fitbit
dataset demonstrated high accuracy in predicting exercise distance, speed
sequence, and heart rate sequence compared to similar studies. Furthermore, our
approach is novel compared to existing studies as it does not require
collecting and using users sensitive information, and thus it preserves the
users privacy.",1,1,0,0,0,0,0.828175,12.0,0.91305,41
858ba835-f461-4bff-a7fa-b56eda9eed5d,FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs,12,0.231795,0.851459,"Predicting the future motion of road agents is a critical task in an
autonomous driving pipeline. In this work, we address the problem of generating
a set of scene-level, or joint, future trajectory predictions in multi-agent
driving scenarios. To this end, we propose FJMP, a Factorized Joint Motion
Prediction framework for multi-agent interactive driving scenarios. FJMP models
the future scene interaction dynamics as a sparse directed interaction graph,
where edges denote explicit interactions between agents. We then prune the
graph into a directed acyclic graph (DAG) and decompose the joint prediction
task into a sequence of marginal and conditional predictions according to the
partial ordering of the DAG, where joint future trajectories are decoded using
a directed acyclic graph neural network (DAGNN). We conduct experiments on the
INTERACTION and Argoverse 2 datasets and demonstrate that FJMP produces more
accurate and scene-consistent joint trajectory predictions than non-factorized
approaches, especially on the most interactive and kinematically interesting
agents. FJMP ranks 1st on the multi-agent test leaderboard of the INTERACTION
dataset.",1,0,0,0,1,0,0.84307,4.0,0.751701,50
cf112544-35f3-4c5f-b97c-5b5d5a6f2221,Black-Box Tuning for Language-Model-as-a-Service,164,0.992971,0.999964,"Extremely large pre-trained language models (PTMs) such as GPT-3 are usually
released as a service. It allows users to design task-specific prompts to query
the PTMs through some black-box APIs. In such a scenario, which we call
Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually
unavailable. Can we optimize the task prompts by only accessing the model
inference APIs? This paper proposes the black-box tuning framework to optimize
the continuous prompt prepended to the input text via derivative-free
optimization. Instead of optimizing in the original high-dimensional prompt
space, which is intractable for traditional derivative-free optimization, we
perform optimization in a randomly generated subspace due to the low intrinsic
dimensionality of large PTMs. The experimental results show that the black-box
tuning with RoBERTa on a few labeled samples not only significantly outperforms
manual prompt and GPT-3's in-context learning, but also surpasses the
gradient-based counterparts, i.e., prompt tuning and full model tuning.",1,1,0,0,0,0,0.986682,4.0,0.963374,68
8b45f0d4-a72f-4a81-ae8f-03f2968d05c8,How Adults Understand What Young Children Say,2,0.0422328,0.489061,"Children's early speech often bears little resemblance to that of adults, and
yet parents and other caregivers are able to interpret that speech and react
accordingly. Here we investigate how these adult inferences as listeners
reflect sophisticated beliefs about what children are trying to communicate, as
well as how children are likely to pronounce words. Using a Bayesian framework
for modeling spoken word recognition, we find that computational models can
replicate adult interpretations of children's speech only when they include
strong, context-specific prior expectations about the messages that children
will want to communicate. This points to a critical role of adult cognitive
processes in supporting early communication and reveals how children can
actively prompt adults to take actions on their behalf even when they have only
a nascent understanding of the adult language. We discuss the wide-ranging
implications of the powerful listening capabilities of adults for theories of
first language acquisition.",0,0,0,0,0,0,0.0497171,28.0,0.836234,84
6bd1b311-1e4b-4f79-a397-c8044b80c490,HEATGait: Hop-Extracted Adjacency Technique in Graph Convolution based Gait Recognition,3,0.036338,0.152832,"Biometric authentication using gait has become a promising field due to its
unobtrusive nature. Recent approaches in model-based gait recognition
techniques utilize spatio-temporal graphs for the elegant extraction of gait
features. However, existing methods often rely on multi-scale operators for
extracting long-range relationships among joints resulting in biased weighting.
In this paper, we present HEATGait, a gait recognition system that improves the
existing multi-scale graph convolution by efficient hop-extraction technique to
alleviate the issue. Combined with preprocessing and augmentation techniques,
we propose a powerful feature extractor that utilizes ResGCN to achieve
state-of-the-art performance in model-based gait recognition on the CASIA-B
gait dataset.",0,1,0,0,1,0,0.573442,7.0,0.7472,25
0270533b-7676-4555-9219-97beda2c136e,Multi-level Latent Space Structuring for Generative Control,2,0.073011,0.0584282,"Truncation is widely used in generative models for improving the quality of
the generated samples, at the expense of reducing their diversity. We propose
to leverage the StyleGAN generative architecture to devise a new truncation
technique, based on a decomposition of the latent space into clusters, enabling
customized truncation to be performed at multiple semantic levels. We do so by
learning to re-generate W-space, the extended intermediate latent space of
StyleGAN, using a learnable mixture of Gaussians, while simultaneously training
a classifier to identify, for each latent vector, the cluster that it belongs
to. The resulting truncation scheme is more faithful to the original
untruncated samples and allows a better trade-off between quality and
diversity. We compare our method to other truncation approaches for StyleGAN,
both qualitatively and quantitatively.",0,0,0,0,0,0,0.784296,8.0,0.852293,27
931f4dfe-002a-49fd-a1cf-784a0d57e044,Box-supervised Instance Segmentation with Level Set Evolution,31,0.221414,0.900773,"In contrast to the fully supervised methods using pixel-wise mask labels,
box-supervised instance segmentation takes advantage of the simple box
annotations, which has recently attracted a lot of research attentions. In this
paper, we propose a novel single-shot box-supervised instance segmentation
approach, which integrates the classical level set model with deep neural
network delicately. Specifically, our proposed method iteratively learns a
series of level sets through a continuous Chan-Vese energy-based function in an
end-to-end fashion. A simple mask supervised SOLOv2 model is adapted to predict
the instance-aware mask map as the level set for each instance. Both the input
image and its deep features are employed as the input data to evolve the level
set curves, where a box projection function is employed to obtain the initial
boundary. By minimizing the fully differentiable energy function, the level set
for each instance is iteratively optimized within its corresponding bounding
box annotation. The experimental results on four challenging benchmarks
demonstrate the leading performance of our proposed approach to robust instance
segmentation in various scenarios. The code is available at:
https://github.com/LiWentomng/boxlevelset.",1,1,0,0,1,0,0.391213,10.0,0.768996,55
399977b9-5468-4abb-883c-e339fe144729,Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models,82,0.29803,0.998493,"Image restoration under adverse weather conditions has been of significant
interest for various computer vision applications. Recent successful methods
rely on the current progress in deep neural network architectural designs
(e.g., with vision transformers). Motivated by the recent progress achieved
with state-of-the-art conditional generative models, we present a novel
patch-based image restoration algorithm based on denoising diffusion
probabilistic models. Our patch-based diffusion modeling approach enables
size-agnostic image restoration by using a guided denoising process with
smoothed noise estimates across overlapping patches during inference. We
empirically evaluate our model on benchmark datasets for image desnowing,
combined deraining and dehazing, and raindrop removal. We demonstrate our
approach to achieve state-of-the-art performances on both weather-specific and
multi-weather image restoration, and experimentally show strong generalization
to real-world test images.",1,1,0,0,1,0,0.666941,6.0,0.74756,77
1c1db1b6-76c0-4066-a852-ee50d2d49192,Adam Mickiewicz University at WMT 2022: NER-Assisted and Quality-Aware Neural Machine Translation,4,0.0306506,0.49849,"This paper presents Adam Mickiewicz University's (AMU) submissions to the
constrained track of the WMT 2022 General MT Task. We participated in the
Ukrainian $\leftrightarrow$ Czech translation directions. The systems are a
weighted ensemble of four models based on the Transformer (big) architecture.
The models use source factors to utilize the information about named entities
present in the input. Each of the models in the ensemble was trained using only
the data provided by the shared task organizers. A noisy back-translation
technique was used to augment the training corpora. One of the models in the
ensemble is a document-level model, trained on parallel and synthetic longer
sequences. During the sentence-level decoding process, the ensemble generated
the n-best list. The n-best list was merged with the n-best list generated by a
single document-level model which translated multiple sentences at a time.
Finally, existing quality estimation models and minimum Bayes risk decoding
were used to rerank the n-best list so that the best hypothesis was chosen
according to the COMET evaluation metric. According to the automatic evaluation
results, our systems rank first in both translation directions.",0,1,0,0,0,0,0.278483,10.0,0.727093,39
9ca28cf6-7c23-44fe-a4a6-2e629d201d20,What Dense Graph Do You Need for Self-Attention?,3,0.011229,0.059273,"Transformers have made progress in miscellaneous tasks, but suffer from
quadratic computational and memory complexities. Recent works propose sparse
Transformers with attention on sparse graphs to reduce complexity and remain
strong performance. While effective, the crucial parts of how dense a graph
needs to be to perform well are not fully explored. In this paper, we propose
Normalized Information Payload (NIP), a graph scoring function measuring
information transfer on graph, which provides an analysis tool for trade-offs
between performance and complexity. Guided by this theoretical analysis, we
present Hypercube Transformer, a sparse Transformer that models token
interactions in a hypercube and shows comparable or even better results with
vanilla Transformer while yielding $O(N\log N)$ complexity with sequence length
$N$. Experiments on tasks requiring various sequence lengths lay validation for
our graph function well.",0,0,0,0,0,0,0.419348,6.0,0.630171,32
639117bd-5a73-4acd-99b3-8d12b2cee2f4,Feature Overcorrelation in Deep Graph Neural Networks: A New Perspective,28,0.0166269,0.302117,"Recent years have witnessed remarkable success achieved by graph neural
networks (GNNs) in many real-world applications such as recommendation and drug
discovery. Despite the success, oversmoothing has been identified as one of the
key issues which limit the performance of deep GNNs. It indicates that the
learned node representations are highly indistinguishable due to the stacked
aggregators. In this paper, we propose a new perspective to look at the
performance degradation of deep GNNs, i.e., feature overcorrelation. Through
empirical and theoretical study on this matter, we demonstrate the existence of
feature overcorrelation in deeper GNNs and reveal potential reasons leading to
this issue. To reduce the feature correlation, we propose a general framework
DeCorr which can encourage GNNs to encode less redundant information. Extensive
experiments have demonstrated that DeCorr can help enable deeper GNNs and is
complementary to existing techniques tackling the oversmoothing issue.",1,0,0,0,0,0,0.0771031,8.0,0.483475,53
f501973d-d80a-4868-9849-d75e02a5bb0b,InBiaseD: Inductive Bias Distillation to Improve Generalization and Robustness through Shape-awareness,4,0.039217,0.159579,"Humans rely less on spurious correlations and trivial cues, such as texture,
compared to deep neural networks which lead to better generalization and
robustness. It can be attributed to the prior knowledge or the high-level
cognitive inductive bias present in the brain. Therefore, introducing
meaningful inductive bias to neural networks can help learn more generic and
high-level representations and alleviate some of the shortcomings. We propose
InBiaseD to distill inductive bias and bring shape-awareness to the neural
networks. Our method includes a bias alignment objective that enforces the
networks to learn more generic representations that are less vulnerable to
unintended cues in the data which results in improved generalization
performance. InBiaseD is less susceptible to shortcut learning and also
exhibits lower texture bias. The better representations also aid in improving
robustness to adversarial attacks and we hence plugin InBiaseD seamlessly into
the existing adversarial training schemes to show a better trade-off between
generalization and robustness.",1,0,0,0,0,0,0.73978,9.0,0.854208,36
2aa94fab-c145-48d4-beaf-8dce1d14b64d,Transformers in Action: Weakly Supervised Action Segmentation,4,0.0515599,0.0747763,"The video action segmentation task is regularly explored under weaker forms
of supervision, such as transcript supervision, where a list of actions is
easier to obtain than dense frame-wise labels. In this formulation, the task
presents various challenges for sequence modeling approaches due to the
emphasis on action transition points, long sequence lengths, and frame
contextualization, making the task well-posed for transformers. Given
developments enabling transformers to scale linearly, we demonstrate through
our architecture how they can be applied to improve action alignment accuracy
over the equivalent RNN-based models with the attention mechanism focusing
around salient action transition regions. Additionally, given the recent focus
on inference-time transcript selection, we propose a supplemental transcript
embedding approach to select transcripts more quickly at inference-time.
Furthermore, we subsequently demonstrate how this approach can also improve the
overall segmentation performance. Finally, we evaluate our proposed methods
across the benchmark datasets to better understand the applicability of
transformers and the importance of transcript selection on this video-driven
weakly-supervised task.",0,1,0,0,0,0,0.836854,6.0,0.830934,47
44567ba3-5fa7-4586-9373-842da7b9ac3f,OptG: Optimizing Gradient-driven Criteria in Network Sparsity,4,0.00602212,0.145053,"Network sparsity receives popularity mostly due to its capability to reduce
the network complexity. Extensive studies excavate gradient-driven sparsity.
Typically, these methods are constructed upon premise of weight independence,
which however, is contrary to the fact that weights are mutually influenced.
Thus, their performance remains to be improved. In this paper, we propose to
optimize gradient-driven sparsity (OptG) by solving this independence paradox.
Our motive comes from the recent advances in supermask training which shows
that high-performing sparse subnetworks can be located by simply updating mask
values without modifying any weight. We prove that supermask training is to
accumulate the criteria of gradient-driven sparsity for both removed and
preserved weights, and it can partly solve the independence paradox.
Consequently, OptG integrates supermask training into gradient-driven sparsity,
and a novel supermask optimizer is further proposed to comprehensively mitigate
the independence paradox. Experiments show that OptG can well surpass many
existing state-of-the-art competitors, especially at ultra-high sparsity
levels. Our code is available at \url{https://github.com/zyxxmu/OptG}.",1,0,0,0,1,0,0.337759,6.0,0.584022,52
cbad936b-914a-4aed-b01f-3f034a457785,Continuous Scene Representations for Embodied AI,36,0.12263,0.861507,"We propose Continuous Scene Representations (CSR), a scene representation
constructed by an embodied agent navigating within a space, where objects and
their relationships are modeled by continuous valued embeddings. Our method
captures feature relationships between objects, composes them into a graph
structure on-the-fly, and situates an embodied agent within the representation.
Our key insight is to embed pair-wise relationships between objects in a latent
space. This allows for a richer representation compared to discrete relations
(e.g., [support], [next-to]) commonly used for building scene representations.
CSR can track objects as the agent moves in a scene, update the representation
accordingly, and detect changes in room configurations. Using CSR, we
outperform state-of-the-art approaches for the challenging downstream task of
visual room rearrangement, without any task specific training. Moreover, we
show the learned embeddings capture salient spatial details of the scene and
show applicability to real world data. A summery video and code is available at
https://prior.allenai.org/projects/csr.",0,0,0,0,1,0,0.273587,7.0,0.607141,66
987b1fbb-93eb-4c1f-b8ec-bf7860b58bb9,Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer,9,0.236427,0.445919,"We exploit the pre-trained seq2seq model mBART for multilingual text style
transfer. Using machine translated data as well as gold aligned English
sentences yields state-of-the-art results in the three target languages we
consider. Besides, in view of the general scarcity of parallel data, we propose
a modular approach for multilingual formality transfer, which consists of two
training strategies that target adaptation to both language and task. Our
approach achieves competitive performance without monolingual task-specific
parallel data and can be applied to other style transfer tasks as well as to
other languages.",1,1,0,0,1,0,0.887036,5.0,0.83404,35
04d79125-8613-46eb-8e10-bde679d4b5bf,Multi-Spectral Image Classification with Ultra-Lean Complex-Valued Models,2,0.040005,0.147378,"Multi-spectral imagery is invaluable for remote sensing due to different
spectral signatures exhibited by materials that often appear identical in
greyscale and RGB imagery. Paired with modern deep learning methods, this
modality has great potential utility in a variety of remote sensing
applications, such as humanitarian assistance and disaster recovery efforts.
State-of-the-art deep learning methods have greatly benefited from large-scale
annotations like in ImageNet, but existing MSI image datasets lack annotations
at a similar scale. As an alternative to transfer learning on such data with
few annotations, we apply complex-valued co-domain symmetric models to classify
real-valued MSI images. Our experiments on 8-band xView data show that our
ultra-lean model trained on xView from scratch without data augmentations can
outperform ResNet with data augmentation and modified transfer learning on
xView. Our work is the first to demonstrate the value of complex-valued deep
learning on real-valued MSI data.",0,1,0,0,1,0,0.598692,10.0,0.829957,21
d0dbbc66-8d24-43c9-b767-ca1d941e3652,ADAS: A Direct Adaptation Strategy for Multi-Target Domain Adaptive Semantic Segmentation,11,0.106159,0.577297,"In this paper, we present a direct adaptation strategy (ADAS), which aims to
directly adapt a single model to multiple target domains in a semantic
segmentation task without pretrained domain-specific models. To do so, we
design a multi-target domain transfer network (MTDT-Net) that aligns visual
attributes across domains by transferring the domain distinctive features
through a new target adaptive denormalization (TAD) module. Moreover, we
propose a bi-directional adaptive region selection (BARS) that reduces the
attribute ambiguity among the class labels by adaptively selecting the regions
with consistent feature statistics. We show that our single MTDT-Net can
synthesize visually pleasing domain transferred images with complex driving
datasets, and BARS effectively filters out the unnecessary region of training
images for each target domain. With the collaboration of MTDT-Net and BARS, our
ADAS achieves state-of-the-art performance for multi-target domain adaptation
(MTDA). To the best of our knowledge, our method is the first MTDA method that
directly adapts to multiple domains in semantic segmentation.",0,1,1,0,1,0,0.862463,8.0,0.884451,63
0acc7c2f-6333-4251-a092-ff9df3de981a,Unsupervised Cross-Modality Domain Adaptation for Vestibular Schwannoma Segmentation and Koos Grade Prediction based on Semi-Supervised Contrastive Learning,5,0.0298213,0.379787,"Domain adaptation has been widely adopted to transfer styles across
multi-vendors and multi-centers, as well as to complement the missing
modalities. In this challenge, we proposed an unsupervised domain adaptation
framework for cross-modality vestibular schwannoma (VS) and cochlea
segmentation and Koos grade prediction. We learn the shared representation from
both ceT1 and hrT2 images and recover another modality from the latent
representation, and we also utilize proxy tasks of VS segmentation and brain
parcellation to restrict the consistency of image structures in domain
adaptation. After generating missing modalities, the nnU-Net model is utilized
for VS and cochlea segmentation, while a semi-supervised contrastive learning
pre-train approach is employed to improve the model performance for Koos grade
prediction. On CrossMoDA validation phase Leaderboard, our method received rank
4 in task1 with a mean Dice score of 0.8394 and rank 2 in task2 with
Macro-Average Mean Square Error of 0.3941. Our code is available at
https://github.com/fiy2W/cmda2022.superpolymerization.",1,1,0,0,0,0,0.636884,4.0,0.600878,16
3cc42bb7-f165-4427-865e-3cdc77bac527,3D Dual-Fusion: Dual-Domain Dual-Query Camera-LiDAR Fusion for 3D Object Detection,9,0.379808,0.635653,"Fusing data from cameras and LiDAR sensors is an essential technique to
achieve robust 3D object detection. One key challenge in camera-LiDAR fusion
involves mitigating the large domain gap between the two sensors in terms of
coordinates and data distribution when fusing their features. In this paper, we
propose a novel camera-LiDAR fusion architecture called, 3D Dual-Fusion, which
is designed to mitigate the gap between the feature representations of camera
and LiDAR data. The proposed method fuses the features of the camera-view and
3D voxel-view domain and models their interactions through deformable
attention. We redesign the transformer fusion encoder to aggregate the
information from the two domains. Two major changes include 1) dual query-based
deformable attention to fuse the dual-domain features interactively and 2) 3D
local self-attention to encode the voxel-domain queries prior to dual-query
decoding. The results of an experimental evaluation show that the proposed
camera-LiDAR fusion architecture achieved competitive performance on the KITTI
and nuScenes datasets, with state-of-the-art performances in some 3D object
detection benchmarks categories.",0,1,0,0,1,0,0.990262,5.0,0.984696,48
11a09566-14e2-4d15-83e2-b2d04eaeb8dd,A Human-Centric Assessment Framework for AI,5,0.149653,0.456418,"With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner.",0,0,0,0,0,0,0.832128,11.0,0.906339,14
bd76d05a-010d-4ef9-8a99-e96e95ca87f2,The Internet of Senses: Building on Semantic Communications and Edge Intelligence,5,0.458989,0.853393,"The Internet of Senses (IoS) holds the promise of flawless telepresence-style
communication for all human `receptors' and therefore blurs the difference of
virtual and real environments. We commence by highlighting the compelling use
cases empowered by the IoS and also the key network requirements. We then
elaborate on how the emerging semantic communications and Artificial
Intelligence (AI)/Machine Learning (ML) paradigms along with 6G technologies
may satisfy the requirements of IoS use cases. On one hand, semantic
communications can be applied for extracting meaningful and significant
information and hence efficiently exploit the resources and for harnessing a
priori information at the receiver to satisfy IoS requirements. On the other
hand, AI/ML facilitates frugal network resource management by making use of the
enormous amount of data generated in IoS edge nodes and devices, as well as by
optimizing the IoS performance via intelligent agents. However, the intelligent
agents deployed at the edge are not completely aware of each others' decisions
and the environments of each other, hence they operate in a partially rather
than fully observable environment. Therefore, we present a case study of
Partially Observable Markov Decision Processes (POMDP) for improving the User
Equipment (UE) throughput and energy consumption, as they are imperative for
IoS use cases, using Reinforcement Learning for astutely activating and
deactivating the component carriers in carrier aggregation. Finally, we outline
the challenges and open issues of IoS implementations and employing semantic
communications, edge intelligence as well as learning under partial
observability in the IoS context.",0,0,0,0,0,0,0.976393,5.0,0.942262,19
65a94cf9-0280-4742-a96f-46f4f350064a,SALTED: A Framework for SAlient Long-Tail Translation Error Detection,20,0.53708,0.702872,"Traditional machine translation (MT) metrics provide an average measure of
translation quality that is insensitive to the long tail of behavioral problems
in MT. Examples include translation of numbers, physical units, dropped content
and hallucinations. These errors, which occur rarely and unpredictably in
Neural Machine Translation (NMT), greatly undermine the reliability of
state-of-the-art MT systems. Consequently, it is important to have visibility
into these problems during model development. Towards this direction, we
introduce SALTED, a specifications-based framework for behavioral testing of MT
models that provides fine-grained views of salient long-tail errors, permitting
trustworthy visibility into previously invisible problems. At the core of our
approach is the development of high-precision detectors that flag errors (or
alternatively, verify output correctness) between a source sentence and a
system output. We demonstrate that such detectors could be used not just to
identify salient long-tail errors in MT systems, but also for higher-recall
filtering of the training data, fixing targeted errors with model fine-tuning
in NMT and generating novel data for metamorphic testing to elicit further bugs
in models.",0,1,0,0,0,0,0.885764,6.0,0.860842,32
4b21a82f-39bf-43b2-912e-e8c070ec700f,BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets,15,0.0619283,0.737404,"The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article.",0,1,0,0,1,0,0.194056,5.0,0.371377,89
7f97240f-93fe-4eaf-9fb3-4d9725094444,Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models,7,0.424762,0.826226,"Neural network models have achieved state-of-the-art performance on
grapheme-to-phoneme (G2P) conversion. However, their performance relies on
large-scale pronunciation dictionaries, which may not be available for a lot of
languages. Inspired by the success of the pre-trained language model BERT, this
paper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which
is built by self-supervised training on a large, language-specific word list
with only grapheme information. Furthermore, two approaches are developed to
incorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e.,
fine-tuning GBERT or fusing GBERT into the Transformer model by attention.
Experimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean
datasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our
GBERT-based G2P models under both medium-resource and low-resource data
conditions.",1,1,0,1,0,0,0.872617,5.0,0.822708,25
7d21ba25-d9f5-4ab5-b105-da5809a3f785,Correlation-Aware Deep Tracking,68,0.684719,0.97059,"Robustness and discrimination power are two fundamental requirements in
visual object tracking. In most tracking paradigms, we find that the features
extracted by the popular Siamese-like networks cannot fully discriminatively
model the tracked targets and distractor objects, hindering them from
simultaneously meeting these two requirements. While most methods focus on
designing robust correlation operations, we propose a novel target-dependent
feature network inspired by the self-/cross-attention scheme. In contrast to
the Siamese-like feature extraction, our network deeply embeds cross-image
feature correlation in multiple layers of the feature network. By extensively
matching the features of the two images through multiple layers, it is able to
suppress non-target features, resulting in instance-varying feature extraction.
The output features of the search image can be directly used for predicting
target locations without extra correlation step. Moreover, our model can be
flexibly pre-trained on abundant unpaired images, leading to notably faster
convergence than the existing methods. Extensive experiments show our method
achieves the state-of-the-art results while running at real-time. Our feature
networks also can be applied to existing tracking pipelines seamlessly to raise
the tracking performance. Code will be available.",0,1,1,0,1,0,0.954983,6.0,0.920366,58
03de1845-d1ee-4bda-9a2d-4e4e598c0f85,Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding,6,0.0435975,0.223007,"Pre-trained language models (PLM) have demonstrated their effectiveness for a
broad range of information retrieval and natural language processing tasks. As
the core part of PLM, multi-head self-attention is appealing for its ability to
jointly attend to information from different positions. However, researchers
have found that PLM always exhibits fixed attention patterns regardless of the
input (e.g., excessively paying attention to [CLS] or [SEP]), which we argue
might neglect important information in the other positions. In this work, we
propose a simple yet effective attention guiding mechanism to improve the
performance of PLM by encouraging attention towards the established goals.
Specifically, we propose two kinds of attention guiding methods, i.e., map
discrimination guiding (MDG) and attention pattern decorrelation guiding (PDG).
The former definitely encourages the diversity among multiple self-attention
heads to jointly attend to information from different representation subspaces,
while the latter encourages self-attention to attend to as many different
positions of the input as possible. We conduct experiments with multiple
general pre-trained models (i.e., BERT, ALBERT, and Roberta) and
domain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and
SciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and
Cross-genre-IR). Extensive experimental results demonstrate that our proposed
MDG and PDG bring stable performance improvements on all datasets with high
efficiency and low cost.",1,1,0,0,0,1,0.442488,6.0,0.642195,49
07e0ce8c-c6e6-439b-b313-4eed5abbf495,Instance-Dependent Noisy Label Learning via Graphical Modelling,13,0.105752,0.65292,"Noisy labels are unavoidable yet troublesome in the ecosystem of deep
learning because models can easily overfit them. There are many types of label
noise, such as symmetric, asymmetric and instance-dependent noise (IDN), with
IDN being the only type that depends on image information. Such dependence on
image information makes IDN a critical type of label noise to study, given that
labelling mistakes are caused in large part by insufficient or ambiguous
information about the visual classes present in images. Aiming to provide an
effective technique to address IDN, we present a new graphical modelling
approach called InstanceGM, that combines discriminative and generative models.
The main contributions of InstanceGM are: i) the use of the continuous
Bernoulli distribution to train the generative model, offering significant
training advantages, and ii) the exploration of a state-of-the-art noisy-label
discriminative classifier to generate clean labels from instance-dependent
noisy-label samples. InstanceGM is competitive with current noisy-label
learning approaches, particularly in IDN benchmarks using synthetic and
real-world datasets, where our method shows better accuracy than the
competitors in most experiments.",0,1,0,0,1,0,0.575326,6.0,0.70593,75
aacc175c-0477-4675-8f5b-99d5f0986e14,Learning-based Motion Planning in Dynamic Environments Using GNNs and Temporal Encoding,7,0.0341868,0.583777,"Learning-based methods have shown promising performance for accelerating
motion planning, but mostly in the setting of static environments. For the more
challenging problem of planning in dynamic environments, such as multi-arm
assembly tasks and human-robot interaction, motion planners need to consider
the trajectories of the dynamic obstacles and reason about temporal-spatial
interactions in very large state spaces. We propose a GNN-based approach that
uses temporal encoding and imitation learning with data aggregation for
learning both the embeddings and the edge prioritization policies. Experiments
show that the proposed methods can significantly accelerate online planning
over state-of-the-art complete dynamic planning algorithms. The learned models
can often reduce costly collision checking operations by more than 1000x, and
thus accelerating planning by up to 95%, while achieving high success rates on
hard instances as well.",0,1,0,0,1,0,0.0414529,9.0,0.469829,48
3f4672ff-3a14-45c6-bfdf-c18aaf80265e,Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection,28,0.236174,0.623653,"With the vigorous development of computer vision, oriented object detection
has gradually been featured. In this paper, a novel differentiable angle coder
named phase-shifting coder (PSC) is proposed to accurately predict the
orientation of objects, along with a dual-frequency version (PSCD). By mapping
the rotational periodicity of different cycles into the phase of different
frequencies, we provide a unified framework for various periodic fuzzy problems
caused by rotational symmetry in oriented object detection. Upon such a
framework, common problems in oriented object detection such as boundary
discontinuity and square-like problems are elegantly solved in a unified form.
Visual analysis and experiments on three datasets prove the effectiveness and
the potentiality of our approach. When facing scenarios requiring high-quality
bounding boxes, the proposed methods are expected to give a competitive
performance. The codes are publicly available at
https://github.com/open-mmlab/mmrotate.",1,1,0,0,0,0,0.840419,8.0,0.874714,39
3232f4b7-dcbb-4258-bb3c-54f3323996dd,Learning to Efficiently Plan Robust Frictional Multi-Object Grasps,7,0.27888,0.279986,"We consider a decluttering problem where multiple rigid convex polygonal
objects rest in randomly placed positions and orientations on a planar surface
and must be efficiently transported to a packing box using both single and
multi-object grasps. Prior work considered frictionless multi-object grasping.
In this paper, we introduce friction to increase the number of potential grasps
for a given group of objects, and thus increase picks per hour. We train a
neural network using real examples to plan robust multi-object grasps. In
physical experiments, we find a 13.7% increase in success rate, a 1.6x increase
in picks per hour, and a 6.3x decrease in grasp planning time compared to prior
work on multi-object grasping. Compared to single-object grasping, we find a
3.1x increase in picks per hour.",0,1,1,0,1,0,0.216806,9.0,0.664612,54
5d88efd0-9305-4d26-9f7f-c3629a63be6a,PAL: Persona-Augmented Emotional Support Conversation Generation,9,0.212075,0.951647,"Due to the lack of human resources for mental health support, there is an
increasing demand for employing conversational agents for support. Recent work
has demonstrated the effectiveness of dialogue models in providing emotional
support. As previous studies have demonstrated that seekers' persona is an
important factor for effective support, we investigate whether there are
benefits to modeling such information in dialogue models for support. In this
paper, our empirical analysis verifies that persona has an important impact on
emotional support. Therefore, we propose a framework for dynamically inferring
and modeling seekers' persona. We first train a model for inferring the
seeker's persona from the conversation history. Accordingly, we propose PAL, a
model that leverages persona information and, in conjunction with our
strategy-based controllable generation method, provides personalized emotional
support. Automatic and manual evaluations demonstrate that PAL achieves
state-of-the-art results, outperforming the baselines on the studied benchmark.
Our code and data are publicly available at https://github.com/chengjl19/PAL.",1,1,1,0,1,0,0.676684,7.0,0.787429,71
4966d118-6278-439a-b5cf-03f6da254ae1,Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind,10,0.449696,0.934603,"When reading a story, humans can quickly understand new fictional characters
with a few observations, mainly by drawing analogies to fictional and real
people they already know. This reflects the few-shot and meta-learning essence
of humans' inference of characters' mental states, i.e., theory-of-mind (ToM),
which is largely ignored in existing research. We fill this gap with a novel
NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM
in a realistic narrative understanding scenario. Our dataset consists of ~1,000
parsed movie scripts, each corresponding to a few-shot character understanding
task that requires models to mimic humans' ability of fast digesting characters
with a few starting scenes in a new movie.
  We propose a novel ToM prompting approach designed to explicitly assess the
influence of multiple ToM dimensions. It surpasses existing baseline models,
underscoring the significance of modeling multiple ToM dimensions for our task.
Our extensive human study verifies that humans are capable of solving our
problem by inferring characters' mental states based on their previously seen
movies. In comparison, our systems based on either state-of-the-art large
language models (GPT-4) or meta-learning algorithms lags >20% behind,
highlighting a notable limitation in existing approaches' ToM capabilities.",0,0,1,1,0,0,0.552681,9.0,0.797001,49
dada3361-847b-406f-b171-b961eb5fd7b2,SegViT: Semantic Segmentation with Plain Vision Transformers,60,0.136201,0.905516,"We explore the capability of plain Vision Transformers (ViTs) for semantic
segmentation and propose the SegVit. Previous ViT-based segmentation networks
usually learn a pixel-level representation from the output of the ViT.
Differently, we make use of the fundamental component -- attention mechanism,
to generate masks for semantic segmentation. Specifically, we propose the
Attention-to-Mask (ATM) module, in which the similarity maps between a set of
learnable class tokens and the spatial feature maps are transferred to the
segmentation masks. Experiments show that our proposed SegVit using the ATM
module outperforms its counterparts using the plain ViT backbone on the ADE20K
dataset and achieves new state-of-the-art performance on COCO-Stuff-10K and
PASCAL-Context datasets. Furthermore, to reduce the computational cost of the
ViT backbone, we propose query-based down-sampling (QD) and query-based
up-sampling (QU) to build a Shrunk structure. With the proposed Shrunk
structure, the model can save up to $40\%$ computations while maintaining
competitive performance.",1,1,0,0,1,0,0.581424,6.0,0.708722,46
0a7562af-86ef-4e84-981d-481f66e0f217,A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching,11,0.12989,0.43958,"We present a scalable combinatorial algorithm for globally optimizing over
the space of geometrically consistent mappings between 3D shapes. We use the
mathematically elegant formalism proposed by Windheuser et al. (ICCV 2011)
where 3D shape matching was formulated as an integer linear program over the
space of orientation-preserving diffeomorphisms. Until now, the resulting
formulation had limited practical applicability due to its complicated
constraint structure and its large size. We propose a novel primal heuristic
coupled with a Lagrange dual problem that is several orders of magnitudes
faster compared to previous solvers. This allows us to handle shapes with
substantially more triangles than previously solvable. We demonstrate
compelling results on diverse datasets, and, even showcase that we can address
the challenging setting of matching two partial shapes without availability of
complete shapes. Our code is publicly available at
http://github.com/paul0noah/sm-comb .",1,1,0,0,0,0,0.0746983,10.0,0.583483,72
d9ffd8c0-9ec0-43c6-82df-7d7cf5559c92,Automatic Fine-grained Glomerular Lesion Recognition in Kidney Pathology,6,0.068277,0.522108,"Recognition of glomeruli lesions is the key for diagnosis and treatment
planning in kidney pathology; however, the coexisting glomerular structures
such as mesangial regions exacerbate the difficulties of this task. In this
paper, we introduce a scheme to recognize fine-grained glomeruli lesions from
whole slide images. First, a focal instance structural similarity loss is
proposed to drive the model to locate all types of glomeruli precisely. Then an
Uncertainty Aided Apportionment Network is designed to carry out the
fine-grained visual classification without bounding-box annotations. This
double branch-shaped structure extracts common features of the child class from
the parent class and produces the uncertainty factor for reconstituting the
training dataset. Results of slide-wise evaluation illustrate the effectiveness
of the entire scheme, with an 8-22% improvement of the mean Average Precision
compared with remarkable detection methods. The comprehensive results clearly
demonstrate the effectiveness of the proposed method.",0,1,0,0,0,0,0.739186,10.0,0.868617,47
6e19f25c-7b23-4751-a4c2-d2509dc5493b,A model-based approach to meta-Reinforcement Learning: Transformers and tree search,2,0.0247038,0.0939036,"Meta-learning is a line of research that develops the ability to leverage
past experiences to efficiently solve new learning problems. Meta-Reinforcement
Learning (meta-RL) methods demonstrate a capability to learn behaviors that
efficiently acquire and exploit information in several meta-RL problems.
  In this context, the Alchemy benchmark has been proposed by Wang et al.
[2021]. Alchemy features a rich structured latent space that is challenging for
state-of-the-art model-free RL methods. These methods fail to learn to properly
explore then exploit.
  We develop a model-based algorithm. We train a model whose principal block is
a Transformer Encoder to fit the symbolic Alchemy environment dynamics. Then we
define an online planner with the learned model using a tree search method.
This algorithm significantly outperforms previously applied model-free RL
methods on the symbolic Alchemy problem.
  Our results reveal the relevance of model-based approaches with online
planning to perform exploration and exploitation successfully in meta-RL.
Moreover, we show the efficiency of the Transformer architecture to learn
complex dynamics that arise from latent spaces present in meta-RL problems.",0,1,0,0,0,0,0.554981,9.0,0.797711,41
451260e2-4f57-4038-8523-6476f7cffc69,A Dual-Contrastive Framework for Low-Resource Cross-Lingual Named Entity Recognition,4,0.202222,0.324102,"Cross-lingual Named Entity Recognition (NER) has recently become a research
hotspot because it can alleviate the data-hungry problem for low-resource
languages. However, few researches have focused on the scenario where the
source-language labeled data is also limited in some specific domains. A common
approach for this scenario is to generate more training data through
translation or generation-based data augmentation method. Unfortunately, we
find that simply combining source-language data and the corresponding
translation cannot fully exploit the translated data and the improvements
obtained are somewhat limited. In this paper, we describe our novel
dual-contrastive framework ConCNER for cross-lingual NER under the scenario of
limited source-language labeled data. Specifically, based on the
source-language samples and their translations, we design two contrastive
objectives for cross-language NER at different grammatical levels, namely
Translation Contrastive Learning (TCL) to close sentence representations
between translated sentence pairs and Label Contrastive Learning (LCL) to close
token representations within the same labels. Furthermore, we utilize knowledge
distillation method where the NER model trained above is used as the teacher to
train a student model on unlabeled target-language data to better fit the
target language. We conduct extensive experiments on a wide variety of target
languages, and the results demonstrate that ConCNER tends to outperform
multiple baseline methods. For reproducibility, our code for this paper is
available at https://github.com/GKLMIP/ConCNER.",0,1,0,0,0,0,0.956037,6.0,0.921635,49
1628a6c0-1d9f-4558-a8ae-056a42b3676a,Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours,13,0.0263097,0.424159,"Text classification can be useful in many real-world scenarios, saving a lot
of time for end users. However, building a custom classifier typically requires
coding skills and ML knowledge, which poses a significant barrier for many
potential users. To lift this barrier, we introduce Label Sleuth, a free open
source system for labeling and creating text classifiers. This system is unique
for (a) being a no-code system, making NLP accessible to non-experts, (b)
guiding users through the entire labeling process until they obtain a custom
classifier, making the process efficient -- from cold start to classifier in a
few hours, and (c) being open for configuration and extension by developers. By
open sourcing Label Sleuth we hope to build a community of users and developers
that will broaden the utilization of NLP models.",0,1,0,0,0,1,0.00411053,16.0,0.556151,24
906f3362-3402-4ad2-a527-7a2b6c473d11,Weakly Supervised 3D Point Cloud Segmentation via Multi-Prototype Learning,13,0.127831,0.519184,"Addressing the annotation challenge in 3D Point Cloud segmentation has
inspired research into weakly supervised learning. Existing approaches mainly
focus on exploiting manifold and pseudo-labeling to make use of large unlabeled
data points. A fundamental challenge here lies in the large intra-class
variations of local geometric structure, resulting in subclasses within a
semantic class. In this work, we leverage this intuition and opt for
maintaining an individual classifier for each subclass. Technically, we design
a multi-prototype classifier, each prototype serves as the classifier weights
for one subclass. To enable effective updating of multi-prototype classifier
weights, we propose two constraints respectively for updating the prototypes
w.r.t. all point features and for encouraging the learning of diverse
prototypes. Experiments on weakly supervised 3D point cloud segmentation tasks
validate the efficacy of proposed method in particular at low-label regime. Our
hypothesis is also verified given the consistent discovery of semantic
subclasses at no cost of additional annotations.",0,1,0,0,0,0,0.667843,5.0,0.697564,62
e39c8c53-bb81-499b-b842-249b2cc36f41,NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night,23,0.113767,0.505583,"The semantic segmentation of nighttime scenes is a challenging problem that
is key to impactful applications like self-driving cars. Yet, it has received
little attention compared to its daytime counterpart. In this paper, we propose
NightLab, a novel nighttime segmentation framework that leverages multiple deep
learning models imbued with night-aware features to yield State-of-The-Art
(SoTA) performance on multiple night segmentation benchmarks. Notably, NightLab
contains models at two levels of granularity, i.e. image and regional, and each
level is composed of light adaptation and segmentation modules. Given a
nighttime image, the image level model provides an initial segmentation
estimate while, in parallel, a hardness detection module identifies regions and
their surrounding context that need further analysis. A regional level model
focuses on these difficult regions to provide a significantly improved
segmentation. All the models in NightLab are trained end-to-end using a set of
proposed night-aware losses without handcrafted heuristics. Extensive
experiments on the NightCity and BDD100K datasets show NightLab achieves SoTA
performance compared to concurrent methods.",1,1,0,0,1,0,0.436268,8.0,0.72925,70
fb57a366-bd97-4cef-b0c8-7b009a579e8e,B-SMART: A Reference Architecture for Artificially Intelligent Autonomic Smart Buildings,10,0.387249,0.884224,"The pervasive application of artificial intelligence and machine learning
algorithms is transforming many industries and aspects of the human experience.
One very important industry trend is the move to convert existing human
dwellings to smart buildings, and to create new smart buildings. Smart
buildings aim to mitigate climate change by reducing energy consumption and
associated carbon emissions. To accomplish this, they leverage artificial
intelligence, big data, and machine learning algorithms to learn and optimize
system performance. These fields of research are currently very rapidly
evolving and advancing, but there has been very little guidance to help
engineers and architects working on smart buildings apply artificial
intelligence algorithms and technologies in a systematic and effective manner.
In this paper we present B-SMART: the first reference architecture for
autonomic smart buildings. B-SMART facilitates the application of artificial
intelligence techniques and technologies to smart buildings by decoupling
conceptually distinct layers of functionality and organizing them into an
autonomic control loop. We also present a case study illustrating how B-SMART
can be applied to accelerate the introduction of artificial intelligence into
an existing smart building.",0,1,0,0,0,0,0.183222,10.0,0.679299,45
cc4a8544-9951-4344-a948-e9795aacce39,Match Cutting: Finding Cuts with Smooth Visual Transitions,4,0.130515,0.284047,"A match cut is a transition between a pair of shots that uses similar
framing, composition, or action to fluidly bring the viewer from one scene to
the next. Match cuts are frequently used in film, television, and advertising.
However, finding shots that work together is a highly manual and time-consuming
process that can take days. We propose a modular and flexible system to
efficiently find high-quality match cut candidates starting from millions of
shot pairs. We annotate and release a dataset of approximately 20k labeled
pairs that we use to evaluate our system, using both classification and metric
learning approaches that leverage a variety of image, video, audio, and
audio-visual feature extractors. In addition, we release code and embeddings
for reproducing our experiments at github.com/netflix/matchcut.",1,1,0,1,0,0,0.624528,10.0,0.836992,81
3043e978-6b85-438e-b6ae-13eb3c2a1a06,Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting,19,0.279054,0.898452,"We study the knowledge extrapolation problem to embed new components (i.e.,
entities and relations) that come with emerging knowledge graphs (KGs) in the
federated setting. In this problem, a model trained on an existing KG needs to
embed an emerging KG with unseen entities and relations. To solve this problem,
we introduce the meta-learning setting, where a set of tasks are sampled on the
existing KG to mimic the link prediction task on the emerging KG. Based on
sampled tasks, we meta-train a graph neural network framework that can
construct features for unseen components based on structural information and
output embeddings for them. Experimental results show that our proposed method
can effectively embed unseen components and outperforms models that consider
inductive settings for KGs and baselines that directly use conventional KG
embedding methods.",0,0,1,0,0,0,0.922606,10.0,0.933012,32
058ecc12-f391-4e31-b9c5-be5d66b12dbf,First do not fall: learning to exploit a wall with a damaged humanoid robot,2,0.025783,0.20677,"Humanoid robots could replace humans in hazardous situations but most of such
situations are equally dangerous for them, which means that they have a high
chance of being damaged and falling. We hypothesize that humanoid robots would
be mostly used in buildings, which makes them likely to be close to a wall. To
avoid a fall, they can therefore lean on the closest wall, as a human would do,
provided that they find in a few milliseconds where to put the hand(s). This
article introduces a method, called D-Reflex, that learns a neural network that
chooses this contact position given the wall orientation, the wall distance,
and the posture of the robot. This contact position is then used by a
whole-body controller to reach a stable posture. We show that D-Reflex allows a
simulated TALOS robot (1.75m, 100kg, 30 degrees of freedom) to avoid more than
75% of the avoidable falls and can work on the real robot.",1,1,0,0,0,0,0.109392,8.0,0.529389,31
c8d97709-7a55-4952-868a-6b574815b579,CrossHuman: Learning Cross-Guidance from Multi-Frame Images for Human Reconstruction,2,0.0423718,0.0886703,"We propose CrossHuman, a novel method that learns cross-guidance from
parametric human model and multi-frame RGB images to achieve high-quality 3D
human reconstruction. To recover geometry details and texture even in invisible
regions, we design a reconstruction pipeline combined with tracking-based
methods and tracking-free methods. Given a monocular RGB sequence, we track the
parametric human model in the whole sequence, the points (voxels) corresponding
to the target frame are warped to reference frames by the parametric body
motion. Guided by the geometry priors of the parametric body and spatially
aligned features from RGB sequence, the robust implicit surface is fused.
Moreover, a multi-frame transformer (MFT) and a self-supervised warp refinement
module are integrated to the framework to relax the requirements of parametric
body and help to deal with very loose cloth. Compared with previous works, our
CrossHuman enables high-fidelity geometry details and texture in both visible
and invisible regions and improves the accuracy of the human reconstruction
even under estimated inaccurate parametric human models. The experiments
demonstrate that our method achieves state-of-the-art (SOTA) performance.",0,1,0,0,1,0,0.925226,7.0,0.906213,62
e356f915-c1e5-4b4b-9e65-2f01d3b6a22f,On the Relation between Sensitivity and Accuracy in In-context Learning,52,0.10817,0.820593,"In-context learning (ICL) suffers from oversensitivity to the prompt, making
it unreliable in real-world scenarios. We study the sensitivity of ICL with
respect to multiple perturbation types. First, we find that label bias obscures
the true sensitivity, and therefore prior work may have significantly
underestimated ICL sensitivity. Second, we observe a strong negative
correlation between ICL sensitivity and accuracy: predictions sensitive to
perturbations are less likely to be correct. Motivated by these findings, we
propose \textsc{SenSel}, a few-shot selective prediction method that abstains
from sensitive predictions. Experiments on ten classification datasets show
that \textsc{SenSel} consistently outperforms two commonly used
confidence-based and entropy-based baselines on abstention decisions.",1,0,0,0,0,0,0.438376,5.0,0.568103,45
a1534ed2-f91d-4063-9987-e43c53783cbb,Large Language Models with Controllable Working Memory,68,0.147228,0.452256,"Large language models (LLMs) have led to a series of breakthroughs in natural
language processing (NLP), owing to their excellent understanding and
generation abilities. Remarkably, what further sets these models apart is the
massive amounts of world knowledge they internalize during pretraining. While
many downstream applications provide the model with an informational context to
aid its performance on the underlying task, how the model's world knowledge
interacts with the factual information presented in the context remains under
explored. As a desirable behavior, an LLM should give precedence to the context
whenever it contains task-relevant information that conflicts with the model's
memorized knowledge. This enables model predictions to be grounded in the
context, which can then be used to update or correct specific model predictions
without frequent retraining. By contrast, when the context is irrelevant to the
task, the model should ignore it and fall back on its internal knowledge. In
this paper, we undertake a first joint study of the aforementioned two
properties, namely controllability and robustness, in the context of LLMs. We
demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned)
could exhibit poor controllability and robustness, which do not scale with
increasing model size. As a solution, we propose a novel method - Knowledge
Aware FineTuning (KAFT) - to strengthen both controllability and robustness by
incorporating counterfactual and irrelevant contexts to standard supervised
datasets. Our comprehensive evaluation showcases the utility of KAFT across
model architectures and sizes.",0,0,0,0,0,0,0.247393,6.0,0.522088,51
9f855844-d019-4f07-ba7d-637e8f072053,Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning,165,0.379683,0.94417,"We present Bit Diffusion: a simple and generic approach for generating
discrete data with continuous state and continuous time diffusion models. The
main idea behind our approach is to first represent the discrete data as binary
bits, and then train a continuous diffusion model to model these bits as real
numbers which we call analog bits. To generate samples, the model first
generates the analog bits, which are then thresholded to obtain the bits that
represent the discrete variables. We further propose two simple techniques,
namely Self-Conditioning and Asymmetric Time Intervals, which lead to a
significant improvement in sample quality. Despite its simplicity, the proposed
approach can achieve strong performance in both discrete image generation and
image captioning tasks. For discrete image generation, we significantly improve
previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens)
and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the
best autoregressive model in both sample quality (measured by FID) and
efficiency. For image captioning on MS-COCO dataset, our approach achieves
competitive results compared to autoregressive models.",1,0,0,0,0,0,0.586151,7.0,0.752184,68
2244d0b8-52dd-44b5-bb26-22d591b639c0,Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021,3,0.0298213,0.19339,"In this report, we present the technical details of our approach to the
EPIC-KITCHENS-100 Unsupervised Domain Adaptation (UDA) Challenge for Action
Recognition. The EPIC-KITCHENS-100 dataset consists of daily kitchen activities
focusing on the interaction between human hands and their surrounding objects.
It is very challenging to accurately recognize these fine-grained activities,
due to the presence of distracting objects and visually similar action classes,
especially in the unlabelled target domain. Based on an existing method for
video domain adaptation, i.e., TA3N, we propose to learn hand-centric features
by leveraging the hand bounding box information for UDA on fine-grained action
recognition. This helps reduce the distraction from background as well as
facilitate the learning of domain-invariant features. To achieve high quality
hand localization, we adopt an uncertainty-aware domain adaptation network,
i.e., MEAA, to train a domain-adaptive hand detector, which only uses very
limited hand bounding box annotations in the source domain but can generalize
well to the unlabelled target domain. Our submission achieved the 1st place in
terms of top-1 action recognition accuracy, using only RGB and optical flow
modalities as input.",0,1,0,0,1,0,0.213237,6.0,0.493789,8
2148fc1e-5faa-4c4c-9bdf-fc7423628dbd,EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points,18,0.328962,0.58224,"Neural radiance fields (NeRF) achieve highly photo-realistic novel-view
synthesis, but it's a challenging problem to edit the scenes modeled by
NeRF-based methods, especially for dynamic scenes. We propose editable neural
radiance fields that enable end-users to easily edit dynamic scenes and even
support topological changes. Input with an image sequence from a single camera,
our network is trained fully automatically and models topologically varying
dynamics using our picked-out surface key points. Then end-users can edit the
scene by easily dragging the key points to desired new positions. To achieve
this, we propose a scene analysis method to detect and initialize key points by
considering the dynamics in the scene, and a weighted key points strategy to
model topologically varying dynamics by joint key points and weights
optimization. Our method supports intuitive multi-dimensional (up to 3D)
editing and can generate novel scenes that are unseen in the input sequence.
Experiments demonstrate that our method achieves high-quality editing on
various dynamic scenes and outperforms the state-of-the-art. Our code and
captured data are available at https://chengwei-zheng.github.io/EditableNeRF/.",1,1,1,0,1,0,0.958094,4.0,0.886257,64
0c6dfb0c-8de0-411b-9827-da11ff916ef3,LiDAR-MIMO: Efficient Uncertainty Estimation for LiDAR-based 3D Object Detection,2,0.0218812,0.161925,"The estimation of uncertainty in robotic vision, such as 3D object detection,
is an essential component in developing safe autonomous systems aware of their
own performance. However, the deployment of current uncertainty estimation
methods in 3D object detection remains challenging due to timing and
computational constraints. To tackle this issue, we propose LiDAR-MIMO, an
adaptation of the multi-input multi-output (MIMO) uncertainty estimation method
to the LiDAR-based 3D object detection task. Our method modifies the original
MIMO by performing multi-input at the feature level to ensure the detection,
uncertainty estimation, and runtime performance benefits are retained despite
the limited capacity of the underlying detector and the large computational
costs of point cloud processing. We compare LiDAR-MIMO with MC dropout and
ensembles as baselines and show comparable uncertainty estimation results with
only a small number of output heads. Further, LiDAR-MIMO can be configured to
be twice as fast as MC dropout and ensembles, while achieving higher mAP than
MC dropout and approaching that of ensembles.",1,1,1,0,0,0,0.780763,9.0,0.867521,48
f7c5d775-c6c4-47d1-88f7-9ba1752ffb48,Privacy-Preserving Model Upgrades with Bidirectional Compatible Training in Image Retrieval,7,0.0747047,0.447372,"The task of privacy-preserving model upgrades in image retrieval desires to
reap the benefits of rapidly evolving new models without accessing the raw
gallery images. A pioneering work introduced backward-compatible training,
where the new model can be directly deployed in a backfill-free manner, i.e.,
the new query can be directly compared to the old gallery features. Despite a
possible solution, its improvement in sequential model upgrades is gradually
limited by the fixed and under-quality old gallery embeddings. To this end, we
propose a new model upgrade paradigm, termed Bidirectional Compatible Training
(BiCT), which will upgrade the old gallery embeddings by forward-compatible
training towards the embedding space of the backward-compatible new model. We
conduct comprehensive experiments to verify the prominent improvement by BiCT
and interestingly observe that the inconspicuous loss weight of backward
compatibility actually plays an essential role for both backward and forward
retrieval performance. To summarize, we introduce a new and valuable problem
named privacy-preserving model upgrades, with a proper solution BiCT. Several
intriguing insights are further proposed to get the most out of our method.",0,1,1,0,0,0,0.451146,6.0,0.646601,40
8572ca64-f63d-4a9d-a2c3-3a7144bdc339,Policy Architectures for Compositional Generalization in Control,14,0.193134,0.320262,"Many tasks in control, robotics, and planning can be specified using desired
goal configurations for various entities in the environment. Learning
goal-conditioned policies is a natural paradigm to solve such tasks. However,
current approaches struggle to learn and generalize as task complexity
increases, such as variations in number of environment entities or compositions
of goals. In this work, we introduce a framework for modeling entity-based
compositional structure in tasks, and create suitable policy designs that can
leverage this structure. Our policies, which utilize architectures like Deep
Sets and Self Attention, are flexible and can be trained end-to-end without
requiring any action primitives. When trained using standard reinforcement and
imitation learning methods on a suite of simulated robot manipulation tasks, we
find that these architectures achieve significantly higher success rates with
less data. We also find these architectures enable broader and compositional
generalization, producing policies that extrapolate to different numbers of
entities than seen in training, and stitch together (i.e. compose) learned
skills in novel ways. Videos of the results can be found at
https://sites.google.com/view/comp-gen-rl.",1,0,0,0,0,0,0.848439,7.0,0.860775,39
492c209f-9785-4996-897b-ff5bdb1edb31,Transformer Embeddings of Irregularly Spaced Events and Their Participants,32,0.137812,0.658745,"The neural Hawkes process (Mei & Eisner, 2017) is a generative model of
irregularly spaced sequences of discrete events. To handle complex domains with
many event types, Mei et al. (2020a) further consider a setting in which each
event in the sequence updates a deductive database of facts (via
domain-specific pattern-matching rules); future events are then conditioned on
the database contents. They show how to convert such a symbolic system into a
neuro-symbolic continuous-time generative model, in which each database fact
and the possible event has a time-varying embedding that is derived from its
symbolic provenance.
  In this paper, we modify both models, replacing their recurrent LSTM-based
architectures with flatter attention-based architectures (Vaswani et al.,
2017), which are simpler and more parallelizable. This does not appear to hurt
our accuracy, which is comparable to or better than that of the original models
as well as (where applicable) previous attention-based methods (Zuo et al.,
2020; Zhang et al., 2020a).",1,0,0,0,0,0,0.112959,9.0,0.585463,51
753871d4-6984-46ae-a7e4-e50b9fd2ee73,Unsupervised Domain Adaptive Salient Object Detection Through Uncertainty-Aware Pseudo-Label Learning,16,0.177103,0.707335,"Recent advances in deep learning significantly boost the performance of
salient object detection (SOD) at the expense of labeling larger-scale
per-pixel annotations. To relieve the burden of labor-intensive labeling, deep
unsupervised SOD methods have been proposed to exploit noisy labels generated
by handcrafted saliency methods. However, it is still difficult to learn
accurate saliency details from rough noisy labels. In this paper, we propose to
learn saliency from synthetic but clean labels, which naturally has higher
pixel-labeling quality without the effort of manual annotations. Specifically,
we first construct a novel synthetic SOD dataset by a simple copy-paste
strategy. Considering the large appearance differences between the synthetic
and real-world scenarios, directly training with synthetic data will lead to
performance degradation on real-world scenarios. To mitigate this problem, we
propose a novel unsupervised domain adaptive SOD method to adapt between these
two domains by uncertainty-aware self-training. Experimental results show that
our proposed method outperforms the existing state-of-the-art deep unsupervised
SOD methods on several benchmark datasets, and is even comparable to
fully-supervised ones.",0,1,0,1,1,0,0.869284,8.0,0.887615,49
b18bf06b-037c-4658-9002-7b1b00a32867,Receding Horizon Inverse Reinforcement Learning,6,0.414607,0.486583,"Inverse reinforcement learning (IRL) seeks to infer a cost function that
explains the underlying goals and preferences of expert demonstrations. This
paper presents receding horizon inverse reinforcement learning (RHIRL), a new
IRL algorithm for high-dimensional, noisy, continuous systems with black-box
dynamic models. RHIRL addresses two key challenges of IRL: scalability and
robustness. To handle high-dimensional continuous systems, RHIRL matches the
induced optimal trajectories with expert demonstrations locally in a receding
horizon manner and 'stitches' together the local solutions to learn the cost;
it thereby avoids the 'curse of dimensionality'. This contrasts sharply with
earlier algorithms that match with expert demonstrations globally over the
entire high-dimensional state space. To be robust against imperfect expert
demonstrations and control noise, RHIRL learns a state-dependent cost function
'disentangled' from system dynamics under mild conditions. Experiments on
benchmark tasks show that RHIRL outperforms several leading IRL algorithms in
most instances. We also prove that the cumulative error of RHIRL grows linearly
with the task duration.",0,0,0,0,1,0,0.986125,11.0,0.985815,49
642d3d3c-8491-4382-8327-e05236e86e0f,Mixture of basis for interpretable continual learning with distribution shifts,1,0.00892971,0.0199185,"Continual learning in environments with shifting data distributions is a
challenging problem with several real-world applications. In this paper we
consider settings in which the data distribution(task) shifts abruptly and the
timing of these shifts are not known. Furthermore, we consider a
semi-supervised task-agnostic setting in which the learning algorithm has
access to both task-segmented and unsegmented data for offline training. We
propose a novel approach called mixture of Basismodels (MoB) for addressing
this problem setting. The core idea is to learn a small set of basis models and
to construct a dynamic, task-dependent mixture of the models to predict for the
current task. We also propose a new methodology to detect observations that are
out-of-distribution with respect to the existing basis models and to
instantiate new models as needed. We test our approach in multiple domains and
show that it attains better prediction error than existing methods in most
cases while using fewer models than other multiple model approaches. Moreover,
we analyze the latent task representations learned by MoB and show that similar
tasks tend to cluster in the latent space and that the latent representation
shifts at the task boundaries when tasks are dissimilar.",0,0,0,0,1,0,0.657345,7.0,0.779883,21
94ccbc76-0588-45f9-889e-0f5469efc745,DProQ: A Gated-Graph Transformer for Protein Complex Structure Assessment,5,0.0592736,0.342773,"Proteins interact to form complexes to carry out essential biological
functions. Computational methods have been developed to predict the structures
of protein complexes. However, an important challenge in protein complex
structure prediction is to estimate the quality of predicted protein complex
structures without any knowledge of the corresponding native structures. Such
estimations can then be used to select high-quality predicted complex
structures to facilitate biomedical research such as protein function analysis
and drug discovery. We challenge this significant task with DProQ, which
introduces a gated neighborhood-modulating Graph Transformer (GGT) designed to
predict the quality of 3D protein complex structures. Notably, we incorporate
node and edge gates within a novel Graph Transformer framework to control
information flow during graph message passing. We train and evaluate DProQ on
four newly-developed datasets that we make publicly available in this work. Our
rigorous experiments demonstrate that DProQ achieves state-of-the-art
performance in ranking protein complex structures.",1,1,0,1,1,0,0.360304,8.0,0.698104,68
52626d56-dccf-4675-9b34-05ef09f4b292,SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation,9,0.077303,0.425696,"Conventional point cloud semantic segmentation methods usually employ an
encoder-decoder architecture, where mid-level features are locally aggregated
to extract geometric information. However, the over-reliance on these
class-agnostic local geometric representations may raise confusion between
local parts from different categories that are similar in appearance or
spatially adjacent. To address this issue, we argue that mid-level features can
be further enhanced with semantic information, and propose semantic-affine
transformation that transforms features of mid-level points belonging to
different categories with class-specific affine parameters. Based on this
technique, we propose SemAffiNet for point cloud semantic segmentation, which
utilizes the attention mechanism in the Transformer module to implicitly and
explicitly capture global structural knowledge within local parts for overall
comprehension of each category. We conduct extensive experiments on the
ScanNetV2 and NYUv2 datasets, and evaluate semantic-affine transformation on
various 3D point cloud and 2D image segmentation baselines, where both
qualitative and quantitative results demonstrate the superiority and
generalization ability of our proposed approach. Code is available at
https://github.com/wangzy22/SemAffiNet.",1,0,0,0,1,0,0.830505,7.0,0.852046,74
b1ad5f2a-57e2-4382-955d-4ba318ef7b0b,Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users,5,0.1105,0.678684,"Machine-learned models for author profiling in social media often rely on
data acquired via self-reporting-based psychometric tests (questionnaires)
filled out by social media users. This is an expensive but accurate data
collection strategy. Another, less costly alternative, which leads to
potentially more noisy and biased data, is to rely on labels inferred from
publicly available information in the profiles of the users, for instance
self-reported diagnoses or test results. In this paper, we explore a third
strategy, namely to directly use a corpus of items from validated psychometric
tests as training data. Items from psychometric tests often consist of
sentences from an I-perspective (e.g., ""I make friends easily.""). Such corpora
of test items constitute 'small data', but their availability for many concepts
is a rich resource. We investigate this approach for personality profiling, and
evaluate BERT classifiers fine-tuned on such psychometric test items for the
big five personality traits (openness, conscientiousness, extraversion,
agreeableness, neuroticism) and analyze various augmentation strategies
regarding their potential to address the challenges coming with such a small
corpus. Our evaluation on a publicly available Twitter corpus shows a
comparable performance to in-domain training for 4/5 personality traits with
T5-based data augmentation.",0,1,0,1,0,0,0.836988,9.0,0.88734,31
de9c1f03-5319-4c8e-b063-fc1d6de3680a,Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation,3,0.0241485,0.507519,"Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in
Mandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest
challenges is the task of polyphone disambiguation. Most of the previous
polyphone disambiguation models are trained on manually annotated datasets, and
publicly available datasets for polyphone disambiguation are scarce. In this
paper we propose a simple back-translation-style data augmentation method for
mandarin Chinese polyphone disambiguation, utilizing a large amount of
unlabeled text data. Inspired by the back-translation technique proposed in the
field of machine translation, we build a Grapheme-to-Phoneme (G2P) model to
predict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme
(P2G) model to predict pronunciation into text. Meanwhile, a window-based
matching strategy and a multi-model scoring strategy are proposed to judge the
correctness of the pseudo-label. We design a data balance strategy to improve
the accuracy of some typical polyphonic characters in the training set with
imbalanced distribution or data scarcity. The experimental result shows the
effectiveness of the proposed back-translation-style data augmentation method.",0,1,0,0,0,0,0.0569977,7.0,0.365002,26
5780960a-ca79-47dc-9029-299d4194267d,RAILD: Towards Leveraging Relation Features for Inductive Link Prediction In Knowledge Graphs,9,0.0697357,0.689223,"Due to the open world assumption, Knowledge Graphs (KGs) are never complete.
In order to address this issue, various Link Prediction (LP) methods are
proposed so far. Some of these methods are inductive LP models which are
capable of learning representations for entities not seen during training.
However, to the best of our knowledge, none of the existing inductive LP models
focus on learning representations for unseen relations. In this work, a novel
Relation Aware Inductive Link preDiction (RAILD) is proposed for KG completion
which learns representations for both unseen entities and unseen relations. In
addition to leveraging textual literals associated with both entities and
relations by employing language models, RAILD also introduces a novel
graph-based approach to generate features for relations. Experiments are
conducted with different existing and newly created challenging benchmark
datasets and the results indicate that RAILD leads to performance improvement
over the state-of-the-art models. Moreover, since there are no existing
inductive LP models which learn representations for unseen relations, we have
created our own baselines and the results obtained with RAILD also outperform
these baselines.",0,1,0,0,1,0,0.461818,10.0,0.791181,32
05789d07-7cf4-4499-b66f-ccfdb3a3915d,Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,68,0.26318,0.992143,"Human evaluation is the foundation upon which the evaluation of both
summarization systems and automatic metrics rests. However, existing human
evaluation studies for summarization either exhibit a low inter-annotator
agreement or have insufficient scale, and an in-depth analysis of human
evaluation is lacking. Therefore, we address the shortcomings of existing
summarization evaluation along the following axes: (1) We propose a modified
summarization salience protocol, Atomic Content Units (ACUs), which is based on
fine-grained semantic units and allows for a high inter-annotator agreement.
(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large
human evaluation dataset consisting of 22,000 summary-level annotations over 28
top-performing systems on three datasets. (3) We conduct a comparative study of
four human evaluation protocols, underscoring potential confounding factors in
evaluation setups. (4) We evaluate 50 automatic metrics and their variants
using the collected human annotations across evaluation protocols and
demonstrate how our benchmark leads to more statistically stable and
significant results. The metrics we benchmarked include recent methods based on
large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings
have important implications for evaluating LLMs, as we show that LLMs adjusted
by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,
which is affected by the annotators' prior, input-agnostic preferences, calling
for more robust, targeted evaluation methods.",0,0,0,1,0,0,0.365074,5.0,0.520289,114
c1c5a5d5-3fa3-475b-92fe-1b348fbfe232,EC-KitY: Evolutionary Computation Tool Kit in Python with Seamless Machine Learning Integration,7,0.0122148,0.553022,"EC-KitY is a comprehensive Python library for doing evolutionary computation
(EC), licensed under the BSD 3-Clause License, and compatible with
scikit-learn. Designed with modern software engineering and machine learning
integration in mind, EC-KitY can support all popular EC paradigms, including
genetic algorithms, genetic programming, coevolution, evolutionary
multi-objective optimization, and more. This paper provides an overview of the
package, including the ease of setting up an EC experiment, the architecture,
the main features, and a comparison with other libraries.",0,1,0,0,0,0,0.0212111,7.0,0.221147,18
54715a7b-a30f-4adb-9056-81143c0ef40d,Learning to Ask Like a Physician,13,0.214627,0.378173,"Existing question answering (QA) datasets derived from electronic health
records (EHR) are artificially generated and consequently fail to capture
realistic physician information needs. We present Discharge Summary Clinical
Questions (DiSCQ), a newly curated question dataset composed of 2,000+
questions paired with the snippets of text (triggers) that prompted each
question. The questions are generated by medical experts from 100+ MIMIC-III
discharge summaries. We analyze this dataset to characterize the types of
information sought by medical experts. We also train baseline models for
trigger detection and question generation (QG), paired with unsupervised answer
retrieval over EHRs. Our baseline model is able to generate high quality
questions in over 62% of cases when prompted with human selected triggers. We
release this dataset (and all code to reproduce baseline model results) to
facilitate further research into realistic clinical QA and QG:
https://github.com/elehman16/discq.",0,1,1,1,0,0,0.704598,6.0,0.764823,72
bd2330fe-80ba-424b-a383-ef7b38754e55,Dynamic Proposals for Efficient Object Detection,12,0.103712,0.524266,"Object detection is a basic computer vision task to loccalize and categorize
objects in a given image. Most state-of-the-art detection methods utilize a
fixed number of proposals as an intermediate representation of object
candidates, which is unable to adapt to different computational constraints
during inference. In this paper, we propose a simple yet effective method which
is adaptive to different computational resources by generating dynamic
proposals for object detection. We first design a module to make a single
query-based model to be able to inference with different numbers of proposals.
Further, we extend it to a dynamic model to choose the number of proposals
according to the input image, greatly reducing computational costs. Our method
achieves significant speed-up across a wide range of detection models including
two-stage and query-based models while obtaining similar or even better
accuracy.",0,0,0,0,0,0,0.898869,6.0,0.86995,38
85c37304-4c83-4264-a797-a136c478d046,Video Prediction by Efficient Transformers,14,0.099617,0.830643,"Video prediction is a challenging computer vision task that has a wide range
of applications. In this work, we present a new family of Transformer-based
models for video prediction. Firstly, an efficient local spatial-temporal
separation attention mechanism is proposed to reduce the complexity of standard
Transformers. Then, a full autoregressive model, a partial autoregressive model
and a non-autoregressive model are developed based on the new efficient
Transformer. The partial autoregressive model has a similar performance with
the full autoregressive model but a faster inference speed. The
non-autoregressive model not only achieves a faster inference speed but also
mitigates the quality degradation problem of the autoregressive counterparts,
but it requires additional parameters and loss function for learning. Given the
same attention mechanism, we conducted a comprehensive study to compare the
proposed three video prediction variants. Experiments show that the proposed
video prediction models are competitive with more complex state-of-the-art
convolutional-LSTM based models. The source code is available at
https://github.com/XiYe20/VPTR.",1,1,0,0,1,0,0.250759,9.0,0.683131,62
c08c74ad-aeab-4d59-adb9-02bc8ada192e,Bayes risk CTC: Controllable CTC alignment in Sequence-to-Sequence tasks,7,0.146295,0.793711,"Sequence-to-Sequence (seq2seq) tasks transcribe the input sequence to a
target sequence. The Connectionist Temporal Classification (CTC) criterion is
widely used in multiple seq2seq tasks. Besides predicting the target sequence,
a side product of CTC is to predict the alignment, which is the most probable
input-long sequence that specifies a hard aligning relationship between the
input and target units. As there are multiple potential aligning sequences
(called paths) that are equally considered in CTC formulation, the choice of
which path will be most probable and become the predicted alignment is always
uncertain. In addition, it is usually observed that the alignment predicted by
vanilla CTC will drift compared with its reference and rarely provides
practical functionalities. Thus, the motivation of this work is to make the CTC
alignment prediction controllable and thus equip CTC with extra
functionalities. The Bayes risk CTC (BRCTC) criterion is then proposed in this
work, in which a customizable Bayes risk function is adopted to enforce the
desired characteristics of the predicted alignment. With the risk function, the
BRCTC is a general framework to adopt some customizable preference over the
paths in order to concentrate the posterior into a particular subset of the
paths. In applications, we explore one particular preference which yields
models with the down-sampling ability and reduced inference costs. By using
BRCTC with another preference for early emissions, we obtain an improved
performance-latency trade-off for online models. Experimentally, the proposed
BRCTC reduces the inference cost of offline models by up to 47% without
performance degradation and cuts down the overall latency of online systems to
an unseen level.",1,0,0,0,0,1,0.638757,6.0,0.734767,60
1ebc493b-eb6c-443e-8120-d664899bc65c,Let it RAIN for Social Good,1,0.0193665,0.129541,"Artificial Intelligence (AI) as a highly transformative technology take on a
special role as both an enabler and a threat to UN Sustainable Development
Goals (SDGs). AI Ethics and emerging high-level policy efforts stand at the
pivot point between these outcomes but is barred from effect due the
abstraction gap between high-level values and responsible action. In this paper
the Responsible Norms (RAIN) framework is presented, bridging this gap thereby
enabling effective high-level control of AI impact. With effective and
operationalized AI Ethics, AI technologies can be directed towards global
sustainable development.",0,0,0,0,0,0,0.826906,5.0,0.790483,15
f8d12ffa-a34d-4bae-81a1-c340930ea103,Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance,9,0.43986,0.937823,"Human-translated text displays distinct features from naturally written text
in the same language. This phenomena, known as translationese, has been argued
to confound the machine translation (MT) evaluation. Yet, we find that existing
work on translationese neglects some important factors and the conclusions are
mostly correlational but not causal. In this work, we collect CausalMT, a
dataset where the MT training data are also labeled with the human translation
directions. We inspect two critical factors, the train-test direction match
(whether the human translation directions in the training and test sets are
aligned), and data-model direction match (whether the model learns in the same
direction as the human translation direction in the dataset). We show that
these two factors have a large causal effect on the MT performance, in addition
to the test-model direction mismatch highlighted by existing work on the impact
of translationese. In light of our findings, we provide a set of suggestions
for MT training and evaluation. Our code and data are at
https://github.com/EdisonNi-hku/CausalMT",0,0,0,1,0,0,0.947395,8.0,0.933831,80
03d6f023-1a87-4c25-b571-09062fe96783,INSPIRED2: An Improved Dataset for Sociable Conversational Recommendation,3,0.0954015,0.129916,"Conversational recommender systems (CRS) that are able to interact with users
in natural language often utilize recommendation dialogs which were previously
collected with the help of paired humans, where one plays the role of a seeker
and the other as a recommender. These recommendation dialogs include items and
entities that indicate the users' preferences. In order to precisely model the
seekers' preferences and respond consistently, CRS typically rely on item and
entity annotations. A recent example of such a dataset is INSPIRED, which
consists of recommendation dialogs for sociable conversational recommendation,
where items and entities were annotated using automatic keyword or pattern
matching techniques. An analysis of this dataset unfortunately revealed that
there is a substantial number of cases where items and entities were either
wrongly annotated or annotations were missing at all. This leads to the
question to what extent automatic techniques for annotations are effective.
Moreover, it is important to study impact of annotation quality on the overall
effectiveness of a CRS in terms of the quality of the system's responses. To
study these aspects, we manually fixed the annotations in INSPIRED. We then
evaluated the performance of several benchmark CRS using both versions of the
dataset. Our analyses suggest that the improved version of the dataset, i.e.,
INSPIRED2, helped increase the performance of several benchmark CRS,
emphasizing the importance of data quality both for end-to-end learning and
retrieval-based approaches to conversational recommendation. We release our
improved dataset (INSPIRED2) publicly at
https://github.com/ahtsham58/INSPIRED2.",1,1,0,1,0,0,0.822142,4.0,0.734203,43
e4886392-ee39-475a-bdc1-a7442e3d145b,Physics-aware Differentiable Discrete Codesign for Diffractive Optical Neural Networks,8,0.220592,0.733773,"Diffractive optical neural networks (DONNs) have attracted lots of attention
as they bring significant advantages in terms of power efficiency, parallelism,
and computational speed compared with conventional deep neural networks (DNNs),
which have intrinsic limitations when implemented on digital platforms.
However, inversely mapping algorithm-trained physical model parameters onto
real-world optical devices with discrete values is a non-trivial task as
existing optical devices have non-unified discrete levels and non-monotonic
properties. This work proposes a novel device-to-system hardware-software
codesign framework, which enables efficient physics-aware training of DONNs
w.r.t arbitrary experimental measured optical devices across layers.
Specifically, Gumbel-Softmax is employed to enable differentiable discrete
mapping from real-world device parameters into the forward function of DONNs,
where the physical parameters in DONNs can be trained by simply minimizing the
loss function of the ML task. The results have demonstrated that our proposed
framework offers significant advantages over conventional quantization-based
methods, especially with low-precision optical devices. Finally, the proposed
algorithm is fully verified with physical experimental optical systems in
low-precision settings.",0,1,0,0,0,0,0.623561,7.0,0.766756,45
f77e582d-93fe-40f4-8ed1-50abf4772655,Watermarking Pre-trained Encoders in Contrastive Learning,7,0.0625449,0.473558,"Contrastive learning has become a popular technique to pre-train image
encoders, which could be used to build various downstream classification models
in an efficient way. This process requires a large amount of data and
computation resources. Hence, the pre-trained encoders are an important
intellectual property that needs to be carefully protected. It is challenging
to migrate existing watermarking techniques from the classification tasks to
the contrastive learning scenario, as the owner of the encoder lacks the
knowledge of the downstream tasks which will be developed from the encoder in
the future. We propose the \textit{first} watermarking methodology for the
pre-trained encoders. We introduce a task-agnostic loss function to effectively
embed into the encoder a backdoor as the watermark. This backdoor can still
exist in any downstream models transferred from the encoder. Extensive
evaluations over different contrastive learning algorithms, datasets, and
downstream tasks indicate our watermarks exhibit high effectiveness and
robustness against different adversarial operations.",1,1,1,0,0,0,0.67426,5.0,0.701074,28
5faaccf1-51c0-4a94-91b9-f37777d08cee,AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation,6,0.0389684,0.665693,"Dense retrievers have made significant strides in text retrieval and
open-domain question answering, even though most achievements were made
possible only with large amounts of human supervision. In this work, we aim to
develop unsupervised methods by proposing two methods that create pseudo
query-document pairs and train dense retrieval models in an annotation-free and
scalable manner: query extraction and transferred query generation. The former
method produces pseudo queries by selecting salient spans from the original
document. The latter utilizes generation models trained for other NLP tasks
(e.g., summarization) to produce pseudo queries. Extensive experiments show
that models trained with the proposed augmentation methods can perform
comparably well (or better) to multiple strong baselines. Combining those
strategies leads to further improvements, achieving the state-of-the-art
performance of unsupervised dense retrieval on both BEIR and ODQA datasets.",0,1,0,1,1,0,0.549226,4.0,0.540849,45
d52993a1-9859-49ee-b8b7-50a05e91b305,Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions,11,0.0732393,0.483654,"Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural
Networks (NNs) for tasks requiring perception and reasoning. Most NeSy systems
rely on continuous relaxation of logical knowledge, and no discrete decisions
are made within the model pipeline. Furthermore, these methods assume that the
symbolic rules are given. In this paper, we propose Deep Symbolic Learning
(DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a
(set of) perception functions which map continuous data to discrete symbols,
and a symbolic function over the set of symbols. DSL learns simultaneously the
perception and symbolic functions while being trained only on their composition
(NeSy-function). The key novelty of DSL is that it can create internal
(interpretable) symbolic representations and map them to perception inputs
within a differentiable NN learning pipeline. The created symbols are
automatically selected to generate symbolic functions that best explain the
data. We provide experimental analysis to substantiate the efficacy of DSL in
simultaneously learning perception and symbolic functions.",0,0,1,0,0,0,0.12743,9.0,0.599751,32
2b3726f0-7b0e-4e91-85b7-332e24e504a1,Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding,8,0.0355256,0.518345,"ASR systems designed for native English (L1) usually underperform on
non-native English (L2). To address this performance gap, \textbf{(i)} we
extend our previous work to investigate fine-tuning of a pre-trained wav2vec
2.0 model \cite{baevski2020wav2vec,xu2021self} under a rich set of L1 and L2
training conditions. We further \textbf{(ii)} incorporate language model
decoding in the ASR system, along with the fine-tuning method. Quantifying
gains acquired from each of these two approaches separately and an error
analysis allows us to identify different sources of improvement within our
models. We find that while the large self-trained wav2vec 2.0 may be
internalizing sufficient decoding knowledge for clean L1 speech
\cite{xu2021self}, this does not hold for L2 speech and accounts for the
utility of employing language model decoding on L2 data.",0,1,0,0,0,0,0.264075,6.0,0.53474,63
a5169f59-fae1-41cb-85ef-b2a60134bdee,Image Classification with Small Datasets: Overview and Benchmark,7,0.0222034,0.649493,"Image classification with small datasets has been an active research area in
the recent past. However, as research in this scope is still in its infancy,
two key ingredients are missing for ensuring reliable and truthful progress: a
systematic and extensive overview of the state of the art, and a common
benchmark to allow for objective comparisons between published methods. This
article addresses both issues. First, we systematically organize and connect
past studies to consolidate a community that is currently fragmented and
scattered. Second, we propose a common benchmark that allows for an objective
comparison of approaches. It consists of five datasets spanning various domains
(e.g., natural images, medical imagery, satellite data) and data types (RGB,
grayscale, multispectral). We use this benchmark to re-evaluate the standard
cross-entropy baseline and ten existing methods published between 2017 and 2021
at renowned venues. Surprisingly, we find that thorough hyper-parameter tuning
on held-out validation data results in a highly competitive baseline and
highlights a stunted growth of performance over the years. Indeed, only a
single specialized method dating back to 2019 clearly wins our benchmark and
outperforms the baseline classifier.",1,1,0,1,0,0,0.162496,8.0,0.582602,69
0956de76-1270-46b3-be12-acd1714e6db3,Towards customizable reinforcement learning agents: Enabling preference specification through online vocabulary expansion,5,0.13764,0.291826,"There is a growing interest in developing automated agents that can work
alongside humans. In addition to completing the assigned task, such an agent
will undoubtedly be expected to behave in a manner that is preferred by the
human. This requires the human to communicate their preferences to the agent.
To achieve this, the current approaches either require the users to specify the
reward function or the preference is interactively learned from queries that
ask the user to compare behavior. The former approach can be challenging if the
internal representation used by the agent is inscrutable to the human while the
latter is unnecessarily cumbersome for the user if their preference can be
specified more easily in symbolic terms. In this work, we propose PRESCA
(PREference Specification through Concept Acquisition), a system that allows
users to specify their preferences in terms of concepts that they understand.
PRESCA maintains a set of such concepts in a shared vocabulary. If the relevant
concept is not in the shared vocabulary, then it is learned. To make learning a
new concept more feedback efficient, PRESCA leverages causal associations
between the target concept and concepts that are already known. In addition, we
use a novel data augmentation approach to further reduce required feedback. We
evaluate PRESCA by using it on a Minecraft environment and show that it can
effectively align the agent with the user's preference.",0,1,0,0,0,0,0.813713,10.0,0.890963,42
975f6083-99fe-4377-a00c-2715a6d93f72,Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation,30,0.885199,0.928404,"Despite recent advances in detecting fake news generated by neural models,
their results are not readily applicable to effective detection of
human-written disinformation. What limits the successful transfer between them
is the sizable gap between machine-generated fake news and human-authored ones,
including the notable differences in terms of style and underlying intent. With
this in mind, we propose a novel framework for generating training examples
that are informed by the known styles and strategies of human-authored
propaganda. Specifically, we perform self-critical sequence training guided by
natural language inference to ensure the validity of the generated articles,
while also incorporating propaganda techniques, such as appeal to authority and
loaded language. In particular, we create a new training dataset, PropaNews,
with 2,256 examples, which we release for future use. Our experimental results
show that fake news detectors trained on PropaNews are better at detecting
human-written disinformation by 3.62 - 7.69% F1 score on two public datasets.",1,1,0,1,0,0,0.952805,6.0,0.917807,61
a78f766e-c9a3-478c-a144-712635198936,Relational Attention: Generalizing Transformers for Graph-Structured Tasks,21,0.325079,0.412379,"Transformers flexibly operate over sets of real-valued vectors representing
task-specific entities and their attributes, where each vector might encode one
word-piece token and its position in a sequence, or some piece of information
that carries no position at all. But as set processors, transformers are at a
disadvantage in reasoning over more general graph-structured data where nodes
represent entities and edges represent relations between entities. To address
this shortcoming, we generalize transformer attention to consider and update
edge vectors in each transformer layer. We evaluate this relational transformer
on a diverse array of graph-structured tasks, including the large and
challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically
outperforms state-of-the-art graph neural networks expressly designed to reason
over graph-structured data. Our analysis demonstrates that these gains are
attributable to relational attention's inherent ability to leverage the greater
expressivity of graphs over sets.",0,0,0,0,1,0,0.88355,6.0,0.859361,67
ee1818bd-34df-4a9f-b0b4-196e1a388b8c,PHEE: A Dataset for Pharmacovigilance Event Extraction from Text,13,0.243827,0.951396,"The primary goal of drug safety researchers and regulators is to promptly
identify adverse drug reactions. Doing so may in turn prevent or reduce the
harm to patients and ultimately improve public health. Evaluating and
monitoring drug safety (i.e., pharmacovigilance) involves analyzing an ever
growing collection of spontaneous reports from health professionals,
physicians, and pharmacists, and information voluntarily submitted by patients.
In this scenario, facilitating analysis of such reports via automation has the
potential to rapidly identify safety signals. Unfortunately, public resources
for developing natural language models for this task are scant. We present
PHEE, a novel dataset for pharmacovigilance comprising over 5000 annotated
events from medical case reports and biomedical literature, making it the
largest such public dataset to date. We describe the hierarchical event schema
designed to provide coarse and fine-grained information about patients'
demographics, treatments and (side) effects. Along with the discussion of the
dataset, we present a thorough experimental evaluation of current
state-of-the-art approaches for biomedical event extraction, point out their
limitations, and highlight open challenges to foster future research in this
area.",1,1,1,1,0,0,0.318546,16.0,0.839513,48
57cce177-4e92-4c4e-b406-774e3cda46f1,Arbitrary-Scale Image Synthesis,19,0.102354,0.453418,"Positional encodings have enabled recent works to train a single adversarial
network that can generate images of different scales. However, these approaches
are either limited to a set of discrete scales or struggle to maintain good
perceptual quality at the scales for which the model is not trained explicitly.
We propose the design of scale-consistent positional encodings invariant to our
generator's layers transformations. This enables the generation of
arbitrary-scale images even at scales unseen during training. Moreover, we
incorporate novel inter-scale augmentations into our pipeline and partial
generation training to facilitate the synthesis of consistent images at
arbitrary scales. Lastly, we show competitive results for a continuum of scales
on various commonly used datasets for image synthesis.",0,0,0,0,0,0,0.556745,5.0,0.636858,48
c58397b0-f373-4556-9969-e7cb83bd7cc9,Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation,14,0.638058,0.922771,"Non-autoregressive translation (NAT) models are typically trained with the
cross-entropy loss, which forces the model outputs to be aligned verbatim with
the target sentence and will highly penalize small shifts in word positions.
Latent alignment models relax the explicit alignment by marginalizing out all
monotonic latent alignments with the CTC loss. However, they cannot handle
non-monotonic alignments, which is non-negligible as there is typically global
word reordering in machine translation. In this work, we explore non-monotonic
latent alignments for NAT. We extend the alignment space to non-monotonic
alignments to allow for the global word reordering and further consider all
alignments that overlap with the target sentence. We non-monotonically match
the alignments to the target sentence and train the latent alignment model to
maximize the F1 score of non-monotonic matching. Extensive experiments on major
WMT benchmarks show that our method substantially improves the translation
performance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14
En-De with only one-iteration decoding, closing the gap between
non-autoregressive and autoregressive models.",0,0,0,0,0,0,0.941958,7.0,0.919526,51
71f79d1e-32e3-48bf-bb23-ae3b191bd0f1,Evaluating Step-by-Step Reasoning through Symbolic Verification,6,0.130055,0.477801,"Pre-trained language models (LMs) have shown remarkable reasoning performance
using explanations or chain-of-thoughts (CoT)) for in-context learning. On the
other hand, these reasoning tasks are usually presumed to be more approachable
for symbolic programming. To understand the mechanism of reasoning of LMs, we
curate synthetic datasets containing equivalent (natural, symbolic) data pairs,
where symbolic examples contain first-order logic rules and predicates from
non-parametric knowledge bases (KBs), supporting automated verification of
intermediate reasoning results. Then we revisit neuro-symbolic approaches and
propose to learn from demonstrations containing logic rules and corresponding
examples to iteratively reason over KBs, recovering Prolog's backward chaining
algorithm and supporting automated verification of LMs' outputs. Comprehensive
experiments are included to systematically compare LMLP with CoT in deductive
reasoning settings, showing that LMLP enjoys more than $25\%$ higher accuracy
than CoT on length generalization benchmarks even with smaller model sizes.",0,0,0,0,0,0,0.944598,6.0,0.908817,70
72e3689a-37c7-433b-af3a-1b697c4ac6c3,Domain Adaptation meets Individual Fairness. And they get along,10,0.262283,0.540242,"Many instances of algorithmic bias are caused by distributional shifts. For
example, machine learning (ML) models often perform worse on demographic groups
that are underrepresented in the training data. In this paper, we leverage this
connection between algorithmic fairness and distribution shifts to show that
algorithmic fairness interventions can help ML models overcome distribution
shifts, and that domain adaptation methods (for overcoming distribution shifts)
can mitigate algorithmic biases. In particular, we show that (i) enforcing
suitable notions of individual fairness (IF) can improve the
out-of-distribution accuracy of ML models under the covariate shift assumption
and that (ii) it is possible to adapt representation alignment methods for
domain adaptation to enforce individual fairness. The former is unexpected
because IF interventions were not developed with distribution shifts in mind.
The latter is also unexpected because representation alignment is not a common
approach in the individual fairness literature.",0,0,0,0,0,0,0.947744,7.0,0.924701,58
de50c004-2471-45f1-9a72-d11beaf0eb4e,Acknowledging the Unknown for Multi-label Learning with Single Positive Labels,19,0.220311,0.90977,"Due to the difficulty of collecting exhaustive multi-label annotations,
multi-label datasets often contain partial labels. We consider an extreme of
this weakly supervised learning problem, called single positive multi-label
learning (SPML), where each multi-label training image has only one positive
label. Traditionally, all unannotated labels are assumed as negative labels in
SPML, which introduces false negative labels and causes model training to be
dominated by assumed negative labels. In this work, we choose to treat all
unannotated labels from an alternative perspective, i.e. acknowledging they are
unknown. Hence, we propose entropy-maximization (EM) loss to attain a special
gradient regime for providing proper supervision signals. Moreover, we propose
asymmetric pseudo-labeling (APL), which adopts asymmetric-tolerance strategies
and a self-paced procedure, to cooperate with EM loss and then provide more
precise supervision. Experiments show that our method significantly improves
performance and achieves state-of-the-art results on all four benchmarks. Code
is available at https://github.com/Correr-Zhou/SPML-AckTheUnknown.",1,1,1,0,1,0,0.688451,10.0,0.854431,61
91823b49-408f-40ab-951a-9169bd5482f0,Effective and Efficient Training for Sequential Recommendation using Recency Sampling,20,0.291011,0.675775,"Many modern sequential recommender systems use deep neural networks, which
can effectively estimate the relevance of items but require a lot of time to
train. Slow training increases expenses, hinders product development timescales
and prevents the model from being regularly updated to adapt to changing user
preferences. Training such sequential models involves appropriately sampling
past user interactions to create a realistic training objective. The existing
training objectives have limitations. For instance, next item prediction never
uses the beginning of the sequence as a learning target, thereby potentially
discarding valuable data. On the other hand, the item masking used by BERT4Rec
is only weakly related to the goal of the sequential recommendation; therefore,
it requires much more time to obtain an effective model. Hence, we propose a
novel Recency-based Sampling of Sequences training objective that addresses
both limitations. We apply our method to various recent and state-of-the-art
model architectures - such as GRU4Rec, Caser, and SASRec. We show that the
models enhanced with our method can achieve performances exceeding or very
close to stateof-the-art BERT4Rec, but with much less training time.",0,1,0,0,1,0,0.568551,5.0,0.643386,58
708a4ea8-967c-4ba0-8569-24e4d0ec8eb3,GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks,15,0.267911,0.438227,"Time series synthesis is an important research topic in the field of deep
learning, which can be used for data augmentation. Time series data types can
be broadly classified into regular or irregular. However, there are no existing
generative models that show good performance for both types without any model
changes. Therefore, we present a general purpose model capable of synthesizing
regular and irregular time series data. To our knowledge, we are the first
designing a general purpose time series synthesis model, which is one of the
most challenging settings for time series synthesis. To this end, we design a
generative adversarial network-based method, where many related techniques are
carefully integrated into a single framework, ranging from neural
ordinary/controlled differential equations to continuous time-flow processes.
Our method outperforms all existing methods.",1,0,1,0,1,0,0.811622,7.0,0.843281,43
a2db1e46-ac2c-4bfb-9ac2-925a9da89b3e,Towards Privacy-Preserving Person Re-identification via Person Identify Shift,4,0.162036,0.505474,"Recently privacy concerns of person re-identification (ReID) raise more and
more attention and preserving the privacy of the pedestrian images used by ReID
methods become essential. De-identification (DeID) methods alleviate privacy
issues by removing the identity-related of the ReID data. However, most of the
existing DeID methods tend to remove all personal identity-related information
and compromise the usability of de-identified data on the ReID task. In this
paper, we aim to develop a technique that can achieve a good trade-off between
privacy protection and data usability for person ReID. To achieve this, we
propose a novel de-identification method designed explicitly for person ReID,
named Person Identify Shift (PIS). PIS removes the absolute identity in a
pedestrian image while preserving the identity relationship between image
pairs. By exploiting the interpolation property of variational auto-encoder,
PIS shifts each pedestrian image from the current identity to another with a
new identity, resulting in images still preserving the relative identities.
Experimental results show that our method has a better trade-off between
privacy-preserving and model performance than existing de-identification
methods and can defend against human and model attacks for data privacy.",0,1,1,0,0,0,0.97671,12.0,0.976242,40
01ccc45c-2168-45e4-8051-58f1a40b8276,Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding,3,0.0178537,0.0417614,"In recent years, large pre-trained Transformer networks have demonstrated
dramatic improvements in many natural language understanding tasks. However,
the huge size of these models brings significant challenges to their
fine-tuning and online deployment due to latency and cost constraints. New
hardware supporting both N:M semi-structured sparsity and low-precision integer
computation is a promising solution to boost DNN model serving efficiency.
However, there have been very few studies that systematically investigate to
what extent pre-trained Transformer networks benefit from the combination of
these techniques, as well as how to best compress each component of the
Transformer. We propose a flexible compression framework NxMiFormer that
performs simultaneous sparsification and quantization using ADMM and STE-based
QAT. Furthermore, we present and inexpensive, heuristic-driven search algorithm
that identifies promising heterogeneous compression configurations that meet a
compression ratio constraint. When evaluated across the GLUE suite of NLU
benchmarks, our approach can achieve up to 93% compression of the encoders of a
BERT model while retaining 98.2% of the original model accuracy and taking full
advantage of the hardware's capabilities. Heterogeneous configurations found
the by the search heuristic maintain 99.5% of the baseline accuracy while still
compressing the model by 87.5%.",0,1,0,0,0,0,0.633168,6.0,0.732235,55
a94af317-7607-4251-bc60-b7e7fbf92c38,Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers,8,0.0360418,0.193283,"Large-scale transformer models have become the de-facto architectures for
various machine learning applications, e.g., CV and NLP. However, those large
models also introduce prohibitive training costs. To mitigate this issue, we
propose a novel random and layerwise token dropping method (random-LTD), which
skips the computation of a subset of the input tokens at all middle layers.
Particularly, random-LTD achieves considerable speedups and comparable accuracy
as the standard training baseline. Compared to other token dropping methods,
random-LTD does not require (1) any importance score-based metrics, (2) any
special token treatment (e.g., [CLS]), and (3) many layers in full sequence
length training except the first and the last layers. Besides, a new LayerToken
learning rate schedule is proposed for pretraining problems that resolve the
heavy tuning requirement for our proposed training mechanism. Finally, we
demonstrate that random-LTD can be applied to broader applications, including
GPT and BERT pretraining as well as ViT and GPT finetuning tasks. Our results
show that random-LTD can save about 33.3% theoretical compute cost and 25.6%
wall-clock training time while achieving similar zero-shot evaluations on
GPT-31.3B as compared to baseline.",0,1,0,0,0,0,0.460436,8.0,0.738458,58
7328f234-d6b5-47f4-8ae0-1f7245310fa1,MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition,24,0.335471,0.780982,"Unlike the conventional Knowledge Distillation (KD), Self-KD allows a network
to learn knowledge from itself without any guidance from extra networks. This
paper proposes to perform Self-KD from image Mixture (MixSKD), which integrates
these two techniques into a unified framework. MixSKD mutually distills feature
maps and probability distributions between the random pair of original images
and their mixup images in a meaningful way. Therefore, it guides the network to
learn cross-image knowledge by modelling supervisory signals from mixup images.
Moreover, we construct a self-teacher network by aggregating multi-stage
feature maps for providing soft labels to supervise the backbone classifier,
further improving the efficacy of self-boosting. Experiments on image
classification and transfer learning to object detection and semantic
segmentation demonstrate that MixSKD outperforms other state-of-the-art Self-KD
and data augmentation methods. The code is available at
https://github.com/winycg/Self-KD-Lib.",1,1,0,0,1,0,0.831342,7.0,0.852444,59
9cd56ee1-14ec-4013-84d7-6ea44dffdb1c,Sparse2Dense: Learning to Densify 3D Features for 3D Object Detection,6,0.036849,0.442082,"LiDAR-produced point clouds are the major source for most state-of-the-art 3D
object detectors. Yet, small, distant, and incomplete objects with sparse or
few points are often hard to detect. We present Sparse2Dense, a new framework
to efficiently boost 3D detection performance by learning to densify point
clouds in latent space. Specifically, we first train a dense point 3D detector
(DDet) with a dense point cloud as input and design a sparse point 3D detector
(SDet) with a regular point cloud as input. Importantly, we formulate the
lightweight plug-in S2D module and the point cloud reconstruction module in
SDet to densify 3D features and train SDet to produce 3D features, following
the dense 3D features in DDet. So, in inference, SDet can simulate dense 3D
features from regular (sparse) point cloud inputs without requiring dense
inputs. We evaluate our method on the large-scale Waymo Open Dataset and the
Waymo Domain Adaptation Dataset, showing its high performance and efficiency
over the state of the arts.",0,1,0,0,1,0,0.701729,5.0,0.716196,48
32b1c719-a847-4c8d-acd5-6c9c838f6f3a,Unsupervised Explanation Generation via Correct Instantiations,3,0.0371662,0.0450251,"While large pre-trained language models (PLM) have shown their great skills
at solving discriminative tasks, a significant gap remains when compared with
humans for explanation-related tasks. Among them, explaining the reason why a
statement is wrong (e.g., against commonsense) is incredibly challenging. The
major difficulty is finding the conflict point, where the statement contradicts
our real world. This paper proposes Neon, a two-phrase, unsupervised
explanation generation framework. Neon first generates corrected instantiations
of the statement (phase I), then uses them to prompt large PLMs to find the
conflict point and complete the explanation (phase II). We conduct extensive
experiments on two standard explanation benchmarks, i.e., ComVE and e-SNLI.
According to both automatic and human evaluations, Neon outperforms baselines,
even for those with human-annotated instantiations. In addition to explaining a
negative prediction, we further demonstrate that Neon remains effective when
generalizing to different scenarios.",0,1,0,0,0,0,0.758744,6.0,0.790425,49
b637242c-4d9c-482f-8d88-36b3869d9635,Urban feature analysis from aerial remote sensing imagery using self-supervised and semi-supervised computer vision,2,0.0525818,0.221126,"Analysis of overhead imagery using computer vision is a problem that has
received considerable attention in academic literature. Most techniques that
operate in this space are both highly specialised and require expensive manual
annotation of large datasets. These problems are addressed here through the
development of a more generic framework, incorporating advances in
representation learning which allows for more flexibility in analysing new
categories of imagery with limited labeled data. First, a robust representation
of an unlabeled aerial imagery dataset was created based on the momentum
contrast mechanism. This was subsequently specialised for different tasks by
building accurate classifiers with as few as 200 labeled images. The successful
low-level detection of urban infrastructure evolution over a 10-year period
from 60 million unlabeled images, exemplifies the substantial potential of our
approach to advance quantitative urban research.",0,1,0,0,0,0,0.45614,12.0,0.824561,60
4b82c0a7-ad7d-4b10-b3c2-1298df8d285e,Social Diversity Reduces the Complexity and Cost of Fostering Fairness,11,0.0,0.512968,"Institutions and investors are constantly faced with the challenge of
appropriately distributing endowments. No budget is limitless and optimising
overall spending without sacrificing positive outcomes has been approached and
resolved using several heuristics. To date, prior works have failed to consider
how to encourage fairness in a population where social diversity is ubiquitous,
and in which investors can only partially observe the population. Herein, by
incorporating social diversity in the Ultimatum game through heterogeneous
graphs, we investigate the effects of several interference mechanisms which
assume incomplete information and flexible standards of fairness. We quantify
the role of diversity and show how it reduces the need for information
gathering, allowing us to relax a strict, costly interference process.
Furthermore, we find that the influence of certain individuals, expressed by
different network centrality measures, can be exploited to further reduce
spending if minimal fairness requirements are lowered. Our results indicate
that diversity changes and opens up novel mechanisms available to institutions
wishing to promote fairness. Overall, our analysis provides novel insights to
guide institutional policies in socially diverse complex systems.",0,0,0,0,0,0,7.13432e-05,19.0,0.412768,54
5e71df5d-1b60-428c-a1a9-f7301af3923c,A Safety Assurable Human-Inspired Perception Architecture,1,0.032368,0.0627888,"Although artificial intelligence-based perception (AIP) using deep neural
networks (DNN) has achieved near human level performance, its well-known
limitations are obstacles to the safety assurance needed in autonomous
applications. These include vulnerability to adversarial inputs, inability to
handle novel inputs and non-interpretability. While research in addressing
these limitations is active, in this paper, we argue that a fundamentally
different approach is needed to address them. Inspired by dual process models
of human cognition, where Type 1 thinking is fast and non-conscious while Type
2 thinking is slow and based on conscious reasoning, we propose a dual process
architecture for safe AIP. We review research on how humans address the
simplest non-trivial perception problem, image classification, and sketch a
corresponding AIP architecture for this task. We argue that this architecture
can provide a systematic way of addressing the limitations of AIP using DNNs
and an approach to assurance of human-level performance and beyond. We conclude
by discussing what components of the architecture may already be addressed by
existing work and what remains future work.",0,0,0,0,0,0,0.600089,10.0,0.830338,51
7ffe6fd7-3487-4ca6-b445-25dcc2437814,DeepPrivacy2: Towards Realistic Full-Body Anonymization,13,0.198999,0.882599,"Generative Adversarial Networks (GANs) are widely adapted for anonymization
of human figures. However, current state-of-the-art limit anonymization to the
task of face anonymization. In this paper, we propose a novel anonymization
framework (DeepPrivacy2) for realistic anonymization of human figures and
faces. We introduce a new large and diverse dataset for human figure synthesis,
which significantly improves image quality and diversity of generated images.
Furthermore, we propose a style-based GAN that produces high quality, diverse
and editable anonymizations. We demonstrate that our full-body anonymization
framework provides stronger privacy guarantees than previously proposed
methods.",1,1,0,1,1,0,0.753035,10.0,0.872597,52
4199d0f8-edda-460c-bd39-a58ea8f1ed23,3D Moments from Near-Duplicate Photos,12,0.219696,0.196075,"We introduce 3D Moments, a new computational photography effect. As input we
take a pair of near-duplicate photos, i.e., photos of moving subjects from
similar viewpoints, common in people's photo collections. As output, we produce
a video that smoothly interpolates the scene motion from the first photo to the
second, while also producing camera motion with parallax that gives a
heightened sense of 3D. To achieve this effect, we represent the scene as a
pair of feature-based layered depth images augmented with scene flow. This
representation enables motion interpolation along with independent control of
the camera viewpoint. Our system produces photorealistic space-time videos with
motion parallax and scene dynamics, while plausibly recovering regions occluded
in the original views. We conduct extensive experiments demonstrating superior
performance over baselines on public datasets and in-the-wild photos. Project
page: https://3d-moments.github.io/",1,1,1,0,1,0,0.96593,5.0,0.921649,63
8668718b-3ac4-4f36-8b7d-1f4e1dd51325,A Privacy-Preserving Subgraph-Level Federated Graph Neural Network via Differential Privacy,7,0.0301884,0.276738,"Currently, the federated graph neural network (GNN) has attracted a lot of
attention due to its wide applications in reality without violating the privacy
regulations. Among all the privacy-preserving technologies, the differential
privacy (DP) is the most promising one due to its effectiveness and light
computational overhead. However, the DP-based federated GNN has not been well
investigated, especially in the sub-graph-level setting, such as the scenario
of recommendation system. The biggest challenge is how to guarantee the privacy
and solve the non independent and identically distributed (non-IID) data in
federated GNN simultaneously. In this paper, we propose DP-FedRec, a DP-based
federated GNN to fill the gap. Private Set Intersection (PSI) is leveraged to
extend the local graph for each client, and thus solve the non-IID problem.
Most importantly, DP is applied not only on the weights but also on the edges
of the intersection graph from PSI to fully protect the privacy of clients. The
evaluation demonstrates DP-FedRec achieves better performance with the graph
extension and DP only introduces little computations overhead.",0,1,1,0,0,0,0.304473,5.0,0.475489,29
49f434ef-0dde-40d9-8a5f-1c42e6be4e54,The distribution of syntactic dependency distances,1,0.0109464,0.236007,"The syntactic structure of a sentence can be represented as a graph where
vertices are words and edges indicate syntactic dependencies between them. In
this setting, the distance between two syntactically linked words can be
defined as the difference between their positions. Here we want to contribute
to the characterization of the actual distribution of syntactic dependency
distances, and unveil its relationship with short-term memory limitations. We
propose a new double-exponential model in which decay in probability is allowed
to change after a break-point. This transition could mirror the transition from
the processing of words chunks to higher-level structures. We find that a
two-regime model -- where the first regime follows either an exponential or a
power-law decay -- is the most likely one in all 20 languages we considered,
independently of sentence length and annotation style. Moreover, the
break-point is fairly stable across languages and averages values of 4-5 words,
suggesting that the amount of words that can be simultaneously processed
abstracts from the specific language to a high degree. Finally, we give an
account of the relation between the best estimated model and the closeness of
syntactic dependencies, as measured by a recently introduced optimality score.",0,0,0,0,0,0,0.00348204,22.0,0.669644,32
678ac7ca-dca4-4a03-8677-30ee8d569f81,Disentangling Confidence Score Distribution for Out-of-Domain Intent Detection with Energy-Based Learning,2,0.100202,0.404598,"Detecting Out-of-Domain (OOD) or unknown intents from user queries is
essential in a task-oriented dialog system. Traditional softmax-based
confidence scores are susceptible to the overconfidence issue. In this paper,
we propose a simple but strong energy-based score function to detect OOD where
the energy scores of OOD samples are higher than IND samples. Further, given a
small set of labeled OOD samples, we introduce an energy-based margin objective
for supervised OOD detection to explicitly distinguish OOD samples from INDs.
Comprehensive experiments and analysis prove our method helps disentangle
confidence score distributions of IND and OOD data.\footnote{Our code is
available at \url{https://github.com/pris-nlp/EMNLP2022-energy_for_OOD/}.}",1,1,0,0,0,0,0.903766,9.0,0.915681,32
2761f0e9-739a-4fe8-8276-15bedabf0561,Mathematically Modeling the Lexicon Entropy of Emergent Language,1,0.0307172,0.0418085,"We formulate a stochastic process, FiLex, as a mathematical model of lexicon
entropy in deep learning-based emergent language systems. Defining a model
mathematically allows it to generate clear predictions which can be directly
and decisively tested. We empirically verify across four different environments
that FiLex predicts the correct correlation between hyperparameters (training
steps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax
temperature) and the emergent language's entropy in 20 out of 20
environment-hyperparameter combinations. Furthermore, our experiments reveal
that different environments show diverse relationships between their
hyperparameters and entropy which demonstrates the need for a model which can
make well-defined predictions at a precise level of granularity.",0,0,0,0,0,0,0.280479,9.0,0.697709,26
06cfef28-5bb5-4881-8b0b-8a8223ef6fc6,AI Autonomy : Self-Initiated Open-World Continual Learning and Adaptation,3,0.0967043,0.121884,"As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can (1) learn by
themselves continually in a self-motivated and self-initiated manner rather
than being retrained offline periodically on the initiation of human engineers
and (2) accommodate or adapt to unexpected or novel circumstances. As the
real-world is an open environment that is full of unknowns or novelties, the
capabilities of detecting novelties, characterizing them,
accommodating/adapting to them, gathering ground-truth training data and
incrementally learning the unknowns/novelties become critical in making the AI
agent more and more knowledgeable, powerful and self-sustainable over time. The
key challenge here is how to automate the process so that it is carried out
continually on the agent's own initiative and through its own interactions with
humans, other agents and the environment just like human on-the-job learning.
This paper proposes a framework (called SOLA) for this learning paradigm to
promote the research of building autonomous and continual learning enabled AI
agents. To show feasibility, an implemented agent is also described.",0,0,0,0,0,0,0.243033,8.0,0.638999,73
07a2c925-363a-491a-afb1-d3224aa2fe96,Omnivision forecasting: combining satellite observations with sky images for improved intra-hour solar energy predictions,2,0.056067,0.175054,"Integration of intermittent renewable energy sources into electric grids in
large proportions is challenging. A well-established approach aimed at
addressing this difficulty involves the anticipation of the upcoming energy
supply variability to adapt the response of the grid. In solar energy,
short-term changes in electricity production caused by occluding clouds can be
predicted at different time scales from all-sky cameras (up to 30-min ahead)
and satellite observations (up to 6h ahead). In this study, we integrate these
two complementary points of view on the cloud cover in a single machine
learning framework to improve intra-hour (up to 60-min ahead) irradiance
forecasting. Both deterministic and probabilistic predictions are evaluated in
different weather conditions (clear-sky, cloudy, overcast) and with different
input configurations (sky images, satellite observations and/or past irradiance
values). Our results show that the hybrid model benefits predictions in
clear-sky conditions and improves longer-term forecasting. This study lays the
groundwork for future novel approaches of combining sky images and satellite
observations in a single learning framework to advance solar nowcasting.",0,1,0,0,0,0,0.517753,6.0,0.679148,62
bb9157ab-6965-4755-bfde-673924c5d849,MEDIAR: Harmony of Data-Centric and Model-Centric for Multi-Modality Microscopy,6,0.0870297,0.228809,"Cell segmentation is a fundamental task for computational biology analysis.
Identifying the cell instances is often the first step in various downstream
biomedical studies. However, many cell segmentation algorithms, including the
recently emerging deep learning-based methods, still show limited generality
under the multi-modality environment. Weakly Supervised Cell Segmentation in
Multi-modality High-Resolution Microscopy Images was hosted at NeurIPS 2022 to
tackle this problem. We propose MEDIAR, a holistic pipeline for cell instance
segmentation under multi-modality in this challenge. MEDIAR harmonizes
data-centric and model-centric approaches as the learning and inference
strategies, achieving a 0.9067 F1-score at the validation phase while
satisfying the time budget. To facilitate subsequent research, we provide the
source code and trained model as open-source:
https://github.com/Lee-Gihun/MEDIAR",1,1,0,0,0,0,0.929749,7.0,0.909609,68
8481af7d-e8df-4264-944e-51a11b8a26fe,On the Role of Bidirectionality in Language Model Pre-Training,15,0.0380417,0.557968,"Prior work on language model pre-training has explored different
architectures and learning objectives, but differences in data, hyperparameters
and evaluation make a principled comparison difficult. In this work, we focus
on bidirectionality as a key factor that differentiates existing approaches,
and present a comprehensive study of its role in next token prediction, text
infilling, zero-shot priming and fine-tuning. We propose a new framework that
generalizes prior approaches, including fully unidirectional models like GPT,
fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.
Our framework distinguishes between two notions of bidirectionality
(bidirectional context and bidirectional attention) and allows us to control
each of them separately. We find that the optimal configuration is largely
application-dependent (e.g., bidirectional attention is beneficial for
fine-tuning and infilling, but harmful for next token prediction and zero-shot
priming). We train models with up to 6.7B parameters, and find differences to
remain consistent at scale. While prior work on scaling has focused on
left-to-right autoregressive models, our results suggest that this approach
comes with some trade-offs, and it might be worthwhile to develop very large
bidirectional models.",0,0,0,0,0,0,0.826792,6.0,0.82534,51
0a5fc900-1ee0-4911-aff2-8ce6f35bd11b,Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation,12,0.0293577,0.692525,"We introduce Bi-SimCut: a simple but effective training strategy to boost
neural machine translation (NMT) performance. It consists of two procedures:
bidirectional pretraining and unidirectional finetuning. Both procedures
utilize SimCut, a simple regularization method that forces the consistency
between the output distributions of the original and the cutoff sentence pairs.
Without leveraging extra dataset via back-translation or integrating
large-scale pretrained model, Bi-SimCut achieves strong translation performance
across five translation benchmarks (data sizes range from 160K to 20.2M): BLEU
scores of 31.16 for en -> de and 38.37 for de -> en on the IWSLT14 dataset,
30.78 for en -> de and 35.15 for de -> en on the WMT14 dataset, and 27.17 for
zh -> en on the WMT17 dataset. SimCut is not a new method, but a version of
Cutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be
considered as a perturbation-based method. Given the universality and
simplicity of SimCut and Bi-SimCut, we believe they can serve as strong
baselines for future NMT research.",1,1,0,0,0,1,0.281673,7.0,0.612059,36
c58a91c5-c706-43b8-81b6-8ea9e65e525d,HYRR: Hybrid Infused Reranking for Passage Retrieval,1,0.0181384,0.168563,"We present Hybrid Infused Reranking for Passages Retrieval (HYRR), a
framework for training rerankers based on a hybrid of BM25 and neural retrieval
models. Retrievers based on hybrid models have been shown to outperform both
BM25 and neural models alone. Our approach exploits this improved performance
when training a reranker, leading to a robust reranking model. The reranker, a
cross-attention neural model, is shown to be robust to different first-stage
retrieval systems, achieving better performance than rerankers simply trained
upon the first-stage retrievers in the multi-stage systems. We present
evaluations on a supervised passage retrieval task using MS MARCO and zero-shot
retrieval tasks using BEIR. The empirical results show strong performance on
both evaluations.",0,1,0,0,0,0,0.840471,4.0,0.749474,35
ab687e16-d50f-47e2-b829-b46a8d2c0661,"Layout-Aware Information Extraction for Document-Grounded Dialogue: Dataset, Method and Demonstration",5,0.0754732,0.562066,"Building document-grounded dialogue systems have received growing interest as
documents convey a wealth of human knowledge and commonly exist in enterprises.
Wherein, how to comprehend and retrieve information from documents is a
challenging research problem. Previous work ignores the visual property of
documents and treats them as plain text, resulting in incomplete modality. In
this paper, we propose a Layout-aware document-level Information Extraction
dataset, LIE, to facilitate the study of extracting both structural and
semantic knowledge from visually rich documents (VRDs), so as to generate
accurate responses in dialogue systems. LIE contains 62k annotations of three
extraction tasks from 4,061 pages in product and official documents, becoming
the largest VRD-based information extraction dataset to the best of our
knowledge. We also develop benchmark methods that extend the token-based
language model to consider layout features like humans. Empirical results show
that layout is critical for VRD-based extraction, and system demonstration also
verifies that the extracted knowledge can help locate the answers that users
care about.",0,1,1,1,0,0,0.590068,5.0,0.655202,45
0e521768-072f-41e2-b763-6b8698486d26,Recovering Surveillance Video Using RF Cues,1,0.0162683,0.0333335,"Video capture is the most extensively utilized human perception source due to
its intuitively understandable nature. A desired video capture often requires
multiple environmental conditions such as ample ambient-light, unobstructed
space, and proper camera angle. In contrast, wireless measurements are more
ubiquitous and have fewer environmental constraints. In this paper, we propose
CSI2Video, a novel cross-modal method that leverages only WiFi signals from
commercial devices and a source of human identity information to recover
fine-grained surveillance video in a real-time manner. Specifically, two
tailored deep neural networks are designed to conduct cross-modal mapping and
video generation tasks respectively. We make use of an auto-encoder-based
structure to extract pose features from WiFi frames. Afterward, both extracted
pose features and identity information are merged to generate synthetic
surveillance video. Our solution generates realistic surveillance videos
without any expensive wireless equipment and has ubiquitous, cheap, and
real-time characteristics.",0,1,0,0,0,0,0.30991,11.0,0.76353,51
5a69384c-5421-4786-bfd0-400557086955,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",551,0.777656,0.999587,"Pretrained general-purpose language models can achieve state-of-the-art
accuracies in various natural language processing domains by adapting to
downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of
their success, the size of these models has increased rapidly, requiring
high-performance hardware, software, and algorithmic techniques to enable
training such large models. As the result of a joint effort between Microsoft
and NVIDIA, we present details on the training of the largest monolithic
transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530
billion parameters. In this paper, we first focus on the infrastructure as well
as the 3D parallelism methodology used to train this model using DeepSpeed and
Megatron. Next, we detail the training process, the design of our training
corpus, and our data curation techniques, which we believe is a key ingredient
to the success of the model. Finally, we discuss various evaluation results, as
well as other interesting observations and new properties exhibited by MT-NLG.
We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning
accuracies on several NLP benchmarks and establishes new state-of-the-art
results. We believe that our contributions will help further the development of
large-scale training infrastructures, large-scale language models, and natural
language generations.",0,1,0,0,1,0,0.739044,5.0,0.737153,78
a8d92015-20df-4951-ac9b-0331e6a66e40,SPECTRE: Spectral Conditioning Helps to Overcome the Expressivity Limits of One-shot Graph Generators,36,0.433553,0.601672,"We approach the graph generation problem from a spectral perspective by first
generating the dominant parts of the graph Laplacian spectrum and then building
a graph matching these eigenvalues and eigenvectors. Spectral conditioning
allows for direct modeling of the global and local graph structure and helps to
overcome the expressivity and mode collapse issues of one-shot graph
generators. Our novel GAN, called SPECTRE, enables the one-shot generation of
much larger graphs than previously possible with one-shot models. SPECTRE
outperforms state-of-the-art deep autoregressive generators in terms of
modeling fidelity, while also avoiding expensive sequential generation and
dependence on node ordering. A case in point, in sizable synthetic and
real-world graphs SPECTRE achieves a 4-to-170 fold improvement over the best
competitor that does not overfit and is 23-to-30 times faster than
autoregressive generators.",0,0,0,0,1,0,0.645448,9.0,0.8252,83
f520624f-d225-48b9-a2e0-5588dc15e7d3,Multimodal Transformer for Nursing Activity Recognition,18,0.295144,0.805429,"In an aging population, elderly patient safety is a primary concern at
hospitals and nursing homes, which demands for increased nurse care. By
performing nurse activity recognition, we can not only make sure that all
patients get an equal desired care, but it can also free nurses from manual
documentation of activities they perform, leading to a fair and safe place of
care for the elderly. In this work, we present a multimodal transformer-based
network, which extracts features from skeletal joints and acceleration data,
and fuses them to perform nurse activity recognition. Our method achieves
state-of-the-art performance of 81.8% accuracy on the benchmark dataset
available for nurse activity recognition from the Nurse Care Activity
Recognition Challenge. We perform ablation studies to show that our fusion
model is better than single modality transformer variants (using only
acceleration or skeleton joints data). Our solution also outperforms
state-of-the-art ST-GCN, GRU and other classical hand-crafted-feature-based
classifier solutions by a margin of 1.6%, on the NCRC dataset. Code is
available at \url{https://github.com/Momilijaz96/MMT_for_NCRC}.",1,1,0,0,1,0,0.629505,8.0,0.797932,41
15da53dc-43b0-4e4f-83d4-4ee941c69be9,SVNet: Where SO(3) Equivariance Meets Binarization on Point Cloud Representation,8,0.0518758,0.697056,"Efficiency and robustness are increasingly needed for applications on 3D
point clouds, with the ubiquitous use of edge devices in scenarios like
autonomous driving and robotics, which often demand real-time and reliable
responses. The paper tackles the challenge by designing a general framework to
construct 3D learning architectures with SO(3) equivariance and network
binarization. However, a naive combination of equivariant networks and
binarization either causes sub-optimal computational efficiency or geometric
ambiguity. We propose to locate both scalar and vector features in our networks
to avoid both cases. Precisely, the presence of scalar features makes the major
part of the network binarizable, while vector features serve to retain rich
structural information and ensure SO(3) equivariance. The proposed approach can
be applied to general backbones like PointNet and DGCNN. Meanwhile, experiments
on ModelNet40, ShapeNet, and the real-world dataset ScanObjectNN, demonstrated
that the method achieves a great trade-off between efficiency, rotation
robustness, and accuracy. The codes are available at
https://github.com/zhuoinoulu/svnet.",1,1,0,0,0,0,0.507347,7.0,0.72074,55
da1a553d-a0bc-452a-bab0-2aeca2d111e5,Cross-TOP: Zero-Shot Cross-Schema Task-Oriented Parsing,2,0.0847076,0.121533,"Deep learning methods have enabled task-oriented semantic parsing of
increasingly complex utterances. However, a single model is still typically
trained and deployed for each task separately, requiring labeled training data
for each, which makes it challenging to support new tasks, even within a single
business vertical (e.g., food-ordering or travel booking). In this paper we
describe Cross-TOP (Cross-Schema Task-Oriented Parsing), a zero-shot method for
complex semantic parsing in a given vertical. By leveraging the fact that user
requests from the same vertical share lexical and semantic similarities, a
single cross-schema parser is trained to service an arbitrary number of tasks,
seen or unseen, within a vertical. We show that Cross-TOP can achieve high
accuracy on a previously unseen task without requiring any additional training
data, thereby providing a scalable way to bootstrap semantic parsers for new
tasks. As part of this work we release the FoodOrdering dataset, a
task-oriented parsing dataset in the food-ordering vertical, with utterances
and annotations derived from five schemas, each from a different restaurant
menu.",0,1,0,1,0,0,0.966384,4.0,0.903052,22
45e8930b-e5da-48db-b840-414f5ba14fb8,"Graph Neural Networks Meet Wireless Communications: Motivation, Applications, and Future Directions",11,0.339173,0.855809,"As an efficient graph analytical tool, graph neural networks (GNNs) have
special properties that are particularly fit for the characteristics and
requirements of wireless communications, exhibiting good potential for the
advancement of next-generation wireless communications. This article aims to
provide a comprehensive overview of the interplay between GNNs and wireless
communications, including GNNs for wireless communications (GNN4Com) and
wireless communications for GNNs (Com4GNN). In particular, we discuss GNN4Com
based on how graphical models are constructed and introduce Com4GNN with
corresponding incentives. We also highlight potential research directions to
promote future research endeavors for GNNs in wireless communications.",0,0,0,0,0,0,0.878927,6.0,0.856316,15
ab262d17-e9ef-4e5f-a544-46352d1bc170,MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph Completion,10,0.258766,0.821856,"Multimodal knowledge graph completion (MKGC) aims to predict missing entities
in MKGs. Previous works usually share relation representation across
modalities. This results in mutual interference between modalities during
training, since for a pair of entities, the relation from one modality probably
contradicts that from another modality. Furthermore, making a unified
prediction based on the shared relation representation treats the input in
different modalities equally, while their importance to the MKGC task should be
different. In this paper, we propose MoSE, a Modality Split representation
learning and Ensemble inference framework for MKGC. Specifically, in the
training phase, we learn modality-split relation embeddings for each modality
instead of a single modality-shared one, which alleviates the modality
interference. Based on these embeddings, in the inference phase, we first make
modality-split predictions and then exploit various ensemble methods to combine
the predictions with different weights, which models the modality importance
dynamically. Experimental results on three KG datasets show that MoSE
outperforms state-of-the-art MKGC methods. Codes are available at
https://github.com/OreOZhao/MoSE4MKGC.",1,0,0,0,1,0,0.945347,9.0,0.939733,28
665eb152-7e63-4895-ab0e-92c5176b65e2,Scene Aware Person Image Generation through Global Contextual Conditioning,4,0.0597028,0.0675866,"Person image generation is an intriguing yet challenging problem. However,
this task becomes even more difficult under constrained situations. In this
work, we propose a novel pipeline to generate and insert contextually relevant
person images into an existing scene while preserving the global semantics.
More specifically, we aim to insert a person such that the location, pose, and
scale of the person being inserted blends in with the existing persons in the
scene. Our method uses three individual networks in a sequential pipeline. At
first, we predict the potential location and the skeletal structure of the new
person by conditioning a Wasserstein Generative Adversarial Network (WGAN) on
the existing human skeletons present in the scene. Next, the predicted skeleton
is refined through a shallow linear network to achieve higher structural
accuracy in the generated image. Finally, the target image is generated from
the refined skeleton using another generative network conditioned on a given
image of the target person. In our experiments, we achieve high-resolution
photo-realistic generation results while preserving the general context of the
scene. We conclude our paper with multiple qualitative and quantitative
benchmarks on the results.",0,1,0,0,0,0,0.877167,12.0,0.927586,43
db336156-4c42-44fe-a197-b8ee601096fa,Sockeye 3: Fast Neural Machine Translation with PyTorch,6,0.0147882,0.454404,"Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine
Translation (NMT). Now based on PyTorch, Sockeye 3 provides faster model
implementations and more advanced features with a further streamlined codebase.
This enables broader experimentation with faster iteration, efficient training
of stronger and faster models, and the flexibility to move new ideas quickly
from research to production. When running comparable models, Sockeye 3 is up to
126% faster than other PyTorch implementations on GPUs and up to 292% faster on
CPUs. Sockeye 3 is open source software released under the Apache 2.0 license.",1,1,0,0,0,0,0.176064,7.0,0.53556,33
3f3640bd-dd94-48da-8d0b-9a4dd1b1dadb,NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages,47,0.317813,0.935154,"Natural language processing (NLP) has a significant impact on society via
technologies such as machine translation and search engines. Despite its
success, NLP technology is only widely available for high-resource languages
such as English and Chinese, while it remains inaccessible to many languages
due to the unavailability of data resources and benchmarks. In this work, we
focus on developing resources for languages in Indonesia. Despite being the
second most linguistically diverse country, most languages in Indonesia are
categorized as endangered and some are even extinct. We develop the first-ever
parallel resource for 10 low-resource languages in Indonesia. Our resource
includes datasets, a multi-task benchmark, and lexicons, as well as a parallel
Indonesian-English dataset. We provide extensive analyses and describe the
challenges when creating such resources. We hope that our work can spark NLP
research on Indonesian and other underrepresented languages.",1,1,1,1,0,0,0.232809,6.0,0.510435,85
1267352f-2412-4931-9375-0855a3e5e33a,Hierarchical Compositional Representations for Few-shot Action Recognition,5,0.0238281,0.320683,"Recently action recognition has received more and more attention for its
comprehensive and practical applications in intelligent surveillance and
human-computer interaction. However, few-shot action recognition has not been
well explored and remains challenging because of data scarcity. In this paper,
we propose a novel hierarchical compositional representations (HCR) learning
approach for few-shot action recognition. Specifically, we divide a complicated
action into several sub-actions by carefully designed hierarchical clustering
and further decompose the sub-actions into more fine-grained spatially
attentional sub-actions (SAS-actions). Although there exist large differences
between base classes and novel classes, they can share similar patterns in
sub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance in
the transportation problem to measure the similarity between video samples in
terms of sub-action representations. It computes the optimal matching flows
between sub-actions as distance metric, which is favorable for comparing
fine-grained patterns. Extensive experiments show our method achieves the
state-of-the-art results on HMDB51, UCF101 and Kinetics datasets.",0,1,1,0,1,0,0.157106,10.0,0.662397,73
79969bf1-70d6-41f3-93d6-714977313f8e,Stubborn: A Strong Baseline for Indoor Object Navigation,18,0.591653,0.852072,"We present a strong baseline that surpasses the performance of previously
published methods on the Habitat Challenge task of navigating to a target
object in indoor environments. Our method is motivated from primary failure
modes of prior state-of-the-art: poor exploration, inaccurate object
identification, and agent getting trapped due to imprecise map construction. We
make three contributions to mitigate these issues: (i) First, we show that
existing map-based methods fail to effectively use semantic clues for
exploration. We present a semantic-agnostic exploration strategy (called
Stubborn) without any learning that surprisingly outperforms prior work. (ii)
We propose a strategy for integrating temporal information to improve object
identification. (iii) Lastly, due to inaccurate depth observation the agent
often gets trapped in small regions. We develop a multi-scale collision map for
obstacle identification that mitigates this issue.",0,1,0,0,0,0,0.965119,8.0,0.950157,31
848828e0-60e2-4525-add5-c4af75893609,Panoptic-PHNet: Towards Real-Time and High-Precision LiDAR Panoptic Segmentation via Clustering Pseudo Heatmap,42,0.424155,0.897188,"As a rising task, panoptic segmentation is faced with challenges in both
semantic segmentation and instance segmentation. However, in terms of speed and
accuracy, existing LiDAR methods in the field are still limited. In this paper,
we propose a fast and high-performance LiDAR-based framework, referred to as
Panoptic-PHNet, with three attractive aspects: 1) We introduce a clustering
pseudo heatmap as a new paradigm, which, followed by a center grouping module,
yields instance centers for efficient clustering without object-level learning
tasks. 2) A knn-transformer module is proposed to model the interaction among
foreground points for accurate offset regression. 3) For backbone design, we
fuse the fine-grained voxel features and the 2D Bird's Eye View (BEV) features
with different receptive fields to utilize both detailed and global
information. Extensive experiments on both SemanticKITTI dataset and nuScenes
dataset show that our Panoptic-PHNet surpasses state-of-the-art methods by
remarkable margins with a real-time speed. We achieve the 1st place on the
public leaderboard of SemanticKITTI and leading performance on the recently
released leaderboard of nuScenes.",0,1,0,0,1,0,0.934285,6.0,0.898681,42
660a3902-3700-445a-97e3-222308fb34a4,Building Height Prediction with Instance Segmentation,1,0.0171915,0.237442,"Extracting building heights from satellite images is an active research area
used in many fields such as telecommunications, city planning, etc. Many
studies utilize DSM (Digital Surface Models) generated with lidars or stereo
images for this purpose. Predicting the height of the buildings using only RGB
images is challenging due to the insufficient amount of data, low data quality,
variations of building types, different angles of light and shadow, etc. In
this study, we present an instance segmentation-based building height
extraction method to predict building masks with their respective heights from
a single RGB satellite image. We used satellite images with building height
annotations of certain cities along with an open-source satellite dataset with
the transfer learning approach. We reached, the bounding box mAP 59, the mask
mAP 52.6, and the average accuracy value of 70% for buildings belonging to each
height class in our test set.",0,1,0,0,0,0,0.315179,8.0,0.67741,22
5fb16a5d-a14e-4bd6-ab6a-1db0d8bae766,Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech,35,0.603742,0.745477,"We introduce a generic, language-independent method to collect a large
percentage of offensive and hate tweets regardless of their topics or genres.
We harness the extralinguistic information embedded in the emojis to collect a
large number of offensive tweets. We apply the proposed method on Arabic tweets
and compare it with English tweets - analysing key cultural differences. We
observed a constant usage of these emojis to represent offensiveness throughout
different timespans on Twitter. We manually annotate and publicly release the
largest Arabic dataset for offensive, fine-grained hate speech, vulgar and
violence content. Furthermore, we benchmark the dataset for detecting
offensiveness and hate speech using different transformer architectures and
perform in-depth linguistic analysis. We evaluate our models on external
datasets - a Twitter dataset collected using a completely different method, and
a multi-platform dataset containing comments from Twitter, YouTube and
Facebook, for assessing generalization capability. Competitive results on these
datasets suggest that the data collected using our method captures universal
characteristics of offensive language. Our findings also highlight the common
words used in offensive communications, common targets for hate speech,
specific patterns in violence tweets; and pinpoint common classification errors
that can be attributed to limitations of NLP models. We observe that even
state-of-the-art transformer models may fail to take into account culture,
background and context or understand nuances present in real-world data such as
sarcasm.",0,1,0,1,0,0,0.558474,6.0,0.69818,49
568e1748-b0a8-4307-9115-d2ef2315bbbd,Fast Event-based Optical Flow Estimation by Triplet Matching,8,0.152644,0.887678,"Event cameras are novel bio-inspired sensors that offer advantages over
traditional cameras (low latency, high dynamic range, low power, etc.). Optical
flow estimation methods that work on packets of events trade off speed for
accuracy, while event-by-event (incremental) methods have strong assumptions
and have not been tested on common benchmarks that quantify progress in the
field. Towards applications on resource-constrained devices, it is important to
develop optical flow algorithms that are fast, light-weight and accurate. This
work leverages insights from neuroscience, and proposes a novel optical flow
estimation scheme based on triplet matching. The experiments on publicly
available benchmarks demonstrate its capability to handle complex scenes with
comparable results as prior packet-based algorithms. In addition, the proposed
method achieves the fastest execution time (> 10 kHz) on standard CPUs as it
requires only three events in estimation. We hope that our research opens the
door to real-time, incremental motion estimation methods and applications in
real-world scenarios.",0,1,0,0,0,0,0.535264,10.0,0.812436,40
ec9e17c9-4fda-4745-8ed0-c28d128356ba,Probing Script Knowledge from Pre-Trained Models,4,0.0122034,0.225807,"Script knowledge is critical for humans to understand the broad daily tasks
and routine activities in the world. Recently researchers have explored the
large-scale pre-trained language models (PLMs) to perform various script
related tasks, such as story generation, temporal ordering of event, future
event prediction and so on. However, it's still not well studied in terms of
how well the PLMs capture the script knowledge. To answer this question, we
design three probing tasks: inclusive sub-event selection, starting sub-event
selection and temporal ordering to investigate the capabilities of PLMs with
and without fine-tuning. The three probing tasks can be further used to
automatically induce a script for each main event given all the possible
sub-events. Taking BERT as a case study, by analyzing its performance on script
induction as well as each individual probing task, we conclude that the
stereotypical temporal knowledge among the sub-events is well captured in BERT,
however the inclusive or starting sub-event knowledge is barely encoded.",0,0,0,0,0,0,0.334433,6.0,0.581984,48
c8d54867-faae-4b7d-9775-7bcc7e811e35,Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem,2,0.0,0.0222346,"We introduce the task of implicit offensive text detection in dialogues,
where a statement may have either an offensive or non-offensive interpretation,
depending on the listener and context. We argue that reasoning is crucial for
understanding this broader class of offensive utterances and release SLIGHT, a
dataset to support research on this task. Experiments using the data show that
state-of-the-art methods of offense detection perform poorly when asked to
detect implicitly offensive statements, achieving only ${\sim} 11\%$ accuracy.
  In contrast to existing offensive text detection datasets, SLIGHT features
human-annotated chains of reasoning which describe the mental process by which
an offensive interpretation can be reached from each ambiguous statement. We
explore the potential for a multi-hop reasoning approach by utilizing existing
entailment models to score the probability of these chains and show that even
naive reasoning models can yield improved performance in most situations.
Furthermore, analysis of the chains provides insight into the human
interpretation process and emphasizes the importance of incorporating
additional commonsense knowledge.",0,0,1,1,0,0,0.0381141,8.0,0.392845,57
994547a8-54bb-4291-b7fe-17d0a60c4e8e,Learning to Merge Tokens in Vision Transformers,26,0.167831,0.414196,"Transformers are widely applied to solve natural language understanding and
computer vision tasks. While scaling up these architectures leads to improved
performance, it often comes at the expense of much higher computational costs.
In order for large-scale models to remain practical in real-world systems,
there is a need for reducing their computational overhead. In this work, we
present the PatchMerger, a simple module that reduces the number of patches or
tokens the network has to process by merging them between two consecutive
intermediate layers. We show that the PatchMerger achieves a significant
speedup across various model sizes while matching the original performance both
upstream and downstream after fine-tuning.",0,1,0,0,0,0,0.983769,4.0,0.951653,17
75ae2874-4fe4-4ca1-abcf-cbd2ec793ef7,A Benchmark dataset for predictive maintenance,4,0.0582846,0.553488,"The paper describes the MetroPT data set, an outcome of a eXplainable
Predictive Maintenance (XPM) project with an urban metro public transportation
service in Porto, Portugal. The data was collected in 2022 that aimed to
evaluate machine learning methods for online anomaly detection and failure
prediction. By capturing several analogic sensor signals (pressure,
temperature, current consumption), digital signals (control signals, discrete
signals), and GPS information (latitude, longitude, and speed), we provide a
dataset that can be easily used to evaluate online machine learning methods.
This dataset contains some interesting characteristics and can be a good
benchmark for predictive maintenance models.",0,1,0,1,0,0,0.630109,2.0,0.192547,7
d495da6e-ff26-4191-9a77-8bd589900db1,Multi-Behavior Enhanced Recommendation with Cross-Interaction Collaborative Relation Modeling,30,0.450997,0.912593,"Many previous studies aim to augment collaborative filtering with deep neural
network techniques, so as to achieve better recommendation performance.
However, most existing deep learning-based recommender systems are designed for
modeling singular type of user-item interaction behavior, which can hardly
distill the heterogeneous relations between user and item. In practical
recommendation scenarios, there exist multityped user behaviors, such as browse
and purchase. Due to the overlook of user's multi-behavioral patterns over
different items, existing recommendation methods are insufficient to capture
heterogeneous collaborative signals from user multi-behavior data. Inspired by
the strength of graph neural networks for structured data modeling, this work
proposes a Graph Neural Multi-Behavior Enhanced Recommendation (GNMR) framework
which explicitly models the dependencies between different types of user-item
interactions under a graph-based message passing architecture. GNMR devises a
relation aggregation network to model interaction heterogeneity, and
recursively performs embedding propagation between neighboring nodes over the
user-item interaction graph. Experiments on real-world recommendation datasets
show that our GNMR consistently outperforms state-of-the-art methods. The
source code is available at https://github.com/akaxlh/GNMR.",1,0,0,0,1,0,0.913024,6.0,0.880571,21
93d9f5f9-c886-4362-9b4b-8f9e1c68db96,Adapting Pretrained Text-to-Text Models for Long Text Sequences,24,0.136303,0.758419,"We present an empirical study of adapting an existing pretrained text-to-text
model for long-sequence inputs. Through a comprehensive study along three axes
of the pretraining pipeline -- model architecture, optimization objective, and
pretraining corpus, we propose an effective recipe to build long-context models
from existing short-context models. Specifically, we replace the full attention
in transformers with pooling-augmented blockwise attention, and pretrain the
model with a masked-span prediction task with spans of varying length. In terms
of the pretraining corpus, we find that using randomly concatenated
short-documents from a large open-domain corpus results in better performance
than using existing long document corpora which are typically limited in their
domain coverage. With these findings, we build a long-context model that
achieves competitive performance on long-text QA tasks and establishes the new
state of the art on five long-text summarization datasets, often outperforming
previous methods with larger model sizes. Our code has been released at
https://github.com/facebookresearch/bart_ls.",1,1,0,0,1,0,0.684392,5.0,0.70663,55
8b9a9aa2-dcc1-4747-8fbd-7fc50dcb626b,Zero-Shot Cost Models for Out-of-the-box Learned Cost Prediction,27,0.582286,0.49958,"In this paper, we introduce zero-shot cost models which enable learned cost
estimation that generalizes to unseen databases. In contrast to
state-of-the-art workload-driven approaches which require to execute a large
set of training queries on every new database, zero-shot cost models thus allow
to instantiate a learned cost model out-of-the-box without expensive training
data collection. To enable such zero-shot cost models, we suggest a new
learning paradigm based on pre-trained cost models. As core contributions to
support the transfer of such a pre-trained cost model to unseen databases, we
introduce a new model architecture and representation technique for encoding
query workloads as input to those models. As we will show in our evaluation,
zero-shot cost estimation can provide more accurate cost estimates than
state-of-the-art models for a wide range of (real-world) databases without
requiring any query executions on unseen databases. Furthermore, we show that
zero-shot cost models can be used in a few-shot mode that further improves
their quality by retraining them just with a small number of additional
training queries on the unseen database.",0,1,1,0,0,0,0.829806,7.0,0.851714,38
2a3717d4-cb0f-4991-84f0-00c854fd30d6,VeriDark: A Large-Scale Benchmark for Authorship Verification on the Dark Web,9,0.13475,0.522717,"The DarkWeb represents a hotbed for illicit activity, where users communicate
on different market forums in order to exchange goods and services. Law
enforcement agencies benefit from forensic tools that perform authorship
analysis, in order to identify and profile users based on their textual
content. However, authorship analysis has been traditionally studied using
corpora featuring literary texts such as fragments from novels or fan fiction,
which may not be suitable in a cybercrime context. Moreover, the few works that
employ authorship analysis tools for cybercrime prevention usually employ
ad-hoc experimental setups and datasets. To address these issues, we release
VeriDark: a benchmark comprised of three large scale authorship verification
datasets and one authorship identification dataset obtained from user activity
from either Dark Web related Reddit communities or popular illicit Dark Web
market forums. We evaluate competitive NLP baselines on the three datasets and
perform an analysis of the predictions to better understand the limitations of
such approaches. We make the datasets and baselines publicly available at
https://github.com/bit-ml/VeriDark",0,1,0,1,0,0,0.1327,7.0,0.491605,59
5081d9fa-3fc5-4fcb-8975-01ef3025a20a,Non-Isometric Shape Matching via Functional Maps on Landmark-Adapted Bases,11,0.0591605,0.397394,"We propose a principled approach for non-isometric landmark-preserving
non-rigid shape matching. Our method is based on the functional maps framework,
but rather than promoting isometries we focus instead on near-conformal maps
that preserve landmarks exactly. We achieve this, first, by introducing a novel
landmark-adapted basis using an intrinsic Dirichlet-Steklov eigenproblem.
Second, we establish the functional decomposition of conformal maps expressed
in this basis. Finally, we formulate a conformally-invariant energy that
promotes high-quality landmark-preserving maps, and show how it can be solved
via a variant of the recently proposed ZoomOut method that we extend to our
setting. Our method is descriptor-free, efficient and robust to significant
mesh variability. We evaluate our approach on a range of benchmark datasets and
demonstrate state-of-the-art performance on non-isometric benchmarks and near
state-of-the-art performance on isometric ones.",0,0,0,0,1,0,0.0074653,12.0,0.458068,79
5f3c319c-f24a-4d17-a042-b9c9ab83a14b,Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models,21,0.381246,0.87582,"We present a novel way of conditioning a pretrained denoising diffusion
speech model to produce speech in the voice of a novel person unseen during
training. The method requires a short (~3 seconds) sample from the target
person, and generation is steered at inference time, without any training
steps. At the heart of the method lies a sampling process that combines the
estimation of the denoising model with a low-pass version of the new speaker's
sample. The objective and subjective evaluations show that our sampling method
can generate a voice similar to that of the target speaker in terms of
frequency, with an accuracy comparable to state-of-the-art methods, and without
training.",1,1,0,0,0,0,0.823305,4.0,0.735151,25
5502c399-df1a-4bd7-b880-4405d7785a77,Edge-enhanced Feature Distillation Network for Efficient Super-Resolution,20,0.0807341,0.606906,"With the recently massive development in convolution neural networks,
numerous lightweight CNN-based image super-resolution methods have been
proposed for practical deployments on edge devices. However, most existing
methods focus on one specific aspect: network or loss design, which leads to
the difficulty of minimizing the model size. To address the issue, we conclude
block devising, architecture searching, and loss design to obtain a more
efficient SR structure. In this paper, we proposed an edge-enhanced feature
distillation network, named EFDN, to preserve the high-frequency information
under constrained resources. In detail, we build an edge-enhanced convolution
block based on the existing reparameterization methods. Meanwhile, we propose
edge-enhanced gradient loss to calibrate the reparameterized path training.
Experimental results show that our edge-enhanced strategies preserve the edge
and significantly improve the final restoration quality. Code is available at
https://github.com/icandle/EFDN.",1,1,0,0,0,0,0.362233,9.0,0.732397,46
57ecf52d-0981-4026-996e-1a2d1646b1f5,Confident Adaptive Language Modeling,77,0.372907,0.816495,"Recent advances in Transformer-based large language models (LLMs) have led to
significant performance improvements across many tasks. These gains come with a
drastic increase in the models' size, potentially leading to slow and costly
use at inference time. In practice, however, the series of generations made by
LLMs is composed of varying levels of difficulty. While certain predictions
truly benefit from the models' full capacity, other continuations are more
trivial and can be solved with reduced compute. In this work, we introduce
Confident Adaptive Language Modeling (CALM), a framework for dynamically
allocating different amounts of compute per input and generation timestep.
Early exit decoding involves several challenges that we address here, such as:
(1) what confidence measure to use; (2) connecting sequence-level constraints
to local per-token exit decisions; and (3) attending back to missing hidden
representations due to early exits in previous tokens. Through theoretical
analysis and empirical experiments on three diverse text generation tasks, we
demonstrate the efficacy of our framework in reducing compute -- potential
speedup of up to $\times 3$ -- while provably maintaining high performance.",1,0,0,0,0,1,0.517871,5.0,0.615045,92
bbc5b15f-9b3e-46a4-8d83-214c14b5226a,Learning from One and Only One Shot,3,0.013715,0.0619988,"Humans can generalize from only a few examples and from little pre-training
on similar tasks. Yet, machine learning (ML) typically requires large data to
learn or pre-learn to transfer. Inspired by nativism, we directly model basic
human-innate priors in abstract visual tasks e.g., character/doodle
recognition. This yields a white-box model that learns general-appearance
similarity -- how any two images look in general -- by mimicking how humans
naturally ""distort"" an object at first sight. Using simply the nearest-neighbor
classifier on this similarity space, we achieve human-level character
recognition using only 1--10 examples per class and nothing else (no
pre-training). This differs from few-shot learning (FSL) using significant
pre-training. On standard benchmarks MNIST/EMNIST and the Omniglot challenge,
we outperform both neural-network-based and classical ML in the ""tiny-data""
regime, including FSL pre-trained on large data. Our model enables unsupervised
learning too: by learning the non-Euclidean, general-appearance similarity
space in a k-means style, we can generate human-intuitive archetypes as cluster
``centroids''.",0,0,0,0,1,0,0.242045,8.0,0.638412,28
887a46ff-58fc-4720-abc7-d021d22b23d5,MetaCon: Unified Predictive Segments System with Trillion Concept Meta-Learning,1,0.00593509,0.137753,"Accurate understanding of users in terms of predicative segments play an
essential role in the day to day operation of modern internet enterprises.
Nevertheless, there are significant challenges that limit the quality of data,
especially on long tail predictive tasks. In this work, we present MetaCon, our
unified predicative segments system with scalable, trillion concepts meta
learning that addresses these challenges. It builds on top of a flat concept
representation that summarizes entities' heterogeneous digital footprint,
jointly considers the entire spectrum of predicative tasks as a single learning
task, and leverages principled meta learning approach with efficient first
order meta-optimization procedure under a provable performance guarantee in
order to solve the learning task. Experiments on both proprietary production
datasets and public structured learning tasks demonstrate that MetaCon can lead
to substantial improvements over state of the art recommendation and ranking
approaches.",0,1,0,0,1,0,0.0917279,10.0,0.604935,66
c037fa5f-7b9d-4be2-ab4a-12e687152609,Evaluating Explainability for Graph Neural Networks,53,0.113865,0.887414,"As post hoc explanations are increasingly used to understand the behavior of
graph neural networks (GNNs), it becomes crucial to evaluate the quality and
reliability of GNN explanations. However, assessing the quality of GNN
explanations is challenging as existing graph datasets have no or unreliable
ground-truth explanations for a given task. Here, we introduce a synthetic
graph data generator, ShapeGGen, which can generate a variety of benchmark
datasets (e.g., varying graph sizes, degree distributions, homophilic vs.
heterophilic graphs) accompanied by ground-truth explanations. Further, the
flexibility to generate diverse synthetic datasets and corresponding
ground-truth explanations allows us to mimic the data generated by various
real-world applications. We include ShapeGGen and several real-world graph
datasets into an open-source graph explainability library, GraphXAI. In
addition to synthetic and real-world graph datasets with ground-truth
explanations, GraphXAI provides data loaders, data processing functions,
visualizers, GNN model implementations, and evaluation metrics to benchmark the
performance of GNN explainability methods.",1,1,1,1,0,0,0.323152,6.0,0.574958,54
1254fb77-0fcc-48d7-b766-b3fddb431533,A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference,7,0.0763381,0.187978,"Most evaluations of attribution methods focus on the English language. In
this work, we present a multilingual approach for evaluating attribution
methods for the Natural Language Inference (NLI) task in terms of faithfulness
and plausibility. First, we introduce a novel cross-lingual strategy to measure
faithfulness based on word alignments, which eliminates the drawbacks of
erasure-based evaluations.We then perform a comprehensive evaluation of
attribution methods, considering different output mechanisms and aggregation
methods. Finally, we augment the XNLI dataset with highlight-based
explanations, providing a multilingual NLI dataset with highlights, to support
future exNLP studies. Our results show that attribution methods performing best
for plausibility and faithfulness are different.",1,0,0,1,0,0,0.754617,7.0,0.81865,56
833ef556-3ac8-42ce-b95d-a1160eb1f063,Multimodal Detection of Unknown Objects on Roads for Autonomous Driving,6,0.0623852,0.107774,"Tremendous progress in deep learning over the last years has led towards a
future with autonomous vehicles on our roads. Nevertheless, the performance of
their perception systems is strongly dependent on the quality of the utilized
training data. As these usually only cover a fraction of all object classes an
autonomous driving system will face, such systems struggle with handling the
unexpected. In order to safely operate on public roads, the identification of
objects from unknown classes remains a crucial task. In this paper, we propose
a novel pipeline to detect unknown objects. Instead of focusing on a single
sensor modality, we make use of lidar and camera data by combining state-of-the
art detection models in a sequential manner. We evaluate our approach on the
Waymo Open Perception Dataset and point out current research gaps in anomaly
detection.",0,1,0,0,0,0,0.53603,6.0,0.687752,63
c3d5513c-a367-4928-adc3-3369abd39783,Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion,22,0.57241,0.877174,"Digital art synthesis is receiving increasing attention in the multimedia
community because of engaging the public with art effectively. Current digital
art synthesis methods usually use single-modality inputs as guidance, thereby
limiting the expressiveness of the model and the diversity of generated
results. To solve this problem, we propose the multimodal guided artwork
diffusion (MGAD) model, which is a diffusion-based digital artwork generation
approach that utilizes multimodal prompts as guidance to control the
classifier-free diffusion model. Additionally, the contrastive language-image
pretraining (CLIP) model is used to unify text and image modalities. Extensive
experimental results on the quality and quantity of the generated digital art
paintings confirm the effectiveness of the combination of the diffusion model
and multimodal guidance. Code is available at
https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion.",1,0,0,0,0,0,0.924738,5.0,0.868195,75
99dbe990-ac9c-4808-b79f-715beaf68768,On the Privacy Properties of GAN-generated Samples,19,0.146929,0.940059,"The privacy implications of generative adversarial networks (GANs) are a
topic of great interest, leading to several recent algorithms for training GANs
with privacy guarantees. By drawing connections to the generalization
properties of GANs, we prove that under some assumptions, GAN-generated samples
inherently satisfy some (weak) privacy guarantees. First, we show that if a GAN
is trained on m samples and used to generate n samples, the generated samples
are (epsilon, delta)-differentially-private for (epsilon, delta) pairs where
delta scales as O(n/m). We show that under some special conditions, this upper
bound is tight. Next, we study the robustness of GAN-generated samples to
membership inference attacks. We model membership inference as a hypothesis
test in which the adversary must determine whether a given sample was drawn
from the training dataset or from the underlying data distribution. We show
that this adversary can achieve an area under the ROC curve that scales no
better than O(m^{-1/4}).",0,0,0,0,0,0,0.448894,10.0,0.787276,49
1422a914-0235-4af1-b161-918170c5adbb,Towards a Grounded Theory of Causation for Embodied AI,3,0.202632,0.212008,"There exist well-developed frameworks for causal modelling, but these require
rather a lot of human domain expertise to define causal variables and perform
interventions. In order to enable autonomous agents to learn abstract causal
models through interactive experience, the existing theoretical foundations
need to be extended and clarified. Existing frameworks give no guidance
regarding variable choice / representation, and more importantly, give no
indication as to which behaviour policies or physical transformations of state
space shall count as interventions. The framework sketched in this paper
describes actions as transformations of state space, for instance induced by an
agent running a policy. This makes it possible to describe in a uniform way
both transformations of the micro-state space and abstract models thereof, and
say when the latter is veridical / grounded / natural. We then introduce
(causal) variables, define a mechanism as an invariant predictor, and say when
an action can be viewed as a ``surgical intervention'', thus bringing the
objective of causal representation \& intervention skill learning into clearer
focus.",0,0,0,0,0,0,0.826418,8.0,0.868851,54
d8dfec9d-41c9-4c87-9f0b-5690f9e6be25,TEVR: Improving Speech Recognition by Token Entropy Variance Reduction,2,0.00336956,0.115972,"This paper presents TEVR, a speech recognition model designed to minimize the
variation in token entropy w.r.t. to the language model. This takes advantage
of the fact that if the language model will reliably and accurately predict a
token anyway, then the acoustic model doesn't need to be accurate in
recognizing it. We train German ASR models with 900 million parameters and show
that on CommonVoice German, TEVR scores a very competitive 3.64% word error
rate, which outperforms the best reported results by a relative 16.89%
reduction in word error rate. We hope that releasing our fully trained speech
recognition pipeline to the community will lead to privacy-preserving offline
virtual assistants in the future.",1,1,0,0,1,0,0.405131,6.0,0.622585,21
f15fdbb4-6e4d-47fb-8657-7cb979c3c356,Too Big to Fail? Active Few-Shot Learning Guided Logic Synthesis,1,0.00916437,0.0265138,"Generating sub-optimal synthesis transformation sequences (""synthesis
recipe"") is an important problem in logic synthesis. Manually crafted synthesis
recipes have poor quality. State-of-the art machine learning (ML) works to
generate synthesis recipes do not scale to large netlists as the models need to
be trained from scratch, for which training data is collected using time
consuming synthesis runs. We propose a new approach, Bulls-Eye, that fine-tunes
a pre-trained model on past synthesis data to accurately predict the quality of
a synthesis recipe for an unseen netlist. This approach on achieves 2x-10x
run-time improvement and better quality-of-result (QoR) than state-of-the-art
machine learning approaches.",1,1,0,0,1,0,0.144603,8.0,0.566735,28
965abf95-a8ec-4831-8c1f-a9a3497f3374,Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in Dynamic Environments,7,0.0578897,0.692367,"We tackle the problem of perceptive locomotion in dynamic environments. In
this problem, a quadrupedal robot must exhibit robust and agile walking
behaviors in response to environmental clutter and moving obstacles. We present
a hierarchical learning framework, named PRELUDE, which decomposes the problem
of perceptive locomotion into high-level decision-making to predict navigation
commands and low-level gait generation to realize the target commands. In this
framework, we train the high-level navigation controller with imitation
learning on human demonstrations collected on a steerable cart and the
low-level gait controller with reinforcement learning (RL). Therefore, our
method can acquire complex navigation behaviors from human supervision and
discover versatile gaits from trial and error. We demonstrate the effectiveness
of our approach in simulation and with hardware experiments. Videos and code
can be found at the project page: https://ut-austin-rpl.github.io/PRELUDE.",1,1,0,0,0,0,0.191628,7.0,0.548977,45
65e11499-3413-4945-8221-3a6a50b6c6dc,Handling sign language transcription system with the computer-friendly numerical multilabels,1,0.0174705,0.158244,"This paper presents our recent developments in the automatic processing of
sign language corpora using the Hamburg Sign Language Annotation System
(HamNoSys). We designed an automated tool to convert HamNoSys annotations into
numerical labels for defined initial features of body and hand positions. Our
proposed numerical multilabels greatly simplify annotations' structure without
significant loss of gloss meaning. These numerical multilabels can potentially
be used to feed the machine learning models, which would accelerate the
development of vision-based sign language recognition. In addition, this tool
can assist experts in the annotation process and help identify semantic errors.
The code and sample annotations are publicly available at
\url{https://github.com/hearai/parse-hamnosys}.",1,1,0,0,0,0,0.000613707,16.0,0.43718,15
8c36d9f8-2198-4168-94ed-afce8cb2ebc5,On the link between conscious function and general intelligence in humans and machines,14,0.180067,0.235021,"In popular media, there is often a connection drawn between the advent of
awareness in artificial agents and those same agents simultaneously achieving
human or superhuman level intelligence. In this work, we explore the validity
and potential application of this seemingly intuitive link between
consciousness and intelligence. We do so by examining the cognitive abilities
associated with three contemporary theories of conscious function: Global
Workspace Theory (GWT), Information Generation Theory (IGT), and Attention
Schema Theory (AST). We find that all three theories specifically relate
conscious function to some aspect of domain-general intelligence in humans.
With this insight, we turn to the field of Artificial Intelligence (AI) and
find that, while still far from demonstrating general intelligence, many
state-of-the-art deep learning methods have begun to incorporate key aspects of
each of the three functional theories. Having identified this trend, we use the
motivating example of mental time travel in humans to propose ways in which
insights from each of the three theories may be combined into a single unified
and implementable model. Given that it is made possible by cognitive abilities
underlying each of the three functional theories, artificial agents capable of
mental time travel would not only possess greater general intelligence than
current approaches, but also be more consistent with our current understanding
of the functional role of consciousness in humans, thus making it a promising
near-term goal for AI research.",0,0,0,0,0,1,0.579856,10.0,0.824803,249
fb1e05cd-e1ff-48e3-8c2d-90f640cfcb4a,Generative Personas That Behave and Experience Like Humans,13,0.0272345,0.215447,"Using artificial intelligence (AI) to automatically test a game remains a
critical challenge for the development of richer and more complex game worlds
and for the advancement of AI at large. One of the most promising methods for
achieving that long-standing goal is the use of generative AI agents, namely
procedural personas, that attempt to imitate particular playing behaviors which
are represented as rules, rewards, or human demonstrations. All research
efforts for building those generative agents, however, have focused solely on
playing behavior which is arguably a narrow perspective of what a player
actually does in a game. Motivated by this gap in the existing state of the
art, in this paper we extend the notion of behavioral procedural personas to
cater for player experience, thus examining generative agents that can both
behave and experience their game as humans would. For that purpose, we employ
the Go-Explore reinforcement learning paradigm for training human-like
procedural personas, and we test our method on behavior and experience
demonstrations of more than 100 players of a racing game. Our findings suggest
that the generated agents exhibit distinctive play styles and experience
responses of the human personas they were designed to imitate. Importantly, it
also appears that experience, which is tied to playing behavior, can be a
highly informative driver for better behavioral exploration.",0,1,1,0,0,0,0.00472833,10.0,0.303875,47
48f8afff-2ee2-4044-a3a0-88d9bb41ca71,CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer Learning,12,0.703346,0.92756,"Generating artistic portraits is a challenging problem in computer vision.
Existing portrait stylization models that generate good quality results are
based on Image-to-Image Translation and require abundant data from both source
and target domains. However, without enough data, these methods would result in
overfitting. In this work, we propose CtlGAN, a new few-shot artistic portraits
generation model with a novel contrastive transfer learning strategy. We adapt
a pretrained StyleGAN in the source domain to a target artistic domain with no
more than 10 artistic faces. To reduce overfitting to the few training
examples, we introduce a novel Cross-Domain Triplet loss which explicitly
encourages the target instances generated from different latent codes to be
distinguishable. We propose a new encoder which embeds real faces into Z+ space
and proposes a dual-path training strategy to better cope with the adapted
decoder and eliminate the artifacts. Extensive qualitative, quantitative
comparisons and a user study show our method significantly outperforms
state-of-the-arts under 10-shot and 1-shot settings and generates high quality
artistic portraits. The code will be made publicly available.",0,1,1,0,1,0,0.993107,6.0,0.999242,55
88cf7035-7027-4939-8f85-7f2a4aa331ec,"CGiS-Net: Aggregating Colour, Geometry and Implicit Semantic Features for Indoor Place Recognition",5,0.0215303,0.550614,"We describe a novel approach to indoor place recognition from RGB point
clouds based on aggregating low-level colour and geometry features with
high-level implicit semantic features. It uses a 2-stage deep learning
framework, in which the first stage is trained for the auxiliary task of
semantic segmentation and the second stage uses features from layers in the
first stage to generate discriminate descriptors for place recognition. The
auxiliary task encourages the features to be semantically meaningful, hence
aggregating the geometry and colour in the RGB point cloud data with implicit
semantic information. We use an indoor place recognition dataset derived from
the ScanNet dataset for training and evaluation, with a test set comprising
3,608 point clouds generated from 100 different rooms. Comparison with a
traditional feature-based method and four state-of-the-art deep learning
methods demonstrate that our approach significantly outperforms all five
methods, achieving, for example, a top-3 average recall rate of 75% compared
with 41% for the closest rival method. Our code is available at:
https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition",1,1,0,1,1,0,0.187447,6.0,0.469716,36
3c26fe3d-2da4-4037-b2bc-a902840b768a,Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning,20,0.606073,0.940425,"In this paper, we study the named entity recognition (NER) problem under
distant supervision. Due to the incompleteness of the external dictionaries
and/or knowledge bases, such distantly annotated training data usually suffer
from a high false negative rate. To this end, we formulate the Distantly
Supervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU)
learning and propose a theoretically and practically novel CONFidence-based MPU
(Conf-MPU) approach. To handle the incomplete annotations, Conf-MPU consists of
two steps. First, a confidence score is estimated for each token of being an
entity token. Then, the proposed Conf-MPU risk estimation is applied to train a
multi-class classifier for the NER task. Thorough experiments on two benchmark
datasets labeled by various external knowledge demonstrate the superiority of
the proposed Conf-MPU over existing DS-NER methods.",1,0,0,0,1,0,0.974688,8.0,0.961564,32
2251fc8e-9d77-4b09-afa8-07a2c8507161,Injecting Domain Knowledge in Language Models for Task-Oriented Dialogue Systems,14,0.422292,0.904422,"Pre-trained language models (PLM) have advanced the state-of-the-art across
NLP applications, but lack domain-specific knowledge that does not naturally
occur in pre-training data. Previous studies augmented PLMs with symbolic
knowledge for different downstream NLP tasks. However, knowledge bases (KBs)
utilized in these studies are usually large-scale and static, in contrast to
small, domain-specific, and modifiable knowledge bases that are prominent in
real-world task-oriented dialogue (TOD) systems. In this paper, we showcase the
advantages of injecting domain-specific knowledge prior to fine-tuning on TOD
tasks. To this end, we utilize light-weight adapters that can be easily
integrated with PLMs and serve as a repository for facts learned from different
KBs. To measure the efficacy of proposed knowledge injection methods, we
introduce Knowledge Probing using Response Selection (KPRS) -- a probe designed
specifically for TOD models. Experiments on KPRS and the response generation
task show improvements of knowledge injection with adapters over strong
baselines.",1,1,0,0,0,0,0.946835,6.0,0.911175,25
3cc46e52-1df0-4f9a-a2c4-bcd2780b2923,DST: Dynamic Substitute Training for Data-free Black-box Attack,9,0.0452836,0.315779,"With the wide applications of deep neural network models in various computer
vision tasks, more and more works study the model vulnerability to adversarial
examples. For data-free black box attack scenario, existing methods are
inspired by the knowledge distillation, and thus usually train a substitute
model to learn knowledge from the target model using generated data as input.
However, the substitute model always has a static network structure, which
limits the attack ability for various target models and tasks. In this paper,
we propose a novel dynamic substitute training attack method to encourage
substitute model to learn better and faster from the target model.
Specifically, a dynamic substitute structure learning strategy is proposed to
adaptively generate optimal substitute model structure via a dynamic gate
according to different target models and tasks. Moreover, we introduce a
task-driven graph-based structure information learning constrain to improve the
quality of generated training data, and facilitate the substitute model
learning structural relationships from the target model multiple outputs.
Extensive experiments have been conducted to verify the efficacy of the
proposed attack method, which can achieve better performance compared with the
state-of-the-art competitors on several datasets.",0,1,0,0,1,0,0.270282,9.0,0.692854,38
79448429-7471-4d08-b65f-f0a6529ae2d8,Boundary-Guided Camouflaged Object Detection,64,0.195601,0.764698,"Camouflaged object detection (COD), segmenting objects that are elegantly
blended into their surroundings, is a valuable yet challenging task. Existing
deep-learning methods often fall into the difficulty of accurately identifying
the camouflaged object with complete and fine object structure. To this end, in
this paper, we propose a novel boundary-guided network (BGNet) for camouflaged
object detection. Our method explores valuable and extra object-related edge
semantics to guide representation learning of COD, which forces the model to
generate features that highlight object structure, thereby promoting
camouflaged object detection of accurate boundary localization. Extensive
experiments on three challenging benchmark datasets demonstrate that our BGNet
significantly outperforms the existing 18 state-of-the-art methods under four
widely-used evaluation metrics. Our code is publicly available at:
https://github.com/thograce/BGNet.",1,1,0,0,1,0,0.403777,5.0,0.546224,41
686726d5-b3ad-412c-be78-cf79de54ba5f,StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,35,0.504375,0.65591,"Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance.",1,1,1,1,0,0,0.335941,9.0,0.72194,44
d425139e-e4f5-4865-93cc-1fb367568ebe,Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion,6,0.0439336,0.353761,"Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task.",0,1,0,0,1,1,0.241044,10.0,0.710252,49
6ac55474-1ef5-40e4-b441-fb68ba58d5a4,Interspace Pruning: Using Adaptive Filter Representations to Improve Training of Sparse CNNs,14,0.0,0.692795,"Unstructured pruning is well suited to reduce the memory footprint of
convolutional neural networks (CNNs), both at training and inference time. CNNs
contain parameters arranged in $K \times K$ filters. Standard unstructured
pruning (SP) reduces the memory footprint of CNNs by setting filter elements to
zero, thereby specifying a fixed subspace that constrains the filter.
Especially if pruning is applied before or during training, this induces a
strong bias. To overcome this, we introduce interspace pruning (IP), a general
tool to improve existing pruning methods. It uses filters represented in a
dynamic interspace by linear combinations of an underlying adaptive filter
basis (FB). For IP, FB coefficients are set to zero while un-pruned
coefficients and FBs are trained jointly. In this work, we provide mathematical
evidence for IP's superior performance and demonstrate that IP outperforms SP
on all tested state-of-the-art unstructured pruning methods. Especially in
challenging situations, like pruning for ImageNet or pruning to high sparsity,
IP greatly exceeds SP with equal runtime and parameter costs. Finally, we show
that advances of IP are due to improved trainability and superior
generalization ability.",0,1,0,0,1,0,0.546739,7.0,0.736638,95
da084bc5-97e3-495d-9650-2d47c0188c2c,Concentration Network for Reinforcement Learning of Large-Scale Multi-Agent Systems,9,0.0914529,0.468815,"When dealing with a series of imminent issues, humans can naturally
concentrate on a subset of these concerning issues by prioritizing them
according to their contributions to motivational indices, e.g., the probability
of winning a game. This idea of concentration offers insights into
reinforcement learning of sophisticated Large-scale Multi-Agent Systems (LMAS)
participated by hundreds of agents. In such an LMAS, each agent receives a long
series of entity observations at each step, which can overwhelm existing
aggregation networks such as graph attention networks and cause inefficiency.
In this paper, we propose a concentration network called ConcNet. First,
ConcNet scores the observed entities considering several motivational indices,
e.g., expected survival time and state value of the agents, and then ranks,
prunes, and aggregates the encodings of observed entities to extract features.
Second, distinct from the well-known attention mechanism, ConcNet has a unique
motivational subnetwork to explicitly consider the motivational indices when
scoring the observed entities. Furthermore, we present a concentration policy
gradient architecture that can learn effective policies in LMAS from scratch.
Extensive experiments demonstrate that the presented architecture has excellent
scalability and flexibility, and significantly outperforms existing methods on
LMAS benchmarks.",1,0,0,0,1,0,0.566567,10.0,0.821146,39
96163f18-8d52-4f63-8db2-df957e193b31,CTM -- A Model for Large-Scale Multi-View Tweet Topic Classification,3,0.193785,0.368056,"Automatically associating social media posts with topics is an important
prerequisite for effective search and recommendation on many social media
platforms. However, topic classification of such posts is quite challenging
because of (a) a large topic space (b) short text with weak topical cues, and
(c) multiple topic associations per post. In contrast to most prior work which
only focuses on post classification into a small number of topics ($10$-$20$),
we consider the task of large-scale topic classification in the context of
Twitter where the topic space is $10$ times larger with potentially multiple
topic associations per Tweet. We address the challenges above by proposing a
novel neural model, CTM that (a) supports a large topic space of $300$ topics
and (b) takes a holistic approach to tweet content modeling -- leveraging
multi-modal content, author context, and deeper semantic cues in the Tweet. Our
method offers an effective way to classify Tweets into topics at scale by
yielding superior performance to other approaches (a relative lift of
$\mathbf{20}\%$ in median average precision score) and has been successfully
deployed in production at Twitter.",0,1,0,0,1,0,0.434402,14.0,0.844873,31
6c240d63-3cf3-4517-9737-adddb3ae1496,An Entropy-based Measure of Intelligence Degree of System Structures,1,0.0136034,0.0827527,"In this paper, we investigate how to measure the intelligence of systems
under specific structures. Two indicators are adopted to characterize the
intelligence of a given structure, namely the function diversity of the
structure, and the ability to generate order under specific environments. A
measure of intelligence degree is proposed, with which the intelligence degree
of several basic structures is calculated. It is shown that some structures are
indeed ""smarter"" than the others under the proposed measure. The results add a
possible way of revealing the evolution mechanism of natural life and
constructing life-like structures with high intelligence degree.",0,0,0,0,0,0,0.0103418,40.0,0.845605,5
573250c3-b569-4110-8a8b-0c6a50b3f150,Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge Graph Completion,18,0.384378,0.834493,"Knowledge Graph Completion (KGC) has been recently extended to multiple
knowledge graph (KG) structures, initiating new research directions, e.g.
static KGC, temporal KGC and few-shot KGC. Previous works often design KGC
models closely coupled with specific graph structures, which inevitably results
in two drawbacks: 1) structure-specific KGC models are mutually incompatible;
2) existing KGC methods are not adaptable to emerging KGs. In this paper, we
propose KG-S2S, a Seq2Seq generative framework that could tackle different
verbalizable graph structures by unifying the representation of KG facts into
""flat"" text, regardless of their original form. To remedy the KG structure
information loss from the ""flat"" text, we further improve the input
representations of entities and relations, and the inference algorithm in
KG-S2S. Experiments on five benchmarks show that KG-S2S outperforms many
competitive baselines, setting new state-of-the-art performance. Finally, we
analyze KG-S2S's ability on the different relations and the Non-entity
Generations.",1,1,0,0,1,0,0.907726,7.0,0.894131,56
9cb2e46b-fc92-440d-a3d1-4ee4b3db4526,Speciesist bias in AI -- How AI applications perpetuate discrimination and unfair outcomes against animals,21,0.0596317,0.482827,"Massive efforts are made to reduce biases in both data and algorithms in
order to render AI applications fair. These efforts are propelled by various
high-profile cases where biased algorithmic decision-making caused harm to
women, people of color, minorities, etc. However, the AI fairness field still
succumbs to a blind spot, namely its insensitivity to discrimination against
animals. This paper is the first to describe the 'speciesist bias' and
investigate it in several different AI systems. Speciesist biases are learned
and solidified by AI applications when they are trained on datasets in which
speciesist patterns prevail. These patterns can be found in image recognition
systems, large language models, and recommender systems. Therefore, AI
technologies currently play a significant role in perpetuating and normalizing
violence against animals. This can only be changed when AI fairness frameworks
widen their scope and include mitigation measures for speciesist biases. This
paper addresses the AI community in this regard and stresses the influence AI
systems can have on either increasing or reducing the violence that is
inflicted on animals, and especially on farmed animals.",0,0,1,0,0,0,0.0673502,8.0,0.465922,176
578cd393-dba6-4c06-88dc-4a8f09fbaff8,Cross-Modal Mutual Learning for Cued Speech Recognition,7,0.187918,0.500924,"Automatic Cued Speech Recognition (ACSR) provides an intelligent
human-machine interface for visual communications, where the Cued Speech (CS)
system utilizes lip movements and hand gestures to code spoken language for
hearing-impaired people. Previous ACSR approaches often utilize direct feature
concatenation as the main fusion paradigm. However, the asynchronous modalities
i.e., lip, hand shape and hand position) in CS may cause interference for
feature concatenation. To address this challenge, we propose a transformer
based cross-modal mutual learning framework to prompt multi-modal interaction.
Compared with the vanilla self-attention, our model forces modality-specific
information of different modalities to pass through a modality-invariant
codebook, collating linguistic representations for tokens of each modality.
Then the shared linguistic knowledge is used to re-synchronize multi-modal
sequences. Moreover, we establish a novel large-scale multi-speaker CS dataset
for Mandarin Chinese. To our knowledge, this is the first work on ACSR for
Mandarin Chinese. Extensive experiments are conducted for different languages
i.e., Chinese, French, and British English). Results demonstrate that our model
exhibits superior recognition performance to the state-of-the-art by a large
margin.",0,0,0,1,1,0,0.173728,6.0,0.4557,19
0e5e2322-4dd5-4447-b9d3-e805a3bba5f5,Adaptable Adapters,11,0.120354,0.682991,"State-of-the-art pretrained NLP models contain a hundred million to trillion
parameters. Adapters provide a parameter-efficient alternative for the full
finetuning in which we can only finetune lightweight neural network layers on
top of pretrained weights. Adapter layers are initialized randomly. However,
existing work uses the same adapter architecture -- i.e., the same adapter
layer on top of each layer of the pretrained model -- for every dataset,
regardless of the properties of the dataset or the amount of available training
data. In this work, we introduce adaptable adapters that contain (1) learning
different activation functions for different layers and different input data,
and (2) a learnable switch to select and only use the beneficial adapter
layers. We show that adaptable adapters achieve on-par performances with the
standard adapter architecture while using a considerably smaller number of
adapter layers. In addition, we show that the selected adapter architecture by
adaptable adapters transfers well across different data settings and similar
tasks. We propose to use adaptable adapters for designing efficient and
effective adapter architectures. The resulting adapters (a) contain about 50%
of the learning parameters of the standard adapter and are therefore more
efficient at training and inference, and require less storage space, and (b)
achieve considerably higher performances in low-data settings.",1,1,1,0,0,0,0.937703,6.0,0.901919,32
80f5e589-c69e-461e-9235-f43b8d301355,A Report on the Euphemisms Detection Shared Task,3,0.00494428,0.0574088,"This paper presents The Shared Task on Euphemism Detection for the Third
Workshop on Figurative Language Processing (FigLang 2022) held in conjunction
with EMNLP 2022. Participants were invited to investigate the euphemism
detection task: given input text, identify whether it contains a euphemism. The
input data is a corpus of sentences containing potentially euphemistic terms
(PETs) collected from the GloWbE corpus (Davies and Fuchs, 2015), and are
human-annotated as containing either a euphemistic or literal usage of a PET.
In this paper, we present the results and analyze the common themes, methods
and findings of the participating teams",0,1,0,0,0,0,0.341578,3.0,0.172691,26
ae2e1af4-1672-4ef5-accf-82e22a360590,minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models,25,0.0728361,0.362375,"We present minicons, an open source library that provides a standard API for
researchers interested in conducting behavioral and representational analyses
of transformer-based language models (LMs). Specifically, minicons enables
researchers to apply analysis methods at two levels: (1) at the prediction
level -- by providing functions to efficiently extract word/sentence level
probabilities; and (2) at the representational level -- by also facilitating
efficient extraction of word/phrase level vectors from one or more layers. In
this paper, we describe the library and apply it to two motivating case
studies: One focusing on the learning dynamics of the BERT architecture on
relative grammatical judgments, and the other on benchmarking 23 different LMs
on zero-shot abductive reasoning. minicons is available at
https://github.com/kanishkamisra/minicons",1,0,0,0,0,0,0.329456,6.0,0.578906,63
d04ff02f-6e4c-48a1-99c2-202f4db50b56,Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation,58,0.673788,0.922059,"The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks.",0,1,0,0,1,0,0.98103,5.0,0.953609,60
91a3a04a-ae6c-429a-8187-720ccc99bb1a,Unified Speech-Text Pre-training for Speech Translation and Recognition,65,0.242426,0.867796,"We describe a method to jointly pre-train speech and text in an
encoder-decoder modeling framework for speech translation and recognition. The
proposed method incorporates four self-supervised and supervised subtasks for
cross modality learning. A self-supervised speech subtask leverages unlabelled
speech data, and a (self-)supervised text to text subtask makes use of abundant
text training data. Two auxiliary supervised speech tasks are included to unify
speech and text modeling space. Our contribution lies in integrating linguistic
information from the text corpus into the speech pre-training. Detailed
analysis reveals learning interference among subtasks. Two pre-training
configurations for speech translation and recognition, respectively, are
presented to alleviate subtask interference. Our experiments show the proposed
method can effectively fuse speech and text information into one model. It
achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the
MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the
Librispeech speech recognition task.",0,1,0,0,1,0,0.440631,5.0,0.569493,58
2e020c17-aee3-4ab9-8dca-3ed656ae73ef,Open-set Recognition via Augmentation-based Similarity Learning,2,0.0299214,0.0686769,"The primary assumption of conventional supervised learning or classification
is that the test samples are drawn from the same distribution as the training
samples, which is called closed set learning or classification. In many
practical scenarios, this is not the case because there are unknowns or unseen
class samples in the test data, which is called the open set scenario, and the
unknowns need to be detected. This problem is referred to as the open set
recognition problem and is important in safety-critical applications. We
propose to detect unknowns (or unseen class samples) through learning pairwise
similarities. The proposed method works in two steps. It first learns a closed
set classifier using the seen classes that have appeared in training and then
learns how to compare seen classes with pseudo-unseen (automatically generated
unseen class samples). The pseudo-unseen generation is carried out by
performing distribution shifting augmentations on the seen or training samples.
We call our method OPG (Open set recognition based on Pseudo unseen data
Generation). The experimental evaluation shows that the learned
similarity-based features can successfully distinguish seen from unseen in
benchmark datasets for open set recognition.",0,1,1,0,0,0,0.960273,7.0,0.937388,28
1feed4e7-0000-4677-91b4-b50d5f9416d2,Overlooked Implications of the Reconstruction Loss for VAE Disentanglement,3,0.0225636,0.135225,"Learning disentangled representations with variational autoencoders (VAEs) is
often attributed to the regularisation component of the loss. In this work, we
highlight the interaction between data and the reconstruction term of the loss
as the main contributor to disentanglement in VAEs. We show that standard
benchmark datasets have unintended correlations between their subjective
ground-truth factors and perceived axes in the data according to typical VAE
reconstruction losses. Our work exploits this relationship to provide a theory
for what constitutes an adversarial dataset under a given reconstruction loss.
We verify this by constructing an example dataset that prevents disentanglement
in state-of-the-art frameworks while maintaining human-intuitive ground-truth
factors. Finally, we re-enable disentanglement by designing an example
reconstruction loss that is once again able to perceive the ground-truth
factors. Our findings demonstrate the subjective nature of disentanglement and
the importance of considering the interaction between the ground-truth factors,
data and notably, the reconstruction loss, which is under-recognised in the
literature.",1,0,0,0,0,0,0.681599,9.0,0.836164,34
713c14c2-6377-4c5e-b5cb-cc9d40707594,Diffusion Models already have a Semantic Latent Space,119,0.785438,0.997318,"Diffusion models achieve outstanding generative performance in various
domains. Despite their great success, they lack semantic latent space which is
essential for controlling the generative process. To address the problem, we
propose asymmetric reverse process (Asyrp) which discovers the semantic latent
space in frozen pretrained diffusion models. Our semantic latent space, named
h-space, has nice properties for accommodating semantic image manipulation:
homogeneity, linearity, robustness, and consistency across timesteps. In
addition, we introduce a principled design of the generative process for
versatile editing and quality boost ing by quantifiable measures: editing
strength of an interval and quality deficiency at a timestep. Our method is
applicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets
(CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page:
https://kwonminki.github.io/Asyrp/",1,0,0,0,0,0,0.965135,4.0,0.900348,48
0ca27dd1-f0a6-4a44-b340-92c69098e6e1,Drawing Causal Inferences About Performance Effects in NLP,1,0.00447739,0.0417684,"This article emphasizes that NLP as a science seeks to make inferences about
the performance effects that result from applying one method (compared to
another method) in the processing of natural language. Yet NLP research in
practice usually does not achieve this goal: In NLP research articles,
typically only a few models are compared. Each model results from a specific
procedural pipeline (here named processing system) that is composed of a
specific collection of methods that are used in preprocessing, pretraining,
hyperparameter tuning, and training on the target task. To make generalizing
inferences about the performance effect that is caused by applying some method
A vs. another method B, it is not sufficient to compare a few specific models
that are produced by a few specific (probably incomparable) processing systems.
Rather, the following procedure would allow drawing inferences about methods'
performance effects: (1) A population of processing systems that researchers
seek to infer to has to be defined. (2) A random sample of processing systems
from this population is drawn. (The drawn processing systems in the sample will
vary with regard to the methods they apply along their procedural pipelines and
also will vary regarding the compositions of their training and test data sets
used for training and evaluation.) (3) Each processing system is applied once
with method A and once with method B. (4) Based on the sample of applied
processing systems, the expected generalization errors of method A and method B
are approximated. (5) The difference between the expected generalization errors
of method A and method B is the estimated average treatment effect due to
applying method A compared to method B in the population of processing systems.",0,0,0,0,0,1,0.0257219,21.0,0.749674,21
62e5a3f5-ee5c-41a5-884f-cdca65b8b3ce,Improving Persian Relation Extraction Models by Data Augmentation,1,0.0219244,0.101971,"Relation extraction that is the task of predicting semantic relation type
between entities in a sentence or document is an important task in natural
language processing. Although there are many researches and datasets for
English, Persian suffers from sufficient researches and comprehensive datasets.
The only available Persian dataset for this task is PERLEX, which is a Persian
expert-translated version of the SemEval-2010-Task-8 dataset. In this paper, we
present our augmented dataset and the results and findings of our system,
participated in the Persian relation Extraction shared task of NSURL 2021
workshop. We use PERLEX as the base dataset and enhance it by applying some
text preprocessing steps and by increasing its size via data augmentation
techniques to improve the generalization and robustness of applied models. We
then employ two different models including ParsBERT and multilingual BERT for
relation extraction on the augmented PERLEX dataset. Our best model obtained
64.67% of Macro-F1 on the test phase of the contest and it achieved 83.68% of
Macro-F1 on the test set of PERLEX.",0,1,0,1,0,0,0.584211,6.0,0.709995,11
04417baf-d00d-4417-a990-e97e5cc934a2,The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error,12,0.0384946,0.441561,"Discourse analysis allows us to attain inferences of a text document that
extend beyond the sentence-level. The current performance of discourse models
is very low on texts outside of the training distribution's coverage,
diminishing the practical utility of existing models. There is need for a
measure that can inform us to what extent our model generalizes from the
training to the test sample when these samples may be drawn from distinct
distributions. While this can be estimated via distribution shift, we argue
that this does not directly correlate with change in the observed error of a
classifier (i.e. error-gap). Thus, we propose to use a statistic from the
theoretical domain adaptation literature which can be directly tied to
error-gap. We study the bias of this statistic as an estimator of error-gap
both theoretically and through a large-scale empirical study of over 2400
experiments on 6 discourse datasets from domains including, but not limited to:
news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not
only motivate our proposal and help us to understand its limitations, but also
provide insight on the properties of discourse models and datasets which
improve performance in domain adaptation. For instance, we find that non-news
datasets are slightly easier to transfer to than news datasets when the
training and test sets are very different. Our code and an associated Python
package are available to allow practitioners to make more informed model and
dataset choices.",0,0,0,0,0,0,0.00443973,14.0,0.498259,71
6c620f23-2b10-428c-aa52-a278671e4109,A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles,28,0.160136,0.868415,"Modern vehicles, including autonomous vehicles and connected vehicles, are
increasingly connected to the external world, which enables various
functionalities and services. However, the improving connectivity also
increases the attack surfaces of the Internet of Vehicles (IoV), causing its
vulnerabilities to cyber-threats. Due to the lack of authentication and
encryption procedures in vehicular networks, Intrusion Detection Systems (IDSs)
are essential approaches to protect modern vehicle systems from network
attacks. In this paper, a transfer learning and ensemble learning-based IDS is
proposed for IoV systems using convolutional neural networks (CNNs) and
hyper-parameter optimization techniques. In the experiments, the proposed IDS
has demonstrated over 99.25% detection rates and F1-scores on two well-known
public benchmark IoV security datasets: the Car-Hacking dataset and the
CICIDS2017 dataset. This shows the effectiveness of the proposed IDS for
cyber-attack detection in both intra-vehicle and external vehicular networks.",1,1,0,0,1,0,0.315478,5.0,0.484087,23
be507025-d966-4dae-b916-184065616fec,BotSIM: An End-to-End Bot Simulation Toolkit for Commercial Task-Oriented Dialog Systems,1,0.0098,0.0937102,"We introduce BotSIM, a modular, open-source Bot SIMulation environment with
dialog generation, user simulation and conversation analytics capabilities.
BotSIM aims to serve as a one-stop solution for large-scale data-efficient
end-to-end evaluation, diagnosis and remediation of commercial task-oriented
dialog (TOD) systems to significantly accelerate commercial bot development and
evaluation, reduce cost and time-to-market. BotSIM adopts a layered design
comprising the infrastructure layer, the adaptor layer and the application
layer. The infrastructure layer hosts key models and components to support
BotSIM's major functionalities via a streamlined
""generation-simulation-remediation"" pipeline. The adaptor layer is used to
extend BotSIM to accommodate new bot platforms. The application layer provides
a suite of command line tools and a Web App to significantly lower the entry
barrier for BotSIM users such as bot admins or practitioners. In this report,
we focus on the technical designs of various system components. A detailed case
study using Einstein BotBuilder is also presented to show how to apply BotSIM
pipeline for bot evaluation and remediation. The detailed system descriptions
can be found in our system demo paper. The toolkit is available at:
https://github.com/salesforce/BotSIM .",1,1,0,0,0,0,0.220582,9.0,0.666788,14
47d43878-fb81-4fd5-8f84-72e13a8e0aed,VQ-Flows: Vector Quantized Local Normalizing Flows,6,0.334723,0.379527,"Normalizing flows provide an elegant approach to generative modeling that
allows for efficient sampling and exact density evaluation of unknown data
distributions. However, current techniques have significant limitations in
their expressivity when the data distribution is supported on a low-dimensional
manifold or has a non-trivial topology. We introduce a novel statistical
framework for learning a mixture of local normalizing flows as ""chart maps""
over the data manifold. Our framework augments the expressivity of recent
approaches while preserving the signature property of normalizing flows, that
they admit exact density evaluation. We learn a suitable atlas of charts for
the data manifold via a vector quantized auto-encoder (VQ-AE) and the
distributions over them using a conditional flow. We validate experimentally
that our probabilistic framework enables existing approaches to better model
data distributions over complex manifolds.",0,0,0,0,0,0,0.979783,7.0,0.96455,44
a5a5c8cb-c8e4-4d2a-a92d-a6ff30a82900,Personalizing Sustainable Agriculture with Causal Machine Learning,6,0.102559,0.572473,"To fight climate change and accommodate the increasing population, global
crop production has to be strengthened. To achieve the ""sustainable
intensification"" of agriculture, transforming it from carbon emitter to carbon
sink is a priority, and understanding the environmental impact of agricultural
management practices is a fundamental prerequisite to that. At the same time,
the global agricultural landscape is deeply heterogeneous, with differences in
climate, soil, and land use inducing variations in how agricultural systems
respond to farmer actions. The ""personalization"" of sustainable agriculture
with the provision of locally adapted management advice is thus a necessary
condition for the efficient uplift of green metrics, and an integral
development in imminent policies. Here, we formulate personalized sustainable
agriculture as a Conditional Average Treatment Effect estimation task and use
Causal Machine Learning for tackling it. Leveraging climate data, land use
information and employing Double Machine Learning, we estimate the
heterogeneous effect of sustainable practices on the field-level Soil Organic
Carbon content in Lithuania. We thus provide a data-driven perspective for
targeting sustainable practices and effectively expanding the global carbon
sink.",0,1,0,0,0,0,0.435982,13.0,0.833317,25
8a43500a-3725-4cd0-b6b4-2d08be215d13,MINER: Multiscale Implicit Neural Representations,19,0.0819187,0.412058,"We introduce a new neural signal model designed for efficient high-resolution
representation of large-scale signals. The key innovation in our multiscale
implicit neural representation (MINER) is an internal representation via a
Laplacian pyramid, which provides a sparse multiscale decomposition of the
signal that captures orthogonal parts of the signal across scales. We leverage
the advantages of the Laplacian pyramid by representing small disjoint patches
of the pyramid at each scale with a small MLP. This enables the capacity of the
network to adaptively increase from coarse to fine scales, and only represent
parts of the signal with strong signal energy. The parameters of each MLP are
optimized from coarse-to-fine scale which results in faster approximations at
coarser scales, thereby ultimately an extremely fast training process. We apply
MINER to a range of large-scale signal representation tasks, including
gigapixel images and very large point clouds, and demonstrate that it requires
fewer than 25% of the parameters, 33% of the memory footprint, and 10% of the
computation time of competing techniques such as ACORN to reach the same
representation accuracy.",1,0,1,0,1,0,0.822968,4.0,0.734877,33
096618d5-6312-4823-a0ad-7fa0ee636cd3,C-VTON: Context-Driven Image-Based Virtual Try-On Network,36,0.507761,0.999691,"Image-based virtual try-on techniques have shown great promise for enhancing
the user-experience and improving customer satisfaction on fashion-oriented
e-commerce platforms. However, existing techniques are currently still limited
in the quality of the try-on results they are able to produce from input images
of diverse characteristics. In this work, we propose a Context-Driven Virtual
Try-On Network (C-VTON) that addresses these limitations and convincingly
transfers selected clothing items to the target subjects even under challenging
pose configurations and in the presence of self-occlusions. At the core of the
C-VTON pipeline are: (i) a geometric matching procedure that efficiently aligns
the target clothing with the pose of the person in the input images, and (ii) a
powerful image generator that utilizes various types of contextual information
when synthesizing the final try-on result. C-VTON is evaluated in rigorous
experiments on the VITON and MPV datasets and in comparison to state-of-the-art
techniques from the literature. Experimental results show that the proposed
approach is able to produce photo-realistic and visually convincing results and
significantly improves on the existing state-of-the-art.",1,1,0,0,1,0,0.919812,7.0,0.902309,36
57580ee0-bd01-4b95-964b-acf48cd1e444,Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization,17,0.359358,0.301302,"Commonsense question answering (CQA) aims to test if models can answer
questions regarding commonsense knowledge that everyone knows. Prior works that
incorporate external knowledge bases have shown promising results, but
knowledge bases are expensive to construct and are often limited to a fixed set
of relations. In this paper, we instead focus on better utilizing the
\textit{implicit knowledge} stored in pre-trained language models. While
researchers have found that the knowledge embedded in pre-trained language
models can be extracted by having them fill in the blanks of carefully designed
prompts for relation extraction and text classification, it remains unclear if
we can adopt this paradigm in CQA where the inputs and outputs take much more
flexible forms. To this end, we investigate four translation methods that can
translate natural questions into cloze-style sentences to better solicit
commonsense knowledge from language models, including a syntactic-based model,
an unsupervised neural model, and two supervised neural models. In addition, to
combine the different translation methods, we propose to encourage consistency
among model predictions on different translated questions with unlabeled data.
We demonstrate the effectiveness of our methods on three CQA datasets in
zero-shot settings. We show that our methods are complementary to a knowledge
base improved model, and combining them can lead to state-of-the-art zero-shot
performance. Analyses also reveal distinct characteristics of the different
cloze translation methods and provide insights on why combining them can lead
to great improvements.",1,0,0,0,0,0,0.965905,5.0,0.921605,43
5e76edbf-04fc-4d06-aed2-b49a91f4a5de,Synthetic Image Data for Deep Learning,1,0.0111643,0.0847502,"Realistic synthetic image data rendered from 3D models can be used to augment
image sets and train image classification semantic segmentation models. In this
work, we explore how high quality physically-based rendering and domain
randomization can efficiently create a large synthetic dataset based on
production 3D CAD models of a real vehicle. We use this dataset to quantify the
effectiveness of synthetic augmentation using U-net and Double-U-net models. We
found that, for this domain, synthetic images were an effective technique for
augmenting limited sets of real training data. We observed that models trained
on purely synthetic images had a very low mean prediction IoU on real
validation images. We also observed that adding even very small amounts of real
images to a synthetic dataset greatly improved accuracy, and that models
trained on datasets augmented with synthetic images were more accurate than
those trained on real images alone. Finally, we found that in use cases that
benefit from incremental training or model specialization, pretraining a base
model on synthetic images provided a sizeable reduction in the training cost of
transfer learning, allowing up to 90\% of the model training to be
front-loaded.",0,1,0,1,0,0,0.129547,11.0,0.674129,35
97a33ed9-51f6-469f-981c-93e8b09e7016,VideoDex: Learning Dexterity from Internet Videos,36,0.101027,0.842396,"To build general robotic agents that can operate in many environments, it is
often imperative for the robot to collect experience in the real world.
However, this is often not feasible due to safety, time, and hardware
restrictions. We thus propose leveraging the next best thing as real-world
experience: internet videos of humans using their hands. Visual priors, such as
visual features, are often learned from videos, but we believe that more
information from videos can be utilized as a stronger prior. We build a
learning algorithm, VideoDex, that leverages visual, action, and physical
priors from human video datasets to guide robot behavior. These actions and
physical priors in the neural network dictate the typical human behavior for a
particular robot task. We test our approach on a robot arm and dexterous
hand-based system and show strong results on various manipulation tasks,
outperforming various state-of-the-art methods. Videos at
https://video-dex.github.io",0,1,0,0,1,0,0.290798,7.0,0.617475,73
59855050-1faf-418a-a97e-a7f8c7a861f0,NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles,5,0.113314,0.304544,"In this paper, we present the fifth installment of the NELA-GT datasets,
NELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between
January 1st, 2022 and December 31st, 2022. Just as in past releases of the
dataset, NELA-GT-2022 includes outlet-level veracity labels from Media
Bias/Fact Check and tweets embedded in collected news articles. The
NELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H",1,1,0,1,0,0,0.683196,5.0,0.705973,12
8b3be452-cf46-41ee-a851-82f8a44438bb,Global Convergence Analysis of Deep Linear Networks with A One-neuron Layer,1,0.00252535,0.0564881,"In this paper, we follow Eftekhari's work to give a non-local convergence
analysis of deep linear networks. Specifically, we consider optimizing deep
linear networks which have a layer with one neuron under quadratic loss. We
describe the convergent point of trajectories with arbitrary starting point
under gradient flow, including the paths which converge to one of the saddle
points or the original point. We also show specific convergence rates of
trajectories that converge to the global minimizer by stages. To achieve these
results, this paper mainly extends the machinery in Eftekhari's work to
provably identify the rank-stable set and the global minimizer convergent set.
We also give specific examples to show the necessity of our definitions.
Crucially, as far as we know, our results appear to be the first to give a
non-local global analysis of linear neural networks from arbitrary initialized
points, rather than the lazy training regime which has dominated the literature
of neural networks, and restricted benign initialization in Eftekhari's work.
We also note that extending our results to general linear networks without one
hidden neuron assumption remains a challenging open problem.",0,0,0,0,0,0,0.0268309,8.0,0.348241,45
25f71a7d-e01b-4ab5-b63e-e812c59d6cce,Multi-Event-Camera Depth Estimation and Outlier Rejection by Refocused Events Fusion,16,0.163279,0.825855,"Event cameras are bio-inspired sensors that offer advantages over traditional
cameras. They operate asynchronously, sampling the scene at microsecond
resolution and producing a stream of brightness changes. This unconventional
output has sparked novel computer vision methods to unlock the camera's
potential. Here, the problem of event-based stereo 3D reconstruction for SLAM
is considered. Most event-based stereo methods attempt to exploit the high
temporal resolution of the camera and the simultaneity of events across cameras
to establish matches and estimate depth. By contrast, this work investigates
how to estimate depth without explicit data association by fusing Disparity
Space Images (DSIs) originated in efficient monocular methods. Fusion theory is
developed and applied to design multi-camera 3D reconstruction algorithms that
produce state-of-the-art results, as confirmed by comparisons with four
baseline methods and tests on a variety of available datasets.",1,0,0,0,1,0,0.175551,8.0,0.593213,87
91276a66-f285-4175-bf2e-74190901f516,"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",70,0.321793,0.996026,"The formalization of existing mathematical proofs is a notoriously difficult
process. Despite decades of research on automation and proof assistants,
writing formal proofs remains arduous and only accessible to a few experts.
While previous studies to automate formalization focused on powerful search
algorithms, no attempts were made to take advantage of available informal
proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method
that maps informal proofs to formal proof sketches, and uses the sketches to
guide an automated prover by directing its search to easier sub-problems. We
investigate two relevant setups where informal proofs are either written by
humans or generated by a language model. Our experiments and ablation studies
show that large language models are able to produce well-structured formal
sketches that follow the same reasoning steps as the informal proofs. Guiding
an automated prover with these sketches enhances its performance from 20.9% to
39.3% on a collection of mathematical competition problems.",1,0,0,0,0,0,0.415021,5.0,0.553455,42
2fc9ef0f-d8e0-4b91-9c0d-5024e22fac0e,Automatic Related Work Generation: A Meta Study,6,0.325013,0.391008,"Academic research is an exploration activity to solve problems that have
never been resolved before. By this nature, each academic research work is
required to perform a literature review to distinguish its novelties that have
not been addressed by prior works. In natural language processing, this
literature review is usually conducted under the ""Related Work"" section. The
task of automatic related work generation aims to automatically generate the
""Related Work"" section given the rest of the research paper and a list of cited
papers. Although this task was proposed over 10 years ago, it received little
attention until very recently, when it was cast as a variant of the scientific
multi-document summarization problem. However, even today, the problems of
automatic related work and citation text generation are not yet standardized.
In this survey, we conduct a meta-study to compare the existing literature on
related work generation from the perspectives of problem formulation, dataset
collection, methodological approach, performance evaluation, and future
prospects to provide the reader insight into the progress of the
state-of-the-art studies, as well as and how future studies can be conducted.
We also survey relevant fields of study that we suggest future work to consider
integrating.",0,0,0,0,0,0,0.739751,12.0,0.890649,124
8c168188-645f-49af-bcd4-e38100f7b88b,Latent Space Unsupervised Semantic Segmentation,2,0.034226,0.283641,"The development of compact and energy-efficient wearable sensors has led to
an increase in the availability of biosignals. To analyze these continuously
recorded, and often multidimensional, time series at scale, being able to
conduct meaningful unsupervised data segmentation is an auspicious target. A
common way to achieve this is to identify change-points within the time series
as the segmentation basis. However, traditional change-point detection
algorithms often come with drawbacks, limiting their real-world applicability.
Notably, they generally rely on the complete time series to be available and
thus cannot be used for real-time applications. Another common limitation is
that they poorly (or cannot) handle the segmentation of multidimensional time
series. Consequently, the main contribution of this work is to propose a novel
unsupervised segmentation algorithm for multidimensional time series named
Latent Space Unsupervised Semantic Segmentation (LS-USS), which was designed to
work easily with both online and batch data. When comparing LS-USS against
other state-of-the-art change-point detection algorithms on a variety of
real-world datasets, in both the offline and real-time setting, LS-USS
systematically achieves on par or better performances.",0,1,0,0,1,0,0.172004,14.0,0.765944,57
bfed2527-6b64-445c-ade1-66764a1ccef3,Learned Digital Back-Propagation for Dual-Polarization Dispersion Managed Systems,1,0.00654645,0.111187,"Digital back-propagation (DBP) and learned DBP (LDBP) are proposed for
nonlinearity mitigation in WDM dual-polarization dispersion-managed systems.
LDBP achieves Q-factor improvement of 1.8 dB and 1.2 dB, respectively, over
linear equalization and a variant of DBP adapted to DM systems.",0,1,0,0,0,0,0.0200066,10.0,0.448896,8
5b1bd9a9-be38-4e8a-ac80-9238b2876521,ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select,14,0.108082,0.64656,"We study the problem of extracting N-ary relation tuples from scientific
articles. This task is challenging because the target knowledge tuples can
reside in multiple parts and modalities of the document. Our proposed method
ReSel decomposes this task into a two-stage procedure that first retrieves the
most relevant paragraph/table and then selects the target entity from the
retrieved component. For the high-level retrieval stage, ReSel designs a simple
and effective feature set, which captures multi-level lexical and semantic
similarities between the query and components. For the low-level selection
stage, ReSel designs a cross-modal entity correlation graph along with a
multi-view architecture, which models both semantic and document-structural
relations between entities. Our experiments on three scientific information
extraction datasets show that ReSel outperforms state-of-the-art baselines
significantly.",1,1,0,0,1,0,0.59213,6.0,0.713608,37
54a32a81-4eb6-42dd-8b92-d910b95e4a22,Towards Involving End-users in Interactive Human-in-the-loop AI Fairness,22,0.464008,0.745923,"Ensuring fairness in artificial intelligence (AI) is important to counteract
bias and discrimination in far-reaching applications. Recent work has started
to investigate how humans judge fairness and how to support machine learning
(ML) experts in making their AI models fairer. Drawing inspiration from an
Explainable AI (XAI) approach called \emph{explanatory debugging} used in
interactive machine learning, our work explores designing interpretable and
interactive human-in-the-loop interfaces that allow ordinary end-users without
any technical or domain background to identify potential fairness issues and
possibly fix them in the context of loan decisions. Through workshops with
end-users, we co-designed and implemented a prototype system that allowed
end-users to see why predictions were made, and then to change weights on
features to ""debug"" fairness issues. We evaluated the use of this prototype
system through an online study. To investigate the implications of diverse
human values about fairness around the globe, we also explored how cultural
dimensions might play a role in using this prototype. Our results contribute to
the design of interfaces to allow end-users to be involved in judging and
addressing AI fairness through a human-in-the-loop approach.",0,1,0,0,0,0,0.758662,9.0,0.860257,88
8e785d1f-de24-47ef-aea4-9155bdd97556,FFC-SE: Fast Fourier Convolution for Speech Enhancement,11,0.0816325,0.590635,"Fast Fourier convolution (FFC) is the recently proposed neural operator
showing promising performance in several computer vision problems. The FFC
operator allows employing large receptive field operations within early layers
of the neural network. It was shown to be especially helpful for inpainting of
periodic structures which are common in audio processing. In this work, we
design neural network architectures which adapt FFC for speech enhancement. We
hypothesize that a large receptive field allows these networks to produce more
coherent phases than vanilla convolutional models, and validate this hypothesis
experimentally. We found that neural networks based on Fast Fourier convolution
outperform analogous convolutional models and show better or comparable results
with other speech enhancement baselines.",0,1,0,0,0,0,0.737577,6.0,0.780265,29
47f05b1e-4bc4-499b-bdb0-34ba454d61c1,Memory Efficient Continual Learning with Transformers,24,0.242752,0.422233,"In many real-world scenarios, data to train machine learning models becomes
available over time. Unfortunately, these models struggle to continually learn
new concepts without forgetting what has been learnt in the past. This
phenomenon is known as catastrophic forgetting and it is difficult to prevent
due to practical constraints. For instance, the amount of data that can be
stored or the computational resources that can be used might be limited.
Moreover, applications increasingly rely on large pre-trained neural networks,
such as pre-trained Transformers, since the resources or data might not be
available in sufficiently large quantities to practitioners to train the model
from scratch. In this paper, we devise a method to incrementally train a model
on a sequence of tasks using pre-trained Transformers and extending them with
Adapters. Different than the existing approaches, our method is able to scale
to a large number of tasks without significant overhead and allows sharing
information across tasks. On both image and text classification tasks, we
empirically demonstrate that our method maintains a good predictive performance
without retraining the model or increasing the number of model parameters over
time. The resulting model is also significantly faster at inference time
compared to Adapter-based state-of-the-art methods.",0,1,0,0,1,0,0.71228,6.0,0.768386,91
b478efb5-e0d3-4dfa-9721-fc6c236b11f9,Unsupervised Discovery and Composition of Object Light Fields,26,0.320606,0.982555,"Neural scene representations, both continuous and discrete, have recently
emerged as a powerful new paradigm for 3D scene understanding. Recent efforts
have tackled unsupervised discovery of object-centric neural scene
representations. However, the high cost of ray-marching, exacerbated by the
fact that each object representation has to be ray-marched separately, leads to
insufficiently sampled radiance fields and thus, noisy renderings, poor
framerates, and high memory and time complexity during training and rendering.
Here, we propose to represent objects in an object-centric, compositional scene
representation as light fields. We propose a novel light field compositor
module that enables reconstructing the global light field from a set of
object-centric light fields. Dubbed Compositional Object Light Fields (COLF),
our method enables unsupervised learning of object-centric neural scene
representations, state-of-the-art reconstruction and novel view synthesis
performance on standard datasets, and rendering and training speeds at orders
of magnitude faster than existing 3D approaches.",0,0,0,0,1,0,0.902055,6.0,0.872262,59
f5468bf9-fa8e-4ad7-bb49-6c5db388fe73,Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,15,0.763791,0.795867,"End-to-end speech-to-speech translation (S2ST) without relying on
intermediate text representations is a rapidly emerging frontier of research.
Recent works have demonstrated that the performance of such direct S2ST systems
is approaching that of conventional cascade S2ST when trained on comparable
datasets. However, in practice, the performance of direct S2ST is bounded by
the availability of paired S2ST training data. In this work, we explore
multiple approaches for leveraging much more widely available unsupervised and
weakly-supervised speech and text data to improve the performance of direct
S2ST based on Translatotron 2. With our most effective approaches, the average
translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is
improved by +13.6 BLEU (or +113% relatively), as compared to the previous
state-of-the-art trained without additional data. The improvements on
low-resource language are even more significant (+398% relatively on average).
Our comparative studies suggest future research directions for S2ST and speech
representation learning.",0,1,0,0,1,0,0.990308,4.0,0.981127,52
158389ae-20ab-45dd-990c-313b6cc13613,SLING: Sino Linguistic Evaluation of Large Language Models,2,0.0201304,0.308248,"To understand what kinds of linguistic knowledge are encoded by pretrained
Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics
(SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese
grouped into 9 high-level linguistic phenomena. Each pair demonstrates the
acceptability contrast of a specific syntactic or semantic phenomenon (e.g.,
The keys are lost vs. The keys is lost), and an LM should assign lower
perplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang
et al., 2021), which also contains Chinese minimal pairs and was created by
translating the vocabulary of the English BLiMP dataset, the minimal pairs in
SLING are derived primarily by applying syntactic and lexical transformations
to naturally-occurring, linguist-annotated sentences from the Chinese Treebank
9.0, thus addressing severe issues in CLiMP's data generation process. We test
18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and
multi-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show
that the average accuracy for LMs is far below human performance (69.7% vs.
97.1%), while BERT-base-zh achieves the highest accuracy (84.8%) of all tested
LMs, even much larger ones. Additionally, we find that most LMs have a strong
gender and number (singular/plural) bias, and they perform better on local
phenomena than hierarchical ones.",1,1,1,1,0,0,0.15708,8.0,0.577974,64
1fa80982-7e36-4db4-a6b2-c05a83451f53,Stain Based Contrastive Co-training for Histopathological Image Analysis,1,0.0187239,0.0816987,"We propose a novel semi-supervised learning approach for classification of
histopathology images. We employ strong supervision with patch-level
annotations combined with a novel co-training loss to create a semi-supervised
learning framework. Co-training relies on multiple conditionally independent
and sufficient views of the data. We separate the hematoxylin and eosin
channels in pathology images using color deconvolution to create two views of
each slide that can partially fulfill these requirements. Two separate CNNs are
used to embed the two views into a joint feature space. We use a contrastive
loss between the views in this feature space to implement co-training. We
evaluate our approach in clear cell renal cell and prostate carcinomas, and
demonstrate improvement over state-of-the-art semi-supervised learning methods.",0,1,0,0,1,0,0.784969,6.0,0.803396,23
14bada1b-2a4c-4ab9-a648-8a393952b84e,ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer,3,0.140882,0.494068,"The problem of multimodal intent and trajectory prediction for human-driven
vehicles in parking lots is addressed in this paper. Using models designed with
CNN and Transformer networks, we extract temporal-spatial and contextual
information from trajectory history and local bird's eye view (BEV) semantic
images, and generate predictions about intent distribution and future
trajectory sequences. Our methods outperform existing models in accuracy, while
allowing an arbitrary number of modes, encoding complex multi-agent scenarios,
and adapting to different parking maps. To train and evaluate our method, we
present the first public 4K video dataset of human driving in parking lots with
accurate annotation, high frame rate, and rich traffic scenarios.",0,1,0,1,0,0,0.773678,8.0,0.848314,19
19401553-0036-4474-a5d9-4bc11f5ca1bb,Continual Learning with Recursive Gradient Optimization,23,0.221704,0.38371,"Learning multiple tasks sequentially without forgetting previous knowledge,
called Continual Learning(CL), remains a long-standing challenge for neural
networks. Most existing methods rely on additional network capacity or data
replay. In contrast, we introduce a novel approach which we refer to as
Recursive Gradient Optimization(RGO). RGO is composed of an iteratively updated
optimizer that modifies the gradient to minimize forgetting without data replay
and a virtual Feature Encoding Layer(FEL) that represents different long-term
structures with only task descriptors. Experiments demonstrate that RGO has
significantly better performance on popular continual classification benchmarks
when compared to the baselines and achieves new state-of-the-art performance on
20-split-CIFAR100(82.22%) and 20-split-miniImageNet(72.63%). With higher
average accuracy than Single-Task Learning(STL), this method is flexible and
reliable to provide continual learning capabilities for learning models that
rely on gradient descent.",0,1,0,0,1,0,0.580884,9.0,0.80565,43
47abc17a-954e-4a33-8348-cd622383a6f0,PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition,13,0.163179,0.929234,"The widely studied task of Natural Language Inference (NLI) requires a system
to recognize whether one piece of text is textually entailed by another, i.e.
whether the entirety of its meaning can be inferred from the other. In current
NLI datasets and models, textual entailment relations are typically defined on
the sentence- or paragraph-level. However, even a simple sentence often
contains multiple propositions, i.e. distinct units of meaning conveyed by the
sentence. As these propositions can carry different truth values in the context
of a given premise, we argue for the need to recognize the textual entailment
relation of each proposition in a sentence individually.
  We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert
human raters. Our dataset structure resembles the tasks of (1) segmenting
sentences within a document to the set of propositions, and (2) classifying the
entailment relation of each proposition with respect to a different yet
topically-aligned document, i.e. documents describing the same event or entity.
We establish strong baselines for the segmentation and entailment tasks.
Through case studies on summary hallucination detection and document-level NLI,
we demonstrate that our conceptual framework is potentially useful for
understanding and explaining the compositionality of NLI labels.",1,0,1,1,0,0,0.361426,10.0,0.758875,54
1cefc448-241c-46ef-b349-60a96b98b5a3,PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion,5,0.12943,0.417023,"This paper presents a parameter-lite transfer learning approach of pretrained
language models (LM) for knowledge graph (KG) completion. Instead of
finetuning, which modifies all LM parameters, we only tune a few new parameters
while keeping the original LM parameters fixed. We establish this via
reformulating KG completion as a ""fill-in-the-blank"" task, and introducing a
parameter-lite encoder on top of the original LMs. We show that, by tuning far
fewer parameters than finetuning, LMs transfer non-trivially to most tasks and
reach competitiveness with prior state-of-the-art approaches. For instance, we
outperform the fully finetuning approaches on a KG completion benchmark by
tuning only 1% of the parameters. The code and datasets are available at
\url{https://github.com/yuanyehome/PALT}.",1,1,0,0,1,0,0.904256,8.0,0.905413,71
08feb40c-5081-4971-86c8-d74ee81d6608,SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup Training,3,0.00708466,0.132541,"The conventional success of textual classification relies on annotated data,
and the new paradigm of pre-trained language models (PLMs) still requires a few
labeled data for downstream tasks. However, in real-world applications, label
noise inevitably exists in training data, damaging the effectiveness,
robustness, and generalization of the models constructed on such data.
Recently, remarkable achievements have been made to mitigate this dilemma in
visual data, while only a few explore textual data. To fill this gap, we
present SelfMix, a simple yet effective method, to handle label noise in text
classification tasks. SelfMix uses the Gaussian Mixture Model to separate
samples and leverages semi-supervised learning. Unlike previous works requiring
multiple models, our method utilizes the dropout mechanism on a single model to
reduce the confirmation bias in self-training and introduces a textual-level
mixup training strategy. Experimental results on three text classification
benchmarks with different types of text show that the performance of our
proposed method outperforms these strong baselines designed for both textual
and visual data under different noise ratios and noise types. Our code is
available at https://github.com/noise-learning/SelfMix.",1,1,0,0,0,0,0.105201,8.0,0.524218,45
c01ebe93-e254-4fe9-ba56-bb624bd5e9ed,Transforming Gait: Video-Based Spatiotemporal Gait Analysis,9,0.112398,0.709089,"Human pose estimation from monocular video is a rapidly advancing field that
offers great promise to human movement science and rehabilitation. This
potential is tempered by the smaller body of work ensuring the outputs are
clinically meaningful and properly calibrated. Gait analysis, typically
performed in a dedicated lab, produces precise measurements including
kinematics and step timing. Using over 7000 monocular video from an
instrumented gait analysis lab, we trained a neural network to map 3D joint
trajectories and the height of individuals onto interpretable biomechanical
outputs including gait cycle timing and sagittal plane joint kinematics and
spatiotemporal trajectories. This task specific layer produces accurate
estimates of the timing of foot contact and foot off events. After parsing the
kinematic outputs into individual gait cycles, it also enables accurate
cycle-by-cycle estimates of cadence, step time, double and single support time,
walking speed and step length.",0,1,0,0,0,0,0.878762,5.0,0.82745,43
078b8eed-f9e6-4930-ae99-5098a8a0a46b,DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering,14,0.537781,0.705743,"Spoken Question Answering (SQA) is to find the answer from a spoken document
given a question, which is crucial for personal assistants when replying to the
queries from the users. Existing SQA methods all rely on Automatic Speech
Recognition (ASR) transcripts. Not only does ASR need to be trained with
massive annotated data that are time and cost-prohibitive to collect for
low-resourced languages, but more importantly, very often the answers to the
questions include name entities or out-of-vocabulary words that cannot be
recognized correctly. Also, ASR aims to minimize recognition errors equally
over all words, including many function words irrelevant to the SQA task.
Therefore, SQA without ASR transcripts (textless) is always highly desired,
although known to be very difficult.
  This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging
unlabeled data for pre-training and fine-tuned by the SQA downstream task. The
time intervals of spoken answers can be directly predicted from spoken
documents. We also release a new SQA benchmark corpus, NMSQA, for data with
more realistic scenarios. We empirically showed that DUAL yields results
comparable to those obtained by cascading ASR and text QA model and robust to
real-world data. Our code and model will be open-sourced.",0,1,0,1,0,0,0.97233,3.0,0.889331,32
669635fe-637c-41cb-9518-e1af09c560bc,Generative Modelling With Inverse Heat Dissipation,59,0.754736,0.999999,"While diffusion models have shown great success in image generation, their
noise-inverting generative process does not explicitly consider the structure
of images, such as their inherent multi-scale nature. Inspired by diffusion
models and the empirical success of coarse-to-fine modelling, we propose a new
diffusion-like model that generates images through stochastically reversing the
heat equation, a PDE that locally erases fine-scale information when run over
the 2D plane of the image. We interpret the solution of the forward heat
equation with constant additive noise as a variational approximation in the
diffusion latent variable model. Our new model shows emergent qualitative
properties not seen in standard diffusion models, such as disentanglement of
overall colour and shape in images. Spectral analysis on natural images
highlights connections to diffusion models and reveals an implicit
coarse-to-fine inductive bias in them.",1,0,0,0,0,0,0.979155,5.0,0.948795,93
06695a10-f218-4468-bcbe-c57e63eb4406,RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation,29,0.305479,0.978366,"Category-level object pose estimation aims to predict the 6D pose as well as
the 3D metric size of arbitrary objects from a known set of categories. Recent
methods harness shape prior adaptation to map the observed point cloud into the
canonical space and apply Umeyama algorithm to recover the pose and size.
However, their shape prior integration strategy boosts pose estimation
indirectly, which leads to insufficient pose-sensitive feature extraction and
slow inference speed. To tackle this problem, in this paper, we propose a novel
geometry-guided Residual Object Bounding Box Projection network RBP-Pose that
jointly predicts object pose and residual vectors describing the displacements
from the shape-prior-indicated object surface projections on the bounding box
towards the real surface projections. Such definition of residual vectors is
inherently zero-mean and relatively small, and explicitly encapsulates spatial
cues of the 3D object for robust and accurate pose regression. We enforce
geometry-aware consistency terms to align the predicted pose and residual
vectors to further boost performance.",1,1,0,0,1,0,0.809131,6.0,0.815845,53
3e1e7850-0a64-4b99-b89a-16f263d5837a,Federated Non-negative Matrix Factorization for Short Texts Topic Modeling with Mutual Information,7,0.0291427,0.124915,"Non-negative matrix factorization (NMF) based topic modeling is widely used
in natural language processing (NLP) to uncover hidden topics of short text
documents. Usually, training a high-quality topic model requires large amount
of textual data. In many real-world scenarios, customer textual data should be
private and sensitive, precluding uploading to data centers. This paper
proposes a Federated NMF (FedNMF) framework, which allows multiple clients to
collaboratively train a high-quality NMF based topic model with locally stored
data. However, standard federated learning will significantly undermine the
performance of topic models in downstream tasks (e.g., text classification)
when the data distribution over clients is heterogeneous. To alleviate this
issue, we further propose FedNMF+MI, which simultaneously maximizes the mutual
information (MI) between the count features of local texts and their topic
weight vectors to mitigate the performance degradation. Experimental results
show that our FedNMF+MI methods outperform Federated Latent Dirichlet
Allocation (FedLDA) and the FedNMF without MI methods for short texts by a
significant margin on both coherence score and classification F1 score.",0,1,0,0,0,0,0.188577,7.0,0.546426,40
8f711f2e-2906-4fc8-8dec-6a8fb28ff82c,Mildly Conservative Q-Learning for Offline Reinforcement Learning,44,0.670807,0.854359,"Offline reinforcement learning (RL) defines the task of learning from a
static logged dataset without continually interacting with the environment. The
distribution shift between the learned policy and the behavior policy makes it
necessary for the value function to stay conservative such that
out-of-distribution (OOD) actions will not be severely overestimated. However,
existing approaches, penalizing the unseen actions or regularizing with the
behavior policy, are too pessimistic, which suppresses the generalization of
the value function and hinders the performance improvement. This paper explores
mild but enough conservatism for offline learning while not harming
generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD
actions are actively trained by assigning them proper pseudo Q values. We
theoretically show that MCQ induces a policy that behaves at least as well as
the behavior policy and no erroneous overestimation will occur for OOD actions.
Experimental results on the D4RL benchmarks demonstrate that MCQ achieves
remarkable performance compared with prior work. Furthermore, MCQ shows
superior generalization ability when transferring from offline to online, and
significantly outperforms baselines. Our code is publicly available at
https://github.com/dmksjfl/MCQ.",1,1,0,0,1,0,0.937012,5.0,0.881507,81
45ccef75-0249-4f46-a669-524494f9db6e,M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation,9,0.232842,0.21993,"End-to-end speech-to-text translation models are often initialized with
pre-trained speech encoder and pre-trained text decoder. This leads to a
significant training gap between pre-training and fine-tuning, largely due to
the modality differences between speech outputs from the encoder and text
inputs to the decoder. In this work, we aim to bridge the modality gap between
speech and text to improve translation quality. We propose M-Adapter, a novel
Transformer-based module, to adapt speech representations to text. While
shrinking the speech sequence, M-Adapter produces features desired for
speech-to-text translation via modelling global and local dependencies of a
speech sequence. Our experimental results show that our model outperforms a
strong baseline by up to 1 BLEU score on the Must-C En$\rightarrow$DE
dataset.\footnote{Our code is available at
https://github.com/mingzi151/w2v2-st.}",1,0,0,0,0,0,0.884605,4.0,0.790097,31
a90b9025-f71e-46d5-8536-0feecb036809,The Isabelle ENIGMA,8,0.183091,0.765013,"We significantly improve the performance of the E automated theorem prover on
the Isabelle Sledgehammer problems by combining learning and theorem proving in
several ways. In particular, we develop targeted versions of the ENIGMA
guidance for the Isabelle problems, targeted versions of neural premise
selection, and targeted strategies for E. The methods are trained in several
iterations over hundreds of thousands untyped and typed first-order problems
extracted from Isabelle. Our final best single-strategy ENIGMA and premise
selection system improves the best previous version of E by 25.3% in 15
seconds, outperforming also all other previous ATP and SMT systems.",0,1,0,1,1,0,0.0242749,13.0,0.591116,55
2c72d822-76a8-482a-b9ed-dd52deb5454a,Region Embedding with Intra and Inter-View Contrastive Learning,11,0.161865,0.481917,"Unsupervised region representation learning aims to extract dense and
effective features from unlabeled urban data. While some efforts have been made
for solving this problem based on multiple views, existing methods are still
insufficient in extracting representations in a view and/or incorporating
representations from different views. Motivated by the success of contrastive
learning for representation learning, we propose to leverage it for multi-view
region representation learning and design a model called ReMVC (Region
Embedding with Multi-View Contrastive Learning) by following two guidelines: i)
comparing a region with others within each view for effective representation
extraction and ii) comparing a region with itself across different views for
cross-view information sharing. We design the intra-view contrastive learning
module which helps to learn distinguished region embeddings and the inter-view
contrastive learning module which serves as a soft co-regularizer to constrain
the embedding parameters and transfer knowledge across multi-views. We exploit
the learned region embeddings in two downstream tasks named land usage
clustering and region popularity prediction. Extensive experiments demonstrate
that our model achieves impressive improvements compared with seven
state-of-the-art baseline methods, and the margins are over 30% in the land
usage clustering task.",1,1,0,0,1,0,0.327155,7.0,0.637832,23
245f5764-5dff-4f8e-bb00-18d6584561cc,IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages,10,0.126304,0.672191,"The rapid growth of machine translation (MT) systems has necessitated
comprehensive studies to meta-evaluate evaluation metrics being used, which
enables a better selection of metrics that best reflect MT quality.
Unfortunately, most of the research focuses on high-resource languages, mainly
English, the observations for which may not always apply to other languages.
Indian languages, having over a billion speakers, are linguistically different
from English, and to date, there has not been a systematic study of evaluating
MT systems from English into Indian languages. In this paper, we fill this gap
by creating an MQM dataset consisting of 7000 fine-grained annotations,
spanning 5 Indian languages and 7 MT systems, and use it to establish
correlations between annotator scores and scores obtained using existing
automatic metrics. Our results show that pre-trained metrics, such as COMET,
have the highest correlations with annotator scores. Additionally, we find that
the metrics do not adequately capture fluency-based errors in Indian languages,
and there is a need to develop metrics focused on Indian languages. We hope
that our dataset and analysis will help promote further research in this area.",0,1,1,1,0,0,0.720065,6.0,0.772016,60
04c7a8b5-9d7b-46fd-a0f1-4f9eb20e95d0,Information-Theoretic Odometry Learning,3,0.00404509,0.0558926,"In this paper, we propose a unified information theoretic framework for
learning-motivated methods aimed at odometry estimation, a crucial component of
many robotics and vision tasks such as navigation and virtual reality where
relative camera poses are required in real time. We formulate this problem as
optimizing a variational information bottleneck objective function, which
eliminates pose-irrelevant information from the latent representation. The
proposed framework provides an elegant tool for performance evaluation and
understanding in information-theoretic language. Specifically, we bound the
generalization errors of the deep information bottleneck framework and the
predictability of the latent representation. These provide not only a
performance guarantee but also practical guidance for model design, sample
collection, and sensor selection. Furthermore, the stochastic latent
representation provides a natural uncertainty measure without the needs for
extra structures or computations. Experiments on two well-known odometry
datasets demonstrate the effectiveness of our method.",0,0,0,0,0,0,0.0421799,10.0,0.524622,63
8c3d2fe2-ee61-48a9-95eb-3dce66961649,Reducing Hallucinations in Neural Machine Translation with Feature Attribution,5,0.156348,0.140461,"Neural conditional language generation models achieve the state-of-the-art in
Neural Machine Translation (NMT) but are highly dependent on the quality of
parallel training dataset. When trained on low-quality datasets, these models
are prone to various error types, including hallucinations, i.e. outputs that
are fluent, but unrelated to the source sentences. These errors are
particularly dangerous, because on the surface the translation can be perceived
as a correct output, especially if the reader does not understand the source
language. We present a case study focusing on model understanding and
regularisation to reduce hallucinations in NMT. We first use feature
attribution methods to study the behaviour of an NMT model that produces
hallucinations. We then leverage these methods to propose a novel loss function
that substantially helps reduce hallucinations and does not require retraining
the model from scratch.",0,1,0,0,1,0,0.840494,7.0,0.856853,24
69bd5048-38a9-4ad3-91ff-28aa220efa82,Improving Robustness of Retrieval Augmented Translation via Shuffling of Suggestions,1,0.0441301,0.110862,"Several recent studies have reported dramatic performance improvements in
neural machine translation (NMT) by augmenting translation at inference time
with fuzzy-matches retrieved from a translation memory (TM). However, these
studies all operate under the assumption that the TMs available at test time
are highly relevant to the testset. We demonstrate that for existing retrieval
augmented translation methods, using a TM with a domain mismatch to the test
set can result in substantially worse performance compared to not using a TM at
all. We propose a simple method to expose fuzzy-match NMT systems during
training and show that it results in a system that is much more tolerant
(regaining up to 5.8 BLEU) to inference with TMs with domain mismatch. Also,
the model is still competitive to the baseline when fed with suggestions from
relevant TMs.",0,1,0,0,0,0,0.585859,9.0,0.807165,36
7fc7e745-a713-4452-8c21-4093e8c7e7fa,Reconciling Security and Communication Efficiency in Federated Learning,6,0.0288189,0.220262,"Cross-device Federated Learning is an increasingly popular machine learning
setting to train a model by leveraging a large population of client devices
with high privacy and security guarantees. However, communication efficiency
remains a major bottleneck when scaling federated learning to production
environments, particularly due to bandwidth constraints during uplink
communication. In this paper, we formalize and address the problem of
compressing client-to-server model updates under the Secure Aggregation
primitive, a core component of Federated Learning pipelines that allows the
server to aggregate the client updates without accessing them individually. In
particular, we adapt standard scalar quantization and pruning methods to Secure
Aggregation and propose Secure Indexing, a variant of Secure Aggregation that
supports quantization for extreme compression. We establish state-of-the-art
results on LEAF benchmarks in a secure Federated Learning setup with up to
40$\times$ compression in uplink communication with no meaningful loss in
utility compared to uncompressed baselines.",0,1,0,0,1,0,0.510733,7.0,0.722125,70
35043258-95df-465c-aeb6-7168bcefa654,Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue,9,0.107168,0.761775,"Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question
regarding a given video and dialogue context. Despite the recent success of
multi-modal reasoning to generate answer sentences, existing dialogue systems
still suffer from a text hallucination problem, which denotes indiscriminate
text-copying from input texts without an understanding of the question. This is
due to learning spurious correlations from the fact that answer sentences in
the dataset usually include the words of input texts, thus the VGD system
excessively relies on copying words from input texts by hoping those words to
overlap with ground-truth texts. Hence, we design Text Hallucination Mitigating
(THAM) framework, which incorporates Text Hallucination Regularization (THR)
loss derived from the proposed information-theoretic text hallucination
measurement approach. Applying THAM with current dialogue systems validates the
effectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows
enhanced interpretability.",0,1,0,0,0,0,0.145729,10.0,0.654228,38
eb64e2e6-2b40-482b-8cdc-9585d6f5e0c8,Hierarchical Multi-Label Classification of Scientific Documents,9,0.234486,0.673935,"Automatic topic classification has been studied extensively to assist
managing and indexing scientific documents in a digital collection. With the
large number of topics being available in recent years, it has become necessary
to arrange them in a hierarchy. Therefore, the automatic classification systems
need to be able to classify the documents hierarchically. In addition, each
paper is often assigned to more than one relevant topic. For example, a paper
can be assigned to several topics in a hierarchy tree. In this paper, we
introduce a new dataset for hierarchical multi-label text classification
(HMLTC) of scientific papers called SciHTC, which contains 186,160 papers and
1,233 categories from the ACM CCS tree. We establish strong baselines for HMLTC
and propose a multi-task learning approach for topic classification with
keyword labeling as an auxiliary task. Our best model achieves a Macro-F1 score
of 34.57% which shows that this dataset provides significant research
opportunities on hierarchical scientific topic classification. We make our
dataset and code available on Github.",1,1,1,1,0,0,0.9398,13.0,0.955675,28
03ff2004-4640-4941-8e52-adf375b85fba,Chart Question Answering: State of the Art and Future Directions,20,0.185092,0.516864,"Information visualizations such as bar charts and line charts are very common
for analyzing data and discovering critical insights. Often people analyze
charts to answer questions that they have in mind. Answering such questions can
be challenging as they often require a significant amount of perceptual and
cognitive effort. Chart Question Answering (CQA) systems typically take a chart
and a natural language question as input and automatically generate the answer
to facilitate visual data analysis. Over the last few years, there has been a
growing body of literature on the task of CQA. In this survey, we
systematically review the current state-of-the-art research focusing on the
problem of chart question answering. We provide a taxonomy by identifying
several important dimensions of the problem domain including possible inputs
and outputs of the task and discuss the advantages and limitations of proposed
solutions. We then summarize various evaluation techniques used in the surveyed
papers. Finally, we outline the open challenges and future research
opportunities related to chart question answering.",0,0,0,0,0,0,0.298686,6.0,0.55906,114
3c91e5f3-6677-4ef6-a6b2-3fbe1b9254f0,FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations,20,0.61054,0.487823,"Recent advances in generative adversarial networks have shown that it is
possible to generate high-resolution and hyperrealistic images. However, the
images produced by GANs are only as fair and representative as the datasets on
which they are trained. In this paper, we propose a method for directly
modifying a pre-trained StyleGAN2 model that can be used to generate a balanced
set of images with respect to one (e.g., eyeglasses) or more attributes (e.g.,
gender and eyeglasses). Our method takes advantage of the style space of the
StyleGAN2 model to perform disentangled control of the target attributes to be
debiased. Our method does not require training additional models and directly
debiases the GAN model, paving the way for its use in various downstream
applications. Our experiments show that our method successfully debiases the
GAN model within a few minutes without compromising the quality of the
generated images. To promote fair generative models, we share the code and
debiased models at http://catlab-team.github.io/fairstyle.",1,1,0,0,0,0,0.986223,6.0,0.974271,45
763a7072-d35f-4b46-a855-b8df900b48d6,Massively Digitized Power Grid: Opportunities and Challenges of Use-inspired AI,6,0.0624522,0.721697,"This article presents a use-inspired perspective of the opportunities and
challenges in a massively digitized power grid. It argues that the intricate
interplay of data availability, computing capability, and artificial
intelligence (AI) algorithm development are the three key factors driving the
adoption of digitized solutions in the power grid. The impact of these three
factors on critical functions of power system operation and planning practices
are reviewed and illustrated with industrial practice case studies. Open
challenges and research opportunities for data, computing, and AI algorithms
are articulated within the context of the power industry's tremendous
decarbonization efforts.",0,1,0,0,0,0,0.261433,6.0,0.532781,241
9639ff9a-d739-4a00-9584-b5483ed48875,Robust Double-Encoder Network for RGB-D Panoptic Segmentation,7,0.13635,0.66072,"Perception is crucial for robots that act in real-world environments, as
autonomous systems need to see and understand the world around them to act
properly. Panoptic segmentation provides an interpretation of the scene by
computing a pixelwise semantic label together with instance IDs. In this paper,
we address panoptic segmentation using RGB-D data of indoor scenes. We propose
a novel encoder-decoder neural network that processes RGB and depth separately
through two encoders. The features of the individual encoders are progressively
merged at different resolutions, such that the RGB features are enhanced using
complementary depth information. We propose a novel merging approach called
ResidualExcite, which reweighs each entry of the feature map according to its
importance. With our double-encoder architecture, we are robust to missing
cues. In particular, the same model can train and infer on RGB-D, RGB-only, and
depth-only input data, without the need to train specialized models. We
evaluate our method on publicly available datasets and show that our approach
achieves superior results compared to other common approaches for panoptic
segmentation.",1,1,0,0,0,0,0.819652,9.0,0.88097,47
034dc8a1-7d30-4d40-924c-ddc7e9dba8a0,Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue,7,0.240627,0.457297,"Complex conversation settings such as persuasion involve communicating
changes in attitude or behavior, so users' perspectives need to be addressed,
even when not directly related to the topic. In this work, we contribute a
novel modular dialogue system framework that seamlessly integrates factual
information and social content into persuasive dialogue. Our framework is
generalizable to any dialogue tasks that have mixed social and task contents.
We conducted a study that compared user evaluations of our framework versus a
baseline end-to-end generation model. We found our framework was evaluated more
favorably in all dimensions including competence and friendliness, compared to
the end-to-end model which does not explicitly handle social content or factual
questions.",0,0,0,0,0,0,0.68862,6.0,0.757463,62
889de63b-fbe4-46f8-9298-2862db935173,SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data,9,0.121219,0.340137,"We present a method for the accurate 3D reconstruction of partly-symmetric
objects. We build on the strengths of recent advances in neural reconstruction
and rendering such as Neural Radiance Fields (NeRF). A major shortcoming of
such approaches is that they fail to reconstruct any part of the object which
is not clearly visible in the training image, which is often the case for
in-the-wild images and videos. When evidence is lacking, structural priors such
as symmetry can be used to complete the missing information. However,
exploiting such priors in neural rendering is highly non-trivial: while
geometry and non-reflective materials may be symmetric, shadows and reflections
from the ambient scene are not symmetric in general. To address this, we apply
a soft symmetry constraint to the 3D geometry and material properties, having
factored appearance into lighting, albedo colour and reflectivity. We evaluate
our method on the recently introduced CO3D dataset, focusing on the car
category due to the challenge of reconstructing highly-reflective materials. We
show that it can reconstruct unobserved regions with high fidelity and render
high-quality novel view images.",0,0,0,0,0,0,0.882381,5.0,0.830302,49
8bc4e8fb-2964-4750-8318-5307fe3f1f05,D'ARTAGNAN: Counterfactual Video Generation,8,0.140702,0.399764,"Causally-enabled machine learning frameworks could help clinicians to
identify the best course of treatments by answering counterfactual questions.
We explore this path for the case of echocardiograms by looking into the
variation of the Left Ventricle Ejection Fraction, the most essential clinical
metric gained from these examinations. We combine deep neural networks, twin
causal networks and generative adversarial methods for the first time to build
D'ARTAGNAN (Deep ARtificial Twin-Architecture GeNerAtive Networks), a novel
causal generative model. We demonstrate the soundness of our approach on a
synthetic dataset before applying it to cardiac ultrasound videos to answer the
question: ""What would this echocardiogram look like if the patient had a
different ejection fraction?"". To do so, we generate new ultrasound videos,
retaining the video style and anatomy of the original patient, while modifying
the Ejection Fraction conditioned on a given input. We achieve an SSIM score of
0.79 and an R2 score of 0.51 on the counterfactual videos. Code and models are
available at: https://github.com/HReynaud/dartagnan.",1,0,1,0,0,0,0.198897,8.0,0.610553,37
0f28f945-a95e-4e3f-b15e-bbaf7b3a3ee4,Prompt Consistency for Zero-Shot Task Generalization,45,0.539582,0.569058,"One of the most impressive results of recent NLP history is the ability of
pre-trained language models to solve new tasks in a zero-shot setting. To
achieve this, NLP tasks are framed as natural language prompts, generating a
response indicating the predicted output. Nonetheless, the performance in such
settings often lags far behind its supervised counterpart, suggesting a large
space for potential improvement. In this paper, we explore methods to utilize
unlabeled data to improve zero-shot performance. Specifically, we take
advantage of the fact that multiple prompts can be used to specify a single
task, and propose to regularize prompt consistency, encouraging consistent
predictions over this diverse set of prompts. Our method makes it possible to
fine-tune the model either with extra unlabeled training data, or directly on
test input at inference time in an unsupervised manner. In experiments, our
approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,
2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points
in terms of accuracy. The gains are often attained with a small number of
unlabeled examples.",1,0,0,0,1,1,0.99245,6.0,0.996162,56
1c543b3a-2859-4e24-bcf2-1182c30545cd,Redwood: Using Collision Detection to Grow a Large-Scale Intent Classification Dataset,8,0.0922429,0.827265,"Dialog systems must be capable of incorporating new skills via updates over
time in order to reflect new use cases or deployment scenarios. Similarly,
developers of such ML-driven systems need to be able to add new training data
to an already-existing dataset to support these new skills. In intent
classification systems, problems can arise if training data for a new skill's
intent overlaps semantically with an already-existing intent. We call such
cases collisions. This paper introduces the task of intent collision detection
between multiple datasets for the purposes of growing a system's skillset. We
introduce several methods for detecting collisions, and evaluate our methods on
real datasets that exhibit collisions. To highlight the need for intent
collision detection, we show that model performance suffers if new data is
added in such a way that does not arbitrate colliding intents. Finally, we use
collision detection to construct and benchmark a new dataset, Redwood, which is
composed of 451 ntent categories from 13 original intent classification
datasets, making it the largest publicly available intent classification
benchmark.",0,1,1,1,0,0,0.398406,7.0,0.673376,28
2333e970-ed85-46cc-b23e-276322900f3a,ActionFormer: Localizing Moments of Actions with Transformers,187,0.854151,0.991771,"Self-attention based Transformer models have demonstrated impressive results
for image classification and object detection, and more recently for video
understanding. Inspired by this success, we investigate the application of
Transformer networks for temporal action localization in videos. To this end,
we present ActionFormer -- a simple yet powerful model to identify actions in
time and recognize their categories in a single shot, without using action
proposals or relying on pre-defined anchor windows. ActionFormer combines a
multiscale feature representation with local self-attention, and uses a
light-weighted decoder to classify every moment in time and estimate the
corresponding action boundaries. We show that this orchestrated design results
in major improvements upon prior works. Without bells and whistles,
ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best
prior model by 14.1 absolute percentage points. Further, ActionFormer
demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and
EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available
at http://github.com/happyharrycn/actionformer_release.",1,1,0,0,1,0,0.925625,5.0,0.86911,91
1b00acd4-8fba-4a3b-b8a5-358807110f33,C-Pack of IPAs: A C90 Program Benchmark of Introductory Programming Assignments,6,0.205108,0.594346,"Due to the vast number of students enrolled in Massive Open Online Courses
(MOOCs), there has been an increasing number of automated program repair
techniques focused on introductory programming assignments (IPAs). Such
techniques take advantage of previous correct student implementations in order
to provide automated, comprehensive, and personalized feedback to students.
  This paper presents C-Pack-IPAs, a publicly available benchmark of students'
programs submitted for 25 different IPAs. C-Pack-IPAs contains semantically
correct, semantically incorrect, and syntactically incorrect programs plus a
test suite for each IPA. Hence, C-Pack-IPAs can be used to help evaluate the
development of novel semantic, as well as syntactic, automated program repair
frameworks, focused on providing feedback to novice programmers.",0,1,0,1,0,0,0.456103,11.0,0.808601,29
76df7637-2162-4d12-bf51-2f80ad2ba3af,Target-Driven Structured Transformer Planner for Vision-Language Navigation,32,0.26007,0.525488,"Vision-language navigation is the task of directing an embodied agent to
navigate in 3D scenes with natural language instructions. For the agent,
inferring the long-term navigation target from visual-linguistic clues is
crucial for reliable path planning, which, however, has rarely been studied
before in literature. In this article, we propose a Target-Driven Structured
Transformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware
navigation. Specifically, we devise an Imaginary Scene Tokenization mechanism
for explicit estimation of the long-term target (even located in unexplored
environments). In addition, we design a Structured Transformer Planner which
elegantly incorporates the explored room layout into a neural attention
architecture for structured and global planning. Experimental results
demonstrate that our TD-STP substantially improves previous best methods'
success rate by 2% and 5% on the test set of R2R and REVERIE benchmarks,
respectively. Our code is available at https://github.com/YushengZhao/TD-STP .",1,0,0,0,0,0,0.61399,5.0,0.668252,65
1c3ce445-c42a-4362-aa14-f75014e56522,ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations,40,0.279646,0.70853,"Precise representations of 3D faces are beneficial to various computer vision
and graphics applications. Due to the data discretization and model linearity,
however, it remains challenging to capture accurate identity and expression
clues in current studies. This paper presents a novel 3D morphable face model,
namely ImFace, to learn a nonlinear and continuous space with implicit neural
representations. It builds two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
and designs an improved learning strategy to extend embeddings of expressions
to allow more diverse changes. We further introduce a Neural Blend-Field to
learn sophisticated details by adaptively blending a series of local fields. In
addition to ImFace, an effective preprocessing pipeline is proposed to address
the issue of watertight input requirement in implicit representations, enabling
them to work with common facial surfaces for the first time. Extensive
experiments are performed to demonstrate the superiority of ImFace.",1,0,0,0,1,0,0.508806,7.0,0.721337,67
c27f8835-f831-4129-b770-f0cec1e7a102,Estimating the Entropy of Linguistic Distributions,7,0.0,0.280566,"Shannon entropy is often a quantity of interest to linguists studying the
communicative capacity of human language. However, entropy must typically be
estimated from observed data because researchers do not have access to the
underlying probability distribution that gives rise to these data. While
entropy estimation is a well-studied problem in other fields, there is not yet
a comprehensive exploration of the efficacy of entropy estimators for use with
linguistic data. In this work, we fill this void, studying the empirical
effectiveness of different entropy estimators for linguistic distributions. In
a replication of two recent information-theoretic linguistic studies, we find
evidence that the reported effect size is over-estimated due to over-reliance
on poor entropy estimators. Finally, we end our paper with concrete
recommendations for entropy estimation depending on distribution type and data
availability.",1,0,0,0,0,0,3.21273e-05,18.0,0.335821,54
33f56456-7aa2-4d8d-aa32-cddb2af5e811,Differentiable Agent-based Epidemiology,11,0.147719,0.478819,"Mechanistic simulators are an indispensable tool for epidemiology to explore
the behavior of complex, dynamic infections under varying conditions and
navigate uncertain environments. Agent-based models (ABMs) are an increasingly
popular simulation paradigm that can represent the heterogeneity of contact
interactions with granular detail and agency of individual behavior. However,
conventional ABM frameworks are not differentiable and present challenges in
scalability; due to which it is non-trivial to connect them to auxiliary data
sources. In this paper, we introduce GradABM: a scalable, differentiable design
for agent-based modeling that is amenable to gradient-based learning with
automatic differentiation. GradABM can quickly simulate million-size
populations in few seconds on commodity hardware, integrate with deep neural
networks and ingest heterogeneous data sources. This provides an array of
practical benefits for calibration, forecasting, and evaluating policy
interventions. We demonstrate the efficacy of GradABM via extensive experiments
with real COVID-19 and influenza datasets.",1,1,0,0,0,0,0.444893,5.0,0.572109,81
3662c7cf-a397-4a77-b48f-c0b06f58d6a2,Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing,5,0.109737,0.722407,"The explosion of e-commerce has caused the need for processing and analysis
of product titles, like entity typing in product titles. However, the rapid
activity in e-commerce has led to the rapid emergence of new entities, which is
difficult to be solved by general entity typing. Besides, product titles in
e-commerce have very different language styles from text data in general
domain. In order to handle new entities in product titles and address the
special language styles problem of product titles in e-commerce domain, we
propose our textual entailment model with continuous prompt tuning based
hypotheses and fusion embeddings for e-commerce entity typing. First, we
reformulate the entity typing task into a textual entailment problem to handle
new entities that are not present during training. Second, we design a model to
automatically generate textual entailment hypotheses using a continuous prompt
tuning method, which can generate better textual entailment hypotheses without
manual design. Third, we utilize the fusion embeddings of BERT embedding and
CharacterBERT embedding with a two-layer MLP classifier to solve the problem
that the language styles of product titles in e-commerce are different from
that of general domain. To analyze the effect of each contribution, we compare
the performance of entity typing and textual entailment model, and conduct
ablation studies on continuous prompt tuning and fusion embeddings. We also
evaluate the impact of different prompt template initialization for the
continuous prompt tuning. We show our proposed model improves the average F1
score by around 2% compared to the baseline BERT entity typing model.",0,1,0,0,0,0,0.88142,5.0,0.829541,40
1b8bad16-ec45-49f2-b3c7-e7cc74350e21,Learning Prototype via Placeholder for Zero-shot Recognition,5,0.0635938,0.0937207,"Zero-shot learning (ZSL) aims to recognize unseen classes by exploiting
semantic descriptions shared between seen classes and unseen classes. Current
methods show that it is effective to learn visual-semantic alignment by
projecting semantic embeddings into the visual space as class prototypes.
However, such a projection function is only concerned with seen classes. When
applied to unseen classes, the prototypes often perform suboptimally due to
domain shift. In this paper, we propose to learn prototypes via placeholders,
termed LPL, to eliminate the domain shift between seen and unseen classes.
Specifically, we combine seen classes to hallucinate new classes which play as
placeholders of the unseen classes in the visual and semantic space. Placed
between seen classes, the placeholders encourage prototypes of seen classes to
be highly dispersed. And more space is spared for the insertion of
well-separated unseen ones. Empirically, well-separated prototypes help
counteract visual-semantic misalignment caused by domain shift. Furthermore, we
exploit a novel semantic-oriented fine-tuning to guarantee the semantic
reliability of placeholders. Extensive experiments on five benchmark datasets
demonstrate the significant performance gain of LPL over the state-of-the-art
methods. Code is available at https://github.com/zaiquanyang/LPL.",1,1,1,0,1,0,0.686042,8.0,0.81721,32
6c919b6c-3aef-4757-9b9e-dba906cffcc0,CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge Distillation,6,0.0591655,0.325531,"Knowledge distillation (KD) is an efficient framework for compressing
large-scale pre-trained language models. Recent years have seen a surge of
research aiming to improve KD by leveraging Contrastive Learning, Intermediate
Layer Distillation, Data Augmentation, and Adversarial Training. In this work,
we propose a learning based data augmentation technique tailored for knowledge
distillation, called CILDA. To the best of our knowledge, this is the first
time that intermediate layer representations of the main task are used in
improving the quality of augmented samples. More precisely, we introduce an
augmentation technique for KD based on intermediate layer matching using
contrastive loss to improve masked adversarial data augmentation. CILDA
outperforms existing state-of-the-art KD approaches on the GLUE benchmark, as
well as in an out-of-domain evaluation.",0,1,1,0,1,0,0.645288,6.0,0.737727,47
3714e128-4252-4cd7-a0f9-2e5fbe905925,Obtaining Dyadic Fairness by Optimal Transport,4,0.0239201,0.188064,"Fairness has been taken as a critical metric in machine learning models,
which is considered as an important component of trustworthy machine learning.
In this paper, we focus on obtaining fairness for popular link prediction
tasks, which are measured by dyadic fairness. A novel pre-processing
methodology is proposed to establish dyadic fairness through data repairing
based on optimal transport theory. With the well-established theoretical
connection between the dyadic fairness for graph link prediction and a
conditional distribution alignment problem, the dyadic repairing scheme can be
equivalently transformed into a conditional distribution alignment problem.
Furthermore, an optimal transport-based dyadic fairness algorithm called
DyadicOT is obtained by efficiently solving the alignment problem, satisfying
flexibility and unambiguity requirements. The proposed DyadicOT algorithm shows
superior results in obtaining fairness compared to other fairness methods on
two benchmark graph datasets.",0,1,0,0,1,0,0.0969104,8.0,0.513391,31
5de8dc62-7849-4472-8aa4-913fac9d6af8,Exploring Plain Vision Transformer Backbones for Object Detection,445,0.365352,0.999288,"We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone
network for object detection. This design enables the original ViT architecture
to be fine-tuned for object detection without needing to redesign a
hierarchical backbone for pre-training. With minimal adaptations for
fine-tuning, our plain-backbone detector can achieve competitive results.
Surprisingly, we observe: (i) it is sufficient to build a simple feature
pyramid from a single-scale feature map (without the common FPN design) and
(ii) it is sufficient to use window attention (without shifting) aided with
very few cross-window propagation blocks. With plain ViT backbones pre-trained
as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the
previous leading methods that were all based on hierarchical backbones,
reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K
pre-training. We hope our study will draw attention to research on
plain-backbone detectors. Code for ViTDet is available in Detectron2.",1,1,0,0,1,0,0.652821,6.0,0.741144,60
9d9eba74-39b3-4327-81dc-810a589284c4,A study on cross-corpus speech emotion recognition and data augmentation,6,0.0873344,0.371757,"Models that can handle a wide range of speakers and acoustic conditions are
essential in speech emotion recognition (SER). Often, these models tend to show
mixed results when presented with speakers or acoustic conditions that were not
visible during training. This paper investigates the impact of cross-corpus
data complementation and data augmentation on the performance of SER models in
matched (test-set from same corpus) and mismatched (test-set from different
corpus) conditions. Investigations using six emotional speech corpora that
include single and multiple speakers as well as variations in emotion style
(acted, elicited, natural) and recording conditions are presented. Observations
show that, as expected, models trained on single corpora perform best in
matched conditions while performance decreases between 10-40% in mismatched
conditions, depending on corpus specific features. Models trained on mixed
corpora can be more stable in mismatched contexts, and the performance
reductions range from 1 to 8% when compared with single corpus models in
matched conditions. Data augmentation yields additional gains up to 4% and seem
to benefit mismatched conditions more than matched ones.",0,1,0,0,0,0,0.28828,8.0,0.663995,25
c04f1f8e-743d-47d5-b31c-7e43f2cf69c5,Multi-Granularity Prediction for Scene Text Recognition,27,0.371105,0.687028,"Scene text recognition (STR) has been an active research topic in computer
vision for years. To tackle this challenging problem, numerous innovative
methods have been successively proposed and incorporating linguistic knowledge
into STR models has recently become a prominent trend. In this work, we first
draw inspiration from the recent progress in Vision Transformer (ViT) to
construct a conceptually simple yet powerful vision STR model, which is built
upon ViT and outperforms previous state-of-the-art models for scene text
recognition, including both pure vision models and language-augmented methods.
To integrate linguistic knowledge, we further propose a Multi-Granularity
Prediction strategy to inject information from the language modality into the
model in an implicit way, i.e. , subword representations (BPE and WordPiece)
widely-used in NLP are introduced into the output space, in addition to the
conventional character level representation, while no independent language
model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push
the performance envelop of STR to an even higher level. Specifically, it
achieves an average recognition accuracy of 93.35% on standard benchmarks. Code
is available at
https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR.",1,1,0,0,1,0,0.836299,8.0,0.872966,59
cf1bd17e-b45f-485a-aee1-b3b28336df24,Non-Uniformly Terminating Chase: Size and Complexity,6,0.23485,0.950213,"The chase procedure, originally introduced for checking implication of
database constraints, and later on used for computing data exchange solutions,
has recently become a central algorithmic tool in rule-based ontological
reasoning. In this context, a key problem is non-uniform chase termination:
does the chase of a database w.r.t. a rule-based ontology terminate? And if
this is the case, what is the size of the result of the chase? We focus on
guarded tuple-generating dependencies (TGDs), which form a robust rule-based
ontology language, and study the above central questions for the semi-oblivious
version of the chase. One of our main findings is that non-uniform
semi-oblivious chase termination for guarded TGDs is feasible in polynomial
time w.r.t. the database, and the size of the result of the chase (whenever is
finite) is linear w.r.t. the database. Towards our results concerning
non-uniform chase termination, we show that basic techniques such as
simplification and linearization, originally introduced in the context of
ontological query answering, can be safely applied to the chase termination
problem.",0,0,0,0,0,0,0.0861995,11.0,0.634928,40
2dd3ce33-1f58-43a3-b84a-ffb7b0dd41c7,Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics,15,0.0828391,0.567238,"Few images on the Web receive alt-text descriptions that would make them
accessible to blind and low vision (BLV) users. Image-based NLG systems have
progressed to the point where they can begin to address this persistent
societal problem, but these systems will not be fully successful unless we
evaluate them on metrics that guide their development correctly. Here, we argue
against current referenceless metrics -- those that don't rely on
human-generated ground-truth descriptions -- on the grounds that they do not
align with the needs of BLV users. The fundamental shortcoming of these metrics
is that they do not take context into account, whereas contextual information
is highly valued by BLV users. To substantiate these claims, we present a study
with BLV participants who rated descriptions along a variety of dimensions. An
in-depth analysis reveals that the lack of context-awareness makes current
referenceless metrics inadequate for advancing image accessibility. As a
proof-of-concept, we provide a contextual version of the referenceless metric
CLIPScore which begins to address the disconnect to the BLV data. An accessible
HTML version of this paper is available at
https://elisakreiss.github.io/contextual-description-evaluation/paper/reflessmetrics.html",0,0,0,0,0,0,0.143514,7.0,0.503671,42
9d9f7d99-7ac4-42f1-81fa-8e07913bd8b0,"""My nose is running.""""Are you also coughing?"": Building A Medical Diagnosis Agent with Interpretable Inquiry Logics",8,0.161932,0.358487,"With the rise of telemedicine, the task of developing Dialogue Systems for
Medical Diagnosis (DSMD) has received much attention in recent years. Different
from early researches that needed to rely on extra human resources and
expertise to help construct the system, recent researches focused on how to
build DSMD in a purely data-driven manner. However, the previous data-driven
DSMD methods largely overlooked the system interpretability, which is critical
for a medical application, and they also suffered from the data sparsity issue
at the same time. In this paper, we explore how to bring interpretability to
data-driven DSMD. Specifically, we propose a more interpretable decision
process to implement the dialogue manager of DSMD by reasonably mimicking real
doctors' inquiry logics, and we devise a model with highly transparent
components to conduct the inference. Moreover, we collect a new DSMD dataset,
which has a much larger scale, more diverse patterns and is of higher quality
than the existing ones. The experiments show that our method obtains 7.7%,
10.0%, 3.0% absolute improvement in diagnosis accuracy respectively on three
datasets, demonstrating the effectiveness of its rational decision process and
model design. Our codes and the GMD-12 dataset are available at
https://github.com/lwgkzl/BR-Agent.",1,1,0,1,1,0,0.465699,7.0,0.703347,34
aaad281b-e05e-4790-9f51-fc8607596d90,QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity,14,0.124089,0.512302,"The mechanism of existing style transfer algorithms is by minimizing a hybrid
loss function to push the generated image toward high similarities in both
content and style. However, this type of approach cannot guarantee visual
fidelity, i.e., the generated artworks should be indistinguishable from real
ones. In this paper, we devise a new style transfer framework called QuantArt
for high visual-fidelity stylization. QuantArt pushes the latent representation
of the generated artwork toward the centroids of the real artwork distribution
with vector quantization. By fusing the quantized and continuous latent
representations, QuantArt allows flexible control over the generated artworks
in terms of content preservation, style similarity, and visual fidelity.
Experiments on various style transfer settings show that our QuantArt framework
achieves significantly higher visual fidelity compared with the existing style
transfer methods.",1,1,0,0,1,1,0.557195,9.0,0.798393,69
97bcd73f-1288-4721-bf0f-d1a2cc1979f9,Continual Learning Based on OOD Detection and Task Masking,13,0.118013,0.258442,"Existing continual learning techniques focus on either task incremental
learning (TIL) or class incremental learning (CIL) problem, but not both. CIL
and TIL differ mainly in that the task-id is provided for each test sample
during testing for TIL, but not provided for CIL. Continual learning methods
intended for one problem have limitations on the other problem. This paper
proposes a novel unified approach based on out-of-distribution (OOD) detection
and task masking, called CLOM, to solve both problems. The key novelty is that
each task is trained as an OOD detection model rather than a traditional
supervised learning model, and a task mask is trained to protect each task to
prevent forgetting. Our evaluation shows that CLOM outperforms existing
state-of-the-art baselines by large margins. The average TIL/CIL accuracy of
CLOM over six experiments is 87.6/67.9% while that of the best baselines is
only 82.4/55.0%.",1,1,0,0,1,0,0.741539,8.0,0.836612,83
6904ee29-51e1-48fd-9af2-86cda2e49f73,A Visual Navigation Perspective for Category-Level Object Pose Estimation,6,0.044075,0.2605,"This paper studies category-level object pose estimation based on a single
monocular image. Recent advances in pose-aware generative models have paved the
way for addressing this challenging task using analysis-by-synthesis. The idea
is to sequentially update a set of latent variables, e.g., pose, shape, and
appearance, of the generative model until the generated image best agrees with
the observation. However, convergence and efficiency are two challenges of this
inference procedure. In this paper, we take a deeper look at the inference of
analysis-by-synthesis from the perspective of visual navigation, and
investigate what is a good navigation policy for this specific task. We
evaluate three different strategies, including gradient descent, reinforcement
learning and imitation learning, via thorough comparisons in terms of
convergence, robustness and efficiency. Moreover, we show that a simple hybrid
approach leads to an effective and efficient solution. We further compare these
strategies to state-of-the-art methods, and demonstrate superior performance on
synthetic and real-world datasets leveraging off-the-shelf pose-aware
generative models.",1,1,0,0,0,0,0.592649,9.0,0.80923,60
58584ef8-c796-4bb1-825e-9eb5ffeab3ce,Long-tailed Instance Segmentation using Gumbel Optimized Loss,16,0.431272,0.720271,"Major advancements have been made in the field of object detection and
segmentation recently. However, when it comes to rare categories, the
state-of-the-art methods fail to detect them, resulting in a significant
performance gap between rare and frequent categories. In this paper, we
identify that Sigmoid or Softmax functions used in deep detectors are a major
reason for low performance and are sub-optimal for long-tailed detection and
segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for
long-tailed detection and segmentation. It aligns with the Gumbel distribution
of rare classes in imbalanced datasets, considering the fact that most classes
in long-tailed detection have low expected probability. The proposed GOL
significantly outperforms the best state-of-the-art method by 1.1% on AP , and
boosts the overall segmentation by 9.0% and detection by 8.0%, particularly
improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS
dataset. Code available at: https://github.com/kostas1515/GOL",1,1,0,0,1,0,0.970722,6.0,0.94202,44
29bcea78-0a74-4dce-9c8d-043f3845bade,Applying Automatic Text Summarization for Fake News Detection,8,0.147248,0.562473,"The distribution of fake news is not a new but a rapidly growing problem. The
shift to news consumption via social media has been one of the drivers for the
spread of misleading and deliberately wrong information, as in addition to it
of easy use there is rarely any veracity monitoring. Due to the harmful effects
of such fake news on society, the detection of these has become increasingly
important. We present an approach to the problem that combines the power of
transformer-based language models while simultaneously addressing one of their
inherent problems. Our framework, CMTR-BERT, combines multiple text
representations, with the goal of circumventing sequential limits and related
loss of information the underlying transformer architecture typically suffers
from. Additionally, it enables the incorporation of contextual information.
Extensive experiments on two very different, publicly available datasets
demonstrates that our approach is able to set new state-of-the-art performance
benchmarks. Apart from the benefit of using automatic text summarization
techniques we also find that the incorporation of contextual information
contributes to performance gains.",1,1,0,0,1,0,0.797786,7.0,0.837083,85
6a8cb041-0354-4272-88ff-6e097d96e381,Classification of Hyperspectral Images Using SVM with Shape-adaptive Reconstruction and Smoothed Total Variation,8,0.0507043,0.56087,"In this work, a novel algorithm called SVM with Shape-adaptive Reconstruction
and Smoothed Total Variation (SaR-SVM-STV) is introduced to classify
hyperspectral images, which makes full use of spatial and spectral information.
The Shape-adaptive Reconstruction (SaR) is introduced to preprocess each pixel
based on the Pearson Correlation between pixels in its shape-adaptive (SA)
region. Support Vector Machines (SVMs) are trained to estimate the pixel-wise
probability maps of each class. Then the Smoothed Total Variation (STV) model
is applied to denoise and generate the final classification map. Experiments
show that SaR-SVM-STV outperforms the SVM-STV method with a few training
labels, demonstrating the significance of reconstructing hyperspectral images
before classification.",0,1,0,0,0,0,0.0507323,10.0,0.543529,27
a97c8998-5204-4fb6-918c-ba12a21dc2e0,SmartFPS: Neural Network based Wireless-inertial fusion positioning system,2,0.118206,0.387862,"The current fusion positioning systems are mainly based on filtering
algorithms, such as Kalman filtering or particle filtering. However, the system
complexity of practical application scenarios is often very high, such as noise
modeling in pedestrian inertial navigation systems, or environmental noise
modeling in fingerprint matching and localization algorithms. To solve this
problem, this paper proposes a fusion positioning system based on deep learning
and proposes a transfer learning strategy for improving the performance of
neural network models for samples with different distributions. The results
show that in the whole floor scenario, the average positioning accuracy of the
fusion network is 0.506m. The experiment results of transfer learning show that
the estimation accuracy of the inertial navigation positioning step size and
rotation angle of different pedestrians can be improved by 53.3% on average,
the Bluetooth positioning accuracy of different devices can be improved by
33.4%, and the fusion can be improved by 31.6%.",0,1,0,0,0,0,0.498365,12.0,0.834944,105
7dbcbc35-341f-470a-b5f8-2e1e37a9b55a,Online Continual Learning with Contrastive Vision Transformer,25,0.312675,0.948235,"Online continual learning (online CL) studies the problem of learning
sequential tasks from an online data stream without task boundaries, aiming to
adapt to new data while alleviating catastrophic forgetting on the past tasks.
This paper proposes a framework Contrastive Vision Transformer (CVT), which
designs a focal contrastive learning strategy based on a transformer
architecture, to achieve a better stability-plasticity trade-off for online CL.
Specifically, we design a new external attention mechanism for online CL that
implicitly captures previous tasks' information. Besides, CVT contains
learnable focuses for each class, which could accumulate the knowledge of
previous classes to alleviate forgetting. Based on the learnable focuses, we
design a focal contrastive loss to rebalance contrastive learning between new
and past classes and consolidate previously learned representations. Moreover,
CVT contains a dual-classifier structure for decoupling learning current
classes and balancing all observed classes. The extensive experimental results
show that our approach achieves state-of-the-art performance with even fewer
parameters on online CL benchmarks and effectively alleviates the catastrophic
forgetting.",0,0,0,0,1,0,0.827316,5.0,0.790754,94
b0812fb7-5330-45fe-926b-e20270beea4a,Imperceptible Adversarial Attack via Invertible Neural Networks,3,0.0202562,0.250418,"Adding perturbations via utilizing auxiliary gradient information or
discarding existing details of the benign images are two common approaches for
generating adversarial examples. Though visual imperceptibility is the desired
property of adversarial examples, conventional adversarial attacks still
generate traceable adversarial perturbations. In this paper, we introduce a
novel Adversarial Attack via Invertible Neural Networks (AdvINN) method to
produce robust and imperceptible adversarial examples. Specifically, AdvINN
fully takes advantage of the information preservation property of Invertible
Neural Networks and thereby generates adversarial examples by simultaneously
adding class-specific semantic information of the target class and dropping
discriminant information of the original class. Extensive experiments on
CIFAR-10, CIFAR-100, and ImageNet-1K demonstrate that the proposed AdvINN
method can produce less imperceptible adversarial images than the
state-of-the-art methods and AdvINN yields more robust adversarial examples
with high confidence compared to other adversarial attacks.",1,1,0,0,1,0,0.241454,7.0,0.586354,58
0dd5136e-aaae-4aab-b0b7-977b098bc38e,Interacting Hand-Object Pose Estimation via Dense Mutual Attention,6,0.091618,0.64656,"3D hand-object pose estimation is the key to the success of many computer
vision applications. The main focus of this task is to effectively model the
interaction between the hand and an object. To this end, existing works either
rely on interaction constraints in a computationally-expensive iterative
optimization, or consider only a sparse correlation between sampled hand and
object keypoints. In contrast, we propose a novel dense mutual attention
mechanism that is able to model fine-grained dependencies between the hand and
the object. Specifically, we first construct the hand and object graphs
according to their mesh structures. For each hand node, we aggregate features
from every object node by the learned attention and vice versa for each object
node. Thanks to such dense mutual attention, our method is able to produce
physically plausible poses with high quality and real-time inference speed.
Extensive quantitative and qualitative experiments on large benchmark datasets
show that our method outperforms state-of-the-art methods. The code is
available at https://github.com/rongakowang/DenseMutualAttention.git.",1,1,0,0,1,0,0.694878,7.0,0.794575,52
9a2035db-7a07-41fe-aef2-79c283cb2388,Multi-level Contrast Network for Wearables-based Joint Activity Segmentation and Recognition,1,0.0121697,0.0331281,"Human activity recognition (HAR) with wearables is promising research that
can be widely adopted in many smart healthcare applications. In recent years,
the deep learning-based HAR models have achieved impressive recognition
performance. However, most HAR algorithms are susceptible to the multi-class
windows problem that is essential yet rarely exploited. In this paper, we
propose to relieve this challenging problem by introducing the segmentation
technology into HAR, yielding joint activity segmentation and recognition.
Especially, we introduce the Multi-Stage Temporal Convolutional Network
(MS-TCN) architecture for sample-level activity prediction to joint segment and
recognize the activity sequence. Furthermore, to enhance the robustness of HAR
against the inter-class similarity and intra-class heterogeneity, a multi-level
contrastive loss, containing the sample-level and segment-level contrast, has
been proposed to learn a well-structured embedding space for better activity
segmentation and recognition performance. Finally, with comprehensive
experiments, we verify the effectiveness of the proposed method on two public
HAR datasets, achieving significant improvements in the various evaluation
metrics.",0,1,1,0,0,0,0.602398,6.0,0.71828,33
47ffb4f2-dbcb-40cb-904c-f40e60595b7c,SDF-StyleGAN: Implicit SDF-Based StyleGAN for 3D Shape Generation,60,0.479151,0.976614,"We present a StyleGAN2-based deep learning approach for 3D shape generation,
called SDF-StyleGAN, with the aim of reducing visual and geometric
dissimilarity between generated shapes and a shape collection. We extend
StyleGAN2 to 3D generation and utilize the implicit signed distance function
(SDF) as the 3D shape representation, and introduce two novel global and local
shape discriminators that distinguish real and fake SDF values and gradients to
significantly improve shape geometry and visual quality. We further complement
the evaluation metrics of 3D generative models with the shading-image-based
Fr\'echet inception distance (FID) scores to better assess visual quality and
shape distribution of the generated shapes. Experiments on shape generation
demonstrate the superior performance of SDF-StyleGAN over the state-of-the-art.
We further demonstrate the efficacy of SDF-StyleGAN in various tasks based on
GAN inversion, including shape reconstruction, shape completion from partial
point clouds, single-view image-based shape generation, and shape style
editing. Extensive ablation studies justify the efficacy of our framework
design. Our code and trained models are available at
https://github.com/Zhengxinyang/SDF-StyleGAN.",1,0,0,0,1,0,0.594945,6.0,0.71489,87
b750f8c0-8fea-49a0-9b9e-deed41bcb5f2,Compression of Generative Pre-trained Language Models via Quantization,70,0.0867018,0.980507,"The increasing size of generative Pre-trained Language Models (PLMs) has
greatly increased the demand for model compression. Despite various methods to
compress BERT or its variants, there are few attempts to compress generative
PLMs, and the underlying difficulty remains unclear. In this paper, we compress
generative PLMs by quantization. We find that previous quantization methods
fail on generative tasks due to the \textit{homogeneous word embeddings} caused
by reduced capacity, and \textit{varied distribution of weights}.
Correspondingly, we propose a token-level contrastive distillation to learn
distinguishable word embeddings, and a module-wise dynamic scaling to make
quantizers adaptive to different modules. Empirical results on various tasks
show that our proposed method outperforms the state-of-the-art compression
methods on generative PLMs by a clear margin. With comparable performance with
the full-precision models, we achieve 14.4x and 13.4x compression rates on
GPT-2 and BART, respectively.",0,1,0,0,1,0,0.701777,5.0,0.716222,42
035ae3f8-488f-49df-aa2a-18f2925865c6,Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages,2,0.0547534,0.11789,"Translation Quality Estimation (QE) is the task of predicting the quality of
machine translation (MT) output without any reference. This task has gained
increasing attention as an important component in the practical applications of
MT. In this paper, we first propose XLMRScore, which is a cross-lingual
counterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric
can be used as a simple unsupervised QE method, nevertheless facing two issues:
firstly, the untranslated tokens leading to unexpectedly high translation
scores, and secondly, the issue of mismatching errors between source and
hypothesis tokens when applying the greedy matching in XLMRScore. To mitigate
these issues, we suggest replacing untranslated words with the unknown token
and the cross-lingual alignment of the pre-trained model to represent aligned
words closer to each other, respectively. We evaluate the proposed method on
four low-resource language pairs of the WMT21 QE shared task, as well as a new
English$\rightarrow$Persian (En-Fa) test dataset introduced in this paper.
Experiments show that our method could get comparable results with the
supervised baseline for two zero-shot scenarios, i.e., with less than 0.01
difference in Pearson correlation, while outperforming unsupervised rivals in
all the low-resource language pairs for above 8%, on average.",0,1,0,1,0,0,0.689651,7.0,0.792517,52
8b6f39ce-e520-4a1a-a8d4-a4d33d4c5725,GPT Takes the Bar Exam,110,0.732663,0.999797,"Nearly all jurisdictions in the United States require a professional license
exam, commonly referred to as ""the Bar Exam,"" as a precondition for law
practice. To even sit for the exam, most jurisdictions require that an
applicant completes at least seven years of post-secondary education, including
three years at an accredited law school. In addition, most test-takers also
undergo weeks to months of further, exam-specific preparation. Despite this
significant investment of time and capital, approximately one in five
test-takers still score under the rate required to pass the exam on their first
try. In the face of a complex task that requires such depth of knowledge, what,
then, should we expect of the state of the art in ""AI?"" In this research, we
document our experimental evaluation of the performance of OpenAI's
`text-davinci-003` model, often-referred to as GPT-3.5, on the multistate
multiple choice (MBE) section of the exam. While we find no benefit in
fine-tuning over GPT-3.5's zero-shot performance at the scale of our training
data, we do find that hyperparameter optimization and prompt engineering
positively impacted GPT-3.5's zero-shot performance. For best prompt and
parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete
NCBE MBE practice exam, significantly in excess of the 25% baseline guessing
rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5's
ranking of responses is also highly-correlated with correctness; its top two
and top three choices are correct 71% and 88% of the time, respectively,
indicating very strong non-entailment performance. While our ability to
interpret these results is limited by nascent scientific understanding of LLMs
and the proprietary nature of GPT, we believe that these results strongly
suggest that an LLM will pass the MBE component of the Bar Exam in the near
future.",1,1,0,0,0,0,0.466381,6.0,0.654244,31
8269f409-a906-498f-a154-58f80753c6e7,A Data Cartography based MixUp for Pre-trained Language Models,4,0.0143302,0.150975,"MixUp is a data augmentation strategy where additional samples are generated
during training by combining random pairs of training samples and their labels.
However, selecting random pairs is not potentially an optimal choice. In this
work, we propose TDMixUp, a novel MixUp strategy that leverages Training
Dynamics and allows more informative samples to be combined for generating new
data samples. Our proposed TDMixUp first measures confidence, variability,
(Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al.,
2020) to identify the characteristics of training samples (e.g., as
easy-to-learn or ambiguous samples), and then interpolates these characterized
samples. We empirically validate that our method not only achieves competitive
performance using a smaller subset of the training data compared with strong
baselines, but also yields lower expected calibration error on the pre-trained
language model, BERT, on both in-domain and out-of-domain settings in a wide
range of NLP tasks. We publicly release our code.",1,1,0,0,0,0,0.488958,6.0,0.665338,19
77154ad5-c8aa-4963-bfad-11760b2c6026,Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models,5,0.025821,0.205787,"Our goal is to develop fine-grained real-image editing methods suitable for
real-world applications. In this paper, we first summarize four requirements
for these methods and propose a novel diffusion-based image editing framework
with pixel-wise guidance that satisfies these requirements. Specifically, we
train pixel-classifiers with a few annotated data and then infer the
segmentation map of a target image. Users then manipulate the map to instruct
how the image will be edited. We utilize a pre-trained diffusion model to
generate edited images aligned with the user's intention with pixel-wise
guidance. The effective combination of proposed guidance and other techniques
enables highly controllable editing with preserving the outside of the edited
area, which results in meeting our requirements. The experimental results
demonstrate that our proposal outperforms the GAN-based method for editing
quality and speed.",1,1,0,0,1,0,0.904706,5.0,0.849062,62
48975078-7932-43e5-9454-7bf6466b0230,Federated Learning with Position-Aware Neurons,21,0.892226,0.657481,"Federated Learning (FL) fuses collaborative models from local nodes without
centralizing users' data. The permutation invariance property of neural
networks and the non-i.i.d. data across clients make the locally updated
parameters imprecisely aligned, disabling the coordinate-based parameter
averaging. Traditional neurons do not explicitly consider position information.
Hence, we propose Position-Aware Neurons (PANs) as an alternative, fusing
position-related values (i.e., position encodings) into neuron outputs. PANs
couple themselves to their positions and minimize the possibility of
dislocation, even updating on heterogeneous data. We turn on/off PANs to
disable/enable the permutation invariance property of neural networks. PANs are
tightly coupled with positions when applied to FL, making parameters across
clients pre-aligned and facilitating coordinate-based parameter averaging. PANs
are algorithm-agnostic and could universally improve existing FL algorithms.
Furthermore, ""FL with PANs"" is simple to implement and computationally
friendly.",1,1,0,0,0,0,0.992311,8.0,0.996653,55
7fa0b7e0-9241-4d50-8dd5-06fcf59aeaae,Offline Reinforcement Learning with Differential Privacy,9,0.546581,0.571326,"The offline reinforcement learning (RL) problem is often motivated by the
need to learn data-driven decision policies in financial, legal and healthcare
applications. However, the learned policy could retain sensitive information of
individuals in the training data (e.g., treatment and outcome of patients),
thus susceptible to various privacy risks. We design offline RL algorithms with
differential privacy guarantees which provably prevent such risks. These
algorithms also enjoy strong instance-dependent learning bounds under both
tabular and linear Markov decision process (MDP) settings. Our theory and
simulation suggest that the privacy guarantee comes at (almost) no drop in
utility comparing to the non-private counterpart for a medium-size dataset.",0,0,0,0,0,0,0.916981,5.0,0.860464,70
449b44c6-ebb4-41c9-bcd5-cbe9b8fd1a26,Predicting Long-Term Citations from Short-Term Linguistic Influence,1,0.014441,0.0562056,"A standard measure of the influence of a research paper is the number of
times it is cited. However, papers may be cited for many reasons, and citation
count offers limited information about the extent to which a paper affected the
content of subsequent publications. We therefore propose a novel method to
quantify linguistic influence in timestamped document collections. There are
two main steps: first, identify lexical and semantic changes using contextual
embeddings and word frequencies; second, aggregate information about these
changes into per-document influence scores by estimating a high-dimensional
Hawkes process with a low-rank parameter matrix. We show that this measure of
linguistic influence is predictive of $\textit{future}$ citations: the estimate
of linguistic influence from the two years after a paper's publication is
correlated with and predictive of its citation count in the following three
years. This is demonstrated using an online evaluation with incremental
temporal training/test splits, in comparison with a strong baseline that
includes predictors for initial citation counts, topics, and lexical features.",1,0,0,0,0,0,0.132949,17.0,0.790779,57
156644e2-07dd-44e5-918d-001561ceec97,"Event Causality Identification with Causal News Corpus -- Shared Task 3, CASE 2022",23,0.430279,0.797048,"The Event Causality Identification Shared Task of CASE 2022 involved two
subtasks working on the Causal News Corpus. Subtask 1 required participants to
predict if a sentence contains a causal relation or not. This is a supervised
binary classification task. Subtask 2 required participants to identify the
Cause, Effect and Signal spans per causal sentence. This could be seen as a
supervised sequence labeling task. For both subtasks, participants uploaded
their predictions for a held-out test set, and ranking was done based on binary
F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes
the work of the 17 teams that submitted their results to our competition and 12
system description papers that were received. The best F1 scores achieved for
Subtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing
approaches involved pre-trained language models fine-tuned to the targeted
task. We further discuss these approaches and analyze errors across
participants' systems in this paper.",0,1,0,1,0,0,0.218922,7.0,0.57036,70
3dbaffc3-32ad-4e22-9d3a-23cd4ea69aad,"Adversarial Pretraining of Self-Supervised Deep Networks: Past, Present and Future",6,0.0143727,0.109741,"In this paper, we review adversarial pretraining of self-supervised deep
networks including both convolutional neural networks and vision transformers.
Unlike the adversarial training with access to labeled examples, adversarial
pretraining is complicated as it only has access to unlabeled examples. To
incorporate adversaries into pretraining models on either input or feature
level, we find that existing approaches are largely categorized into two
groups: memory-free instance-wise attacks imposing worst-case perturbations on
individual examples, and memory-based adversaries shared across examples over
iterations. In particular, we review several representative adversarial
pretraining models based on Contrastive Learning (CL) and Masked Image Modeling
(MIM), respectively, two popular self-supervised pretraining methods in
literature. We also review miscellaneous issues about computing overheads,
input-/feature-level adversaries, as well as other adversarial pretraining
approaches beyond the above two groups. Finally, we discuss emerging trends and
future directions about the relations between adversarial and cooperative
pretraining, unifying adversarial CL and MIM pretraining, and the trade-off
between accuracy and robustness in adversarial pretraining.",1,0,0,0,0,0,0.461676,6.0,0.651898,83
905f157f-3672-4f4f-a244-eefadfc45657,Disentangling Visual Embeddings for Attributes and Objects,33,0.221633,0.658334,"We study the problem of compositional zero-shot learning for object-attribute
recognition. Prior works use visual features extracted with a backbone network,
pre-trained for object classification and thus do not capture the subtly
distinct features associated with attributes. To overcome this challenge, these
studies employ supervision from the linguistic space, and use pre-trained word
embeddings to better separate and compose attribute-object pairs for
recognition. Analogous to linguistic embedding space, which already has unique
and agnostic embeddings for object and attribute, we shift the focus back to
the visual space and propose a novel architecture that can disentangle
attribute and object features in the visual space. We use visual decomposed
features to hallucinate embeddings that are representative for the seen and
novel compositions to better regularize the learning of our model. Extensive
experiments show that our method outperforms existing work with significant
margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created
based on VAW. The code, models, and dataset splits are publicly available at
https://github.com/nirat1606/OADis.",1,0,0,0,1,0,0.0724336,11.0,0.61844,59
b7b9cd66-3ecc-4b96-af3d-8c579b63bb86,Instance-Specific Image Goal Navigation: Training Embodied Agents to Find Object Instances,12,0.630497,0.797243,"We consider the problem of embodied visual navigation given an image-goal
(ImageNav) where an agent is initialized in an unfamiliar environment and
tasked with navigating to a location 'described' by an image. Unlike related
navigation tasks, ImageNav does not have a standardized task definition which
makes comparison across methods difficult. Further, existing formulations have
two problematic properties; (1) image-goals are sampled from random locations
which can lead to ambiguity (e.g., looking at walls), and (2) image-goals match
the camera specification and embodiment of the agent; this rigidity is limiting
when considering user-driven downstream applications. We present the
Instance-specific ImageNav task (InstanceImageNav) to address these
limitations. Specifically, the goal image is 'focused' on some particular
object instance in the scene and is taken with camera parameters independent of
the agent. We instantiate InstanceImageNav in the Habitat Simulator using
scenes from the Habitat-Matterport3D dataset (HM3D) and release a standardized
benchmark to measure community progress.",1,1,1,1,0,0,0.975029,6.0,0.949366,27
a208236b-1acf-4c0d-b733-6fd125534578,COEM: Cross-Modal Embedding for MetaCell Identification,5,0.0112965,0.204023,"Metacells are disjoint and homogeneous groups of single-cell profiles,
representing discrete and highly granular cell states. Existing metacell
algorithms tend to use only one modality to infer metacells, even though
single-cell multi-omics datasets profile multiple molecular modalities within
the same cell. Here, we present \textbf{C}ross-M\textbf{O}dal
\textbf{E}mbedding for \textbf{M}etaCell Identification (COEM), which utilizes
an embedded space leveraging the information of both scATAC-seq and scRNA-seq
to perform aggregation, balancing the trade-off between fine resolution and
sufficient sequencing coverage. COEM outperforms the state-of-the-art method
SEACells by efficiently identifying accurate and well-separated metacells
across datasets with continuous and discrete cell types. Furthermore, COEM
significantly improves peak-to-gene association analyses, and facilitates
complex gene regulatory inference tasks.",0,1,0,0,1,0,0.18881,5.0,0.36527,21
a1e0d6da-139a-47d7-964d-e55865d63541,FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction,5,0.0189275,0.215563,"This paper presents FAMIE, a comprehensive and efficient active learning (AL)
toolkit for multilingual information extraction. FAMIE is designed to address a
fundamental problem in existing AL frameworks where annotators need to wait for
a long time between annotation batches due to the time-consuming nature of
model training and data selection at each AL iteration. This hinders the
engagement, productivity, and efficiency of annotators. Based on the idea of
using a small proxy network for fast data selection, we introduce a novel
knowledge distillation mechanism to synchronize the proxy network with the main
large model (i.e., BERT-based) to ensure the appropriateness of the selected
annotation examples for the main model. Our AL framework can support multiple
languages. The experiments demonstrate the advantages of FAMIE in terms of
competitive performance and time efficiency for sequence labeling with AL. We
publicly release our code (\url{https://github.com/nlp-uoregon/famie}) and demo
website (\url{http://nlp.uoregon.edu:9000/}). A demo video for FAMIE is
provided at: \url{https://youtu.be/I2i8n_jAyrY}.",1,1,0,0,0,0,0.0976068,7.0,0.444952,41
bb15be99-4ab7-496a-a455-83ffaa18615b,Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks,6,0.0553551,0.380695,"Autonomous agents are able to draw on a wide variety of potential sources of
task knowledge; however current approaches invariably focus on only one or two.
Here we investigate the challenges and impact of exploiting diverse knowledge
sources to learn online, in one-shot, new tasks for a simulated office mobile
robot. The resulting agent, developed in the Soar cognitive architecture, uses
the following sources of domain and task knowledge: interaction with the
environment, task execution and search knowledge, human natural language
instruction, and responses retrieved from a large language model (GPT-3). We
explore the distinct contributions of these knowledge sources and evaluate the
performance of different combinations in terms of learning correct task
knowledge and human workload. Results show that an agent's online integration
of diverse knowledge sources improves one-shot task learning overall, reducing
human feedback needed for rapid and reliable task learning.",0,0,0,0,0,0,0.516947,5.0,0.614519,29
7a712011-9a3a-41bb-a83e-69a25d20e705,Delving Deeper into Cross-lingual Visual Question Answering,7,0.259863,0.228138,"Visual question answering (VQA) is one of the crucial vision-and-language
tasks. Yet, existing VQA research has mostly focused on the English language,
due to a lack of suitable evaluation resources. Previous work on cross-lingual
VQA has reported poor zero-shot transfer performance of current multilingual
multimodal Transformers with large gaps to monolingual performance, without any
deeper analysis. In this work, we delve deeper into the different aspects of
cross-lingual VQA, aiming to understand the impact of 1) modeling methods and
choices, including architecture, inductive bias, fine-tuning; 2) learning
biases: including question types and modality biases in cross-lingual setups.
The key results of our analysis are: 1) We show that simple modifications to
the standard training setup can substantially reduce the transfer gap to
monolingual English performance, yielding +10 accuracy points over existing
methods. 2) We analyze cross-lingual VQA across different question types of
varying complexity for different multilingual multimodal Transformers, and
identify question types that are the most difficult to improve on. 3) We
provide an analysis of modality biases present in training data and models,
revealing why zero-shot performance gaps remain for certain question types and
languages.",0,0,0,0,0,0,0.968792,4.0,0.908469,52
47e2d540-ac4e-404a-b11d-f24f2bcde11e,"EMMT: A simultaneous eye-tracking, 4-electrode EEG and audio corpus for multi-modal reading and translation scenarios",3,0.103669,0.353492,"We present the Eyetracked Multi-Modal Translation (EMMT) corpus, a dataset
containing monocular eye movement recordings, audio and 4-electrode
electroencephalogram (EEG) data of 43 participants. The objective was to
collect cognitive signals as responses of participants engaged in a number of
language intensive tasks involving different text-image stimuli settings when
translating from English to Czech.
  Each participant was exposed to 32 text-image stimuli pairs and asked to (1)
read the English sentence, (2) translate it into Czech, (3) consult the image,
(4) translate again, either updating or repeating the previous translation. The
text stimuli consisted of 200 unique sentences with 616 unique words coupled
with 200 unique images as the visual stimuli.
  The recordings were collected over a two week period and all the participants
included in the study were Czech natives with strong English skills. Due to the
nature of the tasks involved in the study and the relatively large number of
participants involved, the corpus is well suited for research in Translation
Process Studies, Cognitive Sciences among other disciplines.",1,1,0,1,0,0,0.784561,8.0,0.852393,30
502f1f30-ed03-4005-849b-dc03d8d9caca,Provable Benefits of Representational Transfer in Reinforcement Learning,24,0.546703,0.441008,"We study the problem of representational transfer in RL, where an agent first
pretrains in a number of source tasks to discover a shared representation,
which is subsequently used to learn a good policy in a \emph{target task}. We
propose a new notion of task relatedness between source and target tasks, and
develop a novel approach for representational transfer under this assumption.
Concretely, we show that given generative access to source tasks, we can
discover a representation, using which subsequent linear RL techniques quickly
converge to a near-optimal policy in the target task.
  The sample complexity is close to knowing the ground truth features in the
target task, and comparable to prior representation learning results in the
source tasks. We complement our positive results with lower bounds without
generative access, and validate our findings with empirical evaluation on rich
observation MDPs that require deep exploration. In our experiments, we observe
a speed up in learning in the target by pre-training, and also validate the
need for generative access in source tasks.",0,0,1,0,0,0,0.877228,5.0,0.826255,65
9e0d93d7-1a18-45a1-8b88-fdbe36f1bd3e,Debiasing Word Embeddings with Nonlinear Geometry,5,0.0383273,0.423781,"Debiasing word embeddings has been largely limited to individual and
independent social categories. However, real-world corpora typically present
multiple social categories that possibly correlate or intersect with each
other. For instance, ""hair weaves"" is stereotypically associated with African
American females, but neither African American nor females alone. Therefore,
this work studies biases associated with multiple social categories: joint
biases induced by the union of different categories and intersectional biases
that do not overlap with the biases of the constituent categories. We first
empirically observe that individual biases intersect non-trivially (i.e., over
a one-dimensional subspace). Drawing from the intersectional theory in social
science and the linguistic theory, we then construct an intersectional subspace
to debias for multiple social categories using the nonlinear geometry of
individual biases. Empirical evaluations corroborate the efficacy of our
approach. Data and implementation code can be downloaded at
https://github.com/GitHubLuCheng/Implementation-of-JoSEC-COLING-22.",1,0,0,0,0,0,0.388181,9.0,0.742211,48
c1c8550d-f07f-4ea3-a942-5c8f305c53d1,Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations,21,0.44003,0.674969,"Neural networks have achieved tremendous success in a large variety of
applications. However, their memory footprint and computational demand can
render them impractical in application settings with limited hardware or energy
resources. In this work, we propose a novel algorithm to find efficient
low-rank subnetworks. Remarkably, these subnetworks are determined and adapted
already during the training phase and the overall time and memory resources
required by both training and evaluating them are significantly reduced. The
main idea is to restrict the weight matrices to a low-rank manifold and to
update the low-rank factors rather than the full matrix during training. To
derive training updates that are restricted to the prescribed manifold, we
employ techniques from dynamic model order reduction for matrix differential
equations. This allows us to provide approximation, stability, and descent
guarantees. Moreover, our method automatically and dynamically adapts the ranks
during training to achieve the desired approximation accuracy. The efficiency
of the proposed method is demonstrated through a variety of numerical
experiments on fully-connected and convolutional networks.",1,0,0,0,0,0,0.729365,10.0,0.865828,67
6553def9-5034-45fb-8372-8925eb3b8151,How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning,55,0.479498,0.832809,"To avoid collapse in self-supervised learning (SSL), a contrastive loss is
widely used but often requires a large number of negative samples. Without
negative samples yet achieving competitive performance, a recent work has
attracted significant attention for providing a minimalist simple Siamese
(SimSiam) method to avoid collapse. However, the reason for how it avoids
collapse without negative samples remains not fully clear and our investigation
starts by revisiting the explanatory claims in the original SimSiam. After
refuting their claims, we introduce vector decomposition for analyzing the
collapse based on the gradient analysis of the $l_2$-normalized representation
vector. This yields a unified perspective on how negative samples and SimSiam
alleviate collapse. Such a unified perspective comes timely for understanding
the recent progress in SSL.",0,0,0,0,0,0,0.983227,4.0,0.949654,34
2b58fca3-b314-46af-a27a-5255ab793c59,Threshold Treewidth and Hypertree Width,2,0.352812,0.0516171,"Treewidth and hypertree width have proven to be highly successful structural
parameters in the context of the Constraint Satisfaction Problem (CSP). When
either of these parameters is bounded by a constant, then CSP becomes solvable
in polynomial time. However, here the order of the polynomial in the running
time depends on the width, and this is known to be unavoidable; therefore, the
problem is not fixed-parameter tractable parameterized by either of these width
measures. Here we introduce an enhancement of tree and hypertree width through
a novel notion of thresholds, allowing the associated decompositions to take
into account information about the computational costs associated with solving
the given CSP instance. Aside from introducing these notions, we obtain
efficient theoretical as well as empirical algorithms for computing threshold
treewidth and hypertree width and show that these parameters give rise to
fixed-parameter algorithms for CSP as well as other, more general problems. We
complement our theoretical results with experimental evaluations in terms of
heuristics as well as exact methods based on SAT/SMT encodings.",0,0,1,0,0,0,0.000180467,22.0,0.535031,67
f8430f13-504e-4c01-a472-c963bc4cefd7,A Portable Multiscopic Camera for Novel View and Time Synthesis in Dynamic Scenes,2,0.037533,0.0689087,"We present a portable multiscopic camera system with a dedicated model for
novel view and time synthesis in dynamic scenes. Our goal is to render
high-quality images for a dynamic scene from any viewpoint at any time using
our portable multiscopic camera. To achieve such novel view and time synthesis,
we develop a physical multiscopic camera equipped with five cameras to train a
neural radiance field (NeRF) in both time and spatial domains for dynamic
scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal
coordinate, and 2D viewing direction) to view-dependent and time-varying
emitted radiance and volume density. Volume rendering is applied to render a
photo-realistic image at a specified camera pose and time. To improve the
robustness of our physical camera, we propose a camera parameter optimization
module and a temporal frame interpolation module to promote information
propagation across time. We conduct experiments on both real-world and
synthetic datasets to evaluate our system, and the results show that our
approach outperforms alternative solutions qualitatively and quantitatively.
Our code and dataset are available at https://yuenfuilau.github.io.",1,1,0,1,0,0,0.944061,6.0,0.90826,41
1d187d96-f702-4112-bae0-5939156bc61d,Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges,4,0.0461499,0.0528729,"This paper presents the Crowd Score, a novel method to assess the funniness
of jokes using large language models (LLMs) as AI judges. Our method relies on
inducing different personalities into the LLM and aggregating the votes of the
AI judges into a single score to rate jokes. We validate the votes using an
auditing technique that checks if the explanation for a particular vote is
reasonable using the LLM. We tested our methodology on 52 jokes in a crowd of
four AI voters with different humour types: affiliative, self-enhancing,
aggressive and self-defeating. Our results show that few-shot prompting leads
to better results than zero-shot for the voting question. Personality induction
showed that aggressive and self-defeating voters are significantly more
inclined to find more jokes funny of a set of aggressive/self-defeating jokes
than the affiliative and self-enhancing voters. The Crowd Score follows the
same trend as human judges by assigning higher scores to jokes that are also
considered funnier by human judges. We believe that our methodology could be
applied to other creative domains such as story, poetry, slogans, etc. It could
both help the adoption of a flexible and accurate standard approach to compare
different work in the CC community under a common metric and by minimizing
human participation in assessing creative artefacts, it could accelerate the
prototyping of creative artefacts and reduce the cost of hiring human
participants to rate creative artefacts.",0,0,0,0,0,0,0.954626,1.0,0.519646,29
550b8930-a30b-4a62-8d45-a899b7d8a64d,Mukayese: Turkish NLP Strikes Back,15,0.0767465,0.308904,"Having sufficient resources for language X lifts it from the under-resourced
languages class, but not necessarily from the under-researched class. In this
paper, we address the problem of the absence of organized benchmarks in the
Turkish language. We demonstrate that languages such as Turkish are left behind
the state-of-the-art in NLP applications. As a solution, we present Mukayese, a
set of NLP benchmarks for the Turkish language that contains several NLP tasks.
We work on one or more datasets for each benchmark and present two or more
baselines. Moreover, we present four new benchmarking datasets in Turkish for
language modeling, sentence segmentation, and spell checking. All datasets and
baselines are available under: https://github.com/alisafaya/mukayese",0,1,1,1,0,0,0.290342,8.0,0.665057,88
2b9fad92-94d1-4149-aa24-d6ab0c3bfc82,VisCUIT: Visual Auditor for Bias in CNN Image Classifier,8,0.114268,0.2455,"CNN image classifiers are widely used, thanks to their efficiency and
accuracy. However, they can suffer from biases that impede their practical
applications. Most existing bias investigation techniques are either
inapplicable to general image classification tasks or require significant user
efforts in perusing all data subgroups to manually specify which data
attributes to inspect. We present VisCUIT, an interactive visualization system
that reveals how and why a CNN classifier is biased. VisCUIT visually
summarizes the subgroups on which the classifier underperforms and helps users
discover and characterize the cause of the underperformances by revealing image
concepts responsible for activating neurons that contribute to
misclassifications. VisCUIT runs in modern browsers and is open-source,
allowing people to easily access and extend the tool to other model
architectures and datasets. VisCUIT is available at the following public demo
link: https://poloclub.github.io/VisCUIT. A video demo is available at
https://youtu.be/eNDbSyM4R_4.",1,1,0,0,0,0,0.69841,7.0,0.79597,63
91295552-1030-447d-994a-06fe5130432b,AutoField: Automating Feature Selection in Deep Recommender Systems,41,0.0977227,0.835652,"Feature quality has an impactful effect on recommendation performance.
Thereby, feature selection is a critical process in developing deep
learning-based recommender systems. Most existing deep recommender systems,
however, focus on designing sophisticated neural networks, while neglecting the
feature selection process. Typically, they just feed all possible features into
their proposed deep architectures, or select important features manually by
human experts. The former leads to non-trivial embedding parameters and extra
inference time, while the latter requires plenty of expert knowledge and human
labor effort. In this work, we propose an AutoML framework that can adaptively
select the essential feature fields in an automatic manner. Specifically, we
first design a differentiable controller network, which is capable of
automatically adjusting the probability of selecting a particular feature
field; then, only selected feature fields are utilized to retrain the deep
recommendation model. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our framework. We conduct further experiments
to investigate its properties, including the transferability, key components,
and parameter sensitivity.",0,1,0,0,0,0,0.157487,10.0,0.662661,52
9649ec08-ae0a-497b-b059-3715f0e7d4f9,Exploring Customer Price Preference and Product Profit Role in Recommender Systems,5,0.0842805,0.473111,"Most of the research in the recommender systems domain is focused on the
optimization of the metrics based on historical data such as Mean Average
Precision (MAP) or Recall. However, there is a gap between the research and
industry since the leading Key Performance Indicators (KPIs) for businesses are
revenue and profit. In this paper, we explore the impact of manipulating the
profit awareness of a recommender system. An average e-commerce business does
not usually use a complicated recommender algorithm. We propose an adjustment
of a predicted ranking for score-based recommender systems and explore the
effect of the profit and customers' price preferences on two industry datasets
from the fashion domain. In the experiments, we show the ability to improve
both the precision and the generated recommendations' profit. Such an outcome
represents a win-win situation when e-commerce increases the profit and
customers get more valuable recommendations.",0,1,0,0,0,0,0.00927581,14.0,0.551062,25
4b5b8327-8b9e-4fcf-b1aa-6a6b46437d93,CFA: Coupled-hypersphere-based Feature Adaptation for Target-Oriented Anomaly Localization,86,0.409652,0.999463,"For a long time, anomaly localization has been widely used in industries.
Previous studies focused on approximating the distribution of normal features
without adaptation to a target dataset. However, since anomaly localization
should precisely discriminate normal and abnormal features, the absence of
adaptation may make the normality of abnormal features overestimated. Thus, we
propose Coupled-hypersphere-based Feature Adaptation (CFA) which accomplishes
sophisticated anomaly localization using features adapted to the target
dataset. CFA consists of (1) a learnable patch descriptor that learns and
embeds target-oriented features and (2) scalable memory bank independent of the
size of the target dataset. And, CFA adopts transfer learning to increase the
normal feature density so that abnormal features can be clearly distinguished
by applying patch descriptor and memory bank to a pre-trained CNN. The proposed
method outperforms the previous methods quantitatively and qualitatively. For
example, it provides an AUROC score of 99.5% in anomaly detection and 98.5% in
anomaly localization of MVTec AD benchmark. In addition, this paper points out
the negative effects of biased features of pre-trained CNNs and emphasizes the
importance of the adaptation to the target dataset. The code is publicly
available at https://github.com/sungwool/CFA_for_anomaly_localization.",1,1,0,0,1,0,0.887295,7.0,0.881608,23
5c0d8541-6c7b-4dab-8bb9-3f25da99365d,Submodularity In Machine Learning and Artificial Intelligence,36,0.265191,0.999539,"In this manuscript, we offer a gentle review of submodularity and
supermodularity and their properties. We offer a plethora of submodular
definitions; a full description of a number of example submodular functions and
their generalizations; example discrete constraints; a discussion of basic
algorithms for maximization, minimization, and other operations; a brief
overview of continuous submodular extensions; and some historical applications.
We then turn to how submodularity is useful in machine learning and artificial
intelligence. This includes summarization, and we offer a complete account of
the differences between and commonalities amongst sketching, coresets,
extractive and abstractive summarization in NLP, data distillation and
condensation, and data subset selection and feature selection. We discuss a
variety of ways to produce a submodular function useful for machine learning,
including heuristic hand-crafting, learning or approximately learning a
submodular function or aspects thereof, and some advantages of the use of a
submodular function as a coreset producer. We discuss submodular combinatorial
information functions, and how submodularity is useful for clustering, data
partitioning, parallel machine learning, active and semi-supervised learning,
probabilistic modeling, and structured norms and loss functions.",0,0,0,0,0,0,0.0029492,18.0,0.58699,261
209d2944-e973-43b1-afd0-37a637e28260,Which country is this picture from? New data and methods for DNN-based country recognition,1,0.0471831,0.0952985,"Recognizing the country where a picture has been taken has many potential
applications, such as identification of fake news and prevention of
disinformation campaigns. Previous works focused on the estimation of the
geo-coordinates where a picture has been taken. Yet, recognizing in which
country an image was taken could be more critical, from a semantic and forensic
point of view, than estimating its spatial coordinates. In the above framework,
this paper provides two contributions. First, we introduce the VIPPGeo dataset,
containing 3.8 million geo-tagged images. Secondly, we used the dataset to
train a model casting the country recognition problem as a classification
problem. The experiments show that our model provides better results than the
current state of the art. Notably, we found that asking the network to identify
the country provides better results than estimating the geo-coordinates and
then tracing them back to the country where the picture was taken.",0,1,0,1,1,0,0.504841,14.0,0.859856,31
8a788169-2014-417d-b5a8-2c965145dfc8,CORE: Consistent Representation Learning for Face Forgery Detection,18,0.347803,0.694656,"Face manipulation techniques develop rapidly and arouse widespread public
concerns. Despite that vanilla convolutional neural networks achieve acceptable
performance, they suffer from the overfitting issue. To relieve this issue,
there is a trend to introduce some erasing-based augmentations. We find that
these methods indeed attempt to implicitly induce more consistent
representations for different augmentations via assigning the same label for
different augmented images. However, due to the lack of explicit
regularization, the consistency between different representations is less
satisfactory. Therefore, we constrain the consistency of different
representations explicitly and propose a simple yet effective framework,
COnsistent REpresentation Learning (CORE). Specifically, we first capture the
different representations with different augmentations, then regularize the
cosine distance of the representations to enhance the consistency. Extensive
experiments (in-dataset and cross-dataset) demonstrate that CORE performs
favorably against state-of-the-art face forgery detection methods.",1,1,0,0,1,0,0.915073,6.0,0.882191,55
c766c5f2-7a24-4f15-8e9c-c78fa806cc27,UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes,49,0.24181,0.635325,"We introduce UViM, a unified approach capable of modeling a wide range of
computer vision tasks. In contrast to previous models, UViM has the same
functional form for all tasks; it requires no task-specific modifications which
require extensive human expertise. The approach involves two components: (I) a
base model (feed-forward) which is trained to directly predict raw vision
outputs, guided by a learned discrete code and (II) a language model
(autoregressive) that is trained to generate the guiding code. These components
complement each other: the language model is well-suited to modeling structured
interdependent data, while the base model is efficient at dealing with
high-dimensional outputs. We demonstrate the effectiveness of UViM on three
diverse and challenging vision tasks: panoptic segmentation, depth prediction
and image colorization, where we achieve competitive and near state-of-the-art
results. Our experimental results suggest that UViM is a promising candidate
for a unified modeling approach in computer vision.",1,0,0,0,0,1,0.746929,11.0,0.882577,63
193590bc-21e9-403d-9b80-e1b649ea3a7f,Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks,5,0.199207,0.637302,"Early diagnosis and intervention are clinically considered the paramount part
of treating cerebral palsy (CP), so it is essential to design an efficient and
interpretable automatic prediction system for CP. We highlight a significant
difference between CP infants' frequency of human movement and that of the
healthy group, which improves prediction performance. However, the existing
deep learning-based methods did not use the frequency information of infants'
movement for CP prediction. This paper proposes a frequency attention informed
graph convolutional network and validates it on two consumer-grade RGB video
datasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency
attention module aids in improving both classification performance and system
interpretability. In addition, we design a frequency-binning method that
retains the critical frequency of the human joint position data while filtering
the noise. Our prediction performance achieves state-of-the-art research on
both datasets. Our work demonstrates the effectiveness of frequency information
in supporting the prediction of CP non-intrusively and provides a way for
supporting the early diagnosis of CP in the resource-limited regions where the
clinical resources are not abundant.",1,1,0,0,1,0,0.770861,8.0,0.847269,27
50a32756-278a-4c49-9cd4-fce476346a2d,Fair NLP Models with Differentially Private Text Encoders,6,0.0552184,0.332056,"Encoded text representations often capture sensitive attributes about
individuals (e.g., race or gender), which raise privacy concerns and can make
downstream models unfair to certain groups. In this work, we propose FEDERATE,
an approach that combines ideas from differential privacy and adversarial
training to learn private text representations which also induces fairer
models. We empirically evaluate the trade-off between the privacy of the
representations and the fairness and accuracy of the downstream model on four
NLP datasets. Our results show that FEDERATE consistently improves upon
previous methods, and thus suggest that privacy and fairness can positively
reinforce each other.",1,1,0,0,0,0,0.496527,7.0,0.716287,63
99a8f148-9970-4492-bf11-5a6a553de5ea,Visual Knowledge Discovery with Artificial Intelligence: Challenges and Future Directions,8,0.108185,0.333024,"This volume is devoted to the emerging field of Integrated Visual Knowledge
Discovery that combines advances in Artificial Intelligence/Machine Learning
(AI/ML) and Visualization/Visual Analytics. Chapters included are extended
versions of the selected AI and Visual Analytics papers and related symposia at
the recent International Information Visualization Conferences (IV2019 and
IV2020). AI/ML face a long-standing challenge of explaining models to humans.
Models explanation is fundamentally human activity, not only an algorithmic
one. In this chapter we aim to present challenges and future directions within
the field of Visual Analytics, Visual Knowledge Discovery and AI/ML, and to
discuss the role of visualization in visual AI/ML. In addition, we describe
progress in emerging Full 2D ML, natural language processing, and AI/ML in
multidimensional data aided by visual means.",0,0,0,0,0,0,0.0984489,9.0,0.569302,91
15d55e96-66a1-4d0a-974f-2d18764165ef,CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task,64,0.251333,0.997742,"We present the joint contribution of IST and Unbabel to the WMT 2022 Shared
Task on Quality Estimation (QE). Our team participated on all three subtasks:
(i) Sentence and Word-level Quality Prediction; (ii) Explainable QE; and (iii)
Critical Error Detection. For all tasks we build on top of the COMET framework,
connecting it with the predictor-estimator architecture of OpenKiwi, and
equipping it with a word-level sequence tagger and an explanation extractor.
Our results suggest that incorporating references during pretraining improves
performance across several language pairs on downstream tasks, and that jointly
training with sentence and word-level objectives yields a further boost.
Furthermore, combining attention and gradient information proved to be the top
strategy for extracting good explanations of sentence-level QE models. Overall,
our submissions achieved the best results for all three tasks for almost all
language pairs by a considerable margin.",0,1,0,0,0,0,0.348175,5.0,0.508376,35
4984df6e-134c-4bd9-8589-ada6482cb2ca,Benchmarking for Public Health Surveillance tasks on Social Media with a Domain-Specific Pretrained Language Model,22,0.324887,0.899891,"A user-generated text on social media enables health workers to keep track of
information, identify possible outbreaks, forecast disease trends, monitor
emergency cases, and ascertain disease awareness and response to official
health correspondence. This exchange of health information on social media has
been regarded as an attempt to enhance public health surveillance (PHS).
Despite its potential, the technology is still in its early stages and is not
ready for widespread application. Advancements in pretrained language models
(PLMs) have facilitated the development of several domain-specific PLMs and a
variety of downstream applications. However, there are no PLMs for social media
tasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,
to identify tasks related to public health surveillance on social media. We
compared and benchmarked the performance of PHS-BERT on 25 datasets from
different social medial platforms related to 7 different PHS tasks. Compared
with existing PLMs that are mainly evaluated on limited tasks, PHS-BERT
achieved state-of-the-art performance on all 25 tested datasets, showing that
our PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT
available, we aim to facilitate the community to reduce the computational cost
and introduce new baselines for future works across various PHS-related tasks.",0,1,1,1,1,0,0.722912,5.0,0.728018,47
3c307ff7-e00e-4dd4-b51c-d234ae566be8,Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation,36,0.596421,0.960023,"Scene flow estimation, which extracts point-wise motion between scenes, is
becoming a crucial task in many computer vision tasks. However, all of the
existing estimation methods utilize only the unidirectional features,
restricting the accuracy and generality. This paper presents a novel scene flow
estimation architecture using bidirectional flow embedding layers. The proposed
bidirectional layer learns features along both forward and backward directions,
enhancing the estimation performance. In addition, hierarchical feature
extraction and warping improve the performance and reduce computational
overhead. Experimental results show that the proposed architecture achieved a
new state-of-the-art record by outperforming other approaches with large margin
in both FlyingThings3D and KITTI benchmarks. Codes are available at
https://github.com/cwc1260/BiFlow.",1,1,0,0,1,0,0.932356,10.0,0.93814,50
d56d4982-fc0f-404e-a197-c75b1511c86b,CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction,6,0.103712,0.466698,"Knowledge graph (KG) link prediction is a fundamental task in artificial
intelligence, with applications in natural language processing, information
retrieval, and biomedicine. Recently, promising results have been achieved by
leveraging cross-modal information in KGs, using ensembles that combine
knowledge graph embeddings (KGEs) and contextual language models (LMs).
However, existing ensembles are either (1) not consistently effective in terms
of ranking accuracy gains or (2) impractically inefficient on larger datasets
due to the combinatorial explosion problem of pairwise ranking with deep
language models. In this paper, we propose a novel tiered ranking architecture
CascadER to maintain the ranking accuracy of full ensembling while improving
efficiency considerably. CascadER uses LMs to rerank the outputs of more
efficient base KGEs, relying on an adaptive subset selection scheme aimed at
invoking the LMs minimally while maximizing accuracy gain over the KGE.
Extensive experiments demonstrate that CascadER improves MRR by up to 9 points
over KGE baselines, setting new state-of-the-art performance on four benchmarks
while improving efficiency by one or more orders of magnitude over competitive
cross-modal baselines. Our empirical analyses reveal that diversity of models
across modalities and preservation of individual models' confidence signals
help explain the effectiveness of CascadER, and suggest promising directions
for cross-modal cascaded architectures. Code and pretrained models are
available at https://github.com/tsafavi/cascader.",0,1,0,0,1,0,0.918413,6.0,0.884881,51
243b718b-0fdc-42bd-add9-328aad023147,LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling,16,0.13671,0.449597,"Compared with traditional task-irrelevant downsampling methods, task-oriented
neural networks have shown improved performance in point cloud downsampling
range. Recently, Transformer family of networks has shown a more powerful
learning capacity in visual tasks. However, Transformer-based architectures
potentially consume too many resources which are usually worthless for low
overhead task networks in downsampling range. This paper proposes a novel
light-weight Transformer network (LighTN) for task-oriented point cloud
downsampling, as an end-to-end and plug-and-play solution. In LighTN, a
single-head self-correlation module is presented to extract refined global
contextual features, where three projection matrices are simultaneously
eliminated to save resource overhead, and the output of symmetric matrix
satisfies the permutation invariant. Then, we design a novel downsampling loss
function to guide LighTN focuses on critical point cloud regions with more
uniform distribution and prominent points coverage. Furthermore, We introduce a
feed-forward network scaling mechanism to enhance the learnable capacity of
LighTN according to the expand-reduce strategy. The result of extensive
experiments on classification and registration tasks demonstrates LighTN can
achieve state-of-the-art performance with limited resource overhead.",0,1,0,0,1,0,0.676707,5.0,0.702414,40
31f5c7a0-26c1-493e-b055-d6f8a7cdcd2a,SciNLI: A Corpus for Natural Language Inference on Scientific Text,20,0.144767,0.745651,"Existing Natural Language Inference (NLI) datasets, while being instrumental
in the advancement of Natural Language Understanding (NLU) research, are not
related to scientific text. In this paper, we introduce SciNLI, a large dataset
for NLI that captures the formality in scientific text and contains 107,412
sentence pairs extracted from scholarly papers on NLP and computational
linguistics. Given that the text used in scientific literature differs vastly
from the text used in everyday language both in terms of vocabulary and
sentence structure, our dataset is well suited to serve as a benchmark for the
evaluation of scientific NLU models. Our experiments show that SciNLI is harder
to classify than the existing NLI datasets. Our best performing model with
XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23%
showing that there is substantial room for improvement.",0,1,1,1,0,0,0.472582,14.0,0.853136,42
a798e184-174b-4a9e-8b4d-5b2e2ddc780b,Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation,43,0.235094,0.447359,"The performance of multilingual pretrained models is highly dependent on the
availability of monolingual or parallel text present in a target language.
Thus, the majority of the world's languages cannot benefit from recent progress
in NLP as they have no or limited textual data. To expand possibilities of
using NLP technology in these under-represented languages, we systematically
study strategies that relax the reliance on conventional language resources
through the use of bilingual lexicons, an alternative resource with much better
language coverage. We analyze different strategies to synthesize textual or
labeled data using lexicons, and how this data can be combined with monolingual
or parallel text when available. For 19 under-represented languages across 3
tasks, our methods lead to consistent improvements of up to 5 and 15 points
with and without extra monolingual text respectively. Overall, our study
highlights how NLP methods can be adapted to thousands more languages that are
under-served by current technology",1,1,0,0,0,0,0.530559,5.0,0.622226,51
0c0a5264-9798-4bdb-96db-6e15993bb0ab,OCFormer: One-Class Transformer Network for Image Classification,1,0.00279853,0.0778467,"We propose a novel deep learning framework based on Vision Transformers (ViT)
for one-class classification. The core idea is to use zero-centered Gaussian
noise as a pseudo-negative class for latent space representation and then train
the network using the optimal loss function. In prior works, there have been
tremendous efforts to learn a good representation using varieties of loss
functions, which ensures both discriminative and compact properties. The
proposed one-class Vision Transformer (OCFormer) is exhaustively experimented
on CIFAR-10, CIFAR-100, Fashion-MNIST and CelebA eyeglasses datasets. Our
method has shown significant improvements over competing CNN based one-class
classifier approaches.",0,1,0,0,0,0,0.193816,9.0,0.650612,38
9e678550-9b42-44ad-aefb-7fc4dbddf379,Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation,6,0.333994,0.594463,"Research on Korean grammatical error correction (GEC) is limited, compared to
other major languages such as English. We attribute this problematic
circumstance to the lack of a carefully designed evaluation benchmark for
Korean GEC. In this work, we collect three datasets from different sources
(Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean
grammatical errors. Considering the nature of Korean grammar, We then define 14
error types for Korean and provide KAGAS (Korean Automatic Grammatical error
Annotation System), which can automatically annotate error types from parallel
corpora. We use KAGAS on our datasets to make an evaluation benchmark for
Korean, and present baseline models trained from our datasets. We show that the
model trained with our datasets significantly outperforms the currently used
statistical Korean GEC system (Hanspell) on a wider range of error types,
demonstrating the diversity and usefulness of the datasets. The implementations
and datasets are open-sourced.",0,1,0,1,1,0,0.209593,11.0,0.722116,56
5f810fec-2ad0-4b1f-a41d-9f2600a8f69f,Open-Vocabulary Universal Image Segmentation with MaskCLIP,30,0.76295,0.419424,"In this paper, we tackle an emerging computer vision task, open-vocabulary
universal image segmentation, that aims to perform semantic/instance/panoptic
segmentation (background semantic labeling + foreground instance segmentation)
for arbitrary categories of text-based descriptions in inference time. We first
build a baseline method by directly adopting pre-trained CLIP models without
finetuning or distillation. We then develop MaskCLIP, a Transformer-based
approach with a MaskCLIP Visual Encoder, which is an encoder-only module that
seamlessly integrates mask tokens with a pre-trained ViT CLIP model for
semantic/instance segmentation and class prediction. MaskCLIP learns to
efficiently and effectively utilize pre-trained partial/dense CLIP features
within the MaskCLIP Visual Encoder that avoids the time-consuming
student-teacher training process. MaskCLIP outperforms previous methods for
semantic/instance/panoptic segmentation on ADE20K and PASCAL datasets. We show
qualitative illustrations for MaskCLIP with online custom categories. Project
website: https://maskclip.github.io.",0,1,1,0,1,0,0.989404,3.0,0.968366,43
c5e4046c-5948-4a8a-b5a1-fc05f86d1fa1,On the Explainability of Natural Language Processing Deep Models,35,0.0441738,0.737148,"While there has been a recent explosion of work on ExplainableAI ExAI on deep
models that operate on imagery and tabular data, textual datasets present new
challenges to the ExAI community. Such challenges can be attributed to the lack
of input structure in textual data, the use of word embeddings that add to the
opacity of the models and the difficulty of the visualization of the inner
workings of deep models when they are trained on textual data.
  Lately, methods have been developed to address the aforementioned challenges
and present satisfactory explanations on Natural Language Processing (NLP)
models. However, such methods are yet to be studied in a comprehensive
framework where common challenges are properly stated and rigorous evaluation
practices and metrics are proposed. Motivated to democratize ExAI methods in
the NLP field, we present in this work a survey that studies model-agnostic as
well as model-specific explainability methods on NLP models. Such methods can
either develop inherently interpretable NLP models or operate on pre-trained
models in a post-hoc manner. We make this distinction and we further decompose
the methods into three categories according to what they explain: (1) word
embeddings (input-level), (2) inner workings of NLP models (processing-level)
and (3) models' decisions (output-level). We also detail the different
evaluation approaches interpretability methods in the NLP field. Finally, we
present a case-study on the well-known neural machine translation in an
appendix and we propose promising future research directions for ExAI in the
NLP field.",0,0,0,0,0,0,0.0409085,9.0,0.468328,132
b149d9f7-e1d1-4749-953d-eae72dc1c72d,Synthetic Distracted Driving (SynDD2) dataset for analyzing distracted behaviors and various gaze zones of a driver,24,0.205676,0.773011,"This article presents a synthetic distracted driving (SynDD2 - a continuum of
SynDD1) dataset for machine learning models to detect and analyze drivers'
various distracted behavior and different gaze zones. We collected the data in
a stationary vehicle using three in-vehicle cameras positioned at locations: on
the dashboard, near the rearview mirror, and on the top right-side window
corner. The dataset contains two activity types: distracted activities and gaze
zones for each participant, and each activity type has two sets: without
appearance blocks and with appearance blocks such as wearing a hat or
sunglasses. The order and duration of each activity for each participant are
random. In addition, the dataset contains manual annotations for each activity,
having its start and end time annotated. Researchers could use this dataset to
evaluate the performance of machine learning algorithms to classify various
distracting activities and gaze zones of drivers.",0,1,0,1,0,0,0.0747223,9.0,0.537241,7
850c3493-3736-4939-97cf-1dab1fd1426e,What is Software Quality for AI Engineers? Towards a Thinning of the Fog,8,0.351077,0.872147,"It is often overseen that AI-enabled systems are also software systems and
therefore rely on software quality assurance (SQA). Thus, the goal of this
study is to investigate the software quality assurance strategies adopted
during the development, integration, and maintenance of AI/ML components and
code. We conducted semi-structured interviews with representatives of ten
Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the
interview data identified 12 issues in the development of AI/ML components.
Furthermore, we identified when quality issues arise in AI/ML components and
how they are detected. The results of this study should guide future work on
software quality assurance processes and techniques for AI/ML components.",0,1,0,0,0,0,0.882526,6.0,0.858681,28
0d271240-568a-4ca9-b87c-9422519a91de,Improving Data Driven Inverse Text Normalization using Data Augmentation,2,0.0526357,0.0245531,"Inverse text normalization (ITN) is used to convert the spoken form output of
an automatic speech recognition (ASR) system to a written form. Traditional
handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile
neural modeling approaches require quality large-scale spoken-written pair
examples in the same or similar domain as the ASR system (in-domain data), to
train. Both these approaches require costly and complex annotations. In this
paper, we present a data augmentation technique that effectively generates rich
spoken-written numeric pairs from out-of-domain textual data with minimal human
annotation. We empirically demonstrate that ITN model trained using our data
augmentation technique consistently outperform ITN model trained using only
in-domain data across all numeric surfaces like cardinal, currency, and
fraction, by an overall accuracy of 14.44%.",0,1,0,1,0,0,0.0284637,10.0,0.484584,15
b7663622-ee2d-4045-a04d-aa207f88d823,Unsupervised Domain Adaptation for COVID-19 Information Service with Contrastive Adversarial Domain Mixup,5,0.0298213,0.377368,"In the real-world application of COVID-19 misinformation detection, a
fundamental challenge is the lack of the labeled COVID data to enable
supervised end-to-end training of the models, especially at the early stage of
the pandemic. To address this challenge, we propose an unsupervised domain
adaptation framework using contrastive learning and adversarial domain mixup to
transfer the knowledge from an existing source data domain to the target
COVID-19 data domain. In particular, to bridge the gap between the source
domain and the target domain, our method reduces a radial basis function (RBF)
based discrepancy between these two domains. Moreover, we leverage the power of
domain adversarial examples to establish an intermediate domain mixup, where
the latent representations of the input text from both domains could be mixed
during the training process. Extensive experiments on multiple real-world
datasets suggest that our method can effectively adapt misinformation detection
systems to the unseen COVID-19 target domain with significant improvements
compared to the state-of-the-art baselines.",0,1,0,0,1,0,0.414942,5.0,0.553405,19
38ed675e-e56f-46ce-92c2-d7f033b6e41f,WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution,10,0.125422,0.55241,"Exploiting image patches instead of whole images have proved to be a powerful
approach to tackle various problems in image processing. Recently, Wasserstein
patch priors (WPP), which are based on the comparison of the patch
distributions of the unknown image and a reference image, were successfully
used as data-driven regularizers in the variational formulation of
superresolution. However, for each input image, this approach requires the
solution of a non-convex minimization problem which is computationally costly.
In this paper, we propose to learn two kind of neural networks in an
unsupervised way based on WPP loss functions. First, we show how convolutional
neural networks (CNNs) can be incorporated. Once the network, called WPPNet, is
learned, it can be very efficiently applied to any input image. Second, we
incorporate conditional normalizing flows to provide a tool for uncertainty
quantification. Numerical examples demonstrate the very good performance of
WPPNets for superresolution in various image classes even if the forward
operator is known only approximately.",1,1,0,0,0,0,0.261484,9.0,0.688546,76
d26346b9-2cf8-4996-af1b-5e7f7e93bb8c,Modeling Dual Read/Write Paths for Simultaneous Machine Translation,20,0.169504,0.689124,"Simultaneous machine translation (SiMT) outputs translation while reading
source sentence and hence requires a policy to decide whether to wait for the
next source word (READ) or generate a target word (WRITE), the actions of which
form a read/write path. Although the read/write path is essential to SiMT
performance, no direct supervision is given to the path in the existing
methods. In this paper, we propose a method of dual-path SiMT which introduces
duality constraints to direct the read/write path. According to duality
constraints, the read/write path in source-to-target and target-to-source SiMT
models can be mapped to each other. As a result, the two SiMT models can be
optimized jointly by forcing their read/write paths to satisfy the mapping.
Experiments on En-Vi and De-En tasks show that our method can outperform strong
baselines under all latency.",1,0,0,0,0,0,0.135501,7.0,0.494815,38
a1d78bc0-c9f9-43b2-9096-9bf9304002c5,Multi-View Object Pose Refinement With Differentiable Renderer,15,0.182936,0.491179,"This paper introduces a novel multi-view 6 DoF object pose refinement
approach focusing on improving methods trained on synthetic data. It is based
on the DPOD detector, which produces dense 2D-3D correspondences between the
model vertices and the image pixels in each frame. We have opted for the use of
multiple frames with known relative camera transformations, as it allows
introduction of geometrical constraints via an interpretable ICP-like loss
function. The loss function is implemented with a differentiable renderer and
is optimized iteratively. We also demonstrate that a full detection and
refinement pipeline, which is trained solely on synthetic data, can be used for
auto-labeling real data. We perform quantitative evaluation on LineMOD,
Occlusion, Homebrewed and YCB-V datasets and report excellent performance in
comparison to the state-of-the-art methods trained on the synthetic and real
data. We demonstrate empirically that our approach requires only a few frames
and is robust to close camera locations and noise in extrinsic camera
calibration, making its practical usage easier and more ubiquitous.",0,1,0,0,1,0,0.933342,6.0,0.897806,31
6eb7a3a1-5c0a-4aae-ae24-a65700bb2cc7,L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors,2,0.00171272,0.0400932,"In this research, we propose a tiny image segmentation model, L^3U-net, that
works on low-resource edge devices in real-time. We introduce a data folding
technique that reduces inference latency by leveraging the parallel
convolutional layer processing capability of the CNN accelerators. We also
deploy the proposed model to such a device, MAX78000, and the results show that
L^3U-net achieves more than 90% accuracy over two different segmentation
datasets with 10 fps.",0,1,0,0,0,0,0.0578206,9.0,0.507754,34
19f14fc3-f56e-473f-b3ff-22ef12f75246,Fourier Document Restoration for Robust Document Dewarping and Recognition,18,0.386355,0.538112,"State-of-the-art document dewarping techniques learn to predict 3-dimensional
information of documents which are prone to errors while dealing with documents
with irregular distortions or large variations in depth. This paper presents
FDRNet, a Fourier Document Restoration Network that can restore documents with
different distortions and improve document recognition in a reliable and
simpler manner. FDRNet focuses on high-frequency components in the Fourier
space that capture most structural information but are largely free of
degradation in appearance. It dewarps documents by a flexible Thin-Plate Spline
transformation which can handle various deformations effectively without
requiring deformation annotations in training. These features allow FDRNet to
learn from a small amount of simply labeled training images, and the learned
model can dewarp documents with complex geometric distortion and recognize the
restored texts accurately. To facilitate document restoration research, we
create a benchmark dataset consisting of over one thousand camera documents
with different types of geometric and photometric distortion. Extensive
experiments show that FDRNet outperforms the state-of-the-art by large margins
on both dewarping and text recognition tasks. In addition, FDRNet requires a
small amount of simply labeled training data and is easy to deploy.",1,1,0,1,1,0,0.00851984,23.0,0.723021,48
d6f2815e-bfb9-4409-b924-706ea251c93f,An ASP approach for reasoning on neural networks under a finitely many-valued semantics for weighted conditional knowledge bases,17,0.52997,0.937177,"Weighted knowledge bases for description logics with typicality have been
recently considered under a ""concept-wise"" multipreference semantics (in both
the two-valued and fuzzy case), as the basis of a logical semantics of
MultiLayer Perceptrons (MLPs). In this paper we consider weighted conditional
ALC knowledge bases with typicality in the finitely many-valued case, through
three different semantic constructions. For the boolean fragment LC of ALC we
exploit ASP and ""asprin"" for reasoning with the concept-wise multipreference
entailment under a phi-coherent semantics, suitable to characterize the
stationary states of MLPs. As a proof of concept, we experiment the proposed
approach for checking properties of trained MLPs.
  The paper is under consideration for acceptance in TPLP.",0,0,0,0,0,0,0.191466,20.0,0.842095,53
8c589d35-cc50-425c-8134-1b31e21c14ad,Unbiased Multi-Modality Guidance for Image Inpainting,10,0.0687071,0.481432,"Image inpainting is an ill-posed problem to recover missing or damaged image
content based on incomplete images with masks. Previous works usually predict
the auxiliary structures (e.g., edges, segmentation and contours) to help fill
visually realistic patches in a multi-stage fashion. However, imprecise
auxiliary priors may yield biased inpainted results. Besides, it is
time-consuming for some methods to be implemented by multiple stages of complex
neural networks. To solve this issue, we develop an end-to-end multi-modality
guided transformer network, including one inpainting branch and two auxiliary
branches for semantic segmentation and edge textures. Within each transformer
block, the proposed multi-scale spatial-aware attention module can learn the
multi-modal structural features efficiently via auxiliary denormalization.
Different from previous methods relying on direct guidance from biased priors,
our method enriches semantically consistent context in an image based on
discriminative interplay information from multiple modalities. Comprehensive
experiments on several challenging image inpainting datasets show that our
method achieves state-of-the-art performance to deal with various
regular/irregular masks efficiently.",0,1,0,0,1,0,0.681051,9.0,0.835997,50
e3aca4a2-2a0e-494a-832d-d89de9f71419,Understanding Influence Functions and Datamodels via Harmonic Analysis,12,0.0451351,0.799181,"Influence functions estimate effect of individual data points on predictions
of the model on test data and were adapted to deep learning in Koh and Liang
[2017]. They have been used for detecting data poisoning, detecting helpful and
harmful examples, influence of groups of datapoints, etc. Recently, Ilyas et
al. [2022] introduced a linear regression method they termed datamodels to
predict the effect of training points on outputs on test data. The current
paper seeks to provide a better theoretical understanding of such interesting
empirical phenomena. The primary tool is harmonic analysis and the idea of
noise stability. Contributions include: (a) Exact characterization of the
learnt datamodel in terms of Fourier coefficients. (b) An efficient method to
estimate the residual error and quality of the optimum linear datamodel without
having to train the datamodel. (c) New insights into when influences of groups
of datapoints may or may not add up linearly.",0,0,0,0,0,0,0.103869,7.0,0.454325,39
0b04cf0d-bcca-4487-84c3-c020213885bb,Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining,18,0.22591,0.782053,"Deep visuomotor policy learning, which aims to map raw visual observation to
action, achieves promising results in control tasks such as robotic
manipulation and autonomous driving. However, it requires a huge number of
online interactions with the training environment, which limits its real-world
application. Compared to the popular unsupervised feature learning for visual
recognition, feature pretraining for visuomotor control tasks is much less
explored. In this work, we aim to pretrain policy representations for driving
tasks by watching hours-long uncurated YouTube videos. Specifically, we train
an inverse dynamic model with a small amount of labeled data and use it to
predict action labels for all the YouTube video frames. A new contrastive
policy pretraining method is then developed to learn action-conditioned
features from the video frames with pseudo action labels. Experiments show that
the resulting action-conditioned features obtain substantial improvements for
the downstream reinforcement learning and imitation learning tasks,
outperforming the weights pretrained from previous unsupervised learning
methods and ImageNet pretrained weight. Code, model weights, and data are
available at: https://metadriverse.github.io/ACO.",1,1,0,0,0,0,0.919895,6.0,0.886095,51
8066b01f-5a6b-4144-84b7-d75ca0622b20,Masked Autoencoders for Generic Event Boundary Detection CVPR'2022 Kinetics-GEBD Challenge,2,0.0649705,0.129869,"Generic Event Boundary Detection (GEBD) tasks aim at detecting generic,
taxonomy-free event boundaries that segment a whole video into chunks. In this
paper, we apply Masked Autoencoders to improve algorithm performance on the
GEBD tasks. Our approach mainly adopted the ensemble of Masked Autoencoders
fine-tuned on the GEBD task as a self-supervised learner with other base
models. Moreover, we also use a semi-supervised pseudo-label method to take
full advantage of the abundant unlabeled Kinetics-400 data while training. In
addition, we propose a soft-label method to partially balance the positive and
negative samples and alleviate the problem of ambiguous labeling in this task.
Lastly, a tricky segmentation alignment policy is implemented to refine
boundaries predicted by our models to more accurate locations. With our
approach, we achieved 85.94% on the F1-score on the Kinetics-GEBD test set,
which improved the F1-score by 2.31% compared to the winner of the 2021
Kinetics-GEBD Challenge. Our code is available at
https://github.com/ContentAndMaterialPortrait/MAE-GEBD.",1,1,0,0,1,0,0.964095,2.0,0.7963,13
73d3c6ac-131b-46a4-8dcb-63c9b9d01184,A Graph-Based Method for Soccer Action Spotting Using Unsupervised Player Classification,4,0.0574009,0.575975,"Action spotting in soccer videos is the task of identifying the specific time
when a certain key action of the game occurs. Lately, it has received a large
amount of attention and powerful methods have been introduced. Action spotting
involves understanding the dynamics of the game, the complexity of events, and
the variation of video sequences. Most approaches have focused on the latter,
given that their models exploit the global visual features of the sequences. In
this work, we focus on the former by (a) identifying and representing the
players, referees, and goalkeepers as nodes in a graph, and by (b) modeling
their temporal interactions as sequences of graphs. For the player
identification, or player classification task, we obtain an accuracy of 97.72%
in our annotated benchmark. For the action spotting task, our method obtains an
overall performance of 57.83% average-mAP by combining it with other
audiovisual modalities. This performance surpasses similar graph-based methods
and has competitive results with heavy computing methods. Code and data are
available at https://github.com/IPCV/soccer_action_spotting.",1,1,0,0,0,0,0.0499713,10.0,0.541978,49
4e74b9b6-1487-4d99-ba9f-4a1fc30dec14,An Effective Transformer-based Solution for RSNA Intracranial Hemorrhage Detection Competition,2,0.0601208,0.276716,"We present an effective method for Intracranial Hemorrhage Detection (IHD)
which exceeds the performance of the winner solution in RSNA-IHD competition
(2019). Meanwhile, our model only takes quarter parameters and ten percent
FLOPs compared to the winner's solution. The IHD task needs to predict the
hemorrhage category of each slice for the input brain CT. We review the top-5
solutions for the IHD competition held by the Radiological Society of North
America(RSNA) in 2019. Nearly all the top solutions rely on 2D convolutional
networks and sequential models (Bidirectional GRU or LSTM) to extract
intra-slice and inter-slice features, respectively. All the top solutions
enhance the performance by leveraging the model ensemble, and the model number
varies from 7 to 31. In the past years, since much progress has been made in
the computer vision regime especially Transformer-based models, we introduce
the Transformer-based techniques to extract the features in both intra-slice
and inter-slice views for IHD tasks. Additionally, a semi-supervised method is
embedded into our workflow to further improve the performance. The code is
available in the manuscript.",1,1,0,0,1,0,0.990721,10.0,0.993385,14
a0e22556-8296-4293-8c29-5c78e12f983b,4DenoiseNet: Adverse Weather Denoising from Adjacent Point Clouds,18,0.567509,0.902202,"Reliable point cloud data is essential for perception tasks \textit{e.g.} in
robotics and autonomous driving applications. Adverse weather causes a specific
type of noise to light detection and ranging (LiDAR) sensor data, which
degrades the quality of the point clouds significantly. To address this issue,
this letter presents a novel point cloud adverse weather denoising deep
learning algorithm (4DenoiseNet). Our algorithm takes advantage of the time
dimension unlike deep learning adverse weather denoising methods in the
literature. It performs about 10\% better in terms of intersection over union
metric compared to the previous work and is more computationally efficient.
These results are achieved on our novel SnowyKITTI dataset, which has over
40000 adverse weather annotated point clouds. Moreover, strong qualitative
results on the Canadian Adverse Driving Conditions dataset indicate good
generalizability to domain shifts and to different sensor intrinsics.",0,1,0,1,1,0,0.897999,6.0,0.869325,49
4f67c92f-0dc7-4d6c-8f22-d41729985569,Online Continual Learning for Embedded Devices,36,0.430173,0.956598,"Real-time on-device continual learning is needed for new applications such as
home robots, user personalization on smartphones, and augmented/virtual reality
headsets. However, this setting poses unique challenges: embedded devices have
limited memory and compute capacity and conventional machine learning models
suffer from catastrophic forgetting when updated on non-stationary data
streams. While several online continual learning models have been developed,
their effectiveness for embedded applications has not been rigorously studied.
In this paper, we first identify criteria that online continual learners must
meet to effectively perform real-time, on-device learning. We then study the
efficacy of several online continual learning methods when used with mobile
neural networks. We measure their performance, memory usage, compute
requirements, and ability to generalize to out-of-domain inputs.",0,1,0,0,0,0,0.872501,9.0,0.901455,96
27413369-d23c-4388-88b9-778568a98c24,On the Usefulness of the Fit-on-the-Test View on Evaluating Calibration of Classifiers,3,0.0200342,0.273109,"Every uncalibrated classifier has a corresponding true calibration map that
calibrates its confidence. Deviations of this idealistic map from the identity
map reveal miscalibration. Such calibration errors can be reduced with many
post-hoc calibration methods which fit some family of calibration maps on a
validation dataset. In contrast, evaluation of calibration with the expected
calibration error (ECE) on the test set does not explicitly involve fitting.
However, as we demonstrate, ECE can still be viewed as if fitting a family of
functions on the test data. This motivates the fit-on-the-test view on
evaluation: first, approximate a calibration map on the test data, and second,
quantify its distance from the identity. Exploiting this view allows us to
unlock missed opportunities: (1) use the plethora of post-hoc calibration
methods for evaluating calibration; (2) tune the number of bins in ECE with
cross-validation. Furthermore, we introduce: (3) benchmarking on pseudo-real
data where the true calibration map can be estimated very precisely; and (4)
novel calibration and evaluation methods using new calibration map families PL
and PL3.",1,0,0,0,0,0,0.23524,11.0,0.734047,34
4207356c-0a8a-47cf-999b-6cd6fe295915,CounTR: Transformer-based Generalised Visual Counting,28,0.278906,0.701063,"In this paper, we consider the problem of generalised visual object counting,
with the goal of developing a computational model for counting the number of
objects from arbitrary semantic categories, using arbitrary number of
""exemplars"", i.e. zero-shot or few-shot counting. To this end, we make the
following four contributions: (1) We introduce a novel transformer-based
architecture for generalised visual object counting, termed as Counting
Transformer (CounTR), which explicitly capture the similarity between image
patches or with given ""exemplars"" with the attention mechanism;(2) We adopt a
two-stage training regime, that first pre-trains the model with self-supervised
learning, and followed by supervised fine-tuning;(3) We propose a simple,
scalable pipeline for synthesizing training images with a large number of
instances or that from different semantic categories, explicitly forcing the
model to make use of the given ""exemplars"";(4) We conduct thorough ablation
studies on the large-scale counting benchmark, e.g. FSC-147, and demonstrate
state-of-the-art performance on both zero and few-shot settings.",1,1,1,0,1,0,0.516597,11.0,0.824691,30
61a4255d-6707-420a-a5b5-b2b93a3e57a3,Semantic features of object concepts generated with GPT-3,12,0.0314311,0.325566,"Semantic features have been playing a central role in investigating the
nature of our conceptual representations. Yet the enormous time and effort
required to empirically sample and norm features from human raters has
restricted their use to a limited set of manually curated concepts. Given
recent promising developments with transformer-based language models, here we
asked whether it was possible to use such models to automatically generate
meaningful lists of properties for arbitrary object concepts and whether these
models would produce features similar to those found in humans. To this end, we
probed a GPT-3 model to generate semantic features for 1,854 objects and
compared automatically-generated features to existing human feature norms.
GPT-3 generated many more features than humans, yet showed a similar
distribution in the types of generated features. Generated feature norms
rivaled human norms in predicting similarity, relatedness, and category
membership, while variance partitioning demonstrated that these predictions
were driven by similar variance in humans and GPT-3. Together, these results
highlight the potential of large language models to capture important facets of
human knowledge and yield a new approach for automatically generating
interpretable feature sets, thus drastically expanding the potential use of
semantic features in psychological and linguistic studies.",0,0,0,1,0,1,0.114212,8.0,0.535111,32
e9327491-01ad-4166-a6ce-35b6573aee0f,CD Tools -- Condensed Detachment and Structure Generating Theorem Proving (System Description),5,0.0349531,0.584615,"CD Tools is a Prolog library for experimenting with condensed detachment in
first-order ATP, which puts a recent formal view centered around proof
structures into practice. From the viewpoint of first-order ATP, condensed
detachment offers a setting that is relatively simple but with essential
features and serious applications, making it attractive as a basis for
developing and evaluating novel techniques. CD Tools includes specialized
provers based on the enumeration of proof structures. We focus here on one of
these, SGCD, which permits to blend goal- and axiom-driven proof search in
particularly flexible ways. In purely goal-driven configurations it acts
similarly to a prover of the clausal tableaux or connection method family. In
blended configurations its performance is much stronger, close to
state-of-the-art provers, while emitting relatively short proofs. Experiments
show characteristics and application possibilities of the structure generating
approach realized by that prover. For a historic problem often studied in ATP
it produced a new proof that is much shorter than any known one.",0,0,0,0,0,0,6.98663e-13,43.0,0.311649,48
b39236f3-4abb-4eb9-94fd-8ef07b40fdef,A Hazard Analysis Framework for Code Synthesis Large Language Models,19,0.110905,0.907124,"Codex, a large language model (LLM) trained on a variety of codebases,
exceeds the previous state of the art in its capacity to synthesize and
generate code. Although Codex provides a plethora of benefits, models that may
generate code on such scale have significant limitations, alignment problems,
the potential to be misused, and the possibility to increase the rate of
progress in technical fields that may themselves have destabilizing impacts or
have misuse potential. Yet such safety impacts are not yet known or remain to
be explored. In this paper, we outline a hazard analysis framework constructed
at OpenAI to uncover hazards or safety risks that the deployment of models like
Codex may impose technically, socially, politically, and economically. The
analysis is informed by a novel evaluation framework that determines the
capacity of advanced code generation techniques against the complexity and
expressivity of specification prompts, and their capability to understand and
execute them relative to human ability.",0,0,1,0,0,0,0.169432,8.0,0.58833,41
aea68549-0aab-420b-a137-d294253adc64,Patterns of near-crash events in a naturalistic driving dataset: applying rules mining,10,0.0435775,0.741204,"This study aims to explore the associations between near-crash events and
road geometry and trip features by investigating a naturalistic driving dataset
and a corresponding roadway inventory dataset using an association rule mining
method.",0,1,0,0,0,0,3.24886e-05,18.0,0.336442,45
050ad663-bf09-458f-8968-7bf9ecd11e60,Vision Transformers for Single Image Dehazing,194,0.971565,0.999996,"Image dehazing is a representative low-level vision task that estimates
latent haze-free images from hazy images. In recent years, convolutional neural
network-based methods have dominated image dehazing. However, vision
Transformers, which has recently made a breakthrough in high-level vision
tasks, has not brought new dimensions to image dehazing. We start with the
popular Swin Transformer and find that several of its key designs are
unsuitable for image dehazing. To this end, we propose DehazeFormer, which
consists of various improvements, such as the modified normalization layer,
activation function, and spatial information aggregation scheme. We train
multiple variants of DehazeFormer on various datasets to demonstrate its
effectiveness. Specifically, on the most frequently used SOTS indoor set, our
small model outperforms FFA-Net with only 25% #Param and 5% computational cost.
To the best of our knowledge, our large model is the first method with the PSNR
over 40 dB on the SOTS indoor set, dramatically outperforming the previous
state-of-the-art methods. We also collect a large-scale realistic remote
sensing dehazing dataset for evaluating the method's capability to remove
highly non-homogeneous haze.",1,1,0,0,1,0,0.934751,5.0,0.87894,94
1adf3521-b001-4b55-9a3f-52a571dadd36,StretchBEV: Stretching Future Instance Prediction Spatially and Temporally,35,0.609296,0.709014,"In self-driving, predicting future in terms of location and motion of all the
agents around the vehicle is a crucial requirement for planning. Recently, a
new joint formulation of perception and prediction has emerged by fusing rich
sensory information perceived from multiple cameras into a compact bird's-eye
view representation to perform prediction. However, the quality of future
predictions degrades over time while extending to longer time horizons due to
multiple plausible predictions. In this work, we address this inherent
uncertainty in future predictions with a stochastic temporal model. Our model
learns temporal dynamics in a latent space through stochastic residual updates
at each time step. By sampling from a learned distribution at each time step,
we obtain more diverse future predictions that are also more accurate compared
to previous work, especially stretching both spatially further regions in the
scene and temporally over longer time horizons. Despite separate processing of
each time step, our model is still efficient through decoupling of the learning
of dynamics and the generation of future predictions.",1,0,0,0,0,0,0.937963,5.0,0.882604,41
c8faed34-2d3e-410e-b5a5-f98dce038377,Spatio-Temporal Deformable Attention Network for Video Deblurring,14,0.122269,0.295757,"The key success factor of the video deblurring methods is to compensate for
the blurry pixels of the mid-frame with the sharp pixels of the adjacent video
frames. Therefore, mainstream methods align the adjacent frames based on the
estimated optical flows and fuse the alignment frames for restoration. However,
these methods sometimes generate unsatisfactory results because they rarely
consider the blur levels of pixels, which may introduce blurry pixels from
video frames. Actually, not all the pixels in the video frames are sharp and
beneficial for deblurring. To address this problem, we propose the
spatio-temporal deformable attention network (STDANet) for video delurring,
which extracts the information of sharp pixels by considering the pixel-wise
blur levels of the video frames. Specifically, STDANet is an encoder-decoder
network combined with the motion estimator and spatio-temporal deformable
attention (STDA) module, where motion estimator predicts coarse optical flows
that are used as base offsets to find the corresponding sharp pixels in STDA
module. Experimental results indicate that the proposed STDANet performs
favorably against state-of-the-art methods on the GoPro, DVD, and BSD datasets.",1,1,0,0,1,0,0.114454,15.0,0.752209,45
1aa4200a-c11b-4f96-b425-bec47bce056e,InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds,42,0.567632,0.884219,"In this paper, we take a significant step towards real-world applicability of
monocular neural avatar reconstruction by contributing InstantAvatar, a system
that can reconstruct human avatars from a monocular video within seconds, and
these avatars can be animated and rendered at an interactive rate. To achieve
this efficiency we propose a carefully designed and engineered system, that
leverages emerging acceleration structures for neural fields, in combination
with an efficient empty space-skipping strategy for dynamic scenes. We also
contribute an efficient implementation that we will make available for research
purposes. Compared to existing methods, InstantAvatar converges 130x faster and
can be trained in minutes instead of hours. It achieves comparable or even
better reconstruction quality and novel pose synthesis results. When given the
same time budget, our method significantly outperforms SoTA methods.
InstantAvatar can yield acceptable visual quality in as little as 10 seconds
training time.",0,1,0,0,1,0,0.928222,4.0,0.839784,80
51900533-4610-4eef-a421-c9e5a900981b,Boosting human decision-making with AI-generated decision aids,6,0.101384,0.285486,"Human decision-making is plagued by many systematic errors. Many of these
errors can be avoided by providing decision aids that guide decision-makers to
attend to the important information and integrate it according to a rational
decision strategy. Designing such decision aids used to be a tedious manual
process. Advances in cognitive science might make it possible to automate this
process in the future. We recently introduced machine learning methods for
discovering optimal strategies for human decision-making automatically and an
automatic method for explaining those strategies to people. Decision aids
constructed by this method were able to improve human decision-making. However,
following the descriptions generated by this method is very tedious. We
hypothesized that this problem can be overcome by conveying the automatically
discovered decision strategy as a series of natural language instructions for
how to reach a decision. Experiment 1 showed that people do indeed understand
such procedural instructions more easily than the decision aids generated by
our previous method. Encouraged by this finding, we developed an algorithm for
translating the output of our previous method into procedural instructions. We
applied the improved method to automatically generate decision aids for a
naturalistic planning task (i.e., planning a road trip) and a naturalistic
decision task (i.e., choosing a mortgage). Experiment 2 showed that these
automatically generated decision-aids significantly improved people's
performance in planning a road trip and choosing a mortgage. These findings
suggest that AI-powered boosting might have potential for improving human
decision-making in the real world.",1,1,0,0,0,0,0.138509,9.0,0.609707,54
3a02a4b7-2409-44cc-993d-27adb1a0c35d,A Flexible Diffusion Model,6,0.0170583,0.0625904,"Diffusion (score-based) generative models have been widely used for modeling
various types of complex data, including images, audios, and point clouds.
Recently, the deep connection between forward-backward stochastic differential
equations (SDEs) and diffusion-based models has been revealed, and several new
variants of SDEs are proposed (e.g., sub-VP, critically-damped Langevin) along
this line. Despite the empirical success of the hand-crafted fixed forward
SDEs, a great quantity of proper forward SDEs remain unexplored. In this work,
we propose a general framework for parameterizing the diffusion model,
especially the spatial part of the forward SDE. An abstract formalism is
introduced with theoretical guarantees, and its connection with previous
diffusion models is leveraged. We demonstrate the theoretical advantage of our
method from an optimization perspective. Numerical experiments on synthetic
datasets, MINIST and CIFAR10 are also presented to validate the effectiveness
of our framework.",0,0,0,0,0,0,0.517709,5.0,0.614953,66
bfc2a81f-e675-4f1c-9d34-4cddcb0ac2aa,Concept Bottleneck Model with Additional Unsupervised Concepts,34,0.663698,0.645908,"With the increasing demands for accountability, interpretability is becoming
an essential capability for real-world AI applications. However, most methods
utilize post-hoc approaches rather than training the interpretable model. In
this article, we propose a novel interpretable model based on the concept
bottleneck model (CBM). CBM uses concept labels to train an intermediate layer
as the additional visible layer. However, because the number of concept labels
restricts the dimension of this layer, it is difficult to obtain high accuracy
with a small number of labels. To address this issue, we integrate supervised
concepts with unsupervised ones trained with self-explaining neural networks
(SENNs). By seamlessly training these two types of concepts while reducing the
amount of computation, we can obtain both supervised and unsupervised concepts
simultaneously, even for large-sized images. We refer to the proposed model as
the concept bottleneck model with additional unsupervised concepts (CBM-AUC).
We experimentally confirmed that the proposed model outperformed CBM and SENN.
We also visualized the saliency map of each concept and confirmed that it was
consistent with the semantic meanings.",1,1,0,0,1,0,0.854779,7.0,0.863975,46
27da1135-bc40-4800-83ce-9de5b70834ca,Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks,3,0.0670715,0.133459,"The last few years have witnessed an exponential rise in the propagation of
offensive text on social media. Identification of this text with high precision
is crucial for the well-being of society. Most of the existing approaches tend
to give high toxicity scores to innocuous statements (e.g., ""I am a gay man"").
These false positives result from over-generalization on the training data
where specific terms in the statement may have been used in a pejorative sense
(e.g., ""gay""). Emphasis on such words alone can lead to discrimination against
the classes these systems are designed to protect. In this paper, we address
the problem of offensive language detection on Twitter, while also detecting
the type and the target of the offence. We propose a novel approach called
SyLSTM, which integrates syntactic features in the form of the dependency parse
tree of a sentence and semantic features in the form of word embeddings into a
deep learning architecture using a Graph Convolutional Network. Results show
that the proposed approach significantly outperforms the state-of-the-art BERT
model with orders of magnitude fewer number of parameters.",0,1,0,0,1,0,0.989826,11.0,0.992181,34
9d44829e-72ad-45d7-ba1b-c399f4ba0fb9,Unsupervised Extractive Opinion Summarization Using Sparse Coding,15,0.117322,0.884775,"Opinion summarization is the task of automatically generating summaries that
encapsulate information from multiple user reviews. We present Semantic
Autoencoder (SemAE) to perform extractive opinion summarization in an
unsupervised manner. SemAE uses dictionary learning to implicitly capture
semantic information from the review and learns a latent representation of each
sentence over semantic units. A semantic unit is supposed to capture an
abstract semantic concept. Our extractive summarization algorithm leverages the
representations to identify representative opinions among hundreds of reviews.
SemAE is also able to perform controllable summarization to generate
aspect-specific summaries. We report strong performance on SPACE and AMAZON
datasets, and perform experiments to investigate the functioning of our model.
Our code is publicly available at https://github.com/brcsomnath/SemAE.",1,0,0,0,0,0,0.299977,9.0,0.706616,54
2f0ca11c-0740-4fee-82de-548bf35c7129,Learning to Revise References for Faithful Summarization,25,0.128292,0.708969,"In real-world scenarios with naturally occurring datasets, reference
summaries are noisy and may contain information that cannot be inferred from
the source text. On large news corpora, removing low quality samples has been
shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora,
filtering is detrimental to performance. To improve reference quality while
retaining all data, we propose a new approach: to selectively re-write
unsupported reference sentences to better reflect source data. We automatically
generate a synthetic dataset of positive and negative revisions by corrupting
supported sentences and learn to revise reference sentences with contrastive
learning. The intensity of revisions is treated as a controllable attribute so
that, at inference, diverse candidates can be over-generated-then-rescored to
balance faithfulness and abstraction. To test our methods, we extract noisy
references from publicly available MIMIC-III discharge summaries for the task
of hospital-course summarization, and vary the data on which models are
trained. According to metrics and human evaluation, models trained on revised
clinical references are much more faithful, informative, and fluent than models
trained on original or filtered data.",0,1,0,0,0,0,0.4665,4.0,0.481455,88
5a374ec8-f171-4cd0-b5b6-0bdf76ac02dd,UniCLIP: Unified Framework for Contrastive Language-Image Pre-training,21,0.184254,0.600511,"Pre-training vision-language models with contrastive objectives has shown
promising results that are both scalable to large uncurated datasets and
transferable to many downstream applications. Some following works have
targeted to improve data efficiency by adding self-supervision terms, but
inter-domain (image-text) contrastive loss and intra-domain (image-image)
contrastive loss are defined on individual spaces in those works, so many
feasible combinations of supervision are overlooked. To overcome this issue, we
propose UniCLIP, a Unified framework for Contrastive Language-Image
Pre-training. UniCLIP integrates the contrastive loss of both inter-domain
pairs and intra-domain pairs into a single universal space. The discrepancies
that occur when integrating contrastive loss between different domains are
resolved by the three key components of UniCLIP: (1) augmentation-aware feature
embedding, (2) MP-NCE loss, and (3) domain dependent similarity measure.
UniCLIP outperforms previous vision-language pre-training methods on various
single- and multi-modality downstream tasks. In our experiments, we show that
each component that comprises UniCLIP contributes well to the final
performance.",1,1,0,0,1,0,0.963421,11.0,0.962454,49
59517874-9fe4-495c-9fe8-68ba35d4483c,CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction,16,0.144714,0.269478,"Vision transformer (ViT) has achieved competitive accuracy on a variety of
computer vision applications, but its computational cost impedes the deployment
on resource-limited mobile devices.
  We explore the sparsity in ViT and observe that informative patches and heads
are sufficient for accurate image recognition.
  In this paper, we propose a cascade pruning framework named CP-ViT by
predicting sparsity in ViT models progressively and dynamically to reduce
computational redundancy while minimizing the accuracy loss. Specifically, we
define the cumulative score to reserve the informative patches and heads across
the ViT model for better accuracy. We also propose the dynamic pruning ratio
adjustment technique based on layer-aware attention range. CP-ViT has great
general applicability for practical deployment, which can be applied to a wide
range of ViT models and can achieve superior accuracy with or without
fine-tuning.
  Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various
pre-trained models have demonstrated the effectiveness and efficiency of
CP-ViT. By progressively pruning 50\% patches, our CP-ViT method reduces over
40\% FLOPs while maintaining accuracy loss within 1\%.",0,1,0,0,0,0,0.862758,4.0,0.769172,29
6717dbca-9ae9-451d-9087-a92a7ad0dca1,Cooperative Distribution Alignment via JSD Upper Bound,1,0.0153307,0.0811948,"Unsupervised distribution alignment estimates a transformation that maps two
or more source distributions to a shared aligned distribution given only
samples from each distribution. This task has many applications including
generative modeling, unsupervised domain adaptation, and socially aware
learning. Most prior works use adversarial learning (i.e., min-max
optimization), which can be challenging to optimize and evaluate. A few recent
works explore non-adversarial flow-based (i.e., invertible) approaches, but
they lack a unified perspective and are limited in efficiently aligning
multiple distributions. Therefore, we propose to unify and generalize previous
flow-based approaches under a single non-adversarial framework, which we prove
is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence
(JSD). Importantly, our problem reduces to a min-min, i.e., cooperative,
problem and can provide a natural evaluation metric for unsupervised
distribution alignment. We show empirical results on both simulated and
real-world datasets to demonstrate the benefits of our approach. Code is
available at https://github.com/inouye-lab/alignment-upper-bound.",1,0,0,0,0,1,0.558447,9.0,0.798778,38
a5d40fff-c5de-4987-9141-0dfdcd80ffaa,FullSubNet+: Channel Attention FullSubNet with Complex Spectrograms for Speech Enhancement,61,0.355248,0.992897,"Previously proposed FullSubNet has achieved outstanding performance in Deep
Noise Suppression (DNS) Challenge and attracted much attention. However, it
still encounters issues such as input-output mismatch and coarse processing for
frequency bands. In this paper, we propose an extended single-channel real-time
speech enhancement framework called FullSubNet+ with following significant
improvements. First, we design a lightweight multi-scale time sensitive channel
attention (MulCA) module which adopts multi-scale convolution and channel
attention mechanism to help the network focus on more discriminative frequency
bands for noise reduction. Then, to make full use of the phase information in
noisy speech, our model takes all the magnitude, real and imaginary
spectrograms as inputs. Moreover, by replacing the long short-term memory
(LSTM) layers in original full-band model with stacked temporal convolutional
network (TCN) blocks, we design a more efficient full-band module called
full-band extractor. The experimental results in DNS Challenge dataset show the
superior performance of our FullSubNet+, which reaches the state-of-the-art
(SOTA) performance and outperforms other existing speech enhancement
approaches.",1,1,0,0,1,0,0.404589,8.0,0.716719,27
e346ed93-81b0-4ac8-b847-889c89e10db4,Cardinality-Regularized Hawkes-Granger Model,9,0.188291,0.89356,"We propose a new sparse Granger-causal learning framework for temporal event
data. We focus on a specific class of point processes called the Hawkes
process. We begin by pointing out that most of the existing sparse causal
learning algorithms for the Hawkes process suffer from a singularity in maximum
likelihood estimation. As a result, their sparse solutions can appear only as
numerical artifacts. In this paper, we propose a mathematically well-defined
sparse causal learning framework based on a cardinality-regularized Hawkes
process, which remedies the pathological issues of existing approaches. We
leverage the proposed algorithm for the task of instance-wise causal event
analysis, where sparsity plays a critical role. We validate the proposed
framework with two real use-cases, one from the power grid and the other from
the cloud data center management domain.",0,0,0,0,0,0,0.0841381,13.0,0.689145,53
510338b9-49fb-498e-856a-8e9eb073fc9a,Probing Speech Emotion Recognition Transformers for Linguistic Knowledge,19,0.0788295,0.820586,"Large, pre-trained neural networks consisting of self-attention layers
(transformers) have recently achieved state-of-the-art results on several
speech emotion recognition (SER) datasets. These models are typically
pre-trained in self-supervised manner with the goal to improve automatic speech
recognition performance -- and thus, to understand linguistic information. In
this work, we investigate the extent in which this information is exploited
during SER fine-tuning. Using a reproducible methodology based on open-source
tools, we synthesise prosodically neutral speech utterances while varying the
sentiment of the text. Valence predictions of the transformer model are very
reactive to positive and negative sentiment content, as well as negations, but
not to intensifiers or reducers, while none of those linguistic features impact
arousal or dominance. These findings show that transformers can successfully
leverage linguistic information to improve their valence predictions, and that
linguistic analysis should be included in their testing.",0,0,0,0,0,0,0.586535,3.0,0.422113,33
7743ba21-06af-43c5-857a-cba25ca8d12c,Semantic Decomposition Improves Learning of Large Language Models on EHR Data,1,0.00921041,0.0332914,"Electronic health records (EHR) are widely believed to hold a profusion of
actionable insights, encrypted in an irregular, semi-structured format, amidst
a loud noise background. To simplify learning patterns of health and disease,
medical codes in EHR can be decomposed into semantic units connected by
hierarchical graphs. Building on earlier synergy between Bidirectional Encoder
Representations from Transformers (BERT) and Graph Attention Networks (GAT), we
present H-BERT, which ingests complete graph tree expansions of hierarchical
medical codes as opposed to only ingesting the leaves and pushes patient-level
labels down to each visit. This methodology significantly improves prediction
of patient membership in over 500 medical diagnosis classes as measured by
aggregated AUC and APS, and creates distinct representations of patients in
closely related but clinically distinct phenotypes.",0,1,0,0,0,0,0.638717,13.0,0.877576,16
5e02f779-268b-47e9-b0dd-83c1a705de83,Two-stage LLM Fine-tuning with Less Specialization and More Generalization,9,0.265866,0.16829,"Pretrained large language models (LLMs) are general purpose problem solvers
applicable to a diverse set of tasks with prompts. They can be further improved
towards a specific task by fine-tuning on a specialized dataset. However,
fine-tuning usually makes the model narrowly specialized on this dataset with
reduced general in-context learning performances, which is undesirable whenever
the fine-tuned model needs to handle additional tasks where no fine-tuning data
is available. In this work, we first demonstrate that fine-tuning on a single
task indeed decreases LLMs' general in-context learning performance. We
discover one important cause of such forgetting, format specialization, where
the model overfits to the format of the fine-tuned task.We further show that
format specialization happens at the very beginning of fine-tuning. To solve
this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet
effective two-stage fine-tuning framework that reduces format specialization
and improves generalization.ProMoT offloads task-specific format learning into
additional and removable parameters by first doing prompt tuning and then
fine-tuning the model itself with this soft prompt attached. With experiments
on several fine-tuning tasks and 8 in-context evaluation tasks, we show that
ProMoT achieves comparable performance on fine-tuned tasks to standard
fine-tuning, but with much less loss of in-context learning performances across
a board range of out-of-domain evaluation tasks. More importantly, ProMoT can
even enhance generalization on in-context learning tasks that are semantically
related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly
improves performance on other language pairs, and ProMoT on NLI improves
performance on summarization. Experiments also show that ProMoT can improve the
generalization performance of multi-task training.",0,1,0,0,0,0,0.983639,5.0,0.960935,46
d01e3fdc-df92-47e9-aad3-8faeeaaecaa7,Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text,30,0.0757056,0.759209,"As text generated by large language models proliferates, it becomes vital to
understand how humans engage with such text, and whether or not they are able
to detect when the text they are reading did not originate with a human writer.
Prior work on human detection of generated text focuses on the case where an
entire passage is either human-written or machine-generated. In this paper, we
study a more realistic setting where text begins as human-written and
transitions to being generated by state-of-the-art neural language models. We
show that, while annotators often struggle at this task, there is substantial
variance in annotator skill and that given proper incentives, annotators can
improve at this task over time. Furthermore, we conduct a detailed comparison
study and analyze how a variety of variables (model size, decoding strategy,
fine-tuning, prompt genre, etc.) affect human detection performance. Finally,
we collect error annotations from our participants and use them to show that
certain textual genres influence models to make different types of errors and
that certain sentence-level features correlate highly with annotator selection.
We release the RoFT dataset: a collection of over 21,000 human annotations
paired with error classifications to encourage future work in human detection
and evaluation of generated text.",0,1,0,1,0,0,0.303814,6.0,0.562472,26
651956c7-cc4b-4ffc-85a5-ea264dfa4f6f,Watching the News: Towards VideoQA Models that can Read,9,0.0634327,0.517116,"Video Question Answering methods focus on commonsense reasoning and visual
cognition of objects or persons and their interactions over time. Current
VideoQA approaches ignore the textual information present in the video.
Instead, we argue that textual information is complementary to the action and
provides essential contextualisation cues to the reasoning process. To this
end, we propose a novel VideoQA task that requires reading and understanding
the text in the video. To explore this direction, we focus on news videos and
require QA systems to comprehend and answer questions about the topics
presented by combining visual and textual cues in the video. We introduce the
``NewsVideoQA'' dataset that comprises more than $8,600$ QA pairs on $3,000+$
news videos obtained from diverse news channels from around the world. We
demonstrate the limitations of current Scene Text VQA and VideoQA methods and
propose ways to incorporate scene text information into VideoQA methods.",0,0,1,1,0,0,0.488899,9.0,0.776873,41
46517c33-f805-4d6b-8a30-4621fd159916,Restoring Vision in Hazy Weather with Hierarchical Contrastive Learning,10,0.179454,0.67158,"Image restoration under hazy weather condition, which is called single image
dehazing, has been of significant interest for various computer vision
applications. In recent years, deep learning-based methods have achieved
success. However, existing image dehazing methods typically neglect the
hierarchy of features in the neural network and fail to exploit their
relationships fully. To this end, we propose an effective image dehazing method
named Hierarchical Contrastive Dehazing (HCD), which is based on feature fusion
and contrastive learning strategies. HCD consists of a hierarchical dehazing
network (HDN) and a novel hierarchical contrastive loss (HCL). Specifically,
the core design in the HDN is a hierarchical interaction module, which utilizes
multi-scale activation to revise the feature responses hierarchically. To
cooperate with the training of HDN, we propose HCL which performs contrastive
learning on hierarchically paired exemplars, facilitating haze removal.
Extensive experiments on public datasets, RESIDE, HazeRD, and DENSE-HAZE,
demonstrate that HCD quantitatively outperforms the state-of-the-art methods in
terms of PSNR, SSIM and achieves better visual quality.",0,1,0,0,1,0,0.415137,10.0,0.776765,72
9790e377-bf8b-4b9b-acce-da883a783586,A Human Rights-Based Approach to Responsible AI,22,0.429209,0.63646,"Research on fairness, accountability, transparency and ethics of AI-based
interventions in society has gained much-needed momentum in recent years.
However it lacks an explicit alignment with a set of normative values and
principles that guide this research and interventions. Rather, an implicit
consensus is often assumed to hold for the values we impart into our models -
something that is at odds with the pluralistic world we live in. In this paper,
we put forth the doctrine of universal human rights as a set of globally
salient and cross-culturally recognized set of values that can serve as a
grounding framework for explicit value alignment in responsible AI - and
discuss its efficacy as a framework for civil society partnership and
participation. We argue that a human rights framework orients the research in
this space away from the machines and the risks of their biases, and towards
humans and the risks to their rights, essentially helping to center the
conversation around who is harmed, what harms they face, and how those harms
may be mitigated.",0,0,0,0,0,1,0.58533,7.0,0.751862,83
82f203d1-dd19-4113-84dd-0b9d5fbb69b7,A Simple Temporal Information Matching Mechanism for Entity Alignment Between Temporal Knowledge Graphs,8,0.159907,0.420242,"Entity alignment (EA) aims to find entities in different knowledge graphs
(KGs) that refer to the same object in the real world. Recent studies
incorporate temporal information to augment the representations of KGs. The
existing methods for EA between temporal KGs (TKGs) utilize a time-aware
attention mechanism to incorporate relational and temporal information into
entity embeddings. The approaches outperform the previous methods by using
temporal information. However, we believe that it is not necessary to learn the
embeddings of temporal information in KGs since most TKGs have uniform temporal
representations. Therefore, we propose a simple graph neural network (GNN)
model combined with a temporal information matching mechanism, which achieves
better performance with less time and fewer parameters. Furthermore, since
alignment seeds are difficult to label in real-world applications, we also
propose a method to generate unsupervised alignment seeds via the temporal
information of TKG. Extensive experiments on public datasets indicate that our
supervised method significantly outperforms the previous methods and the
unsupervised one has competitive performance.",0,1,0,0,0,0,0.701663,6.0,0.763466,28
b180d468-796a-4173-90fd-7bbc87476624,Unbiased Knowledge Distillation for Recommendation,7,0.0763265,0.478845,"As a promising solution for model compression, knowledge distillation (KD)
has been applied in recommender systems (RS) to reduce inference latency.
Traditional solutions first train a full teacher model from the training data,
and then transfer its knowledge (\ie \textit{soft labels}) to supervise the
learning of a compact student model. However, we find such a standard
distillation paradigm would incur serious bias issue -- popular items are more
heavily recommended after the distillation. This effect prevents the student
model from making accurate and fair recommendations, decreasing the
effectiveness of RS.
  In this work, we identify the origin of the bias in KD -- it roots in the
biased soft labels from the teacher, and is further propagated and intensified
during the distillation. To rectify this, we propose a new KD method with a
stratified distillation strategy. It first partitions items into multiple
groups according to their popularity, and then extracts the ranking knowledge
within each group to supervise the learning of the student. Our method is
simple and teacher-agnostic -- it works on distillation stage without affecting
the training of the teacher model. We conduct extensive theoretical and
empirical studies to validate the effectiveness of our proposal. We release our
code at: https://github.com/chengang95/UnKD.",1,0,0,0,0,0,0.459249,6.0,0.650683,47
f97fe304-7fcf-4ba6-9c6a-fefa4519f6aa,"Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",21,0.249655,0.719545,"The size of pretrained models is increasing, and so is their performance on a
variety of NLP tasks. However, as their memorization capacity grows, they might
pick up more social biases. In this work, we examine the connection between
model size and its gender bias (specifically, occupational gender bias). We
measure bias in three masked language model families (RoBERTa, DeBERTa, and T5)
in two setups: directly using prompt based method, and using a downstream task
(Winogender). We find on the one hand that larger models receive higher bias
scores on the former task, but when evaluated on the latter, they make fewer
gender errors. To examine these potentially conflicting results, we carefully
investigate the behavior of the different models on Winogender. We find that
while larger models outperform smaller ones, the probability that their
mistakes are caused by gender bias is higher. Moreover, we find that the
proportion of stereotypical errors compared to anti-stereotypical ones grows
with the model size. Our findings highlight the potential risks that can arise
from increasing model size.",1,0,0,0,0,0,0.934033,7.0,0.912954,37
746769e9-8317-47a6-aaa9-fc85e1d53fc0,Scalar is Not Enough: Vectorization-based Unbiased Learning to Rank,5,0.0275466,0.363983,"Unbiased learning to rank (ULTR) aims to train an unbiased ranking model from
biased user click logs. Most of the current ULTR methods are based on the
examination hypothesis (EH), which assumes that the click probability can be
factorized into two scalar functions, one related to ranking features and the
other related to bias factors. Unfortunately, the interactions among features,
bias factors and clicks are complicated in practice, and usually cannot be
factorized in this independent way. Fitting click data with EH could lead to
model misspecification and bring the approximation error.
  In this paper, we propose a vector-based EH and formulate the click
probability as a dot product of two vector functions. This solution is complete
due to its universality in fitting arbitrary click functions. Based on it, we
propose a novel model named Vectorization to adaptively learn the relevance
embeddings and sort documents by projecting embeddings onto a base vector.
Extensive experiments show that our method significantly outperforms the
state-of-the-art ULTR methods on complex real clicks as well as simple
simulated clicks.",1,0,0,0,1,0,0.01273,11.0,0.457561,48
2d464100-412f-4358-b351-dd8586c8174f,Natural Language Inference Prompts for Zero-shot Emotion Classification in Text across Corpora,17,0.0862005,0.316883,"Within textual emotion classification, the set of relevant labels depends on
the domain and application scenario and might not be known at the time of model
development. This conflicts with the classical paradigm of supervised learning
in which the labels need to be predefined. A solution to obtain a model with a
flexible set of labels is to use the paradigm of zero-shot learning as a
natural language inference task, which in addition adds the advantage of not
needing any labeled training data. This raises the question how to prompt a
natural language inference model for zero-shot learning emotion classification.
Options for prompt formulations include the emotion name anger alone or the
statement ""This text expresses anger"". With this paper, we analyze how
sensitive a natural language inference-based zero-shot-learning classifier is
to such changes to the prompt under consideration of the corpus: How carefully
does the prompt need to be selected? We perform experiments on an established
set of emotion datasets presenting different language registers according to
different sources (tweets, events, blogs) with three natural language inference
models and show that indeed the choice of a particular prompt formulation needs
to fit to the corpus. We show that this challenge can be tackled with
combinations of multiple prompts. Such ensemble is more robust across corpora
than individual prompts and shows nearly the same performance as the individual
best prompt for a particular corpus.",1,1,0,0,0,0,0.132631,9.0,0.60452,53
23526894-5c3f-4637-ace1-8e6938ee8f32,Reinforced Lin-Kernighan-Helsgaun Algorithms for the Traveling Salesman Problems,12,0.0233622,0.25658,"TSP is a classical NP-hard combinatorial optimization problem with many
practical variants. LKH is one of the state-of-the-art local search algorithms
for the TSP. LKH-3 is a powerful extension of LKH that can solve many TSP
variants. Both LKH and LKH-3 associate a candidate set to each city to improve
the efficiency, and have two different methods, $\alpha$-measure and POPMUSIC,
to decide the candidate sets. In this work, we first propose a Variable
Strategy Reinforced LKH (VSR-LKH) algorithm, which incorporates three
reinforcement learning methods (Q-learning, Sarsa, Monte Carlo) with LKH, for
the TSP. We further propose a new algorithm called VSR-LKH-3 that combines the
variable strategy reinforcement learning method with LKH-3 for typical TSP
variants, including the TSP with time windows (TSPTW) and Colored TSP (CTSP).
The proposed algorithms replace the inflexible traversal operations in LKH and
LKH-3 and let the algorithms learn to make a choice at each search step by
reinforcement learning. Both LKH and LKH-3, with either $\alpha$-measure or
POPMUSIC, can be significantly improved by our methods. Extensive experiments
on 236 widely-used TSP benchmarks with up to 85,900 cities demonstrate the
excellent performance of VSR-LKH. VSR-LKH-3 also significantly outperforms the
state-of-the-art heuristics for TSPTW and CTSP.",1,1,0,0,1,0,0.00625757,9.0,0.257748,57
f85b2dd8-4d90-4980-b6c9-7819ef80d57b,On the Effect of Information Asymmetry in Human-AI Teams,16,0.322209,0.518506,"Over the last years, the rising capabilities of artificial intelligence (AI)
have improved human decision-making in many application areas. Teaming between
AI and humans may even lead to complementary team performance (CTP), i.e., a
level of performance beyond the ones that can be reached by AI or humans
individually. Many researchers have proposed using explainable AI (XAI) to
enable humans to rely on AI advice appropriately and thereby reach CTP.
However, CTP is rarely demonstrated in previous work as often the focus is on
the design of explainability, while a fundamental prerequisite -- the presence
of complementarity potential between humans and AI -- is often neglected.
Therefore, we focus on the existence of this potential for effective human-AI
decision-making. Specifically, we identify information asymmetry as an
essential source of complementarity potential, as in many real-world
situations, humans have access to different contextual information. By
conducting an online experiment, we demonstrate that humans can use such
contextual information to adjust the AI's decision, finally resulting in CTP.",0,0,0,0,0,0,0.931225,4.0,0.843807,31
4d25e921-ccbc-4f83-af8e-75e8f1960a95,Learning Discriminative Representations and Decision Boundaries for Open Intent Detection,9,0.102875,0.328646,"Open intent detection is a significant problem in natural language
understanding, which aims to identify the unseen open intent while ensuring
known intent identification performance. However, current methods face two
major challenges. Firstly, they struggle to learn friendly representations to
detect the open intent with prior knowledge of only known intents. Secondly,
there is a lack of an effective approach to obtaining specific and compact
decision boundaries for known intents. To address these issues, this paper
presents an original framework called DA-ADB, which successively learns
distance-aware intent representations and adaptive decision boundaries for open
intent detection. Specifically, we first leverage distance information to
enhance the distinguishing capability of the intent representations. Then, we
design a novel loss function to obtain appropriate decision boundaries by
balancing both empirical and open space risks. Extensive experiments
demonstrate the effectiveness of the proposed distance-aware and boundary
learning strategies. Compared to state-of-the-art methods, our framework
achieves substantial improvements on three benchmark datasets. Furthermore, it
yields robust performance with varying proportions of labeled data and known
categories.",1,1,0,0,1,0,0.51585,8.0,0.758684,61
a67cdc68-ef5c-4f3c-8d17-ed2bc2447d2f,Instance-Specific Feature Propagation for Referring Segmentation,31,0.438507,0.486951,"Referring segmentation aims to generate a segmentation mask for the target
instance indicated by a natural language expression. There are typically two
kinds of existing methods: one-stage methods that directly perform segmentation
on the fused vision and language features; and two-stage methods that first
utilize an instance segmentation model for instance proposal and then select
one of these instances via matching them with language features. In this work,
we propose a novel framework that simultaneously detects the target-of-interest
via feature propagation and generates a fine-grained segmentation mask. In our
framework, each instance is represented by an Instance-Specific Feature (ISF),
and the target-of-referring is identified by exchanging information among all
ISFs using our proposed Feature Propagation Module (FPM). Our instance-aware
approach learns the relationship among all objects, which helps to better
locate the target-of-interest than one-stage methods. Comparing to two-stage
methods, our approach collaboratively and interactively utilizes both vision
and language information for synchronous identification and segmentation. In
the experimental tests, our method outperforms previous state-of-the-art
methods on all three RefCOCO series datasets.",0,0,0,0,1,0,0.691364,7.0,0.793191,54
e0cbec20-d261-4ada-9507-8841ae121d99,News Headlines Dataset For Sarcasm Detection,15,0.382916,0.244725,"Past studies in Sarcasm Detection mostly make use of Twitter datasets
collected using hashtag-based supervision but such datasets are noisy in terms
of labels and language. Furthermore, many tweets are replies to other tweets,
and detecting sarcasm in these requires the availability of contextual tweets.
To overcome the limitations related to noise in Twitter datasets, we curate
News Headlines Dataset from two news websites: TheOnion aims at producing
sarcastic versions of current events, whereas HuffPost publishes real news. The
dataset contains about 28K headlines out of which 13K are sarcastic. To make it
more useful, we have included the source links of the news articles so that
more data can be extracted as needed. In this paper, we describe various
details about the dataset and potential use cases apart from Sarcasm Detection.",0,1,1,1,0,0,0.943967,14.0,0.960642,4
6ec2a947-5e21-4a01-9c4b-c81179736ed0,Power Grid Congestion Management via Topology Optimization with AlphaZero,8,0.149221,0.902478,"The energy sector is facing rapid changes in the transition towards clean
renewable sources. However, the growing share of volatile, fluctuating
renewable generation such as wind or solar energy has already led to an
increase in power grid congestion and network security concerns. Grid operators
mitigate these by modifying either generation or demand (redispatching,
curtailment, flexible loads). Unfortunately, redispatching of fossil generators
leads to excessive grid operation costs and higher emissions, which is in
direct opposition to the decarbonization of the energy sector. In this paper,
we propose an AlphaZero-based grid topology optimization agent as a non-costly,
carbon-free congestion management alternative. Our experimental evaluation
confirms the potential of topology optimization for power grid operation,
achieves a reduction of the average amount of required redispatching by 60%,
and shows the interoperability with traditional congestion management methods.
Our approach also ranked 1st in the WCCI 2022 Learning to Run a Power Network
(L2RPN) competition. Based on our findings, we identify and discuss open
research problems as well as technical challenges for a productive system on a
real power grid.",1,1,0,0,1,0,0.0976472,10.0,0.61151,30
90f18f7b-ddd6-49c6-ab77-1b196cc0e213,CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI Models,2,0.0340903,0.174673,"As the practicality of Artificial Intelligence (AI) and Machine Learning (ML)
based techniques grow, there is an ever increasing threat of adversarial
attacks. There is a need to red team this ecosystem to identify system
vulnerabilities, potential threats, characterize properties that will enhance
system robustness, and encourage the creation of effective defenses. A
secondary need is to share this AI security threat intelligence between
different stakeholders like, model developers, users, and AI/ML security
professionals. In this paper, we create and describe a prototype system CTI4AI,
to overcome the need to methodically identify and share AI/ML specific
vulnerabilities and threat intelligence.",0,1,0,0,0,0,0.993262,16.0,1.0,10
82533b04-cc6d-493f-9bb7-bff283076c39,Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks,12,0.100031,0.712479,"Iterative text revision improves text quality by fixing grammatical errors,
rephrasing for better readability or contextual appropriateness, or
reorganizing sentence structures throughout a document. Most recent research
has focused on understanding and classifying different types of edits in the
iterative revision process from human-written text instead of building accurate
and robust systems for iterative text revision. In this work, we aim to build
an end-to-end text revision system that can iteratively generate helpful edits
by explicitly detecting editable spans (where-to-edit) with their corresponding
edit intents and then instructing a revision model to revise the detected edit
spans. Leveraging datasets from other related text editing NLP tasks, combined
with the specification of editable spans, leads our system to more accurately
model the process of iterative text refinement, as evidenced by empirical
results and human evaluations. Our system significantly outperforms previous
baselines on our text revision tasks and other standard text revision tasks,
including grammatical error correction, text simplification, sentence fusion,
and style transfer. Through extensive qualitative and quantitative analysis, we
make vital connections between edit intentions and writing quality, and better
computational modeling of iterative text revisions.",1,1,0,0,1,0,0.383636,7.0,0.666389,42
5901bf2c-9b6f-4511-bec6-18bfca44e7f8,Policy-Based Bayesian Experimental Design for Non-Differentiable Implicit Models,5,0.00733122,0.151339,"For applications in healthcare, physics, energy, robotics, and many other
fields, designing maximally informative experiments is valuable, particularly
when experiments are expensive, time-consuming, or pose safety hazards. While
existing approaches can sequentially design experiments based on prior
observation history, many of these methods do not extend to implicit models,
where simulation is possible but computing the likelihood is intractable.
Furthermore, they often require either significant online computation during
deployment or a differentiable simulation system. We introduce Reinforcement
Learning for Deep Adaptive Design (RL-DAD), a method for simulation-based
optimal experimental design for non-differentiable implicit models. RL-DAD
extends prior work in policy-based Bayesian Optimal Experimental Design (BOED)
by reformulating it as a Markov Decision Process with a reward function based
on likelihood-free information lower bounds, which is used to learn a policy
via deep reinforcement learning. The learned design policy maps prior histories
to experiment designs offline and can be quickly deployed during online
execution. We evaluate RL-DAD and find that it performs competitively with
baselines on three benchmarks.",0,1,0,0,0,0,0.0802181,6.0,0.318179,44
c6b71491-7ab6-4149-81d0-145941a82bec,Are AlphaZero-like Agents Robust to Adversarial Perturbations?,8,0.0278291,0.519085,"The success of AlphaZero (AZ) has demonstrated that neural-network-based Go
AIs can surpass human performance by a large margin. Given that the state space
of Go is extremely large and a human player can play the game from any legal
state, we ask whether adversarial states exist for Go AIs that may lead them to
play surprisingly wrong actions. In this paper, we first extend the concept of
adversarial examples to the game of Go: we generate perturbed states that are
``semantically'' equivalent to the original state by adding meaningless moves
to the game, and an adversarial state is a perturbed state leading to an
undoubtedly inferior action that is obvious even for Go beginners. However,
searching the adversarial state is challenging due to the large, discrete, and
non-differentiable search space. To tackle this challenge, we develop the first
adversarial attack on Go AIs that can efficiently search for adversarial states
by strategically reducing the search space. This method can also be extended to
other board games such as NoGo. Experimentally, we show that the actions taken
by both Policy-Value neural network (PV-NN) and Monte Carlo tree search (MCTS)
can be misled by adding one or two meaningless stones; for example, on 58\% of
the AlphaGo Zero self-play games, our method can make the widely used KataGo
agent with 50 simulations of MCTS plays a losing action by adding two
meaningless stones. We additionally evaluated the adversarial examples found by
our algorithm with amateur human Go players and 90\% of examples indeed lead
the Go agent to play an obviously inferior action. Our code is available at
\url{https://PaperCode.cc/GoAttack}.",1,0,1,0,0,0,0.215511,10.0,0.697473,40
7258460a-5d4c-456e-a681-caa673701f85,"Alexa, Let's Work Together: Introducing the First Alexa Prize TaskBot Challenge on Conversational Task Assistance",19,0.197783,0.926114,"Since its inception in 2016, the Alexa Prize program has enabled hundreds of
university students to explore and compete to develop conversational agents
through the SocialBot Grand Challenge. The goal of the challenge is to build
agents capable of conversing coherently and engagingly with humans on popular
topics for 20 minutes, while achieving an average rating of at least 4.0/5.0.
However, as conversational agents attempt to assist users with increasingly
complex tasks, new conversational AI techniques and evaluation platforms are
needed. The Alexa Prize TaskBot challenge, established in 2021, builds on the
success of the SocialBot challenge by introducing the requirements of
interactively assisting humans with real-world Cooking and Do-It-Yourself
tasks, while making use of both voice and visual modalities. This challenge
requires the TaskBots to identify and understand the user's need, identify and
integrate task and domain knowledge into the interaction, and develop new ways
of engaging the user without distracting them from the task at hand, among
other challenges. This paper provides an overview of the TaskBot challenge,
describes the infrastructure support provided to the teams with the CoBot
Toolkit, and summarizes the approaches the participating teams took to overcome
the research challenges. Finally, it analyzes the performance of the competing
TaskBots during the first year of the competition.",0,1,1,0,0,0,0.731357,5.0,0.732784,10
0cc78714-ada0-4cba-8203-f60f5a043992,DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models,34,0.910547,0.952997,"We study the way DALLE-2 maps symbols (words) in the prompt to their
references (entities or properties of entities in the generated image). We show
that in stark contrast to the way human process language, DALLE-2 does not
follow the constraint that each word has a single role in the interpretation,
and sometimes re-use the same symbol for different purposes. We collect a set
of stimuli that reflect the phenomenon: we show that DALLE-2 depicts both
senses of nouns with multiple senses at once; and that a given word can modify
the properties of two distinct entities in the image, or can be depicted as one
object and also modify the properties of another object, creating a semantic
leakage of properties between entities. Taken together, our study highlights
the differences between DALLE-2 and human language processing and opens an
avenue for future study on the inductive biases of text-to-image models.",0,0,0,0,0,0,0.992271,1.0,0.972164,11
754d8cdf-6c30-45af-85cc-594a5d5d55fb,Back to the Future: On Potential Histories in NLP,3,0.0991306,0.113327,"Machine learning and NLP require the construction of datasets to train and
fine-tune models. In this context, previous work has demonstrated the
sensitivity of these data sets. For instance, potential societal biases in this
data are likely to be encoded and to be amplified in the models we deploy. In
this work, we draw from developments in the field of history and take a novel
perspective on these problems: considering datasets and models through the lens
of historical fiction surfaces their political nature, and affords
re-configuring how we view the past, such that marginalized discourses are
surfaced. Building on such insights, we argue that contemporary methods for
machine learning are prejudiced towards dominant and hegemonic histories.
Employing the example of neopronouns, we show that by surfacing marginalized
histories within contemporary conditions, we can create models that better
represent the lived realities of traditionally marginalized and excluded
communities.",0,0,0,0,0,0,0.968178,6.0,0.93804,43
5432686a-e34d-4f7d-8f7a-9e07c57f01a2,Towards Textual Out-of-Domain Detection without In-Domain Labels,14,0.131064,0.872743,"In many real-world settings, machine learning models need to identify user
inputs that are out-of-domain (OOD) so as to avoid performing wrong actions.
This work focuses on a challenging case of OOD detection, where no labels for
in-domain data are accessible (e.g., no intent labels for the intent
classification task). To this end, we first evaluate different language model
based approaches that predict likelihood for a sequence of tokens. Furthermore,
we propose a novel representation learning based method by combining
unsupervised clustering and contrastive learning so that better data
representations for OOD detection can be learned. Through extensive
experiments, we demonstrate that this method can significantly outperform
likelihood-based methods and can be even competitive to the state-of-the-art
supervised approaches with label information.",0,1,0,0,1,0,0.799915,6.0,0.811031,40
dc3eec35-d680-4e24-a9d6-5197d54db718,Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction,94,0.717,0.967533,"Recently, neural implicit surfaces learning by volume rendering has become
popular for multi-view reconstruction. However, one key challenge remains:
existing approaches lack explicit multi-view geometry constraints, hence
usually fail to generate geometry consistent surface reconstruction. To address
this challenge, we propose geometry-consistent neural implicit surfaces
learning for multi-view reconstruction. We theoretically analyze that there
exists a gap between the volume rendering integral and point-based signed
distance function (SDF) modeling. To bridge this gap, we directly locate the
zero-level set of SDF networks and explicitly perform multi-view geometry
optimization by leveraging the sparse geometry from structure from motion (SFM)
and photometric consistency in multi-view stereo. This makes our SDF
optimization unbiased and allows the multi-view geometry constraints to focus
on the true surface optimization. Extensive experiments show that our proposed
method achieves high-quality surface reconstruction in both complex thin
structures and large smooth regions, thus outperforming the state-of-the-arts
by a large margin.",0,0,0,0,1,0,0.946817,6.0,0.911156,52
b30a9518-4aa8-49fd-86c2-e73e921ef016,SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation,96,0.37542,0.995159,"Visual anomaly detection is commonly used in industrial quality inspection.
In this paper, we present a new dataset as well as a new self-supervised
learning method for ImageNet pre-training to improve anomaly detection and
segmentation in 1-class and 2-class 5/10/high-shot training setups. We release
the Visual Anomaly (VisA) Dataset consisting of 10,821 high-resolution color
images (9,621 normal and 1,200 anomalous samples) covering 12 objects in 3
domains, making it the largest industrial anomaly detection dataset to date.
Both image and pixel-level labels are provided. We also propose a new
self-supervised framework - SPot-the-difference (SPD) - which can regularize
contrastive self-supervised pre-training, such as SimSiam, MoCo and SimCLR, to
be more suitable for anomaly detection tasks. Our experiments on VisA and
MVTec-AD dataset show that SPD consistently improves these contrastive
pre-training baselines and even the supervised pre-training. For example, SPD
improves Area Under the Precision-Recall curve (AU-PR) for anomaly segmentation
by 5.9% and 6.8% over SimSiam and supervised pre-training respectively in the
2-class high-shot regime. We open-source the project at
http://github.com/amazon-research/spot-diff .",1,1,1,1,0,0,0.740958,5.0,0.738247,47
8fe6ff7e-c59f-4851-9e3b-468a15b15661,Goal-Conditioned Reinforcement Learning: Problems and Solutions,63,0.468658,0.994954,"Goal-conditioned reinforcement learning (GCRL), related to a set of complex
RL problems, trains an agent to achieve different goals under particular
scenarios. Compared to the standard RL solutions that learn a policy solely
depending on the states or observations, GCRL additionally requires the agent
to make decisions according to different goals. In this survey, we provide a
comprehensive overview of the challenges and algorithms for GCRL. Firstly, we
answer what the basic problems are studied in this field. Then, we explain how
goals are represented and present how existing solutions are designed from
different points of view. Finally, we make the conclusion and discuss potential
future prospects that recent researches focus on.",1,0,0,0,0,0,0.437487,5.0,0.567554,76
408cd507-cc62-4fdb-9358-d50b02cf47da,Asynchronous Optimisation for Event-based Visual Odometry,9,0.123488,0.794295,"Event cameras open up new possibilities for robotic perception due to their
low latency and high dynamic range. On the other hand, developing effective
event-based vision algorithms that fully exploit the beneficial properties of
event cameras remains work in progress. In this paper, we focus on event-based
visual odometry (VO). While existing event-driven VO pipelines have adopted
continuous-time representations to asynchronously process event data, they
either assume a known map, restrict the camera to planar trajectories, or
integrate other sensors into the system. Towards map-free event-only monocular
VO in SE(3), we propose an asynchronous structure-from-motion optimisation
back-end. Our formulation is underpinned by a principled joint optimisation
problem involving non-parametric Gaussian Process motion modelling and
incremental maximum a posteriori inference. A high-performance incremental
computation engine is employed to reason about the camera trajectory with every
incoming event. We demonstrate the robustness of our asynchronous back-end in
comparison to frame-based methods which depend on accurate temporal
accumulation of measurements.",0,0,0,0,0,0,0.210004,10.0,0.694548,32
bb0a0b8b-81fa-454e-902d-0ffdc9d2caad,Neighboring Backdoor Attacks on Graph Convolutional Network,8,0.162334,0.25536,"Backdoor attacks have been widely studied to hide the misclassification rules
in the normal models, which are only activated when the model is aware of the
specific inputs (i.e., the trigger). However, despite their success in the
conventional Euclidean space, there are few studies of backdoor attacks on
graph structured data. In this paper, we propose a new type of backdoor which
is specific to graph data, called neighboring backdoor. Considering the
discreteness of graph data, how to effectively design the triggers while
retaining the model accuracy on the original task is the major challenge. To
address such a challenge, we set the trigger as a single node, and the backdoor
is activated when the trigger node is connected to the target node. To preserve
the model accuracy, the model parameters are not allowed to be modified. Thus,
when the trigger node is not connected, the model performs normally. Under
these settings, in this work, we focus on generating the features of the
trigger node. Two types of backdoors are proposed: (1) Linear Graph Convolution
Backdoor which finds an approximation solution for the feature generation (can
be viewed as an integer programming problem) by looking at the linear part of
GCNs. (2) Variants of existing graph attacks. We extend current gradient-based
attack methods to our backdoor attack scenario. Extensive experiments on two
social networks and two citation networks datasets demonstrate that all
proposed backdoors can achieve an almost 100\% attack success rate while having
no impact on predictive accuracy.",0,0,1,0,0,0,0.937635,6.0,0.901854,46
deff21ef-5a35-4aeb-be83-a48cf4ed1f8c,Explainability of Predictive Process Monitoring Results: Can You See My Data Issues?,6,0.0737642,0.582827,"Predictive business process monitoring (PPM) has been around for several
years as a use case of process mining. PPM enables foreseeing the future of a
business process through predicting relevant information about how a running
process instance might end, related performance indicators, and other
predictable aspects. A big share of PPM approaches adopts a Machine Learning
(ML) technique to address a prediction task, especially non-process-aware PPM
approaches. Consequently, PPM inherits the challenges faced by ML approaches.
One of these challenges concerns the need to gain user trust in the predictions
generated. The field of explainable artificial intelligence (XAI) addresses
this issue. However, the choices made, and the techniques employed in a PPM
task, in addition to ML model characteristics, influence resulting
explanations. A comparison of the influence of different settings on the
generated explanations is missing. To address this gap, we investigate the
effect of different PPM settings on resulting data fed into an ML model and
consequently to a XAI method. We study how differences in resulting
explanations may indicate several issues in underlying data. We construct a
framework for our experiments including different settings at each stage of PPM
with XAI integrated as a fundamental part. Our experiments reveal several
inconsistencies, as well as agreements, between data characteristics (and hence
expectations about these data), important data used by the ML model as a result
of querying it, and explanations of predictions of the investigated ML model.",1,0,0,0,0,0,0.765107,10.0,0.876118,26
067b5163-8d0c-4958-be34-e66f57596b0d,Actual Causality and Responsibility Attribution in Decentralized Partially Observable Markov Decision Processes,6,0.280932,0.638303,"Actual causality and a closely related concept of responsibility attribution
are central to accountable decision making. Actual causality focuses on
specific outcomes and aims to identify decisions (actions) that were critical
in realizing an outcome of interest. Responsibility attribution is
complementary and aims to identify the extent to which decision makers (agents)
are responsible for this outcome. In this paper, we study these concepts under
a widely used framework for multi-agent sequential decision making under
uncertainty: decentralized partially observable Markov decision processes
(Dec-POMDPs). Following recent works in RL that show correspondence between
POMDPs and Structural Causal Models (SCMs), we first establish a connection
between Dec-POMDPs and SCMs. This connection enables us to utilize a language
for describing actual causality from prior work and study existing definitions
of actual causality in Dec-POMDPs. Given that some of the well-known
definitions may lead to counter-intuitive actual causes, we introduce a novel
definition that more explicitly accounts for causal dependencies between
agents' actions. We then turn to responsibility attribution based on actual
causality, where we argue that in ascribing responsibility to an agent it is
important to consider both the number of actual causes in which the agent
participates, as well as its ability to manipulate its own degree of
responsibility. Motivated by these arguments we introduce a family of
responsibility attribution methods that extends prior work, while accounting
for the aforementioned considerations. Finally, through a simulation-based
experiment, we compare different definitions of actual causality and
responsibility attribution methods. The empirical results demonstrate the
qualitative difference between the considered definitions of actual causality
and their impact on attributed responsibility.",0,0,0,0,0,0,0.38043,18.0,0.869664,55
52a685e0-a344-44d5-803d-a3325470fe81,Task-Balanced Distillation for Object Detection,10,0.105787,0.518518,"Mainstream object detectors are commonly constituted of two sub-tasks,
including classification and regression tasks, implemented by two parallel
heads. This classic design paradigm inevitably leads to inconsistent spatial
distributions between classification score and localization quality (IOU).
Therefore, this paper alleviates this misalignment in the view of knowledge
distillation. First, we observe that the massive teacher achieves a higher
proportion of harmonious predictions than the lightweight student. Based on
this intriguing observation, a novel Harmony Score (HS) is devised to estimate
the alignment of classification and regression qualities. HS models the
relationship between two sub-tasks and is seen as prior knowledge to promote
harmonious predictions for the student. Second, this spatial misalignment will
result in inharmonious region selection when distilling features. To alleviate
this problem, a novel Task-decoupled Feature Distillation (TFD) is proposed by
flexibly balancing the contributions of classification and regression tasks.
Eventually, HD and TFD constitute the proposed method, named Task-Balanced
Distillation (TBD). Extensive experiments demonstrate the considerable
potential and generalization of the proposed method. Specifically, when
equipped with TBD, RetinaNet with ResNet-50 achieves 41.0 mAP under the COCO
benchmark, outperforming the recent FGD and FRS.",0,1,0,0,1,0,0.651713,7.0,0.777692,57
d6463ca0-2100-4597-895e-dd6834837127,Data Contamination: From Memorization to Exploitation,88,0.174817,0.955182,"Pretrained language models are typically trained on massive web-based
datasets, which are often ""contaminated"" with downstream test sets. It is not
clear to what extent models exploit the contaminated data for downstream tasks.
We present a principled method to study this question. We pretrain BERT models
on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune
them on the relevant task. Comparing performance between samples seen and
unseen during pretraining enables us to define and quantify levels of
memorization and exploitation. Experiments with two models and three downstream
tasks show that exploitation exists in some cases, but in others the models
memorize the contaminated data, but do not exploit it. We show that these two
measures are affected by different factors such as the number of duplications
of the contaminated data and the model size. Our results highlight the
importance of analyzing massive web-scale datasets to verify that progress in
NLP is obtained by better language understanding and not better data
exploitation.",1,0,0,0,0,0,0.739748,4.0,0.671944,34
5fa194dc-3296-44d6-afa8-cb1ee83d31fc,Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning,26,0.0941391,0.507345,"Automatically discovering failures in vision models under real-world settings
remains an open challenge. This work demonstrates how off-the-shelf,
large-scale, image-to-text and text-to-image models, trained on vast amounts of
data, can be leveraged to automatically find such failures. In essence, a
conditional text-to-image generative model is used to generate large amounts of
synthetic, yet realistic, inputs given a ground-truth label. Misclassified
inputs are clustered and a captioning model is used to describe each cluster.
Each cluster's description is used in turn to generate more inputs and assess
whether specific clusters induce more failures than expected. We use this
pipeline to demonstrate that we can effectively interrogate classifiers trained
on ImageNet to find specific failure cases and discover spurious correlations.
We also show that we can scale the approach to generate adversarial datasets
targeting specific classifier architectures. This work serves as a
proof-of-concept demonstrating the utility of large-scale generative models to
automatically discover bugs in vision models in an open-ended manner. We also
describe a number of limitations and pitfalls related to this approach.",0,1,0,0,0,0,0.509665,8.0,0.756477,73
50e71ecc-c859-4c7c-bc94-0efe208fe148,Revealing interactions between HVDC cross-area flows and frequency stability with explainable AI,4,0.406426,0.607329,"The energy transition introduces more volatile energy sources into the power
grids. In this context, power transfer between different synchronous areas
through High Voltage Direct Current (HVDC) links becomes increasingly
important. Such links can balance volatile generation by enabling long-distance
transport or by leveraging their fast control behavior. Here, we investigate
the interaction of power imbalances - represented through the power grid
frequency - and power flows on HVDC links between synchronous areas in Europe.
We use explainable machine learning to identify key dependencies and
disentangle the interaction of critical features. Our results show that
market-based HVDC flows introduce deterministic frequency deviations, which
however can be mitigated through strict ramping limits. Moreover, varying HVDC
operation modes strongly affect the interaction with the grid. In particular,
we show that load-frequency control via HVDC links can both have control-like
or disturbance-like impacts on frequency stability.",0,0,0,0,0,0,0.913263,9.0,0.920506,50
f1503f99-21b9-470f-b2ce-7f5b28295f50,Robust Domain Adaptation for Pre-trained Multilingual Neural Machine Translation Models,2,0.00602105,0.129157,"Recent literature has demonstrated the potential of multilingual Neural
Machine Translation (mNMT) models. However, the most efficient models are not
well suited to specialized industries. In these cases, internal data is scarce
and expensive to find in all language pairs. Therefore, fine-tuning a mNMT
model on a specialized domain is hard. In this context, we decided to focus on
a new task: Domain Adaptation of a pre-trained mNMT model on a single pair of
language while trying to maintain model quality on generic domain data for all
language pairs. The risk of loss on generic domain and on other pairs is high.
This task is key for mNMT model adoption in the industry and is at the border
of many others. We propose a fine-tuning procedure for the generic mNMT that
combines embeddings freezing and adversarial loss. Our experiments demonstrated
that the procedure improves performances on specialized data with a minimal
loss in initial performances on generic domain for all languages pairs,
compared to a naive standard approach (+10.0 BLEU score on specialized data,
-0.01 to -0.5 BLEU on WMT and Tatoeba datasets on the other pairs with M2M100).",0,1,1,0,0,0,0.281089,8.0,0.660244,44
11fb99c7-bc68-4999-8d00-43ec0b785470,Marginal Contrastive Correspondence for Guided Image Generation,24,0.278045,0.342639,"Exemplar-based image translation establishes dense correspondences between a
conditional input and an exemplar (from two different domains) for leveraging
detailed exemplar styles to achieve realistic image translation. Existing work
builds the cross-domain correspondences implicitly by minimizing feature-wise
distances across the two domains. Without explicit exploitation of
domain-invariant features, this approach may not reduce the domain gap
effectively which often leads to sub-optimal correspondences and image
translation. We design a Marginal Contrastive Learning Network (MCL-Net) that
explores contrastive learning to learn domain-invariant features for realistic
exemplar-based image translation. Specifically, we design an innovative
marginal contrastive loss that guides to establish dense correspondences
explicitly. Nevertheless, building correspondence with domain-invariant
semantics alone may impair the texture patterns and lead to degraded texture
generation. We thus design a Self-Correlation Map (SCM) that incorporates scene
structures as auxiliary information which improves the built correspondences
substantially. Quantitative and qualitative experiments on multifarious image
translation tasks show that the proposed method outperforms the
state-of-the-art consistently.",0,1,0,0,1,0,0.787187,7.0,0.832443,53
32624a50-dd38-43a9-8df9-de1cc63f1598,"""Even if ..."" -- Diverse Semifactual Explanations of Reject",4,0.0326277,0.173056,"Machine learning based decision making systems applied in safety critical
areas require reliable high certainty predictions. For this purpose, the system
can be extended by an reject option which allows the system to reject inputs
where only a prediction with an unacceptably low certainty would be possible.
While being able to reject uncertain samples is important, it is also of
importance to be able to explain why a particular sample was rejected. With the
ongoing rise of eXplainable AI (XAI), a lot of explanation methodologies for
machine learning based systems have been developed -- explaining reject
options, however, is still a novel field where only very little prior work
exists.
  In this work, we propose to explain rejects by semifactual explanations, an
instance of example-based explanation methods, which them self have not been
widely considered in the XAI community yet. We propose a conceptual modeling of
semifactual explanations for arbitrary reject options and empirically evaluate
a specific implementation on a conformal prediction based reject option.",0,0,0,0,0,0,0.0623026,10.0,0.56468,30
891cd8b2-a9a2-4950-ae8f-400ac2ab8ba3,AutoSNN: Towards Energy-Efficient Spiking Neural Networks,41,0.286245,0.938501,"Spiking neural networks (SNNs) that mimic information transmission in the
brain can energy-efficiently process spatio-temporal information through
discrete and sparse spikes, thereby receiving considerable attention. To
improve accuracy and energy efficiency of SNNs, most previous studies have
focused solely on training methods, and the effect of architecture has rarely
been studied. We investigate the design choices used in the previous studies in
terms of the accuracy and number of spikes and figure out that they are not
best-suited for SNNs. To further improve the accuracy and reduce the spikes
generated by SNNs, we propose a spike-aware neural architecture search
framework called AutoSNN. We define a search space consisting of architectures
without undesirable design choices. To enable the spike-aware architecture
search, we introduce a fitness that considers both the accuracy and number of
spikes. AutoSNN successfully searches for SNN architectures that outperform
hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate
the effectiveness of AutoSNN on various datasets including neuromorphic
datasets.",1,1,0,0,1,0,0.73648,7.0,0.811209,69
64117a3a-253e-45d7-a83c-f566837719bc,Improving Few-Shot Part Segmentation using Coarse Supervision,7,0.160422,0.335946,"A significant bottleneck in training deep networks for part segmentation is
the cost of obtaining detailed annotations. We propose a framework to exploit
coarse labels such as figure-ground masks and keypoint locations that are
readily available for some categories to improve part segmentation models. A
key challenge is that these annotations were collected for different tasks and
with different labeling styles and cannot be readily mapped to the part labels.
To this end, we propose to jointly learn the dependencies between labeling
styles and the part segmentation model, allowing us to utilize supervision from
diverse labels. To evaluate our approach we develop a benchmark on the
Caltech-UCSD birds and OID Aircraft dataset. Our approach outperforms baselines
based on multi-task learning, semi-supervised learning, and competitive methods
relying on loss functions manually designed to exploit sparse-supervision.",0,1,0,0,1,0,0.871059,6.0,0.85127,39
77bc1e1a-63ba-4411-8aa7-de42e5bf599f,"MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction",25,0.423596,0.979835,"The diverse relationships among real-world events, including coreference,
temporal, causal, and subevent relations, are fundamental to understanding
natural languages. However, two drawbacks of existing datasets limit event
relation extraction (ERE) tasks: (1) Small scale. Due to the annotation
complexity, the data scale of existing datasets is limited, which cannot well
train and evaluate data-hungry models. (2) Absence of unified annotation.
Different types of event relations naturally interact with each other, but
existing datasets only cover limited relation types at once, which prevents
models from taking full advantage of relation interactions. To address these
issues, we construct a unified large-scale human-annotated ERE dataset
MAVEN-ERE with improved annotation schemes. It contains 103,193 event
coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and
15,841 subevent relations, which is larger than existing datasets of all the
ERE tasks by at least an order of magnitude. Experiments show that ERE on
MAVEN-ERE is quite challenging, and considering relation interactions with
joint learning can improve performances. The dataset and source codes can be
obtained from https://github.com/THU-KEG/MAVEN-ERE.",1,1,1,1,0,0,0.156103,13.0,0.739768,74
71d0ce05-9347-45f6-bf0f-363691623212,Understanding Long Programming Languages with Structure-Aware Sparse Attention,2,0.00526823,0.0707733,"Programming-based Pre-trained Language Models (PPLMs) such as CodeBERT have
achieved great success in many downstream code-related tasks. Since the memory
and computational complexity of self-attention in the Transformer grow
quadratically with the sequence length, PPLMs typically limit the code length
to 512. However, codes in real-world applications are generally long, such as
code searches, which cannot be processed efficiently by existing PPLMs. To
solve this problem, in this paper, we present SASA, a Structure-Aware Sparse
Attention mechanism, which reduces the complexity and improves performance for
long code understanding tasks. The key components in SASA are top-$k$ sparse
attention and Abstract Syntax Tree (AST)-based structure-aware attention. With
top-$k$ sparse attention, the most crucial attention relation can be obtained
with a lower computational cost. As the code structure represents the logic of
the code statements, which is a complement to the code sequence
characteristics, we further introduce AST structures into attention. Extensive
experiments on CodeXGLUE tasks show that SASA achieves better performance than
the competing baselines.",0,1,0,0,0,0,0.531179,6.0,0.685479,26
f01fd314-656c-461a-bdfa-b06621d7c325,Keyword localisation in untranscribed speech using visually grounded speech models,6,0.0268111,0.469489,"Keyword localisation is the task of finding where in a speech utterance a
given query keyword occurs. We investigate to what extent keyword localisation
is possible using a visually grounded speech (VGS) model. VGS models are
trained on unlabelled images paired with spoken captions. These models are
therefore self-supervised -- trained without any explicit textual label or
location information. To obtain training targets, we first tag training images
with soft text labels using a pretrained visual classifier with a fixed
vocabulary. This enables a VGS model to predict the presence of a written
keyword in an utterance, but not its location. We consider four ways to equip
VGS models with localisations capabilities. Two of these -- a saliency approach
and input masking -- can be applied to an arbitrary prediction model after
training, while the other two -- attention and a score aggregation approach --
are incorporated directly into the structure of the model. Masked-based
localisation gives some of the best reported localisation scores from a VGS
model, with an accuracy of 57% when the system knows that a keyword occurs in
an utterance and need to predict its location. In a setting where localisation
is performed after detection, an $F_1$ of 25% is achieved, and in a setting
where a keyword spotting ranking pass is first performed, we get a localisation
P@10 of 32%. While these scores are modest compared to the idealised setting
with unordered bag-of-word-supervision (from transcriptions), these models do
not receive any textual or location supervision. Further analyses show that
these models are limited by the first detection or ranking pass. Moreover,
individual keyword localisation performance is correlated with the tagging
performance from the visual classifier. We also show qualitatively how and
where semantic mistakes occur, e.g. that the model locates surfer when queried
with ocean.",1,0,0,0,0,0,0.0895264,9.0,0.558208,103
8bc01110-5817-4ae3-98e4-e332cbaa930c,Multiple Domain Cyberspace Attack and Defense Game Based on Reward Randomization Reinforcement Learning,3,0.329265,0.981684,"The existing network attack and defense method can be regarded as game, but
most of the game only involves network domain, not multiple domain cyberspace.
To address this challenge, this paper proposed a multiple domain cyberspace
attack and defense game model based on reinforcement learning. We define the
multiple domain cyberspace include physical domain, network domain and digital
domain. By establishing two agents, representing the attacker and the defender
respectively, defender will select the multiple domain actions in the multiple
domain cyberspace to obtain defender's optimal reward by reinforcement
learning. In order to improve the defense ability of defender, a game model
based on reward randomization reinforcement learning is proposed. When the
defender takes the multiple domain defense action, the reward is randomly given
and subject to linear distribution, so as to find the better defense policy and
improve defense success rate. The experimental results show that the game model
can effectively simulate the attack and defense state of multiple domain
cyberspace, and the proposed method has a higher defense success rate than DDPG
and DQN.",0,1,1,0,1,0,0.949742,9.0,0.942892,13
809d689d-d039-4082-a5a8-11df3116e6b5,An End-to-End Transformer Model for Crowd Localization,65,0.160254,0.619025,"Crowd localization, predicting head positions, is a more practical and
high-level task than simply counting. Existing methods employ pseudo-bounding
boxes or pre-designed localization maps, relying on complex post-processing to
obtain the head positions. In this paper, we propose an elegant, end-to-end
Crowd Localization Transformer named CLTR that solves the task in the
regression-based paradigm. The proposed method views the crowd localization as
a direct set prediction problem, taking extracted features and trainable
embeddings as input of the transformer-decoder. To reduce the ambiguous points
and generate more reasonable matching results, we introduce a KMO-based
Hungarian matcher, which adopts the nearby context as the auxiliary matching
cost. Extensive experiments conducted on five datasets in various data settings
show the effectiveness of our method. In particular, the proposed method
achieves the best localization performance on the NWPU-Crowd, UCF-QNRF, and
ShanghaiTech Part A datasets.",1,1,0,0,1,0,0.267483,5.0,0.444693,47
5e5b996d-e381-4c7f-ad27-c2105eb8130f,3DMODT: Attention-Guided Affinities for Joint Detection & Tracking in 3D Point Clouds,6,0.0863842,0.369262,"We propose a method for joint detection and tracking of multiple objects in
3D point clouds, a task conventionally treated as a two-step process comprising
object detection followed by data association. Our method embeds both steps
into a single end-to-end trainable network eliminating the dependency on
external object detectors. Our model exploits temporal information employing
multiple frames to detect objects and track them in a single network, thereby
making it a utilitarian formulation for real-world scenarios. Computing
affinity matrix by employing features similarity across consecutive point cloud
scans forms an integral part of visual tracking. We propose an attention-based
refinement module to refine the affinity matrix by suppressing erroneous
correspondences. The module is designed to capture the global context in
affinity matrix by employing self-attention within each affinity matrix and
cross-attention across a pair of affinity matrices. Unlike competing
approaches, our network does not require complex post-processing algorithms,
and processes raw LiDAR frames to directly output tracking results. We
demonstrate the effectiveness of our method on the three tracking benchmarks:
JRDB, Waymo, and KITTI. Experimental evaluations indicate the ability of our
model to generalize well across datasets.",0,1,0,0,0,0,0.800051,6.0,0.811101,56
cc89a23f-f986-424c-a185-21b822f19830,Self-Supervised Feature Learning from Partial Point Clouds via Pose Disentanglement,2,0.0261871,0.0536982,"Self-supervised learning on point clouds has gained a lot of attention
recently, since it addresses the label-efficiency and domain-gap problems on
point cloud tasks. In this paper, we propose a novel self-supervised framework
to learn informative representations from partial point clouds. We leverage
partial point clouds scanned by LiDAR that contain both content and pose
attributes, and we show that disentangling such two factors from partial point
clouds enhances feature representation learning. To this end, our framework
consists of three main parts: 1) a completion network to capture holistic
semantics of point clouds; 2) a pose regression network to understand the
viewing angle where partial data is scanned from; 3) a partial reconstruction
network to encourage the model to learn content and pose features. To
demonstrate the robustness of the learnt feature representations, we conduct
several downstream tasks including classification, part segmentation, and
registration, with comparisons against state-of-the-art methods. Our method not
only outperforms existing self-supervised methods, but also shows a better
generalizability across synthetic and real-world datasets.",0,1,0,0,0,0,0.829353,7.0,0.8515,52
4eba4a26-51a1-4071-9d8c-92de2c57d4a0,Towards Trustworthy Multi-label Sewer Defect Classification via Evidential Deep Learning,4,0.0715951,0.656177,"An automatic vision-based sewer inspection plays a key role of sewage system
in a modern city. Recent advances focus on utilizing deep learning model to
realize the sewer inspection system, benefiting from the capability of
data-driven feature representation. However, the inherent uncertainty of sewer
defects is ignored, resulting in the missed detection of serious unknown sewer
defect categories. In this paper, we propose a trustworthy multi-label sewer
defect classification (TMSDC) method, which can quantify the uncertainty of
sewer defect prediction via evidential deep learning. Meanwhile, a novel expert
base rate assignment (EBRA) is proposed to introduce the expert knowledge for
describing reliable evidences in practical situations. Experimental results
demonstrate the effectiveness of TMSDC and the superior capability of
uncertainty estimation is achieved on the latest public benchmark.",0,1,0,0,0,0,0.380369,6.0,0.608956,25
ad7f95ad-86fe-4eaa-a488-b6ba088febca,PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence,7,0.101754,0.432169,"Work on social media rumour verification utilises signals from posts, their
propagation and users involved. Other lines of work target identifying and
fact-checking claims based on information from Wikipedia, or trustworthy news
articles without considering social media context. However works combining the
information from social media with external evidence from the wider web are
lacking. To facilitate research in this direction, we release a novel dataset,
PHEMEPlus, an extension of the PHEME benchmark, which contains social media
conversations as well as relevant external evidence for each rumour. We
demonstrate the effectiveness of incorporating such evidence in improving
rumour verification models. Additionally, as part of the evidence collection,
we evaluate various ways of query formulation to identify the most effective
method.",1,1,0,1,0,0,0.502698,7.0,0.718832,42
e9ae66b1-35a4-49b6-ba3e-8d462cffc4de,Self-Supervised Face Image Restoration with a One-Shot Reference,1,0.00640929,0.039764,"For image restoration, methods leveraging priors from generative models have
been proposed and demonstrated a promising capacity to robustly restore
photorealistic and high-quality results. However, these methods are susceptible
to semantic ambiguity, particularly with images that have obviously correct
semantics such as facial images. In this paper, we propose a semantic-aware
latent space exploration method for image restoration (SAIR). By explicitly
modeling semantics information from a given reference image, SAIR is able to
reliably restore severely degraded images not only to high-resolution and
highly realistic looks but also to correct semantics. Quantitative and
qualitative experiments collectively demonstrate the superior performance of
the proposed SAIR. Our code is available at https://github.com/Liamkuo/SAIR.",1,0,0,0,0,0,0.791826,4.0,0.710311,22
0d7975aa-7bbc-4b3c-914c-fcc94c0315f8,Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality,219,0.985167,0.999972,"We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.",0,0,0,0,0,0,0.848152,6.0,0.837404,92
3175ea17-b519-4f6a-953e-d29712916e7e,Generic Temporal Reasoning with Differential Analysis and Explanation,11,0.101512,0.212051,"Temporal reasoning is the task of predicting temporal relations of event
pairs. While temporal reasoning models can perform reasonably well on in-domain
benchmarks, we have little idea of these systems' generalizability due to
existing datasets' limitations. In this work, we introduce a novel task named
TODAY that bridges this gap with temporal differential analysis, which as the
name suggests, evaluates whether systems can correctly understand the effect of
incremental changes. Specifically, TODAY introduces slight contextual changes
for given event pairs, and systems are asked to tell how this subtle contextual
change would affect relevant temporal relation distributions. To facilitate
learning, TODAY also annotates human explanations. We show that existing
models, including GPT-3.5, drop to random guessing on TODAY, suggesting that
they heavily rely on spurious information rather than proper reasoning for
temporal predictions. On the other hand, we show that TODAY's supervision style
and explanation annotations can be used in joint learning, encouraging models
to use more appropriate signals during training and thus outperform across
several benchmarks. TODAY can also be used to train models to solicit
incidental supervision from noisy sources such as GPT-3.5, thus moving us more
toward the goal of generic temporal reasoning systems.",1,0,1,1,0,0,0.353242,7.0,0.651422,48
552bf9be-eb44-420f-ab60-94dcaa49cb8d,The future is different: Large pre-trained language models fail in prediction tasks,2,0.00576241,0.0538775,"Large pre-trained language models (LPLM) have shown spectacular success when
fine-tuned on downstream supervised tasks. Yet, it is known that their
performance can drastically drop when there is a distribution shift between the
data used during training and that used at inference time. In this paper we
focus on data distributions that naturally change over time and introduce four
new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and
POLITICS sub-reddits. First, we empirically demonstrate that LPLM can display
average performance drops of about 88% (in the best case!) when predicting the
popularity of future posts from sub-reddits whose topic distribution changes
with time. We then introduce a simple methodology that leverages neural
variational dynamic topic models and attention mechanisms to infer temporal
language model representations for regression tasks. Our models display
performance drops of only about 40% in the worst cases (2% in the best ones)
when predicting the popularity of future posts, while using only about 7% of
the total number of parameters of LPLM and providing interpretable
representations that offer insight into real-world events, like the GameStop
short squeeze of 2021",0,1,1,1,0,0,0.154547,11.0,0.691461,50
8a1104d4-4357-4a8d-b626-53344f8f5735,Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning,31,0.712601,0.798955,"Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.",0,0,0,0,0,0,0.972721,4.0,0.917988,78
d14518ad-9445-4b20-b504-bb15924e446e,Bayesian Optimization under Stochastic Delayed Feedback,11,0.0387797,0.685853,"Bayesian optimization (BO) is a widely-used sequential method for
zeroth-order optimization of complex and expensive-to-compute black-box
functions. The existing BO methods assume that the function evaluation
(feedback) is available to the learner immediately or after a fixed delay. Such
assumptions may not be practical in many real-life problems like online
recommendations, clinical trials, and hyperparameter tuning where feedback is
available after a random delay. To benefit from the experimental
parallelization in these problems, the learner needs to start new function
evaluations without waiting for delayed feedback. In this paper, we consider
the BO under stochastic delayed feedback problem. We propose algorithms with
sub-linear regret guarantees that efficiently address the dilemma of selecting
new function queries while waiting for randomly delayed feedback. Building on
our results, we also make novel contributions to batch BO and contextual
Gaussian process bandits. Experiments on synthetic and real-life datasets
verify the performance of our algorithms.",1,0,1,0,0,0,0.00966961,10.0,0.375665,86
ad9adfca-77ca-45b5-b65e-d221f9727fba,EAutoDet: Efficient Architecture Search for Object Detection,12,0.0349658,0.58587,"Training CNN for detection is time-consuming due to the large dataset and
complex network modules, making it hard to search architectures on detection
datasets directly, which usually requires vast search costs (usually tens and
even hundreds of GPU-days). In contrast, this paper introduces an efficient
framework, named EAutoDet, that can discover practical backbone and FPN
architectures for object detection in 1.4 GPU-days. Specifically, we construct
a supernet for both backbone and FPN modules and adopt the differentiable
method. To reduce the GPU memory requirement and computational cost, we propose
a kernel reusing technique by sharing the weights of candidate operations on
one edge and consolidating them into one convolution. A dynamic channel
refinement strategy is also introduced to search channel numbers. Extensive
experiments show significant efficacy and efficiency of our method. In
particular, the discovered architectures surpass state-of-the-art object
detection NAS methods and achieve 40.1 mAP with 120 FPS and 49.2 mAP with 41.3
FPS on COCO test-dev set. We also transfer the discovered architectures to
rotation detection task, which achieve 77.05 mAP$_{\text{50}}$ on DOTA-v1.0
test set with 21.1M parameters.",1,1,0,0,1,0,0.543421,5.0,0.629442,51
7a8a5899-9d81-4686-8007-7819cc21d34b,OpenEarthMap: A Benchmark Dataset for Global High-Resolution Land Cover Mapping,34,0.399159,0.986037,"We introduce OpenEarthMap, a benchmark dataset, for global high-resolution
land cover mapping. OpenEarthMap consists of 2.2 million segments of 5000
aerial and satellite images covering 97 regions from 44 countries across 6
continents, with manually annotated 8-class land cover labels at a 0.25--0.5m
ground sampling distance. Semantic segmentation models trained on the
OpenEarthMap generalize worldwide and can be used as off-the-shelf models in a
variety of applications. We evaluate the performance of state-of-the-art
methods for unsupervised domain adaptation and present challenging problem
settings suitable for further technical development. We also investigate
lightweight models using automated neural architecture search for limited
computational resources and fast mapping. The dataset is available at
https://open-earth-map.org.",0,1,0,1,0,0,0.787978,7.0,0.832786,60
0f46b066-52a9-45ae-8b3a-e2add85ff397,A Dataset for Medical Instructional Video Classification and Question Answering,17,0.19493,0.51062,"This paper introduces a new challenge and datasets to foster research toward
designing systems that can understand medical videos and provide visual answers
to natural language questions. We believe medical videos may provide the best
possible answers to many first aids, medical emergency, and medical education
questions. Toward this, we created the MedVidCL and MedVidQA datasets and
introduce the tasks of Medical Video Classification (MVC) and Medical Visual
Answer Localization (MVAL), two tasks that focus on cross-modal (medical
language and medical video) understanding. The proposed tasks and datasets have
the potential to support the development of sophisticated downstream
applications that can benefit the public and medical practitioners. Our
datasets consist of 6,117 annotated videos for the MVC task and 3,010 annotated
questions and answers timestamps from 899 videos for the MVAL task. These
datasets have been verified and corrected by medical informatics experts. We
have also benchmarked each task with the created MedVidCL and MedVidQA datasets
and proposed the multimodal learning methods that set competitive baselines for
future research.",0,1,1,1,0,0,0.9598,9.0,0.950894,41
7a62e1a5-7712-43c9-8ff4-3126b437d4ab,Ontology-enhanced Prompt-tuning for Few-shot Learning,39,0.99675,0.794797,"Few-shot Learning (FSL) is aimed to make predictions based on a limited
number of samples. Structured data such as knowledge graphs and ontology
libraries has been leveraged to benefit the few-shot setting in various tasks.
However, the priors adopted by the existing methods suffer from challenging
knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder
the performance for few-shot learning. In this study, we explore knowledge
injection for FSL with pre-trained language models and propose
ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the
ontology transformation based on the external knowledge graph to address the
knowledge missing issue, which fulfills and converts structure knowledge to
text. We further introduce span-sensitive knowledge injection via a visible
matrix to select informative knowledge to handle the knowledge noise issue. To
bridge the gap between knowledge and text, we propose a collective training
algorithm to optimize representations jointly. We evaluate our proposed
OntoPrompt in three tasks, including relation extraction, event extraction, and
knowledge graph completion, with eight datasets. Experimental results
demonstrate that our approach can obtain better few-shot performance than
baselines.",0,1,0,0,0,0,0.988706,3.0,0.963656,75
d21e9fa2-9893-4637-a6b6-43dd8e4f1aea,Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis,2,0.0325921,0.0514724,"Representational Similarity Analysis is a method from cognitive neuroscience,
which helps in comparing representations from two different sources of data. In
this paper, we propose using Representational Similarity Analysis to probe the
semantic grounding in language models of code. We probe representations from
the CodeBERT model for semantic grounding by using the data from the IBM
CodeNet dataset. Through our experiments, we show that current pre-training
methods do not induce semantic grounding in language models of code, and
instead focus on optimizing form-based patterns. We also show that even a
little amount of fine-tuning on semantically relevant tasks increases the
semantic grounding in CodeBERT significantly. Our ablations with the input
modality to the CodeBERT model show that using bimodal inputs (code and natural
language) over unimodal inputs (only code) gives better semantic grounding and
sample efficiency during semantic fine-tuning. Finally, our experiments with
semantic perturbations in code reveal that CodeBERT is able to robustly
distinguish between semantically correct and incorrect code.",0,0,0,0,0,0,0.959667,5.0,0.911403,19
5a1b5cf0-27e7-429b-ba1e-64611d565f70,Probabilistic Volumetric Fusion for Dense Monocular SLAM,11,0.121334,0.358958,"We present a novel method to reconstruct 3D scenes from images by leveraging
deep dense monocular SLAM and fast uncertainty propagation. The proposed
approach is able to 3D reconstruct scenes densely, accurately, and in real-time
while being robust to extremely noisy depth estimates coming from dense
monocular SLAM. Differently from previous approaches, that either use ad-hoc
depth filters, or that estimate the depth uncertainty from RGB-D cameras'
sensor models, our probabilistic depth uncertainty derives directly from the
information matrix of the underlying bundle adjustment problem in SLAM. We show
that the resulting depth uncertainty provides an excellent signal to weight the
depth-maps for volumetric fusion. Without our depth uncertainty, the resulting
mesh is noisy and with artifacts, while our approach generates an accurate 3D
mesh with significantly fewer artifacts. We provide results on the challenging
Euroc dataset, and show that our approach achieves 92% better accuracy than
directly fusing depths from monocular SLAM, and up to 90% improvements compared
to the best competing approach.",0,1,0,0,1,0,0.743933,10.0,0.869975,31
0fd8e45e-d551-43b8-bc5a-a85d2dd1d157,Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud Analysis,11,0.157228,0.59808,"Point clouds are characterized by irregularity and unstructuredness, which
pose challenges in efficient data exploitation and discriminative feature
extraction. In this paper, we present an unsupervised deep neural architecture
called Flattening-Net to represent irregular 3D point clouds of arbitrary
geometry and topology as a completely regular 2D point geometry image (PGI)
structure, in which coordinates of spatial points are captured in colors of
image pixels. \mr{Intuitively, Flattening-Net implicitly approximates a locally
smooth 3D-to-2D surface flattening process while effectively preserving
neighborhood consistency.} \mr{As a generic representation modality, PGI
inherently encodes the intrinsic property of the underlying manifold structure
and facilitates surface-style point feature aggregation.} To demonstrate its
potential, we construct a unified learning framework directly operating on PGIs
to achieve \mr{diverse types of high-level and low-level} downstream
applications driven by specific task networks, including classification,
segmentation, reconstruction, and upsampling. Extensive experiments demonstrate
that our methods perform favorably against the current state-of-the-art
competitors. We will make the code and data publicly available at
https://github.com/keeganhk/Flattening-Net.",1,0,1,0,1,0,0.83246,11.0,0.90644,92
4a1f2909-17fb-4909-acd3-62a8a45e2213,Robust Local Preserving and Global Aligning Network for Adversarial Domain Adaptation,19,0.214754,0.605125,"Unsupervised domain adaptation (UDA) requires source domain samples with
clean ground truth labels during training. Accurately labeling a large number
of source domain samples is time-consuming and laborious. An alternative is to
utilize samples with noisy labels for training. However, training with noisy
labels can greatly reduce the performance of UDA. In this paper, we address the
problem that learning UDA models only with access to noisy labels and propose a
novel method called robust local preserving and global aligning network
(RLPGA). RLPGA improves the robustness of the label noise from two aspects. One
is learning a classifier by a robust informative-theoretic-based loss function.
The other is constructing two adjacency weight matrices and two negative weight
matrices by the proposed local preserving module to preserve the local topology
structures of input data. We conduct theoretical analysis on the robustness of
the proposed RLPGA and prove that the robust informative-theoretic-based loss
and the local preserving module are beneficial to reduce the empirical risk of
the target domain. A series of empirical studies show the effectiveness of our
proposed RLPGA.",0,1,0,0,0,0,0.698241,12.0,0.880943,71
58e86c86-593f-488e-a3b4-fdaeaaf3e91c,Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,151,0.797223,0.906471,"Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning.",0,0,1,0,0,0,0.8604,6.0,0.844679,139
0c7da365-cf5f-4753-8c3c-7c710d5c073c,On Improving Cross-dataset Generalization of Deepfake Detectors,25,0.30532,0.478737,"Facial manipulation by deep fake has caused major security risks and raised
severe societal concerns. As a countermeasure, a number of deep fake detection
methods have been proposed recently. Most of them model deep fake detection as
a binary classification problem using a backbone convolutional neural network
(CNN) architecture pretrained for the task. These CNN-based methods have
demonstrated very high efficacy in deep fake detection with the Area under the
Curve (AUC) as high as 0.99. However, the performance of these methods degrades
significantly when evaluated across datasets. In this paper, we formulate deep
fake detection as a hybrid combination of supervised and reinforcement learning
(RL) to improve its cross-dataset generalization performance. The proposed
method chooses the top-k augmentations for each test sample by an RL agent in
an image-specific manner. The classification scores, obtained using CNN, of all
the augmentations of each test image are averaged together for final real or
fake classification. Through extensive experimental validation, we demonstrate
the superiority of our method over existing published research in cross-dataset
generalization of deep fake detectors, thus obtaining state-of-the-art
performance.",0,1,0,0,1,0,0.952412,6.0,0.917353,36
8300b048-5071-4e33-b1ad-24e4bd9b65d6,Improving Point Cloud Based Place Recognition with Ranking-based Loss and Large Batch Training,25,0.670959,0.62881,"The paper presents a simple and effective learning-based method for computing
a discriminative 3D point cloud descriptor for place recognition purposes.
Recent state-of-the-art methods have relatively complex architectures such as
multi-scale oyramid of point Transformers combined with a pyramid of feature
aggregation modules. Our method uses a simple and efficient 3D convolutional
feature extraction, based on a sparse voxelized representation, enhanced with
channel attention blocks. We employ recent advances in image retrieval and
propose a modified version of a loss function based on a differentiable average
precision approximation. Such loss function requires training with very large
batches for the best results. This is enabled by using multistaged
backpropagation. Experimental evaluation on the popular benchmarks proves the
effectiveness of our approach, with a consistent improvement over the state of
the art",1,1,0,0,1,0,0.981125,6.0,0.961551,35
c6f4cb98-de53-465e-8ec2-87aafb538547,Diffusion Posterior Sampling for General Noisy Inverse Problems,273,0.794413,0.999829,"Diffusion models have been recently studied as powerful generative inverse
problem solvers, owing to their high quality reconstructions and the ease of
combining existing iterative solvers. However, most works focus on solving
simple linear inverse problems in noiseless settings, which significantly
under-represents the complexity of real-world problems. In this work, we extend
diffusion solvers to efficiently handle general noisy (non)linear inverse
problems via approximation of the posterior sampling. Interestingly, the
resulting posterior sampling scheme is a blended version of diffusion sampling
with the manifold constrained gradient without a strict measurement consistency
projection step, yielding a more desirable generative path in noisy settings
compared to the previous studies. Our method demonstrates that diffusion models
can incorporate various measurement noise statistics such as Gaussian and
Poisson, and also efficiently handle noisy nonlinear inverse problems such as
Fourier phase retrieval and non-uniform deblurring. Code available at
https://github.com/DPS2022/diffusion-posterior-sampling",1,0,0,0,0,0,0.180634,12.0,0.731436,44
04c10fb0-c6e8-4d30-97b0-a5526a205c27,Non-Autoregressive Machine Translation: It's Not as Fast as it Seems,19,0.135798,0.83748,"Efficient machine translation models are commercially important as they can
increase inference speeds, and reduce costs and carbon emissions. Recently,
there has been much interest in non-autoregressive (NAR) models, which promise
faster translation. In parallel to the research on NAR models, there have been
successful attempts to create optimized autoregressive models as part of the
WMT shared task on efficient translation. In this paper, we point out flaws in
the evaluation methodology present in the literature on NAR models and we
provide a fair comparison between a state-of-the-art NAR model and the
autoregressive submissions to the shared task. We make the case for consistent
evaluation of NAR models, and also for the importance of comparing NAR models
with other widely used methods for improving efficiency. We run experiments
with a connectionist-temporal-classification-based (CTC) NAR model implemented
in C++ and compare it with AR models using wall clock times. Our results show
that, although NAR models are faster on GPUs, with small batch sizes, they are
almost always slower under more realistic usage conditions. We call for more
realistic and extensive evaluation of NAR models in future work.",1,1,0,0,0,0,0.540743,9.0,0.793302,41
74af7c23-30f9-460c-84d2-1254cf6a31a3,Generate rather than Retrieve: Large Language Models are Strong Context Generators,180,0.307569,0.815401,"Knowledge-intensive tasks, such as open-domain question answering (QA),
require access to a large amount of world or domain knowledge. A common
approach for knowledge-intensive tasks is to employ a retrieve-then-read
pipeline that first retrieves a handful of relevant contextual documents from
an external corpus such as Wikipedia and then predicts an answer conditioned on
the retrieved documents. In this paper, we present a novel perspective for
solving knowledge-intensive tasks by replacing document retrievers with large
language model generators. We call our method generate-then-read (GenRead),
which first prompts a large language model to generate contextutal documents
based on a given question, and then reads the generated documents to produce
the final answer. Furthermore, we propose a novel clustering-based prompting
method that selects distinct prompts, resulting in the generated documents that
cover different perspectives, leading to better recall over acceptable answers.
We conduct extensive experiments on three different knowledge-intensive tasks,
including open-domain QA, fact checking, and dialogue system. Notably, GenRead
achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly
outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0
and +3.9, without retrieving any documents from any external knowledge source.
Lastly, we demonstrate the model performance can be further improved by
combining retrieval and generation. Our code and generated documents can be
found at https://github.com/wyu97/GenRead.",1,0,0,0,1,1,0.609556,4.0,0.582297,66
56cc1f05-10ff-4d14-9417-8914e27121ff,Multi-Dimensional Model Compression of Vision Transformer,13,0.126826,0.33667,"Vision transformers (ViT) have recently attracted considerable attentions,
but the huge computational cost remains an issue for practical deployment.
Previous ViT pruning methods tend to prune the model along one dimension
solely, which may suffer from excessive reduction and lead to sub-optimal model
quality. In contrast, we advocate a multi-dimensional ViT compression paradigm,
and propose to harness the redundancy reduction from attention head, neuron and
sequence dimensions jointly. We firstly propose a statistical dependence based
pruning criterion that is generalizable to different dimensions for identifying
deleterious components. Moreover, we cast the multi-dimensional compression as
an optimization, learning the optimal pruning policy across the three
dimensions that maximizes the compressed model's accuracy under a computational
budget. The problem is solved by our adapted Gaussian process search with
expected improvement. Experimental results show that our method effectively
reduces the computational cost of various ViT models. For example, our method
reduces 40\% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models,
outperforming previous state-of-the-arts.",0,1,0,0,1,0,0.92135,3.0,0.774603,37
88167ebb-64f6-42ce-9ef0-994995d8a974,"YOLOX-PAI: An Improved YOLOX, Stronger and Faster than YOLOv6",3,0.00262985,0.0672534,"We develop an all-in-one computer vision toolbox named EasyCV to facilitate
the use of various SOTA computer vision methods. Recently, we add YOLOX-PAI, an
improved version of YOLOX, into EasyCV. We conduct ablation studies to
investigate the influence of some detection methods on YOLOX. We also provide
an easy use for PAI-Blade which is used to accelerate the inference process
based on BladeDISC and TensorRT. Finally, we receive 42.8 mAP on COCO dateset
within 1.0 ms on a single NVIDIA V100 GPU, which is a bit faster than YOLOv6. A
simple but efficient predictor api is also designed in EasyCV to conduct
end2end object detection. Codes and models are now available at:
https://github.com/alibaba/EasyCV.",1,1,0,0,1,0,0.5387,4.0,0.5335,8
4c2d39fa-b6f9-4a56-8662-74782d2e00fd,Legal Prompting: Teaching a Language Model to Think Like a Lawyer,36,0.62629,0.555673,"Large language models that are capable of zero or few-shot prompting
approaches have given rise to the new research area of prompt engineering.
Recent advances showed that for example Chain-of-Thought (CoT) prompts can
improve arithmetic or common sense tasks significantly. We explore how such
approaches fare with legal reasoning tasks and take the COLIEE entailment task
based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning
approaches. Our findings show that while CoT prompting and fine-tuning with
explanations approaches show improvements, the best results are produced by
prompts that are derived from specific legal reasoning techniques such as IRAC
(Issue, Rule, Application, Conclusion). Based on our experiments we improve the
2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best
system of 0.6789 accuracy with an accuracy of 0.7431.",0,1,0,0,1,0,0.91669,5.0,0.860182,23
38faf449-9f7a-482c-8845-477f07f90fa8,Deep Non-Crossing Quantiles through the Partial Derivative,10,0.141031,0.298509,"Quantile Regression (QR) provides a way to approximate a single conditional
quantile. To have a more informative description of the conditional
distribution, QR can be merged with deep learning techniques to simultaneously
estimate multiple quantiles. However, the minimisation of the QR-loss function
does not guarantee non-crossing quantiles, which affects the validity of such
predictions and introduces a critical issue in certain scenarios. In this
article, we propose a generic deep learning algorithm for predicting an
arbitrary number of quantiles that ensures the quantile monotonicity constraint
up to the machine precision and maintains its modelling performance with
respect to alternative models. The presented method is evaluated over several
real-world datasets obtaining state-of-the-art results as well as showing that
it scales to large-size data sets.",0,1,0,0,0,0,0.294173,11.0,0.757829,42
9ee64677-dae0-45cc-876d-07735c256346,On Vision Features in Multimodal Machine Translation,41,0.327491,0.882987,"Previous work on multimodal machine translation (MMT) has focused on the way
of incorporating vision features into translation but little attention is on
the quality of vision models. In this work, we investigate the impact of vision
models on MMT. Given the fact that Transformer is becoming popular in computer
vision, we experiment with various strong models (such as Vision Transformer)
and enhanced features (such as object-detection and image captioning). We
develop a selective attention model to study the patch-level contribution of an
image in MMT. On detailed probing tasks, we find that stronger vision models
are helpful for learning translation from the visual modality. Our results also
suggest the need of carefully examining MMT models, especially when current
benchmarks are small-scale and biased. Our code could be found at
\url{https://github.com/libeineu/fairseq_mmt}.",1,1,0,0,0,0,0.301579,7.0,0.623706,35
a1f11077-f013-43c0-9fd7-c3d3df061743,Reconstructed Student-Teacher and Discriminative Networks for Anomaly Detection,17,0.0719879,0.678675,"Anomaly detection is an important problem in computer vision; however, the
scarcity of anomalous samples makes this task difficult. Thus, recent anomaly
detection methods have used only normal images with no abnormal areas for
training. In this work, a powerful anomaly detection method is proposed based
on student-teacher feature pyramid matching (STPM), which consists of a student
and teacher network. Generative models are another approach to anomaly
detection. They reconstruct normal images from an input and compute the
difference between the predicted normal and the input. Unfortunately, STPM does
not have the ability to generate normal images. To improve the accuracy of
STPM, this work uses a student network, as in generative models, to reconstruct
normal features. This improves the accuracy; however, the anomaly maps for
normal images are not clean because STPM does not use anomaly images for
training, which decreases the accuracy of the image-level anomaly detection. To
further improve accuracy, a discriminative network trained with
pseudo-anomalies from anomaly maps is used in our method, which consists of two
pairs of student-teacher networks and a discriminative network. The method
displayed high accuracy on the MVTec anomaly detection dataset.",0,1,0,0,0,0,0.472106,9.0,0.771388,35
87b5ddcd-c6da-4d50-a31a-7280dd9b8161,Smart System: Joint Utility and Frequency for Pattern Classification,1,0.0016186,0.0497234,"Nowadays, the environments of smart systems for Industry 4.0 and Internet of
Things (IoT) are experiencing fast industrial upgrading. Big data technologies
such as design making, event detection, and classification are developed to
help manufacturing organizations to achieve smart systems. By applying data
analysis, the potential values of rich data can be maximized and thus help
manufacturing organizations to finish another round of upgrading. In this
paper, we propose two new algorithms with respect to big data analysis, namely
UFC$_{gen}$ and UFC$_{fast}$. Both algorithms are designed to collect three
types of patterns to help people determine the market positions for different
product combinations. We compare these algorithms on various types of datasets,
both real and synthetic. The experimental results show that both algorithms can
successfully achieve pattern classification by utilizing three different types
of interesting patterns from all candidate patterns based on user-specified
thresholds of utility and frequency. Furthermore, the list-based UFC$_{fast}$
algorithm outperforms the level-wise-based UFC$_{gen}$ algorithm in terms of
both execution time and memory consumption.",0,1,0,0,0,0,0.000406352,14.0,0.32732,53
84dc8454-fbdf-4fac-b2be-d1fb7954c345,Synthetic Dataset Generation for Adversarial Machine Learning Research,2,0.0,0.0555902,"Existing adversarial example research focuses on digitally inserted
perturbations on top of existing natural image datasets. This construction of
adversarial examples is not realistic because it may be difficult, or even
impossible, for an attacker to deploy such an attack in the real-world due to
sensing and environmental effects. To better understand adversarial examples
against cyber-physical systems, we propose approximating the real-world through
simulation. In this paper we describe our synthetic dataset generation tool
that enables scalable collection of such a synthetic dataset with realistic
adversarial examples. We use the CARLA simulator to collect such a dataset and
demonstrate simulated attacks that undergo the same environmental transforms
and processing as real-world images. Our tools have been used to collect
datasets to help evaluate the efficacy of adversarial examples, and can be
found at https://github.com/carla-simulator/carla/pull/4992.",1,1,0,1,0,0,0.359436,11.0,0.780163,24
c098db9b-1479-440c-818e-439845b85e06,Fine-tuned Sentiment Analysis of COVID-19 Vaccine-Related Social Media Data: Comparative Study,13,0.0485065,0.61043,"This study investigated and compared public sentiment related to COVID-19
vaccines expressed on two popular social media platforms, Reddit and Twitter,
harvested from January 1, 2020, to March 1, 2022. To accomplish this task, we
created a fine-tuned DistilRoBERTa model to predict sentiments of approximately
9.5 million Tweets and 70 thousand Reddit comments. To fine-tune our model, our
team manually labeled the sentiment of 3600 Tweets and then augmented our
dataset by the method of back-translation. Text sentiment for each social media
platform was then classified with our fine-tuned model using Python and the
Huggingface sentiment analysis pipeline. Our results determined that the
average sentiment expressed on Twitter was more negative (52% positive) than
positive and the sentiment expressed on Reddit was more positive than negative
(53% positive). Though average sentiment was found to vary between these social
media platforms, both displayed similar behavior related to sentiment shared at
key vaccine-related developments during the pandemic. Considering this similar
trend in shared sentiment demonstrated across social media platforms, Twitter
and Reddit continue to be valuable data sources that public health officials
can utilize to strengthen vaccine confidence and combat misinformation. As the
spread of misinformation poses a range of psychological and psychosocial risks
(anxiety, fear, etc.), there is an urgency in understanding the public
perspective and attitude toward shared falsities. Comprehensive educational
delivery systems tailored to the population's expressed sentiments that
facilitate digital literacy, health information-seeking behavior, and precision
health promotion could aid in clarifying such misinformation.",0,1,0,0,0,0,0.534486,3.0,0.374059,39
1e35ddc0-b1a0-4ccd-8c14-424b38591b34,InFIP: An Explainable DNN Intellectual Property Protection Method based on Intrinsic Features,2,0.0478639,0.461726,"Intellectual property (IP) protection for Deep Neural Networks (DNNs) has
raised serious concerns in recent years. Most existing works embed watermarks
in the DNN model for IP protection, which need to modify the model and lack of
interpretability. In this paper, for the first time, we propose an
interpretable intellectual property protection method for DNN based on
explainable artificial intelligence. Compared with existing works, the proposed
method does not modify the DNN model, and the decision of the ownership
verification is interpretable. We extract the intrinsic features of the DNN
model by using Deep Taylor Decomposition. Since the intrinsic feature is
composed of unique interpretation of the model's decision, the intrinsic
feature can be regarded as fingerprint of the model. If the fingerprint of a
suspected model is the same as the original model, the suspected model is
considered as a pirated model. Experimental results demonstrate that the
fingerprints can be successfully used to verify the ownership of the model and
the test accuracy of the model is not affected. Furthermore, the proposed
method is robust to fine-tuning attack, pruning attack, watermark overwriting
attack, and adaptive attack.",0,1,0,0,0,0,0.669105,8.0,0.811409,24
d397fd5b-2352-44ee-a2f5-226fb537eae6,SW-VAE: Weakly Supervised Learn Disentangled Representation Via Latent Factor Swapping,1,0.0161352,0.0741982,"Representation disentanglement is an important goal of representation
learning that benefits various downstream tasks. To achieve this goal, many
unsupervised learning representation disentanglement approaches have been
developed. However, the training process without utilizing any supervision
signal have been proved to be inadequate for disentanglement representation
learning. Therefore, we propose a novel weakly-supervised training approach,
named as SW-VAE, which incorporates pairs of input observations as supervision
signals by using the generative factors of datasets. Furthermore, we introduce
strategies to gradually increase the learning difficulty during training to
smooth the training process. As shown on several datasets, our model shows
significant improvement over state-of-the-art (SOTA) methods on representation
disentanglement tasks.",0,1,0,0,1,0,0.850321,10.0,0.903203,34
1ab433ad-05c4-43bd-99e9-754c24d42ca8,Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias,2,0.00800002,0.125201,"Artificial Intelligence (AI) is increasingly used to build Decision Support
Systems (DSS) across many domains. This paper describes a series of experiments
designed to observe human response to different characteristics of a DSS such
as accuracy and bias, particularly the extent to which participants rely on the
DSS, and the performance they achieve. In our experiments, participants play a
simple online game inspired by so-called ""wildcat"" (i.e., exploratory) drilling
for oil. The landscape has two layers: a visible layer describing the costs
(terrain), and a hidden layer describing the reward (oil yield). Participants
in the control group play the game without receiving any assistance, while in
treatment groups they are assisted by a DSS suggesting places to drill. For
certain treatments, the DSS does not consider costs, but only rewards, which
introduces a bias that is observable by users. Between subjects, we vary the
accuracy and bias of the DSS, and observe the participants' total score, time
to completion, the extent to which they follow or ignore suggestions. We also
measure the acceptability of the DSS in an exit survey. Our results show that
participants tend to score better with the DSS, that the score increase is due
to users following the DSS advice, and related to the difficulty of the game
and the accuracy of the DSS. We observe that this setting elicits mostly
rational behavior from participants, who place a moderate amount of trust in
the DSS and show neither algorithmic aversion (under-reliance) nor automation
bias (over-reliance).However, their stated willingness to accept the DSS in the
exit survey seems less sensitive to the accuracy of the DSS than their
behavior, suggesting that users are only partially aware of the (lack of)
accuracy of the DSS.",0,1,0,0,0,1,0.0967947,7.0,0.443696,46
329ef5aa-4e5b-427c-abda-aca4001fb708,Towards Human-Agent Communication via the Information Bottleneck Principle,14,0.185862,0.953961,"Emergent communication research often focuses on optimizing task-specific
utility as a driver for communication. However, human languages appear to
evolve under pressure to efficiently compress meanings into communication
signals by optimizing the Information Bottleneck tradeoff between
informativeness and complexity. In this work, we study how trading off these
three factors -- utility, informativeness, and complexity -- shapes emergent
communication, including compared to human communication. To this end, we
propose Vector-Quantized Variational Information Bottleneck (VQ-VIB), a method
for training neural agents to compress inputs into discrete signals embedded in
a continuous space. We train agents via VQ-VIB and compare their performance to
previously proposed neural architectures in grounded environments and in a
Lewis reference game. Across all neural architectures and settings, taking into
account communicative informativeness benefits communication convergence rates,
and penalizing communicative complexity leads to human-like lexicon sizes while
maintaining high utility. Additionally, we find that VQ-VIB outperforms other
discrete communication methods. This work demonstrates how fundamental
principles that are believed to characterize human language evolution may
inform emergent communication in artificial agents.",0,0,0,0,0,0,0.357266,9.0,0.730464,36
599ac3eb-e2fa-45ea-aab8-be99a9c51d21,Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology,11,0.121053,0.533482,"Histopathology whole slide images (WSIs) can reveal significant
inter-hospital variability such as illumination, color or optical artifacts.
These variations, caused by the use of different scanning protocols across
medical centers (staining, scanner), can strongly harm algorithms
generalization on unseen protocols. This motivates development of new methods
to limit such drop of performances. In this paper, to enhance robustness on
unseen target protocols, we propose a new test-time data augmentation based on
multi domain image-to-image translation. It allows to project images from
unseen protocol into each source domain before classifying them and ensembling
the predictions. This test-time augmentation method results in a significant
boost of performances for domain generalization. To demonstrate its
effectiveness, our method has been evaluated on 2 different histopathology
tasks where it outperforms conventional domain generalization, standard H&E
specific color augmentation/normalization and standard test-time augmentation
techniques. Our code is publicly available at
https://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling.",1,1,0,0,0,0,0.783777,7.0,0.830968,24
7e9ac318-0098-4784-85f4-89dbdd907dfb,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,42,0.171288,0.525516,"While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method.",1,0,0,0,0,1,0.617643,6.0,0.7252,44
1c7cbd4c-2226-48c7-8aa7-70291928222c,AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models,11,0.2763,0.849964,"The objective of pre-trained language models is to learn contextual
representations of textual data. Pre-trained language models have become
mainstream in natural language processing and code modeling. Using probes, a
technique to study the linguistic properties of hidden vector spaces, previous
works have shown that these pre-trained language models encode simple
linguistic properties in their hidden representations. However, none of the
previous work assessed whether these models encode the whole grammatical
structure of a programming language. In this paper, we prove the existence of a
syntactic subspace, lying in the hidden representations of pre-trained language
models, which contain the syntactic information of the programming language. We
show that this subspace can be extracted from the models' representations and
define a novel probing method, the AST-Probe, that enables recovering the whole
abstract syntax tree (AST) of an input code snippet. In our experimentations,
we show that this syntactic subspace exists in five state-of-the-art
pre-trained language models. In addition, we highlight that the middle layers
of the models are the ones that encode most of the AST information. Finally, we
estimate the optimal size of this syntactic subspace and show that its
dimension is substantially lower than those of the models' representation
spaces. This suggests that pre-trained language models use a small part of
their representation spaces to encode syntactic information of the programming
languages.",0,0,1,0,0,0,0.94748,5.0,0.894241,45
2678a26d-5149-45a1-a12e-7879ae42f59f,FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information,1,0.0334853,0.132208,"Online forms are widely used to collect data from human and have a
multi-billion market. Many software products provide online services for
creating semi-structured forms where questions and descriptions are organized
by pre-defined structures. However, the design and creation process of forms is
still tedious and requires expert knowledge. To assist form designers, in this
work we present FormLM to model online forms (by enhancing pre-trained language
model with form structural information) and recommend form creation ideas
(including question / options recommendations and block type suggestion). For
model training and evaluation, we collect the first public online form dataset
with 62K online forms. Experiment results show that FormLM significantly
outperforms general-purpose language models on all tasks, with an improvement
by 4.71 on Question Recommendation and 10.6 on Block Type Suggestion in terms
of ROUGE-1 and Macro-F1, respectively.",0,1,0,1,1,0,0.869262,7.0,0.871549,55
94a66700-df7d-4113-9609-0a245cbb3a2d,Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling,8,0.383484,0.509038,"Recent advances in federated learning have demonstrated its promising
capability to learn on decentralized datasets. However, a considerable amount
of work has raised concerns due to the potential risks of adversaries
participating in the framework to poison the global model for an adversarial
purpose. This paper investigates the feasibility of model poisoning for
backdoor attacks through rare word embeddings of NLP models. In text
classification, less than 1% of adversary clients suffices to manipulate the
model output without any drop in the performance on clean sentences. For a less
complex dataset, a mere 0.1% of adversary clients is enough to poison the
global model effectively. We also propose a technique specialized in the
federated learning scheme called Gradient Ensemble, which enhances the backdoor
performance in all our experimental settings.",0,1,0,0,0,0,0.988284,6.0,0.980456,37
5e32f7a2-6e43-4de8-a266-adaa54b43f87,HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification,22,0.898371,0.761262,"Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification due to its complex label hierarchy. Recently, the
pretrained language models (PLM)have been widely adopted in HTC through a
fine-tuning paradigm. However, in this paradigm, there exists a huge gap
between the classification tasks with sophisticated label hierarchy and the
masked language model (MLM) pretraining tasks of PLMs and thus the potentials
of PLMs can not be fully tapped. To bridge the gap, in this paper, we propose
HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label
MLM perspective. Specifically, we construct a dynamic virtual template and
label words that take the form of soft prompts to fuse the label hierarchy
knowledge and introduce a zero-bounded multi-label cross entropy loss to
harmonize the objectives of HTC and MLM. Extensive experiments show HPT
achieves state-of-the-art performances on 3 popular HTC datasets and is adept
at handling the imbalance and low resource situations. Our code is available at
https://github.com/wzh9969/HPT.",0,0,0,0,1,0,0.985128,6.0,0.97127,29
65f33aa7-5cfb-48a6-baf5-7afbb7eeea11,Molecular Substructure-Aware Network for Drug-Drug Interaction Prediction,10,0.129392,0.609175,"Concomitant administration of drugs can cause drug-drug interactions (DDIs).
Some drug combinations are beneficial, but other ones may cause negative
effects which are previously unrecorded. Previous works on DDI prediction
usually rely on hand-engineered domain knowledge, which is laborious to obtain.
In this work, we propose a novel model, Molecular Substructure-Aware Network
(MSAN), to effectively predict potential DDIs from molecular structures of drug
pairs. We adopt a Transformer-like substructure extraction module to acquire a
fixed number of representative vectors that are associated with various
substructure patterns of the drug molecule. Then, interaction strength between
the two drugs' substructures will be captured by a similarity-based interaction
module. We also perform a substructure dropping augmentation before graph
encoding to alleviate overfitting. Experimental results from a real-world
dataset reveal that our proposed model achieves the state-of-the-art
performance. We also show that the predictions of our model are highly
interpretable through a case study.",1,1,0,0,1,0,0.265741,10.0,0.72158,20
3a08c969-9289-49fa-8a33-7d3df6f1e17d,AI Art in Architecture,24,0.0426522,0.38374,"Recent diffusion-based AI art platforms are able to create impressive images
from simple text descriptions. This makes them powerful tools for concept
design in any discipline that requires creativity in visual design tasks. This
is also true for early stages of architectural design with multiple stages of
ideation, sketching and modelling. In this paper, we investigate how applicable
diffusion-based models already are to these tasks. We research the
applicability of the platforms Midjourney, DALL-E 2 and StableDiffusion to a
series of common use cases in architectural design to determine which are
already solvable or might soon be. We also analyze how they are already being
used by analyzing a data set of 40 million Midjourney queries with NLP methods
to extract common usage patterns. With this insights we derived a workflow to
interior and exterior design that combines the strengths of the individual
platforms.",0,1,0,0,0,0,0.899882,3.0,0.74136,23
e5b5680d-39da-48af-9856-69c6fcc8c7ad,R2-Trans:Fine-Grained Visual Categorization with Redundancy Reduction,8,0.0729088,0.467106,"Fine-grained visual categorization (FGVC) aims to discriminate similar
subcategories, whose main challenge is the large intraclass diversities and
subtle inter-class differences. Existing FGVC methods usually select
discriminant regions found by a trained model, which is prone to neglect other
potential discriminant information. On the other hand, the massive interactions
between the sequence of image patches in ViT make the resulting class-token
contain lots of redundant information, which may also impacts FGVC performance.
In this paper, we present a novel approach for FGVC, which can simultaneously
make use of partial yet sufficient discriminative information in environmental
cues and also compress the redundant information in class-token with respect to
the target. Specifically, our model calculates the ratio of high-weight regions
in a batch, adaptively adjusts the masking threshold and achieves moderate
extraction of background information in the input space. Moreover, we also use
the Information Bottleneck~(IB) approach to guide our network to learn a
minimum sufficient representations in the feature space. Experimental results
on three widely-used benchmark datasets verify that our approach can achieve
outperforming performance than other state-of-the-art approaches and baseline
models.",0,1,0,0,1,0,0.254772,8.0,0.645826,52
533a90b8-b823-4cef-beba-66303dc4f953,Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes,21,0.0929942,0.727531,"We present a novel approach to efficiently compute tight non-convex
enclosures of the image through neural networks with ReLU, sigmoid, or
hyperbolic tangent activation functions. In particular, we abstract the
input-output relation of each neuron by a polynomial approximation, which is
evaluated in a set-based manner using polynomial zonotopes. While our approach
can also can be beneficial for open-loop neural network verification, our main
application is reachability analysis of neural network controlled systems,
where polynomial zonotopes are able to capture the non-convexity caused by the
neural network as well as the system dynamics. This results in a superior
performance compared to other methods, as we demonstrate on various benchmarks.",0,0,0,0,0,0,0.105053,7.0,0.456037,49
89e1a713-8216-4e65-8345-42f50733f395,Multi-focus thermal image fusion,23,0.477003,0.513198,"This paper proposes a novel algorithm for multi-focus thermal image fusion.
The algorithm is based on local activity analysis and advanced pre-selection of
images into fusion process. The algorithm improves the object temperature
measurement error up to 5 Celsius degrees. The proposed algorithm is evaluated
by half total error rate, root mean squared error, cross correlation and visual
inspection. To the best of our knowledge, this is the first work devoted to
multi-focus thermal image fusion. For testing of proposed algorithm we acquire
six thermal image set with objects at different focal depth.",0,1,1,0,0,0,0.12389,16.0,0.772975,22
b7635038-bd15-4dad-8e90-a6785909b2dc,On the generalization capabilities of FSL methods through domain adaptation: a case study in endoscopic kidney stone image classification,5,0.0227537,0.129434,"Deep learning has shown great promise in diverse areas of computer vision,
such as image classification, object detection and semantic segmentation, among
many others. However, as it has been repeatedly demonstrated, deep learning
methods trained on a dataset do not generalize well to datasets from other
domains or even to similar datasets, due to data distribution shifts. In this
work, we propose the use of a meta-learning based few-shot learning approach to
alleviate these problems. In order to demonstrate its efficacy, we use two
datasets of kidney stones samples acquired with different endoscopes and
different acquisition conditions. The results show how such methods are indeed
capable of handling domain-shifts by attaining an accuracy of 74.38% and 88.52%
in the 5-way 5-shot and 5-way 20-shot settings respectively. Instead, in the
same dataset, traditional Deep Learning (DL) methods attain only an accuracy of
45%.",0,1,0,0,0,0,0.255378,7.0,0.595624,50
5c70ccf9-0f01-45b7-a9eb-edd7b7fd55d2,Context-Dependent Anomaly Detection with Knowledge Graph Embedding Models,1,0.0217572,0.126169,"Increasing the semantic understanding and contextual awareness of machine
learning models is important for improving robustness and reducing
susceptibility to data shifts. In this work, we leverage contextual awareness
for the anomaly detection problem. Although graphed-based anomaly detection has
been widely studied, context-dependent anomaly detection is an open problem and
without much current research. We develop a general framework for converting a
context-dependent anomaly detection problem to a link prediction problem,
allowing well-established techniques from this domain to be applied. We
implement a system based on our framework that utilizes knowledge graph
embedding models and demonstrates the ability to detect outliers using context
provided by a semantic knowledge base. We show that our method can detect
context-dependent anomalies with a high degree of accuracy and show that
current object detectors can detect enough classes to provide the needed
context for good performance within our example domain.",0,1,1,0,0,0,0.841417,11.0,0.909194,30
82206b4a-a332-4b86-8520-4bff11a6a242,Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation From Monocular RGB Image,14,0.117627,0.746049,"Recently, RGBD-based category-level 6D object pose estimation has achieved
promising improvement in performance, however, the requirement of depth
information prohibits broader applications. In order to relieve this problem,
this paper proposes a novel approach named Object Level Depth reconstruction
Network (OLD-Net) taking only RGB images as input for category-level 6D object
pose estimation. We propose to directly predict object-level depth from a
monocular RGB image by deforming the category-level shape prior into
object-level depth and the canonical NOCS representation. Two novel modules
named Normalized Global Position Hints (NGPH) and Shape-aware Decoupled Depth
Reconstruction (SDDR) module are introduced to learn high fidelity object-level
depth and delicate shape representations. At last, the 6D object pose is solved
by aligning the predicted canonical representation with the back-projected
object-level depth. Extensive experiments on the challenging CAMERA25 and
REAL275 datasets indicate that our model, though simple, achieves
state-of-the-art performance.",0,1,0,0,1,0,0.638297,6.0,0.734559,39
0da71ac1-12dd-4616-b624-3f90d09a7a84,On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence,60,0.465011,0.98567,"Ten years into the revival of deep networks and artificial intelligence, we
propose a theoretical framework that sheds light on understanding deep networks
within a bigger picture of Intelligence in general. We introduce two
fundamental principles, Parsimony and Self-consistency, that address two
fundamental questions regarding Intelligence: what to learn and how to learn,
respectively. We believe the two principles are the cornerstones for the
emergence of Intelligence, artificial or natural. While these two principles
have rich classical roots, we argue that they can be stated anew in entirely
measurable and computable ways. More specifically, the two principles lead to
an effective and efficient computational framework, compressive closed-loop
transcription, that unifies and explains the evolution of modern deep networks
and many artificial intelligence practices. While we mainly use modeling of
visual data as an example, we believe the two principles will unify
understanding of broad families of autonomous intelligent systems and provide a
framework for understanding the brain.",0,0,0,0,0,1,0.581249,12.0,0.854321,150
1ae53a9d-d6b9-4896-b5a5-c2c9c4658336,A case for using rotation invariant features in state of the art feature matchers,24,0.16559,0.893859,"The aim of this paper is to demonstrate that a state of the art feature
matcher (LoFTR) can be made more robust to rotations by simply replacing the
backbone CNN with a steerable CNN which is equivariant to translations and
image rotations. It is experimentally shown that this boost is obtained without
reducing performance on ordinary illumination and viewpoint matching sequences.",1,1,0,0,0,0,0.235142,9.0,0.674893,57
37470a62-4931-4322-bb3d-f5a7ed2b0fcd,Differentiable Point-Based Radiance Fields for Efficient View Synthesis,49,0.299716,0.556855,"We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.",0,0,0,0,1,0,0.745287,5.0,0.740727,48
5cb74da3-a2d9-4bfb-aacf-ef7cc1f8d8df,Classification on Sentence Embeddings for Legal Assistance,1,0.00703667,0.0355164,"Legal proceedings take plenty of time and also cost a lot. The lawyers have
to do a lot of work in order to identify the different sections of prior cases
and statutes. The paper tries to solve the first tasks in AILA2021 (Artificial
Intelligence for Legal Assistance) that will be held in FIRE2021 (Forum for
Information Retrieval Evaluation). The task is to semantically segment the
document into different assigned one of the 7 predefined labels or ""rhetorical
roles."" The paper uses BERT to obtain the sentence embeddings from a sentence,
and then a linear classifier is used to output the final prediction. The
experiments show that when more weightage is assigned to the class with the
highest frequency, the results are better than those when more weightage is
given to the class with a lower frequency. In task 1, the team legalNLP
obtained a F1 score of 0.22.",1,1,0,0,0,0,0.101098,9.0,0.572412,11
6537b083-6490-4909-919a-0ebd3d10b06a,News Category Dataset,81,0.930125,0.996538,"People rely on news to know what is happening around the world and inform
their daily lives. In today's world, when the proliferation of fake news is
rampant, having a large-scale and high-quality source of authentic news
articles with the published category information is valuable to learning
authentic news' Natural Language syntax and semantics. As part of this work, we
present a News Category Dataset that contains around 210k news headlines from
the year 2012 to 2022 obtained from HuffPost, along with useful metadata to
enable various NLP tasks. In this paper, we also produce some novel insights
from the dataset and describe various existing and potential applications of
our dataset.",0,1,0,1,0,1,0.65018,3.0,0.479891,5
04c53430-f2d7-4158-bd41-adbda4fa2430,3D Convolutional Networks for Action Recognition: Application to Sport Gesture Recognition,2,0.00604067,0.0281267,"3D convolutional networks is a good means to perform tasks such as video
segmentation into coherent spatio-temporal chunks and classification of them
with regard to a target taxonomy. In the chapter we are interested in the
classification of continuous video takes with repeatable actions, such as
strokes of table tennis. Filmed in a free marker less ecological environment,
these videos represent a challenge from both segmentation and classification
point of view. The 3D convnets are an efficient tool for solving these problems
with window-based approaches.",0,1,0,0,0,0,1.0,-7.0,0.624347,74
9caa65f1-a25c-4bda-b1b3-dc1f06c4dc9e,De-biasing facial detection system using VAE,1,0.013977,0.060415,"Bias in AI/ML-based systems is a ubiquitous problem and bias in AI/ML systems
may negatively impact society. There are many reasons behind a system being
biased. The bias can be due to the algorithm we are using for our problem or
may be due to the dataset we are using, having some features over-represented
in it. In the face detection system bias due to the dataset is majorly seen.
Sometimes models learn only features that are over-represented in data and
ignore rare features from data which results in being biased toward those
over-represented features. In real life, these biased systems are dangerous to
society. The proposed approach uses generative models which are best suited for
learning underlying features(latent variables) from the dataset and by using
these learned features models try to reduce the threats which are there due to
bias in the system. With the help of an algorithm, the bias present in the
dataset can be removed. And then we train models on two datasets and compare
the results.",0,1,0,0,0,0,0.194115,7.0,0.551032,8
9d100180-e86a-4856-9578-19840bc524f9,Tailoring Self-Supervision for Supervised Learning,8,0.0250162,0.222067,"Recently, it is shown that deploying a proper self-supervision is a
prospective way to enhance the performance of supervised learning. Yet, the
benefits of self-supervision are not fully exploited as previous pretext tasks
are specialized for unsupervised representation learning. To this end, we begin
by presenting three desirable properties for such auxiliary tasks to assist the
supervised objective. First, the tasks need to guide the model to learn rich
features. Second, the transformations involved in the self-supervision should
not significantly alter the training distribution. Third, the tasks are
preferred to be light and generic for high applicability to prior arts.
Subsequently, to show how existing pretext tasks can fulfill these and be
tailored for supervised learning, we propose a simple auxiliary
self-supervision task, predicting localizable rotation (LoRot). Our exhaustive
experiments validate the merits of LoRot as a pretext task tailored for
supervised learning in terms of robustness and generalization capability. Our
code is available at https://github.com/wjun0830/Localizable-Rotation.",1,0,0,0,0,0,0.741977,8.0,0.836769,70
af59f835-336c-468e-b1c9-177345400bd4,Uncertain Case Identifiers in Process Mining: A User Study of the Event-Case Correlation Problem on Click Data,2,0.0598976,0.146355,"Among the many sources of event data available today, a prominent one is user
interaction data. User activity may be recorded during the use of an
application or website, resulting in a type of user interaction data often
called click data. An obstacle to the analysis of click data using process
mining is the lack of a case identifier in the data. In this paper, we show a
case and user study for event-case correlation on click data, in the context of
user interaction events from a mobility sharing company. To reconstruct the
case notion of the process, we apply a novel method to aggregate user
interaction data in separate user sessions-interpreted as cases-based on neural
networks. To validate our findings, we qualitatively discuss the impact of
process mining analyses on the resulting well-formed event log through
interviews with process experts.",0,1,0,0,0,0,0.235333,6.0,0.512495,19
30a127d9-abd7-4842-91cf-de602cfb1eb8,AnyMorph: Learning Transferable Polices By Inferring Agent Morphology,14,0.380544,0.354652,"The prototypical approach to reinforcement learning involves training
policies tailored to a particular agent from scratch for every new morphology.
Recent work aims to eliminate the re-training of policies by investigating
whether a morphology-agnostic policy, trained on a diverse set of agents with
similar task objectives, can be transferred to new agents with unseen
morphologies without re-training. This is a challenging problem that required
previous approaches to use hand-designed descriptions of the new agent's
morphology. Instead of hand-designing this description, we propose a
data-driven method that learns a representation of morphology directly from the
reinforcement learning objective. Ours is the first reinforcement learning
algorithm that can train a policy to generalize to new agent morphologies
without requiring a description of the agent's morphology in advance. We
evaluate our approach on the standard benchmark for agent-agnostic control, and
improve over the current state of the art in zero-shot generalization to new
agents. Importantly, our method attains good performance without an explicit
description of morphology.",0,0,0,0,1,0,0.947789,7.0,0.924743,35
151faea4-77ac-45cf-a264-7d5413b209bb,Shape-Guided Diffusion with Inside-Outside Attention,26,0.183562,0.765696,"We introduce precise object silhouette as a new form of user control in
text-to-image diffusion models, which we dub Shape-Guided Diffusion. Our
training-free method uses an Inside-Outside Attention mechanism during the
inversion and generation process to apply a shape constraint to the cross- and
self-attention maps. Our mechanism designates which spatial region is the
object (inside) vs. background (outside) then associates edits to the correct
region. We demonstrate the efficacy of our method on the shape-guided editing
task, where the model must replace an object according to a text prompt and
object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and
achieve SOTA results in shape faithfulness without a degradation in text
alignment or image realism according to both automatic metrics and annotator
ratings. Our data and code will be made available at
https://shape-guided-diffusion.github.io.",1,1,1,0,1,0,0.97115,3.0,0.885428,48
20ab1573-960d-4e49-9b3e-edfdfd3e7b53,Unifying Approaches in Active Learning and Active Sampling via Fisher Information and Information-Theoretic Quantities,10,0.0236031,0.443142,"Recently proposed methods in data subset selection, that is active learning
and active sampling, use Fisher information, Hessians, similarity matrices
based on gradients, and gradient lengths to estimate how informative data is
for a model's training. Are these different approaches connected, and if so,
how? We revisit the fundamentals of Bayesian optimal experiment design and show
that these recently proposed methods can be understood as approximations to
information-theoretic quantities: among them, the mutual information between
predictions and model parameters, known as expected information gain or BALD in
machine learning, and the mutual information between predictions of acquisition
candidates and test samples, known as expected predictive information gain. We
develop a comprehensive set of approximations using Fisher information and
observed information and derive a unified framework that connects seemingly
disparate literature. Although Bayesian methods are often seen as separate from
non-Bayesian ones, the sometimes fuzzy notion of ""informativeness"" expressed in
various non-Bayesian objectives leads to the same couple of information
quantities, which were, in principle, already known by Lindley (1956) and
MacKay (1992).",1,0,0,0,0,0,0.129162,7.0,0.487461,41
8e0f91b8-c2ac-4e72-a793-3713a49d83da,Domain-Agnostic Prior for Transfer Semantic Segmentation,21,0.193692,0.4126,"Unsupervised domain adaptation (UDA) is an important topic in the computer
vision community. The key difficulty lies in defining a common property between
the source and target domains so that the source-domain features can align with
the target-domain semantics. In this paper, we present a simple and effective
mechanism that regularizes cross-domain representation learning with a
domain-agnostic prior (DAP) that constrains the features extracted from source
and target domains to align with a domain-agnostic space. In practice, this is
easily implemented as an extra loss term that requires a little extra costs. In
the standard evaluation protocol of transferring synthesized data to real data,
we validate the effectiveness of different types of DAP, especially that
borrowed from a text embedding model that shows favorable performance beyond
the state-of-the-art UDA approaches in terms of segmentation accuracy. Our
research reveals that UDA benefits much from better proxies, possibly from
other data modalities.",0,1,0,0,1,0,0.892799,7.0,0.884847,65
1c30df5a-495c-4250-a35f-6fdfb8db35cc,Enhancing Crisis-Related Tweet Classification with Entity-Masked Language Modeling and Multi-Task Learning,3,0.100632,0.554435,"Social media has become an important information source for crisis management
and provides quick access to ongoing developments and critical information.
However, classification models suffer from event-related biases and highly
imbalanced label distributions which still poses a challenging task. To address
these challenges, we propose a combination of entity-masked language modeling
and hierarchical multi-label classification as a multi-task learning problem.
We evaluate our method on tweets from the TREC-IS dataset and show an absolute
performance gain w.r.t. F1-score of up to 10% for actionable information types.
Moreover, we found that entity-masking reduces the effect of overfitting to
in-domain events and enables improvements in cross-event generalization.",0,1,0,0,0,0,0.479182,7.0,0.709056,49
c0dd0f00-159d-4e01-8159-c079f56880eb,TVLT: Textless Vision-Language Transformer,19,0.158082,0.365865,"In this work, we present the Textless Vision-Language Transformer (TVLT),
where homogeneous transformer blocks take raw visual and audio inputs for
vision-and-language representation learning with minimal modality-specific
design, and do not use text-specific modules such as tokenization or automatic
speech recognition (ASR). TVLT is trained by reconstructing masked patches of
continuous video frames and audio spectrograms (masked autoencoding) and
contrastive modeling to align video and audio. TVLT attains performance
comparable to its text-based counterpart on various multimodal tasks, such as
visual question answering, image retrieval, video retrieval, and multimodal
sentiment analysis, with 28x faster inference speed and only 1/3 of the
parameters. Our findings suggest the possibility of learning compact and
efficient visual-linguistic representations from low-level visual and audio
signals without assuming the prior existence of text. Our code and checkpoints
are available at: https://github.com/zinengtang/TVLT",1,0,0,0,0,0,0.775137,7.0,0.827265,98
06676330-c1cb-49ed-8134-8fd33766affe,Cross-modal Contrastive Learning for Speech Translation,60,0.721807,0.93309,"How can we learn unified representations for spoken utterances and their
written text? Learning similar representations for semantically similar speech
and text is important for speech translation. To this end, we propose ConST, a
cross-modal contrastive learning method for end-to-end speech-to-text
translation. We evaluate ConST and a variety of previous baselines on a popular
benchmark MuST-C. Experiments show that the proposed ConST consistently
outperforms the previous methods on, and achieves an average BLEU of 29.4. The
analysis further verifies that ConST indeed closes the representation gap of
different modalities -- its learned representation improves the accuracy of
cross-modal speech-text retrieval from 4% to 88%. Code and models are available
at https://github.com/ReneeYe/ConST.",1,1,0,0,1,0,0.858621,5.0,0.812325,91
50ac2ecd-9291-4300-b749-9d7be7d537d5,ComMU: Dataset for Combinatorial Music Generation,7,0.0828986,0.677715,"Commercial adoption of automatic music composition requires the capability of
generating diverse and high-quality music suitable for the desired context
(e.g., music for romantic movies, action games, restaurants, etc.). In this
paper, we introduce combinatorial music generation, a new task to create
varying background music based on given conditions. Combinatorial music
generation creates short samples of music with rich musical metadata, and
combines them to produce a complete music. In addition, we introduce ComMU, the
first symbolic music dataset consisting of short music samples and their
corresponding 12 musical metadata for combinatorial music generation. Notable
properties of ComMU are that (1) dataset is manually constructed by
professional composers with an objective guideline that induces regularity, and
(2) it has 12 musical metadata that embraces composers' intentions. Our results
show that we can generate diverse high-quality music only with metadata, and
that our unique metadata such as track-role and extended chord quality improves
the capacity of the automatic composition. We highly recommend watching our
video before reading the paper (https://pozalabs.github.io/ComMU).",1,1,1,1,0,0,0.246199,8.0,0.640867,57
a49083ab-b42e-47a2-8dff-0865b034da64,Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections,5,0.161038,0.496928,"We address the problem of detecting tree rings in microscopy images of shrub
cross sections. This can be regarded as a special case of the instance
segmentation task with several unique challenges such as the concentric
circular ring shape of the objects and high precision requirements that result
in inadequate performance of existing methods. We propose a new iterative
method which we term Iterative Next Boundary Detection (INBD). It intuitively
models the natural growth direction, starting from the center of the shrub
cross section and detecting the next ring boundary in each iteration step. In
our experiments, INBD shows superior performance to generic instance
segmentation methods and is the only one with a built-in notion of
chronological order. Our dataset and source code are available at
http://github.com/alexander-g/INBD.",1,0,0,0,0,0,0.604012,11.0,0.846735,29
5759f81f-3709-4018-aba5-4c762c44e404,DrapeNet: Garment Generation and Self-Supervised Draping,10,0.357448,0.114444,"Recent approaches to drape garments quickly over arbitrary human bodies
leverage self-supervision to eliminate the need for large training sets.
However, they are designed to train one network per clothing item, which
severely limits their generalization abilities. In our work, we rely on
self-supervision to train a single network to drape multiple garments. This is
achieved by predicting a 3D deformation field conditioned on the latent codes
of a generative network, which models garments as unsigned distance fields. Our
pipeline can generate and drape previously unseen garments of any topology,
whose shape can be edited by manipulating their latent codes. Being fully
differentiable, our formulation makes it possible to recover accurate 3D models
of garments from partial observations -- images or 3D scans -- via gradient
descent. Our code is publicly available at
https://github.com/liren2515/DrapeNet .",1,0,0,0,0,0,0.864104,6.0,0.84694,66
097443f4-45fd-437d-b5f5-49154af7177e,PixelGame: Infrared small target segmentation as a Nash equilibrium,5,0.0382303,0.580486,"A key challenge of infrared small target segmentation (ISTS) is to balance
false negative pixels (FNs) and false positive pixels (FPs). Traditional
methods combine FNs and FPs into a single objective by weighted sum, and the
optimization process is decided by one actor. Minimizing FNs and FPs with the
same strategy leads to antagonistic decisions. To address this problem, we
propose a competitive game framework (pixelGame) from a novel perspective for
ISTS. In pixelGame, FNs and FPs are controlled by different player whose goal
is to minimize their own utility function. FNs-player and FPs-player are
designed with different strategies: One is to minimize FNs and the other is to
minimize FPs. The utility function drives the evolution of the two participants
in competition. We consider the Nash equilibrium of pixelGame as the optimal
solution. In addition, we propose maximum information modulation (MIM) to
highlight the tar-get information. MIM effectively focuses on the salient
region including small targets. Extensive experiments on two standard public
datasets prove the effectiveness of our method. Compared with other
state-of-the-art methods, our method achieves better performance in terms of
F1-measure (F1) and the intersection of union (IoU).",0,0,0,0,1,0,0.131282,9.0,0.6033,76
5f5a6108-7f14-4735-89f2-8e59d6091370,Co-evolving morphology and control of soft robots using a single genome,6,0.255639,0.793808,"When simulating soft robots, both their morphology and their controllers play
important roles in task performance. This paper introduces a new method to
co-evolve these two components in the same process. We do that by using the
hyperNEAT algorithm to generate two separate neural networks in one pass, one
responsible for the design of the robot body structure and the other for the
control of the robot.
  The key difference between our method and most existing approaches is that it
does not treat the development of the morphology and the controller as separate
processes. Similar to nature, our method derives both the ""brain"" and the
""body"" of an agent from a single genome and develops them together. While our
approach is more realistic and doesn't require an arbitrary separation of
processes during evolution, it also makes the problem more complex because the
search space for this single genome becomes larger and any mutation to the
genome affects ""brain"" and the ""body"" at the same time.
  Additionally, we present a new speciation function that takes into
consideration both the genotypic distance, as is the standard for NEAT, and the
similarity between robot bodies. By using this function, agents with very
different bodies are more likely to be in different species, this allows robots
with different morphologies to have more specialized controllers since they
won't crossover with other robots that are too different from them.
  We evaluate the presented methods on four tasks and observe that even if the
search space was larger, having a single genome makes the evolution process
converge faster when compared to having separated genomes for body and control.
The agents in our population also show morphologies with a high degree of
regularity and controllers capable of coordinating the voxels to produce the
necessary movements.",1,0,1,0,0,0,0.219974,17.0,0.823409,14
ae0c3382-fe9b-4593-8a09-53cf9dcd6db5,DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling,3,0.0105902,0.0732905,"Retrieval-based conversational systems learn to rank response candidates for
a given dialogue context by computing the similarity between their vector
representations. However, training on a single textual form of the multi-turn
context limits the ability of a model to learn representations that generalize
to natural perturbations seen during inference. In this paper we propose a
framework that incorporates augmented versions of a dialogue context into the
learning objective. We utilize contrastive learning as an auxiliary objective
to learn robust dialogue context representations that are invariant to
perturbations injected through the augmentation method. We experiment with four
benchmark dialogue datasets and demonstrate that our framework combines well
with existing augmentation methods and can significantly improve over baseline
BERT-based ranking architectures. Furthermore, we propose a novel data
augmentation method, ConMix, that adds token level perturbations through
stochastic mixing of tokens from other contexts in the batch. We show that our
proposed augmentation method outperforms previous data augmentation approaches,
and provides dialogue representations that are more robust to common
perturbations seen during inference.",0,1,0,1,0,0,0.268861,5.0,0.445899,40
be2920bb-e9d5-4e1d-9a63-91573c2f1b7b,RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks,18,0.117829,0.729292,"Temporal/spatial receptive fields of models play an important role in
sequential/spatial tasks. Large receptive fields facilitate long-term
relations, while small receptive fields help to capture the local details.
Existing methods construct models with hand-designed receptive fields in
layers. Can we effectively search for receptive field combinations to replace
hand-designed patterns? To answer this question, we propose to find better
receptive field combinations through a global-to-local search scheme. Our
search scheme exploits both global search to find the coarse combinations and
local search to get the refined receptive field combinations further. The
global search finds possible coarse combinations other than human-designed
patterns. On top of the global search, we propose an expectation-guided
iterative local search scheme to refine combinations effectively. Our RF-Next
models, plugging receptive field search to various models, boost the
performance on many tasks, e.g., temporal action segmentation, object
detection, instance segmentation, and speech synthesis. The source code is
publicly available on http://mmcheng.net/rfnext.",1,0,0,0,0,1,0.487044,9.0,0.776271,154
3d6f4750-1f41-46ba-8b9f-aad3c2e068e7,CEU-Net: Ensemble Semantic Segmentation of Hyperspectral Images Using Clustering,5,0.164141,0.207823,"Most semantic segmentation approaches of Hyperspectral images (HSIs) use and
require preprocessing steps in the form of patching to accurately classify
diversified land cover in remotely sensed images. These approaches use patching
to incorporate the rich neighborhood information in images and exploit the
simplicity and segmentability of the most common HSI datasets. In contrast,
most landmasses in the world consist of overlapping and diffused classes,
making neighborhood information weaker than what is seen in common HSI
datasets. To combat this issue and generalize the segmentation models to more
complex and diverse HSI datasets, in this work, we propose our novel flagship
model: Clustering Ensemble U-Net (CEU-Net). CEU-Net uses the ensemble method to
combine spectral information extracted from convolutional neural network (CNN)
training on a cluster of landscape pixels. Our CEU-Net model outperforms
existing state-of-the-art HSI semantic segmentation methods and gets
competitive performance with and without patching when compared to baseline
models. We highlight CEU-Net's high performance across Botswana, KSC, and
Salinas datasets compared to HybridSN and AeroRIT methods.",1,1,0,0,1,0,0.55623,8.0,0.772857,52
3c3dbc64-abf5-487d-8e7a-815e18e0df40,Policy Regularization for Legible Behavior,2,0.0246244,0.056174,"In Reinforcement Learning interpretability generally means to provide insight
into the agent's mechanisms such that its decisions are understandable by an
expert upon inspection. This definition, with the resulting methods from the
literature, may however fall short for online settings where the fluency of
interactions prohibits deep inspections of the decision-making algorithm. To
support interpretability in online settings it is useful to borrow from the
Explainable Planning literature methods that focus on the legibility of the
agent, by making its intention easily discernable in an observer model. As we
propose in this paper, injecting legible behavior inside an agent's policy
doesn't require modify components of its learning algorithm. Rather, the
agent's optimal policy can be regularized for legibility by evaluating how the
policy may produce observations that would make an observer infer an incorrect
policy. In our formulation, the decision boundary introduced by legibility
impacts the states in which the agent's policy returns an action that has high
likelihood also in other policies. In these cases, a trade-off between such
action, and legible/sub-optimal action is made.",0,0,0,0,0,0,0.43992,8.0,0.730659,36
068f2be8-c023-415b-a69b-0487eef55c78,Non-rigid Point Cloud Registration with Neural Deformation Pyramid,13,0.401186,0.839812,"Non-rigid point cloud registration is a key component in many computer vision
and computer graphics applications. The high complexity of the unknown
non-rigid motion make this task a challenging problem. In this paper, we break
down this problem via hierarchical motion decomposition. Our method called
Neural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid
architecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),
takes as input a sinusoidally encoded 3D point and outputs its motion
increments from the previous level. The sinusoidal function starts with a low
input frequency and gradually increases when the pyramid level goes down. This
allows a multi-level rigid to nonrigid motion decomposition and also speeds up
the solving by 50 times compared to the existing MLP-based approach. Our method
achieves advanced partialto-partial non-rigid point cloud registration results
on the 4DMatch/4DLoMatch benchmark under both no-learned and supervised
settings.",1,1,0,0,1,0,0.961199,7.0,0.938429,45
e9ef850c-8536-4160-8eb7-aef97a1f8503,Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds,24,0.228371,0.882995,"Dense captioning in 3D point clouds is an emerging vision-and-language task
involving object-level 3D scene understanding. Apart from coarse semantic class
prediction and bounding box regression as in traditional 3D object detection,
3D dense captioning aims at producing a further and finer instance-level label
of natural language description on visual appearance and spatial relations for
each scene object of interest. To detect and describe objects in a scene,
following the spirit of neural machine translation, we propose a
transformer-based encoder-decoder architecture, namely SpaCap3D, to transform
objects into descriptions, where we especially investigate the relative
spatiality of objects in 3D scenes and design a spatiality-guided encoder via a
token-to-token spatial relation learning objective and an object-centric
decoder for precise and spatiality-enhanced object caption generation.
Evaluated on two benchmark datasets, ScanRefer and ReferIt3D, our proposed
SpaCap3D outperforms the baseline method Scan2Cap by 4.94% and 9.61% in
CIDEr@0.5IoU, respectively. Our project page with source code and supplementary
files is available at https://SpaCap3D.github.io/ .",1,0,0,0,1,0,0.825062,7.0,0.849479,42
900e2723-856a-467e-8057-13af8bd3de23,A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis,3,0.0467254,0.408954,"Recently, some span-based methods have achieved encouraging performances for
joint aspect-sentiment analysis, which first extract aspects (aspect
extraction) by detecting aspect boundaries and then classify the span-level
sentiments (sentiment classification). However, most existing approaches either
sequentially extract task-specific features, leading to insufficient feature
interactions, or they encode aspect features and sentiment features in a
parallel manner, implying that feature representation in each task is largely
independent of each other except for input sharing. Both of them ignore the
internal correlations between the aspect extraction and sentiment
classification. To solve this problem, we novelly propose a hierarchical
interactive network (HI-ASA) to model two-way interactions between two tasks
appropriately, where the hierarchical interactions involve two steps:
shallow-level interaction and deep-level interaction. First, we utilize
cross-stitch mechanism to combine the different task-specific features
selectively as the input to ensure proper two-way interactions. Second, the
mutual information technique is applied to mutually constrain learning between
two tasks in the output layer, thus the aspect input and the sentiment input
are capable of encoding features of the other task via backpropagation.
Extensive experiments on three real-world datasets demonstrate HI-ASA's
superiority over baselines.",1,0,0,0,0,0,0.561058,7.0,0.742319,25
10679ec2-aa79-4f98-a3fd-d99e613d2ce6,Rethinking Graph Convolutional Networks in Knowledge Graph Completion,35,0.593813,0.876026,"Graph convolutional networks (GCNs) -- which are effective in modeling graph
structures -- have been increasingly popular in knowledge graph completion
(KGC). GCN-based KGC models first use GCNs to generate expressive entity
representations and then use knowledge graph embedding (KGE) models to capture
the interactions among entities and relations. However, many GCN-based KGC
models fail to outperform state-of-the-art KGE models though introducing
additional computational complexity. This phenomenon motivates us to explore
the real effect of GCNs in KGC. Therefore, in this paper, we build upon
representative GCN-based KGC models and introduce variants to find which factor
of GCNs is critical in KGC. Surprisingly, we observe from experiments that the
graph structure modeling in GCNs does not have a significant impact on the
performance of KGC models, which is in contrast to the common belief. Instead,
the transformations for entity representations are responsible for the
performance improvements. Based on the observation, we propose a simple yet
effective framework named LTE-KGE, which equips existing KGE models with
linearly transformed entity embeddings. Experiments demonstrate that LTE-KGE
models lead to similar performance improvements with GCN-based KGC methods,
while being more computationally efficient. These results suggest that existing
GCNs are unnecessary for KGC, and novel GCN-based KGC models should count on
more ablation studies to validate their effectiveness. The code of all the
experiments is available on GitHub at https://github.com/MIRALab-USTC/GCN4KGC.",1,0,0,0,0,0,0.959721,6.0,0.926239,41
90a7782d-d8f0-4420-b570-e81195238020,Investigation of Data Augmentation Techniques for Disordered Speech Recognition,43,0.0762543,0.609811,"Disordered speech recognition is a highly challenging task. The underlying
neuro-motor conditions of people with speech disorders, often compounded with
co-occurring physical disabilities, lead to the difficulty in collecting large
quantities of speech required for system development. This paper investigates a
set of data augmentation techniques for disordered speech recognition,
including vocal tract length perturbation (VTLP), tempo perturbation and speed
perturbation. Both normal and disordered speech were exploited in the
augmentation process. Variability among impaired speakers in both the original
and augmented data was modeled using learning hidden unit contributions (LHUC)
based speaker adaptive training. The final speaker adapted system constructed
using the UASpeech corpus and the best augmentation approach based on speed
perturbation produced up to 2.92% absolute (9.3% relative) word error rate
(WER) reduction over the baseline system without data augmentation, and gave an
overall WER of 26.37% on the test set containing 16 dysarthric speakers.",0,1,0,0,0,0,0.000168564,15.0,0.313497,45
b0f19f93-4630-406f-99fb-5bfedc39b6c8,DIVA-DAF: A Deep Learning Framework for Historical Document Image Analysis,1,0.0266672,0.170643,"Deep learning methods have shown strong performance in solving tasks for
historical document image analysis. However, despite current libraries and
frameworks, programming an experiment or a set of experiments and executing
them can be time-consuming. This is why we propose an open-source deep learning
framework, DIVA-DAF, which is based on PyTorch Lightning and specifically
designed for historical document analysis. Pre-implemented tasks such as
segmentation and classification can be easily used or customized. It is also
easy to create one's own tasks with the benefit of powerful modules for loading
data, even large data sets, and different forms of ground truth. The
applications conducted have demonstrated time savings for the programming of a
document analysis task, as well as for different scenarios such as pre-training
or changing the architecture. Thanks to its data module, the framework also
allows to reduce the time of model training significantly.",1,1,0,0,0,0,0.222174,9.0,0.667696,51
56841793-693e-4ad5-a7f7-733ecb5d41fe,Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection,6,0.13879,0.629676,"Pre-training large neural language models, such as BERT, has led to
impressive gains on many natural language processing (NLP) tasks. Although this
method has proven to be effective for many domains, it might not always provide
desirable benefits. In this paper, we study the effects of hateful pre-training
on low-resource hate speech classification tasks. While previous studies on the
English language have emphasized its importance, we aim to augment their
observations with some non-obvious insights. We evaluate different variations
of tweet-based BERT models pre-trained on hateful, non-hateful, and mixed
subsets of a 40M tweet dataset. This evaluation is carried out for the Indian
languages Hindi and Marathi. This paper is empirical evidence that hateful
pre-training is not the best pre-training option for hate speech detection. We
show that pre-training on non-hateful text from the target domain provides
similar or better results. Further, we introduce HindTweetBERT and
MahaTweetBERT, the first publicly available BERT models pre-trained on Hindi
and Marathi tweets, respectively. We show that they provide state-of-the-art
performance on hate speech classification tasks. We also release hateful BERT
for the two languages and a gold hate speech evaluation benchmark HateEval-Hi
and HateEval-Mr consisting of manually labeled 2000 tweets each. The models and
data are available at https://github.com/l3cube-pune/MarathiNLP .",1,1,0,1,1,0,0.870842,5.0,0.82136,28
043a6fc9-02eb-4611-8519-afc9e92599a9,Phantom Sponges: Exploiting Non-Maximum Suppression to Attack Deep Object Detectors,13,0.0586867,0.594228,"Adversarial attacks against deep learning-based object detectors have been
studied extensively in the past few years. Most of the attacks proposed have
targeted the model's integrity (i.e., caused the model to make incorrect
predictions), while adversarial attacks targeting the model's availability, a
critical aspect in safety-critical domains such as autonomous driving, have not
yet been explored by the machine learning research community. In this paper, we
propose a novel attack that negatively affects the decision latency of an
end-to-end object detection pipeline. We craft a universal adversarial
perturbation (UAP) that targets a widely used technique integrated in many
object detector pipelines -- non-maximum suppression (NMS). Our experiments
demonstrate the proposed UAP's ability to increase the processing time of
individual frames by adding ""phantom"" objects that overload the NMS algorithm
while preserving the detection of the original objects which allows the attack
to go undetected for a longer period of time.",0,1,1,0,0,0,0.264219,9.0,0.689898,37
ed752675-9324-4588-8fd5-34bcbe2d07ed,Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset,28,0.534875,0.96073,"6D object pose estimation is one of the fundamental problems in computer
vision and robotics research. While a lot of recent efforts have been made on
generalizing pose estimation to novel object instances within the same
category, namely category-level 6D pose estimation, it is still restricted in
constrained environments given the limited number of annotated data. In this
paper, we collect Wild6D, a new unlabeled RGBD object video dataset with
diverse instances and backgrounds. We utilize this data to generalize
category-level 6D object pose estimation in the wild with semi-supervised
learning. We propose a new model, called Rendering for Pose estimation network
RePoNet, that is jointly trained using the free ground-truths with the
synthetic data, and a silhouette matching objective function on the real-world
data. Without using any 3D annotations on real data, our method outperforms
state-of-the-art methods on the previous dataset and our Wild6D test set (with
manual annotations for evaluation) by a large margin. Project page with Wild6D
data: https://oasisyang.github.io/semi-pose .",1,1,1,1,1,0,0.91883,6.0,0.885221,72
457c54c6-91e7-4200-94d1-37c94e325ddc,GRS: Combining Generation and Revision in Unsupervised Sentence Simplification,8,0.0562119,0.581492,"We propose GRS: an unsupervised approach to sentence simplification that
combines text generation and text revision. We start with an iterative
framework in which an input sentence is revised using explicit edit operations,
and add paraphrasing as a new edit operation. This allows us to combine the
advantages of generative and revision-based approaches: paraphrasing captures
complex edit operations, and the use of explicit edit operations in an
iterative manner provides controllability and interpretability. We demonstrate
these advantages of GRS compared to existing methods on the Newsela and ASSET
datasets.",1,1,0,0,0,0,0.192903,8.0,0.606279,73
7f894540-043e-4096-a946-dc943976120f,Dynamic Global Memory for Document-level Argument Extraction,19,0.364633,0.97874,"Extracting informative arguments of events from news articles is a
challenging problem in information extraction, which requires a global
contextual understanding of each document. While recent work on document-level
extraction has gone beyond single-sentence and increased the cross-sentence
inference capability of end-to-end models, they are still restricted by certain
input sequence length constraints and usually ignore the global context between
events. To tackle this issue, we introduce a new global neural generation-based
framework for document-level event argument extraction by constructing a
document memory store to record the contextual event information and leveraging
it to implicitly and explicitly help with decoding of arguments for later
events. Empirical results show that our framework outperforms prior methods
substantially and it is more robust to adversarially annotated examples with
our constrained decoding design. (Our code and resources are available at
https://github.com/xinyadu/memory_docie for research purpose.)",1,1,0,0,1,0,0.884231,7.0,0.879842,28
126fe5e5-ec9f-44e0-aacb-889381273bac,Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance,10,0.0783371,0.854222,"Summarizing legal decisions requires the expertise of law practitioners,
which is both time- and cost-intensive. This paper presents techniques for
extractive summarization of legal decisions in a low-resource setting using
limited expert annotated data. We test a set of models that locate relevant
content using a sequential model and tackle redundancy by leveraging maximal
marginal relevance to compose summaries. We also demonstrate an implicit
approach to help train our proposed models generate more informative summaries.
Our multi-task learning model variant leverages rhetorical role identification
as an auxiliary task to further improve the summarizer. We perform extensive
experiments on datasets containing legal decisions from the US Board of
Veterans' Appeals and conduct quantitative and expert-ranked evaluations of our
models. Our results show that the proposed approaches can achieve ROUGE scores
vis-\`a-vis expert extracted summaries that match those achieved by
inter-annotator comparison.",0,1,0,0,0,0,0.0857992,9.0,0.553259,35
c13ce1cc-3056-44e2-b830-b3be8c546382,Balancing Multi-Domain Corpora Learning for Open-Domain Response Generation,2,0.0260081,0.111977,"Open-domain conversational systems are assumed to generate equally good
responses on multiple domains. Previous work achieved good performance on the
single corpus, but training and evaluating on multiple corpora from different
domains are less studied. This paper explores methods of generating relevant
responses for each of multiple multi-domain corpora. We first examine
interleaved learning which intermingles multiple corpora as the baseline. We
then investigate two multi-domain learning methods, labeled learning and
multi-task labeled learning, which encode each corpus through a unique corpus
embedding. Furthermore, we propose Domain-specific Frequency (DF), a novel
word-level importance weight that measures the relative importance of a word
for a specific corpus compared to other corpora. Based on DF, we propose
weighted learning, a method that integrates DF to the loss function. We also
adopt DF as a new evaluation metric. Extensive experiments show that our
methods gain significant improvements on both automatic and human evaluation.
We share our code and data for reproducibility",0,1,0,0,0,0,0.508848,13.0,0.84996,53
2208793a-c0c2-404a-a48d-f4be2b660248,Crowdsourcing Relative Rankings of Multi-Word Expressions: Experts versus Non-Experts,5,0.0798939,0.168646,"In this study we investigate to which degree experts and non-experts agree on
questions of difficulty in a crowdsourcing experiment. We ask non-experts
(second language learners of Swedish) and two groups of experts (teachers of
Swedish as a second/foreign language and CEFR experts) to rank multi-word
expressions in a crowdsourcing experiment. We find that the resulting rankings
by all the three tested groups correlate to a very high degree, which suggests
that judgments produced in a comparative setting are not influenced by
professional insights into Swedish as a second language.",0,1,0,0,0,0,0.00190676,17.0,0.53701,79
23094c27-887d-467e-baad-37ce52857f0c,Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study,5,0.0519785,0.30678,"This paper presents an empirical study to build relation extraction systems
in low-resource settings. Based upon recent pre-trained language models, we
comprehensively investigate three schemes to evaluate the performance in
low-resource settings: (i) different types of prompt-based methods with
few-shot labeled data; (ii) diverse balancing methods to address the
long-tailed distribution issue; (iii) data augmentation technologies and
self-training to generate more labeled in-domain data. We create a benchmark
with 8 relation extraction (RE) datasets covering different languages, domains
and contexts and perform extensive comparisons over the proposed schemes with
combinations. Our experiments illustrate: (i) Though prompt-based tuning is
beneficial in low-resource RE, there is still much potential for improvement,
especially in extracting relations from cross-sentence contexts with multiple
relational triples; (ii) Balancing methods are not always helpful for RE with
long-tailed distribution; (iii) Data augmentation complements existing
baselines and can bring much performance gain, while self-training may not
consistently achieve advancement to low-resource RE. Code and datasets are in
https://github.com/zjunlp/LREBench.",1,1,0,0,0,0,0.555617,6.0,0.69686,82
d1063c43-9830-4f8c-ad06-4f6e77c38195,Identifying Fixation and Saccades in Virtual Reality,1,0.0497519,0.0546458,"Gaze recognition can significantly reduce the amount of eye movement data for
a better understanding of cognitive and visual processing. Gaze recognition is
an essential precondition for eye-based interaction applications in virtual
reality. However, the three-dimensional characteristics of virtual reality
environments also pose new challenges to existing recognition algorithms. Based
on seven evaluation metrics and the Overall score (the mean of the seven
normalized metric values), we obtain optimal parameters of three existing
recognition algorithms (Velocity-Threshold Identification, Dispersion-Threshold
Identification, and Velocity & Dispersion-Threshold Identification) and our
modified Velocity & Dispersion-Threshold Identification algorithm. We compare
the performance of these four algorithms with optimal parameters. The results
show that our modified Velocity & Dispersion-Threshold Identification performs
the best. The impact of interface complexity on classification results is also
preliminarily explored. The results show that the algorithms are not sensitive
to interface complexity.",0,1,0,0,0,0,0.183031,18.0,0.821769,44
ff9809e7-2a8c-4d2c-8c73-6c61272dc53c,E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning,23,0.478853,0.44811,"The ability to recognize analogies is fundamental to human cognition.
Existing benchmarks to test word analogy do not reveal the underneath process
of analogical reasoning of neural models. Holding the belief that models
capable of reasoning should be right for the right reasons, we propose a
first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning
benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in
English) problems sourced from the Civil Service Exams, which require intensive
background knowledge to solve. More importantly, we design a free-text
explanation scheme to explain whether an analogy should be drawn, and manually
annotate them for each and every question and candidate answer. Empirical
results suggest that this benchmark is very challenging for some
state-of-the-art models for both explanation generation and analogical question
answering tasks, which invites further research in this area.",1,0,1,1,0,0,0.880247,6.0,0.857179,75
2be2c1ce-ebf6-4aa8-9ec5-a81dbf9b78af,Easing Automatic Neurorehabilitation via Classification and Smoothness Analysis,1,0.062522,0.298017,"Assessing the quality of movements for post-stroke patients during the
rehabilitation phase is vital given that there is no standard stroke
rehabilitation plan for all the patients. In fact, it depends basically on the
patient's functional independence and its progress along the rehabilitation
sessions. To tackle this challenge and make neurorehabilitation more agile, we
propose an automatic assessment pipeline that starts by recognizing patients'
movements by means of a shallow deep learning architecture, then measuring the
movement quality using jerk measure and related measures. A particularity of
this work is that the dataset used is clinically relevant, since it represents
movements inspired from Fugl-Meyer a well common upper-limb clinical stroke
assessment scale for stroke patients. We show that it is possible to detect the
contrast between healthy and patients movements in terms of smoothness, besides
achieving conclusions about the patients' progress during the rehabilitation
sessions that correspond to the clinicians' findings about each case.",0,1,0,0,0,0,0.65553,4.0,0.61356,39
3a986eb8-d807-4c64-848b-108ae350a980,Jam or Cream First? Modeling Ambiguity in Neural Machine Translation with SCONES,9,0.0471116,0.459886,"The softmax layer in neural machine translation is designed to model the
distribution over mutually exclusive tokens. Machine translation, however, is
intrinsically uncertain: the same source sentence can have multiple
semantically equivalent translations. Therefore, we propose to replace the
softmax activation with a multi-label classification layer that can model
ambiguity more effectively. We call our loss function Single-label Contrastive
Objective for Non-Exclusive Sequences (SCONES). We show that the multi-label
output layer can still be trained on single reference training data using the
SCONES loss function. SCONES yields consistent BLEU score gains across six
translation directions, particularly for medium-resource language pairs and
small beam sizes. By using smaller beam sizes we can speed up inference by a
factor of 3.9x and still match or improve the BLEU score obtained using
softmax. Furthermore, we demonstrate that SCONES can be used to train NMT
models that assign the highest probability to adequate translations, thus
mitigating the ""beam search curse"". Additional experiments on synthetic
language pairs with varying levels of uncertainty suggest that the improvements
from SCONES can be attributed to better handling of ambiguity.",0,0,0,0,0,0,0.0991414,14.0,0.72365,42
81450877-2e84-4db4-87b0-3767ee36ffe2,SafeText: A Benchmark for Exploring Physical Safety in Language Models,23,0.216393,0.976802,"Understanding what constitutes safe text is an important issue in natural
language processing and can often prevent the deployment of models deemed
harmful and unsafe. One such type of safety that has been scarcely studied is
commonsense physical safety, i.e. text that is not explicitly violent and
requires additional commonsense knowledge to comprehend that it leads to
physical harm. We create the first benchmark dataset, SafeText, comprising
real-life scenarios with paired safe and physically unsafe pieces of advice. We
utilize SafeText to empirically study commonsense physical safety across
various models designed for text generation and commonsense reasoning tasks. We
find that state-of-the-art large language models are susceptible to the
generation of unsafe text and have difficulty rejecting unsafe advice. As a
result, we argue for further studies of safety and the assessment of
commonsense physical safety in models before release.",0,0,1,1,0,0,0.478859,6.0,0.660407,43
7ade40ba-b2dd-4ef3-9c74-3ba009bf950e,Clear Memory-Augmented Auto-Encoder for Surface Defect Detection,2,0.0901013,0.219126,"In surface defect detection, due to the extreme imbalance in the number of
positive and negative samples, positive-samples-based anomaly detection methods
have received more and more attention. Specifically, reconstruction-based
methods are the most popular. However, existing methods are either difficult to
repair abnormal foregrounds or reconstruct clear backgrounds. Therefore, we
propose a clear memory-augmented auto-encoder (CMA-AE). At first, we propose a
novel clear memory-augmented module (CMAM), which combines the encoding and
memoryencoding in a way of forgetting and inputting, thereby repairing abnormal
foregrounds and preserving clear backgrounds. Secondly, a general artificial
anomaly generation algorithm (GAAGA) is proposed to simulate anomalies that are
as realistic and feature-rich as possible. At last, we propose a novel multi
scale feature residual detection method (MSFR) for defect segmentation, which
makes the defect location more accurate. Extensive comparison experiments
demonstrate that CMA-AE achieves state-of-the-art detection accuracy and shows
great potential in industrial applications.",0,1,0,0,1,0,0.823862,8.0,0.867804,48
62dda78a-0855-4c18-ac4b-532c562fa6a4,Better Few-Shot Relation Extraction with Label Prompt Dropout,12,0.407887,0.667347,"Few-shot relation extraction aims to learn to identify the relation between
two entities based on very limited training examples. Recent efforts found that
textual labels (i.e., relation names and relation descriptions) could be
extremely useful for learning class representations, which will benefit the
few-shot learning task. However, what is the best way to leverage such label
information in the learning process is an important research question. Existing
works largely assume such textual labels are always present during both
learning and prediction. In this work, we argue that such approaches may not
always lead to optimal results. Instead, we present a novel approach called
label prompt dropout, which randomly removes label descriptions in the learning
process. Our experiments show that our approach is able to lead to improved
class representations, yielding significantly better results on the few-shot
relation extraction task.",1,1,0,0,0,0,0.975537,7.0,0.957393,32
36b0d1b1-bdac-469e-849e-ac416483fd21,Egocentric Prediction of Action Target in 3D,9,0.250053,0.54661,"We are interested in anticipating as early as possible the target location of
a person's object manipulation action in a 3D workspace from egocentric vision.
It is important in fields like human-robot collaboration, but has not yet
received enough attention from vision and learning communities. To stimulate
more research on this challenging egocentric vision task, we propose a large
multimodality dataset of more than 1 million frames of RGB-D and IMU streams,
and provide evaluation metrics based on our high-quality 2D and 3D labels from
semi-automatic annotation. Meanwhile, we design baseline methods using
recurrent neural networks and conduct various ablation studies to validate
their effectiveness. Our results demonstrate that this new task is worthy of
further study by researchers in robotics, vision, and learning communities.",1,1,1,1,0,0,0.544507,9.0,0.794471,63
250d3dd7-2e16-459f-9a21-7e637a662ad6,Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms,9,0.0977007,0.646242,"Attention has become one of the most commonly used mechanisms in deep
learning approaches. The attention mechanism can help the system focus more on
the feature space's critical regions. For example, high amplitude regions can
play an important role for Speech Emotion Recognition (SER). In this paper, we
identify misalignments between the attention and the signal amplitude in the
existing multi-head self-attention. To improve the attention area, we propose
to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA)
mechanism in combination with the multi-head self-attention. Through the FA
mechanism, the network can detect the largest amplitude part in the segment. By
employing the CA mechanism, the network can modulate the information flow by
assigning different weights to each attention head and improve the utilization
of surrounding contexts. To evaluate the proposed method, experiments are
performed with the IEMOCAP and RAVDESS datasets. Experimental results show that
the proposed framework significantly outperforms the state-of-the-art
approaches on both datasets.",0,1,0,0,1,0,0.595466,7.0,0.755823,31
c0342fbf-94c3-4917-ac43-fb7bd85e6d05,Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints,4,0.0741583,0.31674,"Processing information locked within clinical health records is a challenging
task that remains an active area of research in biomedical NLP. In this work,
we evaluate a broad set of machine learning techniques ranging from simple RNNs
to specialised transformers such as BioBERT on a dataset containing clinical
notes along with a set of annotations indicating whether a sample is
cancer-related or not.
  Furthermore, we specifically employ efficient fine-tuning methods from NLP,
namely, bottleneck adapters and prompt tuning, to adapt the models to our
specialised task. Our evaluations suggest that fine-tuning a frozen BERT model
pre-trained on natural language and with bottleneck adapters outperforms all
other strategies, including full fine-tuning of the specialised BioBERT model.
Based on our findings, we suggest that using bottleneck adapters in
low-resource situations with limited access to labelled data or processing
capacity could be a viable strategy in biomedical text mining. The code used in
the experiments are going to be made available at
https://github.com/omidrohanian/bottleneck-adapters.",1,1,0,0,0,0,0.786668,5.0,0.765105,28
3c1321ad-17f1-41ed-bd14-76c86565b386,An Unsupervised Masking Objective for Abstractive Multi-Document News Summarization,2,0.0127779,0.0954394,"We show that a simple unsupervised masking objective can approach near
supervised performance on abstractive multi-document news summarization. Our
method trains a state-of-the-art neural summarization model to predict the
masked out source document with highest lexical centrality relative to the
multi-document group. In experiments on the Multi-News dataset, our masked
training objective yields a system that outperforms past unsupervised methods
and, in human evaluation, surpasses the best supervised method without
requiring access to any ground-truth summaries. Further, we evaluate how
different measures of lexical centrality, inspired by past work on extractive
summarization, affect final performance.",1,1,0,0,0,0,0.590157,15.0,0.885083,30
a09c7104-6d95-41a6-b0d1-21f2aaed4b69,Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask,14,0.0864958,0.236783,"Reading is integral to everyday life, and yet learning to read is a struggle
for many young learners. During lessons, teachers can use comprehension
questions to increase engagement, test reading skills, and improve retention.
Historically such questions were written by skilled teachers, but recently
language models have been used to generate comprehension questions. However,
many existing Question Generation (QG) systems focus on generating literal
questions from the text, and have no way to control the type of the generated
question. In this paper, we study QG for reading comprehension where
inferential questions are critical and extractive techniques cannot be used. We
propose a two-step model (HTA-WTA) that takes advantage of previous datasets,
and can generate questions for a specific targeted comprehension skill. We
propose a new reading comprehension dataset that contains questions annotated
with story-based reading comprehension skills (SBRCS), allowing for a more
complete reader assessment. Across several experiments, our results show that
HTA-WTA outperforms multiple strong baselines on this new dataset. We show that
the HTA-WTA model tests for strong SCRS by asking deep inferential questions.",0,1,0,1,0,0,0.19414,6.0,0.476228,74
b9f19de7-3531-4172-8f94-29eb3a914970,Intent Contrastive Learning for Sequential Recommendation,138,0.625342,0.967625,"Users' interactions with items are driven by various intents (e.g., preparing
for holiday gifts, shopping for fishing equipment, etc.).However, users'
underlying intents are often unobserved/latent, making it challenging to
leverage such latent intents forSequentialrecommendation(SR). To investigate
the benefits of latent intents and leverage them effectively for
recommendation, we proposeIntentContrastiveLearning(ICL), a general learning
paradigm that leverages a latent intent variable into SR. The core idea is to
learn users' intent distribution functions from unlabeled user behavior
sequences and optimize SR models with contrastive self-supervised learning
(SSL) by considering the learned intents to improve recommendation.
Specifically, we introduce a latent variable to represent users' intents and
learn the distribution function of the latent variable via clustering. We
propose to leverage the learned intents into SR models via contrastive SSL,
which maximizes the agreement between a view of sequence and its corresponding
intent. The training is alternated between intent representation learning and
the SR model optimization steps within the generalized expectation-maximization
(EM) framework. Fusing user intent information into SR also improves model
robustness. Experiments conducted on four real-world datasets demonstrate the
superiority of the proposed learning paradigm, which improves performance, and
robustness against data sparsity and noisy interaction issues.",1,0,0,0,0,0,0.837931,4.0,0.747313,54
6a9b8da3-0b4f-4420-bbea-e6c23a16856d,A Minimal Deductive System for RDFS with Negative Statements,2,0.0210157,0.182602,"The triple language RDFS is designed to represent and reason with
\emph{positive} statements only (e.g.""antipyretics are drugs"").
  In this paper we show how to extend RDFS to express and reason with various
forms of negative statements under the Open World Assumption (OWA). To do so,
we start from $\rho df$, a minimal, but significant RDFS fragment that covers
all essential features of RDFS, and then extend it to $\rho df_\bot^\neg$,
allowing express also statements such as ""radio therapies are non drug
treatments"", ""Ebola has no treatment"", or ""opioids and antipyretics are
disjoint classes"". The main and, to the best of our knowledge, unique features
of our proposal are: (i) $\rho df_\bot^\neg$ remains syntactically a triple
language by extending $\rho df$ with new symbols with specific semantics and
there is no need to revert to the reification method to represent negative
triples; (ii) the logic is defined in such a way that any RDFS reasoner/store
may handle the new predicates as ordinary terms if it does not want to take
account of the extra capabilities; (iii) despite negated statements, every
$\rho df_\bot^\neg$ knowledge base is satisfiable; (iv) the $\rho df_\bot^\neg$
entailment decision procedure is obtained from $\rho df$ via additional
inference rules favouring a potential implementation; and (v) deciding
entailment in $\rho df_\bot^\neg$ ranges from P to NP.",0,0,1,0,0,0,7.05803e-09,30.0,0.320715,47
06418f9f-4322-4111-87f8-d8ebc5e8c16d,Domain-General Crowd Counting in Unseen Scenarios,11,0.0550113,0.61098,"Domain shift across crowd data severely hinders crowd counting models to
generalize to unseen scenarios. Although domain adaptive crowd counting
approaches close this gap to a certain extent, they are still dependent on the
target domain data to adapt (e.g. finetune) their models to the specific
domain. In this paper, we aim to train a model based on a single source domain
which can generalize well on any unseen domain. This falls into the realm of
domain generalization that remains unexplored in crowd counting. We first
introduce a dynamic sub-domain division scheme which divides the source domain
into multiple sub-domains such that we can initiate a meta-learning framework
for domain generalization. The sub-domain division is dynamically refined
during the meta-learning. Next, in order to disentangle domain-invariant
information from domain-specific information in image features, we design the
domain-invariant and -specific crowd memory modules to re-encode image
features. Two types of losses, i.e. feature reconstruction and orthogonal
losses, are devised to enable this disentanglement. Extensive experiments on
several standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, show
the strong generalizability of our method.",1,0,1,0,1,0,0.22994,5.0,0.409685,55
bc48f9a2-c6d7-4551-9116-19a118ad9532,Multi-View Document Representation Learning for Open-Domain Dense Retrieval,43,0.423196,0.783766,"Dense retrieval has achieved impressive advances in first-stage retrieval
from a large-scale document collection, which is built on bi-encoder
architecture to produce single vector representation of query and document.
However, a document can usually answer multiple potential queries from
different views. So the single vector representation of a document is hard to
match with multi-view queries, and faces a semantic mismatch problem. This
paper proposes a multi-view document representation learning framework, aiming
to produce multi-view embeddings to represent documents and enforce them to
align with different queries. First, we propose a simple yet effective method
of generating multiple embeddings through viewers. Second, to prevent
multi-view embeddings from collapsing to the same one, we further propose a
global-local loss with annealed temperature to encourage the multiple viewers
to better align with different potential queries. Experiments show our method
outperforms recent works and achieves state-of-the-art results.",0,1,0,0,1,0,0.93218,4.0,0.845109,42
46e3d9db-71dd-437b-9840-4c570567179f,Contrastive Bayesian Analysis for Deep Metric Learning,8,0.0428322,0.42565,"Recent methods for deep metric learning have been focusing on designing
different contrastive loss functions between positive and negative pairs of
samples so that the learned feature embedding is able to pull positive samples
of the same class closer and push negative samples from different classes away
from each other. In this work, we recognize that there is a significant
semantic gap between features at the intermediate feature layer and class
labels at the final output layer. To bridge this gap, we develop a contrastive
Bayesian analysis to characterize and model the posterior probabilities of
image labels conditioned by their features similarity in a contrastive learning
setting. This contrastive Bayesian analysis leads to a new loss function for
deep metric learning. To improve the generalization capability of the proposed
method onto new classes, we further extend the contrastive Bayesian loss with a
metric variance constraint. Our experimental results and ablation studies
demonstrate that the proposed contrastive Bayesian metric learning method
significantly improves the performance of deep metric learning in both
supervised and pseudo-supervised scenarios, outperforming existing methods by a
large margin.",1,0,0,0,1,0,0.102667,8.0,0.520997,103
a6939f3d-40d0-4f72-b918-30cfa7def209,AI Ethics Issues in Real World: Evidence from AI Incident Database,17,0.500602,0.680139,"With the powerful performance of Artificial Intelligence (AI) also comes
prevalent ethical issues. Though governments and corporations have curated
multiple AI ethics guidelines to curb unethical behavior of AI, the effect has
been limited, probably due to the vagueness of the guidelines. In this paper,
we take a closer look at how AI ethics issues take place in real world, in
order to have a more in-depth and nuanced understanding of different ethical
issues as well as their social impact. With a content analysis of AI Incident
Database, which is an effort to prevent repeated real world AI failures by
cataloging incidents, we identified 13 application areas which often see
unethical use of AI, with intelligent service robots, language/vision models
and autonomous driving taking the lead. Ethical issues appear in 8 different
forms, from inappropriate use and racial discrimination, to physical safety and
unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI
practitioners with a practical guideline when trying to deploy AI applications
ethically.",0,0,0,0,0,0,0.763135,7.0,0.822199,31
ef288dc4-4f08-4a85-885e-ab3147df2283,Momentum Calibration for Text Generation,7,0.0261248,0.519748,"The input and output of most text generation tasks can be transformed to two
sequences of tokens and they can be modeled using sequence-to-sequence learning
modeling tools such as Transformers. These models are usually trained by
maximizing the likelihood the output text sequence and assumes the input
sequence and all gold preceding tokens are given during training, while during
inference the model suffers from the exposure bias problem (i.e., it only has
access to its previously predicted tokens rather gold tokens during beam
search). In this paper, we propose MoCa ({\bf Mo}mentum {\bf Ca}libration) for
text generation. MoCa is an online method that dynamically generates slowly
evolving (but consistent) samples using a momentum moving average generator
with beam search and MoCa learns to align its model scores of these samples
with their actual qualities. Experiments on four text generation datasets
(i.e., CNN/DailyMail, XSum, SAMSum and Gigaword) show MoCa consistently
improves strong pre-trained transformers using vanilla fine-tuning and we
achieve the state-of-the-art results on CNN/DailyMail and SAMSum datasets.",0,1,0,0,1,1,0.350113,9.0,0.727646,44
1deaecf4-defb-47eb-9df4-eafd4c0b278b,ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking,51,0.352293,0.985847,"We introduce ReFinED, an efficient end-to-end entity linking model which uses
fine-grained entity types and entity descriptions to perform linking. The model
performs mention detection, fine-grained entity typing, and entity
disambiguation for all mentions within a document in a single forward pass,
making it more than 60 times faster than competitive existing approaches.
ReFinED also surpasses state-of-the-art performance on standard entity linking
datasets by an average of 3.7 F1. The model is capable of generalising to
large-scale knowledge bases such as Wikidata (which has 15 times more entities
than Wikipedia) and of zero-shot entity linking. The combination of speed,
accuracy and scale makes ReFinED an effective and cost-efficient system for
extracting entities from web-scale datasets, for which the model has been
successfully deployed. Our code and pre-trained models are available at
https://github.com/alexa/ReFinED",1,1,0,0,1,0,0.279708,8.0,0.659515,46
25efdd80-6e7a-44cd-89e6-bcc98dda0e39,TetGAN: A Convolutional Neural Network for Tetrahedral Mesh Generation,8,0.177702,0.525736,"We present TetGAN, a convolutional neural network designed to generate
tetrahedral meshes. We represent shapes using an irregular tetrahedral grid
which encodes an occupancy and displacement field. Our formulation enables
defining tetrahedral convolution, pooling, and upsampling operations to
synthesize explicit mesh connectivity with variable topological genus. The
proposed neural network layers learn deep features over each tetrahedron and
learn to extract patterns within spatial regions across multiple scales. We
illustrate the capabilities of our technique to encode tetrahedral meshes into
a semantically meaningful latent-space which can be used for shape editing and
synthesis. Our project page is at https://threedle.github.io/tetGAN/.",0,0,0,0,0,0,0.751621,8.0,0.840235,56
3dad7f7b-8e93-44bc-84f4-c45372f55947,Meta-Referential Games to Learn Compositional Learning Behaviours,1,0.0111251,0.0710019,"Human beings use compositionality to generalise from past experiences to
novel experiences. We assume a separation of our experiences into fundamental
atomic components that can be recombined in novel ways to support our ability
to engage with novel experiences. We frame this as the ability to learn to
generalise compositionally, and we will refer to behaviours making use of this
ability as compositional learning behaviours (CLBs). A central problem to
learning CLBs is the resolution of a binding problem (BP). While it is another
feat of intelligence that human beings perform with ease, it is not the case
for state-of-the-art artificial agents. Thus, in order to build artificial
agents able to collaborate with human beings, we propose to develop a novel
benchmark to investigate agents' abilities to exhibit CLBs by solving a
domain-agnostic version of the BP. We take inspiration from the language
emergence and grounding framework of referential games and propose a
meta-learning extension of referential games, entitled Meta-Referential Games,
and use this framework to build our benchmark, the Symbolic Behaviour Benchmark
(S2B). We provide baseline results and error analysis showing that our
benchmark is a compelling challenge that we hope will spur the research
community towards developing more capable artificial agents.",1,0,0,0,0,0,0.543615,7.0,0.735393,86
b4cd9873-d026-4e4c-af20-bac38de3ebc7,Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders,8,0.0874223,0.511428,"Knowledge distillation (KD) has been a ubiquitous method for model
compression to strengthen the capability of a lightweight model with the
transferred knowledge from the teacher. In particular, KD has been employed in
quantization-aware training (QAT) of Transformer encoders like BERT to improve
the accuracy of the student model with the reduced-precision weight parameters.
However, little is understood about which of the various KD approaches best
fits the QAT of Transformers. In this work, we provide an in-depth analysis of
the mechanism of KD on attention recovery of quantized large Transformers. In
particular, we reveal that the previously adopted MSE loss on the attention
score is insufficient for recovering the self-attention information. Therefore,
we propose two KD methods; attention-map and attention-output losses.
Furthermore, we explore the unification of both losses to address
task-dependent preference between attention-map and output losses. The
experimental results on various Transformer encoder models demonstrate that the
proposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit
weight quantization.",1,1,0,0,1,0,0.811353,6.0,0.817019,33
887e6780-c73a-42db-aa2e-97ddfe322121,Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training,28,0.369929,0.976004,"Training deep neural networks (DNNs) is becoming increasingly more resource-
and energy-intensive every year. Unfortunately, existing works primarily focus
on optimizing DNN training for faster completion, often without considering the
impact on energy efficiency.
  In this paper, we observe that common practices to improve training
performance can often lead to inefficient energy usage. More importantly, we
demonstrate that there is a tradeoff between energy consumption and performance
optimization. To this end, we propose Zeus, an optimization framework to
navigate this tradeoff by automatically finding optimal job- and GPU-level
configurations for recurring DNN training jobs. Zeus uses an online
exploration-exploitation approach in conjunction with just-in-time energy
profiling, averting the need for expensive offline measurements, while adapting
to data drifts over time. Our evaluation shows that Zeus can improve the energy
efficiency of DNN training by 15.3%-75.8% for diverse workloads.",0,1,0,0,0,0,0.769463,6.0,0.795669,103
070673df-d768-4a30-8e3c-a57529ec0620,Mathematical Cookbook for Snapshot Compressive Imaging,6,0.0726667,0.432806,"The author intends to provide you with a beautiful, elegant, user-friendly
cookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the
cookbook is composed of introduction, conventional optimization, and deep
equilibrium models. The latest releases are strongly recommended! For any other
questions, suggestions, or comments, feel free to email the author.",0,0,0,0,0,0,0.728104,4.0,0.66368,22
5273fd5b-224f-4435-912d-30ed7468986f,Human in the loop: How to effectively create coherent topics by manually labeling only a few documents per class,1,0.011157,0.0303227,"Few-shot methods for accurate modeling under sparse label-settings have
improved significantly. However, the applications of few-shot modeling in
natural language processing remain solely in the field of document
classification. With recent performance improvements, supervised few-shot
methods, combined with a simple topic extraction method pose a significant
challenge to unsupervised topic modeling methods. Our research shows that
supervised few-shot learning, combined with a simple topic extraction method,
can outperform unsupervised topic modeling techniques in terms of generating
coherent topics, even when only a few labeled documents per class are used.",0,1,0,0,0,0,0.306576,9.0,0.709528,43
3a26708a-469e-46ae-92c4-f8f7eb91d56d,BILP-Q: Quantum Coalition Structure Generation,7,0.34642,0.687694,"Quantum AI is an emerging field that uses quantum computing to solve typical
complex problems in AI. In this work, we propose BILP-Q, the first-ever general
quantum approach for solving the Coalition Structure Generation problem (CSGP),
which is notably NP-hard. In particular, we reformulate the CSGP in terms of a
Quadratic Binary Combinatorial Optimization (QUBO) problem to leverage existing
quantum algorithms (e.g., QAOA) to obtain the best coalition structure. Thus,
we perform a comparative analysis in terms of time complexity between the
proposed quantum approach and the most popular classical baselines.
Furthermore, we consider standard benchmark distributions for coalition values
to test the BILP-Q on small-scale experiments using the IBM Qiskit environment.
Finally, since QUBO problems can be solved operating with quantum annealing, we
run BILP-Q on medium-size problems using a real quantum annealer (D-Wave).",0,0,1,0,0,0,0.331312,10.0,0.748035,18
c7ed0475-f315-47a3-8483-75bacc45e03e,Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions,13,0.0854532,0.63702,"Generalization across different environments with the same tasks is critical
for successful applications of visual reinforcement learning (RL) in real
scenarios. However, visual distractions -- which are common in real scenes --
from high-dimensional observations can be hurtful to the learned
representations in visual RL, thus degrading the performance of generalization.
To tackle this problem, we propose a novel approach, namely Characteristic
Reward Sequence Prediction (CRESP), to extract the task-relevant information by
learning reward sequence distributions (RSDs), as the reward signals are
task-relevant in RL and invariant to visual distractions. Specifically, to
effectively capture the task-relevant information via RSDs, CRESP introduces an
auxiliary task -- that is, predicting the characteristic functions of RSDs --
to learn task-relevant representations, because we can well approximate the
high-dimensional distributions by leveraging the corresponding characteristic
functions. Experiments demonstrate that CRESP significantly improves the
performance of generalization on unseen environments, outperforming several
state-of-the-arts on DeepMind Control tasks with different visual distractions.",1,1,0,0,1,0,0.426173,6.0,0.633757,36
4e932f1e-946f-4a3f-a38d-d8d5fd294e24,Entity-driven Fact-aware Abstractive Summarization of Biomedical Literature,6,0.24437,0.408179,"As part of the large number of scientific articles being published every
year, the publication rate of biomedical literature has been increasing.
Consequently, there has been considerable effort to harness and summarize the
massive amount of biomedical research articles. While transformer-based
encoder-decoder models in a vanilla source document-to-summary setting have
been extensively studied for abstractive summarization in different domains,
their major limitations continue to be entity hallucination (a phenomenon where
generated summaries constitute entities not related to or present in source
article(s)) and factual inconsistency. This problem is exacerbated in a
biomedical setting where named entities and their semantics (which can be
captured through a knowledge base) constitute the essence of an article. The
use of named entities and facts mined from background knowledge bases
pertaining to the named entities to guide abstractive summarization has not
been studied in biomedical article summarization literature. In this paper, we
propose an entity-driven fact-aware framework for training end-to-end
transformer-based encoder-decoder models for abstractive summarization of
biomedical articles. We call the proposed approach, whose building block is a
transformer-based model, EFAS, Entity-driven Fact-aware Abstractive
Summarization. We conduct experiments using five state-of-the-art
transformer-based models (two of which are specifically designed for long
document summarization) and demonstrate that injecting knowledge into the
training/inference phase of these models enables the models to achieve
significantly better performance than the standard source document-to-summary
setting in terms of entity-level factual accuracy, N-gram novelty, and semantic
equivalence while performing comparably on ROUGE metrics. The proposed approach
is evaluated on ICD-11-Summ-1000, and PubMed-50k.",1,1,0,1,0,0,0.973684,6.0,0.94698,72
5a9e06f5-279d-48d1-94ec-39395113051d,LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation,5,0.130067,0.606678,"In this study, we propose a simple and effective preprocessing method for
subword segmentation based on a data compression algorithm. Compression-based
subword segmentation has recently attracted significant attention as a
preprocessing method for training data in Neural Machine Translation. Among
them, BPE/BPE-dropout is one of the fastest and most effective method compared
to conventional approaches. However, compression-based approach has a drawback
in that generating multiple segmentations is difficult due to the determinism.
To overcome this difficulty, we focus on a probabilistic string algorithm,
called locally-consistent parsing (LCP), that has been applied to achieve
optimum compression. Employing the probabilistic mechanism of LCP, we propose
LCP-dropout for multiple subword segmentation that improves BPE/BPE-dropout,
and show that it outperforms various baselines in learning from especially
small training data.",0,1,0,0,0,0,0.284918,11.0,0.754364,35
7c151a59-1c9e-43a8-8691-3459d441a393,A baseline revisited: Pushing the limits of multi-segment models for context-aware translation,11,0.293181,0.614115,"This paper addresses the task of contextual translation using multi-segment
models. Specifically we show that increasing model capacity further pushes the
limits of this approach and that deeper models are more suited to capture
context dependencies. Furthermore, improvements observed with larger models can
be transferred to smaller models using knowledge distillation. Our experiments
show that this approach achieves competitive performance across several
languages and benchmarks, without additional language-specific tuning and task
specific architectures.",0,1,0,0,0,0,0.646663,6.0,0.73835,42
b93b6bea-fe6f-4e74-90b0-5805cc177853,TagRec++: Hierarchical Label Aware Attention Network for Question Categorization,1,0.0100033,0.0230288,"Online learning systems have multiple data repositories in the form of
transcripts, books and questions. To enable ease of access, such systems
organize the content according to a well defined taxonomy of hierarchical
nature (subject-chapter-topic). The task of categorizing inputs to the
hierarchical labels is usually cast as a flat multi-class classification
problem. Such approaches ignore the semantic relatedness between the terms in
the input and the tokens in the hierarchical labels. Alternate approaches also
suffer from class imbalance when they only consider leaf level nodes as labels.
To tackle the issues, we formulate the task as a dense retrieval problem to
retrieve the appropriate hierarchical labels for each content. In this paper,
we deal with categorizing questions. We model the hierarchical labels as a
composition of their tokens and use an efficient cross-attention mechanism to
fuse the information with the term representations of the content. We also
propose an adaptive in-batch hard negative sampling approach which samples
better negatives as the training progresses. We demonstrate that the proposed
approach \textit{TagRec++} outperforms existing state-of-the-art approaches on
question datasets as measured by Recall@k. In addition, we demonstrate
zero-shot capabilities of \textit{TagRec++} and ability to adapt to label
changes.",0,1,0,0,1,0,0.27923,8.0,0.659262,42
b63b693f-d73d-4f5e-8d49-07c9c9fcd116,Class-Incremental Learning via Knowledge Amalgamation,4,0.0598959,0.338542,"Catastrophic forgetting has been a significant problem hindering the
deployment of deep learning algorithms in the continual learning setting.
Numerous methods have been proposed to address the catastrophic forgetting
problem where an agent loses its generalization power of old tasks while
learning new tasks. We put forward an alternative strategy to handle the
catastrophic forgetting with knowledge amalgamation (CFA), which learns a
student network from multiple heterogeneous teacher models specializing in
previous tasks and can be applied to current offline methods. The knowledge
amalgamation process is carried out in a single-head manner with only a
selected number of memorized samples and no annotations. The teachers and
students do not need to share the same network structure, allowing
heterogeneous tasks to be adapted to a compact or sparse data representation.
We compare our method with competitive baselines from different strategies,
demonstrating our approach's advantages.",1,1,0,0,0,0,0.834055,12.0,0.914682,31
dfe8387d-8e92-4d49-b36d-0dee4e47add2,Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search,6,0.0315735,0.509918,"Complex reasoning problems contain states that vary in the computational cost
required to determine a good action plan. Taking advantage of this property, we
propose Adaptive Subgoal Search (AdaSubS), a search method that adaptively
adjusts the planning horizon. To this end, AdaSubS generates diverse sets of
subgoals at different distances. A verification mechanism is employed to filter
out unreachable subgoals swiftly, allowing to focus on feasible further
subgoals. In this way, AdaSubS benefits from the efficiency of planning with
longer subgoals and the fine control with the shorter ones, and thus scales
well to difficult planning problems. We show that AdaSubS significantly
surpasses hierarchical planning algorithms on three complex reasoning tasks:
Sokoban, the Rubik's Cube, and inequality proving benchmark INT.",1,1,0,0,1,0,0.177474,7.0,0.536818,57
8f7a64ac-6cf7-49fa-b72d-68f52e33bd61,ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks,1,0.00855112,0.0419964,"Causal chain reasoning (CCR) is an essential ability for many decision-making
AI systems, which requires the model to build reliable causal chains by
connecting causal pairs. However, CCR suffers from two main transitive
problems: threshold effect and scene drift. In other words, the causal pairs to
be spliced may have a conflicting threshold boundary or scenario. To address
these issues, we propose a novel Reliable Causal chain reasoning
framework~(ReCo), which introduces exogenous variables to represent the
threshold and scene factors of each causal pair within the causal chain, and
estimates the threshold and scene contradictions across exogenous variables via
structural causal recurrent neural networks~(SRNN). Experiments show that ReCo
outperforms a series of strong baselines on both Chinese and English CCR
datasets. Moreover, by injecting reliable causal chain knowledge distilled by
ReCo, BERT can achieve better performances on four downstream causal-related
tasks than BERT models enhanced by other kinds of knowledge.",1,1,0,0,0,0,0.0484199,11.0,0.580676,35
1cf6e153-148c-45e3-b0e0-fcac177563e6,Practical Network Acceleration with Tiny Sets,3,0.0283916,0.231149,"Due to data privacy issues, accelerating networks with tiny training sets has
become a critical need in practice. Previous methods mainly adopt filter-level
pruning to accelerate networks with scarce training samples. In this paper, we
reveal that dropping blocks is a fundamentally superior approach in this
scenario. It enjoys a higher acceleration ratio and results in a better
latency-accuracy performance under the few-shot setting. To choose which blocks
to drop, we propose a new concept namely recoverability to measure the
difficulty of recovering the compressed network. Our recoverability is
efficient and effective for choosing which blocks to drop. Finally, we propose
an algorithm named PRACTISE to accelerate networks using only tiny sets of
training images. PRACTISE outperforms previous methods by a significant margin.
For 22% latency reduction, PRACTISE surpasses previous methods by on average 7%
on ImageNet-1k. It also enjoys high generalization ability, working well under
data-free or out-of-domain data settings, too. Our code is at
https://github.com/DoctorKey/Practise.",0,1,0,0,1,0,0.626969,7.0,0.76808,55
2029933c-741c-4e82-b880-36b7496d0161,MonoNeuralFusion: Online Monocular Neural 3D Reconstruction with Geometric Priors,9,0.138181,0.332917,"High-fidelity 3D scene reconstruction from monocular videos continues to be
challenging, especially for complete and fine-grained geometry reconstruction.
The previous 3D reconstruction approaches with neural implicit representations
have shown a promising ability for complete scene reconstruction, while their
results are often over-smooth and lack enough geometric details. This paper
introduces a novel neural implicit scene representation with volume rendering
for high-fidelity online 3D scene reconstruction from monocular videos. For
fine-grained reconstruction, our key insight is to incorporate geometric priors
into both the neural implicit scene representation and neural volume rendering,
thus leading to an effective geometry learning mechanism based on volume
rendering optimization. Benefiting from this, we present MonoNeuralFusion to
perform the online neural 3D reconstruction from monocular videos, by which the
3D scene geometry is efficiently generated and optimized during the on-the-fly
3D monocular scanning. The extensive comparisons with state-of-the-art
approaches show that our MonoNeuralFusion consistently generates much better
complete and fine-grained reconstruction results, both quantitatively and
qualitatively.",0,1,0,0,1,0,0.821348,6.0,0.822372,67
11a21cd3-39f1-41e9-b8e0-bcdb3214a534,An Approach for Improving Automatic Mouth Emotion Recognition,17,0.182255,0.409753,"The study proposes and tests a technique for automated emotion recognition
through mouth detection via Convolutional Neural Networks (CNN), meant to be
applied for supporting people with health disorders with communication skills
issues (e.g. muscle wasting, stroke, autism, or, more simply, pain) in order to
recognize emotions and generate real-time feedback, or data feeding supporting
systems. The software system starts the computation identifying if a face is
present on the acquired image, then it looks for the mouth location and
extracts the corresponding features. Both tasks are carried out using Haar
Feature-based Classifiers, which guarantee fast execution and promising
performance. If our previous works focused on visual micro-expressions for
personalized training on a single user, this strategy aims to train the system
also on generalized faces data sets.",0,1,0,0,0,0,0.0169464,12.0,0.526783,64
ceb1e6f9-3120-46c1-9b22-6fb53c5804de,Using Ontologies for the Formalization and Recognition of Criticality for Automated Driving,14,0.0,0.792145,"Knowledge representation and reasoning has a long history of examining how
knowledge can be formalized, interpreted, and semantically analyzed by
machines. In the area of automated vehicles, recent advances suggest the
ability to formalize and leverage relevant knowledge as a key enabler in
handling the inherently open and complex context of the traffic world. This
paper demonstrates ontologies to be a powerful tool for a) modeling and
formalization of and b) reasoning about factors associated with criticality in
the environment of automated vehicles. For this, we leverage the well-known
6-Layer Model to create a formal representation of the environmental context.
Within this representation, an ontology models domain knowledge as logical
axioms, enabling deduction on the presence of critical factors within traffic
scenes and scenarios. For executing automated analyses, a joint description
logic and rule reasoner is used in combination with an a-priori predicate
augmentation. We elaborate on the modular approach, present a publicly
available implementation, and evaluate the method by means of a large-scale
drone data set of urban traffic scenarios.",1,1,0,0,0,0,0.00163276,11.0,0.270354,57
a098d8d7-1e7b-43f1-8506-39f219ba609b,INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold,3,0.0247143,0.218143,"Binary Neural Networks (BNNs) have emerged as a promising solution for
reducing the memory footprint and compute costs of deep neural networks, but
they suffer from quality degradation due to the lack of freedom as activations
and weights are constrained to the binary values. To compensate for the
accuracy drop, we propose a novel BNN design called Binary Neural Network with
INSTAnce-aware threshold (INSTA-BNN), which controls the quantization threshold
dynamically in an input-dependent or instance-aware manner. According to our
observation, higher-order statistics can be a representative metric to estimate
the characteristics of the input distribution. INSTA-BNN is designed to adjust
the threshold dynamically considering various information, including
higher-order statistics, but it is also optimized judiciously to realize
minimal overhead on a real device. Our extensive study shows that INSTA-BNN
outperforms the baseline by 3.0% and 2.8% on the ImageNet classification task
with comparable computing cost, achieving 68.5% and 72.2% top-1 accuracy on
ResNet-18 and MobileNetV1 based models, respectively.",0,1,0,0,1,0,0.294405,9.0,0.704118,44
7f1e4a78-e4a7-4932-ab4e-b9b0f3551389,Predicting Future Occupancy Grids in Dynamic Environment with Spatio-Temporal Learning,8,0.0900669,0.685907,"Reliably predicting future occupancy of highly dynamic urban environments is
an important precursor for safe autonomous navigation. Common challenges in the
prediction include forecasting the relative position of other vehicles,
modelling the dynamics of vehicles subjected to different traffic conditions,
and vanishing surrounding objects. To tackle these challenges, we propose a
spatio-temporal prediction network pipeline that takes the past information
from the environment and semantic labels separately for generating future
occupancy predictions. Compared to the current SOTA, our approach predicts
occupancy for a longer horizon of 3 seconds and in a relatively complex
environment from the nuScenes dataset. Our experimental results demonstrate the
ability of spatio-temporal networks to understand scene dynamics without the
need for HD-Maps and explicit modeling dynamic objects. We publicly release our
occupancy grid dataset based on nuScenes to support further research.",0,1,0,1,1,0,0.255934,9.0,0.685767,20
21686820-0fba-4d90-bf4a-b356417d98c7,A Dynamic Graph Interactive Framework with Label-Semantic Injection for Spoken Language Understanding,20,0.260148,0.4308,"Multi-intent detection and slot filling joint models are gaining increasing
traction since they are closer to complicated real-world scenarios. However,
existing approaches (1) focus on identifying implicit correlations between
utterances and one-hot encoded labels in both tasks while ignoring explicit
label characteristics; (2) directly incorporate multi-intent information for
each token, which could lead to incorrect slot prediction due to the
introduction of irrelevant intent. In this paper, we propose a framework termed
DGIF, which first leverages the semantic information of labels to give the
model additional signals and enriched priors. Then, a multi-grain interactive
graph is constructed to model correlations between intents and slots.
Specifically, we propose a novel approach to construct the interactive graph
based on the injection of label semantics, which can automatically update the
graph to better alleviate error propagation. Experimental results show that our
framework significantly outperforms existing approaches, obtaining a relative
improvement of 13.7% over the previous best model on the MixATIS dataset in
overall accuracy.",0,1,0,0,1,0,0.342722,7.0,0.646032,37
5b530edb-e3c5-4a99-ba4a-cbc5ebe64de8,Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness,13,0.514638,0.867076,"A notable challenge in Multi-Document Summarization (MDS) is the
extremely-long length of the input. In this paper, we present an
extract-then-abstract Transformer framework to overcome the problem.
Specifically, we leverage pre-trained language models to construct a
hierarchical extractor for salient sentence selection across documents and an
abstractor for rewriting the selected contents as summaries. However, learning
such a framework is challenging since the optimal contents for the abstractor
are generally unknown. Previous works typically create pseudo extraction oracle
to enable the supervised learning for both the extractor and the abstractor.
Nevertheless, we argue that the performance of such methods could be restricted
due to the insufficient information for prediction and inconsistent objectives
between training and testing. To this end, we propose a loss weighting
mechanism that makes the model aware of the unequal importance for the
sentences not in the pseudo extraction oracle, and leverage the fine-tuned
abstractor to generate summary references as auxiliary signals for learning the
extractor. Moreover, we propose a reinforcement learning method that can
efficiently apply to the extractor for harmonizing the optimization between
training and testing. Experiment results show that our framework substantially
outperforms strong baselines with comparable model sizes and achieves the best
results on the Multi-News, Multi-XScience, and WikiCatSum corpora.",1,1,0,0,1,0,0.955343,6.0,0.920797,44
2134958b-5e15-4b7d-b6e3-ddbfb812ffa4,Unsupervised Domain Adaptation with Implicit Pseudo Supervision for Semantic Segmentation,4,0.0224504,0.226125,"Pseudo-labelling is a popular technique in unsuper-vised domain adaptation
for semantic segmentation. However, pseudo labels are noisy and inevitably have
confirmation bias due to the discrepancy between source and target domains and
training process. In this paper, we train the model by the pseudo labels which
are implicitly produced by itself to learn new complementary knowledge about
target domain. Specifically, we propose a tri-learning architecture, where
every two branches produce the pseudo labels to train the third one. And we
align the pseudo labels based on the similarity of the probability
distributions for each two branches. To further implicitly utilize the pseudo
labels, we maximize the distances of features for different classes and
minimize the distances for the same classes by triplet loss. Extensive
experiments on GTA5 to Cityscapes and SYNTHIA to Cityscapes tasks show that the
proposed method has considerable improvements.",0,1,0,0,0,0,0.833921,5.0,0.795146,54
a358b8e0-5685-43ec-ab1c-462d36d51d24,Optimizing Elimination Templates by Greedy Parameter Search,9,0.0869773,0.522688,"We propose a new method for constructing elimination templates for efficient
polynomial system solving of minimal problems in structure from motion, image
matching, and camera tracking. We first construct a particular affine
parameterization of the elimination templates for systems with a finite number
of distinct solutions. Then, we use a heuristic greedy optimization strategy
over the space of parameters to get a template with a small size. We test our
method on 34 minimal problems in computer vision. For all of them, we found the
templates either of the same or smaller size compared to the state-of-the-art.
For some difficult examples, our templates are, e.g., 2.1, 2.5, 3.8, 6.6 times
smaller. For the problem of refractive absolute pose estimation with unknown
focal length, we have found a template that is 20 times smaller. Our
experiments on synthetic data also show that the new solvers are fast and
numerically accurate. We also present a fast and numerically accurate solver
for the problem of relative pose estimation with unknown common focal length
and radial distortion.",1,1,0,0,0,0,0.00219336,16.0,0.516834,60
0caa322b-86eb-4ace-b5f2-d72151dcdd41,SAT: Self-adaptive training for fashion compatibility prediction,4,0.0357996,0.405306,"This paper presents a self-adaptive training (SAT) model for fashion
compatibility prediction. It focuses on the learning of some hard items, such
as those that share similar color, texture, and pattern features but are
considered incompatible due to the aesthetics or temporal shifts. Specifically,
we first design a method to define hard outfits and a difficulty score (DS) is
defined and assigned to each outfit based on the difficulty in recommending an
item for it. Then, we propose a self-adaptive triplet loss (SATL), where the DS
of the outfit is considered. Finally, we propose a very simple conditional
similarity network combining the proposed SATL to achieve the learning of hard
items in the fashion compatibility prediction. Experiments on the publicly
available Polyvore Outfits and Polyvore Outfits-D datasets demonstrate our
SAT's effectiveness in fashion compatibility prediction. Besides, our SATL can
be easily extended to other conditional similarity networks to improve their
performance.",0,1,0,0,0,0,0.141457,6.0,0.418349,18
e99b83b8-8a76-4a83-abc7-0475a9544ec0,BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach,43,0.656106,0.99964,"Bilevel optimization (BO) is useful for solving a variety of important
machine learning problems including but not limited to hyperparameter
optimization, meta-learning, continual learning, and reinforcement learning.
Conventional BO methods need to differentiate through the low-level
optimization process with implicit differentiation, which requires expensive
calculations related to the Hessian matrix. There has been a recent quest for
first-order methods for BO, but the methods proposed to date tend to be
complicated and impractical for large-scale deep learning applications. In this
work, we propose a simple first-order BO algorithm that depends only on
first-order gradient information, requires no implicit differentiation, and is
practical and efficient for large-scale non-convex functions in deep learning.
We provide non-asymptotic convergence analysis of the proposed method to
stationary points for non-convex objectives and present empirical results that
show its superior practical performance.",1,0,0,0,0,0,0.681283,8.0,0.815577,55
9392c43f-d32e-40d8-b290-6d05434b05ac,MIAD: A Maintenance Inspection Dataset for Unsupervised Anomaly Detection,7,0.590084,0.884824,"Visual anomaly detection plays a crucial role in not only manufacturing
inspection to find defects of products during manufacturing processes, but also
maintenance inspection to keep equipment in optimum working condition
particularly outdoors. Due to the scarcity of the defective samples,
unsupervised anomaly detection has attracted great attention in recent years.
However, existing datasets for unsupervised anomaly detection are biased
towards manufacturing inspection, not considering maintenance inspection which
is usually conducted under outdoor uncontrolled environment such as varying
camera viewpoints, messy background and degradation of object surface after
long-term working. We focus on outdoor maintenance inspection and contribute a
comprehensive Maintenance Inspection Anomaly Detection (MIAD) dataset which
contains more than 100K high-resolution color images in various outdoor
industrial scenarios. This dataset is generated by a 3D graphics software and
covers both surface and logical anomalies with pixel-precise ground truth.
Extensive evaluations of representative algorithms for unsupervised anomaly
detection are conducted, and we expect MIAD and corresponding experimental
results can inspire research community in outdoor unsupervised anomaly
detection tasks. Worthwhile and related future work can be spawned from our new
dataset.",1,1,1,1,0,0,0.987622,6.0,0.978383,48
5ed41d31-663d-4dad-9f0d-0c1165013c03,"Sampling-based inference for large linear models, with application to linearised Laplace",9,0.0741823,0.636314,"Large-scale linear models are ubiquitous throughout machine learning, with
contemporary application as surrogate models for neural network uncertainty
quantification; that is, the linearised Laplace method. Alas, the computational
cost associated with Bayesian linear models constrains this method's
application to small networks, small output spaces and small datasets. We
address this limitation by introducing a scalable sample-based Bayesian
inference method for conjugate Gaussian multi-output linear models, together
with a matching method for hyperparameter (regularisation) selection.
Furthermore, we use a classic feature normalisation method (the g-prior) to
resolve a previously highlighted pathology of the linearised Laplace method.
Together, these contributions allow us to perform linearised neural network
inference with ResNet-18 on CIFAR100 (11M parameters, 100 outputs x 50k
datapoints), with ResNet-50 on Imagenet (50M parameters, 1000 outputs x 1.2M
datapoints) and with a U-Net on a high-resolution tomographic reconstruction
task (2M parameters, 251k output~dimensions).",0,0,0,0,0,0,0.258583,7.0,0.597699,78
9aab8ed9-7452-4596-a799-ee46573731ff,Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models,7,0.0280324,0.682942,"End-to-end spoken language understanding (SLU) systems are gaining popularity
over cascaded approaches due to their simplicity and ability to avoid error
propagation. However, these systems model sequence labeling as a sequence
prediction task causing a divergence from its well-established token-level
tagging formulation. We build compositional end-to-end SLU systems that
explicitly separate the added complexity of recognizing spoken mentions in SLU
from the NLU task of sequence labeling. By relying on intermediate decoders
trained for ASR, our end-to-end systems transform the input modality from
speech to token-level representations that can be used in the traditional
sequence labeling framework. This composition of ASR and NLU formulations in
our end-to-end SLU system offers direct compatibility with pre-trained ASR and
NLU systems, allows performance monitoring of individual components and enables
the use of globally normalized losses like CRF, making them attractive in
practical scenarios. Our models outperform both cascaded and direct end-to-end
models on a labeling task of named entity recognition across SLU benchmarks.",1,1,0,0,0,0,0.0522665,11.0,0.587808,47
65ac9333-ab64-43a5-808f-12e5445786f9,Connection-minimal Abduction in EL via Translation to FOL -- Technical Report,6,0.290402,0.909282,"Abduction in description logics finds extensions of a knowledge base to make
it entail an observation. As such, it can be used to explain why the
observation does not follow, to repair incomplete knowledge bases, and to
provide possible explanations for unexpected observations. We consider TBox
abduction in the lightweight description logic EL, where the observation is a
concept inclusion and the background knowledge is a TBox, i.e., a set of
concept inclusions. To avoid useless answers, such problems usually come with
further restrictions on the solution space and/or minimality criteria that help
sort the chaff from the grain. We argue that existing minimality notions are
insufficient, and introduce connection minimality. This criterion follows
Occam's razor by rejecting hypotheses that use concept inclusions unrelated to
the problem at hand. We show how to compute a special class of
connection-minimal hypotheses in a sound and complete way. Our technique is
based on a translation to first-order logic, and constructs hypotheses based on
prime implicates. We evaluate a prototype implementation of our approach on
ontologies from the medical domain.",0,0,0,0,0,0,0.0986768,20.0,0.806308,45
a31be4ae-72af-42e6-a767-dd7693aa9331,Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos,16,0.210991,0.870179,"Understanding dynamic hand motions and actions from egocentric RGB videos is
a fundamental yet challenging task due to self-occlusion and ambiguity. To
address occlusion and ambiguity, we develop a transformer-based framework to
exploit temporal information for robust estimation. Noticing the different
temporal granularity of and the semantic correlation between hand pose
estimation and action recognition, we build a network hierarchy with two
cascaded transformer encoders, where the first one exploits the short-term
temporal cue for hand pose estimation, and the latter aggregates per-frame pose
and object information over a longer time span to recognize the action. Our
approach achieves competitive results on two first-person hand action
benchmarks, namely FPHA and H2O. Extensive ablation studies verify our design
choices.",1,1,0,0,1,0,0.80287,8.0,0.859424,56
3807b5eb-5d43-4950-ae73-7468edab6e71,TODE-Trans: Transparent Object Depth Estimation with Transformer,6,0.082438,0.547453,"Transparent objects are widely used in industrial automation and daily life.
However, robust visual recognition and perception of transparent objects have
always been a major challenge. Currently, most commercial-grade depth cameras
are still not good at sensing the surfaces of transparent objects due to the
refraction and reflection of light. In this work, we present a
transformer-based transparent object depth estimation approach from a single
RGB-D input. We observe that the global characteristics of the transformer make
it easier to extract contextual information to perform depth estimation of
transparent areas. In addition, to better enhance the fine-grained features, a
feature fusion module (FFM) is designed to assist coherent prediction. Our
empirical evidence demonstrates that our model delivers significant
improvements in recent popular datasets, e.g., 25% gain on RMSE and 21% gain on
REL compared to previous state-of-the-art convolutional-based counterparts in
ClearGrasp dataset. Extensive results show that our transformer-based model
enables better aggregation of the object's RGB and inaccurate depth information
to obtain a better depth representation. Our code and the pre-trained model
will be available at https://github.com/yuchendoudou/TODE.",0,1,0,0,1,0,0.598714,9.0,0.81107,33
7e9e3095-30aa-434e-8d4d-f10c99b81868,Face segmentation: A comparison between visible and thermal images,22,0.198857,0.158039,"Face segmentation is a first step for face biometric systems. In this paper
we present a face segmentation algorithm for thermographic images. This
algorithm is compared with the classic Viola and Jones algorithm used for
visible images. Experimental results reveal that, when segmenting a
multispectral (visible and thermal) face database, the proposed algorithm is
more than 10 times faster, while the accuracy of face segmentation in thermal
images is higher than in case of Viola-Jones",0,1,0,0,0,0,0.0963664,20.0,0.80506,8
94235dc2-2379-43d7-b83e-8a1fa2fc7ad6,Fine-Grained Scene Graph Generation with Data Transfer,51,0.802894,0.914471,"Scene graph generation (SGG) is designed to extract (subject, predicate,
object) triplets in images. Recent works have made a steady progress on SGG,
and provide useful tools for high-level vision and language understanding.
However, due to the data distribution problems including long-tail distribution
and semantic ambiguity, the predictions of current SGG models tend to collapse
to several frequent but uninformative predicates (e.g., on, at), which limits
practical application of these models in downstream tasks. To deal with the
problems above, we propose a novel Internal and External Data Transfer
(IETrans) method, which can be applied in a plug-and-play fashion and expanded
to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the
data distribution problem by automatically creating an enhanced dataset that
provides more sufficient and coherent annotations for all predicates. By
training on the enhanced dataset, a Neural Motif model doubles the macro
performance while maintaining competitive micro performance. The code and data
are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch.",1,1,0,0,1,0,0.928588,5.0,0.872214,42
ca4923be-bcb3-4dbb-9614-57152a420143,Asset Allocation: From Markowitz to Deep Reinforcement Learning,3,0.0934331,0.192635,"Asset allocation is an investment strategy that aims to balance risk and
reward by constantly redistributing the portfolio's assets according to certain
goals, risk tolerance, and investment horizon. Unfortunately, there is no
simple formula that can find the right allocation for every individual. As a
result, investors may use different asset allocations' strategy to try to
fulfil their financial objectives. In this work, we conduct an extensive
benchmark study to determine the efficacy and reliability of a number of
optimization techniques. In particular, we focus on traditional approaches
based on Modern Portfolio Theory, and on machine-learning approaches based on
deep reinforcement learning. We assess the model's performance under different
market tendency, i.e., both bullish and bearish markets. For reproducibility,
we provide the code implementation code in this repository.",0,1,0,0,0,0,0.783931,18.0,0.934291,53
f9b8ad81-68f4-4845-bb8b-9b6cedb964d3,SRL-SOA: Self-Representation Learning with Sparse 1D-Operational Autoencoder for Hyperspectral Image Band Selection,6,0.210291,0.551468,"The band selection in the hyperspectral image (HSI) data processing is an
important task considering its effect on the computational complexity and
accuracy. In this work, we propose a novel framework for the band selection
problem: Self-Representation Learning (SRL) with Sparse 1D-Operational
Autoencoder (SOA). The proposed SLR-SOA approach introduces a novel autoencoder
model, SOA, that is designed to learn a representation domain where the data
are sparsely represented. Moreover, the network composes of 1D-operational
layers with the non-linear neuron model. Hence, the learning capability of
neurons (filters) is greatly improved with shallow architectures. Using compact
architectures is especially crucial in autoencoders as they tend to overfit
easily because of their identity mapping objective. Overall, we show that the
proposed SRL-SOA band selection approach outperforms the competing methods over
two HSI data including Indian Pines and Salinas-A considering the achieved land
cover classification accuracies. The software implementation of the SRL-SOA
approach is shared publicly at https://github.com/meteahishali/SRL-SOA.",1,0,1,0,0,0,0.666849,15.0,0.899007,19
7434a2ef-2189-4290-b910-baedc3024f2d,A Distributional Lens for Multi-Aspect Controllable Text Generation,22,0.283003,0.738625,"Multi-aspect controllable text generation is a more challenging and practical
task than single-aspect control. Existing methods achieve complex multi-aspect
control by fusing multiple controllers learned from single-aspect, but suffer
from attribute degeneration caused by the mutual interference of these
controllers. To address this, we provide observations on attribute fusion from
a distributional perspective and propose to directly search for the
intersection areas of multiple attribute distributions as their combination for
generation. Our method first estimates the attribute space with an autoencoder
structure. Afterward, we iteratively approach the intersections by jointly
minimizing distances to points representing different attributes. Finally, we
map them to attribute-relevant sentences with a prefix-tuning-based decoder.
Experiments on the three-aspect control task, including sentiment, topic, and
detoxification aspects, reveal that our method outperforms several strong
baselines on attribute relevance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory support for the effectiveness
of our approach.",1,1,0,0,1,0,0.756617,5.0,0.747272,30
5c46d8d2-6fbb-4fe5-acc6-1fc576de0a4a,Parallel Spatio-Temporal Attention-Based TCN for Multivariate Time Series Prediction,67,0.748023,0.944267,"As industrial systems become more complex and monitoring sensors for
everything from surveillance to our health become more ubiquitous, multivariate
time series prediction is taking an important place in the smooth-running of
our society. A recurrent neural network with attention to help extend the
prediction windows is the current-state-of-the-art for this task. However, we
argue that their vanishing gradients, short memories, and serial architecture
make RNNs fundamentally unsuited to long-horizon forecasting with complex data.
Temporal convolutional networks (TCNs) do not suffer from gradient problems and
they support parallel calculations, making them a more appropriate choice.
Additionally, they have longer memories than RNNs, albeit with some instability
and efficiency problems. Hence, we propose a framework, called PSTA-TCN, that
combines a parallel spatio-temporal attention mechanism to extract dynamic
internal correlations with stacked TCN backbones to extract features from
different window sizes. The framework makes full use parallel calculations to
dramatically reduce training times, while substantially increasing accuracy
with stable prediction windows up to 13 times longer than the status quo.",0,1,0,0,1,0,0.802047,8.0,0.859102,41
9f027b82-59fd-470c-840c-acfd4df45c5d,Registration based Few-Shot Anomaly Detection,64,0.791286,0.998717,"This paper considers few-shot anomaly detection (FSAD), a practical yet
under-studied setting for anomaly detection (AD), where only a limited number
of normal images are provided for each category at training. So far, existing
FSAD studies follow the one-model-per-category learning paradigm used for
standard AD, and the inter-category commonality has not been explored. Inspired
by how humans detect anomalies, i.e., comparing an image in question to normal
images, we here leverage registration, an image alignment task that is
inherently generalizable across categories, as the proxy task, to train a
category-agnostic anomaly detection model. During testing, the anomalies are
identified by comparing the registered features of the test image and its
corresponding support (normal) images. As far as we know, this is the first
FSAD method that trains a single generalizable model and requires no
re-training or parameter fine-tuning for new categories. Experimental results
have shown that the proposed method outperforms the state-of-the-art FSAD
methods by 3%-8% in AUC on the MVTec and MPDD benchmarks.",1,1,1,0,1,0,0.866391,9.0,0.898901,47
7f38a327-c9af-4731-83bb-151ca83fa92a,SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control,37,0.397776,0.451112,"Despite the growing success of diffusion models in continuous-valued domains
(e.g., images), similar efforts for discrete domains such as text have yet to
match the performance of autoregressive language models. In this work, we
present SSD-LM -- a diffusion-based language model with two key design choices.
First, SSD-LM is semi-autoregressive, iteratively generating blocks of text,
allowing for flexible output length at decoding time while enabling local
bidirectional context updates. Second, it is simplex-based, performing
diffusion on the natural vocabulary space rather than a learned latent space,
allowing us to incorporate classifier guidance and modular control using
off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on
unconstrained text generation benchmarks, and show that it matches or
outperforms strong autoregressive GPT-2 models across standard quality and
diversity metrics, while vastly outperforming diffusion-based baselines. On
controlled text generation, SSD-LM also outperforms competitive baselines, with
an extra advantage in modularity.",1,0,0,0,0,0,0.884037,5.0,0.831622,98
32a7df36-97dc-40b1-8134-edbb55c6fff3,A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models,8,0.122692,0.287998,"Pre-trained language models (PLMs) cannot well recall rich factual knowledge
of entities exhibited in large-scale corpora, especially those rare entities.
In this paper, we propose to build a simple but effective Pluggable Entity
Lookup Table (PELT) on demand by aggregating the entity's output
representations of multiple occurrences in the corpora. PELT can be compatibly
plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared
to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation
with capability of acquiring knowledge from out-of-domain corpora for domain
adaptation scenario. The experiments on knowledge-related tasks demonstrate
that our method, PELT, can flexibly and effectively transfer entity knowledge
from related corpora into PLMs with different architectures.",1,1,0,0,0,0,0.968482,6.0,0.938503,34
5ac0dca9-d441-4e47-bb13-0f807b28f866,"One Model, Any CSP: Graph Neural Networks as Fast Global Search Heuristics for Constraint Satisfaction",9,0.0242872,0.309732,"We propose a universal Graph Neural Network architecture which can be trained
as an end-2-end search heuristic for any Constraint Satisfaction Problem (CSP).
Our architecture can be trained unsupervised with policy gradient descent to
generate problem specific heuristics for any CSP in a purely data driven
manner. The approach is based on a novel graph representation for CSPs that is
both generic and compact and enables us to process every possible CSP instance
with one GNN, regardless of constraint arity, relations or domain size. Unlike
previous RL-based methods, we operate on a global search action space and allow
our GNN to modify any number of variables in every step of the stochastic
search. This enables our method to properly leverage the inherent parallelism
of GNNs. We perform a thorough empirical evaluation where we learn heuristics
for well known and important CSPs from random data, including graph coloring,
MaxCut, 3-SAT and MAX-k-SAT. Our approach outperforms prior approaches for
neural combinatorial optimization by a substantial margin. It can compete with,
and even improve upon, conventional search heuristics on test instances that
are several orders of magnitude larger and structurally more complex than those
seen during training.",0,1,0,0,0,0,0.0256866,10.0,0.474176,61
8147a44f-041d-4114-abf4-237d946c586b,Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning,47,0.62323,0.958067,"In recent years, reinforcement learning (RL) has shown outstanding
performance for locomotion control of highly articulated robotic systems. Such
approaches typically involve tedious reward function tuning to achieve the
desired motion style. Imitation learning approaches such as adversarial motion
priors aim to reduce this problem by encouraging a pre-defined motion style. In
this work, we present an approach to augment the concept of adversarial motion
prior-based RL to allow for multiple, discretely switchable styles. We show
that multiple styles and skills can be learned simultaneously without notable
performance differences, even in combination with motion data-free skills. Our
approach is validated in several real-world experiments with a wheeled-legged
quadruped robot showing skills learned from existing RL controllers and
trajectory optimization, such as ducking and walking, and novel skills such as
switching between a quadrupedal and humanoid configuration. For the latter
skill, the robot is required to stand up, navigate on two wheels, and sit down.
Instead of tuning the sit-down motion, we verify that a reverse playback of the
stand-up movement helps the robot discover feasible sit-down behaviors and
avoids tedious reward function tuning.",0,1,0,0,0,0,0.75788,9.0,0.860004,24
1e611b82-3626-4a9b-b5df-523995b5ded7,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,16,0.0272728,0.376032,"While both extractive and generative readers have been successfully applied
to the Question Answering (QA) task, little attention has been paid toward the
systematic comparison of them. Characterizing the strengths and weaknesses of
the two readers is crucial not only for making a more informed reader selection
in practice but also for developing a deeper understanding to foster further
research on improving readers in a principled manner. Motivated by this goal,
we make the first attempt to systematically study the comparison of extractive
and generative readers for question answering. To be aligned with the
state-of-the-art, we explore nine transformer-based large pre-trained language
models (PrLMs) as backbone architectures. Furthermore, we organize our findings
under two main categories: (1) keeping the architecture invariant, and (2)
varying the underlying PrLMs. Among several interesting findings, it is
important to highlight that (1) the generative readers perform better in long
context QA, (2) the extractive readers perform better in short context while
also showing better out-of-domain generalization, and (3) the encoder of
encoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and
outperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also
study the effect of multi-task learning on the two types of readers varying the
underlying PrLMs and perform qualitative and quantitative diagnosis to provide
further insights into future directions in modeling better readers.",0,0,0,0,0,0,0.516785,5.0,0.614427,32
968db164-9509-407c-8975-d6d8c4a4cd63,DEANet: Decomposition Enhancement and Adjustment Network for Low-Light Image Enhancement,6,0.122836,0.418847,"Images obtained under low-light conditions will seriously affect the quality
of the images. Solving the problem of poor low-light image quality can
effectively improve the visual quality of images and better improve the
usability of computer vision. In addition, it has very important applications
in many fields. This paper proposes a DEANet based on Retinex for low-light
image enhancement. It combines the frequency information and content
information of the image into three sub-networks: decomposition network,
enhancement network and adjustment network. These three sub-networks are
respectively used for decomposition, denoising, contrast enhancement and detail
preservation, adjustment, and image generation. Our model has good robust
results for all low-light images. The model is trained on the public data set
LOL, and the experimental results show that our method is better than the
existing state-of-the-art methods in terms of vision and quality.",0,1,0,0,1,0,0.952586,14.0,0.964666,37
a28395d2-596c-463b-b9e0-19da140d64e7,Scalable Joint Learning of Wireless Multiple-Access Policies and their Signaling,4,0.0901277,0.448471,"In this paper, we apply an multi-agent reinforcement learning (MARL)
framework allowing the base station (BS) and the user equipments (UEs) to
jointly learn a channel access policy and its signaling in a wireless multiple
access scenario. In this framework, the BS and UEs are reinforcement learning
(RL) agents that need to cooperate in order to deliver data. The comparison
with a contention-free and a contention-based baselines shows that our
framework achieves a superior performance in terms of goodput even in high
traffic situations while maintaining a low collision rate. The scalability of
the proposed method is studied, since it is a major problem in MARL and this
paper provides the first results in order to address it.",0,1,0,0,0,0,0.423466,7.0,0.684862,12
8d770cc7-f864-4d91-8a62-cf75c3339201,Bokeh-Loss GAN: Multi-Stage Adversarial Training for Realistic Edge-Aware Bokeh,5,0.176362,0.13868,"In this paper, we tackle the problem of monocular bokeh synthesis, where we
attempt to render a shallow depth of field image from a single all-in-focus
image. Unlike in DSLR cameras, this effect can not be captured directly in
mobile cameras due to the physical constraints of the mobile aperture. We thus
propose a network-based approach that is capable of rendering realistic
monocular bokeh from single image inputs. To do this, we introduce three new
edge-aware Bokeh Losses based on a predicted monocular depth map, that sharpens
the foreground edges while blurring the background. This model is then
finetuned using an adversarial loss to generate a realistic Bokeh effect.
Experimental results show that our approach is capable of generating a
pleasing, natural Bokeh effect with sharp edges while handling complicated
scenes.",0,1,0,0,0,0,0.722555,9.0,0.848788,47
b08122b7-f411-478c-8a9b-316a5920bf91,Boosting 3D Adversarial Attacks with Attacking On Frequency,21,0.107377,0.648193,"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial
attacks. Recently, 3D adversarial attacks, especially adversarial attacks on
point clouds, have elicited mounting interest. However, adversarial point
clouds obtained by previous methods show weak transferability and are easy to
defend. To address these problems, in this paper we propose a novel point cloud
attack (dubbed AOF) that pays more attention on the low-frequency component of
point clouds. We combine the losses from point cloud and its low-frequency
component to craft adversarial samples. Extensive experiments validate that AOF
can improve the transferability significantly compared to state-of-the-art
(SOTA) attacks, and is more robust to SOTA 3D defense methods. Otherwise,
compared to clean point clouds, adversarial point clouds obtained by AOF
contain more deformation than outlier.",0,1,0,0,1,0,0.405869,6.0,0.622983,43
99b6144f-6495-4dc7-9181-93d93d269895,Explanations from Large Language Models Make Small Reasoners Better,67,0.674896,0.607495,"Integrating free-text explanations to in-context learning of large language
models (LLM) is shown to elicit strong reasoning capabilities along with
reasonable explanations. In this paper, we consider the problem of leveraging
the explanations generated by LLM to improve the training of small reasoners,
which are more favorable in real-production deployment due to their low cost.
We systematically explore three explanation generation approaches from LLM and
utilize a multi-task learning framework to facilitate small models to acquire
strong reasoning power together with explanation generation capabilities.
Experiments on multiple reasoning tasks show that our method can consistently
and significantly outperform finetuning baselines across different settings,
and even perform better than finetuning/prompting a 60x larger GPT-3 (175B)
model by up to 9.5% in accuracy. As a side benefit, human evaluation further
shows that our method can generate high-quality explanations to justify its
predictions, moving towards the goal of explainable AI.",0,1,0,0,1,0,0.954747,4.0,0.880128,54
82cb002c-1471-4fca-b890-0b47cd221ef7,USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration Network for Multilingual Complex Named Entity Recognition,14,0.34116,0.862492,"This paper describes the system developed by the USTC-NELSLIP team for
SemEval-2022 Task 11 Multilingual Complex Named Entity Recognition
(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to
improve the performance of language models for recognizing complex named
entities. The method first adapts the representations of gazetteer networks to
those of language models by minimizing the KL divergence between them. After
adaptation, these two networks are then integrated for backend supervised named
entity recognition (NER) training. The proposed method is applied to several
state-of-the-art Transformer-based NER models with a gazetteer built from
Wikidata, and shows great generalization ability across them. The final
predictions are derived from an ensemble of these trained models. Experimental
results and detailed analysis verify the effectiveness of the proposed method.
The official results show that our system ranked 1st on three tracks (Chinese,
Code-mixed and Bangla) and 2nd on the other ten tracks in this task.",1,1,0,0,1,0,0.985497,5.0,0.966712,24
2193f18e-f940-49be-97ec-c79f46cd9269,Read Top News First: A Document Reordering Approach for Multi-Document News Summarization,11,0.223255,0.456774,"A common method for extractive multi-document news summarization is to
re-formulate it as a single-document summarization problem by concatenating all
documents as a single meta-document. However, this method neglects the relative
importance of documents. We propose a simple approach to reorder the documents
according to their relative importance before concatenating and summarizing
them. The reordering makes the salient content easier to learn by the
summarization model. Experiments show that our approach outperforms previous
state-of-the-art methods with more complex architectures.",1,1,0,0,1,0,0.860178,8.0,0.883408,28
0c731477-d9ef-49ec-b510-7c7dcb520446,ViWOZ: A Multi-Domain Task-Oriented Dialogue Systems Dataset For Low-resource Language,1,0.0359182,0.10532,"Most of the current task-oriented dialogue systems (ToD), despite having
interesting results, are designed for a handful of languages like Chinese and
English. Therefore, their performance in low-resource languages is still a
significant problem due to the absence of a standard dataset and evaluation
policy. To address this problem, we proposed ViWOZ, a fully-annotated
Vietnamese task-oriented dialogue dataset. ViWOZ is the first multi-turn,
multi-domain tasked oriented dataset in Vietnamese, a low-resource language.
The dataset consists of a total of 5,000 dialogues, including 60,946 fully
annotated utterances. Furthermore, we provide a comprehensive benchmark of both
modular and end-to-end models in low-resource language scenarios. With those
characteristics, the ViWOZ dataset enables future studies on creating a
multilingual task-oriented dialogue system.",0,1,1,1,0,0,0.935975,5.0,0.880322,42
d99446e9-3e6a-4b20-81fe-9f061d385137,Temporal Attention for Language Models,25,0.146603,0.680145,"Pretrained language models based on the transformer architecture have shown
great success in NLP. Textual training data often comes from the web and is
thus tagged with time-specific information, but most language models ignore
this information. They are trained on the textual data alone, limiting their
ability to generalize temporally. In this work, we extend the key component of
the transformer architecture, i.e., the self-attention mechanism, and propose
temporal attention - a time-aware self-attention mechanism. Temporal attention
can be applied to any transformer model and requires the input texts to be
accompanied with their relevant time points. It allows the transformer to
capture this temporal information and create time-specific contextualized word
representations. We leverage these representations for the task of semantic
change detection; we apply our proposed mechanism to BERT and experiment on
three datasets in different languages (English, German, and Latin) that also
vary in time, size, and genre. Our proposed model achieves state-of-the-art
results on all the datasets.",1,1,0,0,1,0,0.249722,6.0,0.523895,36
0a426173-1423-4eab-85c5-904a999c9c60,Understanding Iterative Revision from Human-Written Text,38,0.300032,0.981334,"Writing is, by nature, a strategic, adaptive, and more importantly, an
iterative process. A crucial part of writing is editing and revising the text.
Previous works on text revision have focused on defining edit intention
taxonomies within a single domain or developing computational models with a
single level of edit granularity, such as sentence-level edits, which differ
from human's revision cycles. This work describes IteraTeR: the first
large-scale, multi-domain, edit-intention annotated corpus of iteratively
revised text. In particular, IteraTeR is collected based on a new framework to
comprehensively model the iterative text revisions that generalize to various
domains of formal writing, edit intentions, revision depths, and granularities.
When we incorporate our annotated edit intentions, both generative and
edit-based text revision models significantly improve automatic evaluations.
Through our work, we better understand the text revision process, making vital
connections between edit intentions and writing quality, enabling the creation
of diverse corpora to support computational modeling of iterative text
revisions.",1,1,1,1,0,0,0.101499,10.0,0.615589,53
b1858f6b-161a-4aa5-9d1f-56fff05221cc,A Moral- and Event- Centric Inspection of Gender Bias in Fairy Tales at A Large Scale,5,0.0580828,0.283024,"Fairy tales are a common resource for young children to learn a language or
understand how a society works. However, gender bias, e.g., stereotypical
gender roles, in this literature may cause harm and skew children's world view.
Instead of decades of qualitative and manual analysis of gender bias in fairy
tales, we computationally analyze gender bias in a fairy tale dataset
containing 624 fairy tales from 7 different cultures. We specifically examine
gender difference in terms of moral foundations, which are measures of human
morality, and events, which reveal human activities associated with each
character. We find that the number of male characters is two times that of
female characters, showing a disproportionate gender representation. Our
analysis further reveal stereotypical portrayals of both male and female
characters in terms of moral foundations and events. Female characters turn out
more associated with care-, loyalty- and sanctity- related moral words, while
male characters are more associated with fairness- and authority- related moral
words. Female characters' events are often about emotion (e.g., weep),
appearance (e.g., comb), household (e.g., bake), etc.; while male characters'
events are more about profession (e.g., hunt), violence (e.g., destroy),
justice (e.g., judge), etc. Gender bias in terms of moral foundations shows an
obvious difference across cultures. For example, female characters are more
associated with care and sanctity in high uncertainty-avoidance cultures which
are less open to changes and unpredictability. Based on the results, we propose
implications for children's literature and early literacy research.",1,0,0,0,0,0,0.347783,5.0,0.508095,36
a438c1da-c9f1-484a-95a6-376aeb193cad,What Do We Maximize in Self-Supervised Learning?,12,0.0372886,0.313859,"In this paper, we examine self-supervised learning methods, particularly
VICReg, to provide an information-theoretical understanding of their
construction. As a first step, we demonstrate how information-theoretic
quantities can be obtained for a deterministic network, offering a possible
alternative to prior work that relies on stochastic models. This enables us to
demonstrate how VICReg can be (re)discovered from first principles and its
assumptions about data distribution. Furthermore, we empirically demonstrate
the validity of our assumptions, confirming our novel understanding of VICReg.
Finally, we believe that the derivation and insights we obtain can be
generalized to many other SSL methods, opening new avenues for theoretical and
practical understanding of SSL and transfer learning.",0,0,0,0,0,0,0.185993,8.0,0.601206,42
3f51a76d-c5d6-4239-851b-8ab0f9d4346a,Registering Explicit to Implicit: Towards High-Fidelity Garment mesh Reconstruction from Single Images,24,0.286912,0.869975,"Fueled by the power of deep learning techniques and implicit shape learning,
recent advances in single-image human digitalization have reached unprecedented
accuracy and could recover fine-grained surface details such as garment
wrinkles. However, a common problem for the implicit-based methods is that they
cannot produce separated and topology-consistent mesh for each garment piece,
which is crucial for the current 3D content creation pipeline. To address this
issue, we proposed a novel geometry inference framework ReEF that reconstructs
topology-consistent layered garment mesh by registering the explicit garment
template to the whole-body implicit fields predicted from single images.
Experiments demonstrate that our method notably outperforms its counterparts on
single-image layered garment reconstruction and could bring high-quality
digital assets for further content creation.",0,1,0,0,0,0,0.841694,6.0,0.83368,54
c88e3082-0387-4c15-a590-181f83d122f8,Large Language Models Are Reasoning Teachers,148,0.992709,0.963213,"Recent works have shown that chain-of-thought (CoT) prompting can elicit
language models to solve complex reasoning tasks, step-by-step. However,
prompt-based CoT methods are dependent on very large models such as GPT-3 175B
which are prohibitive to deploy at scale. In this paper, we use these large
models as reasoning teachers to enable complex reasoning in smaller models and
reduce model size requirements by several orders of magnitude. We propose
Fine-tune-CoT, a method that generates reasoning samples from very large
teacher models to fine-tune smaller models. We evaluate our method on a wide
range of public models and complex tasks. We find that Fine-tune-CoT enables
substantial reasoning capability in small models, far outperforming
prompt-based baselines and even the teacher model in many tasks. Additionally,
we extend our method by leveraging the teacher model's ability to generate
multiple distinct rationales for each original sample. Enriching the
fine-tuning data with such diverse reasoning results in a substantial
performance boost across datasets, even for very small models. We conduct
ablations and sample studies to understand the emergence of reasoning
capabilities of student models. Our code implementation and data are available
at https://github.com/itsnamgyu/reasoning-teacher.",1,1,0,0,0,0,0.990779,4.0,0.983797,75
7d2f1985-0c6c-439f-9a2a-b2729011294c,Miko Team: Deep Learning Approach for Legal Question Answering in ALQAC 2022,5,0.118952,0.152625,"We introduce efficient deep learning-based methods for legal document
processing including Legal Document Retrieval and Legal Question Answering
tasks in the Automated Legal Question Answering Competition (ALQAC 2022). In
this competition, we achieve 1\textsuperscript{st} place in the first task and
3\textsuperscript{rd} place in the second task. Our method is based on the
XLM-RoBERTa model that is pre-trained from a large amount of unlabeled corpus
before fine-tuning to the specific tasks. The experimental results showed that
our method works well in legal retrieval information tasks with limited labeled
data. Besides, this method can be applied to other information retrieval tasks
in low-resource languages.",0,1,0,0,1,0,0.810784,10.0,0.890031,23
da155514-1779-4a61-9a8a-d285c2a39d1e,Age of Semantics in Cooperative Communications: To Expedite Simulation Towards Real via Offline Reinforcement Learning,3,0.0592613,0.135395,"The age of information metric fails to correctly describe the intrinsic
semantics of a status update. In an intelligent reflecting surface-aided
cooperative relay communication system, we propose the age of semantics (AoS)
for measuring semantics freshness of the status updates. Specifically, we focus
on the status updating from a source node (SN) to the destination, which is
formulated as a Markov decision process (MDP). The objective of the SN is to
maximize the expected satisfaction of AoS and energy consumption under the
maximum transmit power constraint. To seek the optimal control policy, we first
derive an online deep actor-critic (DAC) learning scheme under the on-policy
temporal difference learning framework. However, implementing the online DAC in
practice poses the key challenge in infinitely repeated interactions between
the SN and the system, which can be dangerous particularly during the
exploration. We then put forward a novel offline DAC scheme, which estimates
the optimal control policy from a previously collected dataset without any
further interactions with the system. Numerical experiments verify the
theoretical results and show that our offline DAC scheme significantly
outperforms the online DAC scheme and the most representative baselines in
terms of mean utility, demonstrating strong robustness to dataset quality.",0,0,0,0,0,0,0.557747,7.0,0.74101,63
ceb1021e-e003-4dac-a254-42a1931b71f4,Treewidth-aware Reductions of Normal ASP to SAT -- Is Normal ASP Harder than SAT after All?,19,0.30936,0.973945,"Answer Set Programming (ASP) is a paradigm for modeling and solving problems
for knowledge representation and reasoning. There are plenty of results
dedicated to studying the hardness of (fragments of) ASP. So far, these studies
resulted in characterizations in terms of computational complexity as well as
in fine-grained insights presented in form of dichotomy-style results, lower
bounds when translating to other formalisms like propositional satisfiability
(SAT), and even detailed parameterized complexity landscapes. A generic
parameter in parameterized complexity originating from graph theory is the
so-called treewidth, which in a sense captures structural density of a program.
Recently, there was an increase in the number of treewidth-based solvers
related to SAT. While there are translations from (normal) ASP to SAT, no
reduction that preserves treewidth or at least keeps track of the treewidth
increase is known. In this paper we propose a novel reduction from normal ASP
to SAT that is aware of the treewidth, and guarantees that a slight increase of
treewidth is indeed sufficient. Further, we show a new result establishing
that, when considering treewidth, already the fragment of normal ASP is
slightly harder than SAT (under reasonable assumptions in computational
complexity). This also confirms that our reduction probably cannot be
significantly improved and that the slight increase of treewidth is
unavoidable. Finally, we present an empirical study of our novel reduction from
normal ASP to SAT, where we compare treewidth upper bounds that are obtained
via known decomposition heuristics. Overall, our reduction works better with
these heuristics than existing translations.",0,0,0,0,0,0,0.0237305,13.0,0.58935,94
06073f6d-6355-4015-8240-eaaaf9cbfa31,Reproducibility in machine learning for medical imaging,5,0.0,0.249446,"Reproducibility is a cornerstone of science, as the replication of findings
is the process through which they become knowledge. It is widely considered
that many fields of science are undergoing a reproducibility crisis. This has
led to the publications of various guidelines in order to improve research
reproducibility.
  This didactic chapter intends at being an introduction to reproducibility for
researchers in the field of machine learning for medical imaging. We first
distinguish between different types of reproducibility. For each of them, we
aim at defining it, at describing the requirements to achieve it and at
discussing its utility. The chapter ends with a discussion on the benefits of
reproducibility and with a plea for a non-dogmatic approach to this concept and
its implementation in research practice.",0,0,0,0,0,0,0.00987238,10.0,0.37775,84
42e0faf8-51de-4d44-b62b-fcb7056cc82c,COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning,49,0.601259,0.930748,"Graph contrastive learning (GCL) improves graph representation learning,
leading to SOTA on various downstream tasks. The graph augmentation step is a
vital but scarcely studied step of GCL. In this paper, we show that the node
embedding obtained via the graph augmentations is highly biased, somewhat
limiting contrastive models from learning discriminative features for
downstream tasks. Thus, instead of investigating graph augmentation in the
input space, we alternatively propose to perform augmentations on the hidden
features (feature augmentation). Inspired by so-called matrix sketching, we
propose COSTA, a novel COvariance-preServing feaTure space Augmentation
framework for GCL, which generates augmented features by maintaining a ""good
sketch"" of original features. To highlight the superiority of feature
augmentation with COSTA, we investigate a single-view setting (in addition to
multi-view one) which conserves memory and computations. We show that the
feature augmentation with COSTA achieves comparable/better results than graph
augmentation based models.",0,0,0,0,0,0,0.903176,5.0,0.847703,52
6687be60-089c-47e2-9f56-0a55eac4d225,Meta-Learning Regrasping Strategies for Physical-Agnostic Objects,10,0.0408174,0.414441,"Grasping inhomogeneous objects in real-world applications remains a
challenging task due to the unknown physical properties such as mass
distribution and coefficient of friction. In this study, we propose a
meta-learning algorithm called ConDex, which incorporates Conditional Neural
Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical
properties of objects using depth images. ConDex efficiently acquires physical
embeddings from limited trials, enabling precise grasping point estimation.
Furthermore, ConDex is capable of updating the predicted grasping quality
iteratively from new trials in an online fashion. To the best of our knowledge,
we are the first who generate two object datasets focusing on inhomogeneous
physical properties with varying mass distributions and friction coefficients.
Extensive evaluations in simulation demonstrate ConDex's superior performance
over DexNet-2.0 and existing meta-learning-based grasping pipelines.
Furthermore, ConDex shows robust generalization to previously unseen real-world
objects despite training solely in the simulation. The synthetic and real-world
datasets will be published as well.",0,1,1,1,1,0,0.280672,5.0,0.456039,60
7704e40c-3f17-4132-8dd6-bb1db82432f6,Speech Resources in the Tamasheq Language,13,0.470006,0.715946,"In this paper we present two datasets for Tamasheq, a developing language
mainly spoken in Mali and Niger. These two datasets were made available for the
IWSLT 2022 low-resource speech translation track, and they consist of
collections of radio recordings from daily broadcast news in Niger (Studio
Kalangou) and Mali (Studio Tamani). We share (i) a massive amount of unlabeled
audio data (671 hours) in five languages: French from Niger, Fulfulde, Hausa,
Tamasheq and Zarma, and (ii) a smaller 17 hours parallel corpus of audio
recordings in Tamasheq, with utterance-level translations in the French
language. All this data is shared under the Creative Commons BY-NC-ND 3.0
license. We hope these resources will inspire the speech community to develop
and benchmark models using the Tamasheq language.",0,1,1,1,0,0,0.939119,7.0,0.917109,27
03ea0dae-d434-496a-b527-bd2ce7cb896e,Deep Understanding based Multi-Document Machine Reading Comprehension,2,0.0362789,0.328947,"Most existing multi-document machine reading comprehension models mainly
focus on understanding the interactions between the input question and
documents, but ignore following two kinds of understandings. First, to
understand the semantic meaning of words in the input question and documents
from the perspective of each other. Second, to understand the supporting cues
for a correct answer from the perspective of intra-document and
inter-documents. Ignoring these two kinds of important understandings would
make the models oversee some important information that may be helpful for
inding correct answers. To overcome this deiciency, we propose a deep
understanding based model for multi-document machine reading comprehension. It
has three cascaded deep understanding modules which are designed to understand
the accurate semantic meaning of words, the interactions between the input
question and documents, and the supporting cues for the correct answer. We
evaluate our model on two large scale benchmark datasets, namely TriviaQA Web
and DuReader. Extensive experiments show that our model achieves
state-of-the-art results on both datasets.",1,1,0,0,1,0,0.384735,7.0,0.666915,57
2308a08c-5c80-4769-ba02-cd11e301ff60,GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs,21,0.11249,0.497026,"Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a
challenging problem due to massive and complicated structures in many KGs.
Recently, many promising works projected entities and queries into a geometric
space to efficiently find answers. However, it remains challenging to model the
negation and union operator. The negation operator has no strict boundaries,
which generates overlapped embeddings and leads to obtaining ambiguous answers.
An additional limitation is that the union operator is non-closure, which
undermines the model to handle a series of union operators. To address these
problems, we propose a novel probabilistic embedding model, namely Gamma
Embeddings (GammaE), for encoding entities and queries to answer different
types of FOL queries on KGs. We utilize the linear property and strong boundary
support of the Gamma distribution to capture more features of entities and
queries, which dramatically reduces model uncertainty. Furthermore, GammaE
implements the Gamma mixture method to design the closed union operator. The
performance of GammaE is validated on three large logical query datasets.
Experimental results show that GammaE significantly outperforms
state-of-the-art models on public benchmarks.",1,0,0,0,1,0,0.136636,9.0,0.608076,41
3d57336e-2f0e-405c-8d11-43ddd5fa561a,Deep Learning Reproducibility and Explainable AI (XAI),7,0.0,0.217851,"The nondeterminism of Deep Learning (DL) training algorithms and its
influence on the explainability of neural network (NN) models are investigated
in this work with the help of image classification examples. To discuss the
issue, two convolutional neural networks (CNN) have been trained and their
results compared. The comparison serves the exploration of the feasibility of
creating deterministic, robust DL models and deterministic explainable
artificial intelligence (XAI) in practice. Successes and limitation of all here
carried out efforts are described in detail. The source code of the attained
deterministic models has been listed in this work. Reproducibility is indexed
as a development-phase-component of the Model Governance Framework, proposed by
the EU within their excellence in AI approach. Furthermore, reproducibility is
a requirement for establishing causality for the interpretation of model
results and building of trust towards the overwhelming expansion of AI systems
applications. Problems that have to be solved on the way to reproducibility and
ways to deal with some of them, are examined in this work.",0,1,0,0,0,1,0.0309469,9.0,0.436751,50
88d3bca3-18fd-4653-a6cc-cbf4878f88f6,Unknown-Aware Domain Adversarial Learning for Open-Set Domain Adaptation,19,0.203927,0.896969,"Open-Set Domain Adaptation (OSDA) assumes that a target domain contains
unknown classes, which are not discovered in a source domain. Existing domain
adversarial learning methods are not suitable for OSDA because distribution
matching with $\textit{unknown}$ classes leads to negative transfer. Previous
OSDA methods have focused on matching the source and the target distribution by
only utilizing $\textit{known}$ classes. However, this $\textit{known}$-only
matching may fail to learn the target-$\textit{unknown}$ feature space.
Therefore, we propose Unknown-Aware Domain Adversarial Learning (UADAL), which
$\textit{aligns}$ the source and the target-$\textit{known}$ distribution while
simultaneously $\textit{segregating}$ the target-$\textit{unknown}$
distribution in the feature alignment procedure. We provide theoretical
analyses on the optimized state of the proposed $\textit{unknown-aware}$
feature alignment, so we can guarantee both $\textit{alignment}$ and
$\textit{segregation}$ theoretically. Empirically, we evaluate UADAL on the
benchmark datasets, which shows that UADAL outperforms other methods with
better feature alignments by reporting state-of-the-art performances.",1,0,1,0,1,0,0.712524,10.0,0.861099,57
88331ce1-62f7-41e6-ad99-da7759f87d83,SpA-Former: Transformer image shadow detection and removal via spatial attention,17,0.199612,0.307232,"In this paper, we propose an end-to-end SpA-Former to recover a shadow-free
image from a single shaded image. Unlike traditional methods that require two
steps for shadow detection and then shadow removal, the SpA-Former unifies
these steps into one, which is a one-stage network capable of directly learning
the mapping function between shadows and no shadows, it does not require a
separate shadow detection. Thus, SpA-former is adaptable to real image
de-shadowing for shadows projected on different semantic regions. SpA-Former
consists of transformer layer and a series of joint Fourier transform residual
blocks and two-wheel joint spatial attention. The network in this paper is able
to handle the task while achieving a very fast processing efficiency.
  Our code is relased on
https://github.com/zhangbaijin/SpA-Former-shadow-removal",1,1,0,0,0,0,0.475145,5.0,0.590298,22
9ae7f0a0-039c-4acf-91db-49cca2baaef8,MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages,30,0.0830148,0.568546,"While there has been a recent burgeoning of applications at the intersection
of natural and programming languages, such as code generation and code
summarization, these applications are usually English-centric. This creates a
barrier for program developers who are not proficient in English. To mitigate
this gap in technology development across languages, we propose a multilingual
dataset, MCoNaLa, to benchmark code generation from natural language commands
extending beyond English. Modeled off of the methodology from the English
Code/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896
NL-code pairs in three languages: Spanish, Japanese, and Russian. We present a
quantitative evaluation of performance on the MCoNaLa dataset by testing with
state-of-the-art code generation systems. While the difficulties vary across
these three languages, all systems lag significantly behind their English
counterparts, revealing the challenges in adapting code generation to new
languages.",0,1,1,1,0,0,0.293981,5.0,0.467067,63
a5d8caae-2b24-4197-8b16-b4cef8532dd1,Localized Vision-Language Matching for Open-vocabulary Object Detection,17,0.148463,0.614434,"In this work, we propose an open-vocabulary object detection method that,
based on image-caption pairs, learns to detect novel object classes along with
a given set of known classes. It is a two-stage training approach that first
uses a location-guided image-caption matching technique to learn class labels
for both novel and known classes in a weakly-supervised manner and second
specializes the model for the object detection task using known class
annotations. We show that a simple language model fits better than a large
contextualized language model for detecting novel objects. Moreover, we
introduce a consistency-regularization technique to better exploit
image-caption pair information. Our method compares favorably to existing
open-vocabulary detection approaches while being data-efficient. Source code is
available at https://github.com/lmb-freiburg/locov .",1,1,0,0,1,0,0.819384,6.0,0.821311,47
331f3e99-ad15-476a-ab06-2e1faae00d6e,Stability of Syntactic Dialect Classification Over Space and Time,5,0.164103,0.116491,"This paper analyses the degree to which dialect classifiers based on
syntactic representations remain stable over space and time. While previous
work has shown that the combination of grammar induction and geospatial text
classification produces robust dialect models, we do not know what influence
both changing grammars and changing populations have on dialect models. This
paper constructs a test set for 12 dialects of English that spans three years
at monthly intervals with a fixed spatial distribution across 1,120 cities.
Syntactic representations are formulated within the usage-based Construction
Grammar paradigm (CxG). The decay rate of classification performance for each
dialect over time allows us to identify regions undergoing syntactic change.
And the distribution of classification accuracy within dialect regions allows
us to identify the degree to which the grammar of a dialect is internally
heterogeneous. The main contribution of this paper is to show that a rigorous
evaluation of dialect classification models can be used to find both variation
over space and change over time.",0,1,0,0,0,0,0.224521,12.0,0.751768,55
f6d569e4-d95e-4ed9-983b-6a9f2b7d8ef2,"mRI: Multi-modal 3D Human Pose Estimation Dataset using mmWave, RGB-D, and Inertial Sensors",29,0.260993,0.983752,"The ability to estimate 3D human body pose and movement, also known as human
pose estimation (HPE), enables many applications for home-based health
monitoring, such as remote rehabilitation training. Several possible solutions
have emerged using sensors ranging from RGB cameras, depth sensors,
millimeter-Wave (mmWave) radars, and wearable inertial sensors. Despite
previous efforts on datasets and benchmarks for HPE, few dataset exploits
multiple modalities and focuses on home-based health monitoring. To bridge the
gap, we present mRI, a multi-modal 3D human pose estimation dataset with
mmWave, RGB-D, and Inertial Sensors. Our dataset consists of over 160k
synchronized frames from 20 subjects performing rehabilitation exercises and
supports the benchmarks of HPE and action detection. We perform extensive
experiments using our dataset and delineate the strength of each modality. We
hope that the release of mRI can catalyze the research in pose estimation,
multi-modal learning, and action understanding, and more importantly facilitate
the applications of home-based health monitoring.",1,1,1,1,0,0,0.26656,12.0,0.768284,56
f4c89dd5-2755-44b0-994f-5c3cbaea43c7,"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",74,0.529267,0.788986,"Humans excel at continually learning from an ever-changing environment
whereas it remains a challenge for deep neural networks which exhibit
catastrophic forgetting. The complementary learning system (CLS) theory
suggests that the interplay between rapid instance-based learning and slow
structured learning in the brain is crucial for accumulating and retaining
knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)
method which maintains short-term and long-term semantic memories that interact
with the episodic memory. Our method employs an effective replay mechanism
whereby new knowledge is acquired while aligning the decision boundaries with
the semantic memories. CLS-ER does not utilize the task boundaries or make any
assumption about the distribution of the data which makes it versatile and
suited for ""general continual learning"". Our approach achieves state-of-the-art
performance on standard benchmarks as well as more realistic general continual
learning settings.",1,0,0,0,1,1,0.793493,8.0,0.855794,47
a8a2136f-9877-490c-8acd-9a39e04e90a6,STEdge: Self-training Edge Detection with Multi-layer Teaching and Regularization,3,0.0239789,0.163589,"Learning-based edge detection has hereunto been strongly supervised with
pixel-wise annotations which are tedious to obtain manually. We study the
problem of self-training edge detection, leveraging the untapped wealth of
large-scale unlabeled image datasets. We design a self-supervised framework
with multi-layer regularization and self-teaching. In particular, we impose a
consistency regularization which enforces the outputs from each of the multiple
layers to be consistent for the input image and its perturbed counterpart. We
adopt L0-smoothing as the 'perturbation' to encourage edge prediction lying on
salient boundaries following the cluster assumption in self-supervised
learning. Meanwhile, the network is trained with multi-layer supervision by
pseudo labels which are initialized with Canny edges and then iteratively
refined by the network as the training proceeds. The regularization and
self-teaching together attain a good balance of precision and recall, leading
to a significant performance boost over supervised methods, with lightweight
refinement on the target dataset. Furthermore, our method demonstrates strong
cross-dataset generality. For example, it attains 4.8% improvement for ODS and
5.8% for OIS when tested on the unseen BIPED dataset, compared to the
state-of-the-art methods.",0,1,0,0,0,1,0.239464,11.0,0.735905,71
a3a3437a-bebe-4a1f-8540-16e91d7f6006,Data augmentation on graphs for table type classification,2,0.0534286,0.09767,"Tables are widely used in documents because of their compact and structured
representation of information. In particular, in scientific papers, tables can
sum up novel discoveries and summarize experimental results, making the
research comparable and easily understandable by scholars. Since the layout of
tables is highly variable, it would be useful to interpret their content and
classify them into categories. This could be helpful to directly extract
information from scientific papers, for instance comparing performance of some
models given their paper result tables. In this work, we address the
classification of tables using a Graph Neural Network, exploiting the table
structure for the message passing algorithm in use. We evaluate our model on a
subset of the Tab2Know dataset. Since it contains few examples manually
annotated, we propose data augmentation techniques directly on the table graph
structures. We achieve promising preliminary results, proposing a data
augmentation method suitable for graph-based table representation.",0,1,0,0,0,0,0.665642,5.0,0.696363,17
cc67d048-b15b-4f21-9823-be64f4a3642d,One-Shot Adaptation of GAN in Just One CLIP,29,0.68231,0.843755,"There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.",1,1,0,0,1,0,0.984668,4.0,0.955088,70
95ab52f8-2163-426d-9e36-cdc910b68aac,Learning Physical Dynamics with Subequivariant Graph Neural Networks,15,0.146432,0.575253,"Graph Neural Networks (GNNs) have become a prevailing tool for learning
physical dynamics. However, they still encounter several challenges: 1)
Physical laws abide by symmetry, which is a vital inductive bias accounting for
model generalization and should be incorporated into the model design. Existing
simulators either consider insufficient symmetry, or enforce excessive
equivariance in practice when symmetry is partially broken by gravity. 2)
Objects in the physical world possess diverse shapes, sizes, and properties,
which should be appropriately processed by the model. To tackle these
difficulties, we propose a novel backbone, Subequivariant Graph Neural Network,
which 1) relaxes equivariance to subequivariance by considering external fields
like gravity, where the universal approximation ability holds theoretically; 2)
introduces a new subequivariant object-aware message passing for learning
physical interactions between multiple objects of various shapes in the
particle-based representation; 3) operates in a hierarchical fashion, allowing
for modeling long-range and complex interactions. Our model achieves on average
over 3% enhancement in contact prediction accuracy across 8 scenarios on
Physion and 2X lower rollout MSE on RigidFall compared with state-of-the-art
GNN simulators, while exhibiting strong generalization and data efficiency.",1,0,0,0,1,0,0.481992,6.0,0.661942,51
7eeb36f6-4f16-4a4a-8dd7-a67994973bc4,Evaluating Long-Term Memory in 3D Mazes,13,0.0352937,0.536736,"Intelligent agents need to remember salient information to reason in
partially-observed environments. For example, agents with a first-person view
should remember the positions of relevant objects even if they go out of view.
Similarly, to effectively navigate through rooms agents need to remember the
floor plan of how rooms are connected. However, most benchmark tasks in
reinforcement learning do not test long-term memory in agents, slowing down
progress in this important research direction. In this paper, we introduce the
Memory Maze, a 3D domain of randomized mazes specifically designed for
evaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze
measures long-term memory separate from confounding agent abilities and
requires the agent to localize itself by integrating information over time.
With Memory Maze, we propose an online reinforcement learning benchmark, a
diverse offline dataset, and an offline probing evaluation. Recording a human
player establishes a strong baseline and verifies the need to build up and
retain memories, which is reflected in their gradually increasing rewards
within each episode. We find that current algorithms benefit from training with
truncated backpropagation through time and succeed on small mazes, but fall
short of human performance on the large mazes, leaving room for future
algorithmic designs to be evaluated on the Memory Maze.",1,0,1,1,0,0,0.426356,7.0,0.68616,31
ef5d304a-42a7-435e-83fb-5555327f12b7,Structured Local Radiance Fields for Human Avatar Modeling,78,0.967603,0.693802,"It is extremely challenging to create an animatable clothed human avatar from
RGB videos, especially for loose clothes due to the difficulties in motion
modeling. To address this problem, we introduce a novel representation on the
basis of recent neural scene rendering techniques. The core of our
representation is a set of structured local radiance fields, which are anchored
to the pre-defined nodes sampled on a statistical human body template. These
local radiance fields not only leverage the flexibility of implicit
representation in shape and appearance modeling, but also factorize cloth
deformations into skeleton motions, node residual translations and the dynamic
detail variations inside each individual radiance field. To learn our
representation from RGB data and facilitate pose generalization, we propose to
learn the node translations and the detail variations in a conditional
generative latent space. Overall, our method enables automatic construction of
animatable human avatars for various types of clothes without the need for
scanning subject-specific templates, and can generate realistic images with
dynamic details for novel poses. Experiment show that our method outperforms
state-of-the-art methods both qualitatively and quantitatively.",0,0,0,0,1,0,0.985925,5.0,0.968123,93
087c677f-6b89-42fa-a90c-99c6b2f09c64,Dreamento: an open-source dream engineering toolbox for sleep EEG wearables,5,0.023388,0.468914,"We introduce Dreamento (Dream engineering toolbox), an open-source Python
package for dream engineering using sleep electroencephalography (EEG)
wearables. Dreamento main functions are (1) real-time recording, monitoring,
analysis, and sensory stimulation, and (2) offline post-processing of the
resulting data, both in a graphical user interface (GUI). In real-time,
Dreamento is capable of (1) data recording, visualization, and navigation, (2)
power-spectrum analysis, (3) automatic sleep scoring, (4) sensory stimulation
(visual, auditory, tactile), (5) establishing text-to-speech communication, and
(6) managing annotations of automatic and manual events. The offline functions
aid in post-processing the acquired data with features to reformat the wearable
data and integrate it with non-wearable recorded modalities such as
electromyography (EMG). While Dreamento was primarily developed for (lucid)
dreaming studies, its applications can be extended to other areas of sleep
research such as closed-loop auditory stimulation and targeted memory
reactivation.",1,1,0,0,0,0,0.0479586,11.0,0.579783,22
a8e210fc-9a57-43d1-ab98-bf2f74e77564,Towards Inter-character Relationship-driven Story Generation,5,0.334392,0.731222,"In this paper, we introduce the task of modeling interpersonal relationships
for story generation. For addressing this task, we propose Relationships as
Latent Variables for Story Generation, (ReLiSt). ReLiSt generates stories
sentence by sentence and has two major components - a relationship selector and
a story continuer. The relationship selector specifies a latent variable to
pick the relationship to exhibit in the next sentence and the story continuer
generates the next sentence while expressing the selected relationship in a
coherent way. Our automatic and human evaluations demonstrate that ReLiSt is
able to generate stories with relationships that are more faithful to desired
relationships while maintaining the content quality. The relationship
assignments to sentences during inference bring interpretability to ReLiSt.",0,0,1,0,0,0,0.91925,8.0,0.914174,47
35f52f12-80d3-461e-bb34-92a346cc2866,Label Anchored Contrastive Learning for Language Understanding,9,0.0210682,0.207411,"Contrastive learning (CL) has achieved astonishing progress in computer
vision, speech, and natural language processing fields recently with
self-supervised learning. However, CL approach to the supervised setting is not
fully explored, especially for the natural language understanding
classification task. Intuitively, the class label itself has the intrinsic
ability to perform hard positive/negative mining, which is crucial for CL.
Motivated by this, we propose a novel label anchored contrastive learning
approach (denoted as LaCon) for language understanding. Specifically, three
contrastive objectives are devised, including a multi-head instance-centered
contrastive loss (ICL), a label-centered contrastive loss (LCL), and a label
embedding regularizer (LER). Our approach does not require any specialized
network architecture or any extra data augmentation, thus it can be easily
plugged into existing powerful pre-trained language models. Compared to the
state-of-the-art baselines, LaCon obtains up to 4.1% improvement on the popular
datasets of GLUE and CLUE benchmarks. Besides, LaCon also demonstrates
significant advantages under the few-shot and data imbalance settings, which
obtains up to 9.4% improvement on the FewGLUE and FewCLUE benchmarking tasks.",0,1,0,0,1,0,0.461771,5.0,0.582334,46
f4025d4d-3720-4742-8a8c-a72fc543d8c5,Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks,9,0.0261966,0.465506,"Randomization is currently a widely used approach in Sim2Real transfer for
data-driven learning algorithms in robotics. Still, most Sim2Real studies
report results for a specific randomization technique and often on a highly
customized robotic system, making it difficult to evaluate different
randomization approaches systematically. To address this problem, we define an
easy-to-reproduce experimental setup for a robotic reach-and-balance
manipulator task, which can serve as a benchmark for comparison. We compare
four randomization strategies with three randomized parameters both in
simulation and on a real robot. Our results show that more randomization helps
in Sim2Real transfer, yet it can also harm the ability of the algorithm to find
a good policy in simulation. Fully randomized simulations and fine-tuning show
differentiated results and translate better to the real robot than the other
approaches tested.",0,1,0,0,0,0,0.038231,11.0,0.558717,38
cd4bb8aa-eda2-4cb3-8c35-56cfddc1d3b6,MatteFormer: Transformer-Based Image Matting via Prior-Tokens,48,0.658952,0.953863,"In this paper, we propose a transformer-based image matting model called
MatteFormer, which takes full advantage of trimap information in the
transformer block. Our method first introduces a prior-token which is a global
representation of each trimap region (e.g. foreground, background and unknown).
These prior-tokens are used as global priors and participate in the
self-attention mechanism of each block. Each stage of the encoder is composed
of PAST (Prior-Attentive Swin Transformer) block, which is based on the Swin
Transformer block, but differs in a couple of aspects: 1) It has PA-WSA
(Prior-Attentive Window Self-Attention) layer, performing self-attention not
only with spatial-tokens but also with prior-tokens. 2) It has prior-memory
which saves prior-tokens accumulatively from the previous blocks and transfers
them to the next block. We evaluate our MatteFormer on the commonly used image
matting datasets: Composition-1k and Distinctions-646. Experiment results show
that our proposed method achieves state-of-the-art performance with a large
margin. Our codes are available at https://github.com/webtoon/matteformer.",1,1,0,0,1,0,0.978869,4.0,0.935114,59
7f53c20e-e1a1-49ce-8c14-68f982299dd2,Enhanced Physics-Informed Neural Networks with Augmented Lagrangian Relaxation Method (AL-PINNs),10,0.261695,0.510209,"Physics-Informed Neural Networks (PINNs) have become a prominent application
of deep learning in scientific computation, as they are powerful approximators
of solutions to nonlinear partial differential equations (PDEs). There have
been numerous attempts to facilitate the training process of PINNs by adjusting
the weight of each component of the loss function, called adaptive
loss-balancing algorithms. In this paper, we propose an Augmented Lagrangian
relaxation method for PINNs (AL-PINNs). We treat the initial and boundary
conditions as constraints for the optimization problem of the PDE residual. By
employing Augmented Lagrangian relaxation, the constrained optimization problem
becomes a sequential max-min problem so that the learnable parameters $\lambda$
adaptively balance each loss component. Our theoretical analysis reveals that
the sequence of minimizers of the proposed loss functions converges to an
actual solution for the Helmholtz, viscous Burgers, and Klein--Gordon
equations. We demonstrate through various numerical experiments that AL-PINNs
yield a much smaller relative error compared with that of state-of-the-art
adaptive loss-balancing algorithms.",0,0,0,0,1,0,0.896029,4.0,0.801885,49
26454123-2fb6-45d3-bb39-b430021ca600,BoundaryFace: A mining framework with noise label self-correction for Face Recognition,4,0.00775127,0.359518,"Face recognition has made tremendous progress in recent years due to the
advances in loss functions and the explosive growth in training sets size. A
properly designed loss is seen as key to extract discriminative features for
classification. Several margin-based losses have been proposed as alternatives
of softmax loss in face recognition. However, two issues remain to consider: 1)
They overlook the importance of hard sample mining for discriminative learning.
2) Label noise ubiquitously exists in large-scale datasets, which can seriously
damage the model's performance. In this paper, starting from the perspective of
decision boundary, we propose a novel mining framework that focuses on the
relationship between a sample's ground truth class center and its nearest
negative class center. Specifically, a closed-set noise label self-correction
module is put forward, making this framework work well on datasets containing a
lot of label noise. The proposed method consistently outperforms SOTA methods
in various face recognition benchmarks. Training code has been released at
https://github.com/SWJTU-3DVision/BoundaryFace.",1,1,0,0,1,0,0.082998,11.0,0.63133,38
39e62539-3a47-4c8d-af80-f22d07055ff7,Learning Efficient Representations for Enhanced Object Detection on Large-scene SAR Images,2,0.0267156,0.0341252,"It is a challenging problem to detect and recognize targets on complex
large-scene Synthetic Aperture Radar (SAR) images. Recently developed deep
learning algorithms can automatically learn the intrinsic features of SAR
images, but still have much room for improvement on large-scene SAR images with
limited data. In this paper, based on learning representations and multi-scale
features of SAR images, we propose an efficient and robust deep learning based
target detection method. Especially, by leveraging the effectiveness of
adversarial autoencoder (AAE) which influences the distribution of the
investigated data explicitly, the raw SAR dataset is augmented into an enhanced
version with a large quantity and diversity. Besides, an auto-labeling scheme
is proposed to improve labeling efficiency. Finally, with jointly training
small target chips and large-scene images, an integrated YOLO network combining
non-maximum suppression on sub-images is used to realize multiple targets
detection of high resolution images. The numerical experimental results on the
MSTAR dataset show that our method can realize target detection and recognition
on large-scene images accurately and efficiently. The superior anti-noise
performance is also confirmed by experiments.",0,1,0,0,0,0,0.118952,11.0,0.665835,54
4cba19b2-e1fd-43f3-8779-78a63cb72369,Automatic Detection of Entity-Manipulated Text using Factual Knowledge,9,0.0889444,0.686449,"In this work, we focus on the problem of distinguishing a human written news
article from a news article that is created by manipulating entities in a human
written news article (e.g., replacing entities with factually incorrect
entities). Such manipulated articles can mislead the reader by posing as a
human written news article. We propose a neural network based detector that
detects manipulated news articles by reasoning about the facts mentioned in the
article. Our proposed detector exploits factual knowledge via graph
convolutional neural network along with the textual information in the news
article. We also create challenging datasets for this task by considering
various strategies to generate the new replacement entity (e.g., entity
generation from GPT-2). In all the settings, our proposed model either matches
or outperforms the state-of-the-art detector in terms of accuracy. Our code and
data are available at https://github.com/UBC-NLP/manipulated_entity_detection.",1,1,1,1,1,0,0.894575,6.0,0.866898,24
b88d4c26-6f16-4dc9-8214-c164285be427,Generalizable Deepfake Detection with Phase-Based Motion Analysis,2,0.0286382,0.205986,"We propose PhaseForensics, a DeepFake (DF) video detection method that
leverages a phase-based motion representation of facial temporal dynamics.
Existing methods relying on temporal inconsistencies for DF detection present
many advantages over the typical frame-based methods. However, they still show
limited cross-dataset generalization and robustness to common distortions.
These shortcomings are partially due to error-prone motion estimation and
landmark tracking, or the susceptibility of the pixel intensity-based features
to spatial distortions and the cross-dataset domain shifts. Our key insight to
overcome these issues is to leverage the temporal phase variations in the
band-pass components of the Complex Steerable Pyramid on face sub-regions. This
not only enables a robust estimate of the temporal dynamics in these regions,
but is also less prone to cross-dataset variations. Furthermore, the band-pass
filters used to compute the local per-frame phase form an effective defense
against the perturbations commonly seen in gradient-based adversarial attacks.
Overall, with PhaseForensics, we show improved distortion and adversarial
robustness, and state-of-the-art cross-dataset generalization, with 91.2%
video-level AUC on the challenging CelebDFv2 (a recent state-of-the-art
compares at 86.9%).",0,1,0,0,1,0,0.928497,6.0,0.893431,66
f015a043-8ff9-4425-9915-9648dcdfab7a,Wavelet Feature Maps Compression for Image-to-Image CNNs,1,0.0170936,0.0652204,"Convolutional Neural Networks (CNNs) are known for requiring extensive
computational resources, and quantization is among the best and most common
methods for compressing them. While aggressive quantization (i.e., less than
4-bits) performs well for classification, it may cause severe performance
degradation in image-to-image tasks such as semantic segmentation and depth
estimation. In this paper, we propose Wavelet Compressed Convolution (WCC) -- a
novel approach for high-resolution activation maps compression integrated with
point-wise convolutions, which are the main computational cost of modern
architectures. To this end, we use an efficient and hardware-friendly
Haar-wavelet transform, known for its effectiveness in image compression, and
define the convolution on the compressed activation map. We experiment with
various tasks that benefit from high-resolution input. By combining WCC with
light quantization, we achieve compression rates equivalent to 1-4bit
activation quantization with relatively small and much more graceful
degradation in performance. Our code is available at
https://github.com/BGUCompSci/WaveletCompressedConvolution.",1,1,0,0,0,0,0.398731,8.0,0.714336,81
40f30f44-0408-4f54-8c05-dc9839251109,Cross-modal Learning for Image-Guided Point Cloud Shape Completion,10,0.149776,0.487635,"In this paper we explore the recent topic of point cloud completion, guided
by an auxiliary image. We show how it is possible to effectively combine the
information from the two modalities in a localized latent space, thus avoiding
the need for complex point cloud reconstruction methods from single views used
by the state-of-the-art. We also investigate a novel weakly-supervised setting
where the auxiliary image provides a supervisory signal to the training process
by using a differentiable renderer on the completed point cloud to measure
fidelity in the image space. Experiments show significant improvements over
state-of-the-art supervised methods for both unimodal and multimodal
completion. We also show the effectiveness of the weakly-supervised approach
which outperforms a number of supervised methods and is competitive with the
latest supervised models only exploiting point cloud information.",1,1,0,0,1,0,0.583256,7.0,0.751051,46
52a3a832-e733-4206-b275-073f1c218445,OpenScene: 3D Scene Understanding with Open Vocabularies,149,0.846545,0.999086,"Traditional 3D scene understanding approaches rely on labeled 3D datasets to
train a model for a single task with supervision. We propose OpenScene, an
alternative approach where a model predicts dense features for 3D scene points
that are co-embedded with text and image pixels in CLIP feature space. This
zero-shot approach enables task-agnostic training and open-vocabulary queries.
For example, to perform SOTA zero-shot 3D semantic segmentation it first infers
CLIP features for every 3D point and later classifies them based on
similarities to embeddings of arbitrary class labels. More interestingly, it
enables a suite of open-vocabulary scene understanding applications that have
never been done before. For example, it allows a user to enter an arbitrary
text query and then see a heat map indicating which parts of a scene match. Our
approach is effective at identifying objects, materials, affordances,
activities, and room types in complex 3D scenes, all using a single model
trained without any labeled 3D data.",0,0,1,0,0,0,0.799996,4.0,0.716609,68
90733bcc-90a2-4c65-8108-a16cd77eaf21,Resolving Copycat Problems in Visual Imitation Learning via Residual Action Prediction,7,0.0673253,0.220817,"Imitation learning is a widely used policy learning method that enables
intelligent agents to acquire complex skills from expert demonstrations. The
input to the imitation learning algorithm is usually composed of both the
current observation and historical observations since the most recent
observation might not contain enough information. This is especially the case
with image observations, where a single image only includes one view of the
scene, and it suffers from a lack of motion information and object occlusions.
In theory, providing multiple observations to the imitation learning agent will
lead to better performance. However, surprisingly people find that sometimes
imitation from observation histories performs worse than imitation from the
most recent observation. In this paper, we explain this phenomenon from the
information flow within the neural network perspective. We also propose a novel
imitation learning neural network architecture that does not suffer from this
issue by design. Furthermore, our method scales to high-dimensional image
observations. Finally, we benchmark our approach on two widely used simulators,
CARLA and MuJoCo, and it successfully alleviates the copycat problem and
surpasses the existing solutions.",0,1,0,0,1,0,0.600802,9.0,0.811703,48
481a1059-bc3b-4ccd-b041-b54ac1c301e0,"Q-Ensemble for Offline RL: Don't Scale the Ensemble, Scale the Batch Size",9,0.187107,0.525486,"Training large neural networks is known to be time-consuming, with the
learning duration taking days or even weeks. To address this problem,
large-batch optimization was introduced. This approach demonstrated that
scaling mini-batch sizes with appropriate learning rate adjustments can speed
up the training process by orders of magnitude. While long training time was
not typically a major issue for model-free deep offline RL algorithms, recently
introduced Q-ensemble methods achieving state-of-the-art performance made this
issue more relevant, notably extending the training duration. In this work, we
demonstrate how this class of methods can benefit from large-batch
optimization, which is commonly overlooked by the deep offline RL community. We
show that scaling the mini-batch size and naively adjusting the learning rate
allows for (1) a reduced size of the Q-ensemble, (2) stronger penalization of
out-of-distribution actions, and (3) improved convergence time, effectively
shortening training duration by 3-4x times on average.",1,1,0,0,0,0,0.826847,6.0,0.82537,54
a86cd6e8-969f-4b59-b2ba-fe3e4021e3cc,3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows,39,0.495468,0.986412,"Text-to-image AI are capable of generating novel images for inspiration, but
their applications for 3D design workflows and how designers can build 3D
models using AI-provided inspiration have not yet been explored. To investigate
this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a
plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users
to construct text and image prompts based on what they are modeling. In a study
with 13 designers, we found that designers saw great potential in 3DALL-E
within their workflows and could use text-to-image AI to produce reference
images, prevent design fixation, and inspire design considerations. We
elaborate on prompting patterns observed across 3D modeling tasks and provide
measures of prompt complexity observed across participants. From our findings,
we discuss how 3DALL-E can merge with existing generative design workflows and
propose prompt bibliographies as a form of human-AI design history.",0,1,0,0,0,0,0.62832,6.0,0.730039,99
6099e8d9-e1d8-4881-8848-39d9b1748fd5,Improving Generalization of Metric Learning via Listwise Self-distillation,1,0.0,0.0335809,"Most deep metric learning (DML) methods employ a strategy that forces all
positive samples to be close in the embedding space while keeping them away
from negative ones. However, such a strategy ignores the internal relationships
of positive (negative) samples and often leads to overfitting, especially in
the presence of hard samples and mislabeled samples. In this work, we propose a
simple yet effective regularization, namely Listwise Self-Distillation (LSD),
which progressively distills a model's own knowledge to adaptively assign a
more appropriate distance target to each sample pair in a batch. LSD encourages
smoother embeddings and information mining within positive (negative) samples
as a way to mitigate overfitting and thus improve generalization. Our LSD can
be directly integrated into general DML frameworks. Extensive experiments show
that LSD consistently boosts the performance of various metric learning methods
on multiple datasets.",0,1,0,0,0,1,0.187615,9.0,0.646588,35
6b7505aa-9b18-4a9c-8a2c-e0415e1e3638,RangeUDF: Semantic Surface Reconstruction from 3D Point Clouds,12,0.0783527,0.574875,"We present RangeUDF, a new implicit representation based framework to recover
the geometry and semantics of continuous 3D scene surfaces from point clouds.
Unlike occupancy fields or signed distance fields which can only model closed
3D surfaces, our approach is not restricted to any type of topology. Being
different from the existing unsigned distance fields, our framework does not
suffer from any surface ambiguity. In addition, our RangeUDF can jointly
estimate precise semantics for continuous surfaces. The key to our approach is
a range-aware unsigned distance function together with a surface-oriented
semantic segmentation module. Extensive experiments show that RangeUDF clearly
surpasses state-of-the-art approaches for surface reconstruction on four point
cloud datasets. Moreover, RangeUDF demonstrates superior generalization
capability across multiple unseen datasets, which is nearly impossible for all
existing approaches.",1,1,0,0,1,0,0.801201,7.0,0.838597,57
edab5509-eb91-492a-bf0e-73c5752baa15,Robust $Q$-learning Algorithm for Markov Decision Processes under Wasserstein Uncertainty,12,0.180672,0.885303,"We present a novel $Q$-learning algorithm to solve distributionally robust
Markov decision problems, where the corresponding ambiguity set of transition
probabilities for the underlying Markov decision process is a Wasserstein ball
around a (possibly estimated) reference measure. We prove convergence of the
presented algorithm and provide several examples also using real data to
illustrate both the tractability of our algorithm as well as the benefits of
considering distributional robustness when solving stochastic optimal control
problems, in particular when the estimated distributions turn out to be
misspecified in practice.",1,0,0,0,0,0,0.125761,11.0,0.67124,55
b59db4e9-fae6-4eef-bc11-77aa2e19077a,Bootstrapped Transformer for Offline Reinforcement Learning,27,0.398588,0.697955,"Offline reinforcement learning (RL) aims at learning policies from previously
collected static trajectory data without interacting with the real environment.
Recent works provide a novel perspective by viewing offline RL as a generic
sequence generation problem, adopting sequence models such as Transformer
architecture to model distributions over trajectories, and repurposing beam
search as a planning algorithm. However, the training datasets utilized in
general offline RL tasks are quite limited and often suffer from insufficient
distribution coverage, which could be harmful to training sequence generation
models yet has not drawn enough attention in the previous works. In this paper,
we propose a novel algorithm named Bootstrapped Transformer, which incorporates
the idea of bootstrapping and leverages the learned model to self-generate more
offline data to further boost the sequence model training. We conduct extensive
experiments on two offline RL benchmarks and demonstrate that our model can
largely remedy the existing offline RL training limitations and beat other
strong baseline methods. We also analyze the generated pseudo data and the
revealed characteristics may shed some light on offline RL training. The codes
are available at https://seqml.github.io/bootorl.",1,1,0,0,0,0,0.962975,5.0,0.916664,68
ec8f8433-73b4-4d80-859c-800b5e8d0606,BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric,5,0.0500415,0.770012,"End-to-End speech-to-speech translation (S2ST) is generally evaluated with
text-based metrics. This means that generated speech has to be automatically
transcribed, making the evaluation dependent on the availability and quality of
automatic speech recognition (ASR) systems. In this paper, we propose a
text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the
dependency on ASR systems. BLASER leverages a multilingual multimodal encoder
to directly encode the speech segments for source input, translation output and
reference into a shared embedding space and computes a score of the translation
quality that can be used as a proxy to human evaluation. To evaluate our
approach, we construct training and evaluation sets from more than 40k human
annotations covering seven language directions. The best results of BLASER are
achieved by training with supervision from human rating scores. We show that
when evaluated at the sentence level, BLASER correlates significantly better
with human judgment compared to ASR-dependent metrics including ASR-SENTBLEU in
all translation directions and ASR-COMET in five of them. Our analysis shows
combining speech and text as inputs to BLASER does not increase the correlation
with human scores, but best correlations are achieved when using speech, which
motivates the goal of our research. Moreover, we show that using ASR for
references is detrimental for text-based metrics.",0,1,0,0,0,0,0.435672,6.0,0.638692,76
ab2e3ad1-ee97-4871-aa01-563a38a3722b,Federated learning for violence incident prediction in a simulated cross-institutional psychiatric setting,13,0.44929,0.903522,"Inpatient violence is a common and severe problem within psychiatry. Knowing
who might become violent can influence staffing levels and mitigate severity.
Predictive machine learning models can assess each patient's likelihood of
becoming violent based on clinical notes. Yet, while machine learning models
benefit from having more data, data availability is limited as hospitals
typically do not share their data for privacy preservation. Federated Learning
(FL) can overcome the problem of data limitation by training models in a
decentralised manner, without disclosing data between collaborators. However,
although several FL approaches exist, none of these train Natural Language
Processing models on clinical notes. In this work, we investigate the
application of Federated Learning to clinical Natural Language Processing,
applied to the task of Violence Risk Assessment by simulating a
cross-institutional psychiatric setting. We train and compare four models: two
local models, a federated model and a data-centralised model. Our results
indicate that the federated model outperforms the local models and has similar
performance as the data-centralised model. These findings suggest that
Federated Learning can be used successfully in a cross-institutional setting
and is a step towards new applications of Federated Learning based on clinical
notes",0,1,0,0,0,0,0.528827,13.0,0.854327,43
f918f1f5-1144-47c2-aba8-70b166b0beca,EpiGNN: Exploring Spatial Transmission with Graph Neural Network for Regional Epidemic Forecasting,13,0.36102,0.497115,"Epidemic forecasting is the key to effective control of epidemic transmission
and helps the world mitigate the crisis that threatens public health. To better
understand the transmission and evolution of epidemics, we propose EpiGNN, a
graph neural network-based model for epidemic forecasting. Specifically, we
design a transmission risk encoding module to characterize local and global
spatial effects of regions in epidemic processes and incorporate them into the
model. Meanwhile, we develop a Region-Aware Graph Learner (RAGL) that takes
transmission risk, geographical dependencies, and temporal information into
account to better explore spatial-temporal dependencies and makes regions aware
of related regions' epidemic situations. The RAGL can also combine with
external resources, such as human mobility, to further improve prediction
performance. Comprehensive experiments on five real-world epidemic-related
datasets (including influenza and COVID-19) demonstrate the effectiveness of
our proposed method and show that EpiGNN outperforms state-of-the-art baselines
by 9.48% in RMSE.",1,1,0,0,1,0,0.720596,7.0,0.804798,22
2ee39fcb-4c5f-463e-955a-1c50a5e76fad,Object Segmentation of Cluttered Airborne LiDAR Point Clouds,1,0.0227527,0.128563,"Airborne topographic LiDAR is an active remote sensing technology that emits
near-infrared light to map objects on the Earth's surface. Derived products of
LiDAR are suitable to service a wide range of applications because of their
rich three-dimensional spatial information and their capacity to obtain
multiple returns. However, processing point cloud data still requires a
significant effort in manual editing. Certain human-made objects are difficult
to detect because of their variety of shapes, irregularly-distributed point
clouds, and low number of class samples. In this work, we propose an efficient
end-to-end deep learning framework to automatize the detection and segmentation
of objects defined by an arbitrary number of LiDAR points surrounded by
clutter. Our method is based on a light version of PointNet that achieves good
performance on both object recognition and segmentation tasks. The results are
tested against manually delineated power transmission towers and show promising
accuracy.",1,1,0,0,0,0,0.79243,10.0,0.884309,18
aaaa9d7e-655d-4326-9ed1-ac678155bddc,MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing,23,0.164529,0.52258,"Snow removal causes challenges due to its characteristic of complex
degradations. To this end, targeted treatment of multi-scale snow degradations
is critical for the network to learn effective snow removal. In order to handle
the diverse scenes, we propose a multi-scale projection transformer
(MSP-Former), which understands and covers a variety of snow degradation
features in a multi-path manner, and integrates comprehensive scene context
information for clean reconstruction via self-attention operation. For the
local details of various snow degradations, the local capture module is
introduced in parallel to assist in the rebuilding of a clean image. Such
design achieves the SOTA performance on three desnowing benchmark datasets
while costing the low parameters and computational complexity, providing a
guarantee of practicality.",0,1,0,0,1,0,0.882743,3.0,0.71765,48
80e32591-c5af-4800-ac1c-ed205c0da2ed,ActMAD: Activation Matching to Align Distributions for Test-Time-Training,10,0.282223,0.48054,"Test-Time-Training (TTT) is an approach to cope with out-of-distribution
(OOD) data by adapting a trained model to distribution shifts occurring at
test-time. We propose to perform this adaptation via Activation Matching
(ActMAD): We analyze activations of the model and align activation statistics
of the OOD test data to those of the training data. In contrast to existing
methods, which model the distribution of entire channels in the ultimate layer
of the feature extractor, we model the distribution of each feature in multiple
layers across the network. This results in a more fine-grained supervision and
makes ActMAD attain state of the art performance on CIFAR-100C and Imagenet-C.
ActMAD is also architecture- and task-agnostic, which lets us go beyond image
classification, and score 15.4% improvement over previous approaches when
evaluating a KITTI-trained object detector on KITTI-Fog. Our experiments
highlight that ActMAD can be applied to online adaptation in realistic
scenarios, requiring little data to attain its full performance.",1,1,0,0,1,0,0.953051,8.0,0.938569,50
feb6af05-4c7b-4402-9d49-bd600664de25,Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation,17,0.141174,0.758711,"Neural networks tend to gradually forget the previously learned knowledge
when learning multiple tasks sequentially from dynamic data distributions. This
problem is called \textit{catastrophic forgetting}, which is a fundamental
challenge in the continual learning of neural networks. In this work, we
observe that catastrophic forgetting not only occurs in continual learning but
also affects the traditional static training. Neural networks, especially
neural machine translation models, suffer from catastrophic forgetting even if
they learn from a static training set. To be specific, the final model pays
imbalanced attention to training samples, where recently exposed samples
attract more attention than earlier samples. The underlying cause is that
training samples do not get balanced training in each model update, so we name
this problem \textit{imbalanced training}. To alleviate this problem, we
propose Complementary Online Knowledge Distillation (COKD), which uses
dynamically updated teacher models trained on specific data orders to
iteratively provide complementary knowledge to the student model. Experimental
results on multiple machine translation tasks show that our method successfully
alleviates the problem of imbalanced training and achieves substantial
improvements over strong baseline systems.",0,1,0,0,0,0,0.257697,11.0,0.743627,42
7bd05783-875e-4fd6-8097-1268dab49cf0,QuantFace: Towards Lightweight Face Recognition by Synthetic Data Low-bit Quantization,28,0.13443,0.874404,"Deep learning-based face recognition models follow the common trend in deep
neural networks by utilizing full-precision floating-point networks with high
computational costs. Deploying such networks in use-cases constrained by
computational requirements is often infeasible due to the large memory required
by the full-precision model. Previous compact face recognition approaches
proposed to design special compact architectures and train them from scratch
using real training data, which may not be available in a real-world scenario
due to privacy concerns. We present in this work the QuantFace solution based
on low-bit precision format model quantization. QuantFace reduces the required
computational cost of the existing face recognition models without the need for
designing a particular architecture or accessing real training data. QuantFace
introduces privacy-friendly synthetic face data to the quantization process to
mitigate potential privacy concerns and issues related to the accessibility to
real training data. Through extensive evaluation experiments on seven
benchmarks and four network architectures, we demonstrate that QuantFace can
successfully reduce the model size up to 5x while maintaining, to a large
degree, the verification performance of the full-precision model without
accessing real training datasets.",0,1,0,0,0,0,0.474589,6.0,0.658307,59
50e1a63a-3c45-429e-8c81-d860ca416d49,Context-dependent Explainability and Contestability for Trustworthy Medical Artificial Intelligence: Misclassification Identification of Morbidity Recognition Models in Preterm Infants,1,0.0386663,0.064756,"Although machine learning (ML) models of AI achieve high performances in
medicine, they are not free of errors. Empowering clinicians to identify
incorrect model recommendations is crucial for engendering trust in medical AI.
Explainable AI (XAI) aims to address this requirement by clarifying AI
reasoning to support the end users. Several studies on biomedical imaging
achieved promising results recently. Nevertheless, solutions for models using
tabular data are not sufficient to meet the requirements of clinicians yet.
This paper proposes a methodology to support clinicians in identifying failures
of ML models trained with tabular data. We built our methodology on three main
pillars: decomposing the feature set by leveraging clinical context latent
space, assessing the clinical association of global explanations, and Latent
Space Similarity (LSS) based local explanations. We demonstrated our
methodology on ML-based recognition of preterm infant morbidities caused by
infection. The risk of mortality, lifelong disability, and antibiotic
resistance due to model failures was an open research question in this domain.
We achieved to identify misclassification cases of two models with our
approach. By contextualizing local explanations, our solution provides
clinicians with actionable insights to support their autonomy for informed
final decisions.",0,1,0,0,0,0,0.628155,8.0,0.797473,94
d42457a6-fac7-402c-8139-41cb857cf41a,OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction,35,0.405686,0.750208,"Learning how humans manipulate objects requires machines to acquire knowledge
from two perspectives: one for understanding object affordances and the other
for learning human's interactions based on the affordances. Even though these
two knowledge bases are crucial, we find that current databases lack a
comprehensive awareness of them. In this work, we propose a multi-modal and
rich-annotated knowledge repository, OakInk, for visual and cognitive
understanding of hand-object interactions. We start to collect 1,800 common
household objects and annotate their affordances to construct the first
knowledge base: Oak. Given the affordance, we record rich human interactions
with 100 selected objects in Oak. Finally, we transfer the interactions on the
100 recorded objects to their virtual counterparts through a novel method:
Tink. The recorded and transferred hand-object interactions constitute the
second knowledge base: Ink. As a result, OakInk contains 50,000 distinct
affordance-aware and intent-oriented hand-object interactions. We benchmark
OakInk on pose estimation and grasp generation tasks. Moreover, we propose two
practical applications of OakInk: intent-based interaction generation and
handover generation. Our datasets and source code are publicly available at
https://github.com/lixiny/OakInk.",1,1,0,1,0,0,0.742063,6.0,0.782399,65
f649e4ed-3192-4104-8acd-d77894f4a6d3,Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection,20,0.578769,0.802509,"We study the selection of transfer languages for automatic abusive language
detection. Instead of preparing a dataset for every language, we demonstrate
the effectiveness of cross-lingual transfer learning for zero-shot abusive
language detection. This way we can use existing data from higher-resource
languages to build better detection systems for low-resource languages. Our
datasets are from seven different languages from three language families. We
measure the distance between the languages using several language similarity
measures, especially by quantifying the World Atlas of Language Structures. We
show that there is a correlation between linguistic similarity and classifier
performance. This discovery allows us to choose an optimal transfer language
for zero shot abusive language detection.",0,1,0,0,0,0,0.716173,9.0,0.846799,82
3439bbf0-e015-4f82-ae90-20a9205ee422,3D Common Corruptions and Data Augmentation,63,0.635441,0.920504,"We introduce a set of image transformations that can be used as corruptions
to evaluate the robustness of models as well as data augmentation mechanisms
for training neural networks. The primary distinction of the proposed
transformations is that, unlike existing approaches such as Common Corruptions,
the geometry of the scene is incorporated in the transformations -- thus
leading to corruptions that are more likely to occur in the real world. We also
introduce a set of semantic corruptions (e.g. natural object occlusions). We
show these transformations are `efficient' (can be computed on-the-fly),
`extendable' (can be applied on most image datasets), expose vulnerability of
existing models, and can effectively make models more robust when employed as
`3D data augmentation' mechanisms. The evaluations on several tasks and
datasets suggest incorporating 3D information into benchmarking and training
opens up a promising direction for robustness research.",0,1,1,0,0,0,0.899068,6.0,0.870093,86
8133a55f-d0d3-4a5f-8922-862bea89770d,"Deep learning and machine learning for Malaria detection: overview, challenges and future directions",15,0.324732,0.244828,"To have the greatest impact, public health initiatives must be made using
evidence-based decision-making. Machine learning Algorithms are created to
gather, store, process, and analyse data to provide knowledge and guide
decisions. A crucial part of any surveillance system is image analysis. The
communities of computer vision and machine learning has ended up curious about
it as of late. This study uses a variety of machine learning and image
processing approaches to detect and forecast the malarial illness. In our
research, we discovered the potential of deep learning techniques as smart
tools with broader applicability for malaria detection, which benefits
physicians by assisting in the diagnosis of the condition. We examine the
common confinements of deep learning for computer frameworks and organising,
counting need of preparing data, preparing overhead, realtime execution, and
explain ability, and uncover future inquire about bearings focusing on these
restrictions.",0,1,0,0,0,0,0.187438,6.0,0.469707,98
58f59309-74ca-4d85-a196-100ffd60493d,TAD: Transfer Learning-based Multi-Adversarial Detection of Evasion Attacks against Network Intrusion Detection Systems,21,0.108936,0.772581,"Nowadays, intrusion detection systems based on deep learning deliver
state-of-the-art performance. However, recent research has shown that specially
crafted perturbations, called adversarial examples, are capable of
significantly reducing the performance of these intrusion detection systems.
The objective of this paper is to design an efficient transfer learning-based
adversarial detector and then to assess the effectiveness of using multiple
strategically placed adversarial detectors compared to a single adversarial
detector for intrusion detection systems. In our experiments, we implement
existing state-of-the-art models for intrusion detection. We then attack those
models with a set of chosen evasion attacks. In an attempt to detect those
adversarial attacks, we design and implement multiple transfer learning-based
adversarial detectors, each receiving a subset of the information passed
through the IDS. By combining their respective decisions, we illustrate that
combining multiple detectors can further improve the detectability of
adversarial traffic compared to a single detector in the case of a parallel IDS
design.",0,1,0,0,0,0,0.0996816,10.0,0.613683,43
7519908b-9ebf-4e93-aafe-056a9fe79925,KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,30,0.602914,0.467197,"Relative positional embeddings (RPE) have received considerable attention
since RPEs effectively model the relative distance among tokens and enable
length extrapolation. We propose KERPLE, a framework that generalizes relative
position embedding for extrapolation by kernelizing positional differences. We
achieve this goal using conditionally positive definite (CPD) kernels, a class
of functions known for generalizing distance metrics. To maintain the inner
product interpretation of self-attention, we show that a CPD kernel can be
transformed into a PD kernel by adding a constant offset. This offset is
implicitly absorbed in the Softmax normalization during self-attention. The
diversity of CPD kernels allows us to derive various RPEs that enable length
extrapolation in a principled way. Experiments demonstrate that the logarithmic
variant achieves excellent extrapolation performance on three large language
modeling datasets. Our implementation and pretrained checkpoints are released
at https://github.com/chijames/KERPLE.git.",0,0,0,0,0,0,0.976856,4.0,0.929146,57
49e04a7d-170a-42d2-b846-369f12d04ef8,EEML: Ensemble Embedded Meta-learning,2,0.00916437,0.0595482,"To accelerate learning process with few samples, meta-learning resorts to
prior knowledge from previous tasks. However, the inconsistent task
distribution and heterogeneity is hard to be handled through a global sharing
model initialization. In this paper, based on gradient-based meta-learning, we
propose an ensemble embedded meta-learning algorithm (EEML) that explicitly
utilizes multi-model-ensemble to organize prior knowledge into diverse specific
experts. We rely on a task embedding cluster mechanism to deliver diverse tasks
to matching experts in training process and instruct how experts collaborate in
test phase. As a result, the multi experts can focus on their own area of
expertise and cooperate in upcoming task to solve the task heterogeneity. The
experimental results show that the proposed method outperforms recent
state-of-the-arts easily in few-shot learning problem, which validates the
importance of differentiation and cooperation.",0,0,0,0,1,0,0.850729,9.0,0.892607,35
9e707ee6-deb8-4bf8-8e4e-d00e61040dfb,DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata,7,0.0934334,0.615552,"Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic
videos. However, they require a slow iterative optimization process to
synthesize a single fixed-size short video, and they do not offer any
post-training control over the synthesis process. We propose Dynamic Neural
Cellular Automata (DyNCA), a framework for real-time and controllable dynamic
texture synthesis. Our method is built upon the recently introduced NCA models
and can synthesize infinitely long and arbitrary-sized realistic video textures
in real time. We quantitatively and qualitatively evaluate our model and show
that our synthesized videos appear more realistic than the existing results. We
improve the SOTA DyTS performance by $2\sim 4$ orders of magnitude. Moreover,
our model offers several real-time video controls including motion speed,
motion direction, and an editing brush tool. We exhibit our trained models in
an online interactive demo that runs on local hardware and is accessible on
personal computers and smartphones.",1,1,0,0,1,0,0.0200704,13.0,0.576321,31
12772d86-c5bb-4164-8003-ca75ed66337b,Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings,13,0.125584,0.668811,"Previous knowledge graph embedding approaches usually map entities to
representations and utilize score functions to predict the target entities, yet
they typically struggle to reason rare or emerging unseen entities. In this
paper, we propose kNN-KGE, a new knowledge graph embedding approach with
pre-trained language models, by linearly interpolating its entity distribution
with k-nearest neighbors. We compute the nearest neighbors based on the
distance in the entity embedding space from the knowledge store. Our approach
can allow rare or emerging entities to be memorized explicitly rather than
implicitly in model parameters. Experimental results demonstrate that our
approach can improve inductive and transductive link prediction results and
yield better performance for low-resource settings with only a few triples,
which might be easier to reason via explicit memory. Code is available at
https://github.com/zjunlp/KNN-KG.",0,1,0,0,0,0,0.886855,3.0,0.723156,52
4fa029ac-391e-4355-a39f-dbfcd88e4fb6,Using Deep Mixture-of-Experts to Detect Word Meaning Shift for TempoWiC,2,0.0162354,0.461566,"This paper mainly describes the dma submission to the TempoWiC task, which
achieves a macro-F1 score of 77.05% and attains the first place in this task.
We first explore the impact of different pre-trained language models. Then we
adopt data cleaning, data augmentation, and adversarial training strategies to
enhance the model generalization and robustness. For further improvement, we
integrate POS information and word semantic representation using a
Mixture-of-Experts (MoE) approach. The experimental results show that MoE can
overcome the feature overuse issue and combine the context, POS, and word
semantic features well. Additionally, we use a model ensemble method for the
final prediction, which has been proven effective by many research works.",0,1,0,0,1,0,0.453987,5.0,0.577644,17
e3618130-eff0-4518-b9c5-327fd0383113,Deep Counterfactual Estimation with Categorical Background Variables,6,0.051544,0.251617,"Referred to as the third rung of the causal inference ladder, counterfactual
queries typically ask the ""What if ?"" question retrospectively. The standard
approach to estimate counterfactuals resides in using a structural equation
model that accurately reflects the underlying data generating process. However,
such models are seldom available in practice and one usually wishes to infer
them from observational data alone. Unfortunately, the correct structural
equation model is in general not identifiable from the observed factual
distribution. Nevertheless, in this work, we show that under the assumption
that the main latent contributors to the treatment responses are categorical,
the counterfactuals can be still reliably predicted. Building upon this
assumption, we introduce CounterFactual Query Prediction (CFQP), a novel method
to infer counterfactuals from continuous observations when the background
variables are categorical. We show that our method significantly outperforms
previously available deep-learning-based counterfactual methods, both
theoretically and empirically on time series and image data. Our code is
available at https://github.com/edebrouwer/cfqp.",1,0,0,0,1,0,0.147973,12.0,0.713236,46
7183f60e-9748-4db6-8d9f-dcdb9d6e997d,A Neural Template Matching Method to Detect Knee Joint Areas,1,0.00763781,0.083666,"In this paper, new methods are considered to detect knee joint areas in
bilateral PA fixed flexion knee X-ray images. The methods are of template
matching type where the distance criterion is based on the negative normalized
cross-correlation. The manual annotations are made on only one side of a single
bilateral image when the templates are selected. The best matching patch search
is formulated as an unconstrained continuous domain minimization problem. For
the minimization problem different optimization methods are considered. The
main method of the paper is a trainable optimizer where the method is taught to
take zoomed and possibly rotated patches from its input images which look like
the template. In the experiments, we compare the minimum values found by
different optimization methods. We also look at some test images to examine the
correspondence between the minimum value and how well the knee area is
localized. It seems that making annotations only to a single image enables to
detect knee joint areas quite precisely.",0,1,0,0,0,0,0.0892235,12.0,0.66836,17
9e13edfb-390c-41a8-99d4-690be4d226c4,"LightEA: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation",11,0.315224,0.325216,"Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which
is the core step of bridging and integrating multi-source KGs. In this paper,
we argue that existing GNN-based EA methods inherit the inborn defects from
their neural network lineage: weak scalability and poor interpretability.
Inspired by recent studies, we reinvent the Label Propagation algorithm to
effectively run on KGs and propose a non-neural EA framework -- LightEA,
consisting of three efficient components: (i) Random Orthogonal Label
Generation, (ii) Three-view Label Propagation, and (iii) Sparse Sinkhorn
Iteration. According to the extensive experiments on public datasets, LightEA
has impressive scalability, robustness, and interpretability. With a mere tenth
of time consumption, LightEA achieves comparable results to state-of-the-art
methods across all datasets and even surpasses them on many.",1,0,0,0,1,0,0.881409,7.0,0.878237,41
e148c606-66a7-44ef-8e89-37978e3da01b,Dynamic Local Aggregation Network with Adaptive Clusterer for Anomaly Detection,17,0.0650287,0.612392,"Existing methods for anomaly detection based on memory-augmented autoencoder
(AE) have the following drawbacks: (1) Establishing a memory bank requires
additional memory space. (2) The fixed number of prototypes from subjective
assumptions ignores the data feature differences and diversity. To overcome
these drawbacks, we introduce DLAN-AC, a Dynamic Local Aggregation Network with
Adaptive Clusterer, for anomaly detection. First, The proposed DLAN can
automatically learn and aggregate high-level features from the AE to obtain
more representative prototypes, while freeing up extra memory space. Second,
The proposed AC can adaptively cluster video data to derive initial prototypes
with prior information. In addition, we also propose a dynamic redundant
clustering strategy (DRCS) to enable DLAN for automatically eliminating feature
clusters that do not contribute to the construction of prototypes. Extensive
experiments on benchmarks demonstrate that DLAN-AC outperforms most existing
methods, validating the effectiveness of our method. Our code is publicly
available at https://github.com/Beyond-Zw/DLAN-AC.",1,1,0,0,1,0,0.17785,10.0,0.676006,54
aaf77109-161c-4e16-b983-a8bca9f3445a,Sample-Efficient Reinforcement Learning of Partially Observable Markov Games,13,0.107633,0.754062,"This paper considers the challenging tasks of Multi-Agent Reinforcement
Learning (MARL) under partial observability, where each agent only sees her own
individual observations and actions that reveal incomplete information about
the underlying state of system. This paper studies these tasks under the
general model of multiplayer general-sum Partially Observable Markov Games
(POMGs), which is significantly larger than the standard model of Imperfect
Information Extensive-Form Games (IIEFGs). We identify a rich subclass of POMGs
-- weakly revealing POMGs -- in which sample-efficient learning is tractable.
In the self-play setting, we prove that a simple algorithm combining optimism
and Maximum Likelihood Estimation (MLE) is sufficient to find approximate Nash
equilibria, correlated equilibria, as well as coarse correlated equilibria of
weakly revealing POMGs, in a polynomial number of samples when the number of
agents is small. In the setting of playing against adversarial opponents, we
show that a variant of our optimistic MLE algorithm is capable of achieving
sublinear regret when being compared against the optimal maximin policies. To
our best knowledge, this work provides the first line of sample-efficient
results for learning POMGs.",0,0,0,0,0,0,0.15443,10.0,0.660525,58
0a03d328-4fe9-4da1-afce-56e5862690ac,Controlling Extra-Textual Attributes about Dialogue Participants -- A Case Study of English-to-Polish Neural Machine Translation,2,0.00956363,0.137413,"Unlike English, morphologically rich languages can reveal characteristics of
speakers or their conversational partners, such as gender and number, via
pronouns, morphological endings of words and syntax. When translating from
English to such languages, a machine translation model needs to opt for a
certain interpretation of textual context, which may lead to serious
translation errors if extra-textual information is unavailable. We investigate
this challenge in the English-to-Polish language direction. We focus on the
underresearched problem of utilising external metadata in automatic translation
of TV dialogue, proposing a case study where a wide range of approaches for
controlling attributes in translation is employed in a multi-attribute
scenario. The best model achieves an improvement of +5.81 chrF++/+6.03 BLEU,
with other models achieving competitive performance. We additionally contribute
a novel attribute-annotated dataset of Polish TV dialogue and a morphological
analysis script used to evaluate attribute control in models.",0,1,0,1,0,0,0.0968783,8.0,0.513347,58
9e754328-045c-4d02-947f-5fc39b63209a,HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images,5,0.14924,0.35174,"In this paper, we present the results of the HSE-NN team in the 4th
competition on Affective Behavior Analysis in-the-wild (ABAW). The novel
multi-task EfficientNet model is trained for simultaneous recognition of facial
expressions and prediction of valence and arousal on static photos. The
resulting MT-EmotiEffNet extracts visual features that are fed into simple
feed-forward neural networks in the multi-task learning challenge. We obtain
performance measure 1.3 on the validation set, which is significantly greater
when compared to either performance of baseline (0.3) or existing models that
are trained only on the s-Aff-Wild2 database. In the learning from synthetic
data challenge, the quality of the original synthetic training set is increased
by using the super-resolution techniques, such as Real-ESRGAN. Next, the
MT-EmotiEffNet is fine-tuned on the new training set. The final prediction is a
simple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our
average validation F1 score is 18% greater than the baseline convolutional
neural network.",0,1,0,0,0,0,0.891523,4.0,0.797154,25
fa725ae1-d843-4094-b95a-1195bdd8b86d,When Is Partially Observable Reinforcement Learning Not Scary?,61,0.857366,0.999933,"Applications of Reinforcement Learning (RL), in which agents learn to make a
sequence of decisions despite lacking complete information about the latent
states of the controlled system, that is, they act under partial observability
of the states, are ubiquitous. Partially observable RL can be notoriously
difficult -- well-known information-theoretic results show that learning
partially observable Markov decision processes (POMDPs) requires an exponential
number of samples in the worst case. Yet, this does not rule out the existence
of large subclasses of POMDPs over which learning is tractable.
  In this paper we identify such a subclass, which we call weakly revealing
POMDPs. This family rules out the pathological instances of POMDPs where
observations are uninformative to a degree that makes learning hard. We prove
that for weakly revealing POMDPs, a simple algorithm combining optimism and
Maximum Likelihood Estimation (MLE) is sufficient to guarantee polynomial
sample complexity. To the best of our knowledge, this is the first provably
sample-efficient result for learning from interactions in overcomplete POMDPs,
where the number of latent states can be larger than the number of
observations.",0,0,0,0,0,0,0.823056,11.0,0.903618,58
a872af26-07b0-48bd-a675-c9b85f386901,Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021,20,0.275456,0.794019,"Human intelligence has the remarkable ability to quickly adapt to new tasks
and environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research in
this direction, we propose \emph{IGLU: Interactive Grounded Language
Understanding in a Collaborative Environment}.
  The primary goal of the competition is to approach the problem of how to
build interactive agents that learn to solve a task while provided with
grounded natural language instructions in a collaborative environment.
Understanding the complexity of the challenge, we split it into sub-tasks to
make it feasible for participants.",0,1,0,0,0,0,0.303848,8.0,0.671871,80
7e5f1eef-2d31-42c4-99e7-2a9244f74b85,KinyaBERT: a Morphology-aware Kinyarwanda Language Model,24,0.566785,0.884318,"Pre-trained language models such as BERT have been successful at tackling
many natural language processing tasks. However, the unsupervised sub-word
tokenization methods commonly used in these models (e.g., byte-pair encoding -
BPE) are sub-optimal at handling morphologically rich languages. Even given a
morphological analyzer, naive sequencing of morphemes into a standard BERT
architecture is inefficient at capturing morphological compositionality and
expressing word-relative syntactic regularities. We address these challenges by
proposing a simple yet effective two-tier BERT architecture that leverages a
morphological analyzer and explicitly represents morphological
compositionality. Despite the success of BERT, most of its evaluations have
been conducted on high-resource languages, obscuring its applicability on
low-resource languages. We evaluate our proposed method on the low-resource
morphologically rich Kinyarwanda language, naming the proposed model
architecture KinyaBERT. A robust set of experimental results reveal that
KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity
recognition task and by 4.3% in average score of a machine-translated GLUE
benchmark. KinyaBERT fine-tuning has better convergence and achieves more
robust results on multiple tasks even in the presence of translation noise.",1,1,0,0,1,0,0.836197,5.0,0.796677,71
5f082014-c33b-44cd-b959-049d2d191b3e,Fast Point Cloud Generation with Straight Flows,16,0.290312,0.708762,"Diffusion models have emerged as a powerful tool for point cloud generation.
A key component that drives the impressive performance for generating
high-quality samples from noise is iteratively denoise for thousands of steps.
While beneficial, the complexity of learning steps has limited its applications
to many 3D real-world. To address this limitation, we propose Point Straight
Flow (PSF), a model that exhibits impressive performance using one step. Our
idea is based on the reformulation of the standard diffusion model, which
optimizes the curvy learning trajectory into a straight path. Further, we
develop a distillation strategy to shorten the straight path into one step
without a performance loss, enabling applications to 3D real-world with latency
constraints. We perform evaluations on multiple 3D tasks and find that our PSF
performs comparably to the standard diffusion model, outperforming other
efficient 3D point cloud generation methods. On real-world applications such as
point cloud completion and training-free text-guided generation in a
low-latency setup, PSF performs favorably.",0,1,0,0,0,0,0.948167,4.0,0.868914,52
a008c6f8-8c81-465d-b146-454f2045a707,ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing,15,0.21426,0.570739,"Sketch-and-extrude is a common and intuitive modeling process in computer
aided design. This paper studies the problem of learning the shape given in the
form of point clouds by inverse sketch-and-extrude. We present ExtrudeNet, an
unsupervised end-to-end network for discovering sketch and extrude from point
clouds. Behind ExtrudeNet are two new technical components: 1) an effective
representation for sketch and extrude, which can model extrusion with freeform
sketches and conventional cylinder and box primitives as well; and 2) a
numerical method for computing the signed distance field which is used in the
network learning. This is the first attempt that uses machine learning to
reverse engineer the sketch-and-extrude modeling process of a shape in an
unsupervised fashion. ExtrudeNet not only outputs a compact, editable and
interpretable representation of the shape that can be seamlessly integrated
into modern CAD software, but also aligns with the standard CAD modeling
process facilitating various editing applications, which distinguishes our work
from existing shape parsing research. Code is released at
https://github.com/kimren227/ExtrudeNet.",0,1,0,0,0,0,0.288029,11.0,0.755538,40
4d1ccc47-6576-4f28-974b-2b52529b9fe4,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,97,0.965718,0.351071,"Does prompting a large language model (LLM) like GPT-3 with explanations
improve in-context learning? We study this question on two NLP tasks that
involve reasoning over text, namely question answering and natural language
inference. We test the performance of four LLMs on three textual reasoning
datasets using prompts that include explanations in multiple different styles.
For these tasks, we find that including explanations in the prompts for OPT,
GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to
moderate accuracy improvements over standard few-show learning. However,
text-davinci-002 is able to benefit more substantially.
  We further show that explanations generated by the LLMs may not entail the
models' predictions nor be factually grounded in the input, even on simple
tasks with extractive explanations. However, these flawed explanations can
still be useful as a way to verify LLMs' predictions post-hoc. Through analysis
in our three settings, we show that explanations judged by humans to be
good--logically consistent with the input and the prediction--more likely
cooccur with accurate predictions. Following these observations, we train
calibrators using automatically extracted scores that assess the reliability of
explanations, allowing us to improve performance post-hoc across all of our
datasets.",0,0,0,0,0,0,0.970209,3.0,0.882397,72
3d630b70-be44-485d-8e07-689c4245ab51,FedX: Unsupervised Federated Learning with Cross Knowledge Distillation,35,0.165003,0.761076,"This paper presents FedX, an unsupervised federated learning framework. Our
model learns unbiased representation from decentralized and heterogeneous local
data. It employs a two-sided knowledge distillation with contrastive learning
as a core component, allowing the federated system to function without
requiring clients to share any data features. Furthermore, its adaptable
architecture can be used as an add-on module for existing unsupervised
algorithms in federated settings. Experiments show that our model improves
performance significantly (1.58--5.52pp) on five unsupervised algorithms.",1,1,0,0,0,0,0.666286,6.0,0.747262,47
d4d2dbce-8040-46e6-9e37-d4e2f9b1dfc1,Communication Beyond Transmitting Bits: Semantics-Guided Source and Channel Coding,30,0.624656,0.810792,"Classical communication paradigms focus on accurately transmitting bits over
a noisy channel, and Shannon theory provides a fundamental theoretical limit on
the rate of reliable communications. In this approach, bits are treated
equally, and the communication system is oblivious to what meaning these bits
convey or how they would be used. Future communications towards intelligence
and conciseness will predictably play a dominant role, and the proliferation of
connected intelligent agents requires a radical rethinking of coded
transmission paradigm to support the new communication morphology on the
horizon. The recent concept of ""semantic communications"" offers a promising
research direction. Injecting semantic guidance into the coded transmission
design to achieve semantics-aware communications shows great potential for
further breakthrough in effectiveness and reliability. This article sheds light
on semantics-guided source and channel coding as a transmission paradigm of
semantic communications, which exploits both data semantics diversity and
wireless channel diversity together to boost the whole system performance. We
present the general system architecture and key techniques, and indicate some
open issues on this topic.",0,0,0,0,0,0,0.992742,10.0,0.998503,21
bb1b660e-8ff8-4bb4-bcde-8983b47d6ba3,Relational Future Captioning Model for Explaining Likely Collisions in Daily Tasks,3,0.0516304,0.300953,"Domestic service robots that support daily tasks are a promising solution for
elderly or disabled people. It is crucial for domestic service robots to
explain the collision risk before they perform actions. In this paper, our aim
is to generate a caption about a future event. We propose the Relational Future
Captioning Model (RFCM), a crossmodal language generation model for the future
captioning task. The RFCM has the Relational Self-Attention Encoder to extract
the relationships between events more effectively than the conventional
self-attention in transformers. We conducted comparison experiments, and the
results show the RFCM outperforms a baseline method on two datasets.",1,1,0,0,0,0,0.544078,7.0,0.735577,32
06a7235e-df85-4bec-be14-38526fa0c206,An Efficient Coarse-to-Fine Facet-Aware Unsupervised Summarization Framework based on Semantic Blocks,4,0.039906,0.150414,"Unsupervised summarization methods have achieved remarkable results by
incorporating representations from pre-trained language models. However,
existing methods fail to consider efficiency and effectiveness at the same time
when the input document is extremely long. To tackle this problem, in this
paper, we proposed an efficient Coarse-to-Fine Facet-Aware Ranking (C2F-FAR)
framework for unsupervised long document summarization, which is based on the
semantic block. The semantic block refers to continuous sentences in the
document that describe the same facet. Specifically, we address this problem by
converting the one-step ranking method into the hierarchical multi-granularity
two-stage ranking. In the coarse-level stage, we propose a new segment
algorithm to split the document into facet-aware semantic blocks and then
filter insignificant blocks. In the fine-level stage, we select salient
sentences in each block and then extract the final summary from selected
sentences. We evaluate our framework on four long document summarization
datasets: Gov-Report, BillSum, arXiv, and PubMed. Our C2F-FAR can achieve new
state-of-the-art unsupervised summarization results on Gov-Report and BillSum.
In addition, our method speeds up 4-28 times more than previous
methods.\footnote{\url{https://github.com/xnliang98/c2f-far}}",1,1,0,0,1,0,0.564262,7.0,0.743585,48
9af7e8d3-1947-4c75-ae08-f40785ae381b,Coalescing Global and Local Information for Procedural Text Understanding,10,0.0571447,0.367772,"Procedural text understanding is a challenging language reasoning task that
requires models to track entity states across the development of a narrative. A
complete procedural understanding solution should combine three core aspects:
local and global views of the inputs, and global view of outputs. Prior methods
considered a subset of these aspects, resulting in either low precision or low
recall. In this paper, we propose Coalescing Global and Local Information
(CGLI), a new model that builds entity- and timestep-aware input
representations (local input) considering the whole context (global input), and
we jointly model the entity states with a structured prediction objective
(global output). Thus, CGLI simultaneously optimizes for both precision and
recall. We extend CGLI with additional output layers and integrate it into a
story reasoning framework. Extensive experiments on a popular procedural text
understanding dataset show that our model achieves state-of-the-art results;
experiments on a story reasoning benchmark show the positive impact of our
model on downstream reasoning.",1,0,0,0,1,0,0.133216,6.0,0.407568,44
3b528929-ed6e-4d13-8ead-fe87fff92200,On Web-based Visual Corpus Construction for Visual Document Understanding,2,0.0241817,0.48695,"In recent years, research on visual document understanding (VDU) has grown
significantly, with a particular emphasis on the development of self-supervised
learning methods. However, one of the significant challenges faced in this
field is the limited availability of publicly accessible visual corpora or
extensive collections of images with detailed text annotations, particularly
for non-Latin or resource-scarce languages. To address this challenge, we
propose Web-based Visual Corpus Builder (Webvicob), a dataset generator engine
capable of constructing large-scale, multilingual visual corpora from raw
Wikipedia HTML dumps. Our experiments demonstrate that the data generated by
Webvicob can be used to train robust VDU models that perform well on various
downstream tasks, such as DocVQA and post-OCR parsing. Furthermore, when using
a dataset of 1 million images generated by Webvicob, we observed an improvement
of over 13% on the DocVQA Task 3 compared to a dataset of 11 million images
from the IIT-CDIP. The implementation of our engine is publicly available on
https://github.com/clovaai/webvicob",1,1,0,1,0,0,0.596783,7.0,0.756337,49
8323484f-0d22-49e9-91ae-4937bf7ff9b6,MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning,8,0.22814,0.714053,"Transformer language models (TLMs) are critical for most NLP tasks, but they
are difficult to create for low-resource languages because of how much
pretraining data they require. In this work, we investigate two techniques for
training monolingual TLMs in a low-resource setting: greatly reducing TLM size,
and complementing the masked language modeling objective with two
linguistically rich supervised tasks (part-of-speech tagging and dependency
parsing). Results from 7 diverse languages indicate that our model, MicroBERT,
is able to produce marked improvements in downstream task evaluations relative
to a typical monolingual TLM pretraining approach. Specifically, we find that
monolingual MicroBERT models achieve gains of up to 18% for parser LAS and 11%
for NER F1 compared to a multilingual baseline, mBERT, while having less than
1% of its parameter count. We conclude reducing TLM parameter count and using
labeled data for pretraining low-resource TLMs can yield large quality benefits
and in some cases produce models that outperform multilingual approaches.",1,1,0,0,0,0,0.895604,7.0,0.886533,49
9c50156d-ce8f-4763-97cf-7951231c8e6e,TableFormer: Robust Transformer Modeling for Table-Text Encoding,75,0.325018,0.92446,"Understanding tables is an important aspect of natural language
understanding. Existing models for table understanding require linearization of
the table structure, where row or column order is encoded as an unwanted bias.
Such spurious biases make the model vulnerable to row and column order
perturbations. Additionally, prior work has not thoroughly modeled the table
structures or table-text alignments, hindering the table-text understanding
ability. In this work, we propose a robust and structurally aware table-text
encoding architecture TableFormer, where tabular structural biases are
incorporated completely through learnable attention biases. TableFormer is (1)
strictly invariant to row and column orders, and, (2) could understand tables
better due to its tabular inductive biases. Our evaluations showed that
TableFormer outperforms strong baselines in all settings on SQA, WTQ and
TabFact table reasoning datasets, and achieves state-of-the-art performance on
SQA, especially when facing answer-invariant row and column order perturbations
(6% improvement over the best baseline), because previous SOTA models'
performance drops by 4% - 6% when facing such perturbations while TableFormer
is not affected.",0,0,0,0,1,0,0.769918,4.0,0.693841,21
1b4332d3-2407-4461-9ce1-801e23388bf9,CSL: A Large-scale Chinese Scientific Literature Dataset,29,0.912617,0.923326,"Scientific literature serves as a high-quality corpus, supporting a lot of
Natural Language Processing (NLP) research. However, existing datasets are
centered around the English language, which restricts the development of
Chinese scientific NLP. In this work, we present CSL, a large-scale Chinese
Scientific Literature dataset, which contains the titles, abstracts, keywords
and academic fields of 396k papers. To our knowledge, CSL is the first
scientific document dataset in Chinese. The CSL can serve as a Chinese corpus.
Also, this semi-structured data is a natural annotation that can constitute
many supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the
performance of models across scientific domain tasks, i.e., summarization,
keyword generation and text classification. We analyze the behavior of existing
text-to-text models on the evaluation tasks and reveal the challenges for
Chinese scientific NLP tasks, which provides a valuable reference for future
research. Data and code are available at https://github.com/ydli-ai/CSL",1,1,1,1,0,0,0.976234,9.0,0.967723,29
b279b77e-2626-407e-a5ce-39983f4c79b9,Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing,11,0.0806252,0.790389,"Entity typing aims at predicting one or more words that describe the type(s)
of a specific mention in a sentence. Due to shortcuts from surface patterns to
annotated entity labels and biased training, existing entity typing models are
subject to the problem of spurious correlations. To comprehensively investigate
the faithfulness and reliability of entity typing methods, we first
systematically define distinct kinds of model biases that are reflected mainly
from spurious correlations. Particularly, we identify six types of existing
model biases, including mention-context bias, lexical overlapping bias, named
entity bias, pronoun bias, dependency bias, and overgeneralization bias. To
mitigate model biases, we then introduce a counterfactual data augmentation
method. By augmenting the original training set with their debiased
counterparts, models are forced to fully comprehend sentences and discover the
fundamental cues for entity typing, rather than relying on spurious
correlations for shortcuts. Experimental results on the UFET dataset show our
counterfactual data augmentation approach helps improve generalization of
different entity typing models with consistently better performance on both the
original and debiased test sets.",1,1,0,0,0,0,0.20235,6.0,0.483953,62
079fd354-4f75-422c-af06-c29419da7ce7,nerf2nerf: Pairwise Registration of Neural Radiance Fields,22,0.340695,0.60803,"We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io",1,1,0,0,0,0,0.777999,10.0,0.87994,66
824cb229-33a0-4c6e-9f17-2c0133c0ba4c,Are Sample-Efficient NLP Models More Robust?,5,0.0172149,0.242371,"Recent results in image classification and extractive question answering have
observed that pre-trained models trained on less in-distribution data have
better out-of-distribution performance. However, it is unclear how broadly
these trends hold. We conduct a large empirical study across three tasks, three
broadly-applicable modeling interventions (increasing model size, using a
different adaptation method, and pre-training on more data), and 14 diverse
datasets to investigate the relationship between sample efficiency (amount of
data needed to reach a given ID accuracy) and robustness (how models fare on
OOD evaluation). We find that higher sample efficiency is only correlated with
better average OOD robustness on some modeling interventions and tasks, but not
others. On individual datasets, models with lower sample efficiency can even be
more robust. These results suggest that general-purpose methods for improving
sample efficiency are unlikely to yield universal OOD robustness improvements,
since such improvements are highly dataset- and task-dependent. Even in an era
of large, multi-purpose pretrained models, task-specific decisions may often be
necessary for OOD generalization.",0,0,0,0,0,1,0.560339,8.0,0.77428,36
112559cb-9867-44f2-a068-50a26323d793,NCTV: Neural Clamping Toolkit and Visualization for Neural Network Calibration,1,0.0145795,0.0638794,"With the advancement of deep learning technology, neural networks have
demonstrated their excellent ability to provide accurate predictions in many
tasks. However, a lack of consideration for neural network calibration will not
gain trust from humans, even for high-accuracy models. In this regard, the gap
between the confidence of the model's predictions and the actual correctness
likelihood must be bridged to derive a well-calibrated model. In this paper, we
introduce the Neural Clamping Toolkit, the first open-source framework designed
to help developers employ state-of-the-art model-agnostic calibrated models.
Furthermore, we provide animations and interactive sections in the
demonstration to familiarize researchers with calibration in neural networks. A
Colab tutorial on utilizing our toolkit is also introduced.",0,1,0,0,1,0,0.213171,12.0,0.746865,10
9efe9995-107c-4fc0-b8b5-7b88f394925b,DeepLIIF: An Online Platform for Quantification of Clinical Pathology Slides,11,0.309435,0.748379,"In the clinic, resected tissue samples are stained with Hematoxylin-and-Eosin
(H&E) and/or Immunhistochemistry (IHC) stains and presented to the pathologists
on glass slides or as digital scans for diagnosis and assessment of disease
progression. Cell-level quantification, e.g. in IHC protein expression scoring,
can be extremely inefficient and subjective. We present DeepLIIF
(https://deepliif.org), a first free online platform for efficient and
reproducible IHC scoring. DeepLIIF outperforms current state-of-the-art
approaches (relying on manual error-prone annotations) by virtually restaining
clinical IHC slides with more informative multiplex immunofluorescence
staining. Our DeepLIIF cloud-native platform supports (1) more than 150
proprietary/non-proprietary input formats via the Bio-Formats standard, (2)
interactive adjustment, visualization, and downloading of the IHC
quantification results and the accompanying restained images, (3) consumption
of an exposed workflow API programmatically or through interactive plugins for
open source whole slide image viewers such as QuPath/ImageJ, and (4) auto
scaling to efficiently scale GPU resources based on user demand.",1,1,0,0,1,0,0.945345,4.0,0.864396,9
ba5a3af6-4de0-4c64-9377-e61c04240646,Normalized Feature Distillation for Semantic Segmentation,1,0.0343231,0.0361114,"As a promising approach in model compression, knowledge distillation improves
the performance of a compact model by transferring the knowledge from a
cumbersome one. The kind of knowledge used to guide the training of the student
is important. Previous distillation methods in semantic segmentation strive to
extract various forms of knowledge from the features, which involve elaborate
manual design relying on prior information and have limited performance gains.
In this paper, we propose a simple yet effective feature distillation method
called normalized feature distillation (NFD), aiming to enable effective
distillation with the original features without the need to manually design new
forms of knowledge. The key idea is to prevent the student from focusing on
imitating the magnitude of the teacher's feature response by normalization. Our
method achieves state-of-the-art distillation results for semantic segmentation
on Cityscapes, VOC 2012, and ADE20K datasets. Code will be available.",0,1,0,0,1,0,0.991302,11.0,0.995236,36
10f8fa2d-5029-4012-80d1-786d9b98231b,NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis,4,0.179037,0.30594,"With the development of natural language processing techniques(NLP),
automatic diagnosis of eye diseases using ophthalmology electronic medical
records (OEMR) has become possible. It aims to evaluate the condition of both
eyes of a patient respectively, and we formulate it as a particular multi-label
classification task in this paper. Although there are a few related studies in
other diseases, automatic diagnosis of eye diseases exhibits unique
characteristics. First, descriptions of both eyes are mixed up in OEMR
documents, with both free text and templated asymptomatic descriptions,
resulting in sparsity and clutter of information. Second, OEMR documents
contain multiple parts of descriptions and have long document lengths. Third,
it is critical to provide explainability to the disease diagnosis model. To
overcome those challenges, we present an effective automatic eye disease
diagnosis framework, NEEDED. In this framework, a preprocessing module is
integrated to improve the density and quality of information. Then, we design a
hierarchical transformer structure for learning the contextualized
representations of each sentence in the OEMR document. For the diagnosis part,
we propose an attention-based predictor that enables traceable diagnosis by
obtaining disease-specific information. Experiments on the real dataset and
comparison with several baseline models show the advantage and explainability
of our framework.",1,1,0,0,0,0,0.354266,7.0,0.65194,32
4698279d-eb26-4d7e-af7b-fd07b5e665fa,A Screen-Shooting Resilient Document Image Watermarking Scheme using Deep Neural Network,9,0.129864,0.649139,"With the advent of the screen-reading era, the confidential documents
displayed on the screen can be easily captured by a camera without leaving any
traces. Thus, this paper proposes a novel screen-shooting resilient
watermarking scheme for document image using deep neural network. By applying
this scheme, when the watermarked image is displayed on the screen and captured
by a camera, the watermark can be still extracted from the captured
photographs. Specifically, our scheme is an end-to-end neural network with an
encoder to embed watermark and a decoder to extract watermark. During the
training process, a distortion layer between encoder and decoder is added to
simulate the distortions introduced by screen-shooting process in real scenes,
such as camera distortion, shooting distortion, light source distortion.
Besides, an embedding strength adjustment strategy is designed to improve the
visual quality of the watermarked image with little loss of extraction
accuracy. The experimental results show that the scheme has higher robustness
and visual quality than other three recent state-of-the-arts. Specially, even
if the shooting distances and angles are in extreme, our scheme can also obtain
high extraction accuracy.",1,1,0,0,1,0,0.0661145,11.0,0.609836,36
6052c2a5-278a-4905-a778-181cba76e4cc,GCS-Q: Quantum Graph Coalition Structure Generation,6,0.143895,0.684159,"The problem of generating an optimal coalition structure for a given
coalition game of rational agents is to find a partition that maximizes their
social welfare and is known to be NP-hard. This paper proposes GCS-Q, a novel
quantum-supported solution for Induced Subgraph Games (ISGs) in coalition
structure generation. GCS-Q starts by considering the grand coalition as
initial coalition structure and proceeds by iteratively splitting the
coalitions into two nonempty subsets to obtain a coalition structure with a
higher coalition value. In particular, given an $n$-agent ISG, the GCS-Q solves
the optimal split problem $\mathcal{O} (n)$ times using a quantum annealing
device, exploring $\mathcal{O}(2^n)$ partitions at each step. We show that
GCS-Q outperforms the currently best classical solvers with its runtime in the
order of $n^2$ and an expected worst-case approximation ratio of $93\%$ on
standard benchmark datasets.",0,0,0,0,1,0,0.38433,4.0,0.416762,13
945fc2fb-94a5-4cef-9d8c-9d20e58ebd1c,The Theory of Artificial Immutability: Protecting Algorithmic Groups Under Anti-Discrimination Law,19,0.0212137,0.6479,"Artificial Intelligence (AI) is increasingly used to make important decisions
about people. While issues of AI bias and proxy discrimination are well
explored, less focus has been paid to the harms created by profiling based on
groups that do not map to or correlate with legally protected groups such as
sex or ethnicity. This raises a question: are existing equality laws able to
protect against emergent AI-driven inequality? This article examines the legal
status of algorithmic groups in North American and European non-discrimination
doctrine, law, and jurisprudence and will show that algorithmic groups are not
comparable to traditional protected groups. Nonetheless, these new groups are
worthy of protection. I propose a new theory of harm - ""the theory of
artificial immutability"" - that aims to bring AI groups within the scope of the
law. My theory describes how algorithmic groups act as de facto immutable
characteristics in practice that limit people's autonomy and prevent them from
achieving important goals.",0,0,1,0,0,0,2.49607e-05,13.0,0.0609512,24
8d29d107-fb38-4e60-b20d-9f7b4b1908c5,Systematic Evaluation of Predictive Fairness,3,0.0518983,0.170699,"Mitigating bias in training on biased datasets is an important open problem.
Several techniques have been proposed, however the typical evaluation regime is
very limited, considering very narrow data conditions. For instance, the effect
of target class imbalance and stereotyping is under-studied. To address this
gap, we examine the performance of various debiasing methods across multiple
tasks, spanning binary classification (Twitter sentiment), multi-class
classification (profession prediction), and regression (valence prediction).
Through extensive experimentation, we find that data conditions have a strong
influence on relative model performance, and that general conclusions cannot be
drawn about method efficacy when evaluating only on standard datasets, as is
current practice in fairness research.",1,1,0,0,0,1,0.511097,7.0,0.722274,47
fde1a35a-53f2-49c5-9154-2d0a4c359f0f,Avalanche RL: a Continual Reinforcement Learning Library,4,0.00554678,0.0820117,"Continual Reinforcement Learning (CRL) is a challenging setting where an
agent learns to interact with an environment that is constantly changing over
time (the stream of experiences). In this paper, we describe Avalanche RL, a
library for Continual Reinforcement Learning which allows to easily train
agents on a continuous stream of tasks. Avalanche RL is based on PyTorch and
supports any OpenAI Gym environment. Its design is based on Avalanche, one of
the more popular continual learning libraries, which allow us to reuse a large
number of continual learning strategies and improve the interaction between
reinforcement learning and continual learning researchers. Additionally, we
propose Continual Habitat-Lab, a novel benchmark and a high-level library which
enables the usage of the photorealistic simulator Habitat-Sim for CRL research.
Overall, Avalanche RL attempts to unify under a common framework continual
reinforcement learning applications, which we hope will foster the growth of
the field.",1,1,0,0,0,0,0.248781,9.0,0.682111,31
45c65db2-9e56-4876-8b80-bd290cca8a68,Grammar-Based Grounded Lexicon Learning,13,0.541744,0.944362,"We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist
approach toward learning a compositional and grounded meaning representation of
language from grounded data, such as paired images and texts. At the core of
G2L2 is a collection of lexicon entries, which map each word to a tuple of a
syntactic type and a neuro-symbolic semantic program. For example, the word
shiny has a syntactic type of adjective; its neuro-symbolic semantic program
has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is
associated with a neural network embedding, which will be used to classify
shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries
associated with each token. It then derives the meaning of the sentence as an
executable neuro-symbolic program by composing lexical meanings based on
syntax. The recovered meaning programs can be executed on grounded inputs. To
facilitate learning in an exponentially-growing compositional space, we
introduce a joint parsing and expected execution algorithm, which does local
marginalization over derivations to reduce the training time. We evaluate G2L2
on two domains: visual reasoning and language-driven navigation. Results show
that G2L2 can generalize from small amounts of data to novel compositions of
words.",0,0,0,0,0,0,0.937997,10.0,0.941322,56
27c3276e-936d-4cc3-892c-7d9c87d540a3,Multi-objective hyperparameter optimization with performance uncertainty,1,0.00499487,0.104688,"The performance of any Machine Learning (ML) algorithm is impacted by the
choice of its hyperparameters. As training and evaluating a ML algorithm is
usually expensive, the hyperparameter optimization (HPO) method needs to be
computationally efficient to be useful in practice. Most of the existing
approaches on multi-objective HPO use evolutionary strategies and
metamodel-based optimization. However, few methods have been developed to
account for uncertainty in the performance measurements. This paper presents
results on multi-objective hyperparameter optimization with uncertainty on the
evaluation of ML algorithms. We combine the sampling strategy of
Tree-structured Parzen Estimators (TPE) with the metamodel obtained after
training a Gaussian Process Regression (GPR) with heterogeneous noise.
Experimental results on three analytical test functions and three ML problems
show the improvement over multi-objective TPE and GPR, achieved with respect to
the hypervolume indicator.",0,1,0,0,0,0,0.0127675,14.0,0.574009,18
48421967-1e08-4b79-ad8c-0b215eabf889,SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model,17,0.312775,0.677774,"Data-driven speech processing models usually perform well with a large amount
of text supervision, but collecting transcribed speech data is costly.
Therefore, we propose SpeechCLIP, a novel framework bridging speech and text
through images to enhance speech models without transcriptions. We leverage
state-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images
and spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior
state-of-the-art on image-speech retrieval and performs zero-shot speech-text
retrieval without direct supervision from transcriptions. Moreover, SpeechCLIP
can directly retrieve semantically related keywords from speech.",1,1,0,0,1,0,0.819336,4.0,0.731927,51
01a56791-637c-4bf7-9899-b9cf4b98b819,Practical Exposure Correction: Great Truths Are Always Simple,11,0.435651,0.672288,"Improving the visual quality of the given degraded observation by correcting
exposure level is a fundamental task in the computer vision community. Existing
works commonly lack adaptability towards unknown scenes because of the
data-driven patterns (deep networks) and limited regularization (traditional
optimization), and they usually need time-consuming inference. These two points
heavily limit their practicability. In this paper, we establish a Practical
Exposure Corrector (PEC) that assembles the characteristics of efficiency and
performance. To be concrete, we rethink the exposure correction to provide a
linear solution with exposure-sensitive compensation. Around generating the
compensation, we introduce an exposure adversarial function as the key engine
to fully extract valuable information from the observation. By applying the
defined function, we construct a segmented shrinkage iterative scheme to
generate the desired compensation. Its shrinkage nature supplies powerful
support for algorithmic stability and robustness. Extensive experimental
evaluations fully reveal the superiority of our proposed PEC. The code is
available at https://rsliu.tech/PEC.",1,1,0,0,0,1,0.972672,8.0,0.958931,38
1e897a38-2269-4a51-b869-4751dc753328,Solutions for Fine-grained and Long-tailed Snake Species Recognition in SnakeCLEF 2022,4,0.0659373,0.576998,"Automatic snake species recognition is important because it has vast
potential to help lower deaths and disabilities caused by snakebites. We
introduce our solution in SnakeCLEF 2022 for fine-grained snake species
recognition on a heavy long-tailed class distribution. First, a network
architecture is designed to extract and fuse features from multiple modalities,
i.e. photograph from visual modality and geographic locality information from
language modality. Then, logit adjustment based methods are studied to relieve
the impact caused by the severe class imbalance. Next, a combination of
supervised and self-supervised learning method is proposed to make full use of
the dataset, including both labeled training data and unlabeled testing data.
Finally, post processing strategies, such as multi-scale and multi-crop
test-time-augmentation, location filtering and model ensemble, are employed for
better performance. With an ensemble of several different models, a private
score 82.65%, ranking the 3rd, is achieved on the final leaderboard.",0,1,0,0,0,0,0.459311,6.0,0.650714,31
4ddb6ca8-890f-4e66-bfc2-dc4ecd3b0cb7,Down and Across: Introducing Crossword-Solving as a New NLP Benchmark,1,0.00592431,0.0234985,"Solving crossword puzzles requires diverse reasoning capabilities, access to
a vast amount of knowledge about language and the world, and the ability to
satisfy the constraints imposed by the structure of the puzzle. In this work,
we introduce solving crossword puzzles as a new natural language understanding
task. We release the specification of a corpus of crossword puzzles collected
from the New York Times daily crossword spanning 25 years and comprised of a
total of around nine thousand puzzles. These puzzles include a diverse set of
clues: historic, factual, word meaning, synonyms/antonyms, fill-in-the-blank,
abbreviations, prefixes/suffixes, wordplay, and cross-lingual, as well as clues
that depend on the answers to other clues. We separately release the
clue-answer pairs from these puzzles as an open-domain question answering
dataset containing over half a million unique clue-answer pairs. For the
question answering task, our baselines include several sequence-to-sequence and
retrieval-based generative models. We also introduce a non-parametric
constraint satisfaction baseline for solving the entire crossword puzzle.
Finally, we propose an evaluation framework which consists of several
complementary performance metrics.",0,0,1,1,0,0,0.270516,7.0,0.605244,42
795773e6-7512-41b6-a89a-a751f17c46f6,The slurk Interaction Server Framework: Better Data for Better Dialog Models,9,0.0884719,0.33126,"This paper presents the slurk software, a lightweight interaction server for
setting up dialog data collections and running experiments. Slurk enables a
multitude of settings including text-based, speech and video interaction
between two or more humans or humans and bots, and a multimodal display area
for presenting shared or private interactive context. The software is
implemented in Python with an HTML and JS frontend that can easily be adapted
to individual needs. It also provides a setup for pairing participants on
common crowdworking platforms such as Amazon Mechanical Turk and some example
bot scripts for common interaction scenarios.",1,1,0,0,0,0,0.0251036,10.0,0.47185,25
a405aa3d-d420-407e-9cb9-82b1419c413a,Action Languages Based Actual Causality for Computational Ethics: a Sound and Complete Implementation in ASP,2,0.0198566,0.0488551,"Although moral responsibility is not circumscribed by causality, they are
both closely intermixed. Furthermore, rationally understanding the evolution of
the physical world is inherently linked with the idea of causality. Thus, the
decision-making applications based on automated planning inevitably have to
deal with causality, especially if they consider imputability aspects or
integrate references to ethical norms. The many debates around causation in the
last decades have shown how complex this notion is and thus, how difficult is
its integration with planning. As a result, much of the work in computational
ethics relegates causality to the background, despite the considerations stated
above. This paper's contribution is to provide a complete and sound translation
into logic programming from an actual causation definition suitable for action
languages, this definition is a formalisation of Wright's NESS test. The
obtained logic program allows to deal with complex causal relations. In
addition to enabling agents to reason about causality, this contribution
specifically enables the computational ethics domain to handle situations that
were previously out of reach. In a context where ethical considerations in
decision-making are increasingly important, advances in computational ethics
can greatly benefit the entire AI community.",0,0,0,0,0,0,0.00340136,19.0,0.616247,74
020d0364-6fa6-4811-aa5c-81c65273996e,A Deep-Discrete Learning Framework for Spherical Surface Registration,7,0.0585112,0.778236,"Cortical surface registration is a fundamental tool for neuroimaging analysis
that has been shown to improve the alignment of functional regions relative to
volumetric approaches. Classically, image registration is performed by
optimizing a complex objective similarity function, leading to long run times.
This contributes to a convention for aligning all data to a global average
reference frame that poorly reflects the underlying cortical heterogeneity. In
this paper, we propose a novel unsupervised learning-based framework that
converts registration to a multi-label classification problem, where each point
in a low-resolution control grid deforms to one of fixed, finite number of
endpoints. This is learned using a spherical geometric deep learning
architecture, in an end-to-end unsupervised way, with regularization imposed
using a deep Conditional Random Field (CRF). Experiments show that our proposed
framework performs competitively, in terms of similarity and areal distortion,
relative to the most popular classical surface registration algorithms and
generates smoother deformations than other learning-based surface registration
methods, even in subjects with atypical cortical morphology.",1,0,0,0,0,0,0.54615,8.0,0.769353,31
35fab2f5-6d71-4687-bb6c-9a0dfc512530,Structured access: an emerging paradigm for safe AI deployment,34,0.534865,0.7871,"Structured access is an emerging paradigm for the safe deployment of
artificial intelligence (AI). Instead of openly disseminating AI systems,
developers facilitate controlled, arm's length interactions with their AI
systems. The aim is to prevent dangerous AI capabilities from being widely
accessible, whilst preserving access to AI capabilities that can be used
safely. The developer must both restrict how the AI system can be used, and
prevent the user from circumventing these restrictions through modification or
reverse engineering of the AI system. Structured access is most effective when
implemented through cloud-based AI services, rather than disseminating AI
software that runs locally on users' hardware. Cloud-based interfaces provide
the AI developer greater scope for controlling how the AI system is used, and
for protecting against unauthorized modifications to the system's design. This
chapter expands the discussion of ""publication norms"" in the AI community,
which to date has focused on the question of how the informational content of
AI research projects should be disseminated (e.g., code and models). Although
this is an important question, there are limits to what can be achieved through
the control of information flows. Structured access views AI software not only
as information that can be shared but also as a tool with which users can have
arm's length interactions. There are early examples of structured access being
practiced by AI developers, but there is much room for further development,
both in the functionality of cloud-based interfaces and in the wider
institutional framework.",0,0,1,0,0,0,0.680198,5.0,0.704327,27
3d5dc918-b4e1-48bb-af7a-6c3bc84e09f7,N-Best Hypotheses Reranking for Text-To-SQL Systems,13,0.634961,0.861995,"Text-to-SQL task maps natural language utterances to structured queries that
can be issued to a database. State-of-the-art (SOTA) systems rely on finetuning
large, pre-trained language models in conjunction with constrained decoding
applying a SQL parser. On the well established Spider dataset, we begin with
Oracle studies: specifically, choosing an Oracle hypothesis from a SOTA model's
10-best list, yields a $7.7\%$ absolute improvement in both exact match (EM)
and execution (EX) accuracy, showing significant potential improvements with
reranking. Identifying coherence and correctness as reranking approaches, we
design a model generating a query plan and propose a heuristic schema linking
algorithm. Combining both approaches, with T5-Large, we obtain a consistent
$1\% $ improvement in EM accuracy, and a $~2.5\%$ improvement in EX,
establishing a new SOTA for this task. Our comprehensive error studies on DEV
data show the underlying difficulty in making progress on this task.",0,1,0,0,1,0,0.981316,6.0,0.961978,45
74cd3b8d-304e-4d40-8ef9-e77ed372efb0,Masked Autoencoders that Listen,157,0.788961,0.999937,"This paper studies a simple extension of image-based Masked Autoencoders
(MAE) to self-supervised representation learning from audio spectrograms.
Following the Transformer encoder-decoder design in MAE, our Audio-MAE first
encodes audio spectrogram patches with a high masking ratio, feeding only the
non-masked tokens through encoder layers. The decoder then re-orders and
decodes the encoded context padded with mask tokens, in order to reconstruct
the input spectrogram. We find it beneficial to incorporate local window
attention in the decoder, as audio spectrograms are highly correlated in local
time and frequency bands. We then fine-tune the encoder with a lower masking
ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art
performance on six audio and speech classification tasks, outperforming other
recent models that use external supervised pre-training. The code and models
will be at https://github.com/facebookresearch/AudioMAE.",1,1,0,0,1,0,0.826353,6.0,0.825099,84
9d446686-eb21-4152-981f-00ef028b3361,General Incremental Learning with Domain-aware Categorical Representations,24,0.10339,0.63111,"Continual learning is an important problem for achieving human-level
intelligence in real-world applications as an agent must continuously
accumulate knowledge in response to streaming data/tasks. In this work, we
consider a general and yet under-explored incremental learning problem in which
both the class distribution and class-specific domain distribution change over
time. In addition to the typical challenges in class incremental learning, this
setting also faces the intra-class stability-plasticity dilemma and intra-class
domain imbalance problems. To address above issues, we develop a novel
domain-aware continual learning method based on the EM framework. Specifically,
we introduce a flexible class representation based on the von Mises-Fisher
mixture model to capture the intra-class structure, using an
expansion-and-reduction strategy to dynamically increase the number of
components according to the class complexity. Moreover, we design a bi-level
balanced memory to cope with data imbalances within and across classes, which
combines with a distillation loss to achieve better inter- and intra-class
stability-plasticity trade-off. We conduct exhaustive experiments on three
benchmarks: iDigits, iDomainNet and iCIFAR-20. The results show that our
approach consistently outperforms previous methods by a significant margin,
demonstrating its superiority.",0,0,1,0,1,0,0.415072,6.0,0.627906,40
01ec561b-637e-4bf1-858d-0c2bafdc3aa6,BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation,16,0.140497,0.711012,"We present BlazePose GHUM Holistic, a lightweight neural network pipeline for
3D human body landmarks and pose estimation, specifically tailored to real-time
on-device inference. BlazePose GHUM Holistic enables motion capture from a
single RGB image including avatar control, fitness tracking and AR/VR effects.
Our main contributions include i) a novel method for 3D ground truth data
acquisition, ii) updated 3D body tracking with additional hand landmarks and
iii) full body pose estimation from a monocular image.",0,1,0,1,0,0,0.784744,5.0,0.76394,21
79285379-31ae-40f1-abaf-26ce3bc69447,Neurodevelopmental Phenotype Prediction: A State-of-the-Art Deep Learning Model,1,0.0155661,0.199115,"A major challenge in medical image analysis is the automated detection of
biomarkers from neuroimaging data. Traditional approaches, often based on image
registration, are limited in capturing the high variability of cortical
organisation across individuals. Deep learning methods have been shown to be
successful in overcoming this difficulty, and some of them have even
outperformed medical professionals on certain datasets. In this paper, we apply
a deep neural network to analyse the cortical surface data of neonates, derived
from the publicly available Developing Human Connectome Project (dHCP). Our
goal is to identify neurodevelopmental biomarkers and to predict gestational
age at birth based on these biomarkers. Using scans of preterm neonates
acquired around the term-equivalent age, we were able to investigate the impact
of preterm birth on cortical growth and maturation during late gestation.
Besides reaching state-of-the-art prediction accuracy, the proposed model has
much fewer parameters than the baselines, and its error stays low on both
unregistered and registered cortical surfaces.",1,1,0,0,1,0,0.473881,11.0,0.813432,34
9fa9cf93-8a77-4710-88dd-b99688c3b4ef,An Extreme-Adaptive Time Series Prediction Model Based on Probability-Enhanced LSTM Neural Networks,3,0.0266252,0.277467,"Forecasting time series with extreme events has been a challenging and
prevalent research topic, especially when the time series data are affected by
complicated uncertain factors, such as is the case in hydrologic prediction.
Diverse traditional and deep learning models have been applied to discover the
nonlinear relationships and recognize the complex patterns in these types of
data. However, existing methods usually ignore the negative influence of
imbalanced data, or severe events, on model training. Moreover, methods are
usually evaluated on a small number of generally well-behaved time series,
which does not show their ability to generalize. To tackle these issues, we
propose a novel probability-enhanced neural network model, called NEC+, which
concurrently learns extreme and normal prediction functions and a way to choose
among them via selective back propagation. We evaluate the proposed model on
the difficult 3-day ahead hourly water level prediction task applied to 9
reservoirs in California. Experimental results demonstrate that the proposed
model significantly outperforms state-of-the-art baselines and exhibits
superior generalization ability on data with diverse distributions.",1,1,0,0,1,0,0.102359,9.0,0.573867,43
44175900-888c-451d-9254-fc46a6ffae20,Ranking-Enhanced Unsupervised Sentence Representation Learning,9,0.0458928,0.708646,"Unsupervised sentence representation learning has progressed through
contrastive learning and data augmentation methods such as dropout masking.
Despite this progress, sentence encoders are still limited to using only an
input sentence when predicting its semantic vector. In this work, we show that
the semantic meaning of a sentence is also determined by nearest-neighbor
sentences that are similar to the input sentence. Based on this finding, we
propose a novel unsupervised sentence encoder, RankEncoder. RankEncoder
predicts the semantic vector of an input sentence by leveraging its
relationship with other sentences in an external corpus, as well as the input
sentence itself. We evaluate RankEncoder on semantic textual benchmark
datasets. From the experimental results, we verify that 1) RankEncoder achieves
80.07% Spearman's correlation, a 1.1% absolute improvement compared to the
previous state-of-the-art performance, 2) RankEncoder is universally applicable
to existing unsupervised sentence embedding methods, and 3) RankEncoder is
specifically effective for predicting the similarity scores of similar sentence
pairs.",1,0,0,0,1,0,0.410913,10.0,0.775414,42
6017263e-b40d-413c-a15d-d1a936810fb6,Linear Array Network for Low-light Image Enhancement,3,0.0634298,0.163402,"Convolution neural networks (CNNs) based methods have dominated the low-light
image enhancement tasks due to their outstanding performance. However, the
convolution operation is based on a local sliding window mechanism, which is
difficult to construct the long-range dependencies of the feature maps.
Meanwhile, the self-attention based global relationship aggregation methods
have been widely used in computer vision, but these methods are difficult to
handle high-resolution images because of the high computational complexity. To
solve this problem, this paper proposes a Linear Array Self-attention (LASA)
mechanism, which uses only two 2-D feature encodings to construct 3-D global
weights and then refines feature maps generated by convolution layers. Based on
LASA, Linear Array Network (LAN) is proposed, which is superior to the existing
state-of-the-art (SOTA) methods in both RGB and RAW based low-light enhancement
tasks with a smaller amount of parameters. The code is released in
https://github.com/cuiziteng/LASA_enhancement.",1,1,0,0,1,0,0.953391,6.0,0.918487,38
65f5fb87-9c57-4464-bc0d-6c3dc3cb81c7,PEANUT: Predicting and Navigating to Unseen Targets,7,0.290299,0.504914,"Efficient ObjectGoal navigation (ObjectNav) in novel environments requires an
understanding of the spatial and semantic regularities in environment layouts.
In this work, we present a straightforward method for learning these
regularities by predicting the locations of unobserved objects from incomplete
semantic maps. Our method differs from previous prediction-based navigation
methods, such as frontier potential prediction or egocentric map completion, by
directly predicting unseen targets while leveraging the global context from all
previously explored areas. Our prediction model is lightweight and can be
trained in a supervised manner using a relatively small amount of passively
collected data. Once trained, the model can be incorporated into a modular
pipeline for ObjectNav without the need for any reinforcement learning. We
validate the effectiveness of our method on the HM3D and MP3D ObjectNav
datasets. We find that it achieves the state-of-the-art on both datasets,
despite not using any additional data for training.",0,0,0,0,0,0,0.9552,6.0,0.920626,52
1ab90e46-6eed-4c5c-85f1-30c8f1c38891,Extreme Compression for Pre-trained Transformers Made Simple and Efficient,21,0.526936,0.69133,"Extreme compression, particularly ultra-low bit precision (binary/ternary)
quantization, has been proposed to fit large NLP models on resource-constraint
devices. However, to preserve the accuracy for such aggressive compression
schemes, cutting-edge methods usually introduce complicated compression
pipelines, e.g., multi-stage expensive knowledge distillation with extensive
hyperparameter tuning. Also, they oftentimes focus less on smaller transformer
models that have already been heavily compressed via knowledge distillation and
lack a systematic study to show the effectiveness of their methods. In this
paper, we perform a very comprehensive systematic study to measure the impact
of many key hyperparameters and training strategies from previous works. As a
result, we find out that previous baselines for ultra-low bit precision
quantization are significantly under-trained. Based on our study, we propose a
simple yet effective compression pipeline for extreme compression, named XTC.
XTC demonstrates that (1) we can skip the pre-training knowledge distillation
to obtain a 5-layer BERT while achieving better performance than previous
state-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization
plus layer reduction is able to reduce the model size by 50x, resulting in new
state-of-the-art results on GLUE tasks.",0,1,0,0,1,0,0.966615,6.0,0.935706,68
17b3d591-9b1c-4b9b-905c-2faba47f485b,Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis,16,0.230695,0.326677,"Existing studies on multimodal sentiment analysis heavily rely on textual
modality and unavoidably induce the spurious correlations between textual words
and sentiment labels. This greatly hinders the model generalization ability. To
address this problem, we define the task of out-of-distribution (OOD)
multimodal sentiment analysis. This task aims to estimate and mitigate the bad
effect of textual modality for strong OOD generalization. To this end, we
embrace causal inference, which inspects the causal relationships via a causal
graph. From the graph, we find that the spurious correlations are attributed to
the direct effect of textual modality on the model prediction while the
indirect one is more reliable by considering multimodal semantics. Inspired by
this, we devise a model-agnostic counterfactual framework for multimodal
sentiment analysis, which captures the direct effect of textual modality via an
extra text model and estimates the indirect one by a multimodal model. During
the inference, we first estimate the direct effect by the counterfactual
inference, and then subtract it from the total effect of all modalities to
obtain the indirect effect for reliable prediction. Extensive experiments show
the superior effectiveness and generalization ability of our proposed
framework.",0,0,1,0,0,0,0.640193,5.0,0.682501,46
14d1856b-efb0-4fc5-96ad-fa912b72d4f3,Improving Subgraph Representation Learning via Multi-View Augmentation,2,0.00793949,0.0694903,"Subgraph representation learning based on Graph Neural Network (GNN) has
exhibited broad applications in scientific advancements, such as predictions of
molecular structure-property relationships and collective cellular function. In
particular, graph augmentation techniques have shown promising results in
improving graph-based and node-based classification tasks. Still, they have
rarely been explored in the existing GNN-based subgraph representation learning
studies. In this study, we develop a novel multi-view augmentation mechanism to
improve subgraph representation learning models and thus the accuracy of
downstream prediction tasks. Our augmentation technique creates multiple
variants of subgraphs and embeds these variants into the original graph to
achieve highly improved training efficiency, scalability, and accuracy.
Benchmark experiments on several real-world biological and physiological
datasets demonstrate the superiority of our proposed multi-view augmentation
techniques in subgraph representation learning.",0,1,0,0,0,0,0.535493,5.0,0.625001,30
ad70d2cd-3e4c-4737-9fc0-d9861eca2c2b,Generative Design Ideation: A Natural Language Generation Approach,11,0.0698278,0.332314,"This paper aims to explore a generative approach for knowledge-based design
ideation by applying the latest pre-trained language models in artificial
intelligence (AI). Specifically, a method of fine-tuning the generative
pre-trained transformer using the USPTO patent database is proposed. The
AI-generated ideas are not only in concise and understandable language but also
able to synthesize the target design with external knowledge sources with
controllable knowledge distance. The method is tested in a case study of
rolling toy design and the results show good performance in generating ideas of
varied novelty with near-field and far-field source knowledge.",0,0,0,0,0,0,0.235777,5.0,0.415426,15
f20e08f2-30eb-474e-b13e-49b98f1d5931,Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers,2,0.0236454,0.0466255,"This paper presents ReasonFormer, a unified reasoning framework for mirroring
the modular and compositional reasoning process of humans in complex
decision-making. Inspired by dual-process theory in cognitive science, the
representation module (automatic thinking) and reasoning modules (controlled
thinking) are decoupled to capture different levels of cognition. Upon the top
of the representation module, the pre-trained reasoning modules are modular and
professional in specific and fundamental reasoning skills (e.g., logic, simple
QA, etc). To mimic the controlled compositional thinking process, different
reasoning modules are dynamically activated and composed in both parallel and
cascaded manners to control what reasoning skills are activated and how deep
the reasoning process will be reached to solve the current problems. The
unified reasoning framework solves multiple tasks with a single model, and is
trained and inferred in an end-to-end manner. Evaluated on 11 datasets
requiring different reasoning skills and complexity, ReasonFormer demonstrates
substantial performance boosts, revealing the compositional reasoning ability.
Few-shot experiments exhibit better generalization ability by learning to
compose pre-trained skills for new tasks with limited data, and decoupling the
representation module and the reasoning modules. Further analysis shows the
modularity of reasoning modules as different tasks activate distinct reasoning
skills at different reasoning depths.",0,0,0,0,0,0,0.693135,6.0,0.759536,54
ea5187eb-18fe-4cc7-b90f-3d7842a49bbb,RankGen: Improving Text Generation with Large Ranking Models,50,0.190135,0.585302,"Given an input sequence (or prefix), modern language models often assign high
probabilities to output sequences that are repetitive, incoherent, or
irrelevant to the prefix; as such, model-generated text also contains such
artifacts. To address these issues we present RankGen, a 1.2B parameter encoder
model for English that scores model generations given a prefix. RankGen can be
flexibly incorporated as a scoring function in beam search and used to decode
from any pretrained language model. We train RankGen using large-scale
contrastive learning to map a prefix close to the ground-truth sequence that
follows it and far away from two types of negatives: (1) random sequences from
the same document as the prefix, and (2) sequences generated from a large
language model conditioned on the prefix. Experiments across four different
language models (345M-11B parameters) and two domains show that RankGen
significantly outperforms decoding algorithms like nucleus, top-k, and typical
sampling, as well as contrastive decoding and search, on both automatic metrics
(85.0 vs 77.3 MAUVE over nucleus) as well as human evaluations with English
writers (74.5% human preference over nucleus sampling). Analysis reveals that
RankGen outputs are more relevant to the prefix and improve continuity and
coherence compared to baselines. We release our model checkpoints, code, and
human preference data with explanations to facilitate future research.",0,1,0,0,1,1,0.421789,6.0,0.631458,137
bf427bf7-5fd2-4854-834c-4acb4c104a19,Is a Modular Architecture Enough?,26,0.107199,0.805171,"Inspired from human cognition, machine learning systems are gradually
revealing advantages of sparser and more modular architectures. Recent work
demonstrates that not only do some modular architectures generalize well, but
they also lead to better out-of-distribution generalization, scaling
properties, learning speed, and interpretability. A key intuition behind the
success of such systems is that the data generating system for most real-world
settings is considered to consist of sparsely interacting parts, and endowing
models with similar inductive biases will be helpful. However, the field has
been lacking in a rigorous quantitative assessment of such systems because
these real-world data distributions are complex and unknown. In this work, we
provide a thorough assessment of common modular architectures, through the lens
of simple and known modular data distributions. We highlight the benefits of
modularity and sparsity and reveal insights on the challenges faced while
optimizing modular systems. In doing so, we propose evaluation metrics that
highlight the benefits of modularity, the regimes in which these benefits are
substantial, as well as the sub-optimality of current end-to-end learned
modular systems as opposed to their claimed potential.",1,0,0,0,0,0,0.17003,9.0,0.634502,46
1d193ead-8453-47e6-8aeb-bafec4e58d7c,Zero-Shot Retrieval with Search Agents and Hybrid Environments,5,0.0387312,0.667085,"Learning to search is the task of building artificial agents that learn to
autonomously use a search box to find information. So far, it has been shown
that current language models can learn symbolic query reformulation policies,
in combination with traditional term-based retrieval, but fall short of
outperforming neural retrievers. We extend the previous learning to search
setup to a hybrid environment, which accepts discrete query refinement
operations, after a first-pass retrieval step via a dual encoder. Experiments
on the BEIR task show that search agents, trained via behavioral cloning,
outperform the underlying search system based on a combined dual encoder
retriever and cross encoder reranker. Furthermore, we find that simple
heuristic Hybrid Retrieval Environments (HRE) can improve baseline performance
by several nDCG points. The search agent based on HRE (HARE) matches
state-of-the-art performance, balanced in both zero-shot and in-domain
evaluations, via interpretable actions, and at twice the speed.",0,1,0,0,0,0,0.597179,5.0,0.659088,49
1f5a7c2a-b950-4e1f-84a2-723723065cc0,"Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data",12,0.699774,0.77021,"Automatic player identification is an essential and complex task in sports
video analysis. Different strategies have been devised over the years, but
identification based on jersey numbers is one of the most common approaches
given its versatility and relative simplicity. However, automatic detection of
jersey numbers is still challenging due to changing camera angles, low video
resolution, small object size in wide-range shots and transient changes in the
player's posture and movement. In this paper we present a novel approach for
jersey number identification in a small, highly imbalanced dataset from the
Seattle Seahawks practice videos. Our results indicate that simple models can
achieve an acceptable performance on the jersey number detection task and that
synthetic data can improve the performance dramatically (accuracy increase of
~9% overall, ~18% on low frequency numbers) making our approach achieve state
of the art results.",0,1,0,1,1,0,0.924813,8.0,0.91767,30
23eff797-9495-4dea-aef0-74bcdbfc1a2c,PolQA: Polish Question Answering Dataset,3,0.00950708,0.172887,"Recently proposed systems for open-domain question answering (OpenQA) require
large amounts of training data to achieve state-of-the-art performance.
However, data annotation is known to be time-consuming and therefore expensive
to acquire. As a result, the appropriate datasets are available only for a
handful of languages (mainly English and Chinese). In this work, we introduce
and publicly release PolQA, the first Polish dataset for OpenQA. It consists of
7,000 questions, 87,525 manually labeled evidence passages, and a corpus of
over 7,097,322 candidate passages. Each question is classified according to its
formulation, type, as well as entity type of the answer. This resource allows
us to evaluate the impact of different annotation choices on the performance of
the QA system and propose an efficient annotation strategy that increases the
passage retrieval accuracy@10 by 10.55 p.p. while reducing the annotation cost
by 82%.",0,1,1,1,0,0,0.222595,7.0,0.573059,33
7c689472-8c9a-4982-95ef-57b6cbe7bcad,Measuring and Improving the Use of Graph Information in Graph Neural Networks,119,0.222768,0.709562,"Graph neural networks (GNNs) have been widely used for representation
learning on graph data. However, there is limited understanding on how much
performance GNNs actually gain from graph data. This paper introduces a
context-surrounding GNN framework and proposes two smoothness metrics to
measure the quantity and quality of information obtained from graph data. A new
GNN model, called CS-GNN, is then designed to improve the use of graph
information based on the smoothness values of a graph. CS-GNN is shown to
achieve better performance than existing methods in different types of real
graphs.",0,1,0,0,0,0,0.547574,8.0,0.769849,69
a16831fc-8e81-470c-8ab3-9c7e789b5313,Understanding the Energy Consumption of HPC Scale Artificial Intelligence,2,0.00590763,0.11778,"This paper contributes towards better understanding the energy consumption
trade-offs of HPC scale Artificial Intelligence (AI), and more specifically
Deep Learning (DL) algorithms. For this task we developed benchmark-tracker, a
benchmark tool to evaluate the speed and energy consumption of DL algorithms in
HPC environments. We exploited hardware counters and Python libraries to
collect energy information through software, which enabled us to instrument a
known AI benchmark tool, and to evaluate the energy consumption of numerous DL
algorithms and models. Through an experimental campaign, we show a case example
of the potential of benchmark-tracker to measure the computing speed and the
energy consumption for training and inference DL algorithms, and also the
potential of Benchmark-Tracker to help better understanding the energy behavior
of DL algorithms in HPC platforms. This work is a step forward to better
understand the energy consumption of Deep Learning in HPC, and it also
contributes with a new tool to help HPC DL developers to better balance the HPC
infrastructure in terms of speed and energy consumption.",1,1,0,0,0,0,0.0400875,9.0,0.466029,16
a7185e4d-9e48-4752-acd9-b19a75905e21,UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue,11,0.15856,0.809392,"The goal-oriented document-grounded dialogue aims at responding to the user
query based on the dialogue context and supporting document. Existing studies
tackle this problem by decomposing it into two sub-tasks: knowledge
identification and response generation. However, such pipeline methods would
unavoidably suffer from the error propagation issue. This paper proposes to
unify these two sub-tasks via sequentially generating the grounding knowledge
and the response. We further develop a prompt-connected multi-task learning
strategy to model the characteristics and connections of different tasks and
introduce linear temperature scheduling to reduce the negative effect of
irrelevant document information. Experimental results demonstrate the
effectiveness of our framework.",0,1,0,0,0,0,0.608529,6.0,0.721065,26
910bcbe3-b454-424c-a1f0-19a1bfcd039f,Omni-Granular Ego-Semantic Propagation for Self-Supervised Graph Representation Learning,12,0.0622574,0.498948,"Unsupervised/self-supervised graph representation learning is critical for
downstream node- and graph-level classification tasks. Global structure of
graphs helps discriminating representations and existing methods mainly utilize
the global structure by imposing additional supervisions. However, their global
semantics are usually invariant for all nodes/graphs and they fail to
explicitly embed the global semantics to enrich the representations. In this
paper, we propose Omni-Granular Ego-Semantic Propagation for Self-Supervised
Graph Representation Learning (OEPG). Specifically, we introduce
instance-adaptive global-aware ego-semantic descriptors, leveraging the first-
and second-order feature differences between each node/graph and hierarchical
global clusters of the entire graph dataset. The descriptors can be explicitly
integrated into local graph convolution as new neighbor nodes. Besides, we
design an omni-granular normalization on the whole scales and hierarchies of
the ego-semantic to assign attentional weight to each descriptor from an
omni-granular perspective. Specialized pretext tasks and cross-iteration
momentum update are further developed for local-global mutual adaptation. In
downstream tasks, OEPG consistently achieves the best performance with a 2%~6%
accuracy gain on multiple datasets cross scales and domains. Notably, OEPG also
generalizes to quantity- and topology-imbalance scenarios.",0,0,0,0,1,0,0.791167,6.0,0.806538,71
7f835733-faf1-4f6d-b3a3-20009a360816,Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words,5,0.222612,0.205463,"Prompt-based fine-tuning for pre-trained models has proven effective for many
natural language processing tasks under few-shot settings in general domain.
However, tuning with prompt in biomedical domain has not been investigated
thoroughly. Biomedical words are often rare in general domain, but quite
ubiquitous in biomedical contexts, which dramatically deteriorates the
performance of pre-trained models on downstream biomedical applications even
after fine-tuning, especially in low-resource scenarios. We propose a simple
yet effective approach to helping models learn rare biomedical words during
tuning with prompt. Experimental results show that our method can achieve up to
6% improvement in biomedical natural language inference task without any extra
parameters or training steps using few-shot vanilla prompt settings.",1,1,0,0,0,0,0.980095,6.0,0.959305,37
2ee82528-d7b7-43bd-973b-1599c1bcb24d,Masked Face Inpainting Through Residual Attention UNet,3,0.0269564,0.147111,"Realistic image restoration with high texture areas such as removing face
masks is challenging. The state-of-the-art deep learning-based methods fail to
guarantee high-fidelity, cause training instability due to vanishing gradient
problems (e.g., weights are updated slightly in initial layers) and spatial
information loss. They also depend on intermediary stage such as segmentation
meaning require external mask. This paper proposes a blind mask face inpainting
method using residual attention UNet to remove the face mask and restore the
face with fine details while minimizing the gap with the ground truth face
structure. A residual block feeds info to the next layer and directly into the
layers about two hops away to solve the gradient vanishing problem. Besides,
the attention unit helps the model focus on the relevant mask region, reducing
resources and making the model faster. Extensive experiments on the publicly
available CelebA dataset show the feasibility and robustness of our proposed
model. Code is available at
\url{https://github.com/mdhosen/Mask-Face-Inpainting-Using-Residual-Attention-Unet}",1,1,0,0,0,0,0.390434,6.0,0.614564,15
0826fe71-6e7a-4d62-a4a8-d475eb2d0d36,Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance,28,0.269598,0.567207,"Denoising diffusion probabilistic models (DDPMs) are a recent family of
generative models that achieve state-of-the-art results. In order to obtain
class-conditional generation, it was suggested to guide the diffusion process
by gradients from a time-dependent classifier. While the idea is theoretically
sound, deep learning-based classifiers are infamously susceptible to
gradient-based adversarial attacks. Therefore, while traditional classifiers
may achieve good accuracy scores, their gradients are possibly unreliable and
might hinder the improvement of the generation results. Recent work discovered
that adversarially robust classifiers exhibit gradients that are aligned with
human perception, and these could better guide a generative process towards
semantically meaningful images. We utilize this observation by defining and
training a time-dependent adversarially robust classifier and use it as
guidance for a generative diffusion model. In experiments on the highly
challenging and diverse ImageNet dataset, our scheme introduces significantly
more intelligible intermediate gradients, better alignment with theoretical
findings, as well as improved generation results under several evaluation
metrics. Furthermore, we conduct an opinion survey whose findings indicate that
human raters prefer our method's results.",1,0,0,0,0,0,0.805663,6.0,0.814023,85
6056f1ef-6287-4fdb-b4f2-3ab0560f5c0c,Red-Teaming the Stable Diffusion Safety Filter,83,0.177464,0.785509,"Stable Diffusion is a recent open-source image generation model comparable to
proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with
a safety filter that aims to prevent generating explicit images. Unfortunately,
the filter is obfuscated and poorly documented. This makes it hard for users to
prevent misuse in their applications, and to understand the filter's
limitations and improve it. We first show that it is easy to generate
disturbing content that bypasses the safety filter. We then reverse-engineer
the filter and find that while it aims to prevent sexual content, it ignores
violence, gore, and other similarly disturbing content. Based on our analysis,
we argue safety measures in future model releases should strive to be fully
open and properly documented to stimulate security contributions from the
community.",0,1,0,0,0,0,0.946198,4.0,0.865746,18
6a79bdda-c4bf-4dd2-8673-30f82d6ef18e,From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,16,0.238747,0.756422,"Multi-hop question answering (QA) is a challenging task requiring QA systems
to perform complex reasoning over multiple documents and provide supporting
facts together with the exact answer. Existing works tend to utilize
graph-based reasoning and question decomposition to obtain the reasoning chain,
which inevitably introduces additional complexity and cumulative error to the
system. To address the above issue, we propose a simple yet effective novel
framework, From Easy to Hard (FE2H), to remove distracting information and
obtain better contextual representations for the multi-hop QA task. Inspired by
the iterative document selection process and the progressive learning custom of
humans, FE2H divides both the document selector and reader into two stages
following an easy-to-hard manner. Specifically, we first select the document
most relevant to the question and then utilize the question together with this
document to select other pertinent documents. As for the QA phase, our reader
is first trained on a single-hop QA dataset and then transferred into the
multi-hop QA task. We comprehensively evaluate our model on the popular
multi-hop QA benchmark HotpotQA. Experimental results demonstrate that our
method ourperforms all other methods in the leaderboard of HotpotQA (distractor
setting).",0,1,0,0,1,0,0.727258,7.0,0.807476,37
a8521e28-8b15-4901-afba-f6bd7904dd03,CLRNet: Cross Layer Refinement Network for Lane Detection,107,0.838252,0.980127,"Lane is critical in the vision navigation system of the intelligent vehicle.
Naturally, lane is a traffic sign with high-level semantics, whereas it owns
the specific local pattern which needs detailed low-level features to localize
accurately. Using different feature levels is of great importance for accurate
lane detection, but it is still under-explored. In this work, we present Cross
Layer Refinement Network (CLRNet) aiming at fully utilizing both high-level and
low-level features in lane detection. In particular, it first detects lanes
with high-level semantic features then performs refinement based on low-level
features. In this way, we can exploit more contextual information to detect
lanes while leveraging local detailed lane features to improve localization
accuracy. We present ROIGather to gather global context, which further enhances
the feature representation of lanes. In addition to our novel network design,
we introduce Line IoU loss which regresses the lane line as a whole unit to
improve the localization accuracy. Experiments demonstrate that the proposed
method greatly outperforms the state-of-the-art lane detection approaches.",0,1,0,0,1,0,0.937031,6.0,0.901274,36
2d5757a1-d9f5-47eb-ae83-c4f0baf9c9ba,Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale,15,0.0634954,0.522143,"We introduce Saga, a next-generation knowledge construction and serving
platform for powering knowledge-based applications at industrial scale. Saga
follows a hybrid batch-incremental design to continuously integrate billions of
facts about real-world entities and construct a central knowledge graph that
supports multiple production use cases with diverse requirements around data
freshness, accuracy, and availability. In this paper, we discuss the unique
challenges associated with knowledge graph construction at industrial scale,
and review the main components of Saga and how they address these challenges.
Finally, we share lessons-learned from a wide array of production use cases
powered by Saga.",0,1,0,0,0,0,0.0107334,14.0,0.56154,94
a3f20dd6-1a10-4573-84a7-e2efdeb34d0b,LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling,11,0.102325,0.373974,"Recent large-scale video-language pre-trained models have shown appealing
performance on various downstream tasks. However, the pre-training process is
computationally expensive due to the requirement of millions of video-text
pairs and the redundant data structure of each video. To mitigate these
problems, we propose LiteVL, which adapts a pre-trained image-language model
BLIP into a video-text model directly on downstream tasks, without heavy
pre-training. To enhance the temporal modeling lacking in the image-language
model, we propose to add temporal attention modules in the image encoder of
BLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also
propose a non-parametric pooling mechanism to adaptively reweight the
fine-grained video embedding conditioned on the text. Experimental results on
text-video retrieval and video question answering show that the proposed LiteVL
even outperforms previous video-language pre-trained models by a clear margin,
though without any video-language pre-training.",0,1,0,0,1,0,0.964958,5.0,0.919976,41
1b2b3667-18e9-49e8-8bc8-782718fe277e,Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection,6,0.0278878,0.222062,"Video anomaly detection is a core problem in vision. Correctly detecting and
identifying anomalous behaviors in pedestrians from video data will enable
safety-critical applications such as surveillance, activity monitoring, and
human-robot interaction. In this paper, we propose to leverage trajectory
localization and prediction for unsupervised pedestrian anomaly event
detection. Different than previous reconstruction-based approaches, our
proposed framework rely on the prediction errors of normal and abnormal
pedestrian trajectories to detect anomalies spatially and temporally. We
present experimental results on real-world benchmark datasets on varying
timescales and show that our proposed trajectory-predictor-based anomaly
detection pipeline is effective and efficient at identifying anomalous
activities of pedestrians in videos. Code will be made available at
https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.",1,1,0,0,0,0,0.314711,10.0,0.741747,32
8d578da3-7c67-4a69-9fb4-bc3aaa6b4013,Distillation-Resistant Watermarking for Model Protection in NLP,11,0.0713509,0.64474,"How can we protect the intellectual property of trained NLP models? Modern
NLP models are prone to stealing by querying and distilling from their publicly
exposed APIs. However, existing protection methods such as watermarking only
work for images but are not applicable to text. We propose
Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP
models from being stolen via distillation. DRW protects a model by injecting
watermarks into the victim's prediction probability corresponding to a secret
key and is able to detect such a key by probing a suspect model. We prove that
a protected model still retains the original accuracy within a certain bound.
We evaluate DRW on a diverse set of NLP tasks including text classification,
part-of-speech tagging, and named entity recognition. Experiments show that DRW
protects the original model and detects stealing suspects at 100% mean average
precision for all four tasks while the prior method fails on two.",1,1,0,0,1,0,0.498176,9.0,0.779865,30
0c3e981d-8794-4b76-a805-9cf0786fcb1b,GODEL: Large-Scale Pre-Training for Goal-Directed Dialog,57,0.344736,0.999988,"We introduce GODEL (Grounded Open Dialogue Language Model), a large
pre-trained language model for dialog. In contrast with earlier models such as
DialoGPT, GODEL leverages a new phase of grounded pre-training designed to
better support adapting GODEL to a wide range of downstream dialog tasks that
require information external to the current conversation (e.g., a database or
document) to produce good responses. Experiments against an array of benchmarks
that encompass task-oriented dialog, conversational QA, and grounded
open-domain dialog show that GODEL outperforms state-of-the-art pre-trained
dialog models in few-shot fine-tuning setups, in terms of both human and
automatic evaluation. A novel feature of our evaluation methodology is the
introduction of a notion of utility that assesses the usefulness of responses
(extrinsic evaluation) in addition to their communicative features (intrinsic
evaluation). We show that extrinsic evaluation offers improved inter-annotator
agreement and correlation with automated metrics. Code and data processing
scripts are publicly available.",0,1,0,0,1,0,0.81711,5.0,0.784105,38
e176bed9-bb12-4ec1-8558-26d5ca6a76b4,Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages,35,0.132442,0.912235,"Scaling multilingual representation learning beyond the hundred most frequent
languages is challenging, in particular to cover the long tail of low-resource
languages. A promising approach has been to train one-for-all multilingual
models capable of cross-lingual transfer, but these models often suffer from
insufficient capacity and interference between unrelated languages. Instead, we
move away from this approach and focus on training multiple language (family)
specific representations, but most prominently enable all languages to still be
encoded in the same representational space. To achieve this, we focus on
teacher-student training, allowing all encoders to be mutually compatible for
bitext mining, and enabling fast learning of new languages. We introduce a new
teacher-student training scheme which combines supervised and self-supervised
training, allowing encoders to take advantage of monolingual training data,
which is valuable in the low-resource setting.
  Our approach significantly outperforms the original LASER encoder. We study
very low-resource languages and handle 50 African languages, many of which are
not covered by any other model. For these languages, we train sentence
encoders, mine bitexts, and validate the bitexts by training NMT systems.",0,1,0,0,1,0,0.391065,5.0,0.537894,39
3b8f3a22-9832-4dad-a42c-cbb6cfeaeaa9,GBSVM: Granular-ball Support Vector Machine,8,0.470952,0.890357,"GBSVM (Granular-ball Support Vector Machine) is a significant attempt to
construct a classifier using the coarse-to-fine granularity of a granular-ball
as input, rather than a single data point. It is the first classifier whose
input contains no points. However, the existing model has some errors, and its
dual model has not been derived. As a result, the current algorithm cannot be
implemented or applied. To address these problems, this paper has fixed the
errors of the original model of the existing GBSVM, and derived its dual model.
Furthermore, a particle swarm optimization algorithm is designed to solve the
dual model. The sequential minimal optimization algorithm is also carefully
designed to solve the dual model. The solution is faster and more stable than
the particle swarm optimization based version. The experimental results on the
UCI benchmark datasets demonstrate that GBSVM has good robustness and
efficiency. All codes have been released in the open source library at
http://www.cquptshuyinxia.com/GBSVM.html or https://github.com/syxiaa/GBSVM.",1,1,0,0,0,0,0.782782,4.0,0.703443,64
30687daf-db87-42a7-a961-2ace5481751b,Vulnerability Prioritization: An Offensive Security Approach,2,0.0784264,0.133851,"Organizations struggle to handle sheer number of vulnerabilities in their
cloud environments. The de facto methodology used for prioritizing
vulnerabilities is to use Common Vulnerability Scoring System (CVSS). However,
CVSS has inherent limitations that makes it not ideal for prioritization. In
this work, we propose a new way of prioritizing vulnerabilities. Our approach
is inspired by how offensive security practitioners perform penetration
testing. We evaluate our approach with a real world case study for a large
client, and the accuracy of machine learning to automate the process end to
end.",0,1,0,0,0,0,0.25471,6.0,0.527721,11
ec1e976b-e43f-4ea3-adc4-26cd83e1dc5e,Autoencoding Video Latents for Adversarial Video Generation,2,0.0206888,0.168703,"Given the three dimensional complexity of a video signal, training a robust
and diverse GAN based video generative model is onerous due to large
stochasticity involved in data space. Learning disentangled representations of
the data help to improve robustness and provide control in the sampling
process. For video generation, there is a recent progress in this area by
considering motion and appearance as orthogonal information and designing
architectures that efficiently disentangle them. These approaches rely on
handcrafting architectures that impose structural priors on the generator to
decompose appearance and motion codes in the latent space. Inspired from the
recent advancements in the autoencoder based image generation, we present AVLAE
(Adversarial Video Latent AutoEncoder) which is a two stream latent autoencoder
where the video distribution is learned by adversarial training. In particular,
we propose to autoencode the motion and appearance latent vectors of the video
generator in the adversarial setting. We demonstrate that our approach learns
to disentangle motion and appearance codes even without the explicit structural
composition in the generator. Several experiments with qualitative and
quantitative results demonstrate the effectiveness of our method.",1,0,0,0,0,0,0.815431,10.0,0.891513,66
6b0e9330-8faf-4191-a7d6-e248657bc590,Learning Feynman Diagrams using Graph Neural Networks,1,0.00408945,0.0457752,"In the wake of the growing popularity of machine learning in particle
physics, this work finds a new application of geometric deep learning on
Feynman diagrams to make accurate and fast matrix element predictions with the
potential to be used in analysis of quantum field theory. This research uses
the graph attention layer which makes matrix element predictions to 1
significant figure accuracy above 90% of the time. Peak performance was
achieved in making predictions to 3 significant figure accuracy over 10% of the
time with less than 200 epochs of training, serving as a proof of concept on
which future works can build upon for better performance. Finally, a procedure
is suggested, to use the network to make advancements in quantum field theory
by constructing Feynman diagrams with effective particles that represent
non-perturbative calculations.",0,1,0,0,0,0,0.096515,10.0,0.610282,28
dd58f012-c842-41ce-9bf3-9a3cfa741292,Provable Defense against Backdoor Policies in Reinforcement Learning,10,0.254598,0.820057,"We propose a provable defense mechanism against backdoor policies in
reinforcement learning under subspace trigger assumption. A backdoor policy is
a security threat where an adversary publishes a seemingly well-behaved policy
which in fact allows hidden triggers. During deployment, the adversary can
modify observed states in a particular way to trigger unexpected actions and
harm the agent. We assume the agent does not have the resources to re-train a
good policy. Instead, our defense mechanism sanitizes the backdoor policy by
projecting observed states to a 'safe subspace', estimated from a small number
of interactions with a clean (non-triggered) environment. Our sanitized policy
achieves $\epsilon$ approximate optimality in the presence of triggers,
provided the number of clean interactions is $O\left(\frac{D}{(1-\gamma)^4
\epsilon^2}\right)$ where $\gamma$ is the discounting factor and $D$ is the
dimension of state space. Empirically, we show that our sanitization defense
performs well on two Atari game environments.",1,0,1,0,0,0,0.893722,8.0,0.899724,21
cf858836-ee75-473d-8fcd-5679f13dfd41,DePlot: One-shot visual language reasoning by plot-to-table translation,40,0.460764,0.531973,"Visual language such as charts and plots is ubiquitous in the human world.
Comprehending plots and charts requires strong reasoning skills. Prior
state-of-the-art (SOTA) models require at least tens of thousands of training
examples and their reasoning capabilities are still much limited, especially on
complex human-written queries. This paper presents the first one-shot solution
to visual language reasoning. We decompose the challenge of visual language
reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over
the translated text. The key in this method is a modality conversion module,
named as DePlot, which translates the image of a plot or chart to a linearized
table. The output of DePlot can then be directly used to prompt a pretrained
large language model (LLM), exploiting the few-shot reasoning capabilities of
LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing
unified task formats and metrics, and train DePlot end-to-end on this task.
DePlot can then be used off-the-shelf together with LLMs in a plug-and-play
fashion. Compared with a SOTA model finetuned on more than >28k data points,
DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over
finetuned SOTA on human-written queries from the task of chart QA.",0,1,0,1,1,0,0.951333,2.0,0.748367,50
fb0872d4-d914-4012-82be-da556b0e41a0,py-irt: A Scalable Item Response Theory Library for Python,5,0.066028,0.688516,"py-irt is a Python library for fitting Bayesian Item Response Theory (IRT)
models. py-irt estimates latent traits of subjects and items, making it
appropriate for use in IRT tasks as well as ideal-point models. py-irt is built
on top of the Pyro and PyTorch frameworks and uses GPU-accelerated training to
scale to large data sets. Code, documentation, and examples can be found at
https://github.com/nd-ball/py-irt. py-irt can be installed from the GitHub page
or the Python Package Index (PyPI).",0,1,0,0,0,0,0.0878155,12.0,0.666971,50
50eea3a8-9d62-43a3-a03b-4e2d60ae8b0e,SMUDLP: Self-Teaching Multi-Frame Unsupervised Endoscopic Depth Estimation with Learnable Patchmatch,2,0.0185492,0.15963,"Unsupervised monocular trained depth estimation models make use of adjacent
frames as a supervisory signal during the training phase. However, temporally
correlated frames are also available at inference time for many clinical
applications, e.g., surgical navigation. The vast majority of monocular systems
do not exploit this valuable signal that could be deployed to enhance the depth
estimates. Those that do, achieve only limited gains due to the unique
challenges in endoscopic scenes, such as low and homogeneous textures and
inter-frame brightness fluctuations. In this work, we present SMUDLP, a novel
and unsupervised paradigm for multi-frame monocular endoscopic depth
estimation. The SMUDLP integrates a learnable patchmatch module to adaptively
increase the discriminative ability in low-texture and homogeneous-texture
regions, and enforces cross-teaching and self-teaching consistencies to provide
efficacious regularizations towards brightness fluctuations. Our detailed
experiments on both SCARED and Hamlyn datasets indicate that the SMUDLP exceeds
state-of-the-art competitors by a large margin, including those that use single
or multiple frames at inference time. The source code and trained models will
be publicly available upon the acceptance.",0,1,0,0,1,0,0.348229,9.0,0.726897,36
1b96a8fb-f71d-411c-80b6-57ca51d18d03,STaR: Bootstrapping Reasoning With Reasoning,208,0.301879,0.9917,"Generating step-by-step ""chain-of-thought"" rationales improves language model
performance on complex reasoning tasks like mathematics or commonsense
question-answering. However, inducing language model rationale generation
currently requires either constructing massive rationale datasets or
sacrificing accuracy by using only few-shot inference. We propose a technique
to iteratively leverage a small number of rationale examples and a large
dataset without rationales, to bootstrap the ability to perform successively
more complex reasoning. This technique, the ""Self-Taught Reasoner"" (STaR),
relies on a simple loop: generate rationales to answer many questions, prompted
with a few rationale examples; if the generated answers are wrong, try again to
generate a rationale given the correct answer; fine-tune on all the rationales
that ultimately yielded correct answers; repeat. We show that STaR
significantly improves performance on multiple datasets compared to a model
fine-tuned to directly predict final answers, and performs comparably to
fine-tuning a 30$\times$ larger state-of-the-art language model on
CommensenseQA. Thus, STaR lets a model improve itself by learning from its own
generated reasoning.",0,0,0,1,0,1,0.738611,3.0,0.561511,57
6ce8fa29-e8b9-4e23-97fc-d5a54b694bc7,Impact of Tokenization on Language Models: An Analysis for Turkish,37,0.120402,0.561189,"Tokenization is an important text preprocessing step to prepare input tokens
for deep language models. WordPiece and BPE are de facto methods employed by
important models, such as BERT and GPT. However, the impact of tokenization can
be different for morphologically rich languages, such as Turkic languages,
where many words can be generated by adding prefixes and suffixes. We compare
five tokenizers at different granularity levels, i.e. their outputs vary from
smallest pieces of characters to the surface form of words, including a
Morphological-level tokenizer. We train these tokenizers and pretrain
medium-sized language models using RoBERTa pretraining procedure on the Turkish
split of the OSCAR corpus. We then fine-tune our models on six downstream
tasks. Our experiments, supported by statistical tests, reveal that
Morphological-level tokenizer has challenging performance with de facto
tokenizers. Furthermore, we find that increasing the vocabulary size improves
the performance of Morphological and Word-level tokenizers more than that of de
facto tokenizers. The ratio of the number of vocabulary parameters to the total
number of model parameters can be empirically chosen as 20% for de facto
tokenizers and 40% for other tokenizers to obtain a reasonable trade-off
between model size and performance.",1,0,0,0,0,0,0.118409,6.0,0.38655,68
976333ef-b10d-4332-bdc0-f07944ac1796,OPD: Single-view 3D Openable Part Detection,17,0.426349,0.470841,"We address the task of predicting what parts of an object can open and how
they move when they do so. The input is a single image of an object, and as
output we detect what parts of the object can open, and the motion parameters
describing the articulation of each openable part. To tackle this task, we
create two datasets of 3D objects: OPDSynth based on existing synthetic
objects, and OPDReal based on RGBD reconstructions of real objects. We then
design OPDRCNN, a neural architecture that detects openable parts and predicts
their motion parameters. Our experiments show that this is a challenging task
especially when considering generalization across object categories, and the
limited amount of information in a single image. Our architecture outperforms
baselines and prior work especially for RGB image inputs. Short video summary
at https://www.youtube.com/watch?v=P85iCaD0rfc",0,1,1,1,0,0,0.748021,8.0,0.838936,50
2ffcc61f-63e2-4446-98d1-31dac2ecc80e,"CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework Using CNN, Visual Transformer and Multilayer Perceptron",73,0.0497527,0.768205,"Cervical cancer is the seventh most common cancer among all the cancers
worldwide and the fourth most common cancer among women. Cervical cytopathology
image classification is an important method to diagnose cervical cancer. Manual
screening of cytopathology images is time-consuming and error-prone. The
emergence of the automatic computer-aided diagnosis system solves this problem.
This paper proposes a framework called CVM-Cervix based on deep learning to
perform cervical cell classification tasks. It can analyze pap slides quickly
and accurately. CVM-Cervix first proposes a Convolutional Neural Network module
and a Visual Transformer module for local and global feature extraction
respectively, then a Multilayer Perceptron module is designed to fuse the local
and global features for the final classification. Experimental results show the
effectiveness and potential of the proposed CVM-Cervix in the field of cervical
Pap smear image classification. In addition, according to the practical needs
of clinical work, we perform a lightweight post-processing to compress the
model.",0,1,0,0,0,0,0.15056,6.0,0.429609,44
7b047a22-d26d-4c2b-aef9-5b77ccaf3d67,Ray Priors through Reprojection: Improving Neural Radiance Fields for Novel View Extrapolation,22,0.192716,0.55764,"Neural Radiance Fields (NeRF) have emerged as a potent paradigm for
representing scenes and synthesizing photo-realistic images. A main limitation
of conventional NeRFs is that they often fail to produce high-quality
renderings under novel viewpoints that are significantly different from the
training viewpoints. In this paper, instead of exploiting few-shot image
synthesis, we study the novel view extrapolation setting that (1) the training
images can well describe an object, and (2) there is a notable discrepancy
between the training and test viewpoints' distributions. We present RapNeRF
(RAy Priors) as a solution. Our insight is that the inherent appearances of a
3D surface's arbitrary visible projections should be consistent. We thus
propose a random ray casting policy that allows training unseen views using
seen views. Furthermore, we show that a ray atlas pre-computed from the
observed rays' viewing directions could further enhance the rendering quality
for extrapolated views. A main limitation is that RapNeRF would remove the
strong view-dependent effects because it leverages the multi-view consistency
property.",0,1,0,0,0,0,0.94304,3.0,0.814425,53
e856c9bc-3321-4d2c-9255-af34ae856c14,BigBraveBN: algorithm of structural learning for bayesian networks with a large number of nodes,2,0.0121611,0.0918904,"Learning a Bayesian network is an NP-hard problem and with an increase in the
number of nodes, classical algorithms for learning the structure of Bayesian
networks become inefficient. In recent years, some methods and algorithms for
learning Bayesian networks with a high number of nodes (more than 50) were
developed. But these solutions have their disadvantages, for instance, they
only operate one type of data (discrete or continuous) or their algorithm has
been created to meet a specific nature of data (medical, social, etc.). The
article presents a BigBraveBN algorithm for learning large Bayesian Networks
with a high number of nodes (over 100). The algorithm utilizes the Brave
coefficient that measures the mutual occurrence of instances in several groups.
To form these groups, we use the method of nearest neighbours based on the
Mutual information (MI) measure. In the experimental part of the article, we
compare the performance of BigBraveBN to other existing solutions on multiple
data sets both discrete and continuous. The experimental part also represents
tests on real data. The aforementioned experimental results demonstrate the
efficiency of the BigBraveBN algorithm in structure learning of Bayesian
Networks.",1,1,0,0,0,0,0.00121945,12.0,0.306818,21
9c603c1c-f09b-450d-8cff-3bcf5e28b18a,Contextual road lane and symbol generation for autonomous driving,2,0.087187,0.025488,"In this paper we present a novel approach for lane detection and segmentation
using generative models. Traditionally discriminative models have been employed
to classify pixels semantically on a road. We model the probability
distribution of lanes and road symbols by training a generative adversarial
network. Based on the learned probability distribution, context-aware lanes and
road signs are generated for a given image which are further quantized for
nearest class label. Proposed method has been tested on BDD100K and Baidu
ApolloScape datasets and performs better than state of the art and exhibits
robustness to adverse conditions by generating lanes in faded out and occluded
scenarios.",0,0,0,0,1,0,0.892358,13.0,0.937853,31
978f0187-d03b-42c3-a378-c58ab0d8556e,Learning Control Admissibility Models with Graph Neural Networks for Multi-Agent Navigation,10,0.290108,0.862066,"Deep reinforcement learning in continuous domains focuses on learning control
policies that map states to distributions over actions that ideally concentrate
on the optimal choices in each step. In multi-agent navigation problems, the
optimal actions depend heavily on the agents' density. Their interaction
patterns grow exponentially with respect to such density, making it hard for
learning-based methods to generalize. We propose to switch the learning
objectives from predicting the optimal actions to predicting sets of admissible
actions, which we call control admissibility models (CAMs), such that they can
be easily composed and used for online inference for an arbitrary number of
agents. We design CAMs using graph neural networks and develop training methods
that optimize the CAMs in the standard model-free setting, with the additional
benefit of eliminating the need for reward engineering typically required to
balance collision avoidance and goal-reaching requirements. We evaluate the
proposed approach in multi-agent navigation environments. We show that the CAM
models can be trained in environments with only a few agents and be easily
composed for deployment in dense environments with hundreds of agents,
achieving better performance than state-of-the-art methods.",0,1,0,0,1,0,0.872475,11.0,0.919363,53
d93957a4-2256-48d2-8ab0-7da64cbfa65f,Instance-specific and Model-adaptive Supervision for Semi-supervised Semantic Segmentation,12,0.187087,0.692204,"Recently, semi-supervised semantic segmentation has achieved promising
performance with a small fraction of labeled data. However, most existing
studies treat all unlabeled data equally and barely consider the differences
and training difficulties among unlabeled instances. Differentiating unlabeled
instances can promote instance-specific supervision to adapt to the model's
evolution dynamically. In this paper, we emphasize the cruciality of instance
differences and propose an instance-specific and model-adaptive supervision for
semi-supervised semantic segmentation, named iMAS. Relying on the model's
performance, iMAS employs a class-weighted symmetric intersection-over-union to
evaluate quantitative hardness of each unlabeled instance and supervises the
training on unlabeled data in a model-adaptive manner. Specifically, iMAS
learns from unlabeled instances progressively by weighing their corresponding
consistency losses based on the evaluated hardness. Besides, iMAS dynamically
adjusts the augmentation for each instance such that the distortion degree of
augmented instances is adapted to the model's generalization capability across
the training course. Not integrating additional losses and training procedures,
iMAS can obtain remarkable performance gains against current state-of-the-art
approaches on segmentation benchmarks under different semi-supervised partition
protocols.",1,1,0,0,1,0,0.81195,7.0,0.84343,57
02fcecbe-8190-4270-bd6c-b54e46e3d919,ExPUNations: Augmenting Puns with Keywords and Explanations,6,0.0429746,0.283724,"The tasks of humor understanding and generation are challenging and
subjective even for humans, requiring commonsense and real-world knowledge to
master. Puns, in particular, add the challenge of fusing that knowledge with
the ability to interpret lexical-semantic ambiguity. In this paper, we present
the ExPUNations (ExPUN) dataset, in which we augment an existing dataset of
puns with detailed crowdsourced annotations of keywords denoting the most
distinctive words that make the text funny, pun explanations describing why the
text is funny, and fine-grained funniness ratings. This is the first humor
dataset with such extensive and fine-grained annotations specifically for puns.
Based on these annotations, we propose two tasks: explanation generation to aid
with pun classification and keyword-conditioned pun generation, to challenge
the current state-of-the-art natural language understanding and generation
models' ability to understand and generate humor. We showcase that the
annotated keywords we collect are helpful for generating better novel humorous
texts in human evaluation, and that our natural language explanations can be
leveraged to improve both the accuracy and robustness of humor classifiers.",0,0,1,1,0,0,0.111872,7.0,0.465556,52
14a2e81c-78ba-43e0-a154-6223cfcca236,Gaussian-Bernoulli RBMs Without Tears,10,0.458884,0.972855,"We revisit the challenging problem of training Gaussian-Bernoulli restricted
Boltzmann machines (GRBMs), introducing two innovations. We propose a novel
Gibbs-Langevin sampling algorithm that outperforms existing methods like Gibbs
sampling. We propose a modified contrastive divergence (CD) algorithm so that
one can generate images with GRBMs starting from noise. This enables direct
comparison of GRBMs with deep generative models, improving evaluation protocols
in the RBM literature. Moreover, we show that modified CD and gradient clipping
are enough to robustly train GRBMs with large learning rates, thus removing the
necessity of various tricks in the literature. Experiments on Gaussian
Mixtures, MNIST, FashionMNIST, and CelebA show GRBMs can generate good samples,
despite their single-hidden-layer architecture. Our code is released at:
\url{https://github.com/lrjconan/GRBM}.",1,1,0,0,0,0,0.512164,25.0,0.922359,44
c87317d8-5413-48c1-bdce-5a59c9a0044e,Low-Resource Multilingual and Zero-Shot Multispeaker TTS,10,0.367925,0.304947,"While neural methods for text-to-speech (TTS) have shown great advances in
modeling multiple speakers, even in zero-shot settings, the amount of data
needed for those approaches is generally not feasible for the vast majority of
the world's over 6,000 spoken languages. In this work, we bring together the
tasks of zero-shot voice cloning and multilingual low-resource TTS. Using the
language agnostic meta learning (LAML) procedure and modifications to a TTS
encoder, we show that it is possible for a system to learn speaking a new
language using just 5 minutes of training data while retaining the ability to
infer the voice of even unseen speakers in the newly learned language. We show
the success of our proposed approach in terms of intelligibility, naturalness
and similarity to target speaker using objective metrics as well as human
studies and provide our code and trained models open source.",0,1,0,0,0,0,0.761001,5.0,0.749828,56
4b717aab-ae10-4464-b056-009c91042b59,Controlling Styles in Neural Machine Translation with Activation Prompt,6,0.0258339,0.400829,"Controlling styles in neural machine translation (NMT) has attracted wide
attention, as it is crucial for enhancing user experience. Earlier studies on
this topic typically concentrate on regulating the level of formality and
achieve some progress in this area. However, they still encounter two major
challenges. The first is the difficulty in style evaluation. The style
comprises various aspects such as lexis, syntax, and others that provide
abundant information. Nevertheless, only formality has been thoroughly
investigated. The second challenge involves excessive dependence on incremental
adjustments, particularly when new styles are necessary. To address both
challenges, this paper presents a new benchmark and approach. A multiway
stylized machine translation (MSMT) benchmark is introduced, incorporating
diverse categories of styles across four linguistic domains. Then, we propose a
method named style activation prompt (StyleAP) by retrieving prompts from
stylized monolingual corpus, which does not require extra fine-tuning.
Experiments show that StyleAP could effectively control the style of
translation and achieve remarkable performance.",0,1,0,1,0,0,0.14638,7.0,0.506729,38
c190662f-1b3a-4458-a648-d83be6448bf0,Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers,13,0.292423,0.404382,"When a clinician refers a patient for an imaging exam, they include the
reason (e.g. relevant patient history, suspected disease) in the scan request;
this appears as the indication field in the radiology report. The
interpretation and reporting of the image are substantially influenced by this
request text, steering the radiologist to focus on particular aspects of the
image. We use the indication field to drive better image classification, by
taking a transformer network which is unimodally pre-trained on text (BERT) and
fine-tuning it for multimodal classification of a dual image-text input. We
evaluate the method on the MIMIC-CXR dataset, and present ablation studies to
investigate the effect of the indication field on the classification
performance. The experimental results show our approach achieves 87.8 average
micro AUROC, outperforming the state-of-the-art methods for unimodal (84.4) and
multimodal (86.0) classification. Our code is available at
https://github.com/jacenkow/mmbt.",1,1,0,0,1,0,0.519993,6.0,0.680209,24
716ceaa6-267c-4ef1-a5b6-a198b7902dd9,LingMess: Linguistically Informed Multi Expert Scorers for Coreference Resolution,15,0.267206,0.882429,"While coreference resolution typically involves various linguistic
challenges, recent models are based on a single pairwise scorer for all types
of pairs. We present LingMess, a new coreference model that defines different
categories of coreference cases and optimize multiple pairwise scorers, where
each scorer learns a specific set of linguistic challenges. Our model
substantially improves pairwise scores for most categories and outperforms
cluster-level performance on Ontonotes and 5 additional datasets. Our model is
available in https://github.com/shon-otmazgin/lingmess-coref",1,1,0,0,0,0,0.439913,9.0,0.760584,43
48e2af5f-b5c4-477f-b491-02dcb7189395,VALHALLA: Visual Hallucination for Machine Translation,20,0.327588,0.908053,"Designing better machine translation systems by considering auxiliary inputs
such as images has attracted much attention in recent years. While existing
methods show promising performance over the conventional text-only translation
systems, they typically require paired text and image as input during
inference, which limits their applicability to real-world scenarios. In this
paper, we introduce a visual hallucination framework, called VALHALLA, which
requires only source sentences at inference time and instead uses hallucinated
visual representations for multimodal machine translation. In particular, given
a source sentence an autoregressive hallucination transformer is used to
predict a discrete visual representation from the input text, and the combined
text and hallucinated representations are utilized to obtain the target
translation. We train the hallucination transformer jointly with the
translation transformer using standard backpropagation with cross-entropy
losses while being guided by an additional loss that encourages consistency
between predictions using either ground-truth or hallucinated visual
representations. Extensive experiments on three standard translation datasets
with a diverse set of language pairs demonstrate the effectiveness of our
approach over both text-only baselines and state-of-the-art methods. Project
page: http://www.svcl.ucsd.edu/projects/valhalla.",1,1,0,0,1,0,0.651206,7.0,0.777495,86
46d09fe6-c2cf-45a0-a2ea-33fc2a0c2e19,CellTranspose: Few-shot Domain Adaptation for Cellular Instance Segmentation,7,0.0620604,0.484522,"Automated cellular instance segmentation is a process utilized for
accelerating biological research for the past two decades, and recent
advancements have produced higher quality results with less effort from the
biologist. Most current endeavors focus on completely cutting the researcher
out of the picture by generating highly generalized models. However, these
models invariably fail when faced with novel data, distributed differently than
the ones used for training. Rather than approaching the problem with methods
that presume the availability of large amounts of target data and computing
power for retraining, in this work we address the even greater challenge of
designing an approach that requires minimal amounts of new annotated data as
well as training time. We do so by designing specialized contrastive losses
that leverage the few annotated samples very efficiently. A large set of
results show that 3 to 5 annotations lead to models with accuracy that: 1)
significantly mitigate the covariate shift effects; 2) matches or surpasses
other adaptation methods; 3) even approaches methods that have been fully
retrained on the target distribution. The adaptation training is only a few
minutes, paving a path towards a balance between model performance, computing
requirements and expert-level annotation needs.",0,1,0,0,0,0,0.335156,7.0,0.642082,66
1ed62e7c-7c03-480e-bd68-8f7e1f3367b2,OntoSeer -- A Recommendation System to Improve the Quality of Ontologies,2,0.0112655,0.285999,"Building an ontology is not only a time-consuming process, but it is also
confusing, especially for beginners and the inexperienced. Although ontology
developers can take the help of domain experts in building an ontology, they
are not readily available in several cases for a variety of reasons. Ontology
developers have to grapple with several questions related to the choice of
classes, properties, and the axioms that should be included. Apart from this,
there are aspects such as modularity and reusability that should be taken care
of. From among the thousands of publicly available ontologies and vocabularies
in repositories such as Linked Open Vocabularies (LOV) and BioPortal, it is
hard to know the terms (classes and properties) that can be reused in the
development of an ontology. A similar problem exists in implementing the right
set of ontology design patterns (ODPs) from among the several available.
Generally, ontology developers make use of their experience in handling these
issues, and the inexperienced ones have a hard time. In order to bridge this
gap, we propose a tool named OntoSeer, that monitors the ontology development
process and provides suggestions in real-time to improve the quality of the
ontology under development. It can provide suggestions on the naming
conventions to follow, vocabulary to reuse, ODPs to implement, and axioms to be
added to the ontology. OntoSeer has been implemented as a Prot\'eg\'e plug-in.",0,1,0,0,0,0,0.00229137,16.0,0.519569,24
1588b47a-2f59-4941-8dbd-24a261641e3d,The Stack: 3 TB of permissively licensed source code,153,0.402506,0.999445,"Large Language Models (LLMs) play an ever-increasing role in the field of
Artificial Intelligence (AI)--not only for natural language processing but also
for code understanding and generation. To stimulate open and responsible
research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting
of permissively licensed source code in 30 programming languages. We describe
how we collect the full dataset, construct a permissively licensed subset,
present a data governance plan, discuss limitations, and show promising results
on text2code benchmarks by training 350M-parameter decoders on different Python
subsets. We find that (1) near-deduplicating the data significantly boosts
performance across all experiments, and (2) it is possible to match previously
reported HumanEval and MBPP performance using only permissively licensed data.
We make the dataset available at https://hf.co/BigCode, provide a tool called
""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers
to search The Stack for copies of their code, and provide a process for code to
be removed from the dataset by following the instructions at
https://www.bigcode-project.org/docs/about/the-stack/.",0,1,1,1,0,0,0.791883,3.0,0.613806,48
2e450d49-d3b4-4698-93de-0e1e4cc51250,GazeOnce: Real-Time Multi-Person Gaze Estimation,18,0.309016,0.904979,"Appearance-based gaze estimation aims to predict the 3D eye gaze direction
from a single image. While recent deep learning-based approaches have
demonstrated excellent performance, they usually assume one calibrated face in
each input image and cannot output multi-person gaze in real time. However,
simultaneous gaze estimation for multiple people in the wild is necessary for
real-world applications. In this paper, we propose the first one-stage
end-to-end gaze estimation method, GazeOnce, which is capable of simultaneously
predicting gaze directions for multiple faces (>10) in an image. In addition,
we design a sophisticated data generation pipeline and propose a new dataset,
MPSGaze, which contains full images of multiple people with 3D gaze ground
truth. Experimental results demonstrate that our unified framework not only
offers a faster speed, but also provides a lower gaze estimation error compared
with state-of-the-art methods. This technique can be useful in real-time
applications with multiple users.",0,1,1,1,1,0,0.735047,8.0,0.834299,32
fe96519a-b853-4ebd-aab6-ae17764156f5,EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL,13,0.421549,0.458687,"Reinforcement learning (RL) in long horizon and sparse reward tasks is
notoriously difficult and requires a lot of training steps. A standard solution
to speed up the process is to leverage additional reward signals, shaping it to
better guide the learning process. In the context of language-conditioned RL,
the abstraction and generalisation properties of the language input provide
opportunities for more efficient ways of shaping the reward. In this paper, we
leverage this idea and propose an automated reward shaping method where the
agent extracts auxiliary objectives from the general language goal. These
auxiliary objectives use a question generation (QG) and question answering (QA)
system: they consist of questions leading the agent to try to reconstruct
partial information about the global goal using its own trajectory. When it
succeeds, it receives an intrinsic reward proportional to its confidence in its
answer. This incentivizes the agent to generate trajectories which
unambiguously explain various aspects of the general language goal. Our
experimental study shows that this approach, which does not require engineer
intervention to design the auxiliary objectives, improves sample efficiency by
effectively directing exploration.",1,0,0,0,0,0,0.930746,7.0,0.910376,49
fead796c-918b-4911-9bda-759c651cb7eb,Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning,25,0.666646,0.589504,"The spread of rumors along with breaking events seriously hinders the truth
in the era of social media. Previous studies reveal that due to the lack of
annotated resources, rumors presented in minority languages are hard to be
detected. Furthermore, the unforeseen breaking events not involved in
yesterday's news exacerbate the scarcity of data resources. In this work, we
propose a novel zero-shot framework based on prompt learning to detect rumors
falling in different domains or presented in different languages. More
specifically, we firstly represent rumor circulated on social media as diverse
propagation threads, then design a hierarchical prompt encoding mechanism to
learn language-agnostic contextual representations for both prompts and rumor
data. To further enhance domain adaptation, we model the domain-invariant
structural features from the propagation threads, to incorporate structural
position representations of influential community response. In addition, a new
virtual response augmentation method is used to improve model training.
Extensive experiments conducted on three real-world datasets demonstrate that
our proposed model achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages.",0,1,1,0,1,0,0.899248,7.0,0.888762,61
a514b3af-a9ae-4ce3-93d6-9780ace33b43,Splicing ViT Features for Semantic Appearance Transfer,105,0.412481,0.990579,"We present a method for semantically transferring the visual appearance of
one natural image to another. Specifically, our goal is to generate an image in
which objects in a source structure image are ""painted"" with the visual
appearance of their semantically related objects in a target appearance image.
Our method works by training a generator given only a single
structure/appearance image pair as input. To integrate semantic information
into our framework - a pivotal component in tackling this task - our key idea
is to leverage a pre-trained and fixed Vision Transformer (ViT) model which
serves as an external semantic prior. Specifically, we derive novel
representations of structure and appearance extracted from deep ViT features,
untwisting them from the learned self-attention modules. We then establish an
objective function that splices the desired structure and appearance
representations, interweaving them together in the space of ViT features. Our
framework, which we term ""Splice"", does not involve adversarial training, nor
does it require any additional input information such as semantic segmentation
or correspondences, and can generate high-resolution results, e.g., work in HD.
We demonstrate high quality results on a variety of in-the-wild image pairs,
under significant variations in the number of objects, their pose and
appearance.",1,1,0,0,0,0,0.460583,10.0,0.790811,42
0934517c-4df3-4bee-8967-de8f7c410a64,Focal Sparse Convolutional Networks for 3D Object Detection,130,0.491257,0.999569,"Non-uniformed 3D sparse data, e.g., point clouds or voxels in different
spatial positions, make contribution to the task of 3D object detection in
different ways. Existing basic components in sparse convolutional networks
(Sparse CNNs) process all sparse data, regardless of regular or submanifold
sparse convolution. In this paper, we introduce two new modules to enhance the
capability of Sparse CNNs, both are based on making feature sparsity learnable
with position-wise importance prediction. They are focal sparse convolution
(Focals Conv) and its multi-modal variant of focal sparse convolution with
fusion, or Focals Conv-F for short. The new modules can readily substitute
their plain counterparts in existing Sparse CNNs and be jointly trained in an
end-to-end fashion. For the first time, we show that spatially learnable
sparsity in sparse convolution is essential for sophisticated 3D object
detection. Extensive experiments on the KITTI, nuScenes and Waymo benchmarks
validate the effectiveness of our approach. Without bells and whistles, our
results outperform all existing single-model entries on the nuScenes test
benchmark at the paper submission time. Code and models are at
https://github.com/dvlab-research/FocalsConv.",1,1,0,0,0,0,0.730035,6.0,0.776696,66
3fefd5a0-9095-414d-a62b-c3fd6718ef28,Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks,74,0.441991,0.999016,"The wide adoption and application of Masked language models~(MLMs) on
sensitive data (from legal to medical) necessitates a thorough quantitative
investigation into their privacy vulnerabilities -- to what extent do MLMs leak
information about their training data? Prior attempts at measuring leakage of
MLMs via membership inference attacks have been inconclusive, implying the
potential robustness of MLMs to privacy attacks. In this work, we posit that
prior attempts were inconclusive because they based their attack solely on the
MLM's model score. We devise a stronger membership inference attack based on
likelihood ratio hypothesis testing that involves an additional reference MLM
to more accurately quantify the privacy risks of memorization in MLMs. We show
that masked language models are extremely susceptible to likelihood ratio
membership inference attacks: Our empirical results, on models trained on
medical notes, show that our attack improves the AUC of prior membership
inference attacks from 0.66 to an alarmingly high 0.90 level, with a
significant improvement in the low-error region: at 1% false positive rate, our
attack is 51X more powerful than prior work.",0,1,0,0,0,0,0.613106,5.0,0.66777,47
9bb1bae4-7028-420d-963f-3c9c0394f7e6,On the Generalization of BasicVSR++ to Video Deblurring and Denoising,17,0.308969,0.298844,"The exploitation of long-term information has been a long-standing problem in
video restoration. The recent BasicVSR and BasicVSR++ have shown remarkable
performance in video super-resolution through long-term propagation and
effective alignment. Their success has led to a question of whether they can be
transferred to different video restoration tasks. In this work, we extend
BasicVSR++ to a generic framework for video restoration tasks. In tasks where
inputs and outputs possess identical spatial size, the input resolution is
reduced by strided convolutions to maintain efficiency. With only minimal
changes from BasicVSR++, the proposed framework achieves compelling performance
with great efficiency in various video restoration tasks including video
deblurring and denoising. Notably, BasicVSR++ achieves comparable performance
to Transformer-based approaches with up to 79% of parameter reduction and 44x
speedup. The promising results demonstrate the importance of propagation and
alignment in video restoration tasks beyond just video super-resolution. Code
and models are available at https://github.com/ckkelvinchan/BasicVSR_PlusPlus.",1,1,0,0,0,0,0.839701,6.0,0.832545,39
a2d90ec4-dae0-4690-980c-80c2d597b454,Multi-modal Multi-label Facial Action Unit Detection with Transformer,14,0.23493,0.904693,"Facial Action Coding System is an important approach of facial expression
analysis.This paper describes our submission to the third Affective Behavior
Analysis (ABAW) 2022 competition. We proposed a transfomer based model to
detect facial action unit (FAU) in video. To be specific, we firstly trained a
multi-modal model to extract both audio and visual feature. After that, we
proposed a action units correlation module to learn relationships between each
action unit labels and refine action unit detection result. Experimental
results on validation dataset shows that our method achieves better performance
than baseline model, which verifies that the effectiveness of proposed network.",0,1,0,0,0,0,0.892604,5.0,0.838623,16
79cb4499-14d3-457d-aed2-bb7cdefcd2f8,Unified Pretraining Framework for Document Understanding,71,0.308905,0.973794,"Document intelligence automates the extraction of information from documents
and supports many business applications. Recent self-supervised learning
methods on large-scale unlabeled document datasets have opened up promising
directions towards reducing annotation efforts by training models with
self-supervised objectives. However, most of the existing document pretraining
methods are still language-dominated. We present UDoc, a new unified
pretraining framework for document understanding. UDoc is designed to support
most document understanding tasks, extending the Transformer to take multimodal
embeddings as input. Each input element is composed of words and visual
features from a semantic region of the input document image. An important
feature of UDoc is that it learns a generic representation by making use of
three self-supervised losses, encouraging the representation to model
sentences, learn similarities, and align modalities. Extensive empirical
analysis demonstrates that the pretraining procedure learns better joint
representations and leads to improvements in downstream tasks.",0,1,0,0,0,0,0.662659,10.0,0.847367,34
eaa2d5df-eb79-4805-a8e0-e977899a48d7,Disentangling visual and written concepts in CLIP,25,0.158578,0.513664,"The CLIP network measures the similarity between natural text and images; in
this work, we investigate the entanglement of the representation of word images
and natural images in its image encoder. First, we find that the image encoder
has an ability to match word images with natural images of scenes described by
those words. This is consistent with previous research that suggests that the
meaning and the spelling of a word might be entangled deep within the network.
On the other hand, we also find that CLIP has a strong ability to match
nonsense words, suggesting that processing of letters is separated from
processing of their meaning. To explicitly determine whether the spelling
capability of CLIP is separable, we devise a procedure for identifying
representation subspaces that selectively isolate or eliminate spelling
capabilities. We benchmark our methods against a range of retrieval tasks, and
we also test them by measuring the appearance of text in CLIP-guided generated
images. We find that our methods are able to cleanly separate spelling
capabilities of CLIP from the visual processing of natural images.",0,0,0,0,0,0,0.922222,6.0,0.88803,26
6cd3aebd-b136-496d-93ef-723c9aa2bbba,Automatic Depression Detection: An Emotional Audio-Textual Corpus and a GRU/BiLSTM-based Model,34,0.69752,0.395149,"Depression is a global mental health problem, the worst case of which can
lead to suicide. An automatic depression detection system provides great help
in facilitating depression self-assessment and improving diagnostic accuracy.
In this work, we propose a novel depression detection approach utilizing speech
characteristics and linguistic contents from participants' interviews. In
addition, we establish an Emotional Audio-Textual Depression Corpus
(EATD-Corpus) which contains audios and extracted transcripts of responses from
depressed and non-depressed volunteers. To the best of our knowledge,
EATD-Corpus is the first and only public depression dataset that contains audio
and text data in Chinese. Evaluated on two depression datasets, the proposed
method achieves the state-of-the-art performances. The outperforming results
demonstrate the effectiveness and generalization ability of the proposed
method. The source code and EATD-Corpus are available at
https://github.com/speechandlanguageprocessing/ICASSP2022-Depression.",1,1,0,1,1,0,0.962294,11.0,0.961616,26
eae91e55-b5ac-4baf-b905-cdac176489d8,MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance Fields,5,0.0799377,0.218716,"Hinged on the representation power of neural networks, neural radiance fields
(NeRF) have recently emerged as one of the promising and widely applicable
methods for 3D object and scene representation. However, NeRF faces challenges
in practical applications, such as large-scale scenes and edge devices with a
limited amount of memory, where data needs to be processed sequentially. Under
such incremental learning scenarios, neural networks are known to suffer
catastrophic forgetting: easily forgetting previously seen data after training
with new data. We observe that previous incremental learning algorithms are
limited by either low performance or memory scalability issues. As such, we
develop a Memory-Efficient Incremental Learning algorithm for NeRF (MEIL-NeRF).
MEIL-NeRF takes inspiration from NeRF itself in that a neural network can serve
as a memory that provides the pixel RGB values, given rays as queries. Upon the
motivation, our framework learns which rays to query NeRF to extract previous
pixel values. The extracted pixel values are then used to train NeRF in a
self-distillation manner to prevent catastrophic forgetting. As a result,
MEIL-NeRF demonstrates constant memory consumption and competitive performance.",0,1,0,0,0,0,0.989249,5.0,0.98038,61
83b577ff-3b3c-4338-8e4e-00ea581eedb0,Skeleton-based Action Recognition via Temporal-Channel Aggregation,18,0.316459,0.883219,"Skeleton-based action recognition methods are limited by the semantic
extraction of spatio-temporal skeletal maps. However, current methods have
difficulty in effectively combining features from both temporal and spatial
graph dimensions and tend to be thick on one side and thin on the other. In
this paper, we propose a Temporal-Channel Aggregation Graph Convolutional
Networks (TCA-GCN) to learn spatial and temporal topologies dynamically and
efficiently aggregate topological features in different temporal and channel
dimensions for skeleton-based action recognition. We use the Temporal
Aggregation module to learn temporal dimensional features and the Channel
Aggregation module to efficiently combine spatial dynamic channel-wise
topological features with temporal dynamic topological features. In addition,
we extract multi-scale skeletal features on temporal modeling and fuse them
with an attention mechanism. Extensive experiments show that our model results
outperform state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and
NW-UCLA datasets.",0,1,0,0,1,0,0.915502,9.0,0.921689,67
32d8be1a-db0f-4fe1-b1b7-6fa5e15adb3f,SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection,131,0.629068,0.940169,"Convolutional neural networks (CNNs) are good at extracting contexture
features within certain receptive fields, while transformers can model the
global long-range dependency features. By absorbing the advantage of
transformer and the merit of CNN, Swin Transformer shows strong feature
representation ability. Based on it, we propose a cross-modality fusion model
SwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin
Transformer to extract the hierarchical features, boosted by attention
mechanism to bridge the gap between two modalities, and guided by edge
information to sharp the contour of salient object. To be specific, two-stream
Swin Transformer encoder first extracts multi-modality features, and then
spatial alignment and channel re-calibration module is presented to optimize
intra-level cross-modality features. To clarify the fuzzy boundary, edge-guided
decoder achieves inter-level cross-modality fusion under the guidance of edge
features. The proposed model outperforms the state-of-the-art models on RGB-D
and RGB-T datasets, showing that it provides more insight into the
cross-modality complementarity task.",1,1,0,0,1,0,0.575029,4.0,0.558692,97
992705e6-93e8-4a47-a85f-cd72b15b0185,Neural Transition-based Parsing of Library Deprecations,1,0.0,0.134048,"This paper tackles the challenging problem of automating code updates to fix
deprecated API usages of open source libraries by analyzing their release
notes. Our system employs a three-tier architecture: first, a web crawler
service retrieves deprecation documentation from the web; then a specially
built parser processes those text documents into tree-structured
representations; finally, a client IDE plugin locates and fixes identified
deprecated usages of libraries in a given codebase. The focus of this paper in
particular is the parsing component. We introduce a novel transition-based
parser in two variants: based on a classical feature engineered classifier and
a neural tree encoder. To confirm the effectiveness of our method, we gathered
and labeled a set of 426 API deprecations from 7 well-known Python data science
libraries, and demonstrated our approach decisively outperforms a non-trivial
neural machine translation baseline.",0,1,0,0,1,0,0.0338197,8.0,0.377625,26
c2c815cc-4005-4ec5-a9d7-711a85ff34b5,Capitalization Normalization for Language Modeling with an Accurate and Efficient Hierarchical RNN Model,5,0.014494,0.208077,"Capitalization normalization (truecasing) is the task of restoring the
correct case (uppercase or lowercase) of noisy text. We propose a fast,
accurate and compact two-level hierarchical word-and-character-based recurrent
neural network model. We use the truecaser to normalize user-generated text in
a Federated Learning framework for language modeling. A case-aware language
model trained on this normalized text achieves the same perplexity as a model
trained on text with gold capitalization. In a real user A/B experiment, we
demonstrate that the improvement translates to reduced prediction error rates
in a virtual keyboard application. Similarly, in an ASR language model fusion
experiment, we show reduction in uppercase character error rate and word error
rate.",0,1,0,0,0,0,0.00732404,10.0,0.347764,28
38d31900-02e2-46a8-8fba-2ab667a75c70,Tackling Background Distraction in Video Object Segmentation,21,0.136251,0.51607,"Semi-supervised video object segmentation (VOS) aims to densely track certain
designated objects in videos. One of the main challenges in this task is the
existence of background distractors that appear similar to the target objects.
We propose three novel strategies to suppress such distractors: 1) a
spatio-temporally diversified template construction scheme to obtain
generalized properties of the target objects; 2) a learnable distance-scoring
function to exclude spatially-distant distractors by exploiting the temporal
consistency between two consecutive frames; 3) swap-and-attach augmentation to
force each object to have unique features by providing training samples
containing entangled objects. On all public benchmark datasets, our model
achieves a comparable performance to contemporary state-of-the-art approaches,
even with real-time performance. Qualitative results also demonstrate the
superiority of our approach over existing methods. We believe our approach will
be widely used for future VOS research.",1,1,0,0,1,0,0.528771,5.0,0.621218,47
9c46af7f-1e02-4065-8d0a-d4b2b26015b2,Optimizing Partial Area Under the Top-k Curve: Theory and Practice,3,0.219756,0.356966,"Top-k error has become a popular metric for large-scale classification
benchmarks due to the inevitable semantic ambiguity among classes. Existing
literature on top-k optimization generally focuses on the optimization method
of the top-k objective, while ignoring the limitations of the metric itself. In
this paper, we point out that the top-k objective lacks enough discrimination
such that the induced predictions may give a totally irrelevant label a top
rank. To fix this issue, we develop a novel metric named partial Area Under the
top-k Curve (AUTKC). Theoretical analysis shows that AUTKC has a better
discrimination ability, and its Bayes optimal score function could give a
correct top-K ranking with respect to the conditional probability. This shows
that AUTKC does not allow irrelevant labels to appear in the top list.
Furthermore, we present an empirical surrogate risk minimization framework to
optimize the proposed metric. Theoretically, we present (1) a sufficient
condition for Fisher consistency of the Bayes optimal score function; (2) a
generalization upper bound which is insensitive to the number of classes under
a simple hyperparameter setting. Finally, the experimental results on four
benchmark datasets validate the effectiveness of our proposed framework.",0,0,0,0,0,0,0.805083,19.0,0.941175,89
5312f080-d74a-462b-aee4-4d7fbfece6e4,SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,20,0.78388,0.894775,"We present SpeechMatrix, a large-scale multilingual corpus of
speech-to-speech translations mined from real speech of European Parliament
recordings. It contains speech alignments in 136 language pairs with a total of
418 thousand hours of speech. To evaluate the quality of this parallel speech,
we train bilingual speech-to-speech translation models on mined data only and
establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test
sets. Enabled by the multilinguality of SpeechMatrix, we also explore
multilingual speech-to-speech translation, a topic which was addressed by few
other works. We also demonstrate that model pre-training and sparse scaling
using Mixture-of-Experts bring large gains to translation performance. The
mined data and models are freely available.",1,1,0,1,0,0,0.977505,5.0,0.94482,55
3ae62a9e-39fc-4602-b13b-9da4a0083584,An attention mechanism based convolutional network for satellite precipitation downscaling over China,14,0.249621,0.831177,"Precipitation is a key part of hydrological circulation and is a sensitive
indicator of climate change. The Integrated Multi-satellitE Retrievals for the
Global Precipitation Measurement (GPM) mission (IMERG) datasets are widely used
for global and regional precipitation investigations. However, their local
application is limited by the relatively coarse spatial resolution. Therefore,
in this paper, an attention mechanism based convolutional network (AMCN) is
proposed to downscale GPM IMERG monthly precipitation data. The proposed method
is an end-to-end network, which consists of a global cross-attention module, a
multi-factor cross-attention module, and a residual convolutional module,
comprehensively considering the potential relationships between precipitation
and complicated surface characteristics. In addition, a degradation loss
function based on low-resolution precipitation is designed to physically
constrain the network training, to improve the robustness of the proposed
network under different time and scale variations. The experiments demonstrate
that the proposed network significantly outperforms three baseline methods.
Finally, a geographic difference analysis method is introduced to further
improve the downscaled results by incorporating in-situ measurements for
high-quality and fine-scale precipitation estimation.",0,1,0,0,1,0,0.330119,7.0,0.639415,60
5c9b6ccf-c6cc-4a46-b3b8-56f7d77ac997,SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models,22,0.127359,0.58979,"Vision-language models such as CLIP are pretrained on large volumes of
internet sourced image and text pairs, and have been shown to sometimes exhibit
impressive zero- and low-shot image classification performance. However, due to
their size, fine-tuning these models on new datasets can be prohibitively
expensive, both in terms of the supervision and compute required. To combat
this, a series of light-weight adaptation methods have been proposed to
efficiently adapt such models when limited supervision is available. In this
work, we show that while effective on internet-style datasets, even those
remedies under-deliver on classification tasks with images that differ
significantly from those commonly found online. To address this issue, we
present a new approach called SVL-Adapter that combines the complementary
strengths of both vision-language pretraining and self-supervised
representation learning. We report an average classification accuracy
improvement of 10% in the low-shot setting when compared to existing methods,
on a set of challenging visual classification tasks. Further, we present a
fully automatic way of selecting an important blending hyperparameter for our
model that does not require any held-out labeled validation data. Code for our
project is available here: https://github.com/omipan/svl_adapter.",1,1,0,0,1,0,0.942518,6.0,0.906681,68
a0a6949d-aa9d-4142-be28-dc7e15468bc8,Worldwide city transport typology prediction with sentence-BERT based supervised learning via Wikipedia,6,0.0798386,0.649246,"An overwhelming majority of the world's human population lives in urban areas
and cities. Understanding a city's transportation typology is immensely
valuable for planners and policy makers whose decisions can potentially impact
millions of city residents. Despite the value of understanding a city's
typology, labeled data (city and it's typology) is scarce, and spans at most a
few hundred cities in the current transportation literature. To break this
barrier, we propose a supervised machine learning approach to predict a city's
typology given the information in its Wikipedia page. Our method leverages
recent breakthroughs in natural language processing, namely sentence-BERT, and
shows how the text-based information from Wikipedia can be effectively used as
a data source for city typology prediction tasks that can be applied to over
2000 cities worldwide. We propose a novel method for low-dimensional city
representation using a city's Wikipedia page, which makes supervised learning
of city typology labels tractable even with a few hundred labeled samples.
These features are used with labeled city samples to train binary classifiers
(logistic regression) for four different city typologies: (i) congestion, (ii)
auto-heavy, (iii) transit-heavy, and (iv) bike-friendly cities resulting in
reasonably high AUC scores of 0.87, 0.86, 0.61 and 0.94 respectively. Our
approach provides sufficient flexibility for incorporating additional variables
in the city typology models and can be applied to study other city typologies
as well. Our findings can assist a diverse group of stakeholders in
transportation and urban planning fields, and opens up new opportunities for
using text-based information from Wikipedia (or similar platforms) as data
sources in such fields.",0,1,0,0,0,0,0.0931331,15.0,0.737688,78
4dfc3cf2-3188-468a-bf95-787c2121f803,PriorLane: A Prior Knowledge Enhanced Lane Detection Approach Based on Transformer,7,0.101618,0.253747,"Lane detection is one of the fundamental modules in self-driving. In this
paper we employ a transformer-only method for lane detection, thus it could
benefit from the blooming development of fully vision transformer and achieve
the state-of-the-art (SOTA) performance on both CULane and TuSimple benchmarks,
by fine-tuning the weight fully pre-trained on large datasets. More
importantly, this paper proposes a novel and general framework called
PriorLane, which is used to enhance the segmentation performance of the fully
vision transformer by introducing the low-cost local prior knowledge.
Specifically, PriorLane utilizes an encoder-only transformer to fuse the
feature extracted by a pre-trained segmentation model with prior knowledge
embeddings. Note that a Knowledge Embedding Alignment (KEA) module is adapted
to enhance the fusion performance by aligning the knowledge embedding.
Extensive experiments on our Zjlab dataset show that PriorLane outperforms SOTA
lane detection methods by a 2.82% mIoU when prior knowledge is employed, and
the code will be released at: https://github.com/vincentqqb/PriorLane.",1,1,0,0,1,0,0.748048,5.0,0.742314,28
df860de0-870c-4f43-9b00-a33699d25380,Relighting4D: Neural Relightable Human from Videos,19,0.199654,0.559845,"Human relighting is a highly desirable yet challenging task. Existing works
either require expensive one-light-at-a-time (OLAT) captured data using light
stage or cannot freely change the viewpoints of the rendered body. In this
work, we propose a principled framework, Relighting4D, that enables
free-viewpoints relighting from only human videos under unknown illuminations.
Our key insight is that the space-time varying geometry and reflectance of the
human body can be decomposed as a set of neural fields of normal, occlusion,
diffuse, and specular maps. These neural fields are further integrated into
reflectance-aware physically based rendering, where each vertex in the neural
field absorbs and reflects the light from the environment. The whole framework
can be learned from videos in a self-supervised manner, with physically
informed priors designed for regularization. Extensive experiments on both real
and synthetic datasets demonstrate that our framework is capable of relighting
dynamic human actors with free-viewpoints.",0,0,1,0,0,0,0.817078,4.0,0.730105,65
5b917f6b-9457-408f-930b-65270d138b3a,Longtonotes: OntoNotes with Longer Coreference Chains,1,0.064147,0.0336702,"Ontonotes has served as the most important benchmark for coreference
resolution. However, for ease of annotation, several long documents in
Ontonotes were split into smaller parts. In this work, we build a corpus of
coreference-annotated documents of significantly longer length than what is
currently available. We do so by providing an accurate, manually-curated,
merging of annotations from documents that were split into multiple parts in
the original Ontonotes annotation process. The resulting corpus, which we call
LongtoNotes contains documents in multiple genres of the English language with
varying lengths, the longest of which are up to 8x the length of documents in
Ontonotes, and 2x those in Litbank. We evaluate state-of-the-art neural
coreference systems on this new corpus, analyze the relationships between model
architectures/hyperparameters and document length on performance and efficiency
of the models, and demonstrate areas of improvement in long-document
coreference modeling revealed by our new corpus. Our data and code is available
at: https://github.com/kumar-shridhar/LongtoNotes.",1,1,1,1,0,0,0.412122,10.0,0.775801,35
541ee0e1-c310-49d8-a59a-05e4b7ef3b5b,Proceedings of the 2nd International Workshop on Reading Music Systems,2,0.0349053,0.533495,"The International Workshop on Reading Music Systems (WoRMS) is a workshop
that tries to connect researchers who develop systems for reading music, such
as in the field of Optical Music Recognition, with other researchers and
practitioners that could benefit from such systems, like librarians or
musicologists.
  The relevant topics of interest for the workshop include, but are not limited
to: Music reading systems; Optical music recognition; Datasets and performance
evaluation; Image processing on music scores; Writer identification; Authoring,
editing, storing and presentation systems for music scores; Multi-modal
systems; Novel input-methods for music to produce written music; Web-based
Music Information Retrieval services; Applications and projects; Use-cases
related to written music.
  These are the proceedings of the 2nd International Workshop on Reading Music
Systems, held in Delft on the 2nd of November 2019.",0,1,0,0,0,0,0.0103714,15.0,0.588471,99
c2ea177c-47d2-4a8e-9e75-0f0bf7da5c48,Neural Network Compression of ACAS Xu Early Prototype is Unsafe: Closed-Loop Verification through Quantized State Backreachability,10,0.0433968,0.366395,"ACAS Xu is an air-to-air collision avoidance system designed for unmanned
aircraft that issues horizontal turn advisories to avoid an intruder aircraft.
Due the use of a large lookup table in the design, a neural network compression
of the policy was proposed. Analysis of this system has spurred a significant
body of research in the formal methods community on neural network
verification. While many powerful methods have been developed, most work
focuses on open-loop properties of the networks, rather than the main point of
the system -- collision avoidance -- which requires closed-loop analysis.
  In this work, we develop a technique to verify a closed-loop approximation of
the system using state quantization and backreachability. We use favorable
assumptions for the analysis -- perfect sensor information, instant following
of advisories, ideal aircraft maneuvers and an intruder that only flies
straight. When the method fails to prove the system is safe, we refine the
quantization parameters until generating counterexamples where the original
(non-quantized) system also has collisions.",1,0,0,0,0,0,0.0535417,7.0,0.355808,34
9c32738a-ccd1-4bb1-a9cc-337c4338b54b,The four-fifths rule is not disparate impact: a woeful tale of epistemic trespassing in algorithmic fairness,19,0.0995817,0.732937,"Computer scientists are trained to create abstractions that simplify and
generalize. However, a premature abstraction that omits crucial contextual
details creates the risk of epistemic trespassing, by falsely asserting its
relevance into other contexts. We study how the field of responsible AI has
created an imperfect synecdoche by abstracting the four-fifths rule (a.k.a. the
4/5 rule or 80% rule), a single part of disparate impact discrimination law,
into the disparate impact metric. This metric incorrectly introduces a new
deontic nuance and new potentials for ethical harms that were absent in the
original 4/5 rule. We also survey how the field has amplified the potential for
harm in codifying the 4/5 rule into popular AI fairness software toolkits. The
harmful erasure of legal nuances is a wake-up call for computer scientists to
self-critically re-evaluate the abstractions they create and use, particularly
in the interdisciplinary field of AI ethics.",0,0,0,0,0,0,0.062311,7.0,0.378134,69
03c561a0-a79b-43c0-946b-68623707cfd4,RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments,4,0.0422586,0.390983,"As a fundamental task for intelligent robots, visual SLAM has made great
progress over the past decades. However, robust SLAM under highly weak-textured
environments still remains very challenging. In this paper, we propose a novel
visual SLAM system named RWT-SLAM to tackle this problem. We modify LoFTR
network which is able to produce dense point matching under low-textured scenes
to generate feature descriptors. To integrate the new features into the popular
ORB-SLAM framework, we develop feature masks to filter out the unreliable
features and employ KNN strategy to strengthen the matching robustness. We also
retrained visual vocabulary upon new descriptors for efficient loop closing.
The resulting RWT-SLAM is tested in various public datasets such as TUM and
OpenLORIS, as well as our own data. The results shows very promising
performance under highly weak-textured environments.",0,1,0,0,0,0,0.813529,8.0,0.863631,40
11f7ca6f-4d62-4b8b-9682-503053ec0b18,Strategy Synthesis for Zero-Sum Neuro-Symbolic Concurrent Stochastic Games,8,0.426587,0.864665,"Neuro-symbolic approaches to artificial intelligence, which combine neural
networks with classical symbolic techniques, are growing in prominence,
necessitating formal approaches to reason about their correctness. We propose a
novel modelling formalism called neuro-symbolic concurrent stochastic games
(NS-CSGs), which comprise two probabilistic finite-state agents interacting in
a shared continuous-state environment. Each agent observes the environment
using a neural perception mechanism, which converts inputs such as images into
symbolic percepts, and makes decisions symbolically. We focus on the class of
NS-CSGs with Borel state spaces and prove the existence and measurability of
the value function for zero-sum discounted cumulative rewards under
piecewise-constant restrictions on the components of this class of models. To
compute values and synthesise strategies, we present, for the first time,
practical value iteration (VI) and policy iteration (PI) algorithms to solve
this new subclass of continuous-state CSGs. These require a finite
decomposition of the environment induced by the neural perception mechanisms of
the agents and rely on finite abstract representations of value functions and
strategies closed under VI or PI. First, we introduce a Borel measurable
piecewise-constant (B-PWC) representation of value functions, extend minimax
backups to this representation and propose a value iteration algorithm called
B-PWC VI. Second, we introduce two novel representations for the value
functions and strategies, constant-piecewise-linear (CON-PWL) and
constant-piecewise-constant (CON-PWC) respectively, and propose
Minimax-action-free PI by extending a recent PI method based on alternating
player choices for finite state spaces to Borel state spaces, which does not
require normal-form games to be solved.",0,0,1,0,0,0,0.564872,15.0,0.880452,57
421ec993-fc1a-4686-be80-4fd63d967f77,Weakly Supervised 3D Multi-person Pose Estimation for Large-scale Scenes based on Monocular Camera and Single LiDAR,15,0.34592,0.896511,"Depth estimation is usually ill-posed and ambiguous for monocular
camera-based 3D multi-person pose estimation. Since LiDAR can capture accurate
depth information in long-range scenes, it can benefit both the global
localization of individuals and the 3D pose estimation by providing rich
geometry features. Motivated by this, we propose a monocular camera and single
LiDAR-based method for 3D multi-person pose estimation in large-scale scenes,
which is easy to deploy and insensitive to light. Specifically, we design an
effective fusion strategy to take advantage of multi-modal input data,
including images and point cloud, and make full use of temporal information to
guide the network to learn natural and coherent human motions. Without relying
on any 3D pose annotations, our method exploits the inherent geometry
constraints of point cloud for self-supervision and utilizes 2D keypoints on
images for weak supervision. Extensive experiments on public datasets and our
newly collected dataset demonstrate the superiority and generalization
capability of our proposed method.",1,1,0,1,1,0,0.888104,6.0,0.862425,59
df372e27-e66e-4c4f-bfbe-434cd2dff790,Summarization as Indirect Supervision for Relation Extraction,39,0.39407,0.996651,"Relation extraction (RE) models have been challenged by their reliance on
training data with expensive annotations. Considering that summarization tasks
aim at acquiring concise expressions of synoptical information from the longer
context, these tasks naturally align with the objective of RE, i.e., extracting
a kind of synoptical information that describes the relation of entity
mentions. We present SuRE, which converts RE into a summarization formulation.
SuRE leads to more precise and resource-efficient RE based on indirect
supervision from summarization tasks. To achieve this goal, we develop sentence
and relation conversion techniques that essentially bridge the formulation of
summarization and RE tasks. We also incorporate constraint decoding techniques
with Trie scoring to further enhance summarization-based RE with robust
inference. Experiments on three RE datasets demonstrate the effectiveness of
SuRE in both full-dataset and low-resource settings, showing that summarization
is a promising source of indirect supervision to improve RE models.",1,0,0,0,0,0,0.871434,4.0,0.77726,59
a25d1ba6-fe86-4cd4-82f5-28b4535959c7,Assessing the trade-off between prediction accuracy and interpretability for topic modeling on energetic materials corpora,1,0.00133946,0.0438025,"As the amount and variety of energetics research increases, machine aware
topic identification is necessary to streamline future research pipelines. The
makeup of an automatic topic identification process consists of creating
document representations and performing classification. However, the
implementation of these processes on energetics research imposes new
challenges. Energetics datasets contain many scientific terms that are
necessary to understand the context of a document but may require more complex
document representations. Secondly, the predictions from classification must be
understandable and trusted by the chemists within the pipeline. In this work,
we study the trade-off between prediction accuracy and interpretability by
implementing three document embedding methods that vary in computational
complexity. With our accuracy results, we also introduce local interpretability
model-agnostic explanations (LIME) of each prediction to provide a localized
understanding of each prediction and to validate classifier decisions with our
team of energetics experts. This study was carried out on a novel labeled
energetics dataset created and validated by our team of energetics experts.",0,1,0,1,0,0,0.00625293,20.0,0.66595,12
1976db2a-b140-4f4d-95d9-dcc4288a1f03,Diverse Imagenet Models Transfer Better,9,0.015869,0.711708,"A commonly accepted hypothesis is that models with higher accuracy on
Imagenet perform better on other downstream tasks, leading to much research
dedicated to optimizing Imagenet accuracy. Recently this hypothesis has been
challenged by evidence showing that self-supervised models transfer better than
their supervised counterparts, despite their inferior Imagenet accuracy. This
calls for identifying the additional factors, on top of Imagenet accuracy, that
make models transferable. In this work we show that high diversity of the
features learnt by the model promotes transferability jointly with Imagenet
accuracy. Encouraged by the recent transferability results of self-supervised
models, we propose a method that combines self-supervised and supervised
pretraining to generate models with both high diversity and high accuracy, and
as a result high transferability. We demonstrate our results on several
architectures and multiple downstream tasks, including both single-label and
multi-label classification.",0,0,0,0,0,0,0.09402,13.0,0.698098,81
83531106-4d5e-42d8-8a1c-786d04f7c38c,FisheyeHDK: Hyperbolic Deformable Kernel Learning for Ultra-Wide Field-of-View Image Recognition,13,0.53096,0.793149,"Conventional convolution neural networks (CNNs) trained on narrow
Field-of-View (FoV) images are the state-of-the-art approaches for object
recognition tasks. Some methods proposed the adaptation of CNNs to ultra-wide
FoV images by learning deformable kernels. However, they are limited by the
Euclidean geometry and their accuracy degrades under strong distortions caused
by fisheye projections. In this work, we demonstrate that learning the shape of
convolution kernels in non-Euclidean spaces is better than existing deformable
kernel methods. In particular, we propose a new approach that learns deformable
kernel parameters (positions) in hyperbolic space. FisheyeHDK is a hybrid CNN
architecture combining hyperbolic and Euclidean convolution layers for
positions and features learning. First, we provide an intuition of hyperbolic
space for wide FoV images. Using synthetic distortion profiles, we demonstrate
the effectiveness of our approach. We select two datasets - Cityscapes and
BDD100K 2020 - of perspective images which we transform to fisheye equivalents
at different scaling factors (analog to focal lengths). Finally, we provide an
experiment on data collected by a real fisheye camera. Validations and
experiments show that our approach improves existing deformable kernel methods
for CNN adaptation on fisheye images.",0,0,0,0,0,0,0.941279,9.0,0.936954,20
2694bee1-f257-476a-8b71-47516e0ea4ad,A dynamic programming algorithm for span-based nested named-entity recognition in O(n^2),5,0.0575536,0.624836,"Span-based nested named-entity recognition (NER) has a cubic-time complexity
using a variant of the CYK algorithm. We show that by adding a supplementary
structural constraint on the search space, nested NER has a quadratic-time
complexity, that is the same asymptotic complexity than the non-nested case.
The proposed algorithm covers a large part of three standard English benchmarks
and delivers comparable experimental results.",0,0,0,0,0,0,0.0669235,14.0,0.694342,53
cccf5e9f-827a-4aae-a9a0-d96f180741e8,Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization,13,0.590221,0.599108,"The most advanced abstractive dialogue summarizers lack generalization
ability on new domains and the existing researches for domain adaptation in
summarization generally rely on large-scale pre-trainings. To explore the
lightweight fine-tuning methods for domain adaptation of dialogue
summarization, in this paper, we propose an efficient and generalizable
Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized
prefix module to alleviate domain entanglement and adopts discrete prompts to
guide the model to focus on key contents of dialogues and enhance model
generalization. We conduct zero-shot experiments and build domain adaptation
benchmarks on two multi-domain dialogue summarization datasets, TODSum and
QMSum. Adequate experiments and qualitative analysis prove the effectiveness of
our methods.",0,1,0,0,0,0,0.972353,5.0,0.933646,42
73f57456-239f-4336-9cd3-f6f63bdd1f17,Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models,13,0.152444,0.897133,"Machine learning based traffic forecasting models leverage sophisticated
spatiotemporal auto-correlations to provide accurate predictions of city-wide
traffic states. However, existing methods assume a reliable and unbiased
forecasting environment, which is not always available in the wild. In this
work, we investigate the vulnerability of spatiotemporal traffic forecasting
models and propose a practical adversarial spatiotemporal attack framework.
Specifically, instead of simultaneously attacking all geo-distributed data
sources, an iterative gradient-guided node saliency method is proposed to
identify the time-dependent set of victim nodes. Furthermore, we devise a
spatiotemporal gradient descent based scheme to generate real-valued
adversarial traffic states under a perturbation constraint. Meanwhile, we
theoretically demonstrate the worst performance bound of adversarial traffic
forecasting attacks. Extensive experiments on two real-world datasets show that
the proposed two-step framework achieves up to $67.8\%$ performance degradation
on various advanced spatiotemporal forecasting models. Remarkably, we also show
that adversarial training with our proposed attacks can significantly improve
the robustness of spatiotemporal traffic forecasting models. Our code is
available in \url{https://github.com/luckyfan-cs/ASTFA}.",1,1,0,0,0,0,0.68916,7.0,0.792323,33
d09d2e2c-4d43-468d-851d-1de465540b72,Prompt Distribution Learning,135,0.324483,0.943454,"We present prompt distribution learning for effectively adapting a
pre-trained vision-language model to address downstream recognition tasks. Our
method not only learns low-bias prompts from a few samples but also captures
the distribution of diverse prompts to handle the varying visual
representations. In this way, we provide high-quality task-related content for
facilitating recognition. This prompt distribution learning is realized by an
efficient approach that learns the output embeddings of prompts instead of the
input embeddings. Thus, we can employ a Gaussian distribution to model them
effectively and derive a surrogate loss for efficient training. Extensive
experiments on 12 datasets demonstrate that our method consistently and
significantly outperforms existing methods. For example, with 1 sample per
category, it relatively improves the average result by 9.1% compared to
human-crafted prompts.",0,1,0,0,1,0,0.818543,6.0,0.820858,52
4e1be1eb-0741-4bd6-b2f7-86d3ef45c7c4,The Glass Ceiling of Automatic Evaluation in Natural Language Generation,5,0.0133421,0.202771,"Automatic evaluation metrics capable of replacing human judgments are
critical to allowing fast development of new methods. Thus, numerous research
efforts have focused on crafting such metrics. In this work, we take a step
back and analyze recent progress by comparing the body of existing automatic
metrics and human metrics altogether. As metrics are used based on how they
rank systems, we compare metrics in the space of system rankings. Our extensive
statistical analysis reveals surprising findings: automatic metrics -- old and
new -- are much more similar to each other than to humans. Automatic metrics
are not complementary and rank systems similarly. Strikingly, human metrics
predict each other much better than the combination of all automatic metrics
used to predict a human metric. It is surprising because human metrics are
often designed to be independent, to capture different aspects of quality, e.g.
content fidelity or readability. We provide a discussion of these findings and
recommendations for future work in the field of evaluation.",0,0,0,0,0,1,0.0225482,9.0,0.401094,43
61187334-c75e-4f98-ad0e-8f625dc29e13,Optical Flow Based Motion Detection for Autonomous Driving,1,0.00437864,0.0924037,"Motion detection is a fundamental but challenging task for autonomous
driving. In particular scenes like highway, remote objects have to be paid
extra attention for better controlling decision. Aiming at distant vehicles, we
train a neural network model to classify the motion status using optical flow
field information as the input. The experiments result in high accuracy,
showing that our idea is viable and promising. The trained model also achieves
an acceptable performance for nearby vehicles. Our work is implemented in
PyTorch. Open tools including nuScenes, FastFlowNet and RAFT are used.
Visualization videos are available at
https://www.youtube.com/playlist?list=PLVVrWgq4OrlBnRebmkGZO1iDHEksMHKGk .",1,1,0,0,0,0,0.04661,7.0,0.335485,15
0ede90d7-6602-4f5a-bb8c-f53ac387cedc,SPBERTQA: A Two-Stage Question Answering System Based on Sentence Transformers for Medical Texts,8,0.0137307,0.220079,"Question answering (QA) systems have gained explosive attention in recent
years. However, QA tasks in Vietnamese do not have many datasets.
Significantly, there is mostly no dataset in the medical domain. Therefore, we
built a Vietnamese Healthcare Question Answering dataset (ViHealthQA),
including 10,015 question-answer passage pairs for this task, in which
questions from health-interested users were asked on prestigious health
websites and answers from highly qualified experts. This paper proposes a
two-stage QA system based on Sentence-BERT (SBERT) using multiple negatives
ranking (MNR) loss combined with BM25. Then, we conduct diverse experiments
with many bag-of-words models to assess our system's performance. With the
obtained results, this system achieves better performance than traditional
methods.",0,1,1,1,0,0,0.0363325,9.0,0.454885,26
0887b990-bbd7-47a8-b58e-31bec1a11747,"From Perception to Programs: Regularize, Overparameterize, and Amortize",6,0.0858965,0.558913,"Toward combining inductive reasoning with perception abilities, we develop
techniques for neurosymbolic program synthesis where perceptual input is first
parsed by neural nets into a low-dimensional interpretable representation,
which is then processed by a synthesized program. We explore several techniques
for relaxing the problem and jointly learning all modules end-to-end with
gradient descent: multitask learning; amortized inference;
overparameterization; and a differentiable strategy for penalizing lengthy
programs. Collectedly this toolbox improves the stability of gradient-guided
program search, and suggests ways of learning both how to perceive input as
discrete abstractions, and how to symbolically process those abstractions as
programs.",0,0,0,0,0,0,0.319646,10.0,0.743642,54
26d55242-afc7-4054-a6b0-e1f9f79f569b,Monotonic Differentiable Sorting Networks,14,0.369624,0.94116,"Differentiable sorting algorithms allow training with sorting and ranking
supervision, where only the ordering or ranking of samples is known. Various
methods have been proposed to address this challenge, ranging from optimal
transport-based differentiable Sinkhorn sorting algorithms to making classic
sorting networks differentiable. One problem of current differentiable sorting
methods is that they are non-monotonic. To address this issue, we propose a
novel relaxation of conditional swap operations that guarantees monotonicity in
differentiable sorting networks. We introduce a family of sigmoid functions and
prove that they produce differentiable sorting networks that are monotonic.
Monotonicity ensures that the gradients always have the correct sign, which is
an advantage in gradient-based optimization. We demonstrate that monotonic
differentiable sorting networks improve upon previous differentiable sorting
methods.",1,0,0,0,0,0,0.632034,9.0,0.821147,22
892b4b81-eb12-4d83-be20-3144b165e458,Batch Normalization Is Blind to the First and Second Derivatives of the Loss,2,0.0107828,0.121392,"In this paper, we prove the effects of the BN operation on the
back-propagation of the first and second derivatives of the loss. When we do
the Taylor series expansion of the loss function, we prove that the BN
operation will block the influence of the first-order term and most influence
of the second-order term of the loss. We also find that such a problem is
caused by the standardization phase of the BN operation. Experimental results
have verified our theoretical conclusions, and we have found that the BN
operation significantly affects feature representations in specific tasks,
where losses of different samples share similar analytic formulas.",0,0,0,0,0,0,0.492003,9.0,0.777877,46
9ee27393-3dd0-47c9-8d40-ecfaf922da80,Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis,31,0.245833,0.793283,"Recent literature focuses on utilizing the entity information in the
sentence-level relation extraction (RE), but this risks leaking superficial and
spurious clues of relations. As a result, RE still suffers from unintended
entity bias, i.e., the spurious correlation between entity mentions (names) and
relations. Entity bias can mislead the RE models to extract the relations that
do not exist in the text. To combat this issue, some previous work masks the
entity mentions to prevent the RE models from overfitting entity mentions.
However, this strategy degrades the RE performance because it loses the
semantic information of entities. In this paper, we propose the CORE
(Counterfactual Analysis based Relation Extraction) debiasing method that
guides the RE models to focus on the main effects of textual context without
losing the entity information. We first construct a causal graph for RE, which
models the dependencies between variables in RE models. Then, we propose to
conduct counterfactual analysis on our causal graph to distill and mitigate the
entity bias, that captures the causal effects of specific entity mentions in
each instance. Note that our CORE method is model-agnostic to debias existing
RE systems during inference without changing their training processes.
Extensive experimental results demonstrate that our CORE yields significant
gains on both effectiveness and generalization for RE. The source code is
provided at: https://github.com/vanoracai/CoRE.",1,0,0,0,0,0,0.56824,7.0,0.745153,47
cb1d5de3-7cf2-4cd6-ae32-c773b4bbe887,Towards End-to-End Open Conversational Machine Reading,2,0.0226946,0.426778,"In open-retrieval conversational machine reading (OR-CMR) task, machines are
required to do multi-turn question answering given dialogue history and a
textual knowledge base. Existing works generally utilize two independent
modules to approach this problem's two successive sub-tasks: first with a
hard-label decision making and second with a question generation aided by
various entailment reasoning methods. Such usual cascaded modeling is
vulnerable to error propagation and prevents the two sub-tasks from being
consistently optimized. In this work, we instead model OR-CMR as a unified
text-to-text task in a fully end-to-end style. Experiments on the OR-ShARC
dataset show the effectiveness of our proposed end-to-end framework on both
sub-tasks by a large margin, achieving new state-of-the-art results. Further
ablation studies support that our framework can generalize to different
backbone models.",0,0,0,0,1,0,0.239851,7.0,0.585259,35
3b7eed1d-0543-4f39-b4f2-a79835379820,Multi-Frames Temporal Abnormal Clues Learning Method for Face Anti-Spoofing,1,0.0269736,0.198274,"Face anti-spoofing researches are widely used in face recognition and has
received more attention from industry and academics. In this paper, we propose
the EulerNet, a new temporal feature fusion network in which the differential
filter and residual pyramid are used to extract and amplify abnormal clues from
continuous frames, respectively. A lightweight sample labeling method based on
face landmarks is designed to label large-scale samples at a lower cost and has
better results than other methods such as 3D camera. Finally, we collect 30,000
live and spoofing samples using various mobile ends to create a dataset that
replicates various forms of attacks in a real-world setting. Extensive
experiments on public OULU-NPU show that our algorithm is superior to the state
of art and our solution has already been deployed in real-world systems
servicing millions of users.",0,1,0,1,1,0,0.541295,9.0,0.793474,38
9501737a-b4b5-453e-be78-67224436afaf,Self-Supervision Can Be a Good Few-Shot Learner,19,0.248227,0.756771,"Existing few-shot learning (FSL) methods rely on training with a large
labeled dataset, which prevents them from leveraging abundant unlabeled data.
From an information-theoretic perspective, we propose an effective unsupervised
FSL method, learning representations with self-supervision. Following the
InfoMax principle, our method learns comprehensive representations by capturing
the intrinsic structure of the data. Specifically, we maximize the mutual
information (MI) of instances and their representations with a low-bias MI
estimator to perform self-supervised pre-training. Rather than supervised
pre-training focusing on the discriminable features of the seen classes, our
self-supervised model has less bias toward the seen classes, resulting in
better generalization for unseen classes. We explain that supervised
pre-training and self-supervised pre-training are actually maximizing different
MI objectives. Extensive experiments are further conducted to analyze their FSL
performance with various training settings. Surprisingly, the results show that
self-supervised pre-training can outperform supervised pre-training under the
appropriate conditions. Compared with state-of-the-art FSL methods, our
approach achieves comparable performance on widely used FSL benchmarks without
any labels of the base classes.",0,0,1,0,0,0,0.891671,6.0,0.864872,90
1f73aa40-5bca-4890-9c62-6611c79b3996,Aligned Weight Regularizers for Pruning Pretrained Neural Networks,2,0.0650085,0.143807,"While various avenues of research have been explored for iterative pruning,
little is known what effect pruning has on zero-shot test performance and its
potential implications on the choice of pruning criteria. This pruning setup is
particularly important for cross-lingual models that implicitly learn alignment
between language representations during pretraining, which if distorted via
pruning, not only leads to poorer performance on language data used for
retraining but also on zero-shot languages that are evaluated.
  In this work, we show that there is a clear performance discrepancy in
magnitude-based pruning when comparing standard supervised learning to the
zero-shot setting. From this finding, we propose two weight regularizers that
aim to maximize the alignment between units of pruned and unpruned networks to
mitigate alignment distortion in pruned cross-lingual models and perform well
for both non zero-shot and zero-shot settings.
  We provide experimental results on cross-lingual tasks for the zero-shot
setting using XLM-RoBERTa$_{\mathrm{Base}}$, where we also find that pruning
has varying degrees of representational degradation depending on the language
corresponding to the zero-shot test set. This is also the first study that
focuses on cross-lingual language model compression.",0,1,0,0,0,0,0.977368,10.0,0.972251,47
00fa8add-a0a5-42d2-9128-3402e170410c,DigNet: Digging Clues from Local-Global Interactive Graph for Aspect-level Sentiment Classification,11,0.0793,0.746621,"In aspect-level sentiment classification (ASC), state-of-the-art models
encode either syntax graph or relation graph to capture the local syntactic
information or global relational information. Despite the advantages of syntax
and relation graphs, they have respective shortages which are neglected,
limiting the representation power in the graph modeling process. To resolve
their limitations, we design a novel local-global interactive graph, which
marries their advantages by stitching the two graphs via interactive edges. To
model this local-global interactive graph, we propose a novel neural network
termed DigNet, whose core module is the stacked local-global interactive (LGI)
layers performing two processes: intra-graph message passing and cross-graph
message passing. In this way, the local syntactic and global relational
information can be reconciled as a whole in understanding the aspect-level
sentiment. Concretely, we design two variants of local-global interactive
graphs with different kinds of interactive edges and three variants of LGI
layers. We conduct experiments on several public benchmark datasets and the
results show that we outperform previous best scores by 3\%, 2.32\%, and 6.33\%
in terms of Macro-F1 on Lap14, Res14, and Res15 datasets, respectively,
confirming the effectiveness and superiority of the proposed local-global
interactive graph and DigNet.",0,0,0,0,1,0,0.658419,8.0,0.807764,35
9a8fb4b3-4606-4c93-9f93-4efd0e9b5da4,BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment,30,0.786749,0.731738,"This work addresses the Burst Super-Resolution (BurstSR) task using a new
architecture, which requires restoring a high-quality image from a sequence of
noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in
BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can
significantly improve the capability of extracting inter-frame information and
reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided
Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin
Transformer Blocks and Groups as our main backbone. More specifically, we
combine optical flows and deformable convolutions, hence our BSRT can handle
misalignment and aggregate the potential texture information in multi-frames
more efficiently. In addition, our Transformer-based structure can capture
long-range dependency to further improve the performance. The evaluation on
both synthetic and real-world tracks demonstrates that our approach achieves a
new state-of-the-art in BurstSR task. Further, our BSRT wins the championship
in the NTIRE2022 Burst Super-Resolution Challenge.",1,1,0,0,1,0,0.984065,8.0,0.976384,61
1faf2ecc-de48-426a-90cb-d81577c1e0cd,Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost,9,0.481945,0.772238,"State-of-the-art NLP systems represent inputs with word embeddings, but these
are brittle when faced with Out-of-Vocabulary (OOV) words. To address this
issue, we follow the principle of mimick-like models to generate vectors for
unseen words, by learning the behavior of pre-trained embeddings using only the
surface form of words. We present a simple contrastive learning framework,
LOVE, which extends the word representation of an existing pre-trained language
model (such as BERT), and makes it robust to OOV with few additional
parameters. Extensive evaluations demonstrate that our lightweight model
achieves similar or even better performances than prior competitors, both on
original datasets and on corrupted variants. Moreover, it can be used in a
plug-and-play fashion with FastText and BERT, where it significantly improves
their robustness.",1,1,0,0,1,0,0.970988,8.0,0.956837,71
f8edd77b-fdd6-41cf-91ad-e008f55ac441,Deep Learning for Hate Speech Detection: A Comparative Study,27,0.537907,0.919224,"Automated hate speech detection is an important tool in combating the spread
of hate speech, particularly in social media. Numerous methods have been
developed for the task, including a recent proliferation of deep-learning based
approaches. A variety of datasets have also been developed, exemplifying
various manifestations of the hate-speech detection problem. We present here a
large-scale empirical comparison of deep and shallow hate-speech detection
methods, mediated through the three most commonly used datasets. Our goal is to
illuminate progress in the area, and identify strengths and weaknesses in the
current state-of-the-art. We particularly focus our analysis on measures of
practical performance, including detection accuracy, computational efficiency,
capability in using pre-trained models, and domain generalization. In doing so
we aim to provide guidance as to the use of hate-speech detection in practice,
quantify the state-of-the-art, and identify future research directions. Code
and dataset are available at
https://github.com/jmjmalik22/Hate-Speech-Detection.",1,1,0,0,0,0,0.761147,8.0,0.843696,67
60ba9fd8-dfbc-46a0-8d53-37479a613a70,Neural Knowledge Bank for Pretrained Transformers,12,0.141831,0.483295,"The ability of pretrained Transformers to remember factual knowledge is
essential but still limited for existing models. Inspired by existing work that
regards Feed-Forward Networks (FFNs) in Transformers as key-value memories, we
design a Neural Knowledge Bank (NKB) and a knowledge injection strategy to
introduce extra factual knowledge for pretrained Transformers. The NKB is in
the form of additional knowledgeable memory slots to the FFN and the
memory-like architecture makes it highly interpretable and flexible. When
injecting extra knowledge with the Salient Span Masking (SSM) pretraining
objective, we fix the original pretrained model and train only the NKB. This
training strategy makes sure the general language modeling ability of the
original pretrained model is not influenced. By mounting the NKB onto the T5
model, we verify its strong ability to store extra factual knowledge based on
three closed-book question answering datasets. Also, we prove that mounting the
NKB will not degrade the general language modeling ability of T5 through two
representative tasks, summarization and machine translation. Further, we
thoroughly analyze the interpretability of the NKB and reveal the meaning of
its keys and values in a human-readable way. Finally, we show the flexibility
of the NKB by directly modifying its value vectors to update the factual
knowledge stored in it.",0,1,0,0,0,0,0.932374,6.0,0.896917,39
2950a477-20a2-49e9-b1e2-33734cda888f,Adapted Multimodal BERT with Layer-wise Fusion for Sentiment Analysis,2,0.0207337,0.156356,"Multimodal learning pipelines have benefited from the success of pretrained
language models. However, this comes at the cost of increased model parameters.
In this work, we propose Adapted Multimodal BERT (AMB), a BERT-based
architecture for multimodal tasks that uses a combination of adapter modules
and intermediate fusion layers. The adapter adjusts the pretrained language
model for the task at hand, while the fusion layers perform task-specific,
layer-wise fusion of audio-visual information with textual BERT
representations. During the adaptation process the pre-trained language model
parameters remain frozen, allowing for fast, parameter-efficient training. In
our ablations we see that this approach leads to efficient models, that can
outperform their fine-tuned counterparts and are robust to input noise. Our
experiments on sentiment analysis with CMU-MOSEI show that AMB outperforms the
current state-of-the-art across metrics, with 3.4% relative reduction in the
resulting error and 2.1% relative improvement in 7-class classification
accuracy.",0,1,0,0,1,0,0.785786,7.0,0.831836,37
7cabcc2e-b3d3-4871-8048-841961e0932c,A Local Optima Network Analysis of the Feedforward Neural Architecture Space,6,0.0,0.37027,"This study investigates the use of local optima network (LON) analysis, a
derivative of the fitness landscape of candidate solutions, to characterise and
visualise the neural architecture space. The search space of feedforward neural
network architectures with up to three layers, each with up to 10 neurons, is
fully enumerated by evaluating trained model performance on a selection of data
sets. Extracted LONs, while heterogeneous across data sets, all exhibit simple
global structures, with single global funnels in all cases but one. These
results yield early indication that LONs may provide a viable paradigm by which
to analyse and optimise neural architectures.",0,0,0,0,0,0,0.0100565,16.0,0.612255,25
4fc99a62-49d7-4dee-bf8e-db326e685527,Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking,8,0.242429,0.304535,"In dialogue state tracking, dialogue history is a crucial material, and its
utilization varies between different models. However, no matter how the
dialogue history is used, each existing model uses its own consistent dialogue
history during the entire state tracking process, regardless of which slot is
updated. Apparently, it requires different dialogue history to update different
slots in different turns. Therefore, using consistent dialogue contents may
lead to insufficient or redundant information for different slots, which
affects the overall performance. To address this problem, we devise DiCoS-DST
to dynamically select the relevant dialogue contents corresponding to each slot
for state updating. Specifically, it first retrieves turn-level utterances of
dialogue history and evaluates their relevance to the slot from a combination
of three perspectives: (1) its explicit connection to the slot name; (2) its
relevance to the current turn dialogue; (3) Implicit Mention Oriented
Reasoning. Then these perspectives are combined to yield a decision, and only
the selected dialogue contents are fed into State Generator, which explicitly
minimizes the distracting information passed to the downstream state
prediction. Experimental results show that our approach achieves new
state-of-the-art performance on MultiWOZ 2.1 and MultiWOZ 2.2, and achieves
superior performance on multiple mainstream benchmark datasets (including
Sim-M, Sim-R, and DSTC2).",0,1,0,0,1,0,0.879334,6.0,0.856582,36
399d97fd-771e-4e17-9d1c-5f32d8928330,Progressively Generating Better Initial Guesses Towards Next Stages for High-Quality Human Motion Prediction,67,0.640946,0.936725,"This paper presents a high-quality human motion prediction method that
accurately predicts future human poses given observed ones. Our method is based
on the observation that a good initial guess of the future poses is very
helpful in improving the forecasting accuracy. This motivates us to propose a
novel two-stage prediction framework, including an init-prediction network that
just computes the good guess and then a formal-prediction network that predicts
the target future poses based on the guess. More importantly, we extend this
idea further and design a multi-stage prediction framework where each stage
predicts initial guess for the next stage, which brings more performance gain.
To fulfill the prediction task at each stage, we propose a network comprising
Spatial Dense Graph Convolutional Networks (S-DGCN) and Temporal Dense Graph
Convolutional Networks (T-DGCN). Alternatively executing the two networks helps
extract spatiotemporal features over the global receptive field of the whole
pose sequence. All the above design choices cooperating together make our
method outperform previous approaches by large margins: 6%-7% on Human3.6M,
5%-10% on CMU-MoCap, and 13%-16% on 3DPW.",1,1,0,0,1,0,0.716245,7.0,0.803056,47
85fcaf62-3a48-4c53-97b8-973f6a0d8af6,SLiDE: Self-supervised LiDAR De-snowing through Reconstruction Difficulty,6,0.0665949,0.496823,"LiDAR is widely used to capture accurate 3D outdoor scene structures.
However, LiDAR produces many undesirable noise points in snowy weather, which
hamper analyzing meaningful 3D scene structures. Semantic segmentation with
snow labels would be a straightforward solution for removing them, but it
requires laborious point-wise annotation. To address this problem, we propose a
novel self-supervised learning framework for snow points removal in LiDAR point
clouds. Our method exploits the structural characteristic of the noise points:
low spatial correlation with their neighbors. Our method consists of two deep
neural networks: Point Reconstruction Network (PR-Net) reconstructs each point
from its neighbors; Reconstruction Difficulty Network (RD-Net) predicts
point-wise difficulty of the reconstruction by PR-Net, which we call
reconstruction difficulty. With simple post-processing, our method effectively
detects snow points without any label. Our method achieves the state-of-the-art
performance among label-free approaches and is comparable to the
fully-supervised method. Moreover, we demonstrate that our method can be
exploited as a pretext task to improve label-efficiency of supervised training
of de-snowing.",0,1,1,0,1,0,0.6912,8.0,0.818985,61
d4446ab2-4360-49d9-95fe-70f92f2d8590,Adversarial random forests for density estimation and generative modeling,6,0.029872,0.245852,"We propose methods for density estimation and data synthesis using a novel
form of unsupervised random forests. Inspired by generative adversarial
networks, we implement a recursive procedure in which trees gradually learn
structural properties of the data through alternating rounds of generation and
discrimination. The method is provably consistent under minimal assumptions.
Unlike classic tree-based alternatives, our approach provides smooth
(un)conditional densities and allows for fully synthetic data generation. We
achieve comparable or superior performance to state-of-the-art probabilistic
circuits and deep learning models on various tabular data benchmarks while
executing about two orders of magnitude faster on average. An accompanying
$\texttt{R}$ package, $\texttt{arf}$, is available on $\texttt{CRAN}$.",1,0,0,0,1,0,0.0569069,11.0,0.595761,139
6e6aff83-0345-47cf-b6ea-8e8bcaf809fc,Usage-based learning of grammatical categories,3,0.0209555,0.253805,"Human languages use a wide range of grammatical categories to constrain which
words or phrases can fill certain slots in grammatical patterns and to express
additional meanings, such as tense or aspect, through morpho-syntactic means.
These grammatical categories, which are most often language-specific and
changing over time, are difficult to define and learn. This paper raises the
question how these categories can be acquired and where they have come from. We
explore a usage-based approach. This means that categories and grammatical
constructions are selected and aligned by their success in language
interactions. We report on a multi-agent experiment in which agents are endowed
with mechanisms for understanding and producing utterances as well as
mechanisms for expanding their inventories using a meta-level learning process
based on pro- and anti-unification. We show that a categorial type network
which has scores based on the success in a language interaction leads to the
spontaneous formation of grammatical categories in tandem with the formation of
grammatical patterns.",0,0,0,0,0,0,1.72801e-06,32.0,0.535063,22
95afd49e-26f7-4228-b401-8bc7c91c4926,Deep Multi-Task Networks For Occluded Pedestrian Pose Estimation,6,0.0250304,0.335536,"Most of the existing works on pedestrian pose estimation do not consider
estimating the pose of an occluded pedestrian, as the annotations of the
occluded parts are not available in relevant automotive datasets. For example,
CityPersons, a well-known dataset for pedestrian detection in automotive scenes
does not provide pose annotations, whereas MS-COCO, a non-automotive dataset,
contains human pose estimation. In this work, we propose a multi-task framework
to extract pedestrian features through detection and instance segmentation
tasks performed separately on these two distributions. Thereafter, an encoder
learns pose specific features using an unsupervised instance-level domain
adaptation method for the pedestrian instances from both distributions. The
proposed framework has improved state-of-the-art performances of pose
estimation, pedestrian detection, and instance segmentation.",0,1,0,0,1,0,0.0793711,8.0,0.487251,13
5d55a162-f81c-4dc6-adfb-ecf6ded0eb39,Dog nose print matching with dual global descriptor based on Contrastive Learning,2,0.0158656,0.210894,"Recent studies in biometric-based identification tasks have shown that deep
learning methods can achieve better performance. These methods generally
extract the global features as descriptor to represent the original image.
Nonetheless, it does not perform well for biometric identification under
fine-grained tasks. The main reason is that the single image descriptor
contains insufficient information to represent image. In this paper, we present
a dual global descriptor model, which combines multiple global descriptors to
exploit multi level image features. Moreover, we utilize a contrastive loss to
enlarge the distance between image representations of confusing classes. The
proposed framework achieves the top2 on the CVPR2022 Biometrics Workshop Pet
Biometric Challenge. The source code and trained models are publicly available
at: https://github.com/flyingsheepbin/pet-biometrics",1,1,0,0,1,0,0.244565,6.0,0.519874,10
0a10d4d3-f7e2-4b49-ad42-2b74e0415e3f,FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing,36,0.513908,0.987216,"We present a benchmark suite of four datasets for evaluating the fairness of
pre-trained language models and the techniques used to fine-tune them for
downstream tasks. Our benchmarks cover four jurisdictions (European Council,
USA, Switzerland, and China), five languages (English, German, French, Italian
and Chinese) and fairness across five attributes (gender, age, region,
language, and legal area). In our experiments, we evaluate pre-trained language
models using several group-robust fine-tuning techniques and show that
performance group disparities are vibrant in many cases, while none of these
techniques guarantee fairness, nor consistently mitigate group disparities.
Furthermore, we provide a quantitative and qualitative analysis of our results,
highlighting open challenges in the development of robustness methods in legal
NLP.",0,0,0,0,0,0,0.811115,5.0,0.780271,66
71538419-70b8-4226-884c-b42a3a0af0fa,PolyHope: Two-Level Hope Speech Detection from Tweets,14,0.237086,0.848905,"Hope is characterized as openness of spirit toward the future, a desire,
expectation, and wish for something to happen or to be true that remarkably
affects human's state of mind, emotions, behaviors, and decisions. Hope is
usually associated with concepts of desired expectations and
possibility/probability concerning the future. Despite its importance, hope has
rarely been studied as a social media analysis task. This paper presents a hope
speech dataset that classifies each tweet first into ""Hope"" and ""Not Hope"",
then into three fine-grained hope categories: ""Generalized Hope"", ""Realistic
Hope"", and ""Unrealistic Hope"" (along with ""Not Hope""). English tweets in the
first half of 2022 were collected to build this dataset. Furthermore, we
describe our annotation process and guidelines in detail and discuss the
challenges of classifying hope and the limitations of the existing hope speech
detection corpora. In addition, we reported several baselines based on
different learning approaches, such as traditional machine learning, deep
learning, and transformers, to benchmark our dataset. We evaluated our
baselines using weighted-averaged and macro-averaged F1-scores. Observations
show that a strict process for annotator selection and detailed annotation
guidelines enhanced the dataset's quality. This strict annotation process
resulted in promising performance for simple machine learning classifiers with
only bi-grams; however, binary and multiclass hope speech detection results
reveal that contextual embedding models have higher performance in this
dataset.",0,1,1,1,0,0,0.00144977,21.0,0.61214,64
1158d86a-377c-454f-9d48-7fbf8f926b9b,DDNeRF: Depth Distribution Neural Radiance Fields,2,0.0248048,0.0575111,"In recent years, the field of implicit neural representation has progressed
significantly. Models such as neural radiance fields (NeRF), which uses
relatively small neural networks, can represent high-quality scenes and achieve
state-of-the-art results for novel view synthesis. Training these types of
networks, however, is still computationally very expensive. We present depth
distribution neural radiance field (DDNeRF), a new method that significantly
increases sampling efficiency along rays during training while achieving
superior results for a given sampling budget. DDNeRF achieves this by learning
a more accurate representation of the density distribution along rays. More
specifically, we train a coarse model to predict the internal distribution of
the transparency of an input volume in addition to the volume's total density.
This finer distribution then guides the sampling procedure of the fine model.
This method allows us to use fewer samples during training while reducing
computational resources.",0,1,0,0,0,0,0.987536,5.0,0.973746,28
2af96e8d-02a9-4bd7-a856-cfe4c08fc47b,Few-shot Learning with Noisy Labels,29,0.342377,0.898638,"Few-shot learning (FSL) methods typically assume clean support sets with
accurately labeled samples when training on novel classes. This assumption can
often be unrealistic: support sets, no matter how small, can still include
mislabeled samples. Robustness to label noise is therefore essential for FSL
methods to be practical, but this problem surprisingly remains largely
unexplored. To address mislabeled samples in FSL settings, we make several
technical contributions. (1) We offer simple, yet effective, feature
aggregation methods, improving the prototypes used by ProtoNet, a popular FSL
technique. (2) We describe a novel Transformer model for Noisy Few-Shot
Learning (TraNFS). TraNFS leverages a transformer's attention mechanism to
weigh mislabeled versus correct samples. (3) Finally, we extensively test these
methods on noisy versions of MiniImageNet and TieredImageNet. Our results show
that TraNFS is on-par with leading FSL methods on clean support sets, yet
outperforms them, by far, in the presence of label noise.",1,1,0,0,0,0,0.830215,7.0,0.851909,71
ad35d77a-aa7f-4fc9-b0b7-7bfed59234d6,Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction,69,0.761806,0.999995,"In this paper, we propose an effective yet efficient model PAIE for both
sentence-level and document-level Event Argument Extraction (EAE), which also
generalizes well when there is a lack of training data. On the one hand, PAIE
utilizes prompt tuning for extractive objectives to take the best advantages of
Pre-trained Language Models (PLMs). It introduces two span selectors based on
the prompt to select start/end tokens among input texts for each role. On the
other hand, it captures argument interactions via multi-role prompts and
conducts joint optimization with optimal span assignments via a bipartite
matching loss. Also, with a flexible prompt design, PAIE can extract multiple
arguments with the same role instead of conventional heuristic threshold
tuning. We have conducted extensive experiments on three benchmarks, including
both sentence- and document-level EAE. The results present promising
improvements from PAIE (3.5\% and 2.3\% F1 gains in average on three
benchmarks, for PAIE-base and PAIE-large respectively). Further analysis
demonstrates the efficiency, generalization to few-shot settings, and
effectiveness of different extractive prompt tuning strategies. Our code is
available at https://github.com/mayubo2333/PAIE.",1,1,0,0,0,0,0.953787,5.0,0.90274,42
812f6c05-c1ed-4589-b2bc-8e44b9a5a410,Blur Interpolation Transformer for Real-World Motion from Blur,8,0.292072,0.789912,"This paper studies the challenging problem of recovering motion from blur,
also known as joint deblurring and interpolation or blur temporal
super-resolution. The challenges are twofold: 1) the current methods still
leave considerable room for improvement in terms of visual quality even on the
synthetic dataset, and 2) poor generalization to real-world data. To this end,
we propose a blur interpolation transformer (BiT) to effectively unravel the
underlying temporal correlation encoded in blur. Based on multi-scale residual
Swin transformer blocks, we introduce dual-end temporal supervision and
temporally symmetric ensembling strategies to generate effective features for
time-varying motion rendering. In addition, we design a hybrid camera system to
collect the first real-world dataset of one-to-many blur-sharp video pairs.
Experimental results show that BiT has a significant gain over the
state-of-the-art methods on the public dataset Adobe240. Besides, the proposed
real-world dataset effectively helps the model generalize well to real blurry
scenarios. Code and data are available at https://github.com/zzh-tech/BiT.",1,1,0,1,1,0,0.700147,8.0,0.822074,61
b04fb4d5-1407-46f2-a179-c4269a443702,CL3D: Unsupervised Domain Adaptation for Cross-LiDAR 3D Detection,10,0.0413936,0.527695,"Domain adaptation for Cross-LiDAR 3D detection is challenging due to the
large gap on the raw data representation with disparate point densities and
point arrangements. By exploring domain-invariant 3D geometric characteristics
and motion patterns, we present an unsupervised domain adaptation method that
overcomes above difficulties. First, we propose the Spatial Geometry Alignment
module to extract similar 3D shape geometric features of the same object class
to align two domains, while eliminating the effect of distinct point
distributions. Second, we present Temporal Motion Alignment module to utilize
motion features in sequential frames of data to match two domains. Prototypes
generated from two modules are incorporated into the pseudo-label reweighting
procedure and contribute to our effective self-training framework for the
target domain. Extensive experiments show that our method achieves
state-of-the-art performance on cross-device datasets, especially for the
datasets with large gaps captured by mechanical scanning LiDARs and solid-state
LiDARs in various scenes. Project homepage is at
https://github.com/4DVLab/CL3D.git",1,1,0,0,1,0,0.188439,7.0,0.546309,68
b388be45-ac20-43e0-9ad0-3b43997eeb96,Watermarking Pre-trained Language Models with Backdooring,20,0.38117,0.493193,"Large pre-trained language models (PLMs) have proven to be a crucial
component of modern natural language processing systems. PLMs typically need to
be fine-tuned on task-specific downstream datasets, which makes it hard to
claim the ownership of PLMs and protect the developer's intellectual property
due to the catastrophic forgetting phenomenon. We show that PLMs can be
watermarked with a multi-task learning framework by embedding backdoors
triggered by specific inputs defined by the owners, and those watermarks are
hard to remove even though the watermarked PLMs are fine-tuned on multiple
downstream tasks. In addition to using some rare words as triggers, we also
show that the combination of common words can be used as backdoor triggers to
avoid them being easily detected. Extensive experiments on multiple datasets
demonstrate that the embedded watermarks can be robustly extracted with a high
success rate and less influenced by the follow-up fine-tuning.",0,1,0,0,0,0,0.852306,7.0,0.862719,26
2b527dcf-a635-4545-82e5-5fe83cf9a5e8,SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems,13,0.181924,0.702603,"We design deep neural networks (DNNs) and corresponding networks' splittings
to distribute DNNs' workload to camera sensors and a centralized aggregator on
head mounted devices to meet system performance targets in inference accuracy
and latency under the given hardware resource constraints. To achieve an
optimal balance among computation, communication, and performance, a
split-aware neural architecture search framework, SplitNets, is introduced to
conduct model designing, splitting, and communication reduction simultaneously.
We further extend the framework to multi-view systems for learning to fuse
inputs from multiple camera sensors with optimal performance and systemic
efficiency. We validate SplitNets for single-view system on ImageNet as well as
multi-view system on 3D classification, and show that the SplitNets framework
achieves state-of-the-art (SOTA) performance and system latency compared with
existing approaches.",0,1,0,0,1,0,0.690692,7.0,0.792926,66
57fd8608-eb53-408a-80dd-35851c654e2e,UnCommonSense: Informative Negative Knowledge about Everyday Concepts,10,0.218032,0.732947,"Commonsense knowledge about everyday concepts is an important asset for AI
applications, such as question answering and chatbots. Recently, we have seen
an increasing interest in the construction of structured commonsense knowledge
bases (CSKBs). An important part of human commonsense is about properties that
do not apply to concepts, yet existing CSKBs only store positive statements.
Moreover, since CSKBs operate under the open-world assumption, absent
statements are considered to have unknown truth rather than being invalid. This
paper presents the UNCOMMONSENSE framework for materializing informative
negative commonsense statements. Given a target concept, comparable concepts
are identified in the CSKB, for which a local closed-world assumption is
postulated. This way, positive statements about comparable concepts that are
absent for the target concept become seeds for negative statement candidates.
The large set of candidates is then scrutinized, pruned and ranked by
informativeness. Intrinsic and extrinsic evaluations show that our method
significantly outperforms the state-of-the-art. A large dataset of informative
negations is released as a resource for future research.",0,1,1,1,1,0,0.545361,7.0,0.736089,50
3c3dbb71-5a78-4779-b955-e2ef1723bf64,Understanding Natural Language in Context,1,0.0022741,0.0380673,"Recent years have seen an increasing number of applications that have a
natural language interface, either in the form of chatbots or via personal
assistants such as Alexa (Amazon), Google Assistant, Siri (Apple), and Cortana
(Microsoft). To use these applications, a basic dialog between the robot and
the human is required.
  While this kind of dialog exists today mainly within ""static"" robots that do
not make any movement in the household space, the challenge of reasoning about
the information conveyed by the environment increases significantly when
dealing with robots that can move and manipulate objects in our home
environment.
  In this paper, we focus on cognitive robots, which have some knowledge-based
models of the world and operate by reasoning and planning with this model.
Thus, when the robot and the human communicate, there is already some formalism
they can use - the robot's knowledge representation formalism.
  Our goal in this research is to translate natural language utterances into
this robot's formalism, allowing much more complicated household tasks to be
completed. We do so by combining off-the-shelf SOTA language models, planning
tools, and the robot's knowledge-base for better communication. In addition, we
analyze different directive types and illustrate the contribution of the
world's context to the translation process.",0,1,0,0,0,0,0.223276,8.0,0.626861,23
be767968-04ab-4da7-921a-727315394181,GUSOT: Green and Unsupervised Single Object Tracking for Long Video Sequences,6,0.0332713,0.426632,"Supervised and unsupervised deep trackers that rely on deep learning
technologies are popular in recent years. Yet, they demand high computational
complexity and a high memory cost. A green unsupervised single-object tracker,
called GUSOT, that aims at object tracking for long videos under a
resource-constrained environment is proposed in this work. Built upon a
baseline tracker, UHP-SOT++, which works well for short-term tracking, GUSOT
contains two additional new modules: 1) lost object recovery, and 2)
color-saliency-based shape proposal. They help resolve the tracking loss
problem and offer a more flexible object proposal, respectively. Thus, they
enable GUSOT to achieve higher tracking accuracy in the long run. We conduct
experiments on the large-scale dataset LaSOT with long video sequences, and
show that GUSOT offers a lightweight high-performance tracking solution that
finds applications in mobile and edge computing platforms.",0,1,0,0,0,0,0.440545,9.0,0.7608,42
24cc29be-2823-4fe7-8a94-1e11e651450e,BioTABQA: Instruction Learning for Biomedical Table Question Answering,14,0.226296,0.780816,"Table Question Answering (TQA) is an important but under-explored task. Most
of the existing QA datasets are in unstructured text format and only few of
them use tables as the context. To the best of our knowledge, none of TQA
datasets exist in the biomedical domain where tables are frequently used to
present information. In this paper, we first curate a table question answering
dataset, BioTABQA, using 22 templates and the context from a biomedical
textbook on differential diagnosis. BioTABQA can not only be used to teach a
model how to answer questions from tables but also evaluate how a model
generalizes to unseen questions, an important scenario for biomedical
applications. To achieve the generalization evaluation, we divide the templates
into 17 training and 5 cross-task evaluations. Then, we develop two baselines
using single and multi-tasks learning on BioTABQA. Furthermore, we explore
instructional learning, a recent technique showing impressive generalizing
performance. Experimental results show that our instruction-tuned model
outperforms single and multi-task baselines on an average by ~23% and ~6%
across various evaluation settings, and more importantly, instruction-tuned
model outperforms baselines by ~5% on cross-tasks.",0,1,1,1,0,0,0.859409,5.0,0.812895,47
67da14fa-281b-4a50-99de-598a78046fc7,Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems,7,0.0162188,0.364833,"Stochastic optimization has found wide applications in minimizing objective
functions in machine learning, which motivates a lot of theoretical studies to
understand its practical success. Most of existing studies focus on the
convergence of optimization errors, while the generalization analysis of
stochastic optimization is much lagging behind. This is especially the case for
nonconvex and nonsmooth problems often encountered in practice. In this paper,
we initialize a systematic stability and generalization analysis of stochastic
optimization on nonconvex and nonsmooth problems. We introduce novel
algorithmic stability measures and establish their quantitative connection on
the gap between population gradients and empirical gradients, which is then
further extended to study the gap between the Moreau envelope of the empirical
risk and that of the population risk. To our knowledge, these quantitative
connection between stability and generalization in terms of either gradients or
Moreau envelopes have not been studied in the literature. We introduce a class
of sampling-determined algorithms, for which we develop bounds for three
stability measures. Finally, we apply these discussions to derive error bounds
for stochastic gradient descent and its adaptive variant, where we show how to
achieve an implicit regularization by tuning the step sizes and the number of
iterations.",0,0,0,0,0,0,0.0151495,9.0,0.356489,78
77cc5721-97a0-489b-ab52-f76bb70507b1,Emergent Communication through Metropolis-Hastings Naming Game with Deep Generative Models,10,0.149187,0.880075,"Constructive studies on symbol emergence systems seek to investigate
computational models that can better explain human language evolution, the
creation of symbol systems, and the construction of internal representations.
This study provides a new model for emergent communication, which is based on a
probabilistic generative model (PGM) instead of a discriminative model based on
deep reinforcement learning. We define the Metropolis-Hastings (MH) naming game
by generalizing previously proposed models. It is not a referential game with
explicit feedback, as assumed by many emergent communication studies. Instead,
it is a game based on joint attention without explicit feedback.
Mathematically, the MH naming game is proved to be a type of MH algorithm for
an integrative PGM that combines two agents that play the naming game. From
this viewpoint, symbol emergence is regarded as decentralized Bayesian
inference, and semiotic communication is regarded as inter-personal cross-modal
inference. This notion leads to the collective predictive coding hypothesis}
regarding language evolution and, in general, the emergence of symbols. We also
propose the inter-Gaussian mixture model (GMM)+ variational autoencoder (VAE),
a deep generative model for emergent communication based on the MH naming game.
The model has been validated on MNIST and Fruits 360 datasets. Experimental
findings demonstrate that categories are formed from real images observed by
agents, and signs are correctly shared across agents by successfully utilizing
both of the observations of agents via the MH naming game. Furthermore,
scholars verified that visual images were recalled from signs uttered by
agents. Notably, emergent communication without supervision and reward feedback
improved the performance of the unsupervised representation learning of agents.",0,0,0,0,0,0,0.126396,13.0,0.722233,70
588f8245-ad84-479f-9605-9629ba924614,MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations,30,0.132278,0.865856,"Poor sample efficiency continues to be the primary challenge for deployment
of deep Reinforcement Learning (RL) algorithms for real-world applications, and
in particular for visuo-motor control. Model-based RL has the potential to be
highly sample efficient by concurrently learning a world model and using
synthetic rollouts for planning and policy improvement. However, in practice,
sample-efficient learning with model-based RL is bottlenecked by the
exploration challenge. In this work, we find that leveraging just a handful of
demonstrations can dramatically improve the sample-efficiency of model-based
RL. Simply appending demonstrations to the interaction dataset, however, does
not suffice. We identify key ingredients for leveraging demonstrations in model
learning -- policy pretraining, targeted exploration, and oversampling of
demonstration data -- which forms the three phases of our model-based RL
framework. We empirically study three complex visuo-motor control domains and
find that our method is 150%-250% more successful in completing sparse reward
tasks compared to prior approaches in the low data regime (100K interaction
steps, 5 demonstrations). Code and videos are available at:
https://nicklashansen.github.io/modemrl",1,1,0,0,1,0,0.469128,7.0,0.704807,67
5f9bf4a9-b074-4f98-8617-ae9c19eb5773,Does the Market of Citations Reward Reproducible Work?,8,0.0279804,0.351938,"The field of bibliometrics, studying citations and behavior, is critical to
the discussion of reproducibility. Citations are one of the primary incentive
and reward systems for academic work, and so we desire to know if this
incentive rewards reproducible work. Yet to the best of our knowledge, only one
work has attempted to look at this combined space, concluding that
non-reproducible work is more highly cited. We show that answering this
question is more challenging than first proposed, and subtle issues can inhibit
a robust conclusion. To make inferences with more robust behavior, we propose a
hierarchical Bayesian model that incorporates the citation rate over time,
rather than the total number of citations after a fixed amount of time. In
doing so we show that, under current evidence the answer is more likely that
certain fields of study such as Medicine and Machine Learning (ML) do correlate
reproducible works with more citations, but other fields appear to have no
relationship. Further, we find that making code available and thoroughly
referencing prior works appear to also positively correlate with increased
citations. Our code and data can be found at
https://github.com/EdwardRaff/ReproducibleCitations .",1,0,0,0,0,1,0.0687535,9.0,0.527638,47
ff279b75-2c88-4580-9623-82589bcad37d,Language Models of Code are Few-Shot Commonsense Learners,121,0.287524,0.983474,"We address the general task of structured commonsense reasoning: given a
natural language input, the goal is to generate a graph such as an event -- or
a reasoning-graph. To employ large language models (LMs) for this task,
existing approaches ``serialize'' the output graph as a flat list of nodes and
edges. Although feasible, these serialized graphs strongly deviate from the
natural language corpora that LMs were pre-trained on, hindering LMs from
generating them correctly. In this paper, we show that when we instead frame
structured commonsense reasoning tasks as code generation tasks, pre-trained
LMs of code are better structured commonsense reasoners than LMs of natural
language, even when the downstream task does not involve source code at all. We
demonstrate our approach across three diverse structured commonsense reasoning
tasks. In all these natural language tasks, we show that using our approach, a
code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the
target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot
setting.",1,0,0,0,0,0,0.52115,4.0,0.521134,38
1b44cba2-4f51-459d-93ef-11113339be5b,Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically Ambiguous Settings for Low Resource Languages,2,0.0212025,0.286689,"We leverage pre-trained language models to solve the task of complex NER for
two low-resource languages: Chinese and Spanish. We use the technique of Whole
Word Masking(WWM) to boost the performance of masked language modeling
objective on large and unsupervised corpora. We experiment with multiple neural
network architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on
top of a fine-tuned BERT layer. All our models outperform the baseline by a
significant margin and our best performing model obtains a competitive position
on the evaluation leaderboard for the blind test set.",1,1,0,0,0,0,0.613347,5.0,0.667901,28
4c1e58a5-ecc8-4ab0-b323-7cf41a20c230,Not always about you: Prioritizing community needs when developing endangered language technology,14,0.292177,0.757213,"Languages are classified as low-resource when they lack the quantity of data
necessary for training statistical and machine learning tools and models.
Causes of resource scarcity vary but can include poor access to technology for
developing these resources, a relatively small population of speakers, or a
lack of urgency for collecting such resources in bilingual populations where
the second language is high-resource. As a result, the languages described as
low-resource in the literature are as different as Finnish on the one hand,
with millions of speakers using it in every imaginable domain, and Seneca, with
only a small-handful of fluent speakers using the language primarily in a
restricted domain. While issues stemming from the lack of resources necessary
to train models unite this disparate group of languages, many other issues cut
across the divide between widely-spoken low resource languages and endangered
languages. In this position paper, we discuss the unique technological,
cultural, practical, and ethical challenges that researchers and indigenous
speech community members face when working together to develop language
technology to support endangered language documentation and revitalization. We
report the perspectives of language teachers, Master Speakers and elders from
indigenous communities, as well as the point of view of academics. We describe
an ongoing fruitful collaboration and make recommendations for future
partnerships between academic researchers and language community stakeholders.",0,0,0,0,0,0,0.0690999,9.0,0.528216,55
a4551737-c1d4-4d01-9d41-be6ec2362573,A global analysis of metrics used for measuring performance in natural language processing,11,0.454351,0.286283,"Measuring the performance of natural language processing models is
challenging. Traditionally used metrics, such as BLEU and ROUGE, originally
devised for machine translation and summarization, have been shown to suffer
from low correlation with human judgment and a lack of transferability to other
tasks and languages. In the past 15 years, a wide range of alternative metrics
have been proposed. However, it is unclear to what extent this has had an
impact on NLP benchmarking efforts. Here we provide the first large-scale
cross-sectional analysis of metrics used for measuring performance in natural
language processing. We curated, mapped and systematized more than 3500 machine
learning model performance results from the open repository 'Papers with Code'
to enable a global and comprehensive analysis. Our results suggest that the
large majority of natural language processing metrics currently used have
properties that may result in an inadequate reflection of a models'
performance. Furthermore, we found that ambiguities and inconsistencies in the
reporting of metrics may lead to difficulties in interpreting and comparing
model performances, impairing transparency and reproducibility in NLP research.",0,0,0,0,0,0,0.768387,15.0,0.918056,33
4ecbf7fc-6a20-4a71-8838-683a9950c653,Xplique: A Deep Learning Explainability Toolbox,15,0.0112313,0.418994,"Today's most advanced machine-learning models are hardly scrutable. The key
challenge for explainability methods is to help assisting researchers in
opening up these black boxes, by revealing the strategy that led to a given
decision, by characterizing their internal states or by studying the underlying
data representation. To address this challenge, we have developed Xplique: a
software library for explainability which includes representative
explainability methods as well as associated evaluation metrics. It interfaces
with one of the most popular learning libraries: Tensorflow as well as other
libraries including PyTorch, scikit-learn and Theano. The code is licensed
under the MIT license and is freely available at github.com/deel-ai/xplique.",1,1,0,0,0,0,0.0706105,8.0,0.472047,51
40842168-cba8-4e27-984d-2c26b861722c,"Towards Automated Document Revision: Grammatical Error Correction, Fluency Edits, and Beyond",8,0.0968759,0.884681,"Natural language processing technology has rapidly improved automated
grammatical error correction tasks, and the community begins to explore
document-level revision as one of the next challenges. To go beyond
sentence-level automated grammatical error correction to NLP-based
document-level revision assistant, there are two major obstacles: (1) there are
few public corpora with document-level revisions being annotated by
professional editors, and (2) it is not feasible to elicit all possible
references and evaluate the quality of revision with such references because
there are infinite possibilities of revision. This paper tackles these
challenges. First, we introduce a new document-revision corpus, TETRA, where
professional editors revised academic papers sampled from the ACL anthology
which contain few trivial grammatical errors that enable us to focus more on
document- and paragraph-level edits such as coherence and consistency. Second,
we explore reference-less and interpretable methods for meta-evaluation that
can detect quality improvements by document revision. We show the uniqueness of
TETRA compared with existing document revision corpora and demonstrate that a
fine-tuned pre-trained language model can discriminate the quality of documents
after revision even when the difference is subtle. This promising result will
encourage the community to further explore automated document revision models
and metrics in future.",0,0,1,1,0,0,0.0287732,13.0,0.60437,79
1d7028ab-2685-40de-8789-a41a44baebb1,CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation,21,0.354935,0.411054,"The full power of human language-based communication cannot be realized
without negation. All human languages have some form of negation. Despite this,
negation remains a challenging phenomenon for current natural language
understanding systems. To facilitate the future development of models that can
process negation effectively, we present CONDAQA, the first English reading
comprehension dataset which requires reasoning about the implications of
negated statements in paragraphs. We collect paragraphs with diverse negation
cues, then have crowdworkers ask questions about the implications of the
negated statement in the passage. We also have workers make three kinds of
edits to the passage -- paraphrasing the negated statement, changing the scope
of the negation, and reversing the negation -- resulting in clusters of
question-answer pairs that are difficult for models to answer with spurious
shortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique
negation cues and is challenging for current state-of-the-art models. The best
performing model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our
consistency metric, well below human performance which is 81%. We release our
dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to
facilitate the development of future NLP methods that work on negated language.",1,0,1,1,0,0,0.622868,7.0,0.766487,91
a26040c9-8c65-410c-b6ee-4d5611fa2816,Hybrid Reinforced Medical Report Generation with M-Linear Attention and Repetition Penalty,2,0.122436,0.145095,"To reduce doctors' workload, deep-learning-based automatic medical report
generation has recently attracted more and more research efforts, where deep
convolutional neural networks (CNNs) are employed to encode the input images,
and recurrent neural networks (RNNs) are used to decode the visual features
into medical reports automatically. However, these state-of-the-art methods
mainly suffer from three shortcomings: (i) incomprehensive optimization, (ii)
low-order and unidimensional attention mechanisms, and (iii) repeated
generation. In this article, we propose a hybrid reinforced medical report
generation method with m-linear attention and repetition penalty mechanism
(HReMRG-MR) to overcome these problems. Specifically, a hybrid reward with
different weights is employed to remedy the limitations of single-metric-based
rewards. We also propose a search algorithm with linear complexity to
approximate the best weight combination. Furthermore, we use m-linear attention
modules to explore high-order feature interactions and to achieve multi-modal
reasoning, while a repetition penalty applies penalties to repeated terms
during the model's training process. Extensive experimental studies on two
public datasets show that HReMRG-MR greatly outperforms the state-of-the-art
baselines in terms of all metrics. We also conducted a series of ablation
experiments to prove the effectiveness of all our proposed components. We also
performed a reward search toy experiment to give evidence that our proposed
search approach can significantly reduce the search time while approximating
the best performance.",0,1,0,0,1,0,0.975405,11.0,0.972754,25
abffdf14-f765-4f0a-96f3-be51b6ef668f,iCaps: Iterative Category-level Object Pose and Shape Estimation,31,0.595159,0.831346,"This paper proposes a category-level 6D object pose and shape estimation
approach iCaps, which allows tracking 6D poses of unseen objects in a category
and estimating their 3D shapes. We develop a category-level auto-encoder
network using depth images as input, where feature embeddings from the
auto-encoder encode poses of objects in a category. The auto-encoder can be
used in a particle filter framework to estimate and track 6D poses of objects
in a category. By exploiting an implicit shape representation based on signed
distance functions, we build a LatentNet to estimate a latent representation of
the 3D shape given the estimated pose of an object. Then the estimated pose and
shape can be used to update each other in an iterative way. Our category-level
6D object pose and shape estimation pipeline only requires 2D detection and
segmentation for initialization. We evaluate our approach on a publicly
available dataset and demonstrate its effectiveness. In particular, our method
achieves comparably high accuracy on shape estimation.",1,1,0,0,0,0,0.964661,9.0,0.955263,46
5e3e358d-66e2-4d42-ab85-0dae8dca2d89,On the Power of Foundation Models,16,0.0730467,0.396804,"With infinitely many high-quality data points, infinite computational power,
an infinitely large foundation model with a perfect training algorithm and
guaranteed zero generalization error on the pretext task, can the model be used
for everything? This question cannot be answered by the existing theory of
representation, optimization or generalization, because the issues they mainly
investigate are assumed to be nonexistent here. In this paper, we show that
category theory provides powerful machinery to answer this question. We have
proved three results. The first one limits the power of prompt-based learning,
saying that the model can solve a downstream task with prompts if and only if
the task is representable. The second one says fine tuning does not have this
limit, as a foundation model with the minimum required power (up to symmetry)
can theoretically solve downstream tasks for the category defined by pretext
task, with fine tuning and enough resources. Our final result can be seen as a
new type of generalization theorem, showing that the foundation model can
generate unseen objects from the target category (e.g., images) using the
structural information from the source category (e.g., texts). Along the way,
we provide a categorical framework for supervised and self-supervised learning,
which might be of independent interest.",0,0,0,0,0,0,0.466698,7.0,0.703773,83
55a6c2f5-3a4b-44a6-a8be-8558f274e573,To Answer or Not to Answer? Improving Machine Reading Comprehension Model with Span-based Contrastive Learning,5,0.05545,0.716549,"Machine Reading Comprehension with Unanswerable Questions is a difficult NLP
task, challenged by the questions which can not be answered from passages. It
is observed that subtle literal changes often make an answerable question
unanswerable, however, most MRC models fail to recognize such changes. To
address this problem, in this paper, we propose a span-based method of
Contrastive Learning (spanCL) which explicitly contrast answerable questions
with their answerable and unanswerable counterparts at the answer span level.
With spanCL, MRC models are forced to perceive crucial semantic changes from
slight literal differences. Experiments on SQuAD 2.0 dataset show that spanCL
can improve baselines significantly, yielding 0.86-2.14 absolute EM
improvements. Additional experiments also show that spanCL is an effective way
to utilize generated questions.",0,1,0,0,0,1,0.814146,7.0,0.844431,59
0d9f5c4a-f99e-427f-9a09-380214c6e181,A Low-Shot Object Counting Network With Iterative Prototype Adaptation,15,0.169111,0.336799,"We consider low-shot counting of arbitrary semantic categories in the image
using only few annotated exemplars (few-shot) or no exemplars (no-shot). The
standard few-shot pipeline follows extraction of appearance queries from
exemplars and matching them with image features to infer the object counts.
Existing methods extract queries by feature pooling which neglects the shape
information (e.g., size and aspect) and leads to a reduced object localization
accuracy and count estimates. We propose a Low-shot Object Counting network
with iterative prototype Adaptation (LOCA). Our main contribution is the new
object prototype extraction module, which iteratively fuses the exemplar shape
and appearance information with image features. The module is easily adapted to
zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot
counting problems. LOCA outperforms all recent state-of-the-art methods on
FSC147 benchmark by 20-30% in RMSE on one-shot and few-shot and achieves
state-of-the-art on zero-shot scenarios, while demonstrating better
generalization capabilities.",1,1,0,0,1,0,0.402471,9.0,0.747431,34
08cb1604-2de3-499f-8ed8-71b1e78b03d5,Unfooling Perturbation-Based Post Hoc Explainers,10,0.228091,0.778764,"Monumental advancements in artificial intelligence (AI) have lured the
interest of doctors, lenders, judges, and other professionals. While these
high-stakes decision-makers are optimistic about the technology, those familiar
with AI systems are wary about the lack of transparency of its decision-making
processes. Perturbation-based post hoc explainers offer a model agnostic means
of interpreting these systems while only requiring query-level access. However,
recent work demonstrates that these explainers can be fooled adversarially.
This discovery has adverse implications for auditors, regulators, and other
sentinels. With this in mind, several natural questions arise - how can we
audit these black box systems? And how can we ascertain that the auditee is
complying with the audit in good faith? In this work, we rigorously formalize
this problem and devise a defense against adversarial attacks on
perturbation-based explainers. We propose algorithms for the detection
(CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our
novel conditional anomaly detection approach, KNN-CAD. We demonstrate that our
approach successfully detects whether a black box system adversarially conceals
its decision-making process and mitigates the adversarial attack on real-world
data for the prevalent explainers, LIME and SHAP.",1,1,0,0,0,0,0.731473,5.0,0.73285,49
77f96f24-fbb9-417f-85de-debb33d81d91,Multimodal learning with graphs,34,0.286394,0.808843,"Artificial intelligence for graphs has achieved remarkable success in
modeling complex systems, ranging from dynamic networks in biology to
interacting particle systems in physics. However, the increasingly
heterogeneous graph datasets call for multimodal methods that can combine
different inductive biases: the set of assumptions that algorithms use to make
predictions for inputs they have not encountered during training. Learning on
multimodal datasets presents fundamental challenges because the inductive
biases can vary by data modality and graphs might not be explicitly given in
the input. To address these challenges, multimodal graph AI methods combine
different modalities while leveraging cross-modal dependencies using graphs.
Diverse datasets are combined using graphs and fed into sophisticated
multimodal architectures, specified as image-intensive, knowledge-grounded and
language-intensive models. Using this categorization, we introduce a blueprint
for multimodal graph learning, use it to study existing methods and provide
guidelines to design new models.",0,0,0,0,0,0,0.487486,6.0,0.664622,150
5357fa98-19d9-4a00-bed9-1467ede17b84,K-level Reasoning for Zero-Shot Coordination in Hanabi,21,0.115066,0.344294,"The standard problem setting in cooperative multi-agent settings is self-play
(SP), where the goal is to train a team of agents that works well together.
However, optimal SP policies commonly contain arbitrary conventions
(""handshakes"") and are not compatible with other, independently trained agents
or humans. This latter desiderata was recently formalized by Hu et al. 2020 as
the zero-shot coordination (ZSC) setting and partially addressed with their
Other-Play (OP) algorithm, which showed improved ZSC and human-AI performance
in the card game Hanabi. OP assumes access to the symmetries of the environment
and prevents agents from breaking these in a mutually incompatible way during
training. However, as the authors point out, discovering symmetries for a given
environment is a computationally hard problem. Instead, we show that through a
simple adaption of k-level reasoning (KLR) Costa Gomes et al. 2006,
synchronously training all levels, we can obtain competitive ZSC and ad-hoc
teamplay performance in Hanabi, including when paired with a human-like proxy
bot. We also introduce a new method, synchronous-k-level reasoning with a best
response (SyKLRBR), which further improves performance on our synchronous KLR
by co-training a best response.",0,1,0,0,0,0,0.19338,9.0,0.650333,50
14fb4035-eecd-4b4a-a84c-97c4b5823d46,Third Time's the Charm? Image and Video Editing with StyleGAN3,54,0.682009,0.894966,"StyleGAN is arguably one of the most intriguing and well-studied generative
models, demonstrating impressive performance in image generation, inversion,
and manipulation. In this work, we explore the recent StyleGAN3 architecture,
compare it to its predecessor, and investigate its unique advantages, as well
as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained
on unaligned data, one can still use aligned data for training, without
hindering the ability to generate unaligned imagery. Next, our analysis of the
disentanglement of the different latent spaces of StyleGAN3 indicates that the
commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts,
underscoring the benefits of using the StyleSpace for fine-grained editing.
Considering image inversion, we observe that existing encoder-based techniques
struggle when trained on unaligned data. We therefore propose an encoding
scheme trained solely on aligned data, yet can still invert unaligned images.
Finally, we introduce a novel video inversion and editing workflow that
leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce
texture sticking and expand the field of view of the edited video.",1,1,0,0,0,0,0.968312,4.0,0.907366,94
86fd3f2a-3c4f-4247-9bcc-5f4c75cb9a8c,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,68,0.830393,0.98535,"Conversational agents have come increasingly closer to human competence in
open-domain dialogue settings; however, such models can reflect insensitive,
hurtful, or entirely incoherent viewpoints that erode a user's trust in the
moral integrity of the system. Moral deviations are difficult to mitigate
because moral judgments are not universal, and there may be multiple competing
judgments that apply to a situation simultaneously. In this work, we introduce
a new resource, not to authoritatively resolve moral ambiguities, but instead
to facilitate systematic understanding of the intuitions, values and moral
judgments reflected in the utterances of dialogue systems. The Moral Integrity
Corpus, MIC, is such a resource, which captures the moral assumptions of 38k
prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects
a particular moral conviction that can explain why a chatbot's reply may appear
acceptable or problematic. We further organize RoTs with a set of 9 moral and
social attributes and benchmark performance for attribute classification. Most
importantly, we show that current neural language models can automatically
generate new RoTs that reasonably describe previously unseen interactions, but
they still struggle with certain scenarios. Our findings suggest that MIC will
be a useful resource for understanding and language models' implicit moral
assumptions and flexibly benchmarking the integrity of conversational agents.
To download the data, see https://github.com/GT-SALT/mic",0,0,0,1,0,0,0.929494,6.0,0.894316,109
0ecec887-f046-427e-bd26-63ff775758c0,Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography,5,0.150386,0.513725,"Displacement estimation is a critical step of virtually all Ultrasound
Elastography (USE) techniques. Two main features make this task unique compared
to the general optical flow problem: the high-frequency nature of ultrasound
radio-frequency (RF) data and the governing laws of physics on the displacement
field. Recently, the architecture of the optical flow networks has been
modified to be able to use RF data. Also, semi-supervised and unsupervised
techniques have been employed for USE by considering prior knowledge of
displacement continuity in the form of the first- and second-derivative
regularizers. Despite these attempts, no work has considered the tissue
compression pattern, and displacements in axial and lateral directions have
been assumed to be independent. However, tissue motion pattern is governed by
laws of physics in USE, rendering the axial and the lateral displacements
highly correlated. In this paper, we propose Physically Inspired ConsTraint for
Unsupervised Regularized Elastography (PICTURE), where we impose constraints on
the Poisson's ratio to improve lateral displacement estimates. Experiments on
phantom and in vivo data show that PICTURE substantially improves the quality
of the lateral displacement estimation.",0,0,0,0,0,0,0.571429,7.0,0.746409,16
164683b4-638d-42ed-957a-160f582ebe9b,Learning First-Order Symbolic Planning Representations That Are Grounded,4,0.0517346,0.456069,"Two main approaches have been developed for learning first-order planning
(action) models from unstructured data: combinatorial approaches that yield
crisp action schemas from the structure of the state space, and deep learning
approaches that produce action schemas from states represented by images. A
benefit of the former approach is that the learned action schemas are similar
to those that can be written by hand; a benefit of the latter is that the
learned representations (predicates) are grounded on the images, and as a
result, new instances can be given in terms of images. In this work, we develop
a new formulation for learning crisp first-order planning models that are
grounded on parsed images, a step to combine the benefits of the two
approaches. Parsed images are assumed to be given in a simple O2D language
(objects in 2D) that involves a small number of unary and binary predicates
like ""left"", ""above"", ""shape"", etc. After learning, new planning instances can
be given in terms of pairs of parsed images, one for the initial situation and
the other for the goal. Learning and planning experiments are reported for
several domains including Blocks, Sokoban, IPC Grid, and Hanoi.",0,0,0,0,0,0,0.0725589,12.0,0.650386,81
85f29d91-f3bd-4238-9041-030e8f531f18,A Fully Memristive Spiking Neural Network with Unsupervised Learning,5,0.0611237,0.371531,"We present a fully memristive spiking neural network (MSNN) consisting of
physically-realizable memristive neurons and memristive synapses to implement
an unsupervised Spiking Time Dependent Plasticity (STDP) learning rule. The
system is fully memristive in that both neuronal and synaptic dynamics can be
realized by using memristors. The neuron is implemented using the SPICE-level
memristive integrate-and-fire (MIF) model, which consists of a minimal number
of circuit elements necessary to achieve distinct depolarization,
hyperpolarization, and repolarization voltage waveforms. The proposed MSNN
uniquely implements STDP learning by using cumulative weight changes in
memristive synapses from the voltage waveform changes across the synapses,
which arise from the presynaptic and postsynaptic spiking voltage signals
during the training process. Two types of MSNN architectures are investigated:
1) a biologically plausible memory retrieval system, and 2) a multi-class
classification system. Our circuit simulation results verify the MSNN's
unsupervised learning efficacy by replicating biological memory retrieval
mechanisms, and achieving 97.5% accuracy in a 4-pattern recognition problem in
a large scale discriminative MSNN.",0,0,0,0,0,0,0.347718,10.0,0.754024,41
d09bc21d-4bd2-400c-8de8-0d64b1aa9384,Apport des ontologies pour le calcul de la similarit smantique au sein d'un systme de recommandation,7,0.139266,0.635075,"Measurement of the semantic relatedness or likeness between terms, words, or
text data plays an important role in different applications dealing with
textual data such as knowledge acquisition, recommender system, and natural
language processing. Over the past few years, many ontologies have been
developed and used as a form of structured representation of knowledge bases
for information systems. The calculation of semantic similarity from ontology
has developed and depending on the context is complemented by other similarity
calculation methods. In this paper, we propose and carry on an approach for the
calculation of ontology-based semantic similarity using in the context of a
recommender system.",0,1,0,0,0,0,0.0347503,22.0,0.774938,34
7a8759ee-06e4-425c-ac46-aafb719d6d4d,Error Compensation Framework for Flow-Guided Video Inpainting,24,0.285004,0.453738,"The key to video inpainting is to use correlation information from as many
reference frames as possible. Existing flow-based propagation methods split the
video synthesis process into multiple steps: flow completion -> pixel
propagation -> synthesis. However, there is a significant drawback that the
errors in each step continue to accumulate and amplify in the next step. To
this end, we propose an Error Compensation Framework for Flow-guided Video
Inpainting (ECFVI), which takes advantage of the flow-based method and offsets
its weaknesses. We address the weakness with the newly designed flow completion
module and the error compensation network that exploits the error guidance map.
Our approach greatly improves the temporal consistency and the visual quality
of the completed videos. Experimental results show the superior performance of
our proposed method with the speed up of x6, compared to the state-of-the-art
methods. In addition, we present a new benchmark dataset for evaluation by
supplementing the weaknesses of existing test datasets.",0,1,0,0,1,0,0.760158,7.0,0.820954,26
469c9a45-8e5d-4513-a028-7bcba5e64089,Unsupervised Segmentation of Hyperspectral Remote Sensing Images with Superpixels,16,0.47558,0.570077,"In this paper, we propose an unsupervised method for hyperspectral remote
sensing image segmentation. The method exploits the mean-shift clustering
algorithm that takes as input a preliminary hyperspectral superpixels
segmentation together with the spectral pixel information. The proposed method
does not require the number of segmentation classes as input parameter, and it
does not exploit any a-priori knowledge about the type of land-cover or
land-use to be segmented (e.g. water, vegetation, building etc.). Experiments
on Salinas, SalinasA, Pavia Center and Pavia University datasets are carried
out. Performance are measured in terms of normalized mutual information,
adjusted Rand index and F1-score. Results demonstrate the validity of the
proposed method in comparison with the state of the art.",1,1,0,0,1,0,0.591525,7.0,0.754285,66
85737084-fa24-4fd0-b2b7-4e46a85b527e,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,152,0.631486,0.991911,"This study focuses on using large language models (LLMs) as a planner for
embodied agents that can follow natural language instructions to complete
complex tasks in a visually-perceived environment. The high data cost and poor
sample efficiency of existing methods hinders the development of versatile
agents that are capable of many tasks and can learn new tasks quickly. In this
work, we propose a novel method, LLM-Planner, that harnesses the power of large
language models to do few-shot planning for embodied agents. We further propose
a simple but effective way to enhance LLMs with physical grounding to generate
and update plans that are grounded in the current environment. Experiments on
the ALFRED dataset show that our method can achieve very competitive few-shot
performance: Despite using less than 0.5% of paired training data, LLM-Planner
achieves competitive performance with recent baselines that are trained using
the full training data. Existing methods can barely complete any task
successfully under the same few-shot setting. Our work opens the door for
developing versatile and sample-efficient embodied agents that can quickly
learn many tasks. Website: https://dki-lab.github.io/LLM-Planner",0,1,0,0,1,0,0.773593,4.0,0.696565,52
905efe13-dd93-43fc-91dc-3dedcef55fc0,Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models,23,0.826601,0.858371,"Motivations for methods in explainable artificial intelligence (XAI) often
include detecting, quantifying and mitigating bias, and contributing to making
machine learning models fairer. However, exactly how an XAI method can help in
combating biases is often left unspecified. In this paper, we briefly review
trends in explainability and fairness in NLP research, identify the current
practices in which explainability methods are applied to detect and mitigate
bias, and investigate the barriers preventing XAI methods from being used more
widely in tackling fairness issues.",0,0,0,0,0,0,0.977858,6.0,0.954711,99
59ab8fd5-5d39-47cc-9840-24649ab46042,On the Importance of Hyperparameters and Data Augmentation for Self-Supervised Learning,12,0.0403325,0.312292,"Self-Supervised Learning (SSL) has become a very active area of Deep Learning
research where it is heavily used as a pre-training method for classification
and other tasks. However, the rapid pace of advancements in this area comes at
a price: training pipelines vary significantly across papers, which presents a
potentially crucial confounding factor. Here, we show that, indeed, the choice
of hyperparameters and data augmentation strategies can have a dramatic impact
on performance. To shed light on these neglected factors and help maximize the
power of SSL, we hyperparameterize these components and optimize them with
Bayesian optimization, showing improvements across multiple datasets for the
SimSiam SSL approach. Realizing the importance of data augmentations for SSL,
we also introduce a new automated data augmentation algorithm, GroupAugment,
which considers groups of augmentations and optimizes the sampling across
groups. In contrast to algorithms designed for supervised learning,
GroupAugment achieved consistently high linear evaluation accuracy across all
datasets we considered. Overall, our results indicate the importance and likely
underestimated role of data augmentation for SSL.",0,1,0,1,0,0,0.785794,6.0,0.803812,31
3f91487e-11db-4262-929c-5af558381fb9,A Theoretical Framework for AI Models Explainability with Application in Biomedicine,2,0.0120856,0.161416,"EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the
artificial intelligence community, with growing interest across methods and
domains. Much has been written about the subject, yet XAI still lacks shared
terminology and a framework capable of providing structural soundness to
explanations. In our work, we address these issues by proposing a novel
definition of explanation that is a synthesis of what can be found in the
literature. We recognize that explanations are not atomic but the combination
of evidence stemming from the model and its input-output mapping, and the human
interpretation of this evidence. Furthermore, we fit explanations into the
properties of faithfulness (i.e., the explanation being a true description of
the model's inner workings and decision-making process) and plausibility (i.e.,
how much the explanation looks convincing to the user). Using our proposed
theoretical framework simplifies how these properties are operationalized and
it provides new insight into common explanation methods that we analyze as case
studies.",0,0,0,0,0,0,0.526069,9.0,0.788718,42
240acf2e-100a-46cf-b5b2-3032cd243525,ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework,18,0.213714,0.86921,"In this paper, a computation efficient regression framework is presented for
estimating the 6D pose of rigid objects from a single RGB-D image, which is
applicable to handling symmetric objects. This framework is designed in a
simple architecture that efficiently extracts point-wise features from RGB-D
data using a fully convolutional network, called XYZNet, and directly regresses
the 6D pose without any post refinement. In the case of symmetric object, one
object has multiple ground-truth poses, and this one-to-many relationship may
lead to estimation ambiguity. In order to solve this ambiguity problem, we
design a symmetry-invariant pose distance metric, called average (maximum)
grouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the
regression network converge to the correct state, i.e., all minima in the
A(M)GPD loss surface are mapped to the correct poses. Extensive experiments on
YCB-Video and T-LESS datasets demonstrate the proposed framework's
substantially superior performance in top accuracy and low computational cost.",1,1,0,0,1,0,0.732128,8.0,0.833263,41
6522d3ae-56dd-448f-bd88-a451df7e35f9,Memorizing Transformers,125,0.198577,0.746492,"Language models typically need to be trained or finetuned in order to acquire
new knowledge, which involves updating their weights. We instead envision
language models that can simply read and memorize new data at inference time,
thus acquiring new knowledge immediately. In this work, we extend language
models with the ability to memorize the internal representations of past
inputs. We demonstrate that an approximate kNN lookup into a non-differentiable
memory of recent (key, value) pairs improves language modeling across various
benchmarks and tasks, including generic webtext (C4), math papers (arXiv),
books (PG-19), code (Github), as well as formal theorems (Isabelle). We show
that the performance steadily improves when we increase the size of memory up
to 262K tokens. On benchmarks including code and mathematics, we find that the
model is capable of making use of newly defined functions and theorems during
test time.",0,0,0,0,0,0,0.542534,4.0,0.536182,55
31e266f1-ecdb-4fb1-8d5f-e5ff7fefc4d1,Frontiers and Exact Learning of ELI Queries under DL-Lite Ontologies,12,0.0465642,0.361861,"We study ELI queries (ELIQs) in the presence of ontologies formulated in the
description logic DL-Lite. For the dialect DL-LiteH, we show that ELIQs have a
frontier (set of least general generalizations) that is of polynomial size and
can be computed in polynomial time. In the dialect DL-LiteF, in contrast,
frontiers may be infinite. We identify a natural syntactic restriction that
enables the same positive results as for DL-LiteH. We use out results on
frontiers to show that ELIQs are learnable in polynomial time in the presence
of a DL-LiteH / restricted DL-LiteF ontology in Angluin's framework of exact
learning with only membership queries.",0,0,0,0,0,0,0.00044646,16.0,0.417289,44
ac833b4f-895e-4572-b734-ad7f97743f55,CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation,65,0.1426,0.757249,"We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based
framework for panoptic segmentation designed around clustering. It rethinks the
existing transformer architectures used in segmentation and detection;
CMT-DeepLab considers the object queries as cluster centers, which fill the
role of grouping the pixels when applied to segmentation. The clustering is
computed with an alternating procedure, by first assigning pixels to the
clusters by their feature affinity, and then updating the cluster centers and
pixel features. Together, these operations comprise the Clustering Mask
Transformer (CMT) layer, which produces cross-attention that is denser and more
consistent with the final segmentation task. CMT-DeepLab improves the
performance over prior art significantly by 4.4% PQ, achieving a new
state-of-the-art of 55.7% PQ on the COCO test-dev set.",1,1,0,0,1,0,0.458254,8.0,0.737638,102
60fd210e-d2a6-4fc1-8511-8bb39ac457b0,Multimodal data matters: language model pre-training over structured and unstructured electronic health records,11,0.179363,0.473879,"As two important textual modalities in electronic health records (EHR), both
structured data (clinical codes) and unstructured data (clinical narratives)
have recently been increasingly applied to the healthcare domain. Most existing
EHR-oriented studies, however, either focus on a particular modality or
integrate data from different modalities in a straightforward manner, which
usually treats structured and unstructured data as two independent sources of
information about patient admission and ignore the intrinsic interactions
between them. In fact, the two modalities are documented during the same
encounter where structured data inform the documentation of unstructured data
and vice versa. In this paper, we proposed a Medical Multimodal Pre-trained
Language Model, named MedM-PLM, to learn enhanced EHR representations over
structured and unstructured data and explore the interaction of two modalities.
In MedM-PLM, two Transformer-based neural network components are firstly
adopted to learn representative characteristics from each modality. A
cross-modal module is then introduced to model their interactions. We
pre-trained MedM-PLM on the MIMIC-III dataset and verified the effectiveness of
the model on three downstream clinical tasks, i.e., medication recommendation,
30-day readmission prediction and ICD coding. Extensive experiments demonstrate
the power of MedM-PLM compared with state-of-the-art methods. Further analyses
and visualizations show the robustness of our model, which could potentially
provide more comprehensive interpretations for clinical decision-making.",1,1,0,0,1,0,0.715211,7.0,0.802643,107
2235b2da-4c2b-4c35-9aa4-27e07738a0e5,EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start,35,0.802928,0.960276,"We present EdiT5 - a novel semi-autoregressive text-editing model designed to
combine the strengths of non-autoregressive text-editing and autoregressive
decoding. EdiT5 is faster during inference than conventional
sequence-to-sequence (seq2seq) models, while being capable of modelling
flexible input-output transformations.
  This is achieved by decomposing the generation process into three sub-tasks:
(1) tagging to decide on the subset of input tokens to be preserved in the
output, (2) re-ordering to define their order in the output text, and (3)
insertion to infill the missing tokens that are not present in the input. The
tagging and re-ordering steps, which are responsible for generating the largest
portion of the output, are non-autoregressive, while the insertion step uses an
autoregressive decoder.
  Depending on the task, EdiT5 on average requires significantly fewer
autoregressive steps, demonstrating speedups of up to 25x when compared to
seq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5
checkpoint yielding comparable performance to T5 in high-resource settings when
evaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction,
and Decontextualization while clearly outperforming T5 in low-resource
settings.",1,1,0,0,0,0,0.969819,6.0,0.94058,42
db1cf5f2-c01c-4767-a1c4-2db5af3dc0ac,Sobolev Training for Implicit Neural Representations with Approximated Image Derivatives,3,0.0134045,0.122892,"Recently, Implicit Neural Representations (INRs) parameterized by neural
networks have emerged as a powerful and promising tool to represent different
kinds of signals due to its continuous, differentiable properties, showing
superiorities to classical discretized representations. However, the training
of neural networks for INRs only utilizes input-output pairs, and the
derivatives of the target output with respect to the input, which can be
accessed in some cases, are usually ignored. In this paper, we propose a
training paradigm for INRs whose target output is image pixels, to encode image
derivatives in addition to image values in the neural network. Specifically, we
use finite differences to approximate image derivatives. We show how the
training paradigm can be leveraged to solve typical INRs problems, i.e., image
regression and inverse rendering, and demonstrate this training paradigm can
improve the data-efficiency and generalization capabilities of INRs. The code
of our method is available at
\url{https://github.com/megvii-research/Sobolev_INRs}.",1,1,0,0,0,0,0.841039,6.0,0.833306,46
19ada7c0-2acb-41e5-a9c6-63187a21492e,UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization,11,0.0487596,0.265938,"The high annotation costs and diverse demands of various summarization tasks
motivate the development of few-shot summarization. However, despite the
emergence of many summarization tasks and datasets, the current training
paradigm for few-shot summarization systems ignores potentially shareable
knowledge in heterogeneous datasets. To this end, we propose \textsc{UniSumm},
a unified few-shot summarization model pre-trained with multiple summarization
tasks and can be prefix-tuned to excel at any few-shot summarization task.
Meanwhile, to better evaluate few-shot summarizers, under the principles of
diversity and robustness, we assemble and release a new benchmark
\textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of
few-shot samples for each task, covering diverse domains. Experimental results
and analysis show that \textsc{UniSumm} outperforms strong baselines by a large
margin across all sub-tasks in \textsc{SummZoo} under both automatic and human
evaluations and achieves comparable results in human evaluation compared with a
GPT-3.5 model.",1,1,0,1,1,0,0.308053,6.0,0.56526,73
54fae58a-f482-4035-bc90-fea11f533dcb,From Representation to Reasoning: Towards both Evidence and Commonsense Reasoning for Video Question-Answering,27,0.397683,0.372598,"Video understanding has achieved great success in representation learning,
such as video caption, video object grounding, and video descriptive
question-answer. However, current methods still struggle on video reasoning,
including evidence reasoning and commonsense reasoning. To facilitate deeper
video understanding towards video reasoning, we present the task of
Causal-VidQA, which includes four types of questions ranging from scene
description (description) to evidence reasoning (explanation) and commonsense
reasoning (prediction and counterfactual). For commonsense reasoning, we set up
a two-step solution by answering the question and providing a proper reason.
Through extensive experiments on existing VideoQA methods, we find that the
state-of-the-art methods are strong in descriptions but weak in reasoning. We
hope that Causal-VidQA can guide the research of video understanding from
representation learning to deeper reasoning. The dataset and related resources
are available at \url{https://github.com/bcmi/Causal-VidQA.git}.",1,0,1,1,0,0,0.849271,9.0,0.892038,57
84fcc7dd-c5f0-4851-8448-0447b400d477,Improving Graph-Based Text Representations with Character and Word Level N-grams,1,0.0182085,0.0615752,"Graph-based text representation focuses on how text documents are represented
as graphs for exploiting dependency information between tokens and documents
within a corpus. Despite the increasing interest in graph representation
learning, there is limited research in exploring new ways for graph-based text
representation, which is important in downstream natural language processing
tasks. In this paper, we first propose a new heterogeneous word-character text
graph that combines word and character n-gram nodes together with document
nodes, allowing us to better learn dependencies among these entities.
Additionally, we propose two new graph-based neural models, WCTextGCN and
WCTextGAT, for modeling our proposed text graph. Extensive experiments in text
classification and automatic text summarization benchmarks demonstrate that our
proposed models consistently outperform competitive baselines and
state-of-the-art graph-based models.",0,1,0,0,1,0,0.470966,9.0,0.771012,27
4c561c1e-0c94-42ec-84e5-cc5f27e4f03f,Comparative layer-wise analysis of self-supervised speech models,38,0.839691,0.837552,"Many self-supervised speech models, varying in their pre-training objective,
input modality, and pre-training data, have been proposed in the last few
years. Despite impressive successes on downstream tasks, we still have a
limited understanding of the properties encoded by the models and the
differences across models. In this work, we examine the intermediate
representations for a variety of recent models. Specifically, we measure
acoustic, phonetic, and word-level properties encoded in individual layers,
using a lightweight analysis tool based on canonical correlation analysis
(CCA). We find that these properties evolve across layers differently depending
on the model, and the variations relate to the choice of pre-training
objective. We further investigate the utility of our analyses for downstream
tasks by comparing the property trends with performance on speech recognition
and spoken language understanding tasks. We discover that CCA trends provide
reliable guidance to choose layers of interest for downstream tasks and that
single-layer performance often matches or improves upon using all layers,
suggesting implications for more efficient use of pre-trained models.",1,0,0,0,0,0,0.916534,3.0,0.766719,31
7504e7c2-e55c-412a-a882-f5730ad4ebc3,TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy Qur'an Using a Post-Processed Ensemble of BERT-based Models,7,0.0611275,0.224954,"In recent years, we witnessed great progress in different tasks of natural
language understanding using machine learning. Question answering is one of
these tasks which is used by search engines and social media platforms for
improved user experience. Arabic is the language of the Holy Qur'an; the sacred
text for 1.8 billion people across the world. Arabic is a challenging language
for Natural Language Processing (NLP) due to its complex structures. In this
article, we describe our attempts at OSACT5 Qur'an QA 2022 Shared Task, which
is a question answering challenge on the Holy Qur'an in Arabic. We propose an
ensemble learning model based on Arabic variants of BERT models. In addition,
we perform post-processing to enhance the model predictions. Our system
achieves a Partial Reciprocal Rank (pRR) score of 56.6% on the official test
set.",0,1,0,0,0,0,0.589472,5.0,0.654876,13
1f49c93d-9785-41b3-b51d-4d5489966cb7,Summarizing Patients Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models,18,0.449693,0.692584,"Automatically summarizing patients' main problems from daily progress notes
using natural language processing methods helps to battle against information
and cognitive overload in hospital settings and potentially assists providers
with computerized diagnostic decision support. Problem list summarization
requires a model to understand, abstract, and generate clinical documentation.
In this work, we propose a new NLP task that aims to generate a list of
problems in a patient's daily care plan using input from the provider's
progress notes during hospitalization. We investigate the performance of T5 and
BART, two state-of-the-art seq2seq transformer architectures, in solving this
problem. We provide a corpus built on top of progress notes from publicly
available electronic health record progress notes in the Medical Information
Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain
text, and we experiment with a data augmentation method and a domain adaptation
pre-training method to increase exposure to medical vocabulary and knowledge.
Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence
embedding, and F-score on medical concepts. Results show that T5 with domain
adaptive pre-training achieves significant performance gains compared to a
rule-based system and general domain pre-trained language models, indicating a
promising direction for tackling the problem summarization task.",1,1,1,1,0,0,0.705997,6.0,0.76547,49
7fbff6c9-104c-4b32-90c3-07d7a8e90727,Self-Supervised Learning of Image Scale and Orientation,9,0.0342351,0.225409,"We study the problem of learning to assign a characteristic pose, i.e., scale
and orientation, for an image region of interest. Despite its apparent
simplicity, the problem is non-trivial; it is hard to obtain a large-scale set
of image regions with explicit pose annotations that a model directly learns
from. To tackle the issue, we propose a self-supervised learning framework with
a histogram alignment technique. It generates pairs of image patches by random
rescaling/rotating and then train an estimator to predict their
scale/orientation values so that their relative difference is consistent with
the rescaling/rotating used. The estimator learns to predict a non-parametric
histogram distribution of scale/orientation without any supervision.
Experiments show that it significantly outperforms previous methods in
scale/orientation estimation and also improves image matching and 6 DoF camera
pose estimation by incorporating our patch poses into a matching process.",1,0,0,0,0,0,0.128375,11.0,0.673243,50
7edd5795-3e60-44a8-938d-b54f9b766e9b,Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language,9,0.0406381,0.544912,"To solve difficult tasks, humans ask questions to acquire knowledge from
external sources. In contrast, classical reinforcement learning agents lack
such an ability and often resort to exploratory behavior. This is exacerbated
as few present-day environments support querying for knowledge. In order to
study how agents can be taught to query external knowledge via language, we
first introduce two new environments: the grid-world-based Q-BabyAI and the
text-based Q-TextWorld. In addition to physical interactions, an agent can
query an external knowledge source specialized for these environments to gather
information. Second, we propose the ""Asking for Knowledge"" (AFK) agent, which
learns to generate language commands to query for meaningful knowledge that
helps solve the tasks. AFK leverages a non-parametric memory, a pointer
mechanism and an episodic exploration bonus to tackle (1) irrelevant
information, (2) a large query language space, (3) delayed reward for making
meaningful queries. Extensive experiments demonstrate that the AFK agent
outperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld
environments.",1,0,1,1,0,0,0.14476,10.0,0.653505,73
5ebe8324-2625-42ac-8465-8930b761ccf2,HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks,8,0.161164,0.534684,"Implicit neural representations (INRs) are a rapidly growing research field,
which provides alternative ways to represent multimedia signals. Recent
applications of INRs include image super-resolution, compression of
high-dimensional signals, or 3D rendering. However, these solutions usually
focus on visual data, and adapting them to the audio domain is not trivial.
Moreover, it requires a separately trained model for every data sample. To
address this limitation, we propose HyperSound, a meta-learning method
leveraging hypernetworks to produce INRs for audio signals unseen at training
time. We show that our approach can reconstruct sound waves with quality
comparable to other state-of-the-art models.",0,1,0,0,0,0,0.87514,8.0,0.8904,25
45c2e06f-4937-4013-9020-f68f34839539,Resolving Semantic Confusions for Improved Zero-Shot Detection,5,0.05545,0.0933065,"Zero-shot detection (ZSD) is a challenging task where we aim to recognize and
localize objects simultaneously, even when our model has not been trained with
visual samples of a few target (""unseen"") classes. Recently, methods employing
generative models like GANs have shown some of the best results, where
unseen-class samples are generated based on their semantics by a GAN trained on
seen-class data, enabling vanilla object detectors to recognize unseen objects.
However, the problem of semantic confusion still remains, where the model is
sometimes unable to distinguish between semantically-similar classes. In this
work, we propose to train a generative model incorporating a triplet loss that
acknowledges the degree of dissimilarity between classes and reflects them in
the generated samples. Moreover, a cyclic-consistency loss is also enforced to
ensure that generated visual samples of a class highly correspond to their own
semantics. Extensive experiments on two benchmark ZSD datasets - MSCOCO and
PASCAL-VOC - demonstrate significant gains over the current ZSD methods,
reducing semantic confusion and improving detection for the unseen classes.",1,1,0,0,1,0,0.246392,12.0,0.760653,63
0930e7f6-33da-4f5c-aa3a-e3b0b8f5f47e,Conflict-Based Search for Explainable Multi-Agent Path Finding,5,0.30837,0.551247,"In the Multi-Agent Path Finding (MAPF) problem, the goal is to find
non-colliding paths for agents in an environment, such that each agent reaches
its goal from its initial location. In safety-critical applications, a human
supervisor may want to verify that the plan is indeed collision-free. To this
end, a recent work introduces a notion of explainability for MAPF based on a
visualization of the plan as a short sequence of images representing time
segments, where in each time segment the trajectories of the agents are
disjoint. Then, the explainable MAPF problem asks for a set of non-colliding
paths that admits a short-enough explanation. Explainable MAPF adds a new
difficulty to MAPF, in that it is NP-hard with respect to the size of the
environment, and not just the number of agents. Thus, traditional MAPF
algorithms are not equipped to directly handle explainable-MAPF. In this work,
we adapt Conflict Based Search (CBS), a well-studied algorithm for MAPF, to
handle explainable MAPF. We show how to add explainability constraints on top
of the standard CBS tree and its underlying A* search. We examine the
usefulness of this approach and, in particular, the tradeoff between planning
time and explainability.",0,1,0,0,0,0,0.827184,6.0,0.825555,30
dbe5dcc6-7ff3-46a6-9b8b-70d8f1409a4c,MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection,36,0.264082,0.847963,"Due to the inherent ill-posed nature of 2D-3D projection, monocular 3D object
detection lacks accurate depth recovery ability. Although the deep neural
network (DNN) enables monocular depth-sensing from high-level learned features,
the pixel-level cues are usually omitted due to the deep convolution mechanism.
To benefit from both the powerful feature representation in DNN and pixel-level
geometric constraints, we reformulate the monocular object depth estimation as
a progressive refinement problem and propose a joint semantic and geometric
cost volume to model the depth error. Specifically, we first leverage neural
networks to learn the object position, dimension, and dense normalized 3D
object coordinates. Based on the object depth, the dense coordinates patch
together with the corresponding object features is reprojected to the image
space to build a cost volume in a joint semantic and geometric error manner.
The final depth is obtained by feeding the cost volume to a refinement network,
where the distribution of semantic and geometric error is regularized by direct
depth supervision. Through effectively mitigating depth error by the refinement
framework, we achieve state-of-the-art results on both the KITTI and Waymo
datasets.",1,1,0,0,1,0,0.790481,5.0,0.767426,40
332ba513-6202-4372-9be8-2866a14101aa,Deep Hyperspectral-Depth Reconstruction Using Single Color-Dot Projection,3,0.0433757,0.337256,"Depth reconstruction and hyperspectral reflectance reconstruction are two
active research topics in computer vision and image processing. Conventionally,
these two topics have been studied separately using independent imaging setups
and there is no existing method which can acquire depth and spectral
reflectance simultaneously in one shot without using special hardware. In this
paper, we propose a novel single-shot hyperspectral-depth reconstruction method
using an off-the-shelf RGB camera and projector. Our method is based on a
single color-dot projection, which simultaneously acts as structured light for
depth reconstruction and spatially-varying color illuminations for
hyperspectral reflectance reconstruction. To jointly reconstruct the depth and
the hyperspectral reflectance from a single color-dot image, we propose a novel
end-to-end network architecture that effectively incorporates a geometric
color-dot pattern loss and a photometric hyperspectral reflectance loss.
Through the experiments, we demonstrate that our hyperspectral-depth
reconstruction method outperforms the combination of an existing
state-of-the-art single-shot hyperspectral reflectance reconstruction method
and depth reconstruction method.",1,0,0,0,0,0,0.0295971,11.0,0.535042,38
e5d5771b-0c43-491d-ab68-55876206128c,Constrained Dynamic Movement Primitives for Safe Learning of Motor Skills,5,0.199909,0.744271,"Dynamic movement primitives are widely used for learning skills which can be
demonstrated to a robot by a skilled human or controller. While their
generalization capabilities and simple formulation make them very appealing to
use, they possess no strong guarantees to satisfy operational safety
constraints for a task. In this paper, we present constrained dynamic movement
primitives (CDMP) which can allow for constraint satisfaction in the robot
workspace. We present a formulation of a non-linear optimization to perturb the
DMP forcing weights regressed by locally-weighted regression to admit a Zeroing
Barrier Function (ZBF), which certifies workspace constraint satisfaction. We
demonstrate the proposed CDMP under different constraints on the end-effector
movement such as obstacle avoidance and workspace constraints on a physical
robot. A video showing the implementation of the proposed algorithm using
different manipulators in different environments could be found here
https://youtu.be/hJegJJkJfys.",0,1,0,0,0,0,0.112886,17.0,0.780499,30
ba596697-dd0e-408d-83c6-8cb00aed4432,Unified Chinese License Plate Detection and Recognition with High Efficiency,22,0.677213,0.997996,"Recently, deep learning-based methods have reached an excellent performance
on License Plate (LP) detection and recognition tasks. However, it is still
challenging to build a robust model for Chinese LPs since there are not enough
large and representative datasets. In this work, we propose a new dataset named
Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP
images as a supplement to the existing public benchmarks. The images are mainly
captured with electronic monitoring systems with detailed annotations. To our
knowledge, CRPD is the largest public multi-objective Chinese LP dataset with
annotations of vertices. With CRPD, a unified detection and recognition network
with high efficiency is presented as the baseline. The network is end-to-end
trainable with totally real-time inference efficiency (30 fps with 640p). The
experiments on several public benchmarks demonstrate that our method has
reached competitive performance. The code and dataset will be publicly
available at https://github.com/yxgong0/CRPD.",1,1,1,1,0,0,0.872346,10.0,0.911251,38
07dfdb23-8990-4842-9db0-9c932cd4afbe,Estimating the Uncertainty in Emotion Class Labels with Utterance-Specific Dirichlet Priors,7,0.0554682,0.398536,"Emotion recognition is a key attribute for artificial intelligence systems
that need to naturally interact with humans. However, the task definition is
still an open problem due to the inherent ambiguity of emotions. In this paper,
a novel Bayesian training loss based on per-utterance Dirichlet prior
distributions is proposed for verbal emotion recognition, which models the
uncertainty in one-hot labels created when human annotators assign the same
utterance to different emotion classes. An additional metric is used to
evaluate the performance by detection test utterances with high labelling
uncertainty. This removes a major limitation that emotion classification
systems only consider utterances with labels where the majority of annotators
agree on the emotion class. Furthermore, a frequentist approach is studied to
leverage the continuous-valued ""soft"" labels obtained by averaging the one-hot
labels. We propose a two-branch model structure for emotion classification on a
per-utterance basis, which achieves state-of-the-art classification results on
the widely used IEMOCAP dataset. Based on this, uncertainty estimation
experiments were performed. The best performance in terms of the area under the
precision-recall curve when detecting utterances with high uncertainty was
achieved by interpolating the Bayesian training loss with the Kullback-Leibler
divergence training loss for the soft labels. The generality of the proposed
approach was verified using the MSP-Podcast dataset which yielded the same
pattern of results.",0,0,0,0,1,0,0.134825,9.0,0.60648,81
5d222704-2b99-42e4-b24a-30a32d544cc7,Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks,15,0.0293081,0.311146,"The field of Natural Language Processing has experienced a dramatic leap in
capabilities with the recent introduction of huge Language Models. Despite this
success, natural language problems that involve several compounded steps are
still practically unlearnable, even by the largest LMs. This complies with
experimental failures for end-to-end learning of composite problems that were
demonstrated in a variety of domains. An effective mitigation is to introduce
intermediate supervision for solving sub-tasks of the compounded problem.
Recently, several works have demonstrated high gains by taking a
straightforward approach for incorporating intermediate supervision in
compounded natural language problems: the sequence-to-sequence LM is fed with
an augmented input, in which the decomposed tasks' labels are simply
concatenated to the original input. In this paper, we prove a positive learning
result that motivates these recent efforts. We show that when concatenating
intermediate supervision to the input and training a sequence-to-sequence model
on this modified input, unlearnable composite problems can become learnable. We
show that this is true for any family of tasks which on the one hand, are
unlearnable, and on the other hand, can be decomposed into a polynomial number
of simple sub-tasks, each of which depends only on O(1) previous sub-task
results. Beyond motivating contemporary empirical efforts for incorporating
intermediate supervision in sequence-to-sequence language models, our positive
theoretical result is the first of its kind in the landscape of results on the
benefits of intermediate supervision for neural-network learning: Until now,
all theoretical results on the subject are negative, i.e., show cases where
learning is impossible without intermediate supervision, while our result is
positive, showing that learning is facilitated in the presence of intermediate
supervision.",0,0,0,0,0,0,0.215907,6.0,0.496134,62
e81d14ed-0a6b-4c12-ac84-80fb1d417bfd,GMF: General Multimodal Fusion Framework for Correspondence Outlier Rejection,8,0.119371,0.817084,"Rejecting correspondence outliers enables to boost the correspondence
quality, which is a critical step in achieving high point cloud registration
accuracy. The current state-of-the-art correspondence outlier rejection methods
only utilize the structure features of the correspondences. However, texture
information is critical to reject the correspondence outliers in our human
vision system. In this paper, we propose General Multimodal Fusion (GMF) to
learn to reject the correspondence outliers by leveraging both the structure
and texture information. Specifically, two cross-attention-based fusion layers
are proposed to fuse the texture information from paired images and structure
information from point correspondences. Moreover, we propose a convolutional
position encoding layer to enhance the difference between Tokens and enable the
encoding feature pay attention to neighbor information. Our position encoding
layer will make the cross-attention operation integrate both local and global
information. Experiments on multiple datasets(3DMatch, 3DLoMatch, KITTI) and
recent state-of-the-art models (3DRegNet, DGR, PointDSC) prove that our GMF
achieves wide generalization ability and consistently improves the point cloud
registration accuracy. Furthermore, several ablation studies demonstrate the
robustness of the proposed GMF on different loss functions, lighting conditions
and noises.The code is available at https://github.com/XiaoshuiHuang/GMF.",0,1,0,0,1,0,0.51573,8.0,0.758642,32
f8829d4c-3cdd-430d-b92c-9bcb8e2bfdd7,Ultrahyperbolic Knowledge Graph Embeddings,21,0.140057,0.727421,"Recent knowledge graph (KG) embeddings have been advanced by hyperbolic
geometry due to its superior capability for representing hierarchies. The
topological structures of real-world KGs, however, are rather heterogeneous,
i.e., a KG is composed of multiple distinct hierarchies and non-hierarchical
graph structures. Therefore, a homogeneous (either Euclidean or hyperbolic)
geometry is not sufficient for fairly representing such heterogeneous
structures. To capture the topological heterogeneity of KGs, we present an
ultrahyperbolic KG embedding (UltraE) in an ultrahyperbolic (or
pseudo-Riemannian) manifold that seamlessly interleaves hyperbolic and
spherical manifolds. In particular, we model each relation as a
pseudo-orthogonal transformation that preserves the pseudo-Riemannian bilinear
form. The pseudo-orthogonal transformation is decomposed into various operators
(i.e., circular rotations, reflections and hyperbolic rotations), allowing for
simultaneously modeling heterogeneous structures as well as complex relational
patterns. Experimental results on three standard KGs show that UltraE
outperforms previous Euclidean- and hyperbolic-based approaches.",0,0,0,0,1,0,0.122971,9.0,0.595516,39
1eff7b31-2e1a-4c7d-9e93-7d6d1f71ccb6,OCR Improves Machine Translation for Low-Resource Languages,7,0.165413,0.789373,"We aim to investigate the performance of current OCR systems on low resource
languages and low resource scripts. We introduce and make publicly available a
novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with
noise, for 60 low-resource languages in low resource scripts. We evaluate
state-of-the-art OCR systems on our benchmark and analyse most common errors.
We show that OCR monolingual data is a valuable resource that can increase
performance of Machine Translation models, when used in backtranslation. We
then perform an ablation study to investigate how OCR errors impact Machine
Translation performance and determine what is the minimum level of OCR quality
needed for the monolingual data to be useful for Machine Translation.",1,1,1,1,0,0,0.810949,6.0,0.816805,29
494f3a3d-79b7-4959-809d-45a172582e84,Multi-Document Summarization with Centroid-Based Pretraining,5,0.131346,0.882681,"In Multi-Document Summarization (MDS), the input can be modeled as a set of
documents, and the output is its summary. In this paper, we focus on
pretraining objectives for MDS. Specifically, we introduce a novel pretraining
objective, which involves selecting the ROUGE-based centroid of each document
cluster as a proxy for its summary. Our objective thus does not require human
written summaries and can be utilized for pretraining on a dataset consisting
solely of document sets. Through zero-shot, few-shot, and fully supervised
experiments on multiple MDS datasets, we show that our model Centrum is better
or comparable to a state-of-the-art model. We make the pretrained and
fine-tuned models freely available to the research community
https://github.com/ratishsp/centrum.",1,1,0,0,1,0,0.932316,6.0,0.896864,22
4f0e9897-b6ce-4306-96be-0b3c2cbe59f9,RELIC: Retrieving Evidence for Literary Claims,11,0.144243,0.768384,"Humanities scholars commonly provide evidence for claims that they make about
a work of literature (e.g., a novel) in the form of quotations from the work.
We collect a large-scale dataset (RELiC) of 78K literary quotations and
surrounding critical analysis and use it to formulate the novel task of
literary evidence retrieval, in which models are given an excerpt of literary
analysis surrounding a masked quotation and asked to retrieve the quoted
passage from the set of all passages in the work. Solving this retrieval task
requires a deep understanding of complex literary and linguistic phenomena,
which proves challenging to methods that overwhelmingly rely on lexical and
semantic similarity matching. We implement a RoBERTa-based dense passage
retriever for this task that outperforms existing pretrained information
retrieval baselines; however, experiments and analysis by human domain experts
indicate that there is substantial room for improvement over our dense
retriever.",0,0,1,1,0,0,0.256886,9.0,0.686247,77
0448d194-ac49-4724-996b-17b8aa5d3385,Improving Language Model Prompting in Support of Semi-autonomous Task Learning,8,0.0628326,0.106543,"Language models (LLMs) offer potential as a source of knowledge for agents
that need to acquire new task competencies within a performance environment. We
describe efforts toward a novel agent capability that can construct cues (or
""prompts"") that result in useful LLM responses for an agent learning a new
task. Importantly, responses must not only be ""reasonable"" (a measure used
commonly in research on knowledge extraction from LLMs) but also specific to
the agent's task context and in a form that the agent can interpret given its
native language capacities. We summarize a series of empirical investigations
of prompting strategies and evaluate responses against the goals of targeted
and actionable responses for task learning. Our results demonstrate that
actionable task knowledge can be obtained from LLMs in support of online agent
task learning.",0,0,0,0,0,0,0.624591,4.0,0.592523,18
56358f29-650c-402a-ad9a-c27a7185695b,Towards Summary Candidates Fusion,11,0.117368,0.848198,"Sequence-to-sequence deep neural models fine-tuned for abstractive
summarization can achieve great performance on datasets with enough human
annotations. Yet, it has been shown that they have not reached their full
potential, with a wide gap between the top beam search output and the oracle
beam. Recently, re-ranking methods have been proposed, to learn to select a
better summary candidate. However, such methods are limited by the summary
quality aspects captured by the first-stage candidates. To bypass this
limitation, we propose a new paradigm in second-stage abstractive summarization
called SummaFusion that fuses several summary candidates to produce a novel
abstractive second-stage summary. Our method works well on several
summarization datasets, improving both the ROUGE scores and qualitative
properties of fused summaries. It is especially good when the candidates to
fuse are worse, such as in the few-shot setup where we set a new
state-of-the-art. We will make our code and checkpoints available at
https://github.com/ntunlp/SummaFusion/.",1,0,0,0,0,1,0.670212,6.0,0.749049,43
8a6e7c97-d852-49be-9cf8-0434382478c1,Transformer-Based Self-Supervised Learning for Emotion Recognition,17,0.147551,0.63353,"In order to exploit representations of time-series signals, such as
physiological signals, it is essential that these representations capture
relevant information from the whole signal. In this work, we propose to use a
Transformer-based model to process electrocardiograms (ECG) for emotion
recognition. Attention mechanisms of the Transformer can be used to build
contextualized representations for a signal, giving more importance to relevant
parts. These representations may then be processed with a fully-connected
network to predict emotions. To overcome the relatively small size of datasets
with emotional labels, we employ self-supervised learning. We gathered several
ECG datasets with no labels of emotion to pre-train our model, which we then
fine-tuned for emotion recognition on the AMIGOS dataset. We show that our
approach reaches state-of-the-art performances for emotion recognition using
ECG signals on AMIGOS. More generally, our experiments show that transformers
and pre-training are promising strategies for emotion recognition with
physiological signals.",0,1,0,0,1,0,0.532162,5.0,0.623129,61
58568e53-8614-4a9e-b97f-8e21224a72ae,JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus,11,0.318803,0.818451,"Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes.",0,1,0,1,0,0,0.780713,12.0,0.900629,36
5df0d6d4-cea1-4254-982d-c50ce0115e21,Compositional Generalisation with Structured Reordering and Fertility Layers,5,0.059204,0.299989,"Seq2seq models have been shown to struggle with compositional generalisation,
i.e. generalising to new and potentially more complex structures than seen
during training. Taking inspiration from grammar-based models that excel at
compositional generalisation, we present a flexible end-to-end differentiable
neural model that composes two structural operations: a fertility step, which
we introduce in this work, and a reordering step based on previous work (Wang
et al., 2021). To ensure differentiability, we use the expected value of each
step. Our model outperforms seq2seq models by a wide margin on challenging
compositional splits of realistic semantic parsing tasks that require
generalisation to longer examples. It also compares favourably to other models
targeting compositional generalisation.",1,0,0,1,0,0,0.314933,9.0,0.713148,49
cceac6cc-8a49-4ea9-aa19-8b448655490b,Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET,34,0.397345,0.801211,"Neural metrics have achieved impressive correlation with human judgements in
the evaluation of machine translation systems, but before we can safely
optimise towards such metrics, we should be aware of (and ideally eliminate)
biases toward bad translations that receive high scores. Our experiments show
that sample-based Minimum Bayes Risk decoding can be used to explore and
quantify such weaknesses. When applying this strategy to COMET for en-de and
de-en, we find that COMET models are not sensitive enough to discrepancies in
numbers and named entities. We further show that these biases are hard to fully
remove by simply training on additional synthetic data and release our code and
data for facilitating further experiments.",1,0,0,0,0,0,0.519093,8.0,0.759837,55
396f78ab-3bcc-4fe0-be8c-74ab6e101ed3,"The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",20,0.132317,0.940257,"Construction Grammar (CxG) is a paradigm from cognitive linguistics
emphasising the connection between syntax and semantics. Rather than rules that
operate on lexical items, it posits constructions as the central building
blocks of language, i.e., linguistic units of different granularity that
combine syntax and semantics. As a first step towards assessing the
compatibility of CxG with the syntactic and semantic knowledge demonstrated by
state-of-the-art pretrained language models (PLMs), we present an investigation
of their capability to classify and understand one of the most commonly studied
constructions, the English comparative correlative (CC). We conduct experiments
examining the classification accuracy of a syntactic probe on the one hand and
the models' behaviour in a semantic application task on the other, with BERT,
RoBERTa, and DeBERTa as the example PLMs. Our results show that all three
investigated PLMs are able to recognise the structure of the CC but fail to use
its meaning. While human-like performance of PLMs on many NLP tasks has been
alleged, this indicates that PLMs still suffer from substantial shortcomings in
central domains of linguistic knowledge.",1,0,0,0,0,0,0.189013,10.0,0.682754,52
a1cd9850-121a-432b-a61a-1cb2c16984db,Towards Generalizable and Robust Text-to-SQL Parsing,5,0.157192,0.339921,"Text-to-SQL parsing tackles the problem of mapping natural language questions
to executable SQL queries. In practice, text-to-SQL parsers often encounter
various challenging scenarios, requiring them to be generalizable and robust.
While most existing work addresses a particular generalization or robustness
challenge, we aim to study it in a more comprehensive manner. In specific, we
believe that text-to-SQL parsers should be (1) generalizable at three levels of
generalization, namely i.i.d., zero-shot, and compositional, and (2) robust
against input perturbations. To enhance these capabilities of the parser, we
propose a novel TKK framework consisting of Task decomposition, Knowledge
acquisition, and Knowledge composition to learn text-to-SQL parsing in stages.
By dividing the learning process into multiple stages, our framework improves
the parser's ability to acquire general SQL knowledge instead of capturing
spurious patterns, making it more generalizable and robust. Experimental
results under various generalization and robustness settings show that our
framework is effective in all scenarios and achieves state-of-the-art
performance on the Spider, SParC, and CoSQL datasets. Code can be found at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/tkk.",1,1,0,0,1,0,0.922056,5.0,0.86547,53
ef002ab3-db60-4218-9d9c-c35a4b5de99e,Uncertainty estimation for Cross-dataset performance in Trajectory prediction,10,0.0414613,0.633854,"While a lot of work has been carried on developing trajectory prediction
methods, and various datasets have been proposed for benchmarking this task,
little study has been done so far on the generalizability and the
transferability of these methods across dataset. In this paper, we observe the
performance of two of the latest state-of-the-art trajectory prediction methods
across four different datasets (Argoverse, NuScenes, Interaction, Shifts). This
analysis allows to gain some insights on the generalizability proprieties of
most recent trajectory prediction models and to analyze which dataset is more
representative of real driving scenes and therefore enables better
transferability. Furthermore we present a novel method to estimate prediction
uncertainty and show how it could be used to achieve better performance across
datasets.",0,1,0,0,0,0,0.629107,3.0,0.46079,60
b98b5da4-03db-4a40-8992-93590526bf3b,BigColor: Colorization using a Generative Color Prior for Natural Images,29,0.0439464,0.820194,"For realistic and vivid colorization, generative priors have recently been
exploited. However, such generative priors often fail for in-the-wild complex
images due to their limited representation space. In this paper, we propose
BigColor, a novel colorization approach that provides vivid colorization for
diverse in-the-wild images with complex structures. While previous generative
priors are trained to synthesize both image structures and colors, we learn a
generative color prior to focus on color synthesis given the spatial structure
of an image. In this way, we reduce the burden of synthesizing image structures
from the generative prior and expand its representation space to cover diverse
images. To this end, we propose a BigGAN-inspired encoder-generator network
that uses a spatial feature map instead of a spatially-flattened BigGAN latent
code, resulting in an enlarged representation space. Our method enables robust
colorization for diverse inputs in a single forward pass, supports arbitrary
input resolutions, and provides multi-modal colorization results. We
demonstrate that BigColor significantly outperforms existing methods especially
on in-the-wild images with complex structures.",0,1,0,0,1,0,0.0181233,13.0,0.568395,39
11a61a75-39fb-48f1-9e25-df616cd77c99,Hierarchical Decision Transformer,6,0.0443034,0.593557,"Sequence models in reinforcement learning require task knowledge to estimate
the task policy. This paper presents a hierarchical algorithm for learning a
sequence model from demonstrations. The high-level mechanism guides the
low-level controller through the task by selecting sub-goals for the latter to
reach. This sequence replaces the returns-to-go of previous methods, improving
its performance overall, especially in tasks with longer episodes and scarcer
rewards. We validate our method in multiple tasks of OpenAIGym, D4RL and
RoboMimic benchmarks. Our method outperforms the baselines in eight out of ten
tasks of varied horizons and reward frequencies without prior task knowledge,
showing the advantages of the hierarchical model approach for learning from
demonstrations using a sequence model.",0,1,0,0,1,0,0.458254,6.0,0.650184,34
79a558eb-cac3-4d00-9885-7f7f0d11ad36,Boosting 3D Object Detection by Simulating Multimodality on Point Clouds,19,0.115988,0.673167,"This paper presents a new approach to boost a single-modality (LiDAR) 3D
object detector by teaching it to simulate features and responses that follow a
multi-modality (LiDAR-image) detector. The approach needs LiDAR-image data only
when training the single-modality detector, and once well-trained, it only
needs LiDAR data at inference. We design a novel framework to realize the
approach: response distillation to focus on the crucial response samples and
avoid the background samples; sparse-voxel distillation to learn voxel
semantics and relations from the estimated crucial voxels; a fine-grained
voxel-to-point distillation to better attend to features of small and distant
objects; and instance distillation to further enhance the deep-feature
consistency. Experimental results on the nuScenes dataset show that our
approach outperforms all SOTA LiDAR-only 3D detectors and even surpasses the
baseline LiDAR-image detector on the key NDS metric, filling 72% mAP gap
between the single- and multi-modality detectors.",0,1,0,0,1,0,0.81153,5.0,0.780535,57
11663ed5-1435-40a2-9e6b-e7d61101b261,"Computational analyses of the topics, sentiments, literariness, creativity and beauty of texts in a large Corpus of English Literature",3,0.0921315,0.112306,"The Gutenberg Literary English Corpus (GLEC, Jacobs, 2018a) provides a rich
source of textual data for research in digital humanities, computational
linguistics or neurocognitive poetics. In this study we address differences
among the different literature categories in GLEC, as well as differences
between authors. We report the results of three studies providing i) topic and
sentiment analyses for six text categories of GLEC (i.e., children and youth,
essays, novels, plays, poems, stories) and its >100 authors, ii) novel measures
of semantic complexity as indices of the literariness, creativity and book
beauty of the works in GLEC (e.g., Jane Austen's six novels), and iii) two
experiments on text classification and authorship recognition using novel
features of semantic complexity. The data on two novel measures estimating a
text's literariness, intratextual variance and stepwise distance (van
Cranenburgh et al., 2019) revealed that plays are the most literary texts in
GLEC, followed by poems and novels. Computation of a novel index of text
creativity (Gray et al., 2016) revealed poems and plays as the most creative
categories with the most creative authors all being poets (Milton, Pope, Keats,
Byron, or Wordsworth). We also computed a novel index of perceived beauty of
verbal art (Kintsch, 2012) for the works in GLEC and predict that Emma is the
theoretically most beautiful of Austen's novels. Finally, we demonstrate that
these novel measures of semantic complexity are important features for text
classification and authorship recognition with overall predictive accuracies in
the range of .75 to .97. Our data pave the way for future computational and
empirical studies of literature or experiments in reading psychology and offer
multiple baselines and benchmarks for analysing and validating other book
corpora.",0,0,0,0,0,1,0.186416,12.0,0.734347,47
eb3434d6-a793-4281-99c9-ab83ae9b7c90,ADVISE: ADaptive Feature Relevance and VISual Explanations for Convolutional Neural Networks,1,0.0107302,0.0565552,"To equip Convolutional Neural Networks (CNNs) with explainability, it is
essential to interpret how opaque models take specific decisions, understand
what causes the errors, improve the architecture design, and identify unethical
biases in the classifiers. This paper introduces ADVISE, a new explainability
method that quantifies and leverages the relevance of each unit of the feature
map to provide better visual explanations. To this end, we propose using
adaptive bandwidth kernel density estimation to assign a relevance score to
each unit of the feature map with respect to the predicted class. We also
propose an evaluation protocol to quantitatively assess the visual
explainability of CNN models. We extensively evaluate our idea in the image
classification task using AlexNet, VGG16, ResNet50, and Xception pretrained on
ImageNet. We compare ADVISE with the state-of-the-art visual explainable
methods and show that the proposed method outperforms competing approaches in
quantifying feature-relevance and visual explainability while maintaining
competitive time complexity. Our experiments further show that ADVISE fulfils
the sensitivity and implementation independence axioms while passing the sanity
checks. The implementation is accessible for reproducibility purposes on
https://github.com/dehshibi/ADVISE.",1,0,0,0,0,0,0.412964,11.0,0.796428,61
d132656b-9ee6-4f89-9c58-ac8fd0b3f2fb,Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021,15,0.23316,0.843807,"Automatic detection of fake news is a highly important task in the
contemporary world. This study reports the 2nd shared task called
UrduFake@FIRE2021 on identifying fake news detection in Urdu. The goal of the
shared task is to motivate the community to come up with efficient methods for
solving this vital problem, particularly for the Urdu language. The task is
posed as a binary classification problem to label a given news article as a
real or a fake news article. The organizers provide a dataset comprising news
in five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and
(v) Business, split into training and testing sets. The training set contains
1300 annotated news articles -- 750 real news, 550 fake news, while the testing
set contains 300 news articles -- 200 real, 100 fake news. 34 teams from 7
different countries (China, Egypt, Israel, India, Mexico, Pakistan, and UAE)
registered to participate in the UrduFake@FIRE2021 shared task. Out of those,
18 teams submitted their experimental results, and 11 of those submitted their
technical reports, which is substantially higher compared to the UrduFake
shared task in 2020 when only 6 teams submitted their technical reports. The
technical reports submitted by the participants demonstrated different data
representation techniques ranging from count-based BoW features to word vector
embeddings as well as the use of numerous machine learning algorithms ranging
from traditional SVM to various neural network architectures including
Transformers such as BERT and RoBERTa. In this year's competition, the best
performing system obtained an F1-macro score of 0.679, which is lower than the
past year's best result of 0.907 F1-macro. Admittedly, while training sets from
the past and the current years overlap to a large extent, the testing set
provided this year is completely different.",1,1,0,0,0,0,0.357976,8.0,0.697083,59
d0728487-f21f-4293-af6c-5f3cf4e1c7b6,A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems,9,0.0494408,0.187105,"Self-supervised models for speech processing emerged recently as popular
foundation blocks in speech processing pipelines. These models are pre-trained
on unlabeled audio data and then used in speech processing downstream tasks
such as automatic speech recognition (ASR) or speech translation (ST). Since
these models are now used in research and industrial systems alike, it becomes
necessary to understand the impact caused by some features such as gender
distribution within pre-training data. Using French as our investigation
language, we train and compare gender-specific wav2vec 2.0 models against
models containing different degrees of gender balance in their pre-training
data. The comparison is performed by applying these models to two
speech-to-text downstream tasks: ASR and ST. Results show the type of
downstream integration matters. We observe lower overall performance using
gender-specific pre-training before fine-tuning an end-to-end ASR system.
However, when self-supervised models are used as feature extractors, the
overall ASR and ST results follow more complex patterns in which the balanced
pre-trained model does not necessarily lead to the best results. Lastly, our
crude 'fairness' metric, the relative performance difference measured between
female and male test sets, does not display a strong variation from balanced to
gender-specific pre-trained wav2vec 2.0 models.",0,1,0,1,0,0,0.40601,5.0,0.54767,46
35512f5d-e004-4448-a2ef-9d6682fc6a80,Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization,30,0.191192,0.875828,"This paper presents Z-Code++, a new pre-trained language model optimized for
abstractive text summarization. The model extends the state of the art
encoder-decoder model using three techniques. First, we use a two-phase
pre-training process to improve model's performance on low-resource
summarization tasks. The model is first pre-trained using text corpora for
language understanding, and then is continually pre-trained on summarization
corpora for grounded text generation. Second, we replace self-attention layers
in the encoder with disentangled attention layers, where each word is
represented using two vectors that encode its content and position,
respectively. Third, we use fusion-in-encoder, a simple yet effective method of
encoding long sequences in a hierarchical manner. Z-Code++ creates new state of
the art on 9 out of 13 text summarization tasks across 5 languages. Our model
is parameter-efficient in that it outperforms the 600x larger PaLM-540B on
XSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and
few-shot settings, our model substantially outperforms the competing models.",0,1,0,0,1,0,0.797966,5.0,0.772028,67
3658edac-0c1b-4d82-8577-da8405c42d45,Bi-Noising Diffusion: Towards Conditional Diffusion Models with Generative Restoration Priors,4,0.0612825,0.0671218,"Conditional diffusion probabilistic models can model the distribution of
natural images and can generate diverse and realistic samples based on given
conditions. However, oftentimes their results can be unrealistic with
observable color shifts and textures. We believe that this issue results from
the divergence between the probabilistic distribution learned by the model and
the distribution of natural images. The delicate conditions gradually enlarge
the divergence during each sampling timestep. To address this issue, we
introduce a new method that brings the predicted samples to the training data
manifold using a pretrained unconditional diffusion model. The unconditional
model acts as a regularizer and reduces the divergence introduced by the
conditional model at each sampling step. We perform comprehensive experiments
to demonstrate the effectiveness of our approach on super-resolution,
colorization, turbulence removal, and image-deraining tasks. The improvements
obtained by our method suggest that the priors can be incorporated as a general
plugin for improving conditional diffusion models.",1,0,0,0,0,0,0.954358,6.0,0.919623,42
771af564-1f55-4264-a941-90dc101de501,Exploring Extreme Parameter Compression for Pre-trained Language Models,15,0.0541389,0.26565,"Recent work explored the potential of large-scale Transformer-based
pre-trained models, especially Pre-trained Language Models (PLMs) in natural
language processing. This raises many concerns from various perspectives, e.g.,
financial costs and carbon emissions. Compressing PLMs like BERT with
negligible performance loss for faster inference and cheaper deployment has
attracted much attention. In this work, we aim to explore larger compression
ratios for PLMs, among which tensor decomposition is a potential but
under-investigated one. Two decomposition and reconstruction protocols are
further proposed to improve the effectiveness and efficiency during
compression. Our compressed BERT with ${1}/{7}$ parameters in Transformer
layers performs on-par with, sometimes slightly better than the original BERT
in GLUE benchmark. A tiny version achieves $96.7\%$ performance of BERT-base
with $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding
the embedding layer) and $2.7 \times$ faster on inference. To show that the
proposed method is orthogonal to existing compression methods like knowledge
distillation, we also explore the benefit of the proposed method on a distilled
BERT.",0,1,0,0,0,0,0.521474,6.0,0.680909,69
b00bba84-6a85-4451-9b8d-3f75d178d682,A Curriculum Learning Approach for Multi-domain Text Classification Using Keyword weight Ranking,1,0.0237023,0.14201,"Text classification is a very classic NLP task, but it has two prominent
shortcomings: On the one hand, text classification is deeply domain-dependent.
That is, a classifier trained on the corpus of one domain may not perform so
well in another domain. On the other hand, text classification models require a
lot of annotated data for training. However, for some domains, there may not
exist enough annotated data. Therefore, it is valuable to investigate how to
efficiently utilize text data from different domains to improve the performance
of models in various domains. Some multi-domain text classification models are
trained by adversarial training to extract shared features among all domains
and the specific features of each domain. We noted that the distinctness of the
domain-specific features is different, so in this paper, we propose to use a
curriculum learning strategy based on keyword weight ranking to improve the
performance of multi-domain text classification models. The experimental
results on the Amazon review and FDU-MTL datasets show that our curriculum
learning strategy effectively improves the performance of multi-domain text
classification models based on adversarial learning and outperforms
state-of-the-art methods.",0,0,0,0,1,0,0.533104,8.0,0.764787,13
2abd93f0-0e93-4503-a3d9-b4b0b1229a8e,Efficient Long Sequence Modeling via State Space Augmented Transformer,20,0.0810907,0.809395,"Transformer models have achieved superior performance in various natural
language processing tasks. However, the quadratic computational cost of the
attention mechanism limits its practicality for long sequences. There are
existing attention variants that improve the computational efficiency, but they
have limited ability to effectively compute global information. In parallel to
Transformer models, state space models (SSMs) are tailored for long sequences,
but they are not flexible enough to capture complicated local information. We
propose SPADE, short for $\underline{\textbf{S}}$tate
s$\underline{\textbf{P}}$ace
$\underline{\textbf{A}}$ugmente$\underline{\textbf{D}}$
Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the
bottom layer of SPADE, and we employ efficient local attention methods for the
other layers. The SSM augments global information, which complements the lack
of long-range dependency issue in local attention methods. Experimental results
on the Long Range Arena benchmark and language modeling tasks demonstrate the
effectiveness of the proposed method. To further demonstrate the scalability of
SPADE, we pre-train large encoder-decoder models and present fine-tuning
results on natural language understanding and natural language generation
tasks.",1,0,0,0,0,0,0.669778,6.0,0.748852,78
9bd7848f-ab79-4534-ae2f-4cee9fdef4ab,Combining Commonsense Reasoning and Knowledge Acquisition to Guide Deep Learning in Robotics,1,0.0214515,0.0709716,"Algorithms based on deep network models are being used for many pattern
recognition and decision-making tasks in robotics and AI. Training these models
requires a large labeled dataset and considerable computational resources,
which are not readily available in many domains. Also, it is difficult to
explore the internal representations and reasoning mechanisms of these models.
As a step towards addressing the underlying knowledge representation,
reasoning, and learning challenges, the architecture described in this paper
draws inspiration from research in cognitive systems. As a motivating example,
we consider an assistive robot trying to reduce clutter in any given scene by
reasoning about the occlusion of objects and stability of object configurations
in an image of the scene. In this context, our architecture incrementally
learns and revises a grounding of the spatial relations between objects and
uses this grounding to extract spatial information from input images.
Non-monotonic logical reasoning with this information and incomplete
commonsense domain knowledge is used to make decisions about stability and
occlusion. For images that cannot be processed by such reasoning, regions
relevant to the tasks at hand are automatically identified and used to train
deep network models to make the desired decisions. Image regions used to train
the deep networks are also used to incrementally acquire previously unknown
state constraints that are merged with the existing knowledge for subsequent
reasoning. Experimental evaluation performed using simulated and real-world
images indicates that in comparison with baselines based just on deep networks,
our architecture improves reliability of decision making and reduces the effort
involved in training data-driven deep network models.",0,0,0,0,0,0,0.15376,10.0,0.660051,75
8741a635-6805-478a-9750-ddd8c7d7ec6e,AnyFace: Free-style Text-to-Face Synthesis and Manipulation,37,0.197029,0.994248,"Existing text-to-image synthesis methods generally are only applicable to
words in the training dataset. However, human faces are so variable to be
described with limited words. So this paper proposes the first free-style
text-to-face method namely AnyFace enabling much wider open world applications
such as metaverse, social media, cosmetics, forensics, etc. AnyFace has a novel
two-stream framework for face image synthesis and manipulation given arbitrary
descriptions of the human face. Specifically, one stream performs text-to-face
generation and the other conducts face image reconstruction. Facial text and
image features are extracted using the CLIP (Contrastive Language-Image
Pre-training) encoders. And a collaborative Cross Modal Distillation (CMD)
module is designed to align the linguistic and visual features across these two
streams. Furthermore, a Diverse Triplet Loss (DT loss) is developed to model
fine-grained features and improve facial diversity. Extensive experiments on
Multi-modal CelebA-HQ and CelebAText-HQ demonstrate significant advantages of
AnyFace over state-of-the-art methods. AnyFace can achieve high-quality,
high-resolution, and high-diversity face synthesis and manipulation results
without any constraints on the number and content of input captions.",0,1,1,0,1,0,0.638159,5.0,0.681396,38
1ee40cec-def7-4e9c-b2f0-07e6d132f1d0,Investigating Reasons for Disagreement in Natural Language Inference,19,0.0677165,0.526118,"We investigate how disagreement in natural language inference (NLI)
annotation arises. We developed a taxonomy of disagreement sources with 10
categories spanning 3 high-level classes. We found that some disagreements are
due to uncertainty in the sentence meaning, others to annotator biases and task
artifacts, leading to different interpretations of the label distribution. We
explore two modeling approaches for detecting items with potential
disagreement: a 4-way classification with a ""Complicated"" label in addition to
the three standard NLI labels, and a multilabel classification approach. We
found that the multilabel classification is more expressive and gives better
recall of the possible interpretations in the data.",0,0,0,0,0,1,0.00976609,11.0,0.43333,64
5ff2535e-7c66-4f26-bf1b-fc3d586aa737,A Psychological Theory of Explainability,12,0.00750159,0.320897,"The goal of explainable Artificial Intelligence (XAI) is to generate
human-interpretable explanations, but there are no computationally precise
theories of how humans interpret AI generated explanations. The lack of theory
means that validation of XAI must be done empirically, on a case-by-case basis,
which prevents systematic theory-building in XAI. We propose a psychological
theory of how humans draw conclusions from saliency maps, the most common form
of XAI explanation, which for the first time allows for precise prediction of
explainee inference conditioned on explanation. Our theory posits that absent
explanation humans expect the AI to make similar decisions to themselves, and
that they interpret an explanation by comparison to the explanations they
themselves would give. Comparison is formalized via Shepard's universal law of
generalization in a similarity space, a classic theory from cognitive science.
A pre-registered user study on AI image classifications with saliency map
explanations demonstrate that our theory quantitatively matches participants'
predictions of the AI.",0,0,0,0,0,1,0.076836,8.0,0.483023,43
918d8d31-dbae-4a02-b1a6-dc346a0555d3,Learning Optimal Phase-Shifts of Holographic Metasurface Transceivers,2,0.0446191,0.204331,"Holographic metasurface transceivers (HMT) is an emerging technology for
enhancing the coverage and rate of wireless communication systems. However,
acquiring accurate channel state information in HMT-assisted wireless
communication systems is critical for achieving these goals. In this paper, we
propose an algorithm for learning the optimal phase-shifts at a HMT for the
far-field channel model. Our proposed algorithm exploits the structure of the
channel gains in the far-field regions and learns the optimal phase-shifts in
presence of noise in the received signals. We prove that the probability that
the optimal phase-shifts estimated by our proposed algorithm deviate from the
true values decays exponentially in the number of pilot signals. Extensive
numerical simulations validate the theoretical guarantees and also demonstrate
significant gains as compared to the state-of-the-art policies.",0,1,0,0,1,0,0.604315,7.0,0.759272,21
522e8809-b875-46f6-a9e1-082844cd05a5,"Global Counterfactual Explanations: Investigations, Implementations and Improvements",7,0.18485,0.568708,"Counterfactual explanations have been widely studied in explainability, with
a range of application dependent methods emerging in fairness, recourse and
model understanding. However, the major shortcoming associated with these
methods is their inability to provide explanations beyond the local or
instance-level. While some works touch upon the notion of a global explanation,
typically suggesting to aggregate masses of local explanations in the hope of
ascertaining global properties, few provide frameworks that are either reliable
or computationally tractable. Meanwhile, practitioners are requesting more
efficient and interactive explainability tools. We take this opportunity to
investigate existing global methods, with a focus on implementing and improving
Actionable Recourse Summaries (AReS), the only known global counterfactual
explanation framework for recourse.",0,0,0,0,0,0,0.893101,5.0,0.839039,29
16f023e3-a6a2-4470-9eb1-6080b95ab37c,GIFS: Neural Implicit Function for General Shape Representation,39,0.47439,0.844034,"Recent development of neural implicit function has shown tremendous success
on high-quality 3D shape reconstruction. However, most works divide the space
into inside and outside of the shape, which limits their representing power to
single-layer and watertight shapes. This limitation leads to tedious data
processing (converting non-watertight raw data to watertight) as well as the
incapability of representing general object shapes in the real world. In this
work, we propose a novel method to represent general shapes including
non-watertight shapes and shapes with multi-layer surfaces. We introduce
General Implicit Function for 3D Shape (GIFS), which models the relationships
between every two points instead of the relationships between points and
surfaces. Instead of dividing 3D space into predefined inside-outside regions,
GIFS encodes whether two points are separated by any surface. Experiments on
ShapeNet show that GIFS outperforms previous state-of-the-art methods in terms
of reconstruction quality, rendering efficiency, and visual fidelity. Project
page is available at https://jianglongye.com/gifs .",1,0,0,0,1,0,0.942645,7.0,0.920122,70
2e3b4f2d-4768-48e9-b1df-51a634c18d3e,Poseur: Direct Human Pose Regression with Transformers,49,0.165957,0.914351,"We propose a direct, regression-based approach to 2D human pose estimation
from single images. We formulate the problem as a sequence prediction task,
which we solve using a Transformer network. This network directly learns a
regression mapping from images to the keypoint coordinates, without resorting
to intermediate representations such as heatmaps. This approach avoids much of
the complexity associated with heatmap-based approaches. To overcome the
feature misalignment issues of previous regression-based methods, we propose an
attention mechanism that adaptively attends to the features that are most
relevant to the target keypoints, considerably improving the accuracy.
Importantly, our framework is end-to-end differentiable, and naturally learns
to exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,
two predominant pose-estimation datasets, demonstrate that our method
significantly improves upon the state-of-the-art in regression-based pose
estimation. More notably, ours is the first regression-based approach to
perform favorably compared to the best heatmap-based pose estimation methods.",1,1,0,0,1,0,0.625845,6.0,0.728917,40
2533ff1d-9667-416c-9311-70bdf6406b47,Attention Based Neural Networks for Wireless Channel Estimation,11,0.0788897,0.76809,"In this paper, we deploy the self-attention mechanism to achieve improved
channel estimation for orthogonal frequency-division multiplexing waveforms in
the downlink. Specifically, we propose a new hybrid encoder-decoder structure
(called HA02) for the first time which exploits the attention mechanism to
focus on the most important input information. In particular, we implement a
transformer encoder block as the encoder to achieve the sparsity in the input
features and a residual neural network as the decoder respectively, inspired by
the success of the attention mechanism. Using 3GPP channel models, our
simulations show superior estimation performance compared with other candidate
neural network methods for channel estimation.",0,1,0,0,0,0,0.0804896,9.0,0.545844,21
416e576b-9336-4980-a8da-3910ea347079,Improving Monocular Visual Odometry Using Learned Depth,12,0.195117,0.616519,"Monocular visual odometry (VO) is an important task in robotics and computer
vision. Thus far, how to build accurate and robust monocular VO systems that
can work well in diverse scenarios remains largely unsolved. In this paper, we
propose a framework to exploit monocular depth estimation for improving VO. The
core of our framework is a monocular depth estimation module with a strong
generalization capability for diverse scenes. It consists of two separate
working modes to assist the localization and mapping. With a single monocular
image input, the depth estimation module predicts a relative depth to help the
localization module on improving the accuracy. With a sparse depth map and an
RGB image input, the depth estimation module can generate accurate
scale-consistent depth for dense mapping. Compared with current learning-based
VO methods, our method demonstrates a stronger generalization ability to
diverse scenes. More significantly, our framework is able to boost the
performances of existing geometry-based VO methods by a large margin.",0,1,0,0,0,0,0.876723,11.0,0.920847,78
9c1533dc-c3e2-4711-92f9-ecb8eb993f38,Data Augmentation for Dementia Detection in Spoken Language,6,0.0200046,0.234353,"Dementia is a growing problem as our society ages, and detection methods are
often invasive and expensive. Recent deep-learning techniques can offer a
faster diagnosis and have shown promising results. However, they require large
amounts of labelled data which is not easily available for the task of dementia
detection. One effective solution to sparse data problems is data augmentation,
though the exact methods need to be selected carefully. To date, there has been
no empirical study of data augmentation on Alzheimer's disease (AD) datasets
for NLP and speech processing. In this work, we investigate data augmentation
techniques for the task of AD detection and perform an empirical evaluation of
the different approaches on two kinds of models for both the text and audio
domains. We use a transformer-based model for both domains, and SVM and Random
Forest models for the text and audio domains, respectively. We generate
additional samples using traditional as well as deep learning based methods and
show that data augmentation improves performance for both the text- and
audio-based models and that such results are comparable to state-of-the-art
results on the popular ADReSS set, with carefully crafted architectures and
features.",1,1,0,0,0,0,0.438078,4.0,0.459899,35
fe7955aa-36b7-4b74-b38d-66e8fc9b784f,The University of Edinburgh's Submission to the WMT22 Code-Mixing Shared Task (MixMT),1,0.00937077,0.0945691,"The University of Edinburgh participated in the WMT22 shared task on
code-mixed translation. This consists of two subtasks: i) generating code-mixed
Hindi/English (Hinglish) text generation from parallel Hindi and English
sentences and ii) machine translation from Hinglish to English. As both
subtasks are considered low-resource, we focused our efforts on careful data
generation and curation, especially the use of backtranslation from monolingual
resources. For subtask 1 we explored the effects of constrained decoding on
English and transliterated subwords in order to produce Hinglish. For subtask
2, we investigated different pretraining techniques, namely comparing simple
initialisation from existing machine translation models and aligned
augmentation. For both subtasks, we found that our baseline systems worked
best. Our systems for both subtasks were one of the overall top-performing
submissions.",0,1,0,0,0,0,0.283502,8.0,0.661511,53
a7b904a6-4d78-454f-8c2f-4303bfb4d6ef,MRF-UNets: Searching UNet with Markov Random Fields,5,0.0202628,0.375947,"UNet [27] is widely used in semantic segmentation due to its simplicity and
effectiveness. However, its manually-designed architecture is applied to a
large number of problem settings, either with no architecture optimizations, or
with manual tuning, which is time consuming and can be sub-optimal. In this
work, firstly, we propose Markov Random Field Neural Architecture Search
(MRF-NAS) that extends and improves the recent Adaptive and Optimal Network
Width Search (AOWS) method [4] with (i) a more general MRF framework (ii)
diverse M-best loopy inference (iii) differentiable parameter learning. This
provides the necessary NAS framework to efficiently explore network
architectures that induce loopy inference graphs, including loops that arise
from skip connections. With UNet as the backbone, we find an architecture,
MRF-UNet, that shows several interesting characteristics. Secondly, through the
lens of these characteristics, we identify the sub-optimality of the original
UNet architecture and further improve our results with MRF-UNetV2. Experiments
show that our MRF-UNets significantly outperform several benchmarks on three
aerial image datasets and two medical image datasets while maintaining low
computational costs. The code is available at:
https://github.com/zifuwanggg/MRF-UNets.",1,0,0,0,0,0,0.340205,7.0,0.644725,45
2107581f-8b1b-4b85-9cd0-c379173c3e18,e-CARE: a New Dataset for Exploring Explainable Causal Reasoning,32,0.309985,0.55242,"Understanding causality has vital importance for various Natural Language
Processing (NLP) applications. Beyond the labeled instances, conceptual
explanations of the causality can provide deep understanding of the causal
facts to facilitate the causal reasoning process. However, such explanation
information still remains absent in existing causal reasoning resources. In
this paper, we fill this gap by presenting a human-annotated explainable CAusal
REasoning dataset (e-CARE), which contains over 21K causal reasoning questions,
together with natural language formed explanations of the causal questions.
Experimental results show that generating valid explanations for causal facts
still remains especially challenging for the state-of-the-art models, and the
explanation information can be helpful for promoting the accuracy and stability
of causal reasoning models.",1,0,1,1,0,0,0.559345,10.0,0.819149,59
7ddfbae5-a180-45b9-b2e2-04b3befb085f,ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection,7,0.252535,0.828128,"Ensuring traffic safety and mitigating accidents in modern driving is of
paramount importance, and computer vision technologies have the potential to
significantly contribute to this goal. This paper presents a multi-modal Vision
Transformer for Driver Distraction Detection (termed ViT-DD), which
incorporates inductive information from training signals related to both
distraction detection and driver emotion recognition. Additionally, a
self-learning algorithm is developed, allowing for the seamless integration of
driver data without emotion labels into the multi-task training process of
ViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existing
state-of-the-art methods for driver distraction detection by 6.5% and 0.9% on
the SFDDD and AUCDD datasets, respectively.",0,1,0,0,1,0,0.916995,2.0,0.651191,60
5da1538b-14fa-4f3b-a9cb-2ae515fc6183,Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social-Text Classification,8,0.0742147,0.582176,"Social media has become the fulcrum of all forms of communication.
Classifying social texts such as fake news, rumour, sarcasm, etc. has gained
significant attention. The surface-level signals expressed by a social-text
itself may not be adequate for such tasks; therefore, recent methods attempted
to incorporate other intrinsic signals such as user behavior and the underlying
graph structure. Oftentimes, the `public wisdom' expressed through the
comments/replies to a social-text acts as a surrogate of crowd-sourced view and
may provide us with complementary signals. State-of-the-art methods on
social-text classification tend to ignore such a rich hierarchical signal.
Here, we propose Hyphen, a discourse-aware hyperbolic spectral co-attention
network. Hyphen is a fusion of hyperbolic graph representation learning with a
novel Fourier co-attention mechanism in an attempt to generalise the
social-text classification tasks by incorporating public discourse. We parse
public discourse as an Abstract Meaning Representation (AMR) graph and use the
powerful hyperbolic geometric representation to model graphs with hierarchical
structure. Finally, we equip it with a novel Fourier co-attention mechanism to
capture the correlation between the source post and public discourse. Extensive
experiments on four different social-text classification tasks, namely
detecting fake news, hate speech, rumour, and sarcasm, show that Hyphen
generalises well, and achieves state-of-the-art results on ten benchmark
datasets. We also employ a sentence-level fact-checked and annotated dataset to
evaluate how Hyphen is capable of producing explanations as analogous evidence
to the final prediction.",1,0,0,0,1,0,0.338596,7.0,0.643885,80
f3765fb7-3db7-4fd5-a499-56adbda6191c,Disentangling Task Relations for Few-shot Text Classification via Self-Supervised Hierarchical Task Clustering,3,0.018495,0.126818,"Few-Shot Text Classification (FSTC) imitates humans to learn a new text
classifier efficiently with only few examples, by leveraging prior knowledge
from historical tasks. However, most prior works assume that all the tasks are
sampled from a single data source, which cannot adapt to real-world scenarios
where tasks are heterogeneous and lie in different distributions. As such,
existing methods may suffer from their globally knowledge-shared mechanisms to
handle the task heterogeneity. On the other hand, inherent task relation are
not explicitly captured, making task knowledge unorganized and hard to transfer
to new tasks. Thus, we explore a new FSTC setting where tasks can come from a
diverse range of data sources. To address the task heterogeneity, we propose a
self-supervised hierarchical task clustering (SS-HTC) method. SS-HTC not only
customizes cluster-specific knowledge by dynamically organizing heterogeneous
tasks into different clusters in hierarchical levels but also disentangles
underlying relations between tasks to improve the interpretability. Extensive
experiments on five public FSTC benchmark datasets demonstrate the
effectiveness of SS-HTC.",0,0,0,0,0,0,0.266706,8.0,0.652506,55
a0ffac54-fffa-4609-a59e-54dc05fe105a,MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure,5,0.0474345,0.127134,"In this paper, we propose a comprehensive benchmark to investigate models'
logical reasoning capabilities in complex real-life scenarios. Current
explanation datasets often employ synthetic data with simple reasoning
structures. Therefore, it cannot express more complex reasoning processes, such
as the rebuttal to a reasoning step and the degree of certainty of the
evidence. To this end, we propose a comprehensive logical reasoning explanation
form. Based on the multi-hop chain of reasoning, the explanation form includes
three main components: (1) The condition of rebuttal that the reasoning node
can be challenged; (2) Logical formulae that uncover the internal texture of
reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The
fine-grained structure conforms to the real logical reasoning scenario, better
fitting the human cognitive process but, simultaneously, is more challenging
for the current models. We evaluate the current best models' performance on
this new explanation form. The experimental results show that generating
reasoning graphs remains a challenging task for current models, even with the
help of giant pre-trained language models.",1,0,0,1,0,0,0.476585,6.0,0.65929,42
9c6077b5-dc0e-463f-ba52-11846dfbc133,Extending Compositional Attention Networks for Social Reasoning in Videos,1,0.00848675,0.124718,"We propose a novel deep architecture for the task of reasoning about social
interactions in videos. We leverage the multi-step reasoning capabilities of
Compositional Attention Networks (MAC), and propose a multimodal extension
(MAC-X). MAC-X is based on a recurrent cell that performs iterative mid-level
fusion of input modalities (visual, auditory, text) over multiple reasoning
steps, by use of a temporal attention mechanism. We then combine MAC-X with
LSTMs for temporal input processing in an end-to-end architecture. Our ablation
studies show that the proposed MAC-X architecture can effectively leverage
multimodal input cues using mid-level fusion mechanisms. We apply MAC-X to the
task of Social Video Question Answering in the Social IQ dataset and obtain a
2.5% absolute improvement in terms of binary accuracy over the current
state-of-the-art.",1,1,0,0,1,0,0.28367,10.0,0.729279,28
e6fb63e0-a40e-45f4-8e7e-22f4c0624350,g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin,7,0.0782516,0.705446,"Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have approached this
problem using pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we
propose a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments show that learning a soft-weighting function for the candidate
phonemes benefits performance. In addition, our proposed g2pW does not require
extra pre-trained POS tagging models while using POS tags as auxiliary features
since we train the POS tagging model simultaneously with the unified encoder.
Experimental results show that our g2pW outperforms existing methods on the
public CPP dataset. All codes, model weights, and a user-friendly package are
publicly available.",1,1,0,1,1,0,0.0349367,6.0,0.175679,18
52cb5838-0a7e-493a-a358-081f977d5052,Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models,6,0.0627406,0.148122,"Current Natural Language Inference (NLI) models achieve impressive results,
sometimes outperforming humans when evaluating on in-distribution test sets.
However, as these models are known to learn from annotation artefacts and
dataset biases, it is unclear to what extent the models are learning the task
of NLI instead of learning from shallow heuristics in their training data. We
address this issue by introducing a logical reasoning framework for NLI,
creating highly transparent model decisions that are based on logical rules.
Unlike prior work, we show that improved interpretability can be achieved
without decreasing the predictive accuracy. We almost fully retain performance
on SNLI, while also identifying the exact hypothesis spans that are responsible
for each model prediction. Using the e-SNLI human explanations, we verify that
our model makes sensible decisions at a span level, despite not using any span
labels during training. We can further improve model performance and span-level
decisions by using the e-SNLI explanations during training. Finally, our model
is more robust in a reduced data setting. When training with only 1,000
examples, out-of-distribution performance improves on the MNLI matched and
mismatched validation sets by 13% and 16% relative to the baseline. Training
with fewer observations yields further improvements, both in-distribution and
out-of-distribution.",0,0,0,0,0,0,0.324658,6.0,0.575906,51
a516e3cc-c80f-48ff-900c-75a2e38b348c,BadPrompt: Backdoor Attacks on Continuous Prompts,31,0.587567,0.861906,"The prompt-based learning paradigm has gained much research attention
recently. It has achieved state-of-the-art performance on several NLP tasks,
especially in the few-shot scenarios. While steering the downstream tasks, few
works have been reported to investigate the security problems of the
prompt-based models. In this paper, we conduct the first study on the
vulnerability of the continuous prompt learning algorithm to backdoor attacks.
We observe that the few-shot scenarios have posed a great challenge to backdoor
attacks on the prompt-based models, limiting the usability of existing NLP
backdoor methods. To address this challenge, we propose BadPrompt, a
lightweight and task-adaptive algorithm, to backdoor attack continuous prompts.
Specially, BadPrompt first generates candidate triggers which are indicative
for predicting the targeted label and dissimilar to the samples of the
non-targeted labels. Then, it automatically selects the most effective and
invisible trigger for each sample with an adaptive trigger optimization
algorithm. We evaluate the performance of BadPrompt on five datasets and two
continuous prompt models. The results exhibit the abilities of BadPrompt to
effectively attack continuous prompts while maintaining high performance on the
clean test sets, outperforming the baseline models by a large margin. The
source code of BadPrompt is publicly available at
https://github.com/papersPapers/BadPrompt.",0,0,0,0,0,0,0.978884,5.0,0.948128,50
609339f1-8ce8-4cb8-b834-b2d19e459c9a,Audio-Adaptive Activity Recognition Across Video Domains,26,0.310176,0.611457,"This paper strives for activity recognition under domain shift, for example
caused by change of scenery or camera viewpoint. The leading approaches reduce
the shift in activity appearance by adversarial training and self-supervised
learning. Different from these vision-focused works we leverage activity sounds
for domain adaptation as they have less variance across domains and can
reliably indicate which activities are not happening. We propose an
audio-adaptive encoder and associated learning methods that discriminatively
adjust the visual feature representation as well as addressing shifts in the
semantic distribution. To further eliminate domain-specific features and
include domain-invariant activity sounds for recognition, an audio-infused
recognizer is proposed, which effectively models the cross-modal interaction
across domains. We also introduce the new task of actor shift, with a
corresponding audio-visual dataset, to challenge our method with situations
where the activity appearance changes dramatically. Experiments on this
dataset, EPIC-Kitchens and CharadesEgo show the effectiveness of our approach.",1,1,1,1,0,0,0.800786,6.0,0.811482,55
5484159d-65cf-4c92-bfba-73bc719cb179,Earnings-22: A Practical Benchmark for Accents in the Wild,10,0.460442,0.453664,"Modern automatic speech recognition (ASR) systems have achieved superhuman
Word Error Rate (WER) on many common corpora despite lacking adequate
performance on speech in the wild. Beyond that, there is a lack of real-world,
accented corpora to properly benchmark academic and commercial models. To
ensure this type of speech is represented in ASR benchmarking, we present
Earnings-22, a 125 file, 119 hour corpus of English-language earnings calls
gathered from global companies. We run a comparison across 4 commercial models
showing the variation in performance when taking country of origin into
consideration. Looking at hypothesis transcriptions, we explore errors common
to all ASR systems tested. By examining Individual Word Error Rate (IWER), we
find that key speech features impact model performance more for certain accents
than others. Earnings-22 provides a free-to-use benchmark of real-world,
accented audio to bridge academic and industrial research.",0,1,1,1,0,0,0.892751,2.0,0.596864,12
4f7b7215-1204-4c67-8352-874ebf500f3e,Bridging the Gap between Reality and Ideality of Entity Matching: A Revisiting and Benchmark Re-Construction,6,0.0780758,0.269737,"Entity matching (EM) is the most critical step for entity resolution (ER).
While current deep learningbased methods achieve very impressive performance on
standard EM benchmarks, their realworld application performance is much
frustrating. In this paper, we highlight that such the gap between reality and
ideality stems from the unreasonable benchmark construction process, which is
inconsistent with the nature of entity matching and therefore leads to biased
evaluations of current EM approaches. To this end, we build a new EM corpus and
re-construct EM benchmarks to challenge critical assumptions implicit in the
previous benchmark construction process by step-wisely changing the restricted
entities, balanced labels, and single-modal records in previous benchmarks into
open entities, imbalanced labels, and multimodal records in an open
environment. Experimental results demonstrate that the assumptions made in the
previous benchmark construction process are not coincidental with the open
environment, which conceal the main challenges of the task and therefore
significantly overestimate the current progress of entity matching. The
constructed benchmarks and code are publicly released",0,0,1,1,0,0,0.452865,5.0,0.576965,24
84671fe5-e948-49fc-b58e-9c9925872971,3D Pose Based Feedback for Physical Exercises,8,0.179737,0.901406,"Unsupervised self-rehabilitation exercises and physical training can cause
serious injuries if performed incorrectly. We introduce a learning-based
framework that identifies the mistakes made by a user and proposes corrective
measures for easier and safer individual training. Our framework does not rely
on hard-coded, heuristic rules. Instead, it learns them from data, which
facilitates its adaptation to specific user needs. To this end, we use a Graph
Convolutional Network (GCN) architecture acting on the user's pose sequence to
model the relationship between the body joints trajectories. To evaluate our
approach, we introduce a dataset with 3 different physical exercises. Our
approach yields 90.9% mistake identification accuracy and successfully corrects
94.2% of the mistakes.",0,1,0,1,0,0,0.48431,9.0,0.775383,43
1d9ed28e-345a-4fcc-91ec-a75dde854e64,Meta-Learning Priors for Safe Bayesian Optimization,22,0.197958,0.910318,"In robotics, optimizing controller parameters under safety constraints is an
important challenge. Safe Bayesian optimization (BO) quantifies uncertainty in
the objective and constraints to safely guide exploration in such settings.
Hand-designing a suitable probabilistic model can be challenging, however. In
the presence of unknown safety constraints, it is crucial to choose reliable
model hyper-parameters to avoid safety violations. Here, we propose a
data-driven approach to this problem by meta-learning priors for safe BO from
offline data. We build on a meta-learning algorithm, F-PACOH, capable of
providing reliable uncertainty quantification in settings of data scarcity. As
core contribution, we develop a novel framework for choosing safety-compliant
priors in a data-riven manner via empirical uncertainty metrics and a frontier
search algorithm. On benchmark functions and a high-precision motion system, we
demonstrate that our meta-learned priors accelerate the convergence of safe BO
approaches while maintaining safety.",1,1,0,0,0,0,0.222739,10.0,0.701216,60
03a4c408-4d0b-4c65-9a4a-d421d2b4df94,SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation,30,0.150435,0.669765,"Recent advances in image captioning have focused on scaling the data and
model size, substantially increasing the cost of pre-training and finetuning.
As an alternative to large models, we present SmallCap, which generates a
caption conditioned on an input image and related captions retrieved from a
datastore. Our model is lightweight and fast to train, as the only learned
parameters are in newly introduced cross-attention layers between a pre-trained
CLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without
additional finetuning and can exploit large-scale data in a training-free
fashion since the contents of the datastore can be readily replaced. Our
experiments show that SmallCap, trained only on COCO, has competitive
performance on this benchmark, and also transfers to other domains without
retraining, solely through retrieval from target-domain data. Further
improvement is achieved through the training-free exploitation of diverse
human-labeled and web data, which proves to be effective for a range of
domains, including the nocaps benchmark, designed to test generalization to
unseen visual concepts.",1,1,0,0,0,0,0.580051,6.0,0.708094,53
140ca47d-51a5-4d2b-9db5-e78aaea207a5,Tencent's Multilingual Machine Translation System for WMT22 Large-Scale African Languages,11,0.168237,0.590783,"This paper describes Tencent's multilingual machine translation systems for
the WMT22 shared task on Large-Scale Machine Translation Evaluation for African
Languages. We participated in the $\mathbf{constrained}$ translation track in
which only the data and pretrained models provided by the organizer are
allowed. The task is challenging due to three problems, including the absence
of training data for some to-be-evaluated language pairs, the uneven
optimization of language pairs caused by data imbalance, and the curse of
multilinguality. To address these problems, we adopt data augmentation,
distributionally robust optimization, and language family grouping,
respectively, to develop our multilingual neural machine translation (MNMT)
models. Our submissions won the $\mathbf{1st\ place}$ on the blind test sets in
terms of the automatic evaluation metrics. Codes, models, and detailed
competition results are available at
https://github.com/wxjiao/WMT2022-Large-Scale-African.",0,1,0,0,1,0,0.567121,6.0,0.702164,27
b0b5b6b1-3a7d-41de-86ca-288f0651535a,A general-purpose material property data extraction pipeline from large polymer corpora using Natural Language Processing,17,0.530073,0.842864,"The ever-increasing number of materials science articles makes it hard to
infer chemistry-structure-property relations from published literature. We used
natural language processing (NLP) methods to automatically extract material
property data from the abstracts of polymer literature. As a component of our
pipeline, we trained MaterialsBERT, a language model, using 2.4 million
materials science abstracts, which outperforms other baseline models in three
out of five named entity recognition datasets when used as the encoder for
text. Using this pipeline, we obtained ~300,000 material property records from
~130,000 abstracts in 60 hours. The extracted data was analyzed for a diverse
range of applications such as fuel cells, supercapacitors, and polymer solar
cells to recover non-trivial insights. The data extracted through our pipeline
is made available through a web platform at https://polymerscholar.org which
can be used to locate material property data recorded in abstracts
conveniently. This work demonstrates the feasibility of an automatic pipeline
that starts from published literature and ends with a complete set of extracted
material property information.",1,1,0,0,0,0,0.782514,7.0,0.830423,89
9cf99867-c081-4ea5-83db-8b41905c6b4a,Teaching Where to Look: Attention Similarity Knowledge Distillation for Low Resolution Face Recognition,12,0.427955,0.817373,"Deep learning has achieved outstanding performance for face recognition
benchmarks, but performance reduces significantly for low resolution (LR)
images. We propose an attention similarity knowledge distillation approach,
which transfers attention maps obtained from a high resolution (HR) network as
a teacher into an LR network as a student to boost LR recognition performance.
Inspired by humans being able to approximate an object's region from an LR
image based on prior knowledge obtained from HR images, we designed the
knowledge distillation loss using the cosine similarity to make the student
network's attention resemble the teacher network's attention. Experiments on
various LR face related benchmarks confirmed the proposed method generally
improved recognition performances on LR settings, outperforming
state-of-the-art results by simply transferring well-constructed attention
maps. The code and pretrained models are publicly available in the
https://github.com/gist-ailab/teaching-where-to-look.",1,1,0,0,1,0,0.887932,12.0,0.931154,40
f76fd05d-f5f8-401e-be4e-45736413a462,A Framework for Multi-stage Bonus Allocation in meal delivery Platform,5,0.0534606,0.714977,"Online meal delivery is undergoing explosive growth, as this service is
becoming increasingly popular. A meal delivery platform aims to provide
excellent and stable services for customers and restaurants. However, in
reality, several hundred thousand orders are canceled per day in the Meituan
meal delivery platform since they are not accepted by the crowd soucing
drivers. The cancellation of the orders is incredibly detrimental to the
customer's repurchase rate and the reputation of the Meituan meal delivery
platform. To solve this problem, a certain amount of specific funds is provided
by Meituan's business managers to encourage the crowdsourcing drivers to accept
more orders. To make better use of the funds, in this work, we propose a
framework to deal with the multi-stage bonus allocation problem for a meal
delivery platform. The objective of this framework is to maximize the number of
accepted orders within a limited bonus budget. This framework consists of a
semi-black-box acceptance probability model, a Lagrangian dual-based dynamic
programming algorithm, and an online allocation algorithm. The semi-black-box
acceptance probability model is employed to forecast the relationship between
the bonus allocated to order and its acceptance probability, the Lagrangian
dual-based dynamic programming algorithm aims to calculate the empirical
Lagrangian multiplier for each allocation stage offline based on the historical
data set, and the online allocation algorithm uses the results attained in the
offline part to calculate a proper delivery bonus for each order. To verify the
effectiveness and efficiency of our framework, both offline experiments on a
real-world data set and online A/B tests on the Meituan meal delivery platform
are conducted. Our results show that using the proposed framework, the total
order cancellations can be decreased by more than 25\% in reality.",0,1,0,0,0,0,0.207312,9.0,0.658994,22
a4f83f9f-cda6-418f-8fc5-e6580fb67b8a,Coreference Resolution through a seq2seq Transition-Based System,19,0.437055,0.508337,"Most recent coreference resolution systems use search algorithms over
possible spans to identify mentions and resolve coreference. We instead present
a coreference resolution system that uses a text-to-text (seq2seq) paradigm to
predict mentions and links jointly. We implement the coreference system as a
transition system and use multilingual T5 as an underlying language model. We
obtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score
for English (a 2.3 higher F1-score than previous work (Dobrovolskii, 2021))
using only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than
previous work) and 74.3 F1-score for Chinese (+5.3). In addition we use the
SemEval-2010 data sets for experiments in the zero-shot setting, a few-shot
setting, and supervised setting using all available training data. We get
substantially higher zero-shot F1-scores for 3 out of 4 languages than previous
approaches and significantly exceed previous supervised state-of-the-art
results for all five tested languages.",1,1,0,0,1,0,0.578666,7.0,0.749251,45
c9f41029-e672-4a76-84c9-1328e7c972c9,META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI,21,0.464468,0.521167,"Task-oriented dialogue (TOD) systems have been widely used by mobile phone
intelligent assistants to accomplish tasks such as calendar scheduling or hotel
reservation. Current TOD systems usually focus on multi-turn text/speech
interaction, then they would call back-end APIs designed for TODs to perform
the task. However, this API-based architecture greatly limits the
information-searching capability of intelligent assistants and may even lead to
task failure if TOD-specific APIs are not available or the task is too
complicated to be executed by the provided APIs. In this paper, we propose a
new TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A
GUI-TOD system can directly perform GUI operations on real APPs and execute
tasks without invoking TOD-specific backend APIs. Furthermore, we release
META-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile
GUI. We also propose a multi-model action prediction and response model, which
show promising results on META-GUI. The dataset, codes and leaderboard are
publicly available.",0,1,1,1,0,0,0.676853,5.0,0.702494,42
336e0c8f-81d5-4ead-83e6-61ca07550d6c,In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models,12,0.0257714,0.349714,"Given the success with in-context learning of large pre-trained language
models, we introduce in-context learning distillation to transfer in-context
few-shot learning ability from large models to smaller models. We propose to
combine in-context learning objectives with language modeling objectives to
distill both the ability to read in-context examples and task knowledge to the
smaller models. We perform in-context learning distillation under two different
few-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask
In-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask
few-shot learning but also requires more computation than Meta-ICT. Our method
shows consistent improvements for both Meta-ICT and Multitask-ICT on two
benchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal
that in-context learning objectives and language modeling objectives are
complementary under the Multitask-ICT paradigm. In-context learning objectives
achieve the best performance when combined with language modeling objectives.",0,1,0,0,0,0,0.395516,4.0,0.426038,42
516816cb-5a10-404c-ab54-ceaad7d49ac7,Cross-Attention Transformer for Video Interpolation,11,0.16321,0.277342,"We propose TAIN (Transformers and Attention for video INterpolation), a
residual neural network for video interpolation, which aims to interpolate an
intermediate frame given two consecutive image frames around it. We first
present a novel vision transformer module, named Cross Similarity (CS), to
globally aggregate input image features with similar appearance as those of the
predicted interpolated frame. These CS features are then used to refine the
interpolated prediction. To account for occlusions in the CS features, we
propose an Image Attention (IA) module to allow the network to focus on CS
features from one frame over those of the other. TAIN outperforms existing
methods that do not require flow estimation and performs comparably to
flow-based methods while being computationally efficient in terms of inference
time on Vimeo90k, UCF101, and SNU-FILM benchmarks.",1,1,0,0,1,0,0.742012,8.0,0.836781,58
9d63739b-15a7-42ad-9d12-08b68f51eb1e,Bounding Counterfactuals under Selection Bias,8,0.0276389,0.586299,"Causal analysis may be affected by selection bias, which is defined as the
systematic exclusion of data from a certain subpopulation. Previous work in
this area focused on the derivation of identifiability conditions. We propose
instead a first algorithm to address both identifiable and unidentifiable
queries. We prove that, in spite of the missingness induced by the selection
bias, the likelihood of the available data is unimodal. This enables us to use
the causal expectation-maximisation scheme to obtain the values of causal
queries in the identifiable case, and to compute bounds otherwise. Experiments
demonstrate the approach to be practically viable. Theoretical convergence
characterisations are provided.",0,0,0,0,0,0,0.0885178,5.0,0.202399,13
c830a01f-25db-41c1-8f92-146e0d56ff84,Adversarial Learned Fair Representations using Dampening and Stacking,1,0.00474941,0.0255329,"As more decisions in our daily life become automated, the need to have
machine learning algorithms that make fair decisions increases. In fair
representation learning we are tasked with finding a suitable representation of
the data in which a sensitive variable is censored. Recent work aims to learn
fair representations through adversarial learning. This paper builds upon this
work by introducing a novel algorithm which uses dampening and stacking to
learn adversarial fair representations. Results show that that our algorithm
improves upon earlier work in both censoring and reconstruction.",0,1,0,0,0,0,0.429139,9.0,0.75687,21
16ee8f1f-2d2d-4cb5-9cf0-bf083dbe2a71,Confidence Score for Source-Free Unsupervised Domain Adaptation,42,0.29934,0.956563,"Source-free unsupervised domain adaptation (SFUDA) aims to obtain high
performance in the unlabeled target domain using the pre-trained source model,
not the source data. Existing SFUDA methods assign the same importance to all
target samples, which is vulnerable to incorrect pseudo-labels. To
differentiate between sample importance, in this study, we propose a novel
sample-wise confidence score, the Joint Model-Data Structure (JMDS) score for
SFUDA. Unlike existing confidence scores that use only one of the source or
target domain knowledge, the JMDS score uses both knowledge. We then propose a
Confidence score Weighting Adaptation using the JMDS (CoWA-JMDS) framework for
SFUDA. CoWA-JMDS consists of the JMDS scores as sample weights and weight Mixup
that is our proposed variant of Mixup. Weight Mixup promotes the model make
more use of the target domain knowledge. The experimental results show that the
JMDS score outperforms the existing confidence scores. Moreover, CoWA-JMDS
achieves state-of-the-art performance on various SFUDA scenarios: closed, open,
and partial-set scenarios.",1,1,0,0,1,0,0.769736,9.0,0.863869,43
665b22f9-4f4d-4b8a-bd71-24aabe0fef08,A Time Series is Worth 64 Words: Long-term Forecasting with Transformers,298,0.825703,1.0,"We propose an efficient design of Transformer-based models for multivariate
time series forecasting and self-supervised representation learning. It is
based on two key components: (i) segmentation of time series into
subseries-level patches which are served as input tokens to Transformer; (ii)
channel-independence where each channel contains a single univariate time
series that shares the same embedding and Transformer weights across all the
series. Patching design naturally has three-fold benefit: local semantic
information is retained in the embedding; computation and memory usage of the
attention maps are quadratically reduced given the same look-back window; and
the model can attend longer history. Our channel-independent patch time series
Transformer (PatchTST) can improve the long-term forecasting accuracy
significantly when compared with that of SOTA Transformer-based models. We also
apply our model to self-supervised pre-training tasks and attain excellent
fine-tuning performance, which outperforms supervised training on large
datasets. Transferring of masked pre-trained representation on one dataset to
others also produces SOTA forecasting accuracy. Code is available at:
https://github.com/yuqinie98/PatchTST.",0,1,0,0,1,0,0.874556,5.0,0.824191,46
5d482973-5d24-44ba-a96a-84436ffe171b,Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning,85,0.552368,0.990294,"Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks for both high-resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is \textit{language
adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual
texts of a language using the pre-training objective. However, adapting to a
target language individually takes a large disk space and limits the
cross-lingual transfer abilities of the resulting models because they have been
specialized for a single language. In this paper, we perform
\textit{multilingual adaptive fine-tuning} on 17 most-resourced African
languages and three other high-resource languages widely spoken on the African
continent to encourage cross-lingual transfer learning. To further specialize
the multilingual PLM, we removed vocabulary tokens from the embedding layer
that corresponds to non-African writing scripts before MAFT, thus reducing the
model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa
and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space.
Additionally, we show that our adapted PLM also improves the zero-shot
cross-lingual transfer abilities of parameter efficient fine-tuning methods.",1,1,0,0,0,0,0.78893,4.0,0.7081,47
0d8f4b79-9e44-4046-8fb2-9f2ee09f426d,A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation,24,0.757296,0.610134,"Early exiting allows instances to exit at different layers according to the
estimation of difficulty. Previous works usually adopt heuristic metrics such
as the entropy of internal outputs to measure instance difficulty, which
suffers from generalization and threshold-tuning. In contrast, learning to
exit, or learning to predict instance difficulty is a more appealing way.
Though some effort has been devoted to employing such ""learn-to-exit"" modules,
it is still unknown whether and how well the instance difficulty can be
learned. As a response, we first conduct experiments on the learnability of
instance difficulty, which demonstrates that modern neural models perform
poorly on predicting instance difficulty. Based on this observation, we propose
a simple-yet-effective Hash-based Early Exiting approach (HashEE) that replaces
the learn-to-exit modules with hash functions to assign each token to a fixed
exiting layer. Different from previous methods, HashEE requires no internal
classifiers nor extra parameters, and therefore is more efficient. Experimental
results on classification, regression, and generation tasks demonstrate that
HashEE can achieve higher performance with fewer FLOPs and inference time
compared with previous state-of-the-art early exiting methods.",1,1,0,0,1,1,0.978709,5.0,0.9477,59
47c2dc4d-eb82-4b06-93d1-9c7bce5fc3df,Efficient Hybrid Network: Inducting Scattering Features,3,0.0396222,0.0546577,"Recent work showed that hybrid networks, which combine predefined and learnt
filters within a single architecture, are more amenable to theoretical analysis
and less prone to overfitting in data-limited scenarios. However, their
performance has yet to prove competitive against the conventional counterparts
when sufficient amounts of training data are available. In an attempt to
address this core limitation of current hybrid networks, we introduce an
Efficient Hybrid Network (E-HybridNet). We show that it is the first scattering
based approach that consistently outperforms its conventional counterparts on a
diverse range of datasets. It is achieved with a novel inductive architecture
that embeds scattering features into the network flow using Hybrid Fusion
Blocks. We also demonstrate that the proposed design inherits the key property
of prior hybrid networks -- an effective generalisation in data-limited
scenarios. Our approach successfully combines the best of the two worlds:
flexibility and power of learnt features and stability and predictability of
scattering representations.",1,0,0,0,1,0,0.326564,11.0,0.769328,24
52b410a2-a8cf-4401-9ec1-48dae1412722,READ: Large-Scale Neural Scene Rendering for Autonomous Driving,37,0.319285,0.626653,"Synthesizing free-view photo-realistic images is an important task in
multimedia. With the development of advanced driver assistance systems~(ADAS)
and their applications in autonomous vehicles, experimenting with different
scenarios becomes a challenge. Although the photo-realistic street scenes can
be synthesized by image-to-image translation methods, which cannot produce
coherent scenes due to the lack of 3D information. In this paper, a large-scale
neural rendering method is proposed to synthesize the autonomous driving
scene~(READ), which makes it possible to synthesize large-scale driving
scenarios on a PC through a variety of sampling schemes. In order to represent
driving scenarios, we propose an {\omega} rendering network to learn neural
descriptors from sparse point clouds. Our model can not only synthesize
realistic driving scenes but also stitch and edit driving scenes. Experiments
show that our model performs well in large-scale driving scenarios.",1,1,0,0,0,0,0.711582,4.0,0.652092,38
e8e04a72-0850-488d-b952-ecdb6c18b295,Optimizing LLVM Pass Sequences with Shackleton: A Linear Genetic Programming Framework,8,0.346472,0.566381,"In this paper we introduce Shackleton as a generalized framework enabling the
application of linear genetic programming -- a technique under the umbrella of
evolutionary algorithms -- to a variety of use cases. We also explore here a
novel application for this class of methods: optimizing sequences of LLVM
optimization passes. The algorithm underpinning Shackleton is discussed, with
an emphasis on the effects of different features unique to the framework when
applied to LLVM pass sequences. Combined with analysis of different
hyperparameter settings, we report the results on automatically optimizing pass
sequences using Shackleton for two software applications at differing
complexity levels. Finally, we reflect on the advantages and limitations of our
current implementation and lay out a path for further improvements. These
improvements aim to surpass hand-crafted solutions with an automatic discovery
method for an optimal pass sequence.",0,1,0,0,0,0,0.0166165,34.0,0.832399,27
9dea270e-15e4-4ac3-acaa-4c42d0370394,Layer or Representation Space: What makes BERT-based Evaluation Metrics Robust?,8,0.0302233,0.443329,"The evaluation of recent embedding-based evaluation metrics for text
generation is primarily based on measuring their correlation with human
evaluations on standard benchmarks. However, these benchmarks are mostly from
similar domains to those used for pretraining word embeddings. This raises
concerns about the (lack of) generalization of embedding-based metrics to new
and noisy domains that contain a different vocabulary than the pretraining
data. In this paper, we examine the robustness of BERTScore, one of the most
popular embedding-based metrics for text generation. We show that (a) an
embedding-based metric that has the highest correlation with human evaluations
on a standard benchmark can have the lowest correlation if the amount of input
noise or unknown tokens increases, (b) taking embeddings from the first layer
of pretrained models improves the robustness of all metrics, and (c) the
highest robustness is achieved when using character-level embeddings, instead
of token-based embeddings, from the first layer of the pretrained model.",1,0,0,0,0,0,0.442543,4.0,0.463335,30
90f9dfbc-5f75-4635-9307-d0e96cd5ab1e,The Topological BERT: Transforming Attention into Topology for Natural Language Processing,13,0.0930097,0.775584,"In recent years, the introduction of the Transformer models sparked a
revolution in natural language processing (NLP). BERT was one of the first text
encoders using only the attention mechanism without any recurrent parts to
achieve state-of-the-art results on many NLP tasks.
  This paper introduces a text classifier using topological data analysis. We
use BERT's attention maps transformed into attention graphs as the only input
to that classifier. The model can solve tasks such as distinguishing spam from
ham messages, recognizing whether a sentence is grammatically correct, or
evaluating a movie review as negative or positive. It performs comparably to
the BERT baseline and outperforms it on some tasks.
  Additionally, we propose a new method to reduce the number of BERT's
attention heads considered by the topological classifier, which allows us to
prune the number of heads from 144 down to as few as ten with no reduction in
performance. Our work also shows that the topological model displays higher
robustness against adversarial attacks than the original BERT model, which is
maintained during the pruning process. To the best of our knowledge, this work
is the first to confront topological-based models with adversarial attacks in
the context of NLP.",0,0,0,0,0,0,0.475144,7.0,0.707355,41
a4657055-d354-4de4-925a-de1ccbba1097,RECALL: Rehearsal-free Continual Learning for Object Classification,1,0.00892971,0.0352349,"Convolutional neural networks show remarkable results in classification but
struggle with learning new things on the fly. We present a novel rehearsal-free
approach, where a deep neural network is continually learning new unseen object
categories without saving any data of prior sequences. Our approach is called
RECALL, as the network recalls categories by calculating logits for old
categories before training new ones. These are then used during training to
avoid changing the old categories. For each new sequence, a new head is added
to accommodate the new categories. To mitigate forgetting, we present a
regularization strategy where we replace the classification with a regression.
Moreover, for the known categories, we propose a Mahalanobis loss that includes
the variances to account for the changing densities between known and unknown
categories. Finally, we present a novel dataset for continual learning,
especially suited for object recognition on a mobile robot (HOWS-CL-25),
including 150,795 synthetic images of 25 household object categories. Our
approach RECALL outperforms the current state of the art on CORe50 and
iCIFAR-100 and reaches the best performance on HOWS-CL-25.",1,0,0,0,0,0,0.500852,12.0,0.835542,34
16b70efc-3d52-4eb2-b5c4-1327c5f4490c,Multiple Instance Neuroimage Transformer,7,0.0,0.757316,"For the first time, we propose using a multiple instance learning based
convolution-free transformer model, called Multiple Instance Neuroimage
Transformer (MINiT), for the classification of T1weighted (T1w) MRIs. We first
present several variants of transformer models adopted for neuroimages. These
models extract non-overlapping 3D blocks from the input volume and perform
multi-headed self-attention on a sequence of their linear projections. MINiT,
on the other hand, treats each of the non-overlapping 3D blocks of the input
MRI as its own instance, splitting it further into non-overlapping 3D patches,
on which multi-headed self-attention is computed. As a proof-of-concept, we
evaluate the efficacy of our model by training it to identify sex from T1w-MRIs
of two public datasets: Adolescent Brain Cognitive Development (ABCD) and the
National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA).
The learned attention maps highlight voxels contributing to identifying sex
differences in brain morphometry. The code is available at
https://github.com/singlaayush/MINIT.",0,0,0,0,0,0,0.483235,7.0,0.710757,43
f5f0cecf-0b49-4120-83a8-1b74ed9c101c,Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models,19,0.153088,0.979151,"World models learn the consequences of actions in vision-based interactive
systems. However, in practical scenarios such as autonomous driving, there
commonly exists noncontrollable dynamics independent of the action signals,
making it difficult to learn effective world models. To tackle this problem, we
present a novel reinforcement learning approach named Iso-Dream, which improves
the Dream-to-Control framework in two aspects. First, by optimizing the inverse
dynamics, we encourage the world model to learn controllable and
noncontrollable sources of spatiotemporal changes on isolated state transition
branches. Second, we optimize the behavior of the agent on the decoupled latent
imaginations of the world model. Specifically, to estimate state values, we
roll-out the noncontrollable states into the future and associate them with the
current controllable state. In this way, the isolation of dynamics sources can
greatly benefit long-horizon decision-making of the agent, such as a
self-driving car that can avoid potential risks by anticipating the movement of
other vehicles. Experiments show that Iso-Dream is effective in decoupling the
mixed dynamics and remarkably outperforms existing approaches in a wide range
of visual control and prediction domains.",1,1,0,0,1,0,0.600922,7.0,0.757951,71
0d75ae0b-cc63-49be-a1c7-23ba36930748,How would Stance Detection Techniques Evolve after the Launch of ChatGPT?,66,0.912932,0.784596,"Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection.",0,1,0,0,1,0,0.697943,10.0,0.85705,37
4b3ded05-0bbf-4e00-9411-416fb6d71dce,A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning,7,0.249906,0.513942,"Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.",0,1,0,1,0,0,0.687581,9.0,0.837991,25
7949e384-e5d6-4ae3-99e8-4fc6e1e06196,Imagination-Augmented Natural Language Understanding,16,0.486357,0.86367,"Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances.",1,0,1,0,0,0,0.963129,6.0,0.930764,60
4911b6db-35a2-4387-9699-998a90c5b4b8,Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?,25,0.551282,0.925171,"What can pre-trained multilingual sequence-to-sequence models like mBART
contribute to translating low-resource languages? We conduct a thorough
empirical experiment in 10 languages to ascertain this, considering five
factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning
data, (3) the amount of pre-training data in the model, (4) the impact of
domain mismatch, and (5) language typology. In addition to yielding several
heuristics, the experiments form a framework for evaluating the data
sensitivities of machine translation systems. While mBART is robust to domain
differences, its translations for unseen and typologically distant languages
remain below 3.0 BLEU. In answer to our title's question, mBART is not a
low-resource panacea; we therefore encourage shifting the emphasis from new
models to new data.",1,1,0,0,0,0,0.764705,5.0,0.751999,47
46462065-90d2-4789-97c0-d8e326f50a37,Improved Beam Search for Hallucination Mitigation in Abstractive Summarization,10,0.115885,0.388897,"Advancement in large pretrained language models has significantly improved
their performance for conditional language generation tasks including
summarization albeit with hallucinations. To reduce hallucinations,
conventional methods proposed improving beam search or using a fact checker as
a postprocessing step. In this paper, we investigate the use of the Natural
Language Inference (NLI) entailment metric to detect and prevent hallucinations
in summary generation. We propose an NLI-assisted beam re-ranking mechanism by
computing entailment probability scores between the input context and
summarization model-generated beams during saliency-enhanced greedy decoding.
Moreover, a diversity metric is introduced to compare its effectiveness against
vanilla beam search. Our proposed algorithm significantly outperforms vanilla
beam decoding on XSum and CNN/DM datasets.",0,1,0,0,0,0,0.701639,5.0,0.716146,39
f30edcfd-f18a-493a-9f1e-6b036bd2fe42,CrowdMLP: Weakly-Supervised Crowd Counting via Multi-Granularity MLP,13,0.516197,0.735075,"Existing state-of-the-art crowd counting algorithms rely excessively on
location-level annotations, which are burdensome to acquire. When only
count-level (weak) supervisory signals are available, it is arduous and
error-prone to regress total counts due to the lack of explicit spatial
constraints. To address this issue, a novel and efficient counter (referred to
as CrowdMLP) is presented, which probes into modelling global dependencies of
embeddings and regressing total counts by devising a multi-granularity MLP
regressor. In specific, a locally-focused pre-trained frontend is cascaded to
extract crude feature maps with intrinsic spatial cues, which prevent the model
from collapsing into trivial outcomes. The crude embeddings, along with raw
crowd scenes, are tokenized at different granularity levels. The
multi-granularity MLP then proceeds to mix tokens at the dimensions of
cardinality, channel, and spatial for mining global information. An effective
proxy task, namely Split-Counting, is also proposed to evade the barrier of
limited samples and the shortage of spatial hints in a self-supervised manner.
Extensive experiments demonstrate that CrowdMLP significantly outperforms
existing weakly-supervised counting algorithms and performs on par with
state-of-the-art location-level supervised approaches.",0,1,0,0,1,0,0.969113,5.0,0.92737,58
a3351d9f-194b-48b0-b3d1-2aef59ea4a18,Skill-Based Reinforcement Learning with Intrinsic Reward Matching,3,0.00416298,0.0944769,"While unsupervised skill discovery has shown promise in autonomously
acquiring behavioral primitives, there is still a large methodological
disconnect between task-agnostic skill pretraining and downstream, task-aware
finetuning. We present Intrinsic Reward Matching (IRM), which unifies these two
phases of learning via the $\textit{skill discriminator}$, a pretraining model
component often discarded during finetuning. Conventional approaches finetune
pretrained agents directly at the policy level, often relying on expensive
environment rollouts to empirically determine the optimal skill. However, often
the most concise yet complete description of a task is the reward function
itself, and skill learning methods learn an $\textit{intrinsic}$ reward
function via the discriminator that corresponds to the skill policy. We propose
to leverage the skill discriminator to $\textit{match}$ the intrinsic and
downstream task rewards and determine the optimal skill for an unseen task
without environment samples, consequently finetuning with greater
sample-efficiency. Furthermore, we generalize IRM to sequence skills for
complex, long-horizon tasks and demonstrate that IRM enables us to utilize
pretrained skills far more effectively than previous skill selection methods on
both the Fetch tabletop and Franka Kitchen robot manipulation benchmarks.",1,0,0,0,0,0,0.045701,9.0,0.480914,35
00d0c032-215a-491a-b3a5-a1a0976cafae,Predictive Object-Centric Process Monitoring,2,0.0126899,0.282477,"The automation and digitalization of business processes has resulted in large
amounts of data captured in information systems, which can aid businesses in
understanding their processes better, improve workflows, or provide operational
support. By making predictions about ongoing processes, bottlenecks can be
identified and resources reallocated, as well as insights gained into the state
of a process instance (case). Traditionally, data is extracted from systems in
the form of an event log with a single identifying case notion, such as an
order id for an Order to Cash (O2C) process. However, real processes often have
multiple object types, for example, order, item, and package, so a format that
forces the use of a single case notion does not reflect the underlying
relations in the data. The Object-Centric Event Log (OCEL) format was
introduced to correctly capture this information. The state-of-the-art
predictive methods have been tailored to only traditional event logs. This
thesis shows that a prediction method utilizing Generative Adversarial Networks
(GAN), Long Short-Term Memory (LSTM) architectures, and Sequence to Sequence
models (Seq2seq), can be augmented with the rich data contained in OCEL.
Objects in OCEL can have attributes that are useful in predicting the next
event and timestamp, such as a priority class attribute for an object type
package indicating slower or faster processing. In the metrics of sequence
similarity of predicted remaining events and mean absolute error (MAE) of the
timestamp, the approach in this thesis matches or exceeds previous research,
depending on whether selected object attributes are useful features for the
model. Additionally, this thesis provides a web interface to predict the next
sequence of activities from user input.",0,1,0,0,0,0,0.0723673,12.0,0.650157,55
0778424a-9750-4dd2-bf99-db4820254ffd,DexTransfer: Real World Multi-fingered Dexterous Grasping with Minimal Human Demonstrations,8,0.303099,0.626191,"Teaching a multi-fingered dexterous robot to grasp objects in the real world
has been a challenging problem due to its high dimensional state and action
space. We propose a robot-learning system that can take a small number of human
demonstrations and learn to grasp unseen object poses given partially occluded
observations. Our system leverages a small motion capture dataset and generates
a large dataset with diverse and successful trajectories for a multi-fingered
robot gripper. By adding domain randomization, we show that our dataset
provides robust grasping trajectories that can be transferred to a policy
learner. We train a dexterous grasping policy that takes the point clouds of
the object as input and predicts continuous actions to grasp objects from
different initial robot states. We evaluate the effectiveness of our system on
a 22-DoF floating Allegro Hand in simulation and a 23-DoF Allegro robot hand
with a KUKA arm in real world. The policy learned from our dataset can
generalize well on unseen object poses in both simulation and the real world",1,1,0,1,0,0,0.980246,7.0,0.965396,46
409d36d5-bb47-4b28-8830-4077a0e4dcec,Addressing Randomness in Evaluation Protocols for Out-of-Distribution Detection,2,0.0281089,0.15381,"Deep Neural Networks for classification behave unpredictably when confronted
with inputs not stemming from the training distribution. This motivates
out-of-distribution detection (OOD) mechanisms. The usual lack of prior
information on out-of-distribution data renders the performance estimation of
detection approaches on unseen data difficult. Several contemporary evaluation
protocols are based on open set simulations, which average the performance over
up to five synthetic random splits of a dataset into in- and
out-of-distribution samples. However, the number of possible splits may be much
larger, and the performance of Deep Neural Networks is known to fluctuate
significantly depending on different sources of random variation. We
empirically demonstrate that current protocols may fail to provide reliable
estimates of the expected performance of OOD methods. By casting this
evaluation as a random process, we generalize the concept of open set
simulations and propose to estimate the performance of OOD methods using a
Monte Carlo approach that addresses the randomness.",0,0,0,0,0,0,0.928379,10.0,0.935996,35
4d1dc8df-c046-4199-af10-d99d8acde633,Consistency-based Self-supervised Learning for Temporal Anomaly Localization,8,0.0280989,0.233521,"This work tackles Weakly Supervised Anomaly detection, in which a predictor
is allowed to learn not only from normal examples but also from a few labeled
anomalies made available during training. In particular, we deal with the
localization of anomalous activities within the video stream: this is a very
challenging scenario, as training examples come only with video-level
annotations (and not frame-level). Several recent works have proposed various
regularization terms to address it i.e. by enforcing sparsity and smoothness
constraints over the weakly-learned frame-level anomaly scores. In this work,
we get inspired by recent advances within the field of self-supervised learning
and ask the model to yield the same scores for different augmentations of the
same video sequence. We show that enforcing such an alignment improves the
performance of the model on XD-Violence.",0,1,0,0,0,0,0.121538,9.0,0.594125,55
2e35c5b6-a44f-4482-ad4f-8b823b2e1bb3,SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers,7,0.300226,0.446176,"This paper aims to improve the performance of text-to-SQL parsing by
exploring the intrinsic uncertainties in the neural network based approaches
(called SUN). From the data uncertainty perspective, it is indisputable that a
single SQL can be learned from multiple semantically-equivalent
questions.Different from previous methods that are limited to one-to-one
mapping, we propose a data uncertainty constraint to explore the underlying
complementary semantic information among multiple semantically-equivalent
questions (many-to-one) and learn the robust feature representations with
reduced spurious associations. In this way, we can reduce the sensitivity of
the learned representations and improve the robustness of the parser. From the
model uncertainty perspective, there is often structural information
(dependence) among the weights of neural networks. To improve the
generalizability and stability of neural text-to-SQL parsers, we propose a
model uncertainty constraint to refine the query representations by enforcing
the output representations of different perturbed encoding networks to be
consistent with each other. Extensive experiments on five benchmark datasets
demonstrate that our method significantly outperforms strong competitors and
achieves new state-of-the-art results. For reproducibility, we release our code
and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/sunsql.",1,1,0,0,1,0,0.942038,6.0,0.906195,50
f7bf7dbc-3029-40ca-a654-966068c77df3,Local Directional Gradient Pattern: A Local Descriptor for Face Recognition,75,0.25367,0.915713,"In this paper a local pattern descriptor in high order derivative space is
proposed for face recognition. The proposed local directional gradient pattern
(LDGP) is a 1D local micropattern computed by encoding the relationships
between the higher order derivatives of the reference pixel in four distinct
directions. The proposed descriptor identifies the relationship between the
high order derivatives of the referenced pixel in four different directions to
compute the micropattern which corresponds to the local feature. Proposed
descriptor considerably reduces the length of the micropattern which
consequently reduces the extraction time and matching time while maintaining
the recognition rate. Results of the extensive experiments conducted on
benchmark databases AT&T, Extended Yale B and CMU-PIE show that the proposed
descriptor significantly reduces the extraction as well as matching time while
the recognition rate is almost similar to the existing state of the art
methods.",0,1,0,0,0,0,0.190406,12.0,0.73631,23
e966dabd-48b7-4655-bbb2-31dab547120a,Neural Token Segmentation for High Token-Internal Complexity,5,0.11225,0.0658888,"Tokenizing raw texts into word units is an essential pre-processing step for
critical tasks in the NLP pipeline such as tagging, parsing, named entity
recognition, and more. For most languages, this tokenization step
straightforward. However, for languages with high token-internal complexity,
further token-to-word segmentation is required. Previous canonical segmentation
studies were based on character-level frameworks, with no contextualised
representation involved. Contextualized vectors a la BERT show remarkable
results in many applications, but were not shown to improve performance on
linguistic segmentation per se. Here we propose a novel neural segmentation
model which combines the best of both worlds, contextualised token
representation and char-level decoding, which is particularly effective for
languages with high token-internal complexity and extreme morphological
ambiguity. Our model shows substantial improvements in segmentation accuracy on
Hebrew and Arabic compared to the state-of-the-art, and leads to further
improvements on downstream tasks such as Part-of-Speech Tagging, Dependency
Parsing and Named-Entity Recognition, over existing pipelines. When comparing
our segmentation-first pipeline with joint segmentation and labeling in the
same settings, we show that, contrary to pre-neural studies, the pipeline
performance is superior.",1,1,0,0,1,0,0.282396,10.0,0.728745,25
2d65fea0-e537-42c0-81dd-f23ed755c43b,TE2Rules: Explaining Tree Ensembles using Rules,2,0.00596717,0.216296,"Tree Ensemble (TE) models, such as Gradient Boosted Trees, often achieve
optimal performance on tabular datasets, yet their lack of transparency poses
challenges for comprehending their decision logic. This paper introduces
TE2Rules (Tree Ensemble to Rules), a novel approach for explaining binary
classification tree ensemble models through a list of rules, particularly
focusing on explaining the minority class. Many state-of-the-art explainers
struggle with minority class explanations, making TE2Rules valuable in such
cases. The rules generated by TE2Rules closely approximate the original model,
ensuring high fidelity, providing an accurate and interpretable means to
understand decision-making. Experimental results demonstrate that TE2Rules
scales effectively to tree ensembles with hundreds of trees, achieving higher
fidelity within runtimes comparable to baselines. TE2Rules allows for a
trade-off between runtime and fidelity, enhancing its practical applicability.
The implementation is available here: https://github.com/linkedin/TE2Rules.",0,1,0,0,1,0,0.247088,12.0,0.760925,19
0683430d-e080-422b-82d4-ba3cc649d3b7,Rethinking Knowledge Graph Evaluation Under the Open-World Assumption,5,0.0645566,0.486036,"Most knowledge graphs (KGs) are incomplete, which motivates one important
research topic on automatically complementing knowledge graphs. However,
evaluation of knowledge graph completion (KGC) models often ignores the
incompleteness -- facts in the test set are ranked against all unknown triplets
which may contain a large number of missing facts not included in the KG yet.
Treating all unknown triplets as false is called the closed-world assumption.
This closed-world assumption might negatively affect the fairness and
consistency of the evaluation metrics. In this paper, we study KGC evaluation
under a more realistic setting, namely the open-world assumption, where unknown
triplets are considered to include many missing facts not included in the
training or test sets. For the currently most used metrics such as mean
reciprocal rank (MRR) and Hits@K, we point out that their behavior may be
unexpected under the open-world assumption. Specifically, with not many missing
facts, their numbers show a logarithmic trend with respect to the true strength
of the model, and thus, the metric increase could be insignificant in terms of
reflecting the true model improvement. Further, considering the variance, we
show that the degradation in the reported numbers may result in incorrect
comparisons between different models, where stronger models may have lower
metric numbers. We validate the phenomenon both theoretically and
experimentally. Finally, we suggest possible causes and solutions for this
problem. Our code and data are available at
https://github.com/GraphPKU/Open-World-KG .",1,0,0,0,0,0,0.694588,9.0,0.840136,30
f16b640b-e135-48b7-a93f-0cce5e538aa5,Wav2Vec-Aug: Improved self-supervised training with limited data,14,0.0433668,0.345182,"Self-supervised learning (SSL) of speech representations has received much
attention over the last few years but most work has focused on languages and
domains with an abundance of unlabeled data. However, for many languages there
is a shortage even in the unlabeled data which limits the effectiveness of SSL.
In this work, we focus on the problem of applying SSL to domains with limited
available data by leveraging data augmentation for Wav2Vec 2.0 pretraining.
Further, we propose improvements to each component of the model which result in
a combined relative word error rate (WER) improvement of up to 13% compared to
Wav2Vec 2.0 on Librispeech test-clean / other.",0,1,0,0,1,0,0.763489,7.0,0.822347,29
07c22fc3-f818-4db8-b835-f5110e3b5698,A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning,4,0.0409707,0.117033,"Subword tokenization is a commonly used input pre-processing step in most
recent NLP models. However, it limits the models' ability to leverage
end-to-end task learning. Its frequency-based vocabulary creation compromises
tokenization in low-resource languages, leading models to produce suboptimal
representations. Additionally, the dependency on a fixed vocabulary limits the
subword models' adaptability across languages and domains. In this work, we
propose a vocabulary-free neural tokenizer by distilling segmentation
information from heuristic-based subword tokenization. We pre-train our
character-based tokenizer by processing unique words from multilingual corpus,
thereby extensively increasing word diversity across languages. Unlike the
predefined and fixed vocabularies in subword methods, our tokenizer allows
end-to-end task learning, resulting in optimal task-specific tokenization. The
experimental results show that replacing the subword tokenizer with our neural
tokenizer consistently improves performance on multilingual (NLI) and
code-switching (sentiment analysis) tasks, with larger gains in low-resource
languages. Additionally, our neural tokenizer exhibits a robust performance on
downstream tasks when adversarial noise is present (typos and misspelling),
further increasing the initial improvements over statistical subword
tokenizers.",0,0,0,0,0,0,0.498278,8.0,0.752385,24
8d7bf0d5-651f-4fde-9da8-19d4b55a2f8f,Correspondence Distillation from NeRF-based GAN,6,0.0387645,0.313276,"The neural radiance field (NeRF) has shown promising results in preserving
the fine details of objects and scenes. However, unlike mesh-based
representations, it remains an open problem to build dense correspondences
across different NeRFs of the same category, which is essential in many
downstream tasks. The main difficulties of this problem lie in the implicit
nature of NeRF and the lack of ground-truth correspondence annotations. In this
paper, we show it is possible to bypass these challenges by leveraging the rich
semantics and structural priors encapsulated in a pre-trained NeRF-based GAN.
Specifically, we exploit such priors from three aspects, namely 1) a dual
deformation field that takes latent codes as global structural indicators, 2) a
learning objective that regards generator features as geometric-aware local
descriptors, and 3) a source of infinite object-specific NeRF samples. Our
experiments demonstrate that such priors lead to 3D dense correspondence that
is accurate, smooth, and robust. We also show that established dense
correspondence across NeRFs can effectively enable many NeRF-based downstream
applications such as texture transfer.",0,0,0,0,0,0,0.794405,6.0,0.808192,81
b2d28eb0-8834-45c3-992a-7d9e03a6c4fe,Evaluating and Inducing Personality in Pre-trained Language Models,20,0.184144,0.826481,"Standardized and quantified evaluation of machine behaviors is a crux of
understanding LLMs. In this study, we draw inspiration from psychometric
studies by leveraging human personality theory as a tool for studying machine
behaviors. Originating as a philosophical quest for human behaviors, the study
of personality delves into how individuals differ in thinking, feeling, and
behaving. Toward building and understanding human-like social machines, we are
motivated to ask: Can we assess machine behaviors by leveraging human
psychometric tests in a principled and quantitative manner? If so, can we
induce a specific personality in LLMs? To answer these questions, we introduce
the Machine Personality Inventory (MPI) tool for studying machine behaviors;
MPI follows standardized personality tests, built upon the Big Five Personality
Factors (Big Five) theory and personality assessment inventories. By
systematically evaluating LLMs with MPI, we provide the first piece of evidence
demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise
a Personality Prompting (P^2) method to induce LLMs with specific personalities
in a controllable way, capable of producing diverse and verifiable behaviors.
We hope this work sheds light on future studies by adopting personality as the
essential indicator for various downstream tasks, and could further motivate
research into equally intriguing human-like machine behaviors.",0,0,0,0,0,0,0.884586,4.0,0.790078,84
7d1d5f1e-14d5-4d9f-9e7e-cfe42911e94f,The Quest for a Common Model of the Intelligent Decision Maker,12,0.429547,0.585173,"The premise of the Multi-disciplinary Conference on Reinforcement Learning
and Decision Making is that multiple disciplines share an interest in
goal-directed decision making over time. The idea of this paper is to sharpen
and deepen this premise by proposing a perspective on the decision maker that
is substantive and widely held across psychology, artificial intelligence,
economics, control theory, and neuroscience, which I call the ""common model of
the intelligent agent"". The common model does not include anything specific to
any organism, world, or application domain. The common model does include
aspects of the decision maker's interaction with its world (there must be input
and output, and a goal) and internal components of the decision maker (for
perception, decision-making, internal evaluation, and a world model). I
identify these aspects and components, note that they are given different names
in different disciplines but refer essentially to the same ideas, and discuss
the challenges and benefits of devising a neutral terminology that can be used
across disciplines. It is time to recognize and build on the convergence of
multiple diverse disciplines on a substantive common model of the intelligent
agent.",0,0,0,0,0,1,0.387719,43.0,0.946008,9
c822a27f-16b2-46a4-a777-5cce204f140e,Learning Optical Flow with Adaptive Graph Reasoning,33,0.130809,0.957767,"Estimating per-pixel motion between video frames, known as optical flow, is a
long-standing problem in video understanding and analysis. Most contemporary
optical flow techniques largely focus on addressing the cross-image matching
with feature similarity, with few methods considering how to explicitly reason
over the given scene for achieving a holistic motion understanding. In this
work, taking a fresh perspective, we introduce a novel graph-based approach,
called adaptive graph reasoning for optical flow (AGFlow), to emphasize the
value of scene/context information in optical flow. Our key idea is to decouple
the context reasoning from the matching procedure, and exploit scene
information to effectively assist motion estimation by learning to reason over
the adaptive graph. The proposed AGFlow can effectively exploit the context
information and incorporate it within the matching procedure, producing more
robust and accurate results. On both Sintel clean and final passes, our AGFlow
achieves the best accuracy with EPE of 1.43 and 2.47 pixels, outperforming
state-of-the-art approaches by 11.2% and 13.6%, respectively.",1,0,0,0,1,0,0.345596,7.0,0.647516,41
f8e207f2-9965-424d-9792-35789e5f73ab,BITE: Textual Backdoor Attacks with Iterative Trigger Injection,17,0.387321,0.831134,"Backdoor attacks have become an emerging threat to NLP systems. By providing
poisoned training data, the adversary can embed a ""backdoor"" into the victim
model, which allows input instances satisfying certain textual patterns (e.g.,
containing a keyword) to be predicted as a target label of the adversary's
choice. In this paper, we demonstrate that it is possible to design a backdoor
attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a
high attack success rate). We propose BITE, a backdoor attack that poisons the
training data to establish strong correlations between the target label and a
set of ""trigger words"". These trigger words are iteratively identified and
injected into the target-label instances through natural word-level
perturbations. The poisoned training data instruct the victim model to predict
the target label on inputs containing trigger words, forming the backdoor.
Experiments on four text classification datasets show that our proposed attack
is significantly more effective than baseline methods while maintaining decent
stealthiness, raising alarm on the usage of untrusted training data. We further
propose a defense method named DeBITE based on potential trigger word removal,
which outperforms existing methods in defending against BITE and generalizes
well to handling other backdoor attacks.",1,1,0,0,0,0,0.920128,4.0,0.829431,68
7f515285-0389-454a-8315-97d66c2dd888,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,3644,0.999693,1.0,"We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.",0,1,0,0,1,0,0.788785,2.0,0.41598,118
8d651294-1ee4-4977-8adf-daf2a9a2df11,Concrete Score Matching: Generalized Score Matching for Discrete Data,19,0.158569,0.95781,"Representing probability distributions by the gradient of their density
functions has proven effective in modeling a wide range of continuous data
modalities. However, this representation is not applicable in discrete domains
where the gradient is undefined. To this end, we propose an analogous score
function called the ""Concrete score"", a generalization of the (Stein) score for
discrete settings. Given a predefined neighborhood structure, the Concrete
score of any input is defined by the rate of change of the probabilities with
respect to local directional changes of the input. This formulation allows us
to recover the (Stein) score in continuous domains when measuring such changes
by the Euclidean distance, while using the Manhattan distance leads to our
novel score function in discrete domains. Finally, we introduce a new framework
to learn such scores from samples called Concrete Score Matching (CSM), and
propose an efficient training objective to scale our approach to high
dimensions. Empirically, we demonstrate the efficacy of CSM on density
estimation tasks on a mixture of synthetic, tabular, and high-dimensional image
datasets, and demonstrate that it performs favorably relative to existing
baselines for modeling discrete data.",0,0,1,0,0,0,0.385331,8.0,0.708799,82
8970f13a-3a0f-46e3-badb-07640a18d1ea,Interactive Segmentation of Radiance Fields,31,0.862513,0.837679,"Radiance Fields (RF) are popular to represent casually-captured scenes for
new view synthesis and several applications beyond it. Mixed reality on
personal spaces needs understanding and manipulating scenes represented as RFs,
with semantic segmentation of objects as an important step. Prior segmentation
efforts show promise but don't scale to complex objects with diverse
appearance. We present the ISRF method to interactively segment objects with
fine structure and appearance. Nearest neighbor feature matching using
distilled semantic features identifies high-confidence seed regions. Bilateral
search in a joint spatio-semantic space grows the region to recover accurate
segmentation. We show state-of-the-art results of segmenting objects from RFs
and compositing them to another scene, changing appearance, etc., and an
interactive segmentation tool that others can use.
  Project Page: https://rahul-goel.github.io/isrf/",1,1,0,0,1,0,0.988142,3.0,0.960014,68
4ccd492a-579b-495c-b6f2-8441cae1830f,PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects,25,0.480856,0.84623,"Object pose estimation is crucial for robotic applications and augmented
reality. Beyond instance level 6D object pose estimation methods, estimating
category-level pose and shape has become a promising trend. As such, a new
research field needs to be supported by well-designed datasets. To provide a
benchmark with high-quality ground truth annotations to the community, we
introduce a multimodal dataset for category-level object pose estimation with
photometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high
quality 3D models of household objects over 8 categories including highly
reflective, transparent and symmetric objects. We developed a novel
robot-supported multi-modal (RGB, depth, polarisation) data acquisition and
annotation process. It ensures sub-millimeter accuracy of the pose for opaque
textured, shiny and transparent objects, no motion blur and perfect camera
synchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and
monocular RGB methods are evaluated on the challenging scenes of PhoCaL.",0,0,0,0,0,0,0.714601,9.0,0.846311,45
6ed4c5ce-1c86-4c13-89e8-39d1ea45cb0d,xCloth: Extracting Template-free Textured 3D Clothes from a Monocular Image,8,0.140944,0.415682,"Existing approaches for 3D garment reconstruction either assume a predefined
template for the garment geometry (restricting them to fixed clothing styles)
or yield vertex colored meshes (lacking high-frequency textural details). Our
novel framework co-learns geometric and semantic information of garment surface
from the input monocular image for template-free textured 3D garment
digitization. More specifically, we propose to extend PeeledHuman
representation to predict the pixel-aligned, layered depth and semantic maps to
extract 3D garments. The layered representation is further exploited to UV
parametrize the arbitrary surface of the extracted garment without any human
intervention to form a UV atlas. The texture is then imparted on the UV atlas
in a hybrid fashion by first projecting pixels from the input image to UV space
for the visible region, followed by inpainting the occluded regions. Thus, we
are able to digitize arbitrarily loose clothing styles while retaining
high-frequency textural details from a monocular image. We achieve
high-fidelity 3D garment reconstruction results on three publicly available
datasets and generalization on internet images.",0,1,0,0,0,0,0.813968,7.0,0.84435,27
d38edc56-2fc9-4e22-9ab6-3ee50de6b89b,Towards Unification of Discourse Annotation Frameworks,4,0.0805477,0.128325,"Discourse information is difficult to represent and annotate. Among the major
frameworks for annotating discourse information, RST, PDTB and SDRT are widely
discussed and used, each having its own theoretical foundation and focus.
Corpora annotated under different frameworks vary considerably. To make better
use of the existing discourse corpora and achieve the possible synergy of
different frameworks, it is worthwhile to investigate the systematic relations
between different frameworks and devise methods of unifying the frameworks.
Although the issue of framework unification has been a topic of discussion for
a long time, there is currently no comprehensive approach which considers
unifying both discourse structure and discourse relations and evaluates the
unified framework intrinsically and extrinsically. We plan to use automatic
means for the unification task and evaluate the result with structural
complexity and downstream tasks. We will also explore the application of the
unified framework in multi-task learning and graphical models.",1,0,0,0,0,0,0.0909005,17.0,0.76705,65
b7dc8197-b93d-47ea-b6aa-18a9bb0d7892,Experimental analysis regarding the influence of iris segmentation on the recognition rate,26,0.636404,0.152589,"In this study the authors will look at the detection and segmentation of the
iris and its influence on the overall performance of the iris-biometric tool
chain. The authors will examine whether the segmentation accuracy, based on
conformance with a ground truth, can serve as a predictor for the overall
performance of the iris-biometric tool chain. That is: If the segmentation
accuracy is improved will this always improve the overall performance?
Furthermore, the authors will systematically evaluate the influence of
segmentation parameters, pupillary and limbic boundary and normalisation centre
(based on Daugman's rubbersheet model), on the rest of the iris-biometric tool
chain. The authors will investigate if accurately finding these parameters is
important and how consistency, that is, extracting the same exact region of the
iris during segmenting, influences the overall performance.",0,1,0,0,0,0,0.362737,13.0,0.814871,47
ae842b0e-42a4-4cd2-961f-3df250facb12,Where to go: Agent Guidance with Deep Reinforcement Learning in A City-Scale Online Ride-Hailing Service,1,0.0612491,0.195602,"Online ride-hailing services have become a prevalent transportation system
across the world. In this paper, we study a challenging problem of how to
direct vacant taxis around a city such that supplies and demands can be
balanced in online ride-hailing services. We design a new reward scheme that
considers multiple performance metrics of online ride-hailing services. We also
propose a novel deep reinforcement learning method named Deep-Q-Network with
Action Mask (AM-DQN) masking off unnecessary actions in various locations such
that agents can learn much faster and more efficiently. We conduct extensive
experiments using a city-scale dataset from Chicago. Several popular heuristic
and learning methods are also implemented as baselines for comparison. The
results of the experiments show that the AM-DQN attains the best performances
of all methods with respect to average failure rate, average waiting time for
customers, and average idle search time for vacant taxis.",0,1,0,0,1,0,0.348088,10.0,0.754157,22
99c4916c-ad71-44a5-a9c3-6e6e37c46629,Neural Enhanced Belief Propagation for Data Association in Multiobject Tracking,12,0.0234936,0.571702,"Situation-aware technologies enabled by multiobject tracking (MOT) methods
will create new services and applications in fields such as autonomous
navigation and applied ocean sciences. Belief propagation (BP) is a
state-of-the-art method for Bayesian MOT but fully relies on a statistical
model and preprocessed sensor measurements. In this paper, we establish a
hybrid method for model-based and data-driven MOT. The proposed neural enhanced
belief propagation (NEBP) approach complements BP by information learned from
raw sensor data with the goal to improve data association and to reject false
alarm measurements. We evaluate the performance of our NEBP approach for MOT on
the nuScenes autonomous driving dataset and demonstrate that it can outperform
state-of-the-art reference methods.",0,1,0,0,1,0,0.153852,7.0,0.514453,48
cac84599-4755-4957-9f6a-fa41c58b55da,Implementing Deep Learning-Based Approaches for Article Summarization in Indian Languages,5,0.056336,0.440309,"The research on text summarization for low-resource Indian languages has been
limited due to the availability of relevant datasets. This paper presents a
summary of various deep-learning approaches used for the ILSUM 2022 Indic
language summarization datasets. The ISUM 2022 dataset consists of news
articles written in Indian English, Hindi, and Gujarati respectively, and their
ground-truth summarizations. In our work, we explore different pre-trained
seq2seq models and fine-tune those with the ILSUM 2022 datasets. In our case,
the fine-tuned SoTA PEGASUS model worked the best for English, the fine-tuned
IndicBART model with augmented data for Hindi, and again fine-tuned PEGASUS
model along with a translation mapping-based approach for Gujarati. Our scores
on the obtained inferences were evaluated using ROUGE-1, ROUGE-2, and ROUGE-4
as the evaluation metrics.",0,1,0,1,0,0,0.364571,7.0,0.6571,43
7e7ec353-42d8-437e-8351-976ba08c1252,Self-Supervised Leaf Segmentation under Complex Lighting Conditions,12,0.181861,0.389368,"As an essential prerequisite task in image-based plant phenotyping, leaf
segmentation has garnered increasing attention in recent years. While
self-supervised learning is emerging as an effective alternative to various
computer vision tasks, its adaptation for image-based plant phenotyping remains
rather unexplored. In this work, we present a self-supervised leaf segmentation
framework consisting of a self-supervised semantic segmentation model, a
color-based leaf segmentation algorithm, and a self-supervised color correction
model. The self-supervised semantic segmentation model groups the semantically
similar pixels by iteratively referring to the self-contained information,
allowing the pixels of the same semantic object to be jointly considered by the
color-based leaf segmentation algorithm for identifying the leaf regions.
Additionally, we propose to use a self-supervised color correction model for
images taken under complex illumination conditions. Experimental results on
datasets of different plant species demonstrate the potential of the proposed
self-supervised framework in achieving effective and generalizable leaf
segmentation.",1,1,0,0,0,0,0.442244,10.0,0.785242,66
a0abac83-c20f-4704-bb52-932fb441761e,Scaling Up Probabilistic Circuits by Latent Variable Distillation,15,0.046177,0.881855,"Probabilistic Circuits (PCs) are a unified framework for tractable
probabilistic models that support efficient computation of various
probabilistic queries (e.g., marginal probabilities). One key challenge is to
scale PCs to model large and high-dimensional real-world datasets: we observe
that as the number of parameters in PCs increases, their performance
immediately plateaus. This phenomenon suggests that the existing optimizers
fail to exploit the full expressive power of large PCs. We propose to overcome
such bottleneck by latent variable distillation: we leverage the less tractable
but more expressive deep generative models to provide extra supervision over
the latent variables of PCs. Specifically, we extract information from
Transformer-based generative models to assign values to latent variables of
PCs, providing guidance to PC optimizers. Experiments on both image and
language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent
variable distillation substantially boosts the performance of large PCs
compared to their counterparts without latent variable distillation. In
particular, on the image modeling benchmarks, PCs achieve competitive
performance against some of the widely-used deep generative models, including
variational autoencoders and flow-based models, opening up new avenues for
tractable generative modeling.",1,0,0,0,0,0,0.0918154,8.0,0.506294,37
80d7d81b-bb0f-4734-b752-1f79a887d519,Prompt-Based Metric Learning for Few-Shot NER,11,0.563089,0.756352,"Few-shot named entity recognition (NER) targets generalizing to unseen labels
and/or domains with few labeled examples. Existing metric learning methods
compute token-level similarities between query and support sets, but are not
able to fully incorporate label semantics into modeling. To address this issue,
we propose a simple method to largely improve metric learning for NER: 1)
multiple prompt schemas are designed to enhance label semantics; 2) we propose
a novel architecture to effectively combine multiple prompt-based
representations. Empirically, our method achieves new state-of-the-art (SOTA)
results under 16 of the 18 considered settings, substantially outperforming the
previous SOTA by an average of 8.84% and a maximum of 34.51% in relative gains
of micro F1. Our code is available at https://github.com/AChen-qaq/ProML.",1,1,0,0,1,0,0.988214,7.0,0.983059,37
668e9b79-d94f-4513-9b4e-cbb4ba14d8f8,NeReF: Neural Refractive Field for Fluid Surface Reconstruction and Implicit Representation,6,0.0409836,0.45294,"Existing neural reconstruction schemes such as Neural Radiance Field (NeRF)
are largely focused on modeling opaque objects. We present a novel neural
refractive field(NeReF) to recover wavefront of transparent fluids by
simultaneously estimating the surface position and normal of the fluid front.
Unlike prior arts that treat the reconstruction target as a single layer of the
surface, NeReF is specifically formulated to recover a volumetric normal field
with its corresponding density field. A query ray will be refracted by NeReF
according to its accumulated refractive point and normal, and we employ the
correspondences and uniqueness of refracted ray for NeReF optimization. We show
NeReF, as a global optimization scheme, can more robustly tackle refraction
distortions detrimental to traditional methods for correspondence matching.
Furthermore, the continuous NeReF representation of wavefront enables view
synthesis as well as normal integration. We validate our approach on both
synthetic and real data and show it is particularly suitable for sparse
multi-view acquisition. We hence build a small light field array and experiment
on various surface shapes to demonstrate high fidelity NeReF reconstruction.",0,0,1,0,0,0,0.126937,10.0,0.63936,40
b8310d04-9e71-4ff5-ab21-2caaba1a6d62,REPAIR: REnormalizing Permuted Activations for Interpolation Repair,45,0.217025,0.985914,"In this paper we look into the conjecture of Entezari et al. (2021) which
states that if the permutation invariance of neural networks is taken into
account, then there is likely no loss barrier to the linear interpolation
between SGD solutions. First, we observe that neuron alignment methods alone
are insufficient to establish low-barrier linear connectivity between SGD
solutions due to a phenomenon we call variance collapse: interpolated deep
networks suffer a collapse in the variance of their activations, causing poor
performance. Next, we propose REPAIR (REnormalizing Permuted Activations for
Interpolation Repair) which mitigates variance collapse by rescaling the
preactivations of such interpolated networks. We explore the interaction
between our method and the choice of normalization layer, network width, and
depth, and demonstrate that using REPAIR on top of neuron alignment methods
leads to 60%-100% relative barrier reduction across a wide variety of
architecture families and tasks. In particular, we report a 74% barrier
reduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on
CIFAR10.",1,0,0,0,0,0,0.161308,10.0,0.665279,54
6fa824cd-5df0-4494-9b21-21d6e64682cf,Augmenting Knowledge Graphs for Better Link Prediction,9,0.114453,0.374277,"Embedding methods have demonstrated robust performance on the task of link
prediction in knowledge graphs, by mostly encoding entity relationships. Recent
methods propose to enhance the loss function with a literal-aware term. In this
paper, we propose KGA: a knowledge graph augmentation method that incorporates
literals in an embedding model without modifying its loss function. KGA
discretizes quantity and year values into bins, and chains these bins both
horizontally, modeling neighboring values, and vertically, modeling multiple
levels of granularity. KGA is scalable and can be used as a pre-processing step
for any existing knowledge graph embedding model. Experiments on legacy
benchmarks and a new large benchmark, DWD, show that augmenting the knowledge
graph with quantities and years is beneficial for predicting both entities and
numbers, as KGA outperforms the vanilla models and other relevant baselines.
Our ablation studies confirm that both quantities and years contribute to KGA's
performance, and that its performance depends on the discretization and binning
settings. We make the code, models, and the DWD benchmark publicly available to
facilitate reproducibility and future research.",1,1,0,1,0,0,0.37995,10.0,0.765233,36
37b5ebcc-ce7d-4279-beaa-c166291ce38e,LCRL: Certified Policy Synthesis via Logically-Constrained Reinforcement Learning,11,0.179331,0.213691,"LCRL is a software tool that implements model-free Reinforcement Learning
(RL) algorithms over unknown Markov Decision Processes (MDPs), synthesising
policies that satisfy a given linear temporal specification with maximal
probability. LCRL leverages partially deterministic finite-state machines known
as Limit Deterministic Buchi Automata (LDBA) to express a given linear temporal
specification. A reward function for the RL algorithm is shaped on-the-fly,
based on the structure of the LDBA. Theoretical guarantees under proper
assumptions ensure the convergence of the RL algorithm to an optimal policy
that maximises the satisfaction probability. We present case studies to
demonstrate the applicability, ease of use, scalability, and performance of
LCRL. Owing to the LDBA-guided exploration and LCRL model-free architecture, we
observe robust performance, which also scales well when compared to standard RL
approaches (whenever applicable to LTL specifications). Full instructions on
how to execute all the case studies in this paper are provided on a GitHub page
that accompanies the LCRL distribution www.github.com/grockious/lcrl.",1,0,0,0,0,0,0.262949,9.0,0.689271,46
668dbf34-0816-4f0b-b4d4-aa2eb3faecca,Unobserved Local Structures Make Compositional Generalization Hard,28,0.31525,0.506526,"While recent work has convincingly showed that sequence-to-sequence models
struggle to generalize to new compositions (termed compositional
generalization), little is known on what makes compositional generalization
hard on a particular test instance. In this work, we investigate what are the
factors that make generalization to certain test instances challenging. We
first substantiate that indeed some examples are more difficult than others by
showing that different models consistently fail or succeed on the same test
instances. Then, we propose a criterion for the difficulty of an example: a
test instance is hard if it contains a local structure that was not observed at
training time. We formulate a simple decision rule based on this criterion and
empirically show it predicts instance-level generalization well across 5
different semantic parsing datasets, substantially better than alternative
decision rules. Last, we show local structures can be leveraged for creating
difficult adversarial compositional splits and also to improve compositional
generalization under limited training budgets by strategically selecting
examples for the training set.",1,0,0,0,0,0,0.742031,5.0,0.73886,30
980e5340-8446-45eb-8620-2e68fc7afbcb,AI-based Malware and Ransomware Detection Models,6,0.349774,0.677428,"Cybercrime is one of the major digital threats of this century. In
particular, ransomware attacks have significantly increased, resulting in
global damage costs of tens of billion dollars. In this paper, we train and
test different Machine Learning and Deep Learning models for malware detection,
malware classification and ransomware detection. We introduce a novel and
flexible solution that combines two optimized models for malware and ransomware
detection. Our results demonstrate some improvements both in terms of detection
performances and flexibility. In particular, our combined models pave the way
for easier future enhancements using specialized and thus interchangeable
detection modules.",0,1,0,0,0,0,0.94929,7.0,0.926147,24
410ce9cf-9cd9-49b5-8b49-baf764f56707,Better plain ViT baselines for ImageNet-1k,57,0.304479,0.668979,"It is commonly accepted that the Vision Transformer model requires
sophisticated regularization techniques to excel at ImageNet-1k scale data.
Surprisingly, we find this is not the case and standard data augmentation is
sufficient. This note presents a few minor modifications to the original Vision
Transformer (ViT) vanilla training setting that dramatically improve the
performance of plain ViT models. Notably, 90 epochs of training surpass 76%
top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic
ResNet50 baseline, and 300 epochs of training reach 80% in less than one day.",0,1,0,0,0,0,0.968563,3.0,0.877255,21
bd737b39-0b6b-44ba-bc70-eb1c51a5dd6f,Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers,3,0.0619349,0.298557,"Recently, RNN-Transducers have achieved remarkable results on various
automatic speech recognition tasks. However, lattice-free sequence
discriminative training methods, which obtain superior performance in hybrid
models, are rarely investigated in RNN-Transducers. In this work, we propose
three lattice-free training objectives, namely lattice-free maximum mutual
information, lattice-free segment-level minimum Bayes risk, and lattice-free
minimum Bayes risk, which are used for the final posterior output of the
phoneme-based neural transducer with a limited context dependency. Compared to
criteria using N-best lists, lattice-free methods eliminate the decoding step
for hypotheses generation during training, which leads to more efficient
training. Experimental results show that lattice-free methods gain up to 6.5%
relative improvement in word error rate compared to a sequence-level
cross-entropy trained model. Compared to the N-best-list based minimum Bayes
risk objectives, lattice-free methods gain 40% - 70% relative training time
speedup with a small degradation in performance.",0,1,0,0,0,0,0.472326,8.0,0.742893,38
d06ee1dd-a20e-4c46-9408-089a2a113c4b,Attribute-based Representations for Accurate and Interpretable Video Anomaly Detection,17,0.328936,0.779454,"Video anomaly detection (VAD) is a challenging computer vision task with many
practical applications. As anomalies are inherently ambiguous, it is essential
for users to understand the reasoning behind a system's decision in order to
determine if the rationale is sound. In this paper, we propose a simple but
highly effective method that pushes the boundaries of VAD accuracy and
interpretability using attribute-based representations. Our method represents
every object by its velocity and pose. The anomaly scores are computed using a
density-based approach. Surprisingly, we find that this simple representation
is sufficient to achieve state-of-the-art performance in ShanghaiTech, the
largest and most complex VAD dataset. Combining our interpretable
attribute-based representations with implicit, deep representation yields
state-of-the-art performance with a $99.1\%, 93.3\%$, and $85.9\%$ AUROC on
Ped2, Avenue, and ShanghaiTech, respectively. Our method is accurate,
interpretable, and easy to implement.",1,1,0,0,1,0,0.921898,9.0,0.925173,63
6d5aeedb-1205-4f6b-abcc-6d898392865d,Event-Based Dense Reconstruction Pipeline,6,0.0864591,0.542762,"Event cameras are a new type of sensors that are different from traditional
cameras. Each pixel is triggered asynchronously by event. The trigger event is
the change of the brightness irradiated on the pixel. If the increment or
decrement of brightness is higher than a certain threshold, an event is output.
Compared with traditional cameras, event cameras have the advantages of high
dynamic range and no motion blur. Since events are caused by the apparent
motion of intensity edges, the majority of 3D reconstructed maps consist only
of scene edges, i.e., semi-dense maps, which is not enough for some
applications. In this paper, we propose a pipeline to realize event-based dense
reconstruction. First, deep learning is used to reconstruct intensity images
from events. And then, structure from motion (SfM) is used to estimate camera
intrinsic, extrinsic and sparse point cloud. Finally, multi-view stereo (MVS)
is used to complete dense reconstruction.",1,1,0,0,0,0,0.462685,12.0,0.826201,32
83650ea2-1f0c-4dc0-a3c2-2ef20db349d2,Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives,23,0.306338,0.953495,"This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a
new member in the family of InfoNCE losses that preserves a ranked ordering of
positive samples. In contrast to the standard InfoNCE loss, which requires a
strict binary separation of the training pairs into similar and dissimilar
samples, RINCE can exploit information about a similarity ranking for learning
a corresponding embedding space. We show that the proposed loss function learns
favorable embeddings compared to the standard InfoNCE whenever at least noisy
ranking information can be obtained or when the definition of positives and
negatives is blurry. We demonstrate this for a supervised classification task
with additional superclass labels and noisy similarity scores. Furthermore, we
show that RINCE can also be applied to unsupervised training with experiments
on unsupervised representation learning from videos. In particular, the
embedding yields higher classification accuracy, retrieval rates and performs
better in out-of-distribution detection than the standard InfoNCE loss.",1,0,1,0,1,0,0.934918,6.0,0.899273,63
ac5c4659-6f58-41a8-99a8-8335646d2994,Symmetry Regularization and Saturating Nonlinearity for Robust Quantization,1,0.0,0.0521038,"Robust quantization improves the tolerance of networks for various
implementations, allowing reliable output in different bit-widths or fragmented
low-precision arithmetic. In this work, we perform extensive analyses to
identify the sources of quantization error and present three insights to
robustify a network against quantization: reduction of error propagation, range
clamping for error minimization, and inherited robustness against quantization.
Based on these insights, we propose two novel methods called symmetry
regularization (SymReg) and saturating nonlinearity (SatNL). Applying the
proposed methods during training can enhance the robustness of arbitrary neural
networks against quantization on existing post-training quantization (PTQ) and
quantization-aware training (QAT) algorithms and enables us to obtain a single
weight flexible enough to maintain the output quality under various conditions.
We conduct extensive studies on CIFAR and ImageNet datasets and validate the
effectiveness of the proposed methods.",0,0,0,0,0,0,0.441946,7.0,0.693073,46
4cb63f62-1b3a-411e-9722-110729d2335b,Unsupervised Text Deidentification,2,0.00425074,0.0981922,"Deidentification seeks to anonymize textual data prior to distribution.
Automatic deidentification primarily uses supervised named entity recognition
from human-labeled data points. We propose an unsupervised deidentification
method that masks words that leak personally-identifying information. The
approach utilizes a specially trained reidentification model to identify
individuals from redacted personal documents. Motivated by K-anonymity based
privacy, we generate redactions that ensure a minimum reidentification rank for
the correct profile of the document. To evaluate this approach, we consider the
task of deidentifying Wikipedia Biographies, and evaluate using an adversarial
reidentification metric. Compared to a set of unsupervised baselines, our
approach deidentifies documents more completely while removing fewer words.
Qualitatively, we see that the approach eliminates many identifying aspects
that would fall outside of the common named entity based approach.",0,1,0,0,0,0,0.0678864,7.0,0.390798,46
347b557d-c910-4521-bff6-f1deea05f31b,C2F-TCN: A Framework for Semi and Fully Supervised Temporal Action Segmentation,8,0.111982,0.634983,"Temporal action segmentation tags action labels for every frame in an input
untrimmed video containing multiple actions in a sequence. For the task of
temporal action segmentation, we propose an encoder-decoder-style architecture
named C2F-TCN featuring a ""coarse-to-fine"" ensemble of decoder outputs. The
C2F-TCN framework is enhanced with a novel model agnostic temporal feature
augmentation strategy formed by the computationally inexpensive strategy of the
stochastic max-pooling of segments. It produces more accurate and
well-calibrated supervised results on three benchmark action segmentation
datasets. We show that the architecture is flexible for both supervised and
representation learning. In line with this, we present a novel unsupervised way
to learn frame-wise representation from C2F-TCN. Our unsupervised learning
approach hinges on the clustering capabilities of the input features and the
formation of multi-resolution features from the decoder's implicit structure.
Further, we provide the first semi-supervised temporal action segmentation
results by merging representation learning with conventional supervised
learning. Our semi-supervised learning scheme, called
``Iterative-Contrastive-Classify (ICC)'', progressively improves in performance
with more labeled data. The ICC semi-supervised learning in C2F-TCN, with 40%
labeled videos, performs similar to fully supervised counterparts.",0,1,0,0,1,0,0.536585,8.0,0.766009,87
a97a37c1-74fb-493f-a5ca-3d6af1caaaca,PETR: Position Embedding Transformation for Multi-View 3D Object Detection,298,0.93553,1.0,"In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research. Code is
available at \url{https://github.com/megvii-research/PETR}.",1,1,0,0,1,0,0.97416,5.0,0.937378,58
e45a37dd-7ee2-4646-972e-50f1fc3515f1,Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather,28,0.302991,0.922451,"Most object detection methods for autonomous driving usually assume a
consistent feature distribution between training and testing data, which is not
always the case when weathers differ significantly. The object detection model
trained under clear weather might not be effective enough in foggy weather
because of the domain gap. This paper proposes a novel domain adaptive object
detection framework for autonomous driving under foggy weather. Our method
leverages both image-level and object-level adaptation to diminish the domain
discrepancy in image style and object appearance. To further enhance the
model's capabilities under challenging samples, we also come up with a new
adversarial gradient reversal layer to perform adversarial mining for the hard
examples together with domain adaptation. Moreover, we propose to generate an
auxiliary domain by data augmentation to enforce a new domain-level metric
regularization. Experimental results on public benchmarks show the
effectiveness and accuracy of the proposed method. The code is available at
https://github.com/jinlong17/DA-Detect.",1,1,0,0,0,0,0.598587,6.0,0.716547,66
9649e47c-fece-41b2-812e-ec0b6a9765b7,Architecture and Knowledge Representation for Composable Inductive Programming,1,0.00869499,0.0425539,"We present an update on the current architecture of the Zoea knowledge-based,
Composable Inductive Programming system. The Zoea compiler is built using a
modern variant of the black-board architecture. Zoea integrates a large number
of knowledge sources that encode different aspects of programming language and
software development expertise. We describe the use of synthetic test cases as
a ubiquitous form of knowledge and hypothesis representation that sup-ports a
variety of reasoning strategies. Some future plans are also outlined.",0,0,0,0,0,0,4.4773e-05,20.0,0.418834,25
6d420c40-9a2e-4893-af98-fb3c57253ed4,MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages,7,0.177771,0.424825,"We present the results of the Workshop on Multilingual Information Access
(MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question
answering (QA) systems in 16 typologically diverse languages. In this task, we
adapted two large-scale cross-lingual open-retrieval QA datasets in 14
typologically diverse languages, and newly annotated open-retrieval QA data in
2 underrepresented languages: Tagalog and Tamil. Four teams submitted their
systems. The best system leveraging iteratively mined diverse negative examples
and larger pretrained models achieves 32.2 F1, outperforming our baseline by
4.5 points. The second best system uses entity-aware contextualized
representations for document retrieval, and achieves significant improvements
in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores.",0,1,0,1,0,0,0.787838,5.0,0.765816,47
89ca2831-dc14-4a5d-a670-7de627f72575,The THUEE System Description for the IARPA OpenASR21 Challenge,1,0.0016862,0.0597153,"This paper describes the THUEE team's speech recognition system for the IARPA
Open Automatic Speech Recognition Challenge (OpenASR21), with further
experiment explorations. We achieve outstanding results under both the
Constrained and Constrained-plus training conditions. For the Constrained
training condition, we construct our basic ASR system based on the standard
hybrid architecture. To alleviate the Out-Of-Vocabulary (OOV) problem, we
extend the pronunciation lexicon using Grapheme-to-Phoneme (G2P) techniques for
both OOV and potential new words. Standard acoustic model structures such as
CNN-TDNN-F and CNN-TDNN-F-A are adopted. In addition, multiple data
augmentation techniques are applied. For the Constrained-plus training
condition, we use the self-supervised learning framework wav2vec2.0. We
experiment with various fine-tuning techniques with the Connectionist Temporal
Classification (CTC) criterion on top of the publicly available pre-trained
model XLSR-53. We find that the frontend feature extractor plays an important
role when applying the wav2vec2.0 pre-trained model to the encoder-decoder
based CTC/Attention ASR architecture. Extra improvements can be achieved by
using the CTC model finetuned in the target language as the frontend feature
extractor.",0,1,0,0,0,0,0.110474,9.0,0.582839,39
3b132db9-f340-4c8d-89b8-22728ea9d37e,EnDex: Evaluation of Dialogue Engagingness at Scale,3,0.0996174,0.244103,"We propose EnDex, the first human-reaction based model to evaluate dialogue
engagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED)
curated using a novel distant-supervision framework. Engagingness is a key
measure that captures high-level quality of AI dialogue systems and closely
reflects actual user experience. However, data shortage, plus the abstract and
extensive definition of engagingness makes it challenging to develop an
automatic metric. Our work departs from mainstream approaches that use
synthetic negative examples to train binary classifiers, and instead, proposes
a solution using distant-supervision from human-reaction feedback. To support
the soundness of our EnDex metric, we offer a theoretical foundation for
engagement, an extensive ablation study, and empirical evidence of high
correlation on five engagingness related datasets. We will release code,
off-the-shelf EnDex model, and a large-scale dataset upon paper publication to
facilitate future research.",1,1,0,1,0,0,0.873624,8.0,0.889673,39
dd41cdad-0a8a-4d33-b51d-45958e5edc8e,Tractable Boolean and Arithmetic Circuits,8,0.0642234,0.69657,"Tractable Boolean and arithmetic circuits have been studied extensively in AI
for over two decades now. These circuits were initially proposed as ""compiled
objects,"" meant to facilitate logical and probabilistic reasoning, as they
permit various types of inference to be performed in linear-time and a
feed-forward fashion like neural networks. In more recent years, the role of
tractable circuits has significantly expanded as they became a computational
and semantical backbone for some approaches that aim to integrate knowledge,
reasoning and learning. In this article, we review the foundations of tractable
circuits and some associated milestones, while focusing on their core
properties and techniques that make them particularly useful for the broad aims
of neuro-symbolic AI.",0,0,0,0,0,1,0.00394252,13.0,0.450508,113
0e59c6d6-fd72-4df6-979d-f512fe1e814c,Reasoning about Procedures with Natural Language Processing: A Tutorial,6,0.108187,0.144888,"This tutorial provides a comprehensive and in-depth view of the research on
procedures, primarily in Natural Language Processing. A procedure is a sequence
of steps intended to achieve some goal. Understanding procedures in natural
language has a long history, with recent breakthroughs made possible by
advances in technology. First, we discuss established approaches to collect
procedures, by human annotation or extraction from web resources. Then, we
examine different angles from which procedures can be reasoned about, as well
as ways to represent them. Finally, we enumerate scenarios where procedural
knowledge can be applied to the real world.",0,0,0,0,0,0,0.104145,10.0,0.618307,91
2a023d85-71f8-4711-9b88-d2602103e2a3,HLT-MT: High-resource Language-specific Training for Multilingual Neural Machine Translation,6,0.0306719,0.620594,"Multilingual neural machine translation (MNMT) trained in multiple language
pairs has attracted considerable attention due to fewer model parameters and
lower training costs by sharing knowledge among multiple languages.
Nonetheless, multilingual training is plagued by language interference
degeneration in shared parameters because of the negative interference among
different translation directions, especially on high-resource languages. In
this paper, we propose the multilingual translation model with the
high-resource language-specific training (HLT-MT) to alleviate the negative
interference, which adopts the two-stage training with the language-specific
selection mechanism. Specifically, we first train the multilingual model only
with the high-resource pairs and select the language-specific modules at the
top of the decoder to enhance the translation quality of high-resource
directions. Next, the model is further trained on all available corpora to
transfer knowledge from high-resource languages (HRLs) to low-resource
languages (LRLs). Experimental results show that HLT-MT outperforms various
strong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic
experiments validate the effectiveness of our method in mitigating the negative
interference in multilingual training.",0,1,0,0,1,0,0.469304,5.0,0.586834,24
0ff1501f-8bfd-4896-8a20-dc0915391c33,On the Typicality of Musical Sequences,1,0.0232132,0.0257181,"It has been shown in a recent publication that words in human-produced
English language tend to have an information content close to the conditional
entropy. In this paper, we show that the same is true for events in
human-produced monophonic musical sequences. We also show how ""typical
sampling"" influences the distribution of information around the entropy for
single events and sequences.",0,0,0,0,0,0,0.687863,10.0,0.854269,11
e7cc1d9f-523e-47bd-a4f6-83b92fe3de25,LPAttack: A Feasible Annotation Scheme for Capturing Logic Pattern of Attacks in Arguments,2,0.0409048,0.115664,"In argumentative discourse, persuasion is often achieved by refuting or
attacking others arguments. Attacking is not always straightforward and often
comprise complex rhetorical moves such that arguers might agree with a logic of
an argument while attacking another logic. Moreover, arguer might neither deny
nor agree with any logics of an argument, instead ignore them and attack the
main stance of the argument by providing new logics and presupposing that the
new logics have more value or importance than the logics present in the
attacked argument. However, no existing studies in the computational
argumentation capture such complex rhetorical moves in attacks or the
presuppositions or value judgements in them. In order to address this gap, we
introduce LPAttack, a novel annotation scheme that captures the common modes
and complex rhetorical moves in attacks along with the implicit presuppositions
and value judgements in them. Our annotation study shows moderate
inter-annotator agreement, indicating that human annotation for the proposed
scheme is feasible. We publicly release our annotated corpus and the annotation
guidelines.",0,0,1,1,0,0,0.124047,8.0,0.54612,47
a69ea7bc-a76a-445d-97b7-48cef605fdc8,GASCN: Graph Attention Shape Completion Network,6,0.0877951,0.169818,"Shape completion, the problem of inferring the complete geometry of an object
given a partial point cloud, is an important problem in robotics and computer
vision. This paper proposes the Graph Attention Shape Completion Network
(GASCN), a novel neural network model that solves this problem. This model
combines a graph-based model for encoding local point cloud information with an
MLP-based architecture for encoding global information. For each completed
point, our model infers the normal and extent of the local surface patch which
is used to produce dense yet precise shape completions. We report experiments
that demonstrate that GASCN outperforms standard shape completion methods on a
standard benchmark drawn from the Shapenet dataset.",0,0,0,0,0,0,0.917767,10.0,0.930613,40
fe38a261-0cc1-4a30-9f75-594c5cd48782,Unifying Framework for Optimizations in non-boolean Formalisms,1,0.00289563,0.0670072,"Search-optimization problems are plentiful in scientific and engineering
domains. Artificial intelligence has long contributed to the development of
search algorithms and declarative programming languages geared towards solving
and modeling search-optimization problems. Automated reasoning and knowledge
representation are the subfields of AI that are particularly vested in these
developments. Many popular automated reasoning paradigms provide users with
languages supporting optimization statements. Recall integer linear
programming, MaxSAT, optimization satisfiability modulo theory, and
(constraint) answer set programming. These paradigms vary significantly in
their languages in ways they express quality conditions on computed solutions.
Here we propose a unifying framework of so called extended weight systems that
eliminates syntactic distinctions between paradigms. They allow us to see
essential similarities and differences between optimization statements provided
by distinct automated reasoning languages. We also study formal properties of
the proposed systems that immediately translate into formal properties of
paradigms that can be captured within our framework. Under consideration in
Theory and Practice of Logic Programming (TPLP).",0,0,0,0,0,0,6.81051e-05,17.0,0.340949,27
5c42f552-7365-4dd0-ae3f-793f3453ac65,Ensemble Semi-supervised Entity Alignment via Cycle-teaching,9,0.203237,0.251698,"Entity alignment is to find identical entities in different knowledge graphs.
Although embedding-based entity alignment has recently achieved remarkable
progress, training data insufficiency remains a critical challenge.
Conventional semi-supervised methods also suffer from the incorrect entity
alignment in newly proposed training data. To resolve these issues, we design
an iterative cycle-teaching framework for semi-supervised entity alignment. The
key idea is to train multiple entity alignment models (called aligners)
simultaneously and let each aligner iteratively teach its successor the
proposed new entity alignment. We propose a diversity-aware alignment selection
method to choose reliable entity alignment for each aligner. We also design a
conflict resolution mechanism to resolve the alignment conflict when combining
the new alignment of an aligner and that from its teacher. Besides, considering
the influence of cycle-teaching order, we elaborately design a strategy to
arrange the optimal order that can maximize the overall performance of multiple
aligners. The cycle-teaching process can break the limitations of each model's
learning capability and reduce the noise in new training data, leading to
improved performance. Extensive experiments on benchmark datasets demonstrate
the effectiveness of the proposed cycle-teaching framework, which significantly
outperforms the state-of-the-art models when the training data is insufficient
and the new entity alignment has much noise.",1,1,0,0,1,0,0.968794,6.0,0.938983,36
918918b9-8d82-4c0f-952d-1cd8fcf250fb,Independent Components of Word Embeddings Represent Semantic Features,2,0.0139797,0.285958,"Independent Component Analysis (ICA) is an algorithm originally developed for
finding separate sources in a mixed signal, such as a recording of multiple
people in the same room speaking at the same time. It has also been used to
find linguistic features in distributional representations. In this paper, we
used ICA to analyze words embeddings. We have found that ICA can be used to
find semantic features of the words and these features can easily be combined
to search for words that satisfy the combination. We show that only some of the
independent components represent such features, but those that do are stable
with regard to random initialization of the algorithm.",0,0,0,0,0,1,0.00248946,20.0,0.619806,20
c725174e-b78b-4e0d-a408-a67c616939a0,ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation,13,0.149188,0.499983,"We study the text generation task under the approach of pre-trained language
models (PLMs). Typically, an auto-regressive (AR) method is adopted for
generating texts in a token-by-token manner. Despite many advantages of AR
generation, it usually suffers from inefficient inference. Therefore,
non-autoregressive (NAR) models are proposed to generate all target tokens
simultaneously. However, NAR models usually generate texts of lower quality due
to the absence of token dependency in the output text. In this paper, we
propose ELMER: an efficient and effective PLM for NAR text generation to
explicitly model the token dependency during NAR generation. By leveraging the
early exit technique, ELMER enables the token generations at different layers,
according to their prediction confidence (a more confident token will exit at a
lower layer). Besides, we propose a novel pre-training objective, Layer
Permutation Language Modeling, to pre-train ELMER by permuting the exit layer
for each token in sequences. Experiments on three text generation tasks show
that ELMER significantly outperforms NAR models and further narrows the
performance gap with AR PLMs (\eg ELMER (29.92) vs BART (30.61) ROUGE-L in
XSUM) while achieving over 10 times inference speedup.",1,1,0,0,0,0,0.555747,7.0,0.740217,50
16b83b4f-c120-4494-957e-76acbe1724f0,Language Models as Inductive Reasoners,16,0.206206,0.311559,"Inductive reasoning is a core component of human intelligence. In the past
research of inductive reasoning within computer science, formal language is
used as representations of knowledge (facts and rules, more specifically).
However, formal language can cause systematic problems for inductive reasoning
such as disability of handling raw input such as natural language,
sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To
this end, we propose a new paradigm (task) for inductive reasoning, which is to
induce natural language rules from natural language facts, and create a dataset
termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts
are written in natural language. New automatic metrics are also proposed and
analysed for the evaluation of this task. With DEER, we investigate a modern
approach for inductive reasoning where we use natural language as
representation for knowledge instead of formal language and use pretrained
language models as ''reasoners''. Moreover, we provide the first and
comprehensive analysis of how well pretrained language models can induce
natural language rules from natural language facts. We also propose a new
framework drawing insights from philosophy literature for this task, which we
show in the experiment section that surpasses baselines in both automatic and
human evaluations. We discuss about our future perspectives for inductive
reasoning in Section 7. Dataset and code are available at
https://github.com/ZonglinY/Inductive_Reasoning.",1,0,1,1,0,0,0.609283,5.0,0.665689,61
1913851b-a9dc-4490-9439-d421ec1dfb75,DISCO: Distilling Counterfactuals with Large Language Models,11,0.113046,0.287149,"Models trained with counterfactually augmented data learn representations of
the causal structure of tasks, enabling robust generalization. However,
high-quality counterfactual data is scarce for most tasks and not easily
generated at scale. When crowdsourced, such data is typically limited in scale
and diversity; when generated using supervised methods, it is computationally
expensive to extend to new counterfactual dimensions. In this work, we
introduce DISCO (DIStilled COunterfactual Data), a new method for automatically
generating high quality counterfactual data at scale. DISCO engineers prompts
to generate phrasal perturbations with a large general language model. Then, a
task-specific teacher model filters these generations to distill high-quality
counterfactual data. While task-agnostic, we apply our pipeline to the task of
natural language inference (NLI) and find that on challenging evaluations such
as the NLI stress test, comparatively smaller student models trained with DISCO
generated counterfactuals are more robust (6% absolute) and generalize better
across distributions (2%) compared to models trained without data augmentation.
Furthermore, DISCO augmented models are 10% more consistent between
counterfactual pairs on three evaluation sets, demonstrating that DISCO
augmentation enables models to more reliably learn causal representations. Our
repository is available at: https://github.com/eric11eca/disco",0,1,0,1,0,0,0.668088,7.0,0.78407,52
a3bf09a6-35f4-4fe8-b788-47abbe54d1a8,Distance Matters in Human-Object Interaction Detection,7,0.0472608,0.53429,"Human-Object Interaction (HOI) detection has received considerable attention
in the context of scene understanding. Despite the growing progress on
benchmarks, we realize that existing methods often perform unsatisfactorily on
distant interactions, where the leading causes are two-fold: 1) Distant
interactions are by nature more difficult to recognize than close ones. A
natural scene often involves multiple humans and objects with intricate spatial
relations, making the interaction recognition for distant human-object largely
affected by complex visual context. 2) Insufficient number of distant
interactions in benchmark datasets results in under-fitting on these instances.
To address these problems, in this paper, we propose a novel two-stage method
for better handling distant interactions in HOI detection. One essential
component in our method is a novel Far Near Distance Attention module. It
enables information propagation between humans and objects, whereby the spatial
distance is skillfully taken into consideration. Besides, we devise a novel
Distance-Aware loss function which leads the model to focus more on distant yet
rare interactions. We conduct extensive experiments on two challenging datasets
- HICO-DET and V-COCO. The results demonstrate that the proposed method can
surpass existing approaches by a large margin, resulting in new
state-of-the-art performance.",1,1,0,0,1,0,0.591115,5.0,0.655775,59
759f3a87-2151-49db-bed0-8193a28636f0,TSAM: A Two-Stream Attention Model for Causal Emotion Entailment,10,0.449065,0.582973,"Causal Emotion Entailment (CEE) aims to discover the potential causes behind
an emotion in a conversational utterance. Previous works formalize CEE as
independent utterance pair classification problems, with emotion and speaker
information neglected. From a new perspective, this paper considers CEE in a
joint framework. We classify multiple utterances synchronously to capture the
correlations between utterances in a global view and propose a Two-Stream
Attention Model (TSAM) to effectively model the speaker's emotional influences
in the conversational history. Specifically, the TSAM comprises three modules:
Emotion Attention Network (EAN), Speaker Attention Network (SAN), and
interaction module. The EAN and SAN incorporate emotion and speaker information
in parallel, and the subsequent interaction module effectively interchanges
relevant information between the EAN and SAN via a mutual BiAffine
transformation. Extensive experimental results demonstrate that our model
achieves new State-Of-The-Art (SOTA) performance and outperforms baselines
remarkably.",0,1,1,0,1,0,0.958381,6.0,0.924532,54
ca2cd4dc-1b2d-459c-bfbf-021abfc2ad91,Leveraging QA Datasets to Improve Generative Data Augmentation,10,0.220937,0.368318,"The ability of generative language models (GLMs) to generate text has
improved considerably in the last few years, enabling their use for generative
data augmentation. In this work, we propose CONDA, an approach to further
improve GLMs' ability to generate synthetic data by reformulating data
generation as context generation for a given question-answer (QA) pair and
leveraging QA datasets for training context generators. Then, we cast
downstream tasks into the same question answering format and adapt the
fine-tuned context generators to the target task domain. Finally, we use the
fine-tuned GLM to generate relevant contexts, which are in turn used as
synthetic training data for their corresponding tasks. We perform extensive
experiments on multiple classification datasets and demonstrate substantial
improvements in performance for both few- and zero-shot settings. Our analysis
reveals that QA datasets that require high-level reasoning abilities (e.g.,
abstractive and common-sense QA datasets) tend to give the best boost in
performance in both few-shot and zero-shot settings.",1,1,0,0,0,1,0.937605,6.0,0.901825,55
0416eaa1-1e34-4b29-b70b-1ff207f7a2b1,Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance,13,0.0851293,0.878565,"This paper presents a novel end-to-end method for the problem of
skeleton-based unsupervised human action recognition. We propose a new
architecture with a convolutional autoencoder that uses graph Laplacian
regularization to model the skeletal geometry across the temporal dynamics of
actions. Our approach is robust towards viewpoint variations by including a
self-supervised gradient reverse layer that ensures generalization across
camera views. The proposed method is validated on NTU-60 and NTU-120
large-scale datasets in which it outperforms all prior unsupervised
skeleton-based approaches on the cross-subject, cross-view, and cross-setup
protocols. Although unsupervised, our learnable representation allows our
method even to surpass a few supervised skeleton-based action recognition
methods. The code is available in:
www.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian",1,0,0,0,1,0,0.361213,8.0,0.698502,34
d868d188-c9a9-4a79-9adb-f9f5ef316442,"GigaST: A 10,000-hour Pseudo Speech Translation Corpus",16,0.455678,0.87341,"This paper introduces GigaST, a large-scale pseudo speech translation (ST)
corpus. We create the corpus by translating the text in GigaSpeech, an English
ASR corpus, into German and Chinese. The training set is translated by a strong
machine translation system and the test set is translated by human. ST models
trained with an addition of our corpus obtain new state-of-the-art results on
the MuST-C English-German benchmark test set. We provide a detailed description
of the translation process and verify its quality. We make the translated text
data public and hope to facilitate research in speech translation.
Additionally, we also release the training scripts on NeurST to make it easy to
replicate our systems. GigaST dataset is available at
https://st-benchmark.github.io/resources/GigaST.",1,1,0,1,1,0,0.907075,7.0,0.893709,28
8d9f4245-d097-483d-882b-50e45c1aaf3e,Write It Like You See It: Detectable Differences in Clinical Notes By Race Lead To Differential Model Recommendations,19,0.695924,0.834701,"Clinical notes are becoming an increasingly important data source for machine
learning (ML) applications in healthcare. Prior research has shown that
deploying ML models can perpetuate existing biases against racial minorities,
as bias can be implicitly embedded in data. In this study, we investigate the
level of implicit race information available to ML models and human experts and
the implications of model-detectable differences in clinical notes. Our work
makes three key contributions. First, we find that models can identify patient
self-reported race from clinical notes even when the notes are stripped of
explicit indicators of race. Second, we determine that human experts are not
able to accurately predict patient race from the same redacted clinical notes.
Finally, we demonstrate the potential harm of this implicit information in a
simulation study, and show that models trained on these race-redacted clinical
notes can still perpetuate existing biases in clinical treatment decisions.",0,1,0,0,0,0,0.900684,8.0,0.903446,66
3e52b90f-e576-47ae-ac8a-74f4b69af953,BIC: Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency,11,0.10772,0.893296,"Twitter bots are automatic programs operated by malicious actors to
manipulate public opinion and spread misinformation. Research efforts have been
made to automatically identify bots based on texts and networks on social
media. Existing methods only leverage texts or networks alone, and while few
works explored the shallow combination of the two modalities, we hypothesize
that the interaction and information exchange between texts and graphs could be
crucial for holistically evaluating bot activities on social media. In
addition, according to a recent survey (Cresci, 2020), Twitter bots are
constantly evolving while advanced bots steal genuine users' tweets and dilute
their malicious content to evade detection. This results in greater
inconsistency across the timeline of novel Twitter bots, which warrants more
attention. In light of these challenges, we propose BIC, a Twitter Bot
detection framework with text-graph Interaction and semantic Consistency.
Specifically, in addition to separately modeling the two modalities on social
media, BIC employs a text-graph interaction module to enable information
exchange across modalities in the learning process. In addition, given the
stealing behavior of novel Twitter bots, BIC proposes to model semantic
consistency in tweets based on attention weights while using it to augment the
decision process. Extensive experiments demonstrate that BIC consistently
outperforms state-of-the-art baselines on two widely adopted datasets. Further
analyses reveal that text-graph interactions and modeling semantic consistency
are essential improvements and help combat bot evolution.",1,1,0,0,1,0,0.555435,6.0,0.696776,56
090bcd5c-b994-4bac-b99f-30d604441bf9,Understanding DDPM Latent Codes Through Optimal Transport,33,0.292693,0.990803,"Diffusion models have recently outperformed alternative approaches to model
the distribution of natural images, such as GANs. Such diffusion models allow
for deterministic sampling via the probability flow ODE, giving rise to a
latent space and an encoder map. While having important practical applications,
such as estimation of the likelihood, the theoretical properties of this map
are not yet fully understood. In the present work, we partially address this
question for the popular case of the VP SDE (DDPM) approach. We show that,
perhaps surprisingly, the DDPM encoder map coincides with the optimal transport
map for common distributions; we support this claim theoretically and by
extensive numerical experiments.",0,0,0,0,0,0,0.803833,7.0,0.839771,56
71671215-c49d-4e4f-bef8-0dcb518c43a0,Large-Field Contextual Feature Learning for Glass Detection,9,0.0839997,0.773657,"Glass is very common in our daily life. Existing computer vision systems
neglect it and thus may have severe consequences, e.g., a robot may crash into
a glass wall. However, sensing the presence of glass is not straightforward.
The key challenge is that arbitrary objects/scenes can appear behind the glass.
In this paper, we propose an important problem of detecting glass surfaces from
a single RGB image. To address this problem, we construct the first large-scale
glass detection dataset (GDD) and propose a novel glass detection network,
called GDNet-B, which explores abundant contextual cues in a large
field-of-view via a novel large-field contextual feature integration (LCFI)
module and integrates both high-level and low-level boundary features with a
boundary feature enhancement (BFE) module. Extensive experiments demonstrate
that our GDNet-B achieves satisfying glass detection results on the images
within and beyond the GDD testing set. We further validate the effectiveness
and generalization capability of our proposed GDNet-B by applying it to other
vision tasks, including mirror segmentation and salient object detection.
Finally, we show the potential applications of glass detection and discuss
possible future research directions.",1,1,1,1,1,0,0.638328,9.0,0.823049,117
d9840282-d1c6-432f-8075-5a3d80d13195,Gendered Mental Health Stigma in Masked Language Models,14,0.670265,0.979246,"Mental health stigma prevents many individuals from receiving the appropriate
care, and social psychology studies have shown that mental health tends to be
overlooked in men. In this work, we investigate gendered mental health stigma
in masked language models. In doing so, we operationalize mental health stigma
by developing a framework grounded in psychology research: we use clinical
psychology literature to curate prompts, then evaluate the models' propensity
to generate gendered words. We find that masked language models capture
societal stigma about gender in mental health: models are consistently more
likely to predict female subjects than male in sentences about having a mental
health condition (32% vs. 19%), and this disparity is exacerbated for sentences
that indicate treatment-seeking behavior. Furthermore, we find that different
models capture dimensions of stigma differently for men and women, associating
stereotypes like anger, blame, and pity more with women with mental health
conditions than with men. In showing the complex nuances of models' gendered
mental health stigma, we demonstrate that context and overlapping dimensions of
identity are important considerations when assessing computational models'
social biases.",1,0,0,0,0,0,0.971504,6.0,0.943293,38
911af1ba-acce-4541-b4d7-d6d24215dd56,TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction,6,0.0726136,0.456752,"Knowledge graph embedding methods are important for the knowledge graph
completion (or link prediction) task. One existing efficient method, PairRE,
leverages two separate vectors to model complex relations (i.e., 1-to-N,
N-to-1, and N-to-N) in knowledge graphs. However, such a method strictly
restricts entities on the hyper-ellipsoid surfaces which limits the
optimization of entity distribution, leading to suboptimal performance of
knowledge graph completion. To address this issue, we propose a novel score
function TranSHER, which leverages relation-specific translations between head
and tail entities to relax the constraint of hyper-ellipsoid restrictions. By
introducing an intuitive and simple relation-specific translation, TranSHER can
provide more direct guidance on optimization and capture more semantic
characteristics of entities with complex relations. Experimental results show
that TranSHER achieves significant performance improvements on link prediction
and generalizes well to datasets in different domains and scales. Our codes are
public available at https://github.com/yizhilll/TranSHER.",1,1,0,0,0,0,0.531083,14.0,0.865186,32
ee4a1ad9-3e84-45fd-a921-44f828e72a5c,Demystifying Unsupervised Semantic Correspondence Estimation,7,0.0874155,0.174499,"We explore semantic correspondence estimation through the lens of
unsupervised learning. We thoroughly evaluate several recently proposed
unsupervised methods across multiple challenging datasets using a standardized
evaluation protocol where we vary factors such as the backbone architecture,
the pre-training strategy, and the pre-training and finetuning datasets. To
better understand the failure modes of these methods, and in order to provide a
clearer path for improvement, we provide a new diagnostic framework along with
a new performance metric that is better suited to the semantic matching task.
Finally, we introduce a new unsupervised correspondence approach which utilizes
the strength of pre-trained features while encouraging better matches during
training. This results in significantly better matching performance compared to
current state-of-the-art methods.",0,1,0,0,1,0,0.724663,7.0,0.806431,81
0b8cf10d-5174-470c-b869-55661f57f349,Process Modeling and Conformance Checking in Healthcare: A COVID-19 Case Study,1,0.0873201,0.0908106,"The discipline of process mining has a solid track record of successful
applications to the healthcare domain. Within such research space, we conducted
a case study related to the Intensive Care Unit (ICU) ward of the Uniklinik
Aachen hospital in Germany. The aim of this work is twofold: developing a
normative model representing the clinical guidelines for the treatment of
COVID-19 patients, and analyzing the adherence of the observed behavior
(recorded in the information system of the hospital) to such guidelines. We
show that, through conformance checking techniques, it is possible to analyze
the care process for COVID-19 patients, highlighting the main deviations from
the clinical guidelines. The results provide physicians with useful indications
for improving the process and ensuring service quality and patient
satisfaction. We share the resulting model as an open-source BPMN file.",1,1,0,0,0,0,0.78483,3.0,0.606652,18
2dbe8fd4-e6f9-4ff0-9790-687aad1590a1,Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of AI,16,0.130071,0.726568,"As artificial intelligence becomes more powerful and a ubiquitous presence in
daily life, it is imperative to understand and manage the impact of AI systems
on our lives and decisions. Modern ML systems often change user behavior (e.g.
personalized recommender systems learn user preferences to deliver
recommendations that change online behavior). An externality of behavior change
is preference change. This article argues for the establishment of a
multidisciplinary endeavor focused on understanding how AI systems change
preference: Preference Science. We operationalize preference to incorporate
concepts from various disciplines, outlining the importance of meta-preferences
and preference-change preferences, and proposing a preliminary framework for
how preferences change. We draw a distinction between preference change,
permissible preference change, and outright preference manipulation. A
diversity of disciplines contribute unique insights to this framework.",0,0,0,0,0,0,0.00133588,23.0,0.642307,74
a44208b6-47d9-42ea-b693-957c9b630744,KSS-ICP: Point Cloud Registration based on Kendall Shape Space,7,0.181649,0.407152,"Point cloud registration is a popular topic which has been widely used in 3D
model reconstruction, location, and retrieval. In this paper, we propose a new
registration method, KSS-ICP, to address the rigid registration task in Kendall
shape space (KSS) with Iterative Closest Point (ICP). The KSS is a quotient
space that removes influences of translations, scales, and rotations for shape
feature-based analysis. Such influences can be concluded as the similarity
transformations that do not change the shape feature. The point cloud
representation in KSS is invariant to similarity transformations. We utilize
such property to design the KSS-ICP for point cloud registration. To tackle the
difficulty to achieve the KSS representation in general, the proposed KSS-ICP
formulates a practical solution that does not require complex feature analysis,
data training, and optimization. With a simple implementation, KSS-ICP achieves
more accurate registration from point clouds. It is robust to similarity
transformation, non-uniform density, noise, and defective parts. Experiments
show that KSS-ICP has better performance than the state of the art.",0,1,0,0,1,0,0.0734789,17.0,0.753984,58
377421cc-c6ae-43b0-a0f1-cbcf334f9749,No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models,13,0.0807265,0.565007,"Recent research has shown the existence of significant redundancy in large
Transformer models. One can prune the redundant parameters without
significantly sacrificing the generalization performance. However, we question
whether the redundant parameters could have contributed more if they were
properly trained. To answer this question, we propose a novel training strategy
that encourages all parameters to be trained sufficiently. Specifically, we
adaptively adjust the learning rate for each parameter according to its
sensitivity, a robust gradient-based measure reflecting this parameter's
contribution to the model performance. A parameter with low sensitivity is
redundant, and we improve its fitting by increasing its learning rate. In
contrast, a parameter with high sensitivity is well-trained, and we regularize
it by decreasing its learning rate to prevent further overfitting. We conduct
extensive experiments on natural language understanding, neural machine
translation, and image classification to demonstrate the effectiveness of the
proposed schedule. Analysis shows that the proposed schedule indeed reduces the
redundancy and improves generalization performance.",1,0,0,0,0,1,0.934961,6.0,0.899313,60
f1c4dcbe-021e-4167-9a0e-846ef9619cd5,Text-Only Training for Image Captioning using Noise-Injected CLIP,51,0.26747,0.59811,"We consider the task of image-captioning using only the CLIP model and
additional text data at training time, and no additional captioned images. Our
approach relies on the fact that CLIP is trained to make visual and textual
embeddings similar. Therefore, we only need to learn how to translate CLIP
textual embeddings back into text, and we can learn how to do this by learning
a decoder for the frozen CLIP text encoder using only text. We argue that this
intuition is ""almost correct"" because of a gap between the embedding spaces,
and propose to rectify this via noise injection during training. We demonstrate
the effectiveness of our approach by showing SOTA zero-shot image captioning
across four benchmarks, including style transfer. Code, data, and models are
available on GitHub.",1,1,0,0,0,0,0.5522,7.0,0.73881,62
8b2070b4-4eb1-43cb-aa01-b6af6a5bec75,Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production,8,0.432643,0.606095,"Mixture of Experts (MoE) models with conditional execution of sparsely
activated layers have enabled training models with a much larger number of
parameters. As a result, these models have achieved significantly better
quality on various natural language processing tasks including machine
translation. However, it remains challenging to deploy such models in real-life
scenarios due to the large memory requirements and inefficient inference. In
this work, we introduce a highly efficient inference framework with several
optimization approaches to accelerate the computation of sparse models and cut
down the memory consumption significantly. While we achieve up to 26x speed-up
in terms of throughput, we also reduce the model size almost to one eighth of
the original 32-bit float model by quantizing expert weights into 4-bit
integers. As a result, we are able to deploy 136x larger models with 27% less
cost and significantly better quality compared to the existing solutions. This
enables a paradigm shift in deploying large scale multilingual MoE transformers
models replacing the traditional practice of distilling teacher models into
dozens of smaller models per language or task.",1,1,0,0,0,0,0.985925,5.0,0.968122,16
292adf72-95f7-4920-8789-000fa10246e4,Consistent Representation Learning for Continual Relation Extraction,29,0.282254,0.954679,"Continual relation extraction (CRE) aims to continuously train a model on
data with new relations while avoiding forgetting old ones. Some previous work
has proved that storing a few typical samples of old relations and replaying
them when learning new relations can effectively avoid forgetting. However,
these memory-based methods tend to overfit the memory samples and perform
poorly on imbalanced datasets. To solve these challenges, a consistent
representation learning method is proposed, which maintains the stability of
the relation embedding by adopting contrastive learning and knowledge
distillation when replaying memory. Specifically, supervised contrastive
learning based on a memory bank is first used to train each new task so that
the model can effectively learn the relation representation. Then, contrastive
replay is conducted of the samples in memory and makes the model retain the
knowledge of historical relations through memory knowledge distillation to
prevent the catastrophic forgetting of the old task. The proposed method can
better learn consistent representations to alleviate forgetting effectively.
Extensive experiments on FewRel and TACRED datasets show that our method
significantly outperforms state-of-the-art baselines and yield strong
robustness on the imbalanced dataset.",1,1,0,0,1,0,0.885581,6.0,0.860719,38
a5bdd23a-53d8-44c1-a8f6-3301ef0576e0,4D Unsupervised Object Discovery,11,0.289717,0.665042,"Object discovery is a core task in computer vision. While fast progresses
have been made in supervised object detection, its unsupervised counterpart
remains largely unexplored. With the growth of data volume, the expensive cost
of annotations is the major limitation hindering further study. Therefore,
discovering objects without annotations has great significance. However, this
task seems impractical on still-image or point cloud alone due to the lack of
discriminative information. Previous studies underlook the crucial temporal
information and constraints naturally behind multi-modal inputs. In this paper,
we propose 4D unsupervised object discovery, jointly discovering objects from
4D data -- 3D point clouds and 2D RGB images with temporal information. We
present the first practical approach for this task by proposing a ClusterNet on
3D point clouds, which is jointly iteratively optimized with a 2D localization
network. Extensive experiments on the large-scale Waymo Open Dataset suggest
that the localization network and ClusterNet achieve competitive performance on
both class-agnostic 2D object detection and 3D instance segmentation, bridging
the gap between unsupervised methods and full supervised ones. Codes and models
will be made available at https://github.com/Robertwyq/LSMOL.",1,1,1,0,1,0,0.957533,8.0,0.942603,51
64106600-6c23-4ef1-bf8e-f176ea89ddba,Admissible Policy Teaching through Reward Design,12,0.115273,0.682902,"We study reward design strategies for incentivizing a reinforcement learning
agent to adopt a policy from a set of admissible policies. The goal of the
reward designer is to modify the underlying reward function cost-efficiently
while ensuring that any approximately optimal deterministic policy under the
new reward function is admissible and performs well under the original reward
function. This problem can be viewed as a dual to the problem of optimal reward
poisoning attacks: instead of forcing an agent to adopt a specific policy, the
reward designer incentivizes an agent to avoid taking actions that are
inadmissible in certain states. Perhaps surprisingly, and in contrast to the
problem of optimal reward poisoning attacks, we first show that the reward
design problem for admissible policy teaching is computationally challenging,
and it is NP-hard to find an approximately optimal reward modification. We then
proceed by formulating a surrogate problem whose optimal solution approximates
the optimal solution to the reward design problem in our setting, but is more
amenable to optimization techniques and analysis. For this surrogate problem,
we present characterization results that provide bounds on the value of the
optimal solution. Finally, we design a local search algorithm to solve the
surrogate problem and showcase its utility using simulation-based experiments.",0,0,0,0,0,0,0.206925,9.0,0.65876,40
00110da2-8471-46ed-b4d0-572862c3d17e,Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation,13,0.0884426,0.641134,"Federated learning (FL) can be essential in knowledge representation,
reasoning, and data mining applications over multi-source knowledge graphs
(KGs). A recent study FedE first proposes an FL framework that shares entity
embeddings of KGs across all clients. However, entity embedding sharing from
FedE would incur a severe privacy leakage. Specifically, the known entity
embedding can be used to infer whether a specific relation between two entities
exists in a private client. In this paper, we introduce a novel attack method
that aims to recover the original data based on the embedding information,
which is further used to evaluate the vulnerabilities of FedE. Furthermore, we
propose a Federated learning paradigm with privacy-preserving Relation
embedding aggregation (FedR) to tackle the privacy issue in FedE. Besides,
relation embedding sharing can significantly reduce the communication cost due
to its smaller size of queries. We conduct extensive experiments to evaluate
FedR with five different KG embedding models and three datasets. Compared to
FedE, FedR achieves similar utility and significant improvements regarding
privacy-preserving effect and communication efficiency on the link prediction
task.",0,1,0,0,0,0,0.34988,8.0,0.693498,24
94d6e9d9-a921-4bbf-a37a-6e85a25830f9,Robust Calibration with Multi-domain Temperature Scaling,22,0.150618,0.918521,"Uncertainty quantification is essential for the reliable deployment of
machine learning models to high-stakes application domains. Uncertainty
quantification is all the more challenging when training distribution and test
distribution are different, even the distribution shifts are mild. Despite the
ubiquity of distribution shifts in real-world applications, existing
uncertainty quantification approaches mainly study the in-distribution setting
where the train and test distributions are the same. In this paper, we develop
a systematic calibration model to handle distribution shifts by leveraging data
from multiple domains. Our proposed method -- multi-domain temperature scaling
-- uses the heterogeneity in the domains to improve calibration robustness
under distribution shift. Through experiments on three benchmark data sets, we
find our proposed method outperforms existing methods as measured on both
in-distribution and out-of-distribution test sets.",0,1,0,0,0,0,0.788974,7.0,0.833219,55
600bc485-df53-480c-831a-a9532ae74c13,Conditional Generation with a Question-Answering Blueprint,26,0.723888,0.977926,"The ability to convey relevant and faithful information is critical for many
tasks in conditional generation and yet remains elusive for neural seq-to-seq
models whose outputs often reveal hallucinations and fail to correctly cover
important details. In this work, we advocate planning as a useful intermediate
representation for rendering conditional generation less opaque and more
grounded. Our work proposes a new conceptualization of text plans as a sequence
of question-answer (QA) pairs. We enhance existing datasets (e.g., for
summarization) with a QA blueprint operating as a proxy for both content
selection (i.e.,~what to say) and planning (i.e.,~in what order). We obtain
blueprints automatically by exploiting state-of-the-art question generation
technology and convert input-output pairs into input-blueprint-output tuples.
We develop Transformer-based models, each varying in how they incorporate the
blueprint in the generated output (e.g., as a global plan or iteratively).
Evaluation across metrics and datasets demonstrates that blueprint models are
more factual than alternatives which do not resort to planning and allow
tighter control of the generation output.",0,0,0,0,0,0,0.905855,6.0,0.875077,122
5a41a3f5-3389-4e80-83a0-a9f360e7fc3c,Language Model Pre-training on True Negatives,2,0.00526855,0.0645596,"Discriminative pre-trained language models (PLMs) learn to predict original
texts from intentionally corrupted ones. Taking the former text as positive and
the latter as negative samples, the PLM can be trained effectively for
contextualized representation. However, the training of such a type of PLMs
highly relies on the quality of the automatically constructed samples. Existing
PLMs simply treat all corrupted texts as equal negative without any
examination, which actually lets the resulting model inevitably suffer from the
false negative issue where training is carried out on pseudo-negative data and
leads to less efficiency and less robustness in the resulting PLMs. In this
work, on the basis of defining the false negative issue in discriminative PLMs
that has been ignored for a long time, we design enhanced pre-training methods
to counteract false negative predictions and encourage pre-training language
models on true negatives by correcting the harmful gradient updates subject to
false negative predictions. Experimental results on GLUE and SQuAD benchmarks
show that our counter-false-negative pre-training methods indeed bring about
better performance together with stronger robustness.",0,0,0,0,0,0,0.335488,7.0,0.642256,53
cca70b9b-4812-4c99-81ba-41febe45c4a6,FreGAN: Exploiting Frequency Components for Training GANs under Limited Data,15,0.0701781,0.862824,"Training GANs under limited data often leads to discriminator overfitting and
memorization issues, causing divergent training. Existing approaches mitigate
the overfitting by employing data augmentations, model regularization, or
attention mechanisms. However, they ignore the frequency bias of GANs and take
poor consideration towards frequency information, especially high-frequency
signals that contain rich details. To fully utilize the frequency information
of limited data, this paper proposes FreGAN, which raises the model's frequency
awareness and draws more attention to producing high-frequency signals,
facilitating high-quality generation. In addition to exploiting both real and
generated images' frequency information, we also involve the frequency signals
of real images as a self-supervised constraint, which alleviates the GAN
disequilibrium and encourages the generator to synthesize adequate rather than
arbitrary frequency signals. Extensive results demonstrate the superiority and
effectiveness of our FreGAN in ameliorating generation quality in the low-data
regime (especially when training data is less than 100). Besides, FreGAN can be
seamlessly applied to existing regularization and attention mechanism models to
further boost the performance.",1,1,0,0,1,0,0.443464,6.0,0.642695,62
8858b84e-b7cf-44bf-baf2-822a8211c631,Exploring Linear Feature Disentanglement For Neural Networks,2,0.00662037,0.12485,"Non-linear activation functions, e.g., Sigmoid, ReLU, and Tanh, have achieved
great success in neural networks (NNs). Due to the complex non-linear
characteristic of samples, the objective of those activation functions is to
project samples from their original feature space to a linear separable feature
space. This phenomenon ignites our interest in exploring whether all features
need to be transformed by all non-linear functions in current typical NNs,
i.e., whether there exists a part of features arriving at the linear separable
feature space in the intermediate layers, that does not require further
non-linear variation but an affine transformation instead. To validate the
above hypothesis, we explore the problem of linear feature disentanglement for
neural networks in this paper. Specifically, we devise a learnable mask module
to distinguish between linear and non-linear features. Through our designed
experiments we found that some features reach the linearly separable space
earlier than the others and can be detached partly from the NNs. The explored
method also provides a readily feasible pruning strategy which barely affects
the performance of the original model. We conduct our experiments on four
datasets and present promising results.",0,0,1,0,0,0,0.669624,8.0,0.811586,21
455a60df-4bdc-4dff-9ec8-18ede723a0b5,Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation,8,0.164553,0.210056,"Large pretrained language models (PLMs) are often domain- or task-adapted via
fine-tuning or prompting. Finetuning requires modifying all of the parameters
and having enough data to avoid overfitting while prompting requires no
training and few examples but limits performance. Instead, we prepare PLMs for
data- and parameter-efficient adaptation by learning to learn the difference
between general and adapted PLMs. This difference is expressed in terms of
model weights and sublayer structure through our proposed dynamic low-rank
reparameterization and learned architecture controller. Experiments on few-shot
dialogue completion, low-resource abstractive summarization, and multi-domain
language modeling show improvements in adaptation time and performance over
direct finetuning or preparation via domain-adaptive pretraining. Ablations
show our task-adaptive reparameterization (TARP) and model search (TAMS)
components individually improve on other parameter-efficient transfer like
adapters and structure-learning methods like learned sparsification.",1,0,0,0,0,0,0.970468,6.0,0.941612,54
bbcc5cd3-5ffc-4f97-9877-c190ce7dd333,Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness,11,0.254264,0.830497,"Data-driven predictive solutions predominant in commercial applications tend
to suffer from biases and stereotypes, which raises equity concerns. Prediction
models may discover, use, or amplify spurious correlations based on gender or
other protected personal characteristics, thus discriminating against
marginalized groups. Mitigating gender bias has become an important research
focus in natural language processing (NLP) and is an area where annotated
corpora are available. Data augmentation reduces gender bias by adding
counterfactual examples to the training dataset. In this work, we show that
some of the examples in the augmented dataset can be not important or even
harmful for fairness. We hence propose a general method for pruning both the
factual and counterfactual examples to maximize the model's fairness as
measured by the demographic parity, equality of opportunity, and equality of
odds. The fairness achieved by our method surpasses that of data augmentation
on three text classification datasets, using no more than half of the examples
in the augmented dataset. Our experiments are conducted using models of varying
sizes and pre-training settings.",0,1,0,0,0,0,0.913023,9.0,0.920381,59
cd56b4c9-30de-4c21-a1fc-e8cb7b7529dd,OrthoMAD: Morphing Attack Detection Through Orthogonal Identity Disentanglement,7,0.0751576,0.388971,"Morphing attacks are one of the many threats that are constantly affecting
deep face recognition systems. It consists of selecting two faces from
different individuals and fusing them into a final image that contains the
identity information of both. In this work, we propose a novel regularisation
term that takes into account the existent identity information in both and
promotes the creation of two orthogonal latent vectors. We evaluate our
proposed method (OrthoMAD) in five different types of morphing in the FRLL
dataset and evaluate the performance of our model when trained on five distinct
datasets. With a small ResNet-18 as the backbone, we achieve state-of-the-art
results in the majority of the experiments, and competitive results in the
others. The code of this paper will be publicly available.",1,1,0,0,1,0,0.233529,7.0,0.580878,26
4185b679-6de5-4a5a-b16f-b40629d80d33,Defect detection and segmentation in X-Ray images of magnesium alloy castings using the Detectron2 framework,5,0.251178,0.516926,"New production techniques have emerged that have made it possible to produce
metal parts with more complex shapes, making the quality control process more
difficult. This implies that the visual and superficial analysis has become
even more inefficient. On top of that, it is also not possible to detect
internal defects that these parts could have. The use of X-Ray images has made
this process much easier, allowing not only to detect superficial defects in a
much simpler way, but also to detect welding or casting defects that could
represent a serious hazard for the physical integrity of the metal parts. On
the other hand, the use of an automatic segmentation approach for detecting
defects would help diminish the dependence of defect detection on the
subjectivity of the factory operators and their time dependence variability.
The aim of this paper is to apply a deep learning system based on Detectron2, a
state-of-the-art library applied to object detection and segmentation in
images, for the identification and segmentation of these defects on X-Ray
images obtained mainly from automotive parts",0,1,0,0,0,0,0.472665,9.0,0.771572,25
8fb12a65-0f82-412a-9e19-8f1e60dcd825,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,181,0.726769,1.0,"We present BlenderBot 3, a 175B parameter dialogue model capable of
open-domain conversation with access to the internet and a long-term memory,
and having been trained on a large number of user defined tasks. We release
both the model weights and code, and have also deployed the model on a public
web page to interact with organic users. This technical report describes how
the model was built (architecture, model and training scheme), and details of
its deployment, including safety mechanisms. Human evaluations show its
superiority to existing open-domain dialogue agents, including its predecessors
(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for
continual learning using the data collected from deployment, which will also be
publicly released. The goal of this research program is thus to enable the
community to study ever-improving responsible agents that learn through
interaction.",1,1,0,0,0,1,0.636334,6.0,0.733669,111
93f24a6e-9a93-41cc-93fe-1f3babdfa694,Query Expansion Using Contextual Clue Sampling with Language Models,6,0.0830187,0.25047,"Query expansion is an effective approach for mitigating vocabulary mismatch
between queries and documents in information retrieval. One recent line of
research uses language models to generate query-related contexts for expansion.
Along this line, we argue that expansion terms from these contexts should
balance two key aspects: diversity and relevance. The obvious way to increase
diversity is to sample multiple contexts from the language model. However, this
comes at the cost of relevance, because there is a well-known tendency of
models to hallucinate incorrect or irrelevant contexts. To balance these two
considerations, we propose a combination of an effective filtering strategy and
fusion of the retrieved documents based on the generation probability of each
context. Our lexical matching based approach achieves a similar top-5/top-20
retrieval accuracy and higher top-100 accuracy compared with the
well-established dense retrieval model DPR, while reducing the index size by
more than 96%. For end-to-end QA, the reader model also benefits from our
method and achieves the highest Exact-Match score against several competitive
baselines.",0,1,0,0,0,0,0.893675,5.0,0.83952,31
56d28333-10eb-40e0-852d-0ddead3e05a1,"The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink",136,0.95688,0.999982,"Machine Learning (ML) workloads have rapidly grown in importance, but raised
concerns about their carbon footprint. Four best practices can reduce ML
training energy by up to 100x and CO2 emissions up to 1000x. By following best
practices, overall ML energy use (across research, development, and production)
held steady at <15% of Google's total energy use for the past three years. If
the whole ML field were to adopt best practices, total carbon emissions from
training would reduce. Hence, we recommend that ML papers include emissions
explicitly to foster competition on more than just model quality. Estimates of
emissions in papers that omitted them have been off 100x-100,000x, so
publishing emissions has the added benefit of ensuring accurate accounting.
Given the importance of climate change, we must get the numbers right to make
certain that we work on its biggest challenges.",0,1,0,0,0,1,0.946159,4.0,0.865684,31
059a0bca-5d2c-4306-ab5b-9d588b2ac501,Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations,7,0.130581,0.684499,"We formulated non-speech vocalization (NSV) modeling as a text-to-speech task
and verified its viability. Specifically, we evaluated the phonetic
expressivity of HUBERT speech units on NSVs and verified our model's ability to
control over speaker timbre even though the training data is speaker few-shot.
In addition, we substantiated that the heterogeneity in recording conditions is
the major obstacle for NSV modeling. Finally, we discussed five improvements
over our method for future research. Audio samples of synthesized NSVs are
available on our demo page: https://resemble-ai.github.io/reLaugh.",0,1,0,0,0,0,0.0631283,11.0,0.605491,14
331f1edc-cf8a-4b69-8b33-5318d5d1d7ca,Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,44,0.378075,0.905425,"This work presents a detailed linguistic analysis into why larger
Transformer-based pre-trained language models with more parameters and lower
perplexity nonetheless yield surprisal estimates that are less predictive of
human reading times. First, regression analyses show a strictly monotonic,
positive log-linear relationship between perplexity and fit to reading times
for the more recently released five GPT-Neo variants and eight OPT variants on
two separate datasets, replicating earlier results limited to just GPT-2 (Oh et
al., 2022). Subsequently, analysis of residual errors reveals a systematic
deviation of the larger variants, such as underpredicting reading times of
named entities and making compensatory overpredictions for reading times of
function words such as modals and conjunctions. These results suggest that the
propensity of larger Transformer-based models to 'memorize' sequences during
training makes their surprisal estimates diverge from humanlike expectations,
which warrants caution in using pre-trained language models to study human
language processing.",0,0,0,0,0,0,0.355,7.0,0.652311,50
24e83865-46db-4f1a-ae27-c01157a0a241,Uncertainty-guided Source-free Domain Adaptation,38,0.612651,0.991725,"Source-free domain adaptation (SFDA) aims to adapt a classifier to an
unlabelled target data set by only using a pre-trained source model. However,
the absence of the source data and the domain shift makes the predictions on
the target data unreliable. We propose quantifying the uncertainty in the
source model predictions and utilizing it to guide the target adaptation. For
this, we construct a probabilistic source model by incorporating priors on the
network parameters inducing a distribution over the model predictions.
Uncertainties are estimated by employing a Laplace approximation and
incorporated to identify target data points that do not lie in the source
manifold and to down-weight them when maximizing the mutual information on the
target data. Unlike recent works, our probabilistic treatment is
computationally lightweight, decouples source training and target adaptation,
and requires no specialized source training or changes of the model
architecture. We show the advantages of uncertainty-guided SFDA over
traditional SFDA in the closed-set and open-set settings and provide empirical
evidence that our approach is more robust to strong domain shifts even without
tuning.",0,1,0,0,0,0,0.918473,9.0,0.923286,77
12690778-e838-4891-93f0-ebc6e3a79489,Analyzing Wrap-Up Effects through an Information-Theoretic Lens,7,0.0791132,0.679312,"Numerous analyses of reading time (RT) data have been implemented -- all in
an effort to better understand the cognitive processes driving reading
comprehension. However, data measured on words at the end of a sentence -- or
even at the end of a clause -- is often omitted due to the confounding factors
introduced by so-called ""wrap-up effects,"" which manifests as a skewed
distribution of RTs for these words. Consequently, the understanding of the
cognitive processes that might be involved in these wrap-up effects is limited.
In this work, we attempt to learn more about these processes by examining the
relationship between wrap-up effects and information-theoretic quantities, such
as word and context surprisals. We find that the distribution of information in
prior contexts is often predictive of sentence- and clause-final RTs (while not
of sentence-medial RTs). This lends support to several prior hypotheses about
the processes involved in wrap-up effects.",0,0,0,0,0,0,0.0682242,21.0,0.797178,38
55f06078-072d-4a3d-876b-97820f13b04f,SHREC 2022 Track on Online Detection of Heterogeneous Gestures,5,0.119608,0.282976,"This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.",0,1,0,0,0,0,0.419336,10.0,0.778099,16
c482ad23-bc50-4ab6-8769-deb64803ed96,DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization,32,0.857011,0.804437,"Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve
state-of-the-art performance on many generative NLP tasks. However, such models
pose a great challenge in resource-constrained scenarios owing to their large
memory requirements and high latency. To alleviate this issue, we propose to
jointly distill and quantize the model, where knowledge is transferred from the
full-precision teacher model to the quantized and distilled low-precision
student model. Empirical analyses show that, despite the challenging nature of
generative tasks, we were able to achieve a 16.5x model footprint compression
ratio with little performance drop relative to the full-precision counterparts
on multiple summarization and QA datasets. We further pushed the limit of
compression ratio to 27.7x and presented the performance-efficiency trade-off
for generative tasks using pre-trained models. To the best of our knowledge,
this is the first work aiming to effectively distill and quantize
sequence-to-sequence pre-trained models for language generation tasks.",1,1,0,0,1,0,0.989097,6.0,0.983132,33
463b7a67-d36e-4136-ba50-238b0efa2dd4,Find a Way Forward: a Language-Guided Semantic Map Navigator,3,0.201909,0.112933,"In this paper, we introduce the map-language navigation task where an agent
executes natural language instructions and moves to the target position based
only on a given 3D semantic map. To tackle the task, we design the
instruction-aware Path Proposal and Discrimination model (iPPD). Our approach
leverages map information to provide instruction-aware path proposals, i.e., it
selects all potential instruction-aligned candidate paths to reduce the
solution space. Next, to represent the map observations along a path for a
better modality alignment, a novel Path Feature Encoding scheme tailored for
semantic maps is proposed. An attention-based Language Driven Discriminator is
designed to evaluate path candidates and determine the best path as the final
result. Our method can naturally avoid error accumulation compared with
single-step greedy decision methods. Comparing to a single-step imitation
learning approach, iPPD has performance gains above 17% on navigation success
and 0.18 on path matching measurement nDTW in challenging unseen environments.",0,1,1,0,0,0,0.971384,7.0,0.951226,47
36e55999-1663-424a-b9cf-0298c3298505,"Less Data, More Knowledge: Building Next Generation Semantic Communication Networks",60,0.498111,0.999941,"Semantic communication is viewed as a revolutionary paradigm that can
potentially transform how we design and operate wireless communication systems.
However, despite a recent surge of research activities in this area, the
research landscape remains limited. In this tutorial, we present the first
rigorous vision of a scalable end-to-end semantic communication network that is
founded on novel concepts from artificial intelligence (AI), causal reasoning,
and communication theory. We first discuss how the design of semantic
communication networks requires a move from data-driven networks towards
knowledge-driven ones. Subsequently, we highlight the necessity of creating
semantic representations of data that satisfy the key properties of minimalism,
generalizability, and efficiency so as to do more with less. We then explain
how those representations can form the basis a so-called semantic language. By
using semantic representation and languages, we show that the traditional
transmitter and receiver now become a teacher and apprentice. Then, we define
the concept of reasoning by investigating the fundamentals of causal
representation learning and their role in designing semantic communication
networks. We demonstrate that reasoning faculties are majorly characterized by
the ability to capture causal and associational relationships in datastreams.
For such reasoning-driven networks, we propose novel and essential semantic
communication metrics that include new ""reasoning capacity"" measures that could
go beyond Shannon's bound to capture the convergence of computing and
communication. Finally, we explain how semantic communications can be scaled to
large-scale networks (6G and beyond). In a nutshell, we expect this tutorial to
provide a comprehensive reference on how to properly build, analyze, and deploy
future semantic communication networks.",0,0,0,0,0,0,0.517688,4.0,0.518676,82
a77e94cf-ddc1-4f64-aab0-2c46fa485c62,CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation,8,0.0671818,0.40863,"Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed
inputs during training -- helps reduce model reliance on spurious correlations
and improves generalization to out-of-distribution (OOD) data. Prior work on
generating counterfactuals only considered restricted classes of perturbations,
limiting their effectiveness. We present COunterfactual Generation via
Retrieval and Editing (CORE), a retrieval-augmented generation framework for
creating diverse counterfactual perturbations for CDA. For each training
example, CORE first performs a dense retrieval over a task-related unlabeled
text corpus using a learned bi-encoder and extracts relevant counterfactual
excerpts. CORE then incorporates these into prompts to a large language model
with few-shot learning capabilities, for counterfactual editing. Conditioning
language model edits on naturally occurring data results in diverse
perturbations. Experiments on natural language inference and sentiment analysis
benchmarks show that CORE counterfactuals are more effective at improving
generalization to OOD data compared to other DA approaches. We also show that
the CORE retrieval framework can be used to encourage diversity in manually
authored perturbations",0,1,0,0,0,0,0.684121,5.0,0.706481,72
00b7ff26-18c1-42e3-83a1-bc84b7f12d67,cosFormer: Rethinking Softmax in Attention,140,0.264678,0.956287,"Transformer has shown great successes in natural language processing,
computer vision, and audio processing. As one of its core components, the
softmax attention helps to capture long-range dependencies yet prohibits its
scale-up due to the quadratic space and time complexity to the sequence length.
Kernel methods are often adopted to reduce the complexity by approximating the
softmax operator. Nevertheless, due to the approximation errors, their
performances vary in different tasks/corpus and suffer crucial performance
drops when compared with the vanilla softmax attention. In this paper, we
propose a linear transformer called cosFormer that can achieve comparable or
better accuracy to the vanilla transformer in both casual and cross attentions.
cosFormer is based on two key properties of softmax attention: i).
non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme
that can concentrate the distribution of the attention matrix. As its linear
substitute, cosFormer fulfills these properties with a linear operator and a
cosine-based distance re-weighting mechanism. Extensive experiments on language
modeling and text understanding tasks demonstrate the effectiveness of our
method. We further examine our method on long sequences and achieve
state-of-the-art performance on the Long-Range Arena benchmark. The source code
is available at https://github.com/OpenNLPLab/cosFormer.",0,0,0,0,1,1,0.627004,6.0,0.729442,51
b50e3a46-79f3-45f5-9bf7-cbeb687a5e92,Monte Carlo Tree Search based Variable Selection for High Dimensional Bayesian Optimization,13,0.063039,0.806896,"Bayesian optimization (BO) is a class of popular methods for expensive
black-box optimization, and has been widely applied to many scenarios. However,
BO suffers from the curse of dimensionality, and scaling it to high-dimensional
problems is still a challenge. In this paper, we propose a variable selection
method MCTS-VS based on Monte Carlo tree search (MCTS), to iteratively select
and optimize a subset of variables. That is, MCTS-VS constructs a
low-dimensional subspace via MCTS and optimizes in the subspace with any BO
algorithm. We give a theoretical analysis of the general variable selection
method to reveal how it can work. Experiments on high-dimensional synthetic
functions and real-world problems (i.e., NAS-bench problems and MuJoCo
locomotion tasks) show that MCTS-VS equipped with a proper BO optimizer can
achieve state-of-the-art performance.",1,1,0,0,1,0,0.0672945,10.0,0.572652,56
8016080a-7ef1-43ce-b0de-b593ad9954fa,Unified Semantic Typing with Meaningful Label Inference,21,0.0705141,0.864046,"Semantic typing aims at classifying tokens or spans of interest in a textual
context into semantic categories such as relations, entity types, and event
types. The inferred labels of semantic categories meaningfully interpret how
machines understand components of text. In this paper, we present UniST, a
unified framework for semantic typing that captures label semantics by
projecting both inputs and labels into a joint semantic embedding space. To
formulate different lexical and relational semantic typing tasks as a unified
task, we incorporate task descriptions to be jointly encoded with the input,
allowing UniST to be adapted to different tasks without introducing
task-specific model components. UniST optimizes a margin ranking loss such that
the semantic relatedness of the input and labels is reflected from their
embedding similarity. Our experiments demonstrate that UniST achieves strong
performance across three semantic typing tasks: entity typing, relation
classification and event typing. Meanwhile, UniST effectively transfers
semantic knowledge of labels and substantially improves generalizability on
inferring rarely seen and unseen types. In addition, multiple semantic typing
tasks can be jointly trained within the unified framework, leading to a single
compact multi-tasking model that performs comparably to dedicated single-task
models, while offering even better transferability.",1,0,0,0,0,1,0.190391,6.0,0.472605,56
63c95fdc-2bbd-474d-8235-240f1ac2c87c,Learning Uncertainty with Artificial Neural Networks for Improved Predictive Process Monitoring,9,0.0361349,0.513304,"The inability of artificial neural networks to assess the uncertainty of
their predictions is an impediment to their widespread use. We distinguish two
types of learnable uncertainty: model uncertainty due to a lack of training
data and noise-induced observational uncertainty. Bayesian neural networks use
solid mathematical foundations to learn the model uncertainties of their
predictions. The observational uncertainty can be calculated by adding one
layer to these networks and augmenting their loss functions. Our contribution
is to apply these uncertainty concepts to predictive process monitoring tasks
to train uncertainty-based models to predict the remaining time and outcomes.
Our experiments show that uncertainty estimates allow more and less accurate
predictions to be differentiated and confidence intervals to be constructed in
both regression and classification tasks. These conclusions remain true even in
early stages of running processes. Moreover, the deployed techniques are fast
and produce more accurate predictions. The learned uncertainty could increase
users' confidence in their process prediction systems, promote better
cooperation between humans and these systems, and enable earlier
implementations with smaller datasets.",0,1,0,0,0,0,0.0423528,9.0,0.472267,49
5d471b4f-6a8b-4e99-bfdd-1f6738cad26c,NeRF-RPN: A general framework for object detection in NeRFs,33,0.232118,0.81361,"This paper presents the first significant object detection framework,
NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,
NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting
a novel voxel representation that incorporates multi-scale 3D neural volumetric
features, we demonstrate it is possible to regress the 3D bounding boxes of
objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN
is a general framework and can be applied to detect objects without class
labels. We experimented NeRF-RPN with various backbone architectures, RPN head
designs and loss functions. All of them can be trained in an end-to-end manner
to estimate high quality 3D bounding boxes. To facilitate future research in
object detection for NeRF, we built a new benchmark dataset which consists of
both synthetic and real-world data with careful labeling and clean up. Code and
dataset are available at https://github.com/lyclyc52/NeRF_RPN.",1,0,0,0,0,0,0.890495,8.0,0.898045,75
9d8cf0b0-1c6c-45c0-9a84-d9c5b0d2ee9a,ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale,6,0.0278976,0.2987,"Incremental learning is one paradigm to enable model building and updating at
scale with streaming data. For end-to-end automatic speech recognition (ASR)
tasks, the absence of human annotated labels along with the need for privacy
preserving policies for model building makes it a daunting challenge. Motivated
by these challenges, in this paper we use a cloud based framework for
production systems to demonstrate insights from privacy preserving incremental
learning for automatic speech recognition (ILASR). By privacy preserving, we
mean, usage of ephemeral data which are not human annotated. This system is a
step forward for production levelASR models for incremental/continual learning
that offers near real-time test-bed for experimentation in the cloud for
end-to-end ASR, while adhering to privacy-preserving policies. We show that the
proposed system can improve the production models significantly(3%) over a new
time period of six months even in the absence of human annotated labels with
varying levels of weak supervision and large batch sizes in incremental
learning. This improvement is 20% over test sets with new words and phrases in
the new time period. We demonstrate the effectiveness of model building in a
privacy-preserving incremental fashion for ASR while further exploring the
utility of having an effective teacher model and use of large batch sizes.",0,1,0,0,0,0,0.30744,8.0,0.673644,67
3798ff4b-6c9b-483f-8bc9-d7fd7645a381,Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study),26,0.557154,0.728461,"Test Input Prioritizers (TIP) for Deep Neural Networks (DNN) are an important
technique to handle the typically very large test datasets efficiently, saving
computation and labeling costs. This is particularly true for large-scale,
deployed systems, where inputs observed in production are recorded to serve as
potential test or training data for the next versions of the system. Feng et.
al. propose DeepGini, a very fast and simple TIP, and show that it outperforms
more elaborate techniques such as neuron- and surprise coverage. In a
large-scale study (4 case studies, 8 test datasets, 32'200 trained models) we
verify their findings. However, we also find that other comparable or even
simpler baselines from the field of uncertainty quantification, such as the
predicted softmax likelihood or the entropy of the predicted softmax
likelihoods perform equally well as DeepGini.",1,1,0,0,0,0,0.838182,7.0,0.855729,47
be96c08c-899d-465f-895f-435394578046,A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing,5,0.383577,0.882681,"Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have
relied to date on various versions of the Hebrew Treebank (HTB, Sima'an et al.
2001). However, the data in HTB, a single-source newswire corpus, is now over
30 years old, and does not cover many aspects of contemporary Hebrew on the
web. This paper presents a new, freely available UD treebank of Hebrew
stratified from a range of topics selected from Hebrew Wikipedia. In addition
to introducing the corpus and evaluating the quality of its annotations, we
deploy automatic validation tools based on grew (Guillaume, 2021), and conduct
the first cross domain parsing experiments in Hebrew. We obtain new
state-of-the-art (SOTA) results on UD NLP tasks, using a combination of the
latest language modelling and some incremental improvements to existing
transformer based approaches. We also release a new version of the UD HTB
matching annotation scheme updates from our new corpus.",0,1,0,1,1,0,0.51924,9.0,0.786569,23
ea7be224-8075-4885-aa3c-57227ba236fc,Benchmarking Self-Supervised Learning on Diverse Pathology Datasets,46,0.167793,0.878163,"Computational pathology can lead to saving human lives, but models are
annotation hungry and pathology images are notoriously expensive to annotate.
Self-supervised learning has shown to be an effective method for utilizing
unlabeled data, and its application to pathology could greatly benefit its
downstream tasks. Yet, there are no principled studies that compare SSL methods
and discuss how to adapt them for pathology. To address this need, we execute
the largest-scale study of SSL pre-training on pathology image data, to date.
Our study is conducted using 4 representative SSL methods on diverse downstream
tasks. We establish that large-scale domain-aligned pre-training in pathology
consistently out-performs ImageNet pre-training in standard SSL settings such
as linear and fine-tuning evaluations, as well as in low-label regimes.
Moreover, we propose a set of domain-specific techniques that we experimentally
show leads to a performance boost. Lastly, for the first time, we apply SSL to
the challenging task of nuclei instance segmentation and show large and
consistent performance improvements under diverse settings.",1,1,0,0,0,0,0.435765,7.0,0.690349,72
d9d20b64-0271-44db-a0ba-261c6cbb0ee0,MDFEND: Multi-domain Fake News Detection,70,0.994954,0.998453,"Fake news spread widely on social media in various domains, which lead to
real-world threats in many aspects like politics, disasters, and finance. Most
existing approaches focus on single-domain fake news detection (SFND), which
leads to unsatisfying performance when these methods are applied to
multi-domain fake news detection. As an emerging field, multi-domain fake news
detection (MFND) is increasingly attracting attention. However, data
distributions, such as word frequency and propagation patterns, vary from
domain to domain, namely domain shift. Facing the challenge of serious domain
shift, existing fake news detection techniques perform poorly for multi-domain
scenarios. Therefore, it is demanding to design a specialized model for MFND.
In this paper, we first design a benchmark of fake news dataset for MFND with
domain label annotated, namely Weibo21, which consists of 4,488 fake news and
4,640 real news from 9 different domains. We further propose an effective
Multi-domain Fake News Detection Model (MDFEND) by utilizing a domain gate to
aggregate multiple representations extracted by a mixture of experts. The
experiments show that MDFEND can significantly improve the performance of
multi-domain fake news detection. Our dataset and code are available at
https://github.com/kennqiang/MDFEND-Weibo21.",0,1,1,1,1,0,0.990801,7.0,0.990815,42
2c140b7e-5b71-47e5-926a-29d2b88226cf,Towards Understanding Mixture of Experts in Deep Learning,23,0.270257,0.856862,"The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by
a router, has achieved great success in deep learning. However, the
understanding of such architecture remains elusive. In this paper, we formally
study how the MoE layer improves the performance of neural network learning and
why the mixture model will not collapse into a single model. Our empirical
results suggest that the cluster structure of the underlying problem and the
non-linearity of the expert are pivotal to the success of MoE. To further
understand this, we consider a challenging classification problem with
intrinsic cluster structures, which is hard to learn using a single expert. Yet
with the MoE layer, by choosing the experts as two-layer nonlinear
convolutional neural networks (CNNs), we show that the problem can be learned
successfully. Furthermore, our theory shows that the router can learn the
cluster-center features, which helps divide the input complex problem into
simpler linear classification sub-problems that individual experts can conquer.
To our knowledge, this is the first result towards formally understanding the
mechanism of the MoE layer for deep learning.",0,0,0,0,0,0,0.608563,11.0,0.847862,58
26234514-850c-4ebc-bc63-83a0b6ba787a,Hierarchical Semi-Supervised Contrastive Learning for Contamination-Resistant Anomaly Detection,8,0.0863342,0.548656,"Anomaly detection aims at identifying deviant samples from the normal data
distribution. Contrastive learning has provided a successful way to sample
representation that enables effective discrimination on anomalies. However,
when contaminated with unlabeled abnormal samples in training set under
semi-supervised settings, current contrastive-based methods generally 1) ignore
the comprehensive relation between training data, leading to suboptimal
performance, and 2) require fine-tuning, resulting in low efficiency. To
address the above two issues, in this paper, we propose a novel hierarchical
semi-supervised contrastive learning (HSCL) framework, for
contamination-resistant anomaly detection. Specifically, HSCL hierarchically
regulates three complementary relations: sample-to-sample, sample-to-prototype,
and normal-to-abnormal relations, enlarging the discrimination between normal
and abnormal samples with a comprehensive exploration of the contaminated data.
Besides, HSCL is an end-to-end learning approach that can efficiently learn
discriminative representations without fine-tuning. HSCL achieves
state-of-the-art performance in multiple scenarios, such as one-class
classification and cross-dataset detection. Extensive ablation studies further
verify the effectiveness of each considered relation. The code is available at
https://github.com/GaoangW/HSCL.",1,1,0,0,1,0,0.812035,5.0,0.780857,83
4d98b820-665e-42ef-b401-8760eaa81a63,rPPG-Toolbox: Deep Remote PPG Toolbox,15,0.132843,0.746494,"Camera-based physiological measurement is a fast growing field of computer
vision. Remote photoplethysmography (rPPG) utilizes imaging devices (e.g.,
cameras) to measure the peripheral blood volume pulse (BVP) via
photoplethysmography, and enables cardiac measurement via webcams and
smartphones. However, the task is non-trivial with important pre-processing,
modeling, and post-processing steps required to obtain state-of-the-art
results. Replication of results and benchmarking of new models is critical for
scientific progress; however, as with many other applications of deep learning,
reliable codebases are not easy to find or use. We present a comprehensive
toolbox, rPPG-Toolbox, that contains unsupervised and supervised rPPG models
with support for public benchmark datasets, data augmentation, and systematic
evaluation: \url{https://github.com/ubicomplab/rPPG-Toolbox}",1,1,0,1,0,0,0.237959,6.0,0.514618,69
18551e7d-caaf-4f20-bf4e-ab808b298f8e,Training Naturalized Semantic Parsers with Very Little Data,10,0.19686,0.465395,"Semantic parsing is an important NLP problem, particularly for voice
assistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic
parsers are seq2seq architectures based on large language models that have been
pretrained on vast amounts of text. To better leverage that pretraining, recent
work has explored a reformulation of semantic parsing whereby the output
sequences are themselves natural language sentences, but in a controlled
fragment of natural language. This approach delivers strong results,
particularly for few-shot semantic parsing, which is of key importance in
practice and the focus of our paper. We push this line of work forward by
introducing an automated methodology that delivers very significant additional
improvements by utilizing modest amounts of unannotated data, which is
typically easy to obtain. Our method is based on a novel synthesis of four
techniques: joint training with auxiliary unsupervised tasks; constrained
decoding; self-training; and paraphrasing. We show that this method delivers
new SOTA few-shot performance on the Overnight dataset, particularly in very
low-resource settings, and very compelling few-shot results on a new semantic
parsing dataset.",1,1,0,0,0,0,0.825756,7.0,0.849805,23
f821bf1e-4b5a-490d-af8f-d6747770c708,Deep Sequence Models for Text Classification Tasks,1,0.000821099,0.00937728,"The exponential growth of data generated on the Internet in the current
information age is a driving force for the digital economy. Extraction of
information is the major value in an accumulated big data. Big data dependency
on statistical analysis and hand-engineered rules machine learning algorithms
are overwhelmed with vast complexities inherent in human languages. Natural
Language Processing (NLP) is equipping machines to understand these human
diverse and complicated languages. Text Classification is an NLP task which
automatically identifies patterns based on predefined or undefined labeled
sets. Common text classification application includes information retrieval,
modeling news topic, theme extraction, sentiment analysis, and spam detection.
In texts, some sequences of words depend on the previous or next word sequences
to make full meaning; this is a challenging dependency task that requires the
machine to be able to store some previous important information to impact
future meaning. Sequence models such as RNN, GRU, and LSTM is a breakthrough
for tasks with long-range dependencies. As such, we applied these models to
Binary and Multi-class classification. Results generated were excellent with
most of the models performing within the range of 80% and 94%. However, this
result is not exhaustive as we believe there is room for improvement if
machines are to compete with humans.",0,1,0,0,0,1,0.0373275,10.0,0.51215,30
07a9f5c1-8d3d-4cb0-98f5-0e6c4c2a37a7,I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning,9,0.0899276,0.680076,"Knowledge graph (KG) embedding seeks to learn vector representations for
entities and relations. Conventional models reason over graph structures, but
they suffer from the issues of graph incompleteness and long-tail entities.
Recent studies have used pre-trained language models to learn embeddings based
on the textual information of entities and relations, but they cannot take
advantage of graph structures. In the paper, we show empirically that these two
kinds of features are complementary for KG embedding. To this end, we propose
CoLE, a Co-distillation Learning method for KG Embedding that exploits the
complementarity of graph structures and text information. Its graph embedding
model employs Transformer to reconstruct the representation of an entity from
its neighborhood subgraph. Its text embedding model uses a pre-trained language
model to generate entity representations from the soft prompts of their names,
descriptions, and relational neighbors. To let the two model promote each
other, we propose co-distillation learning that allows them to distill
selective knowledge from each other's prediction logits. In our co-distillation
learning, each model serves as both a teacher and a student. Experiments on
benchmark datasets demonstrate that the two models outperform their related
baselines, and the ensemble method CoLE with co-distillation learning advances
the state-of-the-art of KG embedding.",0,1,0,0,1,0,0.710993,7.0,0.800961,36
e91d147c-d3b1-4cd7-aea8-f8d84f54a2a8,Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering,7,0.0905547,0.321883,"Multi-hop reasoning requires aggregating multiple documents to answer a
complex question. Existing methods usually decompose the multi-hop question
into simpler single-hop questions to solve the problem for illustrating the
explainable reasoning process. However, they ignore grounding on the supporting
facts of each reasoning step, which tends to generate inaccurate
decompositions. In this paper, we propose an interpretable stepwise reasoning
framework to incorporate both single-hop supporting sentence identification and
single-hop question generation at each intermediate step, and utilize the
inference of the current hop for the next until reasoning out the final result.
We employ a unified reader model for both intermediate hop reasoning and final
hop inference and adopt joint optimization for more accurate and robust
multi-hop reasoning. We conduct experiments on two benchmark datasets HotpotQA
and 2WikiMultiHopQA. The results show that our method can effectively boost
performance and also yields a better interpretable reasoning process without
decomposition supervision.",0,0,0,0,0,0,0.693874,6.0,0.759876,36
86a86ddc-3aeb-4772-87db-8495d53efc3a,Strong-TransCenter: Improved Multi-Object Tracking based on Transformers with Dense Representations,5,0.0407726,0.261722,"Transformer networks have been a focus of research in many fields in recent
years, being able to surpass the state-of-the-art performance in different
computer vision tasks. A few attempts have been made to apply this method to
the task of Multiple Object Tracking (MOT), among those the state-of-the-art
was TransCenter, a transformer-based MOT architecture with dense object queries
for accurately tracking all the objects while keeping reasonable runtime.
TransCenter is the first center-based transformer framework for MOT, and is
also among the first to show the benefits of using transformer-based
architectures for MOT. In this paper we show an improvement to this tracker
using post processing mechanism based in the Track-by-Detection paradigm:
motion model estimation using Kalman filter and target Re-identification using
an embedding network. Our new tracker shows significant improvements in the
IDF1 and HOTA metrics and comparable results on the MOTA metric (70.9%, 59.8%
and 75.8% respectively) on the MOTChallenge MOT17 test dataset and improvement
on all 3 metrics (67.5%, 56.3% and 73.0%) on the MOT20 test dataset. Our
tracker is currently ranked first among transformer-based trackers in these
datasets. The code is publicly available at:
https://github.com/amitgalor18/STC_Tracker",1,1,0,0,1,0,0.85981,5.0,0.813186,48
3e18d391-ae51-447d-aa59-5c7ad0a260e8,Unsupervised Domain Adaptation for One-stage Object Detector using Offsets to Bounding Box,4,0.00901797,0.215311,"Most existing domain adaptive object detection methods exploit adversarial
feature alignment to adapt the model to a new domain. Recent advances in
adversarial feature alignment strives to reduce the negative effect of
alignment, or negative transfer, that occurs because the distribution of
features varies depending on the category of objects. However, by analyzing the
features of the anchor-free one-stage detector, in this paper, we find that
negative transfer may occur because the feature distribution varies depending
on the regression value for the offset to the bounding box as well as the
category. To obtain domain invariance by addressing this issue, we align the
feature conditioned on the offset value, considering the modality of the
feature distribution. With a very simple and effective conditioning method, we
propose OADA (Offset-Aware Domain Adaptive object detector) that achieves
state-of-the-art performances in various experimental settings. In addition, by
analyzing through singular value decomposition, we find that our model enhances
both discriminability and transferability.",0,0,0,0,0,0,0.294042,7.0,0.619369,46
29f418bf-17f8-4502-9499-f0618b5c140b,Multi-fidelity Gaussian Process for Biomanufacturing Process Modeling with Small Data,2,0.034689,0.0364649,"In biomanufacturing, developing an accurate model to simulate the complex
dynamics of bioprocesses is an important yet challenging task. This is
partially due to the uncertainty associated with bioprocesses, high data
acquisition cost, and lack of data availability to learn complex relations in
bioprocesses. To deal with these challenges, we propose to use a statistical
machine learning approach, multi-fidelity Gaussian process, for process
modelling in biomanufacturing. Gaussian process regression is a
well-established technique based on probability theory which can naturally
consider uncertainty in a dataset via Gaussian noise, and multi-fidelity
techniques can make use of multiple sources of information with different
levels of fidelity, thus suitable for bioprocess modeling with small data. We
apply the multi-fidelity Gaussian process to solve two significant problems in
biomanufacturing, bioreactor scale-up and knowledge transfer across cell lines,
and demonstrate its efficacy on real-world datasets.",0,1,0,0,0,0,0.0634901,9.0,0.518479,52
07959959-3345-4c84-bd12-d163a93d7723,Accelerating Diffusion Sampling with Classifier-based Feature Distillation,6,0.0170583,0.0471972,"Although diffusion model has shown great potential for generating higher
quality images than GANs, slow sampling speed hinders its wide application in
practice. Progressive distillation is thus proposed for fast sampling by
progressively aligning output images of $N$-step teacher sampler with
$N/2$-step student sampler. In this paper, we argue that this
distillation-based accelerating method can be further improved, especially for
few-step samplers, with our proposed \textbf{C}lassifier-based \textbf{F}eature
\textbf{D}istillation (CFD). Instead of aligning output images, we distill
teacher's sharpened feature distribution into the student with a
dataset-independent classifier, making the student focus on those important
features to improve performance. We also introduce a dataset-oriented loss to
further optimize the model. Experiments on CIFAR-10 show the superiority of our
method in achieving high quality and fast sampling. Code is provided at
\url{https://github.com/zju-SWJ/RCFD}.",1,1,0,0,1,0,0.594173,5.0,0.657446,29
072b8d31-1ef5-452c-bf33-1106c019f994,Multi-View Dreaming: Multi-View World Model with Contrastive Learning,6,0.0416167,0.298509,"In this paper, we propose Multi-View Dreaming, a novel reinforcement learning
agent for integrated recognition and control from multi-view observations by
extending Dreaming. Most current reinforcement learning method assumes a
single-view observation space, and this imposes limitations on the observed
data, such as lack of spatial information and occlusions. This makes obtaining
ideal observational information from the environment difficult and is a
bottleneck for real-world robotics applications. In this paper, we use
contrastive learning to train a shared latent space between different
viewpoints, and show how the Products of Experts approach can be used to
integrate and control the probability distributions of latent states for
multiple viewpoints. We also propose Multi-View DreamingV2, a variant of
Multi-View Dreaming that uses a categorical distribution to model the latent
state instead of the Gaussian distribution. Experiments show that the proposed
method outperforms simple extensions of existing methods in a realistic robot
control task.",0,0,0,0,0,0,0.565073,5.0,0.641467,29
28f679df-767e-4053-9ba1-4859434038c0,Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition,6,0.0920661,0.521623,"Recent studies have revealed the vulnerability of face recognition models
against physical adversarial patches, which raises security concerns about the
deployed face recognition systems. However, it is still challenging to ensure
the reproducibility for most attack algorithms under complex physical
conditions, which leads to the lack of a systematic evaluation of the existing
methods. It is therefore imperative to develop a framework that can enable a
comprehensive evaluation of the vulnerability of face recognition in the
physical world. To this end, we propose to simulate the complex transformations
of faces in the physical world via 3D-face modeling, which serves as a digital
counterpart of physical faces. The generic framework allows us to control
different face variations and physical conditions to conduct reproducible
evaluations comprehensively. With this digital simulator, we further propose a
Face3DAdv method considering the 3D face transformations and realistic physical
variations. Extensive experiments validate that Face3DAdv can significantly
improve the effectiveness of diverse physically realizable adversarial patches
in both simulated and physical environments, against various white-box and
black-box face recognition models.",0,1,0,0,0,0,0.664372,7.0,0.782621,42
5a981d2c-834c-456d-9673-12df77081e31,Domain Mismatch Doesn't Always Prevent Cross-Lingual Transfer Learning,1,0.0189349,0.112297,"Cross-lingual transfer learning without labeled target language data or
parallel text has been surprisingly effective in zero-shot cross-lingual
classification, question answering, unsupervised machine translation, etc.
However, some recent publications have claimed that domain mismatch prevents
cross-lingual transfer, and their results show that unsupervised bilingual
lexicon induction (UBLI) and unsupervised neural machine translation (UNMT) do
not work well when the underlying monolingual corpora come from different
domains (e.g., French text from Wikipedia but English text from UN
proceedings). In this work, we show that a simple initialization regimen can
overcome much of the effect of domain mismatch in cross-lingual transfer. We
pre-train word and contextual embeddings on the concatenated domain-mismatched
corpora, and use these as initializations for three tasks: MUSE UBLI, UN
Parallel UNMT, and the SemEval 2017 cross-lingual word similarity task. In all
cases, our results challenge the conclusions of prior work by showing that
proper initialization can recover a large portion of the losses incurred by
domain mismatch.",0,1,0,0,0,0,0.519958,10.0,0.808115,40
b26aa7fb-8016-4c62-9d83-244d4f93d370,XPrompt: Exploring the Extreme of Prompt Tuning,17,0.186522,0.192711,"Prompt tuning learns soft prompts to condition frozen Pre-trained Language
Models (PLMs) for performing downstream tasks in a parameter-efficient manner.
While prompt tuning has gradually reached the performance level of fine-tuning
as the model scale increases, there is still a large performance gap between
prompt tuning and fine-tuning for models of moderate and small scales
(typically less than 11B parameters). In this paper, we empirically show that
the trained prompt tokens can have a negative impact on a downstream task and
thus degrade its performance. To bridge the gap, we propose a novel Prompt
tuning model with an eXtremely small scale (XPrompt) under the regime of
lottery tickets hypothesis. Specifically, XPrompt eliminates the negative
prompt tokens at different granularity levels through a hierarchical structured
pruning, yielding a more parameter-efficient prompt yet with a competitive
performance. Comprehensive experiments are carried out on SuperGLUE tasks, and
the extensive results indicate that XPrompt is able to close the performance
gap at smaller model scales.",0,1,0,0,0,0,0.880039,6.0,0.857043,52
6d63ee48-d3e1-43cd-84a3-5555eee84f98,"On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation",6,0.0112318,0.108458,"A good automatic evaluation metric for language generation ideally correlates
highly with human judgements of text quality. Yet, there is a dearth of such
metrics, which inhibits the rapid and efficient progress of language
generators. One exception is the recently proposed Mauve. In theory, Mauve
measures an information-theoretic divergence between two probability
distributions over strings: one representing the language generator under
evaluation; the other representing the true natural language distribution.
Mauve's authors argue that its success comes from the qualitative properties of
their proposed divergence. Yet in practice, as this divergence is uncomputable,
Mauve approximates it by measuring the divergence between multinomial
distributions over clusters instead, where cluster assignments are attained by
grouping strings based on a pre-trained language model's embeddings. As we
show, however, this is not a tight approximation -- in either theory or
practice. This begs the question: why does Mauve work so well? In this work, we
show that Mauve was right for the wrong reasons, and that its newly proposed
divergence is not necessary for its high performance. In fact, classical
divergences paired with its proposed cluster-based approximation may actually
serve as better evaluation metrics. We finish the paper with a probing
analysis; this analysis leads us to conclude that -- by encoding syntactic- and
coherence-level features of text, while ignoring surface-level features -- such
cluster-based substitutes to string distributions may simply be better for
evaluating state-of-the-art language generators.",0,0,0,0,0,0,0.381952,6.0,0.609845,45
3be19415-b8cd-4c99-b1de-6ae46a12eb3b,An Active Contour Model with Local Variance Force Term and Its Efficient Minimization Solver for Multi-phase Image Segmentation,1,0.00742481,0.0480144,"In this paper, we propose an active contour model with a local variance force
(LVF) term that can be applied to multi-phase image segmentation problems. With
the LVF, the proposed model is very effective in the segmentation of images
with noise. To solve this model efficiently, we represent the regularization
term by characteristic functions and then design a minimization algorithm based
on a modification of the iterative convolution-thresholding method (ICTM),
namely ICTM-LVF. This minimization algorithm enjoys the energy-decaying
property under some conditions and has highly efficient performance in the
segmentation. To overcome the initialization issue of active contour models, we
generalize the inhomogeneous graph Laplacian initialization method (IGLIM) to
the multi-phase case and then apply it to give the initial contour of the
ICTM-LVF solver. Numerical experiments are conducted on synthetic images and
real images to demonstrate the capability of our initialization method, and the
effectiveness of the local variance force for noise robustness in the
multi-phase image segmentation.",0,0,0,0,0,0,0.00710133,14.0,0.531904,43
de2692c3-8aea-4c1d-a975-31e742574405,Neural Weight Search for Scalable Task Incremental Learning,2,0.0299848,0.0929746,"Task incremental learning aims to enable a system to maintain its performance
on previously learned tasks while learning new tasks, solving the problem of
catastrophic forgetting. One promising approach is to build an individual
network or sub-network for future tasks. However, this leads to an ever-growing
memory due to saving extra weights for new tasks and how to address this issue
has remained an open problem in task incremental learning. In this paper, we
introduce a novel Neural Weight Search technique that designs a fixed search
space where the optimal combinations of frozen weights can be searched to build
new models for novel tasks in an end-to-end manner, resulting in scalable and
controllable memory growth. Extensive experiments on two benchmarks, i.e.,
Split-CIFAR-100 and CUB-to-Sketches, show our method achieves state-of-the-art
performance with respect to both average inference accuracy and total memory
cost.",1,1,0,0,1,0,0.694805,10.0,0.856183,40
6409863a-4b7f-47b2-8ccf-bc70e25368a2,Real-time Full-stack Traffic Scene Perception for Autonomous Driving with Roadside Cameras,18,0.702931,0.906484,"We propose a novel and pragmatic framework for traffic scene perception with
roadside cameras. The proposed framework covers a full-stack of roadside
perception pipeline for infrastructure-assisted autonomous driving, including
object detection, object localization, object tracking, and multi-camera
information fusion. Unlike previous vision-based perception frameworks rely
upon depth offset or 3D annotation at training, we adopt a modular decoupling
design and introduce a landmark-based 3D localization method, where the
detection and localization can be well decoupled so that the model can be
easily trained based on only 2D annotations. The proposed framework applies to
either optical or thermal cameras with pinhole or fish-eye lenses. Our
framework is deployed at a two-lane roundabout located at Ellsworth Rd. and
State St., Ann Arbor, MI, USA, providing 7x24 real-time traffic flow monitoring
and high-precision vehicle trajectory extraction. The whole system runs
efficiently on a low-power edge computing device with all-component end-to-end
delay of less than 20ms.",0,1,0,0,0,0,0.964393,12.0,0.966258,34
7b68dd1a-2d2c-4dca-8f67-898611d5b825,Adversarial Examples for Good: Adversarial Examples Guided Imbalanced Learning,8,0.0470074,0.345781,"Adversarial examples are inputs for machine learning models that have been
designed by attackers to cause the model to make mistakes. In this paper, we
demonstrate that adversarial examples can also be utilized for good to improve
the performance of imbalanced learning. We provide a new perspective on how to
deal with imbalanced data: adjust the biased decision boundary by training with
Guiding Adversarial Examples (GAEs). Our method can effectively increase the
accuracy of minority classes while sacrificing little accuracy on majority
classes. We empirically show, on several benchmark datasets, our proposed
method is comparable to the state-of-the-art method. To our best knowledge, we
are the first to deal with imbalanced learning with adversarial examples.",0,1,1,0,1,0,0.714335,6.0,0.769342,23
3a714ae4-f255-47a8-8860-a2d9eb171d72,Reverse Survival Model (RSM): A Pipeline for Explaining Predictions of Deep Survival Models,1,0.00296097,0.122132,"The aim of survival analysis in healthcare is to estimate the probability of
occurrence of an event, such as a patient's death in an intensive care unit
(ICU). Recent developments in deep neural networks (DNNs) for survival analysis
show the superiority of these models in comparison with other well-known models
in survival analysis applications. Ensuring the reliability and explainability
of deep survival models deployed in healthcare is a necessity. Since DNN models
often behave like a black box, their predictions might not be easily trusted by
clinicians, especially when predictions are contrary to a physician's opinion.
A deep survival model that explains and justifies its decision-making process
could potentially gain the trust of clinicians. In this research, we propose
the reverse survival model (RSM) framework that provides detailed insights into
the decision-making process of survival models. For each patient of interest,
RSM can extract similar patients from a dataset and rank them based on the most
relevant features that deep survival models rely on for their predictions.",0,1,0,0,0,0,0.00218428,14.0,0.447513,76
f056f704-a750-4520-af2b-215782943d64,Investigating Information Inconsistency in Multilingual Open-Domain Question Answering,1,0.00601397,0.0349883,"Retrieval based open-domain QA systems use retrieved documents and
answer-span selection over retrieved documents to find best-answer candidates.
We hypothesize that multilingual Question Answering (QA) systems are prone to
information inconsistency when it comes to documents written in different
languages, because these documents tend to provide a model with varying
information about the same topic. To understand the effects of the biased
availability of information and cultural influence, we analyze the behavior of
multilingual open-domain question answering models with a focus on retrieval
bias. We analyze if different retriever models present different passages given
the same question in different languages on TyDi QA and XOR-TyDi QA, two
multilingualQA datasets. We speculate that the content differences in documents
across languages might reflect cultural divergences and/or social biases.",0,0,0,0,0,0,0.547085,6.0,0.692905,32
7ea3c78d-a46a-4695-a6ca-4256e6aa1950,Bringing Old Films Back to Life,32,0.233938,0.91539,"We present a learning-based framework, recurrent transformer network (RTN),
to restore heavily degraded old films. Instead of performing frame-wise
restoration, our method is based on the hidden knowledge learned from adjacent
frames that contain abundant information about the occlusion, which is
beneficial to restore challenging artifacts of each frame while ensuring
temporal coherency. Moreover, contrasting the representation of the current
frame and the hidden knowledge makes it possible to infer the scratch position
in an unsupervised manner, and such defect localization generalizes well to
real-world degradations. To better resolve mixed degradation and compensate for
the flow estimation error during frame alignment, we propose to leverage more
expressive transformer blocks for spatial restoration. Experiments on both
synthetic dataset and real-world old films demonstrate the significant
superiority of the proposed RTN over existing solutions. In addition, the same
framework can effectively propagate the color from keyframes to the whole
video, ultimately yielding compelling restored films. The implementation and
model will be released at
https://github.com/raywzy/Bringing-Old-Films-Back-to-Life.",1,1,0,0,0,0,0.711268,7.0,0.80107,55
6219b719-c23a-4f1b-9813-baa46419af54,Model-Free Opponent Shaping,21,0.324797,0.585135,"In general-sum games, the interaction of self-interested learning agents
commonly leads to collectively worst-case outcomes, such as defect-defect in
the iterated prisoner's dilemma (IPD). To overcome this, some methods, such as
Learning with Opponent-Learning Awareness (LOLA), shape their opponents'
learning process. However, these methods are myopic since only a small number
of steps can be anticipated, are asymmetric since they treat other agents as
naive learners, and require the use of higher-order derivatives, which are
calculated through white-box access to an opponent's differentiable learning
algorithm. To address these issues, we propose Model-Free Opponent Shaping
(M-FOS). M-FOS learns in a meta-game in which each meta-step is an episode of
the underlying inner game. The meta-state consists of the inner policies, and
the meta-policy produces a new inner policy to be used in the next episode.
M-FOS then uses generic model-free optimisation methods to learn meta-policies
that accomplish long-horizon opponent shaping. Empirically, M-FOS
near-optimally exploits naive learners and other, more sophisticated algorithms
from the literature. For example, to the best of our knowledge, it is the first
method to learn the well-known Zero-Determinant (ZD) extortion strategy in the
IPD. In the same settings, M-FOS leads to socially optimal outcomes under
meta-self-play. Finally, we show that M-FOS can be scaled to high-dimensional
settings.",1,0,0,0,0,0,0.284261,10.0,0.729526,36
dbb4112f-dde0-4ab8-a045-a484be13ed4d,Cross-Enhancement Transformer for Action Segmentation,18,0.232551,0.329097,"Temporal convolutions have been the paradigm of choice in action
segmentation, which enhances long-term receptive fields by increasing
convolution layers. However, high layers cause the loss of local information
necessary for frame recognition. To solve the above problem, a novel
encoder-decoder structure is proposed in this paper, called Cross-Enhancement
Transformer. Our approach can be effective learning of temporal structure
representation with interactive self-attention mechanism. Concatenated each
layer convolutional feature maps in encoder with a set of features in decoder
produced via self-attention. Therefore, local and global information are used
in a series of frame actions simultaneously. In addition, a new loss function
is proposed to enhance the training process that penalizes over-segmentation
errors. Experiments show that our framework performs state-of-the-art on three
challenging datasets: 50Salads, Georgia Tech Egocentric Activities and the
Breakfast dataset.",0,0,0,0,1,0,0.80354,5.0,0.775496,45
a0aa0107-087b-4c35-8f29-63ef8e30a21e,Route Planning for Last-Mile Deliveries Using Mobile Parcel Lockers: A Hybrid Q-Learning Network Approach,3,0.0310365,0.482134,"Mobile parcel lockers have been recently proposed by logistics operators as a
technology that could help reduce traffic congestion and operational costs in
urban freight distribution. Given their ability to relocate throughout their
area of deployment, they hold the potential to improve customer accessibility
and convenience. In this study, we formulate the Mobile Parcel Locker Problem
(MPLP) , a special case of the Location-Routing Problem (LRP) which determines
the optimal stopover location for MPLs throughout the day and plans
corresponding delivery routes. A Hybrid Q Learning Network based Method (HQM)
is developed to resolve the computational complexity of the resulting large
problem instances while escaping local optima. In addition, the HQM is
integrated with global and local search mechanisms to resolve the dilemma of
exploration and exploitation faced by classic reinforcement learning methods.
We examine the performance of HQM under different problem sizes (up to 200
nodes) and benchmarked it against the exact approach and Genetic Algorithm
(GA). Our results indicate that HQM achieves better optimisation performance
with shorter computation time than the exact approach solved by the Gurobi
solver in large problem instances. Additionally, the average reward obtained by
HQM is 1.96 times greater than GA, which demonstrates that HQM has a better
optimisation ability. Further, we identify critical factors that contribute to
fleet size requirements, travel distances, and service delays. Our findings
outline that the efficiency of MPLs is mainly contingent on the length of time
windows and the deployment of MPL stopovers. Finally, we highlight managerial
implications based on parametric analysis to provide guidance for logistics
operators in the context of efficient last-mile distribution operations.",0,1,0,0,1,0,0.0469396,10.0,0.535561,59
ea1ea9b6-16d1-4934-a394-09a2b0bde4a9,BLIND: Bias Removal With No Demographics,9,0.117693,0.661212,"Models trained on real-world data tend to imitate and amplify social biases.
Common methods to mitigate biases require prior information on the types of
biases that should be mitigated (e.g., gender or racial bias) and the social
groups associated with each data sample. In this work, we introduce BLIND, a
method for bias removal with no prior knowledge of the demographics in the
dataset. While training a model on a downstream task, BLIND detects biased
samples using an auxiliary model that predicts the main model's success, and
down-weights those samples during the training process. Experiments with racial
and gender biases in sentiment classification and occupation classification
tasks demonstrate that BLIND mitigates social biases without relying on a
costly demographic annotation process. Our method is competitive with other
methods that require demographic information and sometimes even surpasses them.",1,1,0,0,0,1,0.613905,6.0,0.723505,51
8908128c-c437-4ac5-b8b0-ef7bc9205daa,Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages,10,0.774654,0.652001,"Multilingual Pretrained Language Models (MPLMs) have shown their strong
multilinguality in recent empirical cross-lingual transfer studies. In this
paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)
pipeline to improve the zero-shot performance on low-resource languages (LRLs)
by augmenting the context with semantically similar sentences retrieved from a
high-resource language (HRL) as prompts. PARC improves the zero-shot
performance on three downstream tasks (binary sentiment classification, topic
categorization and natural language inference) with multilingual parallel test
sets across 10 LRLs covering 6 language families in both unlabeled settings
(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the
finetuning baseline by 3.7%. We find a significant positive correlation between
cross-lingual transfer performance on one side, and the similarity between the
high- and low-resource languages as well as the amount of low-resource
pretraining data on the other side. A robustness analysis suggests that PARC
has the potential to achieve even stronger performance with more powerful
MPLMs.",1,1,0,0,1,0,0.988475,6.0,0.981071,48
78185b44-30d2-42ec-bbf4-4425b6c39985,PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model,21,0.156793,0.647594,"In this paper, we introduce PanGu-Bot, a Chinese pre-trained open-domain
dialogue generation model based on a large pre-trained language model (PLM)
PANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue
models trained over a massive amount of dialogue data from scratch, we aim to
build a powerful dialogue model with relatively fewer data and computation
costs by inheriting valuable language capabilities and knowledge from PLMs. To
this end, we train PanGu-Bot from the large PLM PANGU-alpha, which has been
proven well-performed on a variety of Chinese natural language tasks. We
investigate different aspects of responses generated by PanGu-Bot, including
response quality, knowledge, and safety. We show that PanGu-Bot outperforms
state-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA
(Zhou et al., 2021), EVA2.0 (Gu et al., 2022)) w.r.t. the above three aspects.
We also demonstrate that PanGu-Bot can be easily deployed to generate emotional
responses without further training. Throughout our empirical analysis, we also
point out that the PanGu-Bot response quality, knowledge correctness, and
safety are still far from perfect, and further explorations are indispensable
to building reliable and smart dialogue systems. Our model and code will be
available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot
soon.",1,1,0,0,1,0,0.829947,4.0,0.740618,60
c729353b-5ee2-46ed-af4c-85706ab426c8,Learning Rationalizable Equilibria in Multiplayer Games,1,0.00783621,0.242386,"A natural goal in multiagent learning besides finding equilibria is to learn
rationalizable behavior, where players learn to avoid iteratively dominated
actions. However, even in the basic setting of multiplayer general-sum games,
existing algorithms require a number of samples exponential in the number of
players to learn rationalizable equilibria under bandit feedback. This paper
develops the first line of efficient algorithms for learning rationalizable
Coarse Correlated Equilibria (CCE) and Correlated Equilibria (CE) whose sample
complexities are polynomial in all problem parameters including the number of
players. To achieve this result, we also develop a new efficient algorithm for
the simpler task of finding one rationalizable action profile (not necessarily
an equilibrium), whose sample complexity substantially improves over the best
existing results of Wu et al. (2021). Our algorithms incorporate several novel
techniques to guarantee rationalizability and no (swap-)regret simultaneously,
including a correlated exploration scheme and adaptive learning rates, which
may be of independent interest. We complement our results with a sample
complexity lower bound showing the sharpness of our guarantees.",0,0,0,0,0,0,0.0056879,15.0,0.548267,36
1b1d4eeb-2215-4ce9-8fdb-b7583fada4bb,Tourist Guidance Robot Based on HyperCLOVA,2,0.0423386,0.335462,"This paper describes our system submitted to Dialogue Robot Competition 2022.
Our proposed system is a combined model of rule-based and generation-based
dialog systems. The system utilizes HyperCLOVA, a Japanese foundation model,
not only to generate responses but also summarization, search information, etc.
We also used our original speech recognition system, which was fine-tuned for
this dialog task. As a result, our system ranked second in the preliminary
round and moved on to the finals.",0,1,0,0,0,0,0.569193,3.0,0.406234,6
3f8a3ec3-3868-4d61-bd61-4477736f498c,TASTEset -- Recipe Dataset and Food Entities Recognition Benchmark,3,0.0389036,0.310313,"Food Computing is currently a fast-growing field of research. Natural
language processing (NLP) is also increasingly essential in this field,
especially for recognising food entities. However, there are still only a few
well-defined tasks that serve as benchmarks for solutions in this area. We
introduce a new dataset -- called \textit{TASTEset} -- to bridge this gap. In
this dataset, Named Entity Recognition (NER) models are expected to find or
infer various types of entities helpful in processing recipes, e.g.~food
products, quantities and their units, names of cooking processes, physical
quality of ingredients, their purpose, taste.
  The dataset consists of 700 recipes with more than 13,000 entities to
extract. We provide a few state-of-the-art baselines of named entity
recognition models, which show that our dataset poses a solid challenge to
existing models. The best model achieved, on average, 0.95 $F_1$ score,
depending on the entity type -- from 0.781 to 0.982. We share the dataset and
the task to encourage progress on more in-depth and complex information
extraction from recipes.",0,1,1,1,1,0,0.335214,8.0,0.686848,25
94281cab-435d-4e2a-9fec-c866ac194886,Effective Image Tampering Localization with Multi-Scale ConvNeXt Feature Fusion,7,0.158865,0.431287,"With the widespread use of powerful image editing tools, image tampering
becomes easy and realistic. Existing image forensic methods still face
challenges of low generalization performance and robustness. In this letter, we
propose an effective image tampering localization scheme based on ConvNeXt
network and multi-scale feature fusion. Stacked ConvNeXt blocks are used as an
encoder to capture hierarchical multi-scale features, which are then fused in
decoder for locating tampered pixels accurately. Combined loss and effective
data augmentation are adopted to further improve the model performance.
Extensive experimental results show that localization performance of our
proposed scheme outperforms other state-of-the-art ones. The source code will
be available at https://github.com/ZhuHC98/ITL-SSN.",1,1,0,0,1,0,0.624557,9.0,0.818889,33
84e8aa5c-7482-4a82-9426-05f68d250e2b,Type-aware Embeddings for Multi-Hop Reasoning over Knowledge Graphs,22,0.324242,0.695082,"Multi-hop reasoning over real-life knowledge graphs (KGs) is a highly
challenging problem as traditional subgraph matching methods are not capable to
deal with noise and missing information. To address this problem, it has been
recently introduced a promising approach based on jointly embedding logical
queries and KGs into a low-dimensional space to identify answer entities.
However, existing proposals ignore critical semantic knowledge inherently
available in KGs, such as type information. To leverage type information, we
propose a novel TypE-aware Message Passing (TEMP) model, which enhances the
entity and relation representations in queries, and simultaneously improves
generalization, deductive and inductive reasoning. Remarkably, TEMP is a
plug-and-play model that can be easily incorporated into existing
embedding-based models to improve their performance. Extensive experiments on
three real-world datasets demonstrate TEMP's effectiveness.",1,0,0,0,0,0,0.792024,5.0,0.76837,34
22e1b992-d49d-4ec3-b1b9-f546782d7a83,Black-box Few-shot Knowledge Distillation,9,0.173247,0.429478,"Knowledge distillation (KD) is an efficient approach to transfer the
knowledge from a large ""teacher"" network to a smaller ""student"" network.
Traditional KD methods require lots of labeled training samples and a white-box
teacher (parameters are accessible) to train a good student. However, these
resources are not always available in real-world applications. The distillation
process often happens at an external party side where we do not have access to
much data, and the teacher does not disclose its parameters due to security and
privacy concerns. To overcome these challenges, we propose a black-box few-shot
KD method to train the student with few unlabeled training samples and a
black-box teacher. Our main idea is to expand the training set by generating a
diverse set of out-of-distribution synthetic images using MixUp and a
conditional variational auto-encoder. These synthetic images along with their
labels obtained from the teacher are used to train the student. We conduct
extensive experiments to show that our method significantly outperforms recent
SOTA few/zero-shot KD methods on image classification tasks. The code and
models are available at: https://github.com/nphdang/FS-BBT",1,1,0,1,1,0,0.63725,7.0,0.772072,42
527dc796-091f-4981-85c0-24b7001d3a26,BEV-Locator: An End-to-end Visual Semantic Localization Network Using Multi-View Images,5,0.150138,0.264469,"Accurate localization ability is fundamental in autonomous driving.
Traditional visual localization frameworks approach the semantic map-matching
problem with geometric models, which rely on complex parameter tuning and thus
hinder large-scale deployment. In this paper, we propose BEV-Locator: an
end-to-end visual semantic localization neural network using multi-view camera
images. Specifically, a visual BEV (Birds-Eye-View) encoder extracts and
flattens the multi-view images into BEV space. While the semantic map features
are structurally embedded as map queries sequence. Then a cross-model
transformer associates the BEV features and semantic map queries. The
localization information of ego-car is recursively queried out by
cross-attention modules. Finally, the ego pose can be inferred by decoding the
transformer outputs. We evaluate the proposed method in large-scale nuScenes
and Qcraft datasets. The experimental results show that the BEV-locator is
capable to estimate the vehicle poses under versatile scenarios, which
effectively associates the cross-model information from multi-view images and
global semantic maps. The experiments report satisfactory accuracy with mean
absolute errors of 0.052m, 0.135m and 0.251$^\circ$ in lateral, longitudinal
translation and heading angle degree.",0,1,0,0,1,0,0.712845,8.0,0.826486,78
59e57a4d-e70e-4c48-b14c-b822f6cbc1ae,CGELBank: CGEL as a Framework for English Syntax Annotation,1,0.0359828,0.0356544,"We introduce the syntactic formalism of the \textit{Cambridge Grammar of the
English Language} (CGEL) to the world of treebanking through the CGELBank
project. We discuss some issues in linguistic analysis that arose in adapting
the formalism to corpus annotation, followed by quantitative and qualitative
comparisons with parallel UD and PTB treebanks. We argue that CGEL provides a
good tradeoff between comprehensiveness of analysis and usability for
annotation, which motivates expanding the treebank with automatic conversion in
the future.",0,0,1,0,0,0,0.0141174,28.0,0.790618,34
7ab5c308-be92-4aae-817f-d5259c7ac439,Differentiable Logics for Neural Network Training and Verification,2,0.0510299,0.115972,"The rising popularity of neural networks (NNs) in recent years and their
increasing prevalence in real-world applications have drawn attention to the
importance of their verification. While verification is known to be
computationally difficult theoretically, many techniques have been proposed for
solving it in practice. It has been observed in the literature that by default
neural networks rarely satisfy logical constraints that we want to verify. A
good course of action is to train the given NN to satisfy said constraint prior
to verifying them. This idea is sometimes referred to as continuous
verification, referring to the loop between training and verification. Usually
training with constraints is implemented by specifying a translation for a
given formal logic language into loss functions. These loss functions are then
used to train neural networks. Because for training purposes these functions
need to be differentiable, these translations are called differentiable logics
(DL). This raises several research questions. What kind of differentiable
logics are possible? What difference does a specific choice of DL make in the
context of continuous verification? What are the desirable criteria for a DL
viewed from the point of view of the resulting loss function? In this extended
abstract we will discuss and answer these questions.",0,0,0,0,0,0,0.806558,5.0,0.77739,15
7d4fd28c-5d87-4435-bf28-81cb7f023c98,On Linking Level Segments,6,0.264557,0.356888,"An increasingly common area of study in procedural content generation is the
creation of level segments: short pieces that can be used to form larger
levels. Previous work has used basic concatenation to form these larger levels.
However, even if the segments themselves are completable and well-formed,
concatenation can fail to produce levels that are completable and can cause
broken in-game structures (e.g. malformed pipes in Mario). We show this with
three tile-based games: a side-scrolling platformer, a vertical platformer, and
a top-down roguelike. Additionally, we present a Markov chain and a tree search
algorithm that finds a link between two level segments, which uses filters to
ensure completability and unbroken in-game structures in the linked segments.
We further show that these links work well for multi-segment levels. We find
that this method reliably finds links between segments and is customizable to
meet a designer's needs.",1,1,0,0,0,0,0.385237,10.0,0.767008,32
a36fd127-21e5-4bd6-ab5a-ee586bcaab51,LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning,19,0.300413,0.817618,"Recently, deep learning models have made great progress in MWP solving on
answer accuracy. However, they are uninterpretable since they mainly rely on
shallow heuristics to achieve high performance without understanding and
reasoning the grounded math logic. To address this issue and make a step
towards interpretable MWP solving, we first construct a high-quality MWP
dataset named InterMWP which consists of 11,495 MWPs and annotates
interpretable logical formulas based on algebraic knowledge as the grounded
linguistic logic of each solution equation. Different from existing MWP
datasets, our InterMWP benchmark asks for a solver to not only output the
solution expressions but also predict the corresponding logical formulas. We
further propose a novel approach with logical prompt and interpretation
generation, called LogicSolver. For each MWP, our LogicSolver first retrieves
some highly-correlated algebraic knowledge and then passes them to the backbone
model as prompts to improve the semantic representations of MWPs. With these
improved semantic representations, our LogicSolver generates corresponding
solution expressions and interpretable knowledge formulas in accord with the
generated solution expressions, simultaneously. Experimental results show that
our LogicSolver has stronger logical formula-based interpretability than
baselines while achieving higher answer accuracy with the help of logical
prompts, simultaneously. The source code and dataset is available at
https://github.com/yangzhch6/InterMWP.",1,0,1,1,0,0,0.848192,6.0,0.837427,38
230b0985-574c-409a-90d4-5be47f5e91ad,Faithful Reasoning Using Large Language Models,90,0.870596,0.969425,"Although contemporary large language models (LMs) demonstrate impressive
question-answering capabilities, their answers are typically the product of a
single call to the model. This entails an unwelcome degree of opacity and
compromises performance, especially on problems that are inherently multi-step.
To address these limitations, we show how LMs can be made to perform faithful
multi-step reasoning via a process whose causal structure mirrors the
underlying logical structure of the problem. Our approach works by chaining
together reasoning steps, where each step results from calls to two fine-tuned
LMs, one for selection and one for inference, to produce a valid reasoning
trace. Our method carries out a beam search through the space of reasoning
traces to improve reasoning quality. We demonstrate the effectiveness of our
model on multi-step logical deduction and scientific question-answering,
showing that it outperforms baselines on final answer accuracy, and generates
humanly interpretable reasoning traces whose validity can be checked by the
user.",0,0,1,0,0,0,0.980947,2.0,0.883471,36
30b86ed3-3e7e-48a0-af6b-def8c97cc096,CoDo: Contrastive Learning with Downstream Background Invariance for Detection,2,0.0042496,0.0528331,"The prior self-supervised learning researches mainly select image-level
instance discrimination as pretext task. It achieves a fantastic classification
performance that is comparable to supervised learning methods. However, with
degraded transfer performance on downstream tasks such as object detection. To
bridge the performance gap, we propose a novel object-level self-supervised
learning method, called Contrastive learning with Downstream background
invariance (CoDo). The pretext task is converted to focus on instance location
modeling for various backgrounds, especially for downstream datasets. The
ability of background invariance is considered vital for object detection.
Firstly, a data augmentation strategy is proposed to paste the instances onto
background images, and then jitter the bounding box to involve background
information. Secondly, we implement architecture alignment between our
pretraining network and the mainstream detection pipelines. Thirdly,
hierarchical and multi views contrastive learning is designed to improve
performance of visual representation learning. Experiments on MSCOCO
demonstrate that the proposed CoDo with common backbones, ResNet50-FPN, yields
strong transfer learning results for object detection.",0,1,1,0,0,0,0.811743,6.0,0.817226,32
6cf91090-9e3a-4929-bc78-1ee0b53955d1,Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models,17,0.196402,0.188062,"Pre-trained multilingual language models show significant performance gains
for zero-shot cross-lingual model transfer on a wide range of natural language
understanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation,
pre-trained models are only fine-tuned on English data and tested on a variety
of target languages. In this paper, we do cross-lingual evaluation on various
NLU tasks (sentence classification, sequence labeling, question answering)
using prompt-tuning and compare it with fine-tuning. The results show that
prompt tuning achieves much better cross-lingual transfer than fine-tuning
across datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we
demonstrate through the analysis that prompt tuning can have better
cross-lingual transferability of representations on downstream tasks with
better aligned decision boundaries.",1,1,0,0,0,0,0.913875,6.0,0.881241,31
8e6a4e6a-b61c-476d-adc5-a7add9322671,Compositional Semantic Parsing with Large Language Models,70,0.500914,0.999867,"Humans can reason compositionally when presented with new tasks. Previous
research shows that appropriate prompting techniques enable large language
models (LLMs) to solve artificial compositional generalization tasks such as
SCAN. In this work, we identify additional challenges in more realistic
semantic parsing tasks with larger vocabulary and refine these prompting
techniques to address them. Our best method is based on least-to-most
prompting: it decomposes the problem using prompting-based syntactic parsing,
then uses this decomposition to select appropriate exemplars and to
sequentially generate the semantic parse. This method allows us to set a new
state of the art for CFQ while requiring only 1% of the training data used by
traditional approaches. Due to the general nature of our approach, we expect
similar efforts will lead to new results in other tasks and domains, especially
for knowledge-intensive applications.",0,0,0,0,1,0,0.744825,4.0,0.675577,62
a5f29a3e-9850-4fe1-b77e-70afd3214ebb,HumanGen: Generating Human Radiance Fields with Explicit Priors,22,0.424051,0.46181,"Recent years have witnessed the tremendous progress of 3D GANs for generating
view-consistent radiance fields with photo-realism. Yet, high-quality
generation of human radiance fields remains challenging, partially due to the
limited human-related priors adopted in existing methods. We present HumanGen,
a novel 3D human generation scheme with detailed geometry and
$\text{360}^{\circ}$ realistic free-view rendering. It explicitly marries the
3D human generation with various priors from the 2D generator and 3D
reconstructor of humans through the design of ""anchor image"". We introduce a
hybrid feature representation using the anchor image to bridge the latent space
of HumanGen with the existing 2D generator. We then adopt a pronged design to
disentangle the generation of geometry and appearance. With the aid of the
anchor image, we adapt a 3D reconstructor for fine-grained details synthesis
and propose a two-stage blending scheme to boost appearance generation.
Extensive experiments demonstrate our effectiveness for state-of-the-art 3D
human generation regarding geometry details, texture quality, and free-view
performance. Notably, HumanGen can also incorporate various off-the-shelf 2D
latent editing methods, seamlessly lifting them into 3D.",0,1,0,0,1,0,0.949695,4.0,0.87143,95
aa583726-0a99-4f3f-a144-a93e99a31f92,The Arabic Ontology -- An Arabic Wordnet with Ontologically Clean Content,31,0.657731,0.771269,"We present a formal Arabic wordnet built on the basis of a carefully designed
ontology hereby referred to as the Arabic Ontology. The ontology provides a
formal representation of the concepts that the Arabic terms convey, and its
content was built with ontological analysis in mind, and benchmarked to
scientific advances and rigorous knowledge sources as much as this is possible,
rather than to only speakers' beliefs as lexicons typically are. A
comprehensive evaluation was conducted thereby demonstrating that the current
version of the top-levels of the ontology can top the majority of the Arabic
meanings. The ontology consists currently of about 1,300 well-investigated
concepts in addition to 11,000 concepts that are partially validated. The
ontology is accessible and searchable through a lexicographic search engine
(https://ontology.birzeit.edu) that also includes about 150 Arabic-multilingual
lexicons, and which are being mapped and enriched using the ontology. The
ontology is fully mapped with Princeton WordNet, Wikidata, and other resources.",0,0,0,0,0,0,0.0239387,18.0,0.703911,86
68725a5c-42b8-4356-9957-7a690eef58d9,"Deep Insights of Learning based Micro Expression Recognition: A Perspective on Promises, Challenges and Research Needs",5,0.0707796,0.547467,"Micro expression recognition (MER) is a very challenging area of research due
to its intrinsic nature and fine-grained changes. In the literature, the
problem of MER has been solved through handcrafted/descriptor-based techniques.
However, in recent times, deep learning (DL) based techniques have been adopted
to gain higher performance for MER. Also, rich survey articles on MER are
available by summarizing the datasets, experimental settings, conventional and
deep learning methods. In contrast, these studies lack the ability to convey
the impact of network design paradigms and experimental setting strategies for
DL-based MER. Therefore, this paper aims to provide a deep insight into the
DL-based MER frameworks with a perspective on promises in network model
designing, experimental strategies, challenges, and research needs. Also, the
detailed categorization of available MER frameworks is presented in various
aspects of model design and technical characteristics. Moreover, an empirical
analysis of the experimental and validation protocols adopted by MER methods is
presented. The challenges mentioned earlier and network design strategies may
assist the affective computing research community in forging ahead in MER
research. Finally, we point out the future directions, research needs, and draw
our conclusions.",0,0,0,0,0,0,0.372609,8.0,0.703424,103
14782a33-aa1b-4fa6-9c5b-8375f1cc9c9c,Learning Implicit Templates for Point-Based Clothed Human Modeling,27,0.27289,0.986355,"We present FITE, a First-Implicit-Then-Explicit framework for modeling human
avatars in clothing. Our framework first learns implicit surface templates
representing the coarse clothing topology, and then employs the templates to
guide the generation of point sets which further capture pose-dependent
clothing deformations such as wrinkles. Our pipeline incorporates the merits of
both implicit and explicit representations, namely, the ability to handle
varying topology and the ability to efficiently capture fine details. We also
propose diffused skinning to facilitate template training especially for loose
clothing, and projection-based pose-encoding to extract pose information from
mesh templates without predefined UV map or connectivity. Our code is publicly
available at https://github.com/jsnln/fite.",1,1,0,0,0,0,0.768027,6.0,0.794962,68
04a9acd4-f7e5-44c5-9787-fe77f54e16df,3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization,9,0.118497,0.617571,"Although deep-learning based methods for monocular pedestrian detection have
made great progress, they are still vulnerable to heavy occlusions. Using
multi-view information fusion is a potential solution but has limited
applications, due to the lack of annotated training samples in existing
multi-view datasets, which increases the risk of overfitting. To address this
problem, a data augmentation method is proposed to randomly generate 3D
cylinder occlusions, on the ground plane, which are of the average size of
pedestrians and projected to multiple views, to relieve the impact of
overfitting in the training. Moreover, the feature map of each view is
projected to multiple parallel planes at different heights, by using
homographies, which allows the CNNs to fully utilize the features across the
height of each pedestrian to infer the locations of pedestrians on the ground
plane. The proposed 3DROM method has a greatly improved performance in
comparison with the state-of-the-art deep-learning based methods for multi-view
pedestrian detection.",0,1,0,0,1,0,0.0982838,14.0,0.722996,27
3188bbdb-064c-449b-b881-d021666dbfcc,A Knowledge Graph Embeddings based Approach for Author Name Disambiguation using Literals,10,0.100073,0.584179,"Scholarly data is growing continuously containing information about the
articles from a plethora of venues including conferences, journals, etc. Many
initiatives have been taken to make scholarly data available as Knowledge
Graphs (KGs). These efforts to standardize these data and make them accessible
have also led to many challenges such as exploration of scholarly articles,
ambiguous authors, etc. This study more specifically targets the problem of
Author Name Disambiguation (AND) on Scholarly KGs and presents a novel
framework, Literally Author Name Disambiguation (LAND), which utilizes
Knowledge Graph Embeddings (KGEs) using multimodal literal information
generated from these KGs. This framework is based on three components: 1)
Multimodal KGEs, 2) A blocking procedure, and finally, 3) Hierarchical
Agglomerative Clustering. Extensive experiments have been conducted on two
newly created KGs: (i) KG containing information from Scientometrics Journal
from 1978 onwards (OC-782K), and (ii) a KG extracted from a well-known
benchmark for AND provided by AMiner (AMiner-534K). The results show that our
proposed architecture outperforms our baselines of 8-14% in terms of the F1
score and shows competitive performances on a challenging benchmark such as
AMiner. The code and the datasets are publicly available through Github:
https://github.com/sntcristian/and-kge and
Zenodo:https://doi.org/10.5281/zenodo.6309855 respectively.",1,1,0,1,0,0,0.151792,7.0,0.512357,60
87a86289-317c-46c8-9af9-e45e984e4cb7,Rethink about the Word-level Quality Estimation for Machine Translation from Human Judgement,2,0.0153843,0.331667,"Word-level Quality Estimation (QE) of Machine Translation (MT) aims to find
out potential translation errors in the translated sentence without reference.
Typically, conventional works on word-level QE are designed to predict the
translation quality in terms of the post-editing effort, where the word labels
(""OK"" and ""BAD"") are automatically generated by comparing words between MT
sentences and the post-edited sentences through a Translation Error Rate (TER)
toolkit. While the post-editing effort can be used to measure the translation
quality to some extent, we find it usually conflicts with the human judgement
on whether the word is well or poorly translated. To overcome the limitation,
we first create a golden benchmark dataset, namely \emph{HJQE} (Human Judgement
on Quality Estimation), where the expert translators directly annotate the
poorly translated words on their judgements. Additionally, to further make use
of the parallel corpus, we propose the self-supervised pre-training with two
tag correcting strategies, namely tag refinement strategy and tree-based
annotation strategy, to make the TER-based artificial QE corpus closer to
\emph{HJQE}. We conduct substantial experiments based on the publicly available
WMT En-De and En-Zh corpora. The results not only show our proposed dataset is
more consistent with human judgment but also confirm the effectiveness of the
proposed tag correcting strategies.\footnote{The data can be found at
\url{https://github.com/ZhenYangIACAS/HJQE}.}",1,1,1,1,0,0,0.368276,5.0,0.522503,32
73913274-54d6-49ab-a5fe-ca43247ad24f,Path-Aware Graph Attention for HD Maps in Motion Prediction,18,0.12307,0.786419,"The success of motion prediction for autonomous driving relies on integration
of information from the HD maps. As maps are naturally graph-structured,
investigation on graph neural networks (GNNs) for encoding HD maps is
burgeoning in recent years. However, unlike many other applications where GNNs
have been straightforwardly deployed, HD maps are heterogeneous graphs where
vertices (lanes) are connected by edges (lane-lane interaction relationships)
of various nature, and most graph-based models are not designed to understand
the variety of edge types which provide crucial cues for predicting how the
agents would travel the lanes. To overcome this challenge, we propose
Path-Aware Graph Attention, a novel attention architecture that infers the
attention between two vertices by parsing the sequence of edges forming the
paths that connect them. Our analysis illustrates how the proposed attention
mechanism can facilitate learning in a didactic problem where existing graph
networks like GCN struggle. By improving map encoding, the proposed model
surpasses previous state of the art on the Argoverse Motion Forecasting
dataset, and won the first place in the 2021 Argoverse Motion Forecasting
Competition.",0,0,0,0,1,0,0.90415,7.0,0.891834,30
c204860d-870d-4c05-8834-68a4dfc96ba6,Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances,14,0.0593384,0.676772,"With the advent of Deep Learning (DL), Super-Resolution (SR) has also become
a thriving research area. However, despite promising results, the field still
faces challenges that require further research e.g., allowing flexible
upsampling, more effective loss functions, and better evaluation metrics. We
review the domain of SR in light of recent advances, and examine
state-of-the-art models such as diffusion (DDPM) and transformer-based SR
models. We present a critical discussion on contemporary strategies used in SR,
and identify promising yet unexplored research directions. We complement
previous surveys by incorporating the latest developments in the field such as
uncertainty-driven losses, wavelet networks, neural architecture search, novel
normalization methods, and the latests evaluation techniques. We also include
several visualizations for the models and methods throughout each chapter in
order to facilitate a global understanding of the trends in the field. This
review is ultimately aimed at helping researchers to push the boundaries of DL
applied to SR.",0,0,0,0,0,0,0.184035,10.0,0.67979,191
076a86d6-ab8b-4e2a-a1c7-ccd282b8da4e,InducT-GCN: Inductive Graph Convolutional Networks for Text Classification,21,0.0747288,0.73218,"Text classification aims to assign labels to textual units by making use of
global information. Recent studies have applied graph neural network (GNN) to
capture the global word co-occurrence in a corpus. Existing approaches require
that all the nodes (training and test) in a graph are present during training,
which are transductive and do not naturally generalise to unseen nodes. To make
those models inductive, they use extra resources, like pretrained word
embedding. However, high-quality resource is not always available and hard to
train. Under the extreme settings with no extra resource and limited amount of
training set, can we still learn an inductive graph-based text classification
model? In this paper, we introduce a novel inductive graph-based text
classification framework, InducT-GCN (InducTive Graph Convolutional Networks
for Text classification). Compared to transductive models that require test
documents in training, we construct a graph based on the statistics of training
documents only and represent document vectors with a weighted sum of word
vectors. We then conduct one-directional GCN propagation during testing. Across
five text classification benchmarks, our InducT-GCN outperformed
state-of-the-art methods that are either transductive in nature or pre-trained
additional resources. We also conducted scalability testing by gradually
increasing the data size and revealed that our InducT-GCN can reduce the time
and space complexity. The code is available on:
https://github.com/usydnlp/InductTGCN.",1,1,0,0,1,0,0.304419,8.0,0.672154,36
75bf75e9-cb41-4204-b9bd-1f0cd00aa12a,Neural Networks Based on Power Method and Inverse Power Method for Solving Linear Eigenvalue Problems,6,0.0999352,0.240277,"In this article, we propose two kinds of neural networks inspired by power
method and inverse power method to solve linear eigenvalue problems. These
neural networks share similar ideas with traditional methods, in which the
differential operator is realized by automatic differentiation. The
eigenfunction of the eigenvalue problem is learned by the neural network and
the iterative algorithms are implemented by optimizing the specially defined
loss function. The largest positive eigenvalue, smallest eigenvalue and
interior eigenvalues with the given prior knowledge can be solved efficiently.
We examine the applicability and accuracy of our methods in the numerical
experiments in one dimension, two dimensions and higher dimensions. Numerical
results show that accurate eigenvalue and eigenfunction approximations can be
obtained by our methods.",1,1,0,0,0,0,0.169268,8.0,0.588197,41
172b9813-f8b6-46d4-be28-8be9373548dc,Real-time Detection of 2D Tool Landmarks with Synthetic Training Data,3,0.0325234,0.329588,"In this paper a deep learning architecture is presented that can, in real
time, detect the 2D locations of certain landmarks of physical tools, such as a
hammer or screwdriver. To avoid the labor of manual labeling, the network is
trained on synthetically generated data. Training computer vision models on
computer generated images, while still achieving good accuracy on real images,
is a challenge due to the difference in domain. The proposed method uses an
advanced rendering method in combination with transfer learning and an
intermediate supervision architecture to address this problem. It is shown that
the model presented in this paper, named Intermediate Heatmap Model (IHM),
generalizes to real images when trained on synthetic data. To avoid the need
for an exact textured 3D model of the tool in question, it is shown that the
model will generalize to an unseen tool when trained on a set of different 3D
models of the same type of tool. IHM is compared to two existing approaches to
keypoint detection and it is shown that it outperforms those at detecting tool
landmarks, trained on synthetic data.",1,1,0,1,0,0,0.9596,14.0,0.968321,26
30202ac7-f79f-499f-beeb-4b210bce45a2,Logic-based Reward Shaping for Multi-Agent Reinforcement Learning,1,0.0271789,0.142259,"Reinforcement learning (RL) relies heavily on exploration to learn from its
environment and maximize observed rewards. Therefore, it is essential to design
a reward function that guarantees optimal learning from the received
experience. Previous work has combined automata and logic based reward shaping
with environment assumptions to provide an automatic mechanism to synthesize
the reward function based on the task. However, there is limited work on how to
expand logic-based reward shaping to Multi-Agent Reinforcement Learning (MARL).
The environment will need to consider the joint state in order to keep track of
other agents if the task requires cooperation, thus suffering from the curse of
dimensionality with respect to the number of agents. This project explores how
logic-based reward shaping for MARL can be designed for different scenarios and
tasks. We present a novel method for semi-centralized logic-based MARL reward
shaping that is scalable in the number of agents and evaluate it in multiple
scenarios.",1,1,0,0,0,0,0.910107,8.0,0.908729,28
d987055d-9fe1-44a7-93a1-79abc43ef8bc,Overview of The MediaEval 2022 Predicting Video Memorability Task,16,0.10934,0.406176,"This paper describes the 5th edition of the Predicting Video Memorability
Task as part of MediaEval2022. This year we have reorganised and simplified the
task in order to lubricate a greater depth of inquiry. Similar to last year,
two datasets are provided in order to facilitate generalisation, however, this
year we have replaced the TRECVid2019 Video-to-Text dataset with the VideoMem
dataset in order to remedy underlying data quality issues, and to prioritise
short-term memorability prediction by elevating the Memento10k dataset as the
primary dataset. Additionally, a fully fledged electroencephalography
(EEG)-based prediction sub-task is introduced. In this paper, we outline the
core facets of the task and its constituent sub-tasks; describing the datasets,
evaluation metrics, and requirements for participant submissions.",0,1,0,0,0,0,0.0668323,9.0,0.524375,19
49a8400a-a4a2-4e8c-ab98-2734ade3613c,Learning to Reuse Distractors to support Multiple Choice Question Generation in Education,8,0.0394101,0.48607,"Multiple choice questions (MCQs) are widely used in digital learning systems,
as they allow for automating the assessment process. However, due to the
increased digital literacy of students and the advent of social media
platforms, MCQ tests are widely shared online, and teachers are continuously
challenged to create new questions, which is an expensive and time-consuming
task. A particularly sensitive aspect of MCQ creation is to devise relevant
distractors, i.e., wrong answers that are not easily identifiable as being
wrong. This paper studies how a large existing set of manually created answers
and distractors for questions over a variety of domains, subjects, and
languages can be leveraged to help teachers in creating new MCQs, by the smart
reuse of existing distractors. We built several data-driven models based on
context-aware question and distractor representations, and compared them with
static feature-based models. The proposed models are evaluated with automated
metrics and in a realistic user test with teachers. Both automatic and human
evaluations indicate that context-aware models consistently outperform a static
feature-based approach. For our best-performing context-aware model, on average
3 distractors out of the 10 shown to teachers were rated as high-quality
distractors. We create a performance benchmark, and make it public, to enable
comparison between different approaches and to introduce a more standardized
evaluation of the task. The benchmark contains a test of 298 educational
questions covering multiple subjects & languages and a 77k multilingual pool of
distractor vocabulary for future research.",0,1,0,0,0,0,0.00674141,14.0,0.528175,87
034d9d61-e6fc-46ea-acac-49779bb315f2,SRFeat: Learning Locally Accurate and Globally Consistent Non-Rigid Shape Correspondence,7,0.0887343,0.591346,"In this work, we present a novel learning-based framework that combines the
local accuracy of contrastive learning with the global consistency of geometric
approaches, for robust non-rigid matching. We first observe that while
contrastive learning can lead to powerful point-wise features, the learned
correspondences commonly lack smoothness and consistency, owing to the purely
combinatorial nature of the standard contrastive losses. To overcome this
limitation we propose to boost contrastive feature learning with two types of
smoothness regularization that inject geometric information into correspondence
learning. With this novel combination in hand, the resulting features are both
highly discriminative across individual points, and, at the same time, lead to
robust and consistent correspondences, through simple proximity queries. Our
framework is general and is applicable to local feature learning in both the 3D
and 2D domains. We demonstrate the superiority of our approach through
extensive experiments on a wide range of challenging matching benchmarks,
including 3D non-rigid shape correspondence and 2D image keypoint matching.",0,0,0,0,0,0,0.778947,9.0,0.866916,103
538fd6c0-fb55-4845-97bd-0152a126e264,MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation,27,0.347167,0.657132,"Responding with multi-modal content has been recognized as an essential
capability for an intelligent conversational agent. In this paper, we introduce
the MMDialog dataset to better facilitate multi-modal conversation. MMDialog is
composed of a curated set of 1.08 million real-world dialogues with 1.53
million unique images across 4,184 topics. MMDialog has two main and unique
advantages. First, it is the largest multi-modal conversation dataset by the
number of dialogues by 88x. Second, it contains massive topics to generalize
the open-domain. To build engaging dialogue system with this dataset, we
propose and normalize two response producing tasks based on retrieval and
generative scenarios. In addition, we build two baselines for above tasks with
state-of-the-art techniques and report their experimental performance. We also
propose a novel evaluation metric MM-Relevance to measure the multi-modal
responses. Our dataset and scripts are available in
https://github.com/victorsungo/MMDialog.",1,0,1,1,0,0,0.563234,8.0,0.775281,70
792a80ac-d33c-46b1-beab-6989e6464413,Skeptical binary inferences in multi-label problems with sets of probabilities,1,0.00671346,0.10511,"In this paper, we consider the problem of making distributionally robust,
skeptical inferences for the multi-label problem, or more generally for Boolean
vectors. By distributionally robust, we mean that we consider a set of possible
probability distributions, and by skeptical we understand that we consider as
valid only those inferences that are true for every distribution within this
set. Such inferences will provide partial predictions whenever the considered
set is sufficiently big. We study in particular the Hamming loss case, a common
loss function in multi-label problems, showing how skeptical inferences can be
made in this setting. Our experimental results are organised in three sections;
(1) the first one indicates the gain computational obtained from our
theoretical results by using synthetical data sets, (2) the second one
indicates that our approaches produce relevant cautiousness on those
hard-to-predict instances where its precise counterpart fails, and (3) the last
one demonstrates experimentally how our approach copes with imperfect
information (generated by a downsampling procedure) better than the partial
abstention [31] and the rejection rules.",1,0,0,0,0,0,0.000717986,16.0,0.446991,33
5e7e08bf-5f7a-4786-b252-43a23049517c,Character-Aware Models Improve Visual Text Rendering,38,0.574272,0.746574,"Current image generation models struggle to reliably produce well-formed
visual text. In this paper, we investigate a key contributing factor: popular
text-to-image models lack character-level input features, making it much harder
to predict a word's visual makeup as a series of glyphs. To quantify this
effect, we conduct a series of experiments comparing character-aware vs.
character-blind text encoders. In the text-only domain, we find that
character-aware models provide large gains on a novel spelling task
(WikiSpell). Applying our learnings to the visual domain, we train a suite of
image generation models, and show that character-aware variants outperform
their character-blind counterparts across a range of novel text rendering tasks
(our DrawText benchmark). Our models set a much higher state-of-the-art on
visual spelling, with 30+ point accuracy gains over competitors on rare words,
despite training on far fewer examples.",0,1,0,0,1,0,0.921693,4.0,0.831381,182
83e7559f-931b-424a-88fc-120a537e039f,MoDi: Unconditional Motion Synthesis from Diverse Data,26,0.271104,0.383027,"The emergence of neural networks has revolutionized the field of motion
synthesis. Yet, learning to unconditionally synthesize motions from a given
distribution remains challenging, especially when the motions are highly
diverse. In this work, we present MoDi -- a generative model trained in an
unsupervised setting from an extremely diverse, unstructured and unlabeled
dataset. During inference, MoDi can synthesize high-quality, diverse motions.
Despite the lack of any structure in the dataset, our model yields a
well-behaved and highly structured latent space, which can be semantically
clustered, constituting a strong motion prior that facilitates various
applications including semantic editing and crowd simulation. In addition, we
present an encoder that inverts real motions into MoDi's natural motion
manifold, issuing solutions to various ill-posed challenges such as completion
from prefix and spatial editing. Our qualitative and quantitative experiments
achieve state-of-the-art results that outperform recent SOTA techniques. Code
and trained models are available at https://sigal-raab.github.io/MoDi.",1,1,0,0,1,0,0.6865,5.0,0.707789,75
dabcc7ac-3f13-4341-84b7-9d9c119d5915,MAESTRO: Matched Speech Text Representations through Modality Matching,74,0.513737,0.842436,"We present Maestro, a self-supervised training method to unify
representations learnt from speech and text modalities. Self-supervised
learning from speech signals aims to learn the latent structure inherent in the
signal, while self-supervised learning from text attempts to capture lexical
information. Learning aligned representations from unpaired speech and text
sequences is a challenging task. Previous work either implicitly enforced the
representations learnt from these two modalities to be aligned in the latent
space through multitasking and parameter sharing or explicitly through
conversion of modalities via speech synthesis. While the former suffers from
interference between the two modalities, the latter introduces additional
complexity. In this paper, we propose Maestro, a novel algorithm to learn
unified representations from both these modalities simultaneously that can
transfer to diverse downstream tasks such as Automated Speech Recognition (ASR)
and Speech Translation (ST). Maestro learns unified representations through
sequence alignment, duration prediction and matching embeddings in the learned
space through an aligned masked-language model loss. We establish a new
state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative
reduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)
and 21 languages to English multilingual ST on CoVoST 2 with an improvement of
2.8 BLEU averaged over 21 languages.",0,1,0,0,1,0,0.908565,3.0,0.754248,40
69574bb4-ce05-4822-a06d-02114c2a44e4,VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction,26,0.280571,0.804549,"The success of the Neural Radiance Fields (NeRF) in novel view synthesis has
inspired researchers to propose neural implicit scene reconstruction. However,
most existing neural implicit reconstruction methods optimize per-scene
parameters and therefore lack generalizability to new scenes. We introduce
VolRecon, a novel generalizable implicit reconstruction method with Signed Ray
Distance Function (SRDF). To reconstruct the scene with fine details and little
noise, VolRecon combines projection features aggregated from multi-view
features, and volume features interpolated from a coarse global feature volume.
Using a ray transformer, we compute SRDF values of sampled points on a ray and
then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by
about 30% in sparse view reconstruction and achieves comparable accuracy as
MVSNet in full view reconstruction. Furthermore, our approach exhibits good
generalization performance on the large-scale ETH3D benchmark.",1,1,0,0,1,0,0.85991,6.0,0.844382,68
b3b92e4f-c021-4387-84d0-4ffe87e1db94,Detecting Deepfake by Creating Spatio-Temporal Regularity Disruption,7,0.319476,0.502081,"Despite encouraging progress in deepfake detection, generalization to unseen
forgery types remains a significant challenge due to the limited forgery clues
explored during training. In contrast, we notice a common phenomenon in
deepfake: fake video creation inevitably disrupts the statistical regularity in
original videos. Inspired by this observation, we propose to boost the
generalization of deepfake detection by distinguishing the ""regularity
disruption"" that does not appear in real videos. Specifically, by carefully
examining the spatial and temporal properties, we propose to disrupt a real
video through a Pseudo-fake Generator and create a wide range of pseudo-fake
videos for training. Such practice allows us to achieve deepfake detection
without using fake videos and improves the generalization ability in a simple
and efficient manner. To jointly capture the spatial and temporal disruptions,
we propose a Spatio-Temporal Enhancement block to learn the regularity
disruption across space and time on our self-created videos. Through
comprehensive experiments, our method exhibits excellent performance on several
datasets.",1,1,0,1,0,0,0.968228,6.0,0.938116,86
8e6a0d5a-34bf-4e6e-8140-3f186b99890b,Rectifying homographies for stereo vision: analytical solution for minimal distortion,1,0.00659824,0.0863072,"Stereo rectification is the determination of two image transformations (or
homographies) that map corresponding points on the two images, projections of
the same point in the 3D space, onto the same horizontal line in the
transformed images. Rectification is used to simplify the subsequent stereo
correspondence problem and speeding up the matching process. Rectifying
transformations, in general, introduce perspective distortion on the obtained
images, which shall be minimised to improve the accuracy of the following
algorithm dealing with the stereo correspondence problem. The search for the
optimal transformations is usually carried out relying on numerical
optimisation. This work proposes a closed-form solution for the rectifying
homographies that minimise perspective distortion. The experimental comparison
confirms its capability to solve the convergence issues of the previous
formulation. Its Python implementation is provided.",1,1,0,0,0,0,1.88645e-05,24.0,0.479681,25
3ea83bc2-ba26-427f-ab6b-914ae05d2917,Class-Incremental Lifelong Learning in Multi-Label Classification,2,0.0236612,0.192851,"Existing class-incremental lifelong learning studies only the data is with
single-label, which limits its adaptation to multi-label data. This paper
studies Lifelong Multi-Label (LML) classification, which builds an online
class-incremental classifier in a sequential multi-label classification data
stream. Training on the data with Partial Labels in LML classification may
result in more serious Catastrophic Forgetting in old classes. To solve the
problem, the study proposes an Augmented Graph Convolutional Network (AGCN)
with a built Augmented Correlation Matrix (ACM) across sequential partial-label
tasks. The results of two benchmarks show that the method is effective for LML
classification and reducing forgetting.",1,1,1,0,1,0,0.697261,10.0,0.856861,15
b3a9f9db-b88c-4ab1-91aa-1c03f14b28f3,LISA: Learning Implicit Shape and Appearance of Hands,50,0.463362,0.727271,"This paper proposes a do-it-all neural model of human hands, named LISA. The
model can capture accurate hand shape and appearance, generalize to arbitrary
hand subjects, provide dense surface correspondences, be reconstructed from
images in the wild and easily animated. We train LISA by minimizing the shape
and appearance losses on a large set of multi-view RGB image sequences
annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand
local coordinate, our model predicts the color and the signed distance with
respect to each hand bone independently, and then combines the per-bone
predictions using predicted skinning weights. The shape, color and pose
representations are disentangled by design, allowing to estimate or animate
only selected parameters. We experimentally demonstrate that LISA can
accurately reconstruct a dynamic hand from monocular or multi-view sequences,
achieving a noticeably higher quality of reconstructed hand shapes compared to
baseline approaches. Project page:
https://www.iri.upc.edu/people/ecorona/lisa/.",0,1,1,0,0,0,0.936426,4.0,0.851044,74
ece30420-e35c-4c70-9480-367ff4ef4dfc,On the State of the Art in Authorship Attribution and Authorship Verification,12,0.35288,0.834999,"Despite decades of research on authorship attribution (AA) and authorship
verification (AV), inconsistent dataset splits/filtering and mismatched
evaluation methods make it difficult to assess the state of the art. In this
paper, we present a survey of the fields, resolve points of confusion,
introduce Valla that standardizes and benchmarks AA/AV datasets and metrics,
provide a large-scale empirical evaluation, and provide apples-to-apples
comparisons between existing methods. We evaluate eight promising methods on
fifteen datasets (including distribution-shifted challenge sets) and introduce
a new large-scale dataset based on texts archived by Project Gutenberg.
Surprisingly, we find that a traditional Ngram-based model performs best on 5
(of 7) AA tasks, achieving an average macro-accuracy of $76.50\%$ (compared to
$66.71\%$ for a BERT-based model). However, on the two AA datasets with the
greatest number of words per author, as well as on the AV datasets, BERT-based
models perform best. While AV methods are easily applied to AA, they are seldom
included as baselines in AA papers. We show that through the application of
hard-negative mining, AV methods are competitive alternatives to AA methods.
Valla and all experiment code can be found here:
https://github.com/JacobTyo/Valla",1,1,0,1,1,0,0.0489509,12.0,0.616551,74
026d322a-91f5-443b-adb7-2675c744f011,Minorities in networks and algorithms,3,0.0,0.26558,"In this chapter, we provide an overview of recent advances in data-driven and
theory-informed complex models of social networks and their potential in
understanding societal inequalities and marginalization. We focus on
inequalities arising from networks and network-based algorithms and how they
affect minorities. In particular, we examine how homophily and mixing biases
shape large and small social networks, influence perception of minorities, and
affect collaboration patterns. We also discuss dynamical processes on and of
networks and the formation of norms and health inequalities. Additionally, we
argue that network modeling is paramount for unveiling the effect of ranking
and social recommendation algorithms on the visibility of minorities. Finally,
we highlight the key challenges and future opportunities in this emerging
research topic.",0,0,0,0,0,0,0.0671389,14.0,0.69458,70
09b76c21-2fc8-4991-b2d5-7d7230e2948d,PreCogIIITH at HinglishEval : Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,2,0.0862105,0.0665525,"Code-Mixing is a phenomenon of mixing two or more languages in a speech event
and is prevalent in multilingual societies. Given the low-resource nature of
Code-Mixing, machine generation of code-mixed text is a prevalent approach for
data augmentation. However, evaluating the quality of such machine generated
code-mixed text is an open problem. In our submission to HinglishEval, a
shared-task collocated with INLG2022, we attempt to build models factors that
impact the quality of synthetically generated code-mix text by predicting
ratings for code-mix quality.",0,1,0,0,0,0,0.870754,6.0,0.851078,12
782c55ee-ea82-49ec-a9f1-2b9d445014ed,Custom Structure Preservation in Face Aging,13,0.136132,0.932335,"In this work, we propose a novel architecture for face age editing that can
produce structural modifications while maintaining relevant details present in
the original image. We disentangle the style and content of the input image and
propose a new decoder network that adopts a style-based strategy to combine the
style and content representations of the input image while conditioning the
output on the target age. We go beyond existing aging methods allowing users to
adjust the degree of structure preservation in the input image during
inference. To this purpose, we introduce a masking mechanism, the CUstom
Structure Preservation module, that distinguishes relevant regions in the input
image from those that should be discarded. CUSP requires no additional
supervision. Finally, our quantitative and qualitative analysis which include a
user study, show that our method outperforms prior art and demonstrates the
effectiveness of our strategy regarding image editing and adjustable structure
preservation. Code and pretrained models are available at
https://github.com/guillermogotre/CUSP.",1,1,0,0,1,0,0.654249,10.0,0.845075,48
8084f8ca-0e6b-40c4-a2ff-1a3cd81e656e,Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation,22,0.368838,0.420072,"The principal task in supervised neural machine translation (NMT) is to learn
to generate target sentences conditioned on the source inputs from a set of
parallel sentence pairs, and thus produce a model capable of generalizing to
unseen instances. However, it is commonly observed that the generalization
performance of the model is highly influenced by the amount of parallel data
used in training. Although data augmentation is widely used to enrich the
training data, conventional methods with discrete manipulations fail to
generate diverse and faithful training samples. In this paper, we present a
novel data augmentation paradigm termed Continuous Semantic Augmentation
(CsaNMT), which augments each training instance with an adjacency semantic
region that could cover adequate variants of literal expression under the same
meaning. We conduct extensive experiments on both rich-resource and
low-resource settings involving various language pairs, including WMT14
English-{German,French}, NIST Chinese-English and multiple low-resource IWSLT
translation tasks. The provided empirical evidences show that CsaNMT sets a new
level of performance among existing augmentation techniques, improving on the
state-of-the-art by a large margin. The core codes are contained in Appendix E.",1,0,0,0,1,1,0.879306,9.0,0.904376,74
a69e8774-14e6-4ada-8eb3-c08e34d68c17,A Linguistic Investigation of Machine Learning based Contradiction Detection Models: An Empirical Analysis and Future Perspectives,2,0.0171015,0.0620777,"We analyze two Natural Language Inference data sets with respect to their
linguistic features. The goal is to identify those syntactic and semantic
properties that are particularly hard to comprehend for a machine learning
model. To this end, we also investigate the differences between a
crowd-sourced, machine-translated data set (SNLI) and a collection of text
pairs from internet sources. Our main findings are, that the model has
difficulty recognizing the semantic importance of prepositions and verbs,
emphasizing the importance of linguistically aware pre-training tasks.
Furthermore, it often does not comprehend antonyms and homonyms, especially if
those are depending on the context. Incomplete sentences are another problem,
as well as longer paragraphs and rare words or phrases. The study shows that
automated language understanding requires a more informed approach, utilizing
as much external knowledge as possible throughout the training process.",0,0,0,0,0,1,0.119027,7.0,0.474979,13
a01bb471-a050-444b-b3cc-6ddc2c38c86b,SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition,10,0.503731,0.722004,"Few-shot Named Entity Recognition (NER) aims to identify named entities with
very little annotated data. Previous methods solve this problem based on
token-wise classification, which ignores the information of entity boundaries,
and inevitably the performance is affected by the massive non-entity tokens. To
this end, we propose a seminal span-based prototypical network (SpanProto) that
tackles few-shot NER via a two-stage approach, including span extraction and
mention classification. In the span extraction stage, we transform the
sequential tags into a global boundary matrix, enabling the model to focus on
the explicit boundary information. For mention classification, we leverage
prototypical learning to capture the semantic representations for each labeled
span and make the model better adapt to novel-class entities. To further
improve the model performance, we split out the false positives generated by
the span extractor but not labeled in the current episode set, and then present
a margin-based loss to separate them from each prototype region. Experiments
over multiple benchmarks demonstrate that our model outperforms strong
baselines by a large margin.",0,1,0,0,1,0,0.964439,5.0,0.919097,45
41f45234-0aa4-49f4-9dab-768a999330cd,DocBed: A Multi-Stage OCR Solution for Documents with Complex Layouts,9,0.232796,0.716781,"Digitization of newspapers is of interest for many reasons including
preservation of history, accessibility and search ability, etc. While
digitization of documents such as scientific articles and magazines is
prevalent in literature, one of the main challenges for digitization of
newspaper lies in its complex layout (e.g. articles spanning multiple columns,
text interrupted by images) analysis, which is necessary to preserve human
read-order. This work provides a major breakthrough in the digitization of
newspapers on three fronts: first, releasing a dataset of 3000 fully-annotated,
real-world newspaper images from 21 different U.S. states representing an
extensive variety of complex layouts for document layout analysis; second,
proposing layout segmentation as a precursor to existing optical character
recognition (OCR) engines, where multiple state-of-the-art image segmentation
models and several post-processing methods are explored for document layout
segmentation; third, providing a thorough and structured evaluation protocol
for isolated layout segmentation and end-to-end OCR.",0,1,1,1,0,0,0.584329,9.0,0.806699,20
6928c6a7-4dc9-44ce-bff9-4cfd0616f32d,Goal Recognition as Reinforcement Learning,11,0.0378403,0.355466,"Most approaches for goal recognition rely on specifications of the possible
dynamics of the actor in the environment when pursuing a goal. These
specifications suffer from two key issues. First, encoding these dynamics
requires careful design by a domain expert, which is often not robust to noise
at recognition time. Second, existing approaches often need costly real-time
computations to reason about the likelihood of each potential goal. In this
paper, we develop a framework that combines model-free reinforcement learning
and goal recognition to alleviate the need for careful, manual domain design,
and the need for costly online executions. This framework consists of two main
stages: Offline learning of policies or utility functions for each potential
goal, and online inference. We provide a first instance of this framework using
tabular Q-learning for the learning stage, as well as three measures that can
be used to perform the inference stage. The resulting instantiation achieves
state-of-the-art performance against goal recognizers on standard evaluation
domains and superior performance in noisy environments.",0,0,0,0,1,0,0.0051361,10.0,0.312167,37
d8e814f6-6f31-4bae-836a-9497d2428981,Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal Point Processes,15,0.157912,0.388694,"We consider a sequential decision making problem where the agent faces the
environment characterized by the stochastic discrete events and seeks an
optimal intervention policy such that its long-term reward is maximized. This
problem exists ubiquitously in social media, finance and health informatics but
is rarely investigated by the conventional research in reinforcement learning.
To this end, we present a novel framework of the model-based reinforcement
learning where the agent's actions and observations are asynchronous stochastic
discrete events occurring in continuous-time. We model the dynamics of the
environment by Hawkes process with external intervention control term and
develop an algorithm to embed such process in the Bellman equation which guides
the direction of the value gradient. We demonstrate the superiority of our
method in both synthetic simulator and real-world problem.",1,0,1,0,0,0,0.619739,10.0,0.83569,56
551e4fda-4d5f-43f1-875a-2e67e8d6d253,Hand-Object Interaction Reasoning,8,0.121086,0.210811,"This paper proposes an interaction reasoning network for modelling
spatio-temporal relationships between hands and objects in video. The proposed
interaction unit utilises a Transformer module to reason about each acting
hand, and its spatio-temporal relation to the other hand as well as objects
being interacted with. We show that modelling two-handed interactions are
critical for action recognition in egocentric video, and demonstrate that by
using positionally-encoded trajectories, the network can better recognise
observed interactions. We evaluate our proposal on EPIC-KITCHENS and
Something-Else datasets, with an ablation study.",1,1,0,0,0,0,0.970359,8.0,0.956078,45
d4e6ff64-f750-4349-b86b-94ab36b6cae9,RNNCTPs: A Neural Symbolic Reasoning Method Using Dynamic Knowledge Partitioning Technology,2,0.0439575,0.117683,"Although traditional symbolic reasoning methods are highly interpretable,
their application in knowledge graph link prediction is limited due to their
low computational efficiency. In this paper, we propose a new neural symbolic
reasoning method: RNNCTPs, which improves computational efficiency by
re-filtering the knowledge selection of Conditional Theorem Provers (CTPs), and
is less sensitive to the embedding size parameter. RNNCTPs are divided into
relation selectors and predictors. The relation selectors are trained
efficiently and interpretably, so that the whole model can dynamically generate
knowledge for the inference of the predictor. In all four datasets, the method
shows competitive performance against traditional methods on the link
prediction task, and can have higher applicability to the selection of datasets
relative to CTPs.",0,0,0,0,0,0,0.616507,11.0,0.849828,32
036cb429-955d-4793-96fe-4458909ae53f,Fruit Quality Assessment with Densely Connected Convolutional Neural Network,8,0.2702,0.649101,"Accurate recognition of food items along with quality assessment is of
paramount importance in the agricultural industry. Such automated systems can
speed up the wheel of the food processing sector and save tons of manual labor.
In this connection, the recent advancement of Deep learning-based architectures
has introduced a wide variety of solutions offering remarkable performance in
several classification tasks. In this work, we have exploited the concept of
Densely Connected Convolutional Neural Networks (DenseNets) for fruit quality
assessment. The feature propagation towards the deeper layers has enabled the
network to tackle the vanishing gradient problems and ensured the reuse of
features to learn meaningful insights. Evaluating on a dataset of 19,526 images
containing six fruits having three quality grades for each, the proposed
pipeline achieved a remarkable accuracy of 99.67%. The robustness of the model
was further tested for fruit classification and quality assessment tasks where
the model produced a similar performance, which makes it suitable for real-life
applications.",0,1,0,0,0,0,0.81659,3.0,0.639618,17
793e9535-7750-4e48-8bc6-4f39f989112d,Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective,7,0.11906,0.303055,"Unsupervised video domain adaptation is a practical yet challenging task. In
this work, for the first time, we tackle it from a disentanglement view. Our
key idea is to handle the spatial and temporal domain divergence separately
through disentanglement. Specifically, we consider the generation of
cross-domain videos from two sets of latent factors, one encoding the static
information and another encoding the dynamic information. A Transfer Sequential
VAE (TranSVAE) framework is then developed to model such generation. To better
serve for adaptation, we propose several objectives to constrain the latent
factors. With these constraints, the spatial divergence can be readily removed
by disentangling the static domain-specific information out, and the temporal
divergence is further reduced from both frame- and video-levels through
adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and
Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE
compared with several state-of-the-art approaches. Code is publicly available.",1,1,0,0,1,0,0.540693,6.0,0.68993,54
1509a384-f204-4993-b0e4-9d4c28ba9953,Acceptability Judgements via Examining the Topology of Attention Maps,11,0.21277,0.777936,"The role of the attention mechanism in encoding linguistic knowledge has
received special interest in NLP. However, the ability of the attention heads
to judge the grammatical acceptability of a sentence has been underexplored.
This paper approaches the paradigm of acceptability judgments with topological
data analysis (TDA), showing that the geometric properties of the attention
graph can be efficiently exploited for two standard practices in linguistics:
binary judgments and linguistic minimal pairs. Topological features enhance the
BERT-based acceptability classifier scores by $8$%-$24$% on CoLA in three
languages (English, Italian, and Swedish). By revealing the topological
discrepancy between attention maps of minimal pairs, we achieve the human-level
performance on the BLiMP benchmark, outperforming nine statistical and
Transformer LM baselines. At the same time, TDA provides the foundation for
analyzing the linguistic functions of attention heads and interpreting the
correspondence between the graph features and grammatical phenomena.",0,0,0,0,0,0,0.626401,6.0,0.729169,88
96369a8f-98f2-4ae7-b878-0322c28dccf8,Semi-supervised New Event Type Induction and Description via Contrastive Loss-Enforced Batch Attention,7,0.0496853,0.575568,"Most event extraction methods have traditionally relied on an annotated set
of event types. However, creating event ontologies and annotating supervised
training data are expensive and time-consuming. Previous work has proposed
semi-supervised approaches which leverage seen (annotated) types to learn how
to automatically discover new event types. State-of-the-art methods, both
semi-supervised or fully unsupervised, use a form of reconstruction loss on
specific tokens in a context. In contrast, we present a novel approach to
semi-supervised new event type induction using a masked contrastive loss, which
learns similarities between event mentions by enforcing an attention mechanism
over the data minibatch. We further disentangle the discovered clusters by
approximating the underlying manifolds in the data, which allows us to increase
normalized mutual information and Fowlkes-Mallows scores by over 20% absolute.
Building on these clustering results, we extend our approach to two new tasks:
predicting the type name of the discovered clusters and linking them to
FrameNet frames.",0,0,0,0,0,0,0.668009,6.0,0.748046,66
2223f327-4fb6-4704-9139-c6b8c7eaa69b,Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks,31,0.689854,0.290001,"Despite the exciting performance, Transformer is criticized for its excessive
parameters and computation cost. However, compressing Transformer remains as an
open problem due to its internal complexity of the layer designs, i.e.,
Multi-Head Attention (MHA) and Feed-Forward Network (FFN). To address this
issue, we introduce Group-wise Transformation towards a universal yet
lightweight Transformer for vision-and-language tasks, termed as
LW-Transformer. LW-Transformer applies Group-wise Transformation to reduce both
the parameters and computations of Transformer, while also preserving its two
main properties, i.e., the efficient attention modeling on diverse subspaces of
MHA, and the expanding-scaling feature transformation of FFN. We apply
LW-Transformer to a set of Transformer-based networks, and quantitatively
measure them on three vision-and-language tasks and six benchmark datasets.
Experimental results show that while saving a large number of parameters and
computations, LW-Transformer achieves very competitive performance against the
original Transformer networks for vision-and-language tasks. To examine the
generalization ability, we also apply our optimization strategy to a recently
proposed image Transformer called Swin-Transformer for image classification,
where the effectiveness can be also confirmed",0,1,0,0,0,0,0.982724,8.0,0.973921,91
e275bc30-ba16-4951-9727-7f1621fa5a5a,A Cooperation Graph Approach for Multiagent Sparse Reward Reinforcement Learning,1,0.0564401,0.237448,"Multiagent reinforcement learning (MARL) can solve complex cooperative tasks.
However, the efficiency of existing MARL methods relies heavily on well-defined
reward functions. Multiagent tasks with sparse reward feedback are especially
challenging not only because of the credit distribution problem, but also due
to the low probability of obtaining positive reward feedback. In this paper, we
design a graph network called Cooperation Graph (CG). The Cooperation Graph is
the combination of two simple bipartite graphs, namely, the Agent Clustering
subgraph (ACG) and the Cluster Designating subgraph (CDG). Next, based on this
novel graph structure, we propose a Cooperation Graph Multiagent Reinforcement
Learning (CG-MARL) algorithm, which can efficiently deal with the sparse reward
problem in multiagent tasks. In CG-MARL, agents are directly controlled by the
Cooperation Graph. And a policy neural network is trained to manipulate this
Cooperation Graph, guiding agents to achieve cooperation in an implicit way.
This hierarchical feature of CG-MARL provides space for customized
cluster-actions, an extensible interface for introducing fundamental
cooperation knowledge. In experiments, CG-MARL shows state-of-the-art
performance in sparse reward multiagent benchmarks, including the anti-invasion
interception task and the multi-cargo delivery task.",1,1,0,0,1,0,0.813892,11.0,0.900928,21
9a5051f5-2377-4e3b-8fae-46cf19420745,Blueprint Separable Residual Network for Efficient Image Super-Resolution,65,0.233567,0.953037,"Recent advances in single image super-resolution (SISR) have achieved
extraordinary performance, but the computational cost is too heavy to apply in
edge devices. To alleviate this problem, many novel and effective solutions
have been proposed. Convolutional neural network (CNN) with the attention
mechanism has attracted increasing attention due to its efficiency and
effectiveness. However, there is still redundancy in the convolution operation.
In this paper, we propose Blueprint Separable Residual Network (BSRN)
containing two efficient designs. One is the usage of blueprint separable
convolution (BSConv), which takes place of the redundant convolution operation.
The other is to enhance the model ability by introducing more effective
attention modules. The experimental results show that BSRN achieves
state-of-the-art performance among existing efficient SR methods. Moreover, a
smaller variant of our model BSRN-S won the first place in model complexity
track of NTIRE 2022 Efficient SR Challenge. The code is available at
https://github.com/xiaom233/BSRN.",1,1,0,0,1,0,0.54775,10.0,0.815928,58
8d95c39b-fa9c-42b3-95d2-bb0d28c23393,Towards High-Fidelity Single-view Holistic Reconstruction of Indoor Scenes,17,0.252634,0.783021,"We present a new framework to reconstruct holistic 3D indoor scenes including
both room background and indoor objects from single-view images. Existing
methods can only produce 3D shapes of indoor objects with limited geometry
quality because of the heavy occlusion of indoor scenes. To solve this, we
propose an instance-aligned implicit function (InstPIFu) for detailed object
reconstruction. Combining with instance-aligned attention module, our method is
empowered to decouple mixed local features toward the occluded instances.
Additionally, unlike previous methods that simply represents the room
background as a 3D bounding box, depth map or a set of planes, we recover the
fine geometry of the background via implicit representation. Extensive
experiments on the SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT datasets
demonstrate that our method outperforms existing approaches in both background
and foreground object reconstruction. Our code and model will be made publicly
available.",0,1,0,0,1,0,0.747913,8.0,0.838898,58
92db8276-9862-4340-a992-ba728ee6b70f,Incorporating Commonsense Knowledge into Story Ending Generation via Heterogeneous Graph Networks,6,0.0998609,0.399504,"Story ending generation is an interesting and challenging task, which aims to
generate a coherent and reasonable ending given a story context. The key
challenges of the task lie in how to comprehend the story context sufficiently
and handle the implicit knowledge behind story clues effectively, which are
still under-explored by previous work. In this paper, we propose a Story
Heterogeneous Graph Network (SHGN) to explicitly model both the information of
story context at different granularity levels and the multi-grained interactive
relations among them. In detail, we consider commonsense knowledge, words and
sentences as three types of nodes. To aggregate non-local information, a global
node is also introduced. Given this heterogeneous graph network, the node
representations are updated through graph propagation, which adequately
utilizes commonsense knowledge to facilitate story comprehension. Moreover, we
design two auxiliary tasks to implicitly capture the sentiment trend and key
events lie in the context. The auxiliary tasks are jointly optimized with the
primary story ending generation task in a multi-task learning strategy.
Extensive experiments on the ROCStories Corpus show that the developed model
achieves new state-of-the-art performances. Human study further demonstrates
that our model generates more reasonable story endings.",1,0,0,0,1,0,0.674848,7.0,0.786711,36
83265bf0-692b-4a09-b2fa-0355d1bd9fd5,Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,40,0.241588,0.820799,"Language models increasingly rely on massive web dumps for diverse text data.
However, these sources are rife with undesirable content. As such, resources
like Wikipedia, books, and newswire often serve as anchors for automatically
selecting web text most suitable for language modeling, a process typically
referred to as quality filtering. Using a new dataset of U.S. high school
newspaper articles -- written by students from across the country -- we
investigate whose language is preferred by the quality filter used for GPT-3.
We find that newspapers from larger schools, located in wealthier, educated,
and urban ZIP codes are more likely to be classified as high quality. We then
demonstrate that the filter's measurement of quality is unaligned with other
sensible metrics, such as factuality or literary acclaim. We argue that
privileging any corpus as high quality entails a language ideology, and more
care is needed to construct training corpora for language models, with better
transparency and justification for the inclusion or exclusion of various texts.",0,0,0,1,0,0,0.230165,8.0,0.631193,97
5356df50-d714-4199-8fbb-22a352d42005,Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models,19,0.20847,0.739619,"Self-supervised learning (SSL) is seen as a very promising approach with high
performance for several speech downstream tasks. Since the parameters of SSL
models are generally so large that training and inference require a lot of
memory and computational cost, it is desirable to produce compact SSL models
without a significant performance degradation by applying compression methods
such as knowledge distillation (KD). Although the KD approach is able to shrink
the depth and/or width of SSL model structures, there has been little research
on how varying the depth and width impacts the internal representation of the
small-footprint model. This paper provides an empirical study that addresses
the question. We investigate the performance on SUPERB while varying the
structure and KD methods so as to keep the number of parameters constant; this
allows us to analyze the contribution of the representation introduced by
varying the model architecture. Experiments demonstrate that a certain depth is
essential for solving content-oriented tasks (e.g. automatic speech
recognition) accurately, whereas a certain width is necessary for achieving
high performance on several speaker-oriented tasks (e.g. speaker
identification). Based on these observations, we identify, for SUPERB, a more
compressed model with better performance than previous studies.",0,1,0,0,0,0,0.720304,5.0,0.726553,41
ab0255e7-b25b-40f3-bed2-1d241ca2c607,CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation,7,0.166099,0.537378,"Empathetic conversation is psychologically supposed to be the result of
conscious alignment and interaction between the cognition and affection of
empathy. However, existing empathetic dialogue models usually consider only the
affective aspect or treat cognition and affection in isolation, which limits
the capability of empathetic response generation. In this work, we propose the
CASE model for empathetic dialogue generation. It first builds upon a
commonsense cognition graph and an emotional concept graph and then aligns the
user's cognition and affection at both the coarse-grained and fine-grained
levels. Through automatic and manual evaluation, we demonstrate that CASE
outperforms state-of-the-art baselines of empathetic dialogues and can generate
more empathetic and informative responses.",1,0,0,0,1,0,0.673558,6.0,0.750575,52
2095a4cf-fc43-49cd-be98-2f45472c0aca,DraftRec: Personalized Draft Recommendation for Winning in Multi-Player Online Battle Arena Games,5,0.397954,0.882195,"This paper presents a personalized character recommendation system for
Multiplayer Online Battle Arena (MOBA) games which are considered as one of the
most popular online video game genres around the world. When playing MOBA
games, players go through a draft stage, where they alternately select a
virtual character to play. When drafting, players select characters by not only
considering their character preferences, but also the synergy and competence of
their team's character combination. However, the complexity of drafting induces
difficulties for beginners to choose the appropriate characters based on the
characters of their team while considering their own champion preferences. To
alleviate this problem, we propose DraftRec, a novel hierarchical model which
recommends characters by considering each player's champion preferences and the
interaction between the players. DraftRec consists of two networks: the player
network and the match network. The player network captures the individual
player's champion preference, and the match network integrates the complex
relationship between the players and their respective champions. We train and
evaluate our model from a manually collected 280,000 matches of League of
Legends and a publicly available 50,000 matches of Dota2. Empirically, our
method achieved state-of-the-art performance in character recommendation and
match outcome prediction task. Furthermore, a comprehensive user survey
confirms that DraftRec provides convincing and satisfying recommendations. Our
code and dataset are available at https://github.com/dojeon-ai/DraftRec.",0,1,0,0,1,0,0.833759,8.0,0.871898,50
74fd7fa4-397c-456f-b18e-c8bfee8e002a,Pseudo Polynomial-Time Top-k Algorithms for d-DNNF Circuits,4,0.10154,0.500213,"We are interested in computing $k$ most preferred models of a given d-DNNF
circuit $C$, where the preference relation is based on an algebraic structure
called a monotone, totally ordered, semigroup $(K, \otimes, <)$. In our
setting, every literal in $C$ has a value in $K$ and the value of an assignment
is an element of $K$ obtained by aggregating using $\otimes$ the values of the
corresponding literals. We present an algorithm that computes $k$ models of $C$
among those having the largest values w.r.t. $<$, and show that this algorithm
runs in time polynomial in $k$ and in the size of $C$. We also present a pseudo
polynomial-time algorithm for deriving the top-$k$ values that can be reached,
provided that an additional (but not very demanding) requirement on the
semigroup is satisfied. Under the same assumption, we present a pseudo
polynomial-time algorithm that transforms $C$ into a d-DNNF circuit $C'$
satisfied exactly by the models of $C$ having a value among the top-$k$ ones.
Finally, focusing on the semigroup $(\mathbb{N}, +, <)$, we compare on a large
number of instances the performances of our compilation-based algorithm for
computing $k$ top solutions with those of an algorithm tackling the same
problem, but based on a partial weighted MaxSAT solver.",1,0,0,0,0,0,0.0366438,18.0,0.727925,24
7b651287-a98f-4047-aee9-88f3700ab6d1,Monitoring Diversity of AI Conferences: Lessons Learnt and Future Challenges in the DivinAI Project,1,0.0140746,0.0550487,"DivinAI is an open and collaborative initiative promoted by the European
Commission's Joint Research Centre to measure and monitor diversity indicators
related to AI conferences, with special focus on gender balance, geographical
representation, and presence of academia vs companies. This paper summarizes
the main achievements and lessons learnt during the first year of life of the
DivinAI project, and proposes a set of recommendations for its further
development and maintenance by the AI community.",0,1,0,0,0,0,0.547294,5.0,0.631603,18
02eebfcc-a120-4bdf-a9a2-db2cea7b121d,Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance,10,0.347993,0.369535,"We study the challenging problem of recovering detailed motion from a single
motion-blurred image. Existing solutions to this problem estimate a single
image sequence without considering the motion ambiguity for each region.
Therefore, the results tend to converge to the mean of the multi-modal
possibilities. In this paper, we explicitly account for such motion ambiguity,
allowing us to generate multiple plausible solutions all in sharp detail. The
key idea is to introduce a motion guidance representation, which is a compact
quantization of 2D optical flow with only four discrete motion directions.
Conditioned on the motion guidance, the blur decomposition is led to a
specific, unambiguous solution by using a novel two-stage decomposition
network. We propose a unified framework for blur decomposition, which supports
various interfaces for generating our motion guidance, including human input,
motion information from adjacent video frames, and learning from a video
dataset. Extensive experiments on synthesized datasets and real-world data show
that the proposed framework is qualitatively and quantitatively superior to
previous methods, and also offers the merit of producing physically plausible
and diverse solutions. Code is available at
https://github.com/zzh-tech/Animation-from-Blur.",1,1,0,0,1,0,0.954105,8.0,0.939494,50
0b1dc11a-655b-4410-bf4f-5f2e2565b1f8,Few-Shot Diffusion Models,39,0.263923,0.456698,"Denoising diffusion probabilistic models (DDPM) are powerful hierarchical
latent variable models with remarkable sample generation quality and training
stability. These properties can be attributed to parameter sharing in the
generative hierarchy, as well as a parameter-free diffusion-based inference
procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a
framework for few-shot generation leveraging conditional DDPMs. FSDMs are
trained to adapt the generative process conditioned on a small set of images
from a given class by aggregating image patch information using a set-based
Vision Transformer (ViT). At test time, the model is able to generate samples
from previously unseen classes conditioned on as few as 5 samples from that
class. We empirically show that FSDM can perform few-shot generation and
transfer to new datasets. We benchmark variants of our method on complex vision
datasets for few-shot learning and compare to unconditional and conditional
DDPM baselines. Additionally, we show how conditioning the model on patch-based
input set information improves training convergence.",0,0,0,0,0,0,0.90733,5.0,0.851424,111
039824fb-7a01-4ab0-a786-2dd880d4e9d8,Signature Entrenchment and Conceptual Changes in Automated Theory Repair,4,0.0437283,0.496419,"Human beliefs change, but so do the concepts that underpin them. The recent
Abduction, Belief Revision and Conceptual Change (ABC) repair system combines
several methods from automated theory repair to expand, contract, or reform
logical structures representing conceptual knowledge in artificial agents. In
this paper we focus on conceptual change: repair not only of the membership of
logical concepts, such as what animals can fly, but also concepts themselves,
such that birds may be divided into flightless and flying birds, by changing
the signature of the logical theory used to represent them. We offer a method
for automatically evaluating entrenchment in the signature of a Datalog theory,
in order to constrain automated theory repair to succinct and intuitive
outcomes. Formally, signature entrenchment measures the inferential
contributions of every logical language element used to express conceptual
knowledge, i.e., predicates and the arguments, ranking possible repairs to
retain valuable logical concepts and reject redundant or implausible
alternatives. This quantitative measurement of signature entrenchment offers a
guide to the plausibility of conceptual changes, which we aim to contrast with
human judgements of concept entrenchment in future work.",1,0,0,0,0,0,0.000892129,34.0,0.74615,26
bf7a31e8-17af-40d1-b41a-cb8a4faa83f9,Is Conditional Generative Modeling all you need for Decision-Making?,149,0.749103,0.988665,"Recent improvements in conditional generative modeling have made it possible
to generate high-quality images from language descriptions alone. We
investigate whether these methods can directly address the problem of
sequential decision-making. We view decision-making not through the lens of
reinforcement learning (RL), but rather through conditional generative
modeling. To our surprise, we find that our formulation leads to policies that
can outperform existing offline RL approaches across standard benchmarks. By
modeling a policy as a return-conditional diffusion model, we illustrate how we
may circumvent the need for dynamic programming and subsequently eliminate many
of the complexities that come with traditional offline RL. We further
demonstrate the advantages of modeling policies as conditional diffusion models
by considering two other conditioning variables: constraints and skills.
Conditioning on a single constraint or skill during training leads to behaviors
at test-time that can satisfy several constraints together or demonstrate a
composition of skills. Our results illustrate that conditional generative
modeling is a powerful tool for decision-making.",1,0,0,0,0,0,0.840248,5.0,0.799427,71
7e2c1a09-c50a-4b89-bf04-afc111a661f3,Effects of Spectral Normalization in Multi-agent Reinforcement Learning,5,0.0342854,0.321131,"A reliable critic is central to on-policy actor-critic learning. But it
becomes challenging to learn a reliable critic in a multi-agent sparse reward
scenario due to two factors: 1) The joint action space grows exponentially with
the number of agents 2) This, combined with the reward sparseness and
environment noise, leads to large sample requirements for accurate learning. We
show that regularising the critic with spectral normalization (SN) enables it
to learn more robustly, even in multi-agent on-policy sparse reward scenarios.
Our experiments show that the regularised critic is quickly able to learn from
the sparse rewarding experience in the complex SMAC and RWARE domains. These
findings highlight the importance of regularisation in the critic for stable
learning.",0,1,0,0,0,0,0.163262,8.0,0.583245,38
66dadb18-efd4-48e5-8de0-13d76478660b,How good are deep models in understanding the generated images?,3,0.0,0.105738,"My goal in this paper is twofold: to study how well deep models can
understand the images generated by DALL-E 2 and Midjourney, and to
quantitatively evaluate these generative models. Two sets of generated images
are collected for object recognition and visual question answering (VQA) tasks.
On object recognition, the best model, out of 10 state-of-the-art object
recognition models, achieves about 60\% and 80\% top-1 and top-5 accuracy,
respectively. These numbers are much lower than the best accuracy on the
ImageNet dataset (91\% and 99\%). On VQA, the OFA model scores 77.3\% on
answering 241 binary questions across 50 images. This model scores 94.7\% on
the binary VQA-v2 dataset. Humans are able to recognize the generated images
and answer questions on them easily. We conclude that a) deep models struggle
to understand the generated content, and may do better after fine-tuning, and
b) there is a large distribution shift between the generated images and the
real photographs. The distribution shift appears to be category-dependent. Data
is available at:
https://drive.google.com/file/d/1n2nCiaXtYJRRF2R73-LNE3zggeU_HeH0/view?usp=sharing.",0,1,0,0,0,0,0.90429,10.0,0.924346,20
a5d1c956-9b14-4399-8f65-b8c5ec128ffe,DFNet: Enhance Absolute Pose Regression with Direct Feature Matching,31,0.270464,0.807341,"We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. By incorporating
exposure-adaptive novel view synthesis, our method successfully addresses
photometric distortions in outdoor environments that existing photometric-based
methods fail to handle. With domain-invariant feature matching, our solution
improves pose regression accuracy using semi-supervised learning on unlabeled
data. In particular, the pipeline consists of two components: Novel View
Synthesizer and DFNet. The former synthesizes novel views compensating for
changes in exposure and the latter regresses camera poses and extracts robust
features that close the domain gap between real images and synthetic ones.
Furthermore, we introduce an online synthetic data generation scheme. We show
that these approaches effectively enhance camera pose estimation both in indoor
and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by
outperforming existing single-image APR methods by as much as 56%, comparable
to 3D structure-based methods.",1,1,0,0,1,0,0.539984,9.0,0.793066,37
42c749dc-07a0-47a5-9872-8782472d8e34,Semantic Segmentation using Neural Ordinary Differential Equations,9,0.0171892,0.284463,"The idea of neural Ordinary Differential Equations (ODE) is to approximate
the derivative of a function (data model) instead of the function itself. In
residual networks, instead of having a discrete sequence of hidden layers, the
derivative of the continuous dynamics of hidden state can be parameterized by
an ODE. It has been shown that this type of neural network is able to produce
the same results as an equivalent residual network for image classification. In
this paper, we design a novel neural ODE for the semantic segmentation task. We
start by a baseline network that consists of residual modules, then we use the
modules to build our neural ODE network. We show that our neural ODE is able to
achieve the state-of-the-art results using 57% less memory for training, 42%
less memory for testing, and 68% less number of parameters. We evaluate our
model on the Cityscapes, CamVid, LIP, and PASCAL-Context datasets.",0,1,0,0,1,0,0.112118,13.0,0.712402,47
4792756c-d2d0-4ef9-a8d6-5db46f4af4db,SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer,46,0.387045,0.894548,"Point cloud completion has become increasingly popular among generation tasks
of 3D point clouds, as it is a challenging yet indispensable problem to recover
the complete shape of a 3D object from its partial observation. In this paper,
we propose a novel SeedFormer to improve the ability of detail preservation and
recovery in point cloud completion. Unlike previous methods based on a global
feature vector, we introduce a new shape representation, namely Patch Seeds,
which not only captures general structures from partial inputs but also
preserves regional information of local patterns. Then, by integrating seed
features into the generation process, we can recover faithful details for
complete point clouds in a coarse-to-fine manner. Moreover, we devise an
Upsample Transformer by extending the transformer structure into basic
operations of point generators, which effectively incorporates spatial and
semantic relationships between neighboring points. Qualitative and quantitative
evaluations demonstrate that our method outperforms state-of-the-art completion
networks on several benchmark datasets. Our code is available at
https://github.com/hrzhou2/seedformer.",1,1,0,0,1,0,0.882878,8.0,0.894186,47
55b94f03-fdb0-4778-8f59-e63a7ba7bee4,TeleGraph: A Benchmark Dataset for Hierarchical Link Prediction,5,0.0422471,0.299722,"Link prediction is a key problem for network-structured data, attracting
considerable research efforts owing to its diverse applications. The current
link prediction methods focus on general networks and are overly dependent on
either the closed triangular structure of networks or node attributes. Their
performance on sparse or highly hierarchical networks has not been well
studied. On the other hand, the available tree-like benchmark datasets are
either simulated, with limited node information, or small in scale. To bridge
this gap, we present a new benchmark dataset TeleGraph, a highly sparse and
hierarchical telecommunication network associated with rich node attributes,
for assessing and fostering the link inference techniques. Our empirical
results suggest that most of the algorithms fail to produce a satisfactory
performance on a nearly tree-like dataset, which calls for special attention
when designing or deploying the link prediction algorithm in practice.",0,1,1,1,0,0,0.449944,8.0,0.734494,30
844e19c2-e347-4ec2-bd46-9f00c239759b,Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding,18,0.661322,0.69234,"Inspired by the impressive performance of recent face image editing methods,
several studies have been naturally proposed to extend these methods to the
face video editing task. One of the main challenges here is temporal
consistency among edited frames, which is still unresolved. To this end, we
propose a novel face video editing framework based on diffusion autoencoders
that can successfully extract the decomposed features - for the first time as a
face video editing model - of identity and motion from a given video. This
modeling allows us to edit the video by simply manipulating the temporally
invariant feature to the desired direction for the consistency. Another unique
strength of our model is that, since our model is based on diffusion models, it
can satisfy both reconstruction and edit capabilities at the same time, and is
robust to corner cases in wild face videos (e.g. occluded faces) unlike the
existing GAN-based methods.",1,1,0,0,0,0,0.987088,4.0,0.965162,45
53c38d59-08a8-4863-a718-7a072c97d83b,Automated Wheat Disease Detection using a ROS-based Autonomous Guided UAV,4,0.0713228,0.197499,"With the increase in world population, food resources have to be modified to
be more productive, resistive, and reliable. Wheat is one of the most important
food resources in the world, mainly because of the variety of wheat-based
products. Wheat crops are threatened by three main types of diseases which
cause large amounts of annual damage in crop yield. These diseases can be
eliminated by using pesticides at the right time. While the task of manually
spraying pesticides is burdensome and expensive, agricultural robotics can aid
farmers by increasing the speed and decreasing the amount of chemicals. In this
work, a smart autonomous system has been implemented on an unmanned aerial
vehicle to automate the task of monitoring wheat fields. First, an image-based
deep learning approach is used to detect and classify disease-infected wheat
plants. To find the most optimal method, different approaches have been
studied. Because of the lack of a public wheat-disease dataset, a custom
dataset has been created and labeled. Second, an efficient mapping and
navigation system is presented using a simulation in the robot operating system
and Gazebo environments. A 2D simultaneous localization and mapping algorithm
is used for mapping the workspace autonomously with the help of a
frontier-based exploration method.",1,1,0,1,0,0,0.585959,13.0,0.86652,38
e28fb8bc-5926-468e-9ce9-09d3e0c0fb9a,Converge to the Truth: Factual Error Correction via Iterative Constrained Editing,6,0.0278711,0.529544,"Given a possibly false claim sentence, how can we automatically correct it
with minimal editing? Existing methods either require a large number of pairs
of false and corrected claims for supervised training or do not handle well
errors spanning over multiple tokens within an utterance. In this paper, we
propose VENCE, a novel method for factual error correction (FEC) with minimal
edits. VENCE formulates the FEC problem as iterative sampling editing actions
with respect to a target density function. We carefully design the target
function with predicted truthfulness scores from an offline trained fact
verification model. VENCE samples the most probable editing positions based on
back-calculated gradients of the truthfulness score concerning input tokens and
the editing actions using a distantly-supervised language model (T5).
Experiments on a public dataset show that VENCE improves the well-adopted SARI
metric by 5.3 (or a relative improvement of 11.8%) over the previous best
distantly-supervised methods.",1,1,0,0,1,0,0.231821,6.0,0.509624,57
7382bcf5-7fee-483e-83b0-f46c15c20ce8,MMNet: Muscle motion-guided network for micro-expression recognition,14,0.297567,0.754336,"Facial micro-expressions (MEs) are involuntary facial motions revealing
peoples real feelings and play an important role in the early intervention of
mental illness, the national security, and many human-computer interaction
systems. However, existing micro-expression datasets are limited and usually
pose some challenges for training good classifiers. To model the subtle facial
muscle motions, we propose a robust micro-expression recognition (MER)
framework, namely muscle motion-guided network (MMNet). Specifically, a
continuous attention (CA) block is introduced to focus on modeling local subtle
muscle motion patterns with little identity information, which is different
from most previous methods that directly extract features from complete video
frames with much identity information. Besides, we design a position
calibration (PC) module based on the vision transformer. By adding the position
embeddings of the face generated by PC module at the end of the two branches,
the PC module can help to add position information to facial muscle motion
pattern features for the MER. Extensive experiments on three public
micro-expression datasets demonstrate that our approach outperforms
state-of-the-art methods by a large margin.",1,1,0,0,1,0,0.855722,5.0,0.810239,37
403f2f4d-434d-4ae1-a854-26b05ce4abb0,Thermodynamics-informed neural networks for physically realistic mixed reality,7,0.133225,0.712728,"The imminent impact of immersive technologies in society urges for active
research in real-time and interactive physics simulation for virtual worlds to
be realistic. In this context, realistic means to be compliant to the laws of
physics. In this paper we present a method for computing the dynamic response
of (possibly non-linear and dissipative) deformable objects induced by
real-time user interactions in mixed reality using deep learning. The
graph-based architecture of the method ensures the thermodynamic consistency of
the predictions, whereas the visualization pipeline allows a natural and
realistic user experience. Two examples of virtual solids interacting with
virtual or physical solids in mixed reality scenarios are provided to prove the
performance of the method.",0,1,0,0,0,0,0.255793,8.0,0.646408,50
9018e07e-aa42-4fb7-a303-18f854346c4a,Generalizing Multimodal Pre-training into Multilingual via Language Acquisition,3,0.0342904,0.0754042,"English-based Vision-Language Pre-training (VLP) has achieved great success
in various downstream tasks. Some efforts have been taken to generalize this
success to non-English languages through Multilingual Vision-Language
Pre-training (M-VLP). However, due to the large number of languages, M-VLP
models often require huge computing resources and cannot be flexibly extended
to new languages. In this work, we propose a \textbf{M}ulti\textbf{L}ingual
\textbf{A}cquisition (MLA) framework that can easily generalize a monolingual
Vision-Language Pre-training model into multilingual. Specifically, we design a
lightweight language acquisition encoder based on state-of-the-art monolingual
VLP models. We further propose a two-stage training strategy to optimize the
language acquisition encoder, namely the Native Language Transfer stage and the
Language Exposure stage. With much less multilingual training data and
computing resources, our model achieves state-of-the-art performance on
multilingual image-text and video-text retrieval benchmarks.",1,1,0,0,1,0,0.746887,6.0,0.784705,48
e686c0bf-398e-4b70-9731-f9182c1c3304,Improving Robustness to Out-of-Distribution Data by Frequency-based Augmentation,6,0.148122,0.314281,"Although Convolutional Neural Networks (CNNs) have high accuracy in image
recognition, they are vulnerable to adversarial examples and
out-of-distribution data, and the difference from human recognition has been
pointed out. In order to improve the robustness against out-of-distribution
data, we present a frequency-based data augmentation technique that replaces
the frequency components with other images of the same class. When the training
data are CIFAR10 and the out-of-distribution data are SVHN, the Area Under
Receiver Operating Characteristic (AUROC) curve of the model trained with the
proposed method increases from 89.22\% to 98.15\%, and further increased to
98.59\% when combined with another data augmentation method. Furthermore, we
experimentally demonstrate that the robust model for out-of-distribution data
uses a lot of high-frequency components of the image.",0,1,0,0,0,0,0.986788,11.0,0.986849,21
9c519ee7-efec-4a09-91c2-9677c289d5c0,PIE: a Parameter and Inference Efficient Solution for Large Scale Knowledge Graph Embedding Reasoning,6,0.0549701,0.464669,"Knowledge graph (KG) embedding methods which map entities and relations to
unique embeddings in the KG have shown promising results on many reasoning
tasks. However, the same embedding dimension for both dense entities and sparse
entities will cause either over parameterization (sparse entities) or under
fitting (dense entities). Normally, a large dimension is set to get better
performance. Meanwhile, the inference time grows log-linearly with the number
of entities for all entities are traversed and compared. Both the parameter and
inference become challenges when working with huge amounts of entities. Thus,
we propose PIE, a \textbf{p}arameter and \textbf{i}nference \textbf{e}fficient
solution. Inspired from tensor decomposition methods, we find that decompose
entity embedding matrix into low rank matrices can reduce more than half of the
parameters while maintaining comparable performance. To accelerate model
inference, we propose a self-supervised auxiliary task, which can be seen as
fine-grained entity typing. By randomly masking and recovering entities'
connected relations, the task learns the co-occurrence of entity and relations.
Utilizing the fine grained typing, we can filter unrelated entities during
inference and get targets with possibly sub-linear time requirement.
Experiments on link prediction benchmarks demonstrate the proposed key
capabilities. Moreover, we prove effectiveness of the proposed solution on the
Open Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the
state of the art performance.",0,1,0,0,1,0,0.636805,9.0,0.822588,49
34b708d4-083c-4156-b7d4-462d87e19b43,Are Vision Transformers Robust to Spurious Correlations?,15,0.264585,0.460456,"Deep neural networks may be susceptible to learning spurious correlations
that hold on average but not in atypical test samples. As with the recent
emergence of vision transformer (ViT) models, it remains underexplored how
spurious correlations are manifested in such architectures. In this paper, we
systematically investigate the robustness of vision transformers to spurious
correlations on three challenging benchmark datasets and compare their
performance with popular CNNs. Our study reveals that when pre-trained on a
sufficiently large dataset, ViT models are more robust to spurious correlations
than CNNs. Key to their success is the ability to generalize better from the
examples where spurious correlations do not hold. Further, we perform extensive
ablations and experiments to understand the role of the self-attention
mechanism in providing robustness under spuriously correlated environments. We
hope that our work will inspire future research on further understanding the
robustness of ViT models.",1,0,0,0,0,0,0.97609,4.0,0.926975,61
86fda6bc-bd9d-4dd8-ab09-492b9b53ee7c,Self-supervised Semantic Segmentation Grounded in Visual Concepts,6,0.0658299,0.189377,"Unsupervised semantic segmentation requires assigning a label to every pixel
without any human annotations. Despite recent advances in self-supervised
representation learning for individual images, unsupervised semantic
segmentation with pixel-level representations is still a challenging task and
remains underexplored. In this work, we propose a self-supervised pixel
representation learning method for semantic segmentation by using visual
concepts (i.e., groups of pixels with semantic meanings, such as parts,
objects, and scenes) extracted from images. To guide self-supervised learning,
we leverage three types of relationships between pixels and concepts, including
the relationships between pixels and local concepts, local and global concepts,
as well as the co-occurrence of concepts. We evaluate the learned pixel
embeddings and visual concepts on three datasets, including PASCAL VOC 2012,
COCO 2017, and DAVIS 2017. Our results show that the proposed method gains
consistent and substantial improvements over recent unsupervised semantic
segmentation approaches, and also demonstrate that visual concepts can reveal
insights into image datasets.",0,0,0,0,0,0,0.900845,9.0,0.914253,29
6f120c48-6105-4d46-87f6-3ba4f7d06dfa,An Interpretable Deep Semantic Segmentation Method for Earth Observation,6,0.0198942,0.229802,"Earth observation is fundamental for a range of human activities including
flood response as it offers vital information to decision makers. Semantic
segmentation plays a key role in mapping the raw hyper-spectral data coming
from the satellites into a human understandable form assigning class labels to
each pixel. In this paper, we introduce a prototype-based interpretable deep
semantic segmentation (IDSS) method, which is highly accurate as well as
interpretable. Its parameters are in orders of magnitude less than the number
of parameters used by deep networks such as U-Net and are clearly interpretable
by humans. The proposed here IDSS offers a transparent structure that allows
users to inspect and audit the algorithm's decision. Results have demonstrated
that IDSS could surpass other algorithms, including U-Net, in terms of IoU
(Intersection over Union) total water and Recall total water. We used
WorldFloods data set for our experiments and plan to use the semantic
segmentation results combined with masks for permanent water to detect flood
events.",0,1,0,0,0,0,0.072851,11.0,0.618982,39
8be5bb87-6ba8-4794-a4b7-05085fb1b4bb,CaSS: A Channel-aware Self-supervised Representation Learning Framework for Multivariate Time Series Classification,4,0.0837799,0.246933,"Self-supervised representation learning of Multivariate Time Series (MTS) is
a challenging task and attracts increasing research interests in recent years.
Many previous works focus on the pretext task of self-supervised learning and
usually neglect the complex problem of MTS encoding, leading to unpromising
results. In this paper, we tackle this challenge from two aspects: encoder and
pretext task, and propose a unified channel-aware self-supervised learning
framework CaSS. Specifically, we first design a new Transformer-based encoder
Channel-aware Transformer (CaT) to capture the complex relationships between
different time channels of MTS. Second, we combine two novel pretext tasks Next
Trend Prediction (NTP) and Contextual Similarity (CS) for the self-supervised
representation learning with our proposed encoder. Extensive experiments are
conducted on several commonly used benchmark datasets. The experimental results
show that our framework achieves new state-of-the-art comparing with previous
self-supervised MTS representation learning methods (up to +7.70\% improvement
on LSST dataset) and can be well applied to the downstream MTS classification.",0,0,0,0,1,0,0.789641,10.0,0.883457,40
9cc15ae1-3533-46c0-8cda-0d59f8df73cd,Momentum Diminishes the Effect of Spectral Bias in Physics-Informed Neural Networks,4,0.0879456,0.154703,"Physics-informed neural network (PINN) algorithms have shown promising
results in solving a wide range of problems involving partial differential
equations (PDEs). However, they often fail to converge to desirable solutions
when the target function contains high-frequency features, due to a phenomenon
known as spectral bias. In the present work, we exploit neural tangent kernels
(NTKs) to investigate the training dynamics of PINNs evolving under stochastic
gradient descent with momentum (SGDM). This demonstrates SGDM significantly
reduces the effect of spectral bias. We have also examined why training a model
via the Adam optimizer can accelerate the convergence while reducing the
spectral bias. Moreover, our numerical experiments have confirmed that
wide-enough networks using SGDM still converge to desirable solutions, even in
the presence of high-frequency features. In fact, we show that the width of a
network plays a critical role in convergence.",0,0,0,0,0,0,0.960576,7.0,0.937727,26
2c1ddc8d-c413-4499-b61c-b78e5edba5ef,On the Super-exponential Quantum Speedup of Equivariant Quantum Machine Learning Algorithms with SU($d$) Symmetry,15,0.28104,0.822274,"We introduce a framework of the equivariant convolutional algorithms which is
tailored for a number of machine-learning tasks on physical systems with
arbitrary SU($d$) symmetries. It allows us to enhance a natural model of
quantum computation--permutational quantum computing (PQC) [Quantum Inf.
Comput., 10, 470-497 (2010)] --and defines a more powerful model: PQC+. While
PQC was shown to be effectively classically simulatable, we exhibit a problem
which can be efficiently solved on PQC+ machine, whereas the best known
classical algorithms runs in $O(n!n^2)$ time, thus providing strong evidence
against PQC+ being classically simulatable. We further discuss practical
quantum machine learning algorithms which can be carried out in the paradigm of
PQC+.",0,0,1,0,0,0,0.828619,9.0,0.88423,16
510bfd3f-d427-41ad-b61c-8e0456d68a14,Nearest Neighbor Non-autoregressive Text Generation,6,0.188216,0.159313,"Non-autoregressive (NAR) models can generate sentences with less computation
than autoregressive models but sacrifice generation quality. Previous studies
addressed this issue through iterative decoding. This study proposes using
nearest neighbors as the initial state of an NAR decoder and editing them
iteratively. We present a novel training strategy to learn the edit operations
on neighbors to improve NAR text generation. Experimental results show that the
proposed method (NeighborEdit) achieves higher translation quality (1.69 points
higher than the vanilla Transformer) with fewer decoding iterations
(one-eighteenth fewer iterations) on the JRC-Acquis En-De dataset, the common
benchmark dataset for machine translation using nearest neighbors. We also
confirm the effectiveness of the proposed method on a data-to-text task
(WikiBio). In addition, the proposed method outperforms an NAR baseline on the
WMT'14 En-De dataset. We also report analysis on neighbor examples used in the
proposed method.",0,1,0,0,1,0,0.870765,8.0,0.888314,36
30f34c4e-7fec-4d70-a300-0585c9b1abdb,CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization,11,0.242068,0.792252,"The quest for seeking health information has swamped the web with consumers'
health-related questions. Generally, consumers use overly descriptive and
peripheral information to express their medical condition or other healthcare
needs, contributing to the challenges of natural language understanding. One
way to address this challenge is to summarize the questions and distill the key
information of the original question. To address this issue, we introduce a new
dataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health
questions and corresponding summaries. The dataset is derived from the
community question-answering forum and therefore provides a valuable resource
for understanding consumer health-related posts on social media. We benchmark
the dataset on multiple state-of-the-art summarization models to show the
effectiveness of the dataset.",0,1,0,1,0,0,0.84669,7.0,0.859904,28
59fef962-81e6-4369-8bbb-4a923b7855fe,Efficient Speech Translation with Pre-trained Models,1,0.0072346,0.131497,"When building state-of-the-art speech translation models, the need for large
computational resources is a significant obstacle due to the large training
data size and complex models. The availability of pre-trained models is a
promising opportunity to build strong speech translation systems efficiently.
In a first step, we investigate efficient strategies to build cascaded and
end-to-end speech translation systems based on pre-trained models. Using this
strategy, we can train and apply the models on a single GPU. While the
end-to-end models show superior translation performance to cascaded ones, the
application of this technology has a limitation on the need for additional
end-to-end training data. In a second step, we proposed an additional
similarity loss to encourage the model to generate similar hidden
representations for speech and transcript. Using this technique, we can
increase the data efficiency and improve the translation quality by 6 BLEU
points in scenarios with limited end-to-end training data.",0,1,0,0,0,0,0.343009,7.0,0.646181,32
851c200c-f738-44e6-99c9-f68dbce71bc0,HyperShot: Few-Shot Learning by Kernel HyperNetworks,15,0.0352184,0.563876,"Few-shot models aim at making predictions using a minimal number of labeled
examples from a given task. The main challenge in this area is the one-shot
setting where only one element represents each class. We propose HyperShot -
the fusion of kernels and hypernetwork paradigm. Compared to reference
approaches that apply a gradient-based adjustment of the parameters, our model
aims to switch the classification module parameters depending on the task's
embedding. In practice, we utilize a hypernetwork, which takes the aggregated
information from support data and returns the classifier's parameters
handcrafted for the considered problem. Moreover, we introduce the kernel-based
representation of the support examples delivered to hypernetwork to create the
parameters of the classification module. Consequently, we rely on relations
between embeddings of the support examples instead of direct feature values
provided by the backbone models. Thanks to this approach, our model can adapt
to highly different tasks.",0,1,0,0,0,0,0.187561,8.0,0.602372,68
680eb8d5-27e7-4acc-802c-cf2347e81173,Meta Spatio-Temporal Debiasing for Video Scene Graph Generation,16,0.248145,0.818597,"Video scene graph generation (VidSGG) aims to parse the video content into
scene graphs, which involves modeling the spatio-temporal contextual
information in the video. However, due to the long-tailed training data in
datasets, the generalization performance of existing VidSGG models can be
affected by the spatio-temporal conditional bias problem. In this work, from
the perspective of meta-learning, we propose a novel Meta Video Scene Graph
Generation (MVSGG) framework to address such a bias problem. Specifically, to
handle various types of spatio-temporal conditional biases, our framework first
constructs a support set and a group of query sets from the training data,
where the data distribution of each query set is different from that of the
support set w.r.t. a type of conditional bias. Then, by performing a novel meta
training and testing process to optimize the model to obtain good testing
performance on these query sets after training on the support set, our
framework can effectively guide the model to learn to well generalize against
biases. Extensive experiments demonstrate the efficacy of our proposed
framework.",0,0,0,0,0,0,0.74119,6.0,0.781983,47
8de18786-4648-4b44-baee-cb32adc06e90,Abstraction for Deep Reinforcement Learning,22,0.05367,0.524873,"We characterise the problem of abstraction in the context of deep
reinforcement learning. Various well established approaches to analogical
reasoning and associative memory might be brought to bear on this issue, but
they present difficulties because of the need for end-to-end differentiability.
We review developments in AI and machine learning that could facilitate their
adoption.",0,0,0,0,0,0,0.117982,8.0,0.539432,87
a0ef30a3-661c-4633-a0e2-f66a5c7c51ff,Model-based Multi-agent Reinforcement Learning: Recent Progress and Prospects,13,0.337973,0.519939,"Significant advances have recently been achieved in Multi-Agent Reinforcement
Learning (MARL) which tackles sequential decision-making problems involving
multiple participants. However, MARL requires a tremendous number of samples
for effective training. On the other hand, model-based methods have been shown
to achieve provable advantages of sample efficiency. However, the attempts of
model-based methods to MARL have just started very recently. This paper
presents a review of the existing research on model-based MARL, including
theoretical analyses, algorithms, and applications, and analyzes the advantages
and potential of model-based MARL. Specifically, we provide a detailed taxonomy
of the algorithms and point out the pros and cons for each algorithm according
to the challenges inherent to multi-agent scenarios. We also outline promising
directions for future development of this field.",0,0,0,0,0,0,0.808933,5.0,0.778889,43
56338680-8997-4aee-ad56-746720bda879,Modeling Information Change in Science Communication with Semantically Matched Paraphrases,10,0.124255,0.797138,"Whether the media faithfully communicate scientific information has long been
a core issue to the science community. Automatically identifying paraphrased
scientific findings could enable large-scale tracking and analysis of
information changes in the science communication process, but this requires
systems to understand the similarity between scientific information across
multiple domains. To this end, we present the SCIENTIFIC PARAPHRASE AND
INFORMATION CHANGE DATASET (SPICED), the first paraphrase dataset of scientific
findings annotated for degree of information change. SPICED contains 6,000
scientific finding pairs extracted from news stories, social media discussions,
and full texts of original papers. We demonstrate that SPICED poses a
challenging task and that models trained on SPICED improve downstream
performance on evidence retrieval for fact checking of real-world scientific
claims. Finally, we show that models trained on SPICED can reveal large-scale
trends in the degrees to which people and organizations faithfully communicate
new scientific findings. Data, code, and pre-trained models are available at
http://www.copenlu.com/publication/2022_emnlp_wright/.",0,1,1,1,0,0,0.342282,7.0,0.645804,61
6c80a217-221a-443c-be2b-bb9bcb05af8f,Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,35,0.142729,0.901094,"Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain
Question Answering (ODQA). RAG has only been trained and explored with a
Wikipedia-based external knowledge base and is not optimized for use in other
specialized domains such as healthcare and news. In this paper, we evaluate the
impact of joint training of the retriever and generator components of RAG for
the task of domain adaptation in ODQA. We propose \textit{RAG-end2end}, an
extension to RAG, that can adapt to a domain-specific knowledge base by
updating all components of the external knowledge base during training. In
addition, we introduce an auxiliary training signal to inject more
domain-specific knowledge. This auxiliary signal forces \textit{RAG-end2end} to
reconstruct a given sentence by accessing the relevant information from the
external knowledge base. Our novel contribution is unlike RAG, RAG-end2end does
joint training of the retriever and generator for the end QA task and domain
adaptation. We evaluate our approach with datasets from three domains:
COVID-19, News, and Conversations, and achieve significant performance
improvements compared to the original RAG model. Our work has been open-sourced
through the Huggingface Transformers library, attesting to our work's
credibility and technical consistency.",0,1,0,0,0,0,0.505339,6.0,0.673236,44
72fbe0e5-ae26-4e82-9bfd-5e47f070f9c6,Towards Better Chinese-centric Neural Machine Translation for Low-resource Languages,7,0.358109,0.818251,"The last decade has witnessed enormous improvements in science and
technology, stimulating the growing demand for economic and cultural exchanges
in various countries. Building a neural machine translation (NMT) system has
become an urgent trend, especially in the low-resource setting. However, recent
work tends to study NMT systems for low-resource languages centered on English,
while few works focus on low-resource NMT systems centered on other languages
such as Chinese. To achieve this, the low-resource multilingual translation
challenge of the 2021 iFLYTEK AI Developer Competition provides the
Chinese-centric multilingual low-resource NMT tasks, where participants are
required to build NMT systems based on the provided low-resource samples. In
this paper, we present the winner competition system that leverages monolingual
word embeddings data enhancement, bilingual curriculum learning, and
contrastive re-ranking. In addition, a new Incomplete-Trust (In-trust) loss
function is proposed to replace the traditional cross-entropy loss when
training. The experimental results demonstrate that the implementation of these
ideas leads better performance than other state-of-the-art methods. All the
experimental codes are released at:
https://github.com/WENGSYX/Low-resource-text-translation.",1,1,0,0,1,0,0.897195,6.0,0.868751,36
991eaf7e-c494-4d12-b02e-51a36e68e66f,DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing,10,0.233995,0.439695,"The meaningful use of electronic health records (EHR) continues to progress
in the digital era with clinical decision support systems augmented by
artificial intelligence. A priority in improving provider experience is to
overcome information overload and reduce the cognitive burden so fewer medical
errors and cognitive biases are introduced during patient care. One major type
of medical error is diagnostic error due to systematic or predictable errors in
judgment that rely on heuristics. The potential for clinical natural language
processing (cNLP) to model diagnostic reasoning in humans with forward
reasoning from data to diagnosis and potentially reduce the cognitive burden
and medical error has not been investigated. Existing tasks to advance the
science in cNLP have largely focused on information extraction and named entity
recognition through classification tasks. We introduce a novel suite of tasks
coined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for
developing and evaluating cNLP models with clinical diagnostic reasoning
ability. The suite includes six tasks from ten publicly available datasets
addressing clinical text understanding, medical knowledge reasoning, and
diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to
be a natural language generation framework to evaluate pre-trained language
models. Experiments with state-of-the-art pre-trained generative language
models using large general domain models and models that were continually
trained on a medical corpus demonstrate opportunities for improvement when
evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab
repository with a systematic approach to load and evaluate models for the cNLP
community.",1,0,1,1,0,0,0.601556,7.0,0.758198,60
82c15f3e-17f4-4b0d-b47c-b46c307e8fe6,ROAD-R: The Autonomous Driving Dataset with Logical Requirements,18,0.162699,0.807808,"Neural networks have proven to be very powerful at computer vision tasks.
However, they often exhibit unexpected behaviours, violating known requirements
expressing background knowledge. This calls for models (i) able to learn from
the requirements, and (ii) guaranteed to be compliant with the requirements
themselves. Unfortunately, the development of such models is hampered by the
lack of datasets equipped with formally specified requirements. In this paper,
we introduce the ROad event Awareness Dataset with logical Requirements
(ROAD-R), the first publicly available dataset for autonomous driving with
requirements expressed as logical constraints. Given ROAD-R, we show that
current state-of-the-art models often violate its logical constraints, and that
it is possible to exploit them to create models that (i) have a better
performance, and (ii) are guaranteed to be compliant with the requirements
themselves.",0,0,1,1,0,0,0.110936,11.0,0.659089,60
28be8683-fdbe-44cf-b532-bcae33d8f956,Capturing Failures of Large Language Models via Human Cognitive Biases,57,0.547274,0.946232,"Large language models generate complex, open-ended outputs: instead of
outputting a class label they write summaries, generate dialogue, or produce
working code. In order to asses the reliability of these open-ended generation
systems, we aim to identify qualitative categories of erroneous behavior,
beyond identifying individual errors. To hypothesize and test for such
qualitative errors, we draw inspiration from human cognitive biases --
systematic patterns of deviation from rational judgement. Specifically, we use
cognitive biases as motivation to (i) generate hypotheses for problems that
models may have, and (ii) develop experiments that elicit these problems. Using
code generation as a case study, we find that OpenAI's Codex errs predictably
based on how the input prompt is framed, adjusts outputs towards anchors, and
is biased towards outputs that mimic frequent training examples. We then use
our framework to elicit high-impact errors such as incorrectly deleting files.
Our results indicate that experimental methodology from cognitive science can
help characterize how machine learning systems behave.",0,0,0,0,0,0,0.967952,4.0,0.906547,69
67030e47-61b2-480a-a048-698300836d82,Counterfactual Explanations for Predictive Business Process Monitoring,13,0.155846,0.890314,"Predictive business process monitoring increasingly leverages sophisticated
prediction models. Although sophisticated models achieve consistently higher
prediction accuracy than simple models, one major drawback is their lack of
interpretability, which limits their adoption in practice. We thus see growing
interest in explainable predictive business process monitoring, which aims to
increase the interpretability of prediction models. Existing solutions focus on
giving factual explanations.While factual explanations can be helpful, humans
typically do not ask why a particular prediction was made, but rather why it
was made instead of another prediction, i.e., humans are interested in
counterfactual explanations. While research in explainable AI produced several
promising techniques to generate counterfactual explanations, directly applying
them to predictive process monitoring may deliver unrealistic explanations,
because they ignore the underlying process constraints. We propose LORELEY, a
counterfactual explanation technique for predictive process monitoring, which
extends LORE, a recent explainable AI technique. We impose control flow
constraints to the explanation generation process to ensure realistic
counterfactual explanations. Moreover, we extend LORE to enable explaining
multi-class classification models. Experimental results using a real, public
dataset indicate that LORELEY can approximate the prediction models with an
average fidelity of 97.69\% and generate realistic counterfactual explanations.",1,1,0,0,0,0,0.306989,8.0,0.673422,35
149c77f8-6fac-4bf3-a0fd-6243366f6751,"GO-Surf: Neural Feature Grid Optimization for Fast, High-Fidelity RGB-D Surface Reconstruction",54,0.232429,0.977338,"We present GO-Surf, a direct feature grid optimization method for accurate
and fast surface reconstruction from RGB-D sequences. We model the underlying
scene with a learned hierarchical feature voxel grid that encapsulates
multi-level geometric and appearance local information. Feature vectors are
directly optimized such that after being tri-linearly interpolated, decoded by
two shallow MLPs into signed distance and radiance values, and rendered via
surface volume rendering, the discrepancy between synthesized and observed
RGB/depth values is minimized. Our supervision signals -- RGB, depth and
approximate SDF -- can be obtained directly from input images without any need
for fusion or post-processing. We formulate a novel SDF gradient regularization
term that encourages surface smoothness and hole filling while maintaining high
frequency details. GO-Surf can optimize sequences of $1$-$2$K frames in
$15$-$45$ minutes, a speedup of $\times60$ over NeuralRGB-D, the most related
approach based on an MLP representation, while maintaining on par performance
on standard benchmarks. Project page: https://jingwenwang95.github.io/go_surf/",0,0,0,0,0,0,0.895218,4.0,0.801025,41
5aeda20e-da53-4f55-8879-770299a1b788,"""I'm sorry to hear that"": Finding New Biases in Language Models with a Holistic Descriptor Dataset",69,0.958046,0.855002,"As language models grow in popularity, it becomes increasingly important to
clearly measure all possible markers of demographic identity in order to avoid
perpetuating existing societal harms. Many datasets for measuring bias
currently exist, but they are restricted in their coverage of demographic axes
and are commonly used with preset bias tests that presuppose which types of
biases models can exhibit. In this work, we present a new, more inclusive bias
measurement dataset, HolisticBias, which includes nearly 600 descriptor terms
across 13 different demographic axes. HolisticBias was assembled in a
participatory process including experts and community members with lived
experience of these terms. These descriptors combine with a set of bias
measurement templates to produce over 450,000 unique sentence prompts, which we
use to explore, identify, and reduce novel forms of bias in several generative
models. We demonstrate that HolisticBias is effective at measuring previously
undetectable biases in token likelihoods from language models, as well as in an
offensiveness classifier. We will invite additions and amendments to the
dataset, which we hope will serve as a basis for more easy-to-use and
standardized methods for evaluating bias in NLP models.",0,0,1,1,0,0,0.960723,5.0,0.913049,89
03576811-9cd4-4055-9b10-5f47264461f7,The Causal News Corpus: Annotating Causal Relations in Event Sentences from News,37,0.364291,0.875968,"Despite the importance of understanding causality, corpora addressing causal
relations are limited. There is a discrepancy between existing annotation
guidelines of event causality and conventional causality corpora that focus
more on linguistics. Many guidelines restrict themselves to include only
explicit relations or clause-based arguments. Therefore, we propose an
annotation schema for event causality that addresses these concerns. We
annotated 3,559 event sentences from protest event news with labels on whether
it contains causal relations or not. Our corpus is known as the Causal News
Corpus (CNC). A neural network built upon a state-of-the-art pre-trained
language model performed well with 81.20% F1 score on test set, and 83.46% in
5-folds cross-validation. CNC is transferable across two external corpora:
CausalTimeBank (CTB) and Penn Discourse Treebank (PDTB). Leveraging each of
these external datasets for training, we achieved up to approximately 64% F1 on
the CNC test set without additional fine-tuning. CNC also served as an
effective training and pre-training dataset for the two external corpora.
Lastly, we demonstrate the difficulty of our task to the layman in a
crowd-sourced annotation exercise. Our annotated corpus is publicly available,
providing a valuable resource for causal text mining researchers.",1,1,0,1,1,0,0.024613,12.0,0.55821,62
4d1adb7c-ccb4-455f-bc5b-59426b85a047,High-Res Facial Appearance Capture from Polarized Smartphone Images,7,0.0806964,0.74146,"We propose a novel method for high-quality facial texture reconstruction from
RGB images using a novel capturing routine based on a single smartphone which
we equip with an inexpensive polarization foil. Specifically, we turn the
flashlight into a polarized light source and add a polarization filter on top
of the camera. Leveraging this setup, we capture the face of a subject with
cross-polarized and parallel-polarized light. For each subject, we record two
short sequences in a dark environment under flash illumination with different
light polarization using the modified smartphone. Based on these observations,
we reconstruct an explicit surface mesh of the face using structure from
motion. We then exploit the camera and light co-location within a
differentiable renderer to optimize the facial textures using an
analysis-by-synthesis approach. Our method optimizes for high-resolution normal
textures, diffuse albedo, and specular albedo using a coarse-to-fine
optimization scheme. We show that the optimized textures can be used in a
standard rendering pipeline to synthesize high-quality photo-realistic 3D
digital humans in novel environments.",0,1,0,0,0,0,0.154925,7.0,0.515534,74
5bb86b4a-0546-4c21-bfab-7514559ab9c3,Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest,5,0.0771395,0.437841,"Medical Relation Extraction (MRE) task aims to extract relations between
entities in medical texts. Traditional relation extraction methods achieve
impressive success by exploring the syntactic information, e.g., dependency
tree. However, the quality of the 1-best dependency tree for medical texts
produced by an out-of-domain parser is relatively limited so that the
performance of medical relation extraction method may degenerate. To this end,
we propose a method to jointly model semantic and syntactic information from
medical texts based on causal explanation theory. We generate dependency
forests consisting of the semantic-embedded 1-best dependency tree. Then, a
task-specific causal explainer is adopted to prune the dependency forests,
which are further fed into a designed graph convolutional network to learn the
corresponding representation for downstream task. Empirically, the various
comparisons on benchmark medical datasets demonstrate the effectiveness of our
model.",0,1,0,0,1,0,0.390708,9.0,0.743143,36
34ecd008-ade9-45c4-801a-c0138543ebe8,Ham2Pose: Animating Sign Language Notation into Pose Sequences,9,0.46472,0.89581,"Translating spoken languages into Sign languages is necessary for open
communication between the hearing and hearing-impaired communities. To achieve
this goal, we propose the first method for animating a text written in
HamNoSys, a lexical Sign language notation, into signed pose sequences. As
HamNoSys is universal by design, our proposed method offers a generic solution
invariant to the target Sign language. Our method gradually generates pose
predictions using transformer encoders that create meaningful representations
of the text and poses while considering their spatial and temporal information.
We use weak supervision for the training process and show that our method
succeeds in learning from partial and inaccurate data. Additionally, we offer a
new distance measurement that considers missing keypoints, to measure the
distance between pose sequences using DTW-MJE. We validate its correctness
using AUTSL, a large-scale Sign language dataset, show that it measures the
distance between pose sequences more accurately than existing measurements, and
use it to assess the quality of our generated pose sequences. Code for the data
pre-processing, the model, and the distance measurement is publicly released
for future research.",0,1,0,0,0,0,0.958125,6.0,0.92421,64
9940d5da-8210-4d6e-b3b8-225bc3344f69,MMRotate: A Rotated Object Detection Benchmark using PyTorch,154,0.637585,0.951203,"We present an open-source toolbox, named MMRotate, which provides a coherent
algorithm framework of training, inferring, and evaluation for the popular
rotated object detection algorithm based on deep learning. MMRotate implements
18 state-of-the-art algorithms and supports the three most frequently used
angle definition methods. To facilitate future research and industrial
applications of rotated object detection-related problems, we also provide a
large number of trained models and detailed benchmarks to give insights into
the performance of rotated object detection. MMRotate is publicly released at
https://github.com/open-mmlab/mmrotate.",1,1,0,0,0,0,0.731524,6.0,0.777399,49
205845e4-c425-4ccd-a0de-f4ac56a856f0,Table-based Fact Verification with Self-adaptive Mixture of Experts,10,0.109102,0.876011,"The table-based fact verification task has recently gained widespread
attention and yet remains to be a very challenging problem. It inherently
requires informative reasoning over natural language together with different
numerical and logical reasoning on tables (e.g., count, superlative,
comparative). Considering that, we exploit mixture-of-experts and present in
this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE).
Specifically, we have developed a mixture-of-experts neural network to
recognize and execute different types of reasoning -- the network is composed
of multiple experts, each handling a specific part of the semantics for
reasoning, whereas a management module is applied to decide the contribution of
each expert network to the verification result. A self-adaptive method is
developed to teach the management module combining results of different experts
more efficiently without external knowledge. The experimental results
illustrate that our framework achieves 85.1% accuracy on the benchmark dataset
TabFact, comparable with the previous state-of-the-art models. We hope our
framework can serve as a new baseline for table-based verification. Our code is
available at https://github.com/THUMLP/SaMoE.",1,1,0,0,1,0,0.61868,6.0,0.72567,33
ef8b1cb8-fe55-4ccd-beef-5ad9b16f4205,BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla,11,0.0801289,0.326943,"This work presents BanglaNLG, a comprehensive benchmark for evaluating
natural language generation (NLG) models in Bangla, a widely spoken yet
low-resource language. We aggregate six challenging conditional text generation
tasks under the BanglaNLG benchmark, introducing a new dataset on dialogue
generation in the process. Furthermore, using a clean corpus of 27.5 GB of
Bangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language
model for Bangla. BanglaT5 achieves state-of-the-art performance in all of
these tasks, outperforming several multilingual models by up to 9% absolute
gain and 32% relative gain. We are making the new dialogue dataset and the
BanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in
the hope of advancing future research on Bangla NLG.",1,1,1,1,1,0,0.689999,6.0,0.758096,67
4301faa2-d12a-4ee9-b841-fc53b104747c,Sparse Mixture Once-for-all Adversarial Training for Efficient In-Situ Trade-Off Between Accuracy and Robustness of DNNs,3,0.00410532,0.209196,"Existing deep neural networks (DNNs) that achieve state-of-the-art (SOTA)
performance on both clean and adversarially-perturbed images rely on either
activation or weight conditioned convolution operations. However, such
conditional learning costs additional multiply-accumulate (MAC) or addition
operations, increasing inference memory and compute costs. To that end, we
present a sparse mixture once for all adversarial training (SMART), that allows
a model to train once and then in-situ trade-off between accuracy and
robustness, that too at a reduced compute and parameter overhead. In
particular, SMART develops two expert paths, for clean and adversarial images,
respectively, that are then conditionally trained via respective dedicated sets
of binary sparsity masks. Extensive evaluations on multiple image
classification datasets across different models show SMART to have up to 2.72x
fewer non-zero parameters costing proportional reduction in compute overhead,
while yielding SOTA accuracy-robustness trade-off. Additionally, we present
insightful observations in designing sparse masks to successfully condition on
both clean and perturbed images.",0,1,0,0,0,0,0.0519761,9.0,0.495573,32
bb95d676-a78c-43a1-b6cb-2d635aee43f0,Walk this Way! Entity Walks and Property Walks for RDF2vec,10,0.34936,0.650062,"RDF2vec is a knowledge graph embedding mechanism which first extracts
sequences from knowledge graphs by performing random walks, then feeds those
into the word embedding algorithm word2vec for computing vector representations
for entities. In this poster, we introduce two new flavors of walk extraction
coined e-walks and p-walks, which put an emphasis on the structure or the
neighborhood of an entity respectively, and thereby allow for creating
embeddings which focus on similarity or relatedness. By combining the walk
strategies with order-aware and classic RDF2vec, as well as CBOW and skip-gram
word2vec embeddings, we conduct a preliminary evaluation with a total of 12
RDF2vec variants.",1,1,0,0,0,0,0.65078,5.0,0.688261,11
be33605c-2174-4bf5-93ba-077c60b0ba70,Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions,9,0.046218,0.638927,"Current QA systems can generate reasonable-sounding yet false answers without
explanation or evidence for the generated answer, which is especially
problematic when humans cannot readily check the model's answers. This presents
a challenge for building trust in machine learning systems. We take inspiration
from real-world situations where difficult questions are answered by
considering opposing sides (see Irving et al., 2018). For multiple-choice QA
examples, we build a dataset of single arguments for both a correct and
incorrect answer option in a debate-style set-up as an initial step in training
models to produce explanations for two candidate answers. We use long contexts
-- humans familiar with the context write convincing explanations for
pre-selected correct and incorrect answers, and we test if those explanations
allow humans who have not read the full context to more accurately determine
the correct answer. We do not find that explanations in our set-up improve
human accuracy, but a baseline condition shows that providing human-selected
text snippets does improve accuracy. We use these findings to suggest ways of
improving the debate set up for future data collection efforts.",0,0,0,1,0,0,0.569248,6.0,0.703142,15
c7535a15-0ded-4d10-8b89-eaee44a5858b,Spiking Neural Networks for Frame-based and Event-based Single Object Localization,15,0.108452,0.853411,"Spiking neural networks have shown much promise as an energy-efficient
alternative to artificial neural networks. However, understanding the impacts
of sensor noises and input encodings on the network activity and performance
remains difficult with common neuromorphic vision baselines like
classification. Therefore, we propose a spiking neural network approach for
single object localization trained using surrogate gradient descent, for frame-
and event-based sensors. We compare our method with similar artificial neural
networks and show that our model has competitive/better performance in
accuracy, robustness against various corruptions, and has lower energy
consumption. Moreover, we study the impact of neural coding schemes for static
images in accuracy, robustness, and energy efficiency. Our observations differ
importantly from previous studies on bio-plausible learning rules, which helps
in the design of surrogate gradient trained architectures, and offers insight
to design priorities in future neuromorphic technologies in terms of noise
characteristics and data encoding methods.",0,1,0,0,0,0,0.386456,6.0,0.612359,91
65da5259-8a6f-405f-b147-aeb413990d1a,Can Language Models perform Abductive Commonsense Reasoning?,1,0.0267951,0.0240894,"Abductive Reasoning is a task of inferring the most plausible hypothesis
given a set of observations. In literature, the community has approached to
solve this challenge by classifying/generating a likely hypothesis that does
not contradict with a past observation and future observation. Some of the most
well-known benchmarks that tackle this problem are aNLI and aNLG (pronounced as
alpha-NLI and alpha-NLG). In this report, I review over some of the
methodologies that were attempted to solve this challenge, re-implement the
baseline models, and analyze some of the weaknesses that current approaches
have. The code and the re-implemented results are available at this link.",0,0,0,0,0,0,0.949599,6.0,0.91418,27
9a729348-906e-429b-adf3-59a8f1ce6549,Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,109,0.418456,0.983318,"Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create ""latent saliency maps"" that can help explain predictions
in human terms.",0,0,0,0,0,0,0.598157,5.0,0.659622,35
d7ad8ebe-1264-41b3-b206-9a821ed0515e,MaskRange: A Mask-classification Model for Range-view based LiDAR Segmentation,7,0.110377,0.234821,"Range-view based LiDAR segmentation methods are attractive for practical
applications due to their direct inheritance from efficient 2D CNN
architectures. In literature, most range-view based methods follow the
per-pixel classification paradigm. Recently, in the image segmentation domain,
another paradigm formulates segmentation as a mask-classification problem and
has achieved remarkable performance. This raises an interesting question: can
the mask-classification paradigm benefit the range-view based LiDAR
segmentation and achieve better performance than the counterpart per-pixel
paradigm? To answer this question, we propose a unified mask-classification
model, MaskRange, for the range-view based LiDAR semantic and panoptic
segmentation. Along with the new paradigm, we also propose a novel data
augmentation method to deal with overfitting, context-reliance, and
class-imbalance problems. Extensive experiments are conducted on the
SemanticKITTI benchmark. Among all published range-view based methods, our
MaskRange achieves state-of-the-art performance with $66.10$ mIoU on semantic
segmentation and promising results with $53.10$ PQ on panoptic segmentation
with high efficiency. Our code will be released.",0,1,0,0,1,0,0.930593,6.0,0.895301,58
7e4cfba2-e1ec-4fbe-b5b8-4381d7861c4e,The Alberta Plan for AI Research,15,0.192826,0.747482,"Herein we describe our approach to artificial intelligence research, which we
call the Alberta Plan. The Alberta Plan is pursued within our research groups
in Alberta and by others who are like minded throughout the world. We welcome
all who would join us in this pursuit.",0,0,0,0,0,1,0.0137654,19.0,0.690099,44
85e887c9-3dc9-4a38-b9ac-b6fd1d063f15,Tensor-based Sequential Learning via Hankel Matrix Representation for Next Item Recommendations,2,0.00649958,0.147834,"Self-attentive transformer models have recently been shown to solve the next
item recommendation task very efficiently. The learned attention weights
capture sequential dynamics in user behavior and generalize well. Motivated by
the special structure of learned parameter space, we question if it is possible
to mimic it with an alternative and more lightweight approach. We develop a new
tensor factorization-based model that ingrains the structural knowledge about
sequential data within the learning process. We demonstrate how certain
properties of a self-attention network can be reproduced with our approach
based on special Hankel matrix representation. The resulting model has a
shallow linear architecture and compares competitively to its neural
counterpart.",0,0,0,0,0,0,0.0485828,10.0,0.539087,34
f7796e6f-b184-4019-a7dc-adae638362b9,Importance of Synthesizing High-quality Data for Text-to-SQL Parsing,9,0.0386121,0.376599,"Recently, there has been increasing interest in synthesizing data to improve
downstream text-to-SQL tasks. In this paper, we first examined the existing
synthesized datasets and discovered that state-of-the-art text-to-SQL
algorithms did not further improve on popular benchmarks when trained with
augmented synthetic data. We observed two shortcomings: illogical synthetic SQL
queries from independent column sampling and arbitrary table joins. To address
these issues, we propose a novel synthesis framework that incorporates key
relationships from schema, imposes strong typing, and conducts
schema-distance-weighted column sampling. We also adopt an intermediate
representation (IR) for the SQL-to-text task to further improve the quality of
the generated natural language questions. When existing powerful semantic
parsers are pre-finetuned on our high-quality synthesized data, our experiments
show that these models have significant accuracy boosts on popular benchmarks,
including new state-of-the-art performance on Spider.",1,1,0,0,1,0,0.141469,7.0,0.501455,42
10d6b809-ee28-474f-a2cf-be1a8c4d7d8d,CD-FSOD: A Benchmark for Cross-domain Few-shot Object Detection,5,0.0596921,0.349461,"In this paper, we propose a study of the cross-domain few-shot object
detection (CD-FSOD) benchmark, consisting of image data from a diverse data
domain. On the proposed benchmark, we evaluate state-of-art FSOD approaches,
including meta-learning FSOD approaches and fine-tuning FSOD approaches. The
results show that these methods tend to fall, and even underperform the naive
fine-tuning model. We analyze the reasons for their failure and introduce a
strong baseline that uses a mutually-beneficial manner to alleviate the
overfitting problem. Our approach is remarkably superior to existing approaches
by significant margins (2.0\% on average) on the proposed benchmark. Our code
is available at \url{https://github.com/FSOD/CD-FSOD}.",1,1,1,0,1,0,0.909415,7.0,0.895235,41
7fb6e46e-5c39-44ff-943f-5d6bb6a1cc37,FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation,10,0.340826,0.303219,"Recent model-based reference-free metrics for open-domain dialogue evaluation
exhibit promising correlations with human judgment. However, they either
perform turn-level evaluation or look at a single dialogue quality dimension.
One would expect a good evaluation metric to assess multiple quality dimensions
at the dialogue level. To this end, we are motivated to propose a
multi-dimensional dialogue-level metric, which consists of three sub-metrics
with each targeting a specific dimension. The sub-metrics are trained with
novel self-supervised objectives and exhibit strong correlations with human
judgment for their respective dimensions. Moreover, we explore two approaches
to combine the sub-metrics: metric ensemble and multitask learning. Both
approaches yield a holistic metric that significantly outperforms individual
sub-metrics. Compared to the existing state-of-the-art metric, the combined
metrics achieve around 16% relative improvement on average across three
high-quality dialogue-level evaluation benchmarks.",0,1,0,0,1,0,0.919063,6.0,0.885412,59
638773fe-8f82-4402-94a8-8fbde6d95495,The COVID That Wasn't: Counterfactual Journalism Using GPT,2,0.015604,0.288495,"In this paper, we explore the use of large language models to assess human
interpretations of real world events. To do so, we use a language model trained
prior to 2020 to artificially generate news articles concerning COVID-19 given
the headlines of actual articles written during the pandemic. We then compare
stylistic qualities of our artificially generated corpus with a news corpus, in
this case 5,082 articles produced by CBC News between January 23 and May 5,
2020. We find our artificially generated articles exhibits a considerably more
negative attitude towards COVID and a significantly lower reliance on
geopolitical framing. Our methods and results hold importance for researchers
seeking to simulate large scale cultural processes via recent breakthroughs in
text generation.",0,0,0,0,0,0,0.289259,5.0,0.4632,42
6a9e0c30-4e56-45a2-8cc2-5c87011c1b1d,Eliciting Best Practices for Collaboration with Computational Notebooks,13,0.0672125,0.67087,"Despite the widespread adoption of computational notebooks, little is known
about best practices for their usage in collaborative contexts. In this paper,
we fill this gap by eliciting a catalog of best practices for collaborative
data science with computational notebooks. With this aim, we first look for
best practices through a multivocal literature review. Then, we conduct
interviews with professional data scientists to assess their awareness of these
best practices. Finally, we assess the adoption of best practices through the
analysis of 1,380 Jupyter notebooks retrieved from the Kaggle platform.
Findings reveal that experts are mostly aware of the best practices and tend to
adopt them in their daily work. Nonetheless, they do not consistently follow
all the recommendations as, depending on specific contexts, some are deemed
unfeasible or counterproductive due to the lack of proper tool support. As
such, we envision the design of notebook solutions that allow data scientists
not to have to prioritize exploration and rapid prototyping over writing code
of quality.",0,1,0,0,0,0,0.0897482,9.0,0.558496,76
bd6307d1-966b-47e5-a7fe-76d587d44a88,Splicing Detection and Localization In Satellite Imagery Using Conditional GANs,33,0.484225,0.121062,"The widespread availability of image editing tools and improvements in image
processing techniques allow image manipulation to be very easy. Oftentimes,
easy-to-use yet sophisticated image manipulation tools yields
distortions/changes imperceptible to the human observer. Distribution of forged
images can have drastic ramifications, especially when coupled with the speed
and vastness of the Internet. Therefore, verifying image integrity poses an
immense and important challenge to the digital forensic community. Satellite
images specifically can be modified in a number of ways, including the
insertion of objects to hide existing scenes and structures. In this paper, we
describe the use of a Conditional Generative Adversarial Network (cGAN) to
identify the presence of such spliced forgeries within satellite images.
Additionally, we identify their locations and shapes. Trained on pristine and
falsified images, our method achieves high success on these detection and
localization objectives.",0,1,0,0,0,0,0.759953,7.0,0.820869,21
4ce27c3e-3f91-4ad9-ad2d-98c15e44f623,Generating natural images with direct Patch Distributions Matching,14,0.0808529,0.777994,"Many traditional computer vision algorithms generate realistic images by
requiring that each patch in the generated image be similar to a patch in a
training image and vice versa. Recently, this classical approach has been
replaced by adversarial training with a patch discriminator. The adversarial
approach avoids the computational burden of finding nearest neighbors of
patches but often requires very long training times and may fail to match the
distribution of patches. In this paper we leverage the recently developed
Sliced Wasserstein Distance and develop an algorithm that explicitly and
efficiently minimizes the distance between patch distributions in two images.
Our method is conceptually simple, requires no training and can be implemented
in a few lines of codes. On a number of image generation tasks we show that our
results are often superior to single-image-GANs, require no training, and can
generate high quality images in a few seconds. Our implementation is available
at https://github.com/ariel415el/GPDM",1,1,1,0,0,1,0.321337,10.0,0.744286,38
c9247dc2-a830-4c8a-98a2-28c4a926d610,Parametric Classification for Generalized Category Discovery: A Baseline Study,16,0.136,0.407653,"Generalized Category Discovery (GCD) aims to discover novel categories in
unlabelled datasets using knowledge learned from labelled samples. Previous
studies argued that parametric classifiers are prone to overfitting to seen
categories, and endorsed using a non-parametric classifier formed with
semi-supervised k-means. However, in this study, we investigate the failure of
parametric classifiers, verify the effectiveness of previous design choices
when high-quality supervision is available, and identify unreliable
pseudo-labels as a key problem. We demonstrate that two prediction biases
exist: the classifier tends to predict seen classes more often, and produces an
imbalanced distribution across seen and novel categories. Based on these
findings, we propose a simple yet effective parametric classification method
that benefits from entropy regularisation, achieves state-of-the-art
performance on multiple GCD benchmarks and shows strong robustness to unknown
class numbers. We hope the investigation and proposed simple framework can
serve as a strong baseline to facilitate future studies in this field. Our code
is available at: https://github.com/CVMI-Lab/SimGCD.",1,1,0,0,1,0,0.70619,5.0,0.718671,59
03806996-7c42-4c49-b578-84cea4662282,Exploration via Elliptical Episodic Bonuses,20,0.139662,0.770185,"In recent years, a number of reinforcement learning (RL) methods have been
proposed to explore complex environments which differ across episodes. In this
work, we show that the effectiveness of these methods critically relies on a
count-based episodic term in their exploration bonus. As a result, despite
their success in relatively simple, noise-free settings, these methods fall
short in more realistic scenarios where the state space is vast and prone to
noise. To address this limitation, we introduce Exploration via Elliptical
Episodic Bonuses (E3B), a new method which extends count-based episodic bonuses
to continuous state spaces and encourages an agent to explore states that are
diverse under a learned embedding within each episode. The embedding is learned
using an inverse dynamics model in order to capture controllable aspects of the
environment. Our method sets a new state-of-the-art across 16 challenging tasks
from the MiniHack suite, without requiring task-specific inductive biases. E3B
also matches existing methods on sparse reward, pixel-based VizDoom
environments, and outperforms existing methods in reward-free exploration on
Habitat, demonstrating that it can scale to high-dimensional pixel-based
observations and realistic environments.",0,1,0,0,1,0,0.342317,9.0,0.724528,79
b829d694-5d1d-47ba-a46c-6163037ac961,Entity Linking in Tabular Data Needs the Right Attention,2,0.0143729,0.151271,"Understanding the semantic meaning of tabular data requires Entity Linking
(EL), in order to associate each cell value to a real-world entity in a
Knowledge Base (KB). In this work, we focus on end-to-end solutions for EL on
tabular data that do not rely on fact lookup in the target KB. Tabular data
contains heterogeneous and sparse context, including column headers, cell
values and table captions. We experiment with various models to generate a
vector representation for each cell value to be linked. Our results show that
it is critical to apply an attention mechanism as well as an attention mask, so
that the model can only attend to the most relevant context and avoid
information dilution. The most relevant context includes: same-row cells,
same-column cells, headers and caption. Computational complexity, however,
grows quadratically with the size of tabular data for such a complex model. We
achieve constant memory usage by introducing a Tabular Entity Linking Lite
model (TELL ) that generates vector representation for a cell based only on its
value, the table headers and the table caption. TELL achieves 80.8% accuracy on
Wikipedia tables, which is only 0.1% lower than the state-of-the-art model with
quadratic memory usage.",0,1,0,0,1,0,0.0780519,9.0,0.542282,13
8ebe8ddc-3ef9-4147-9bf5-50fc42233f52,ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction,2,0.0302927,0.087404,"In this work, we propose the novel Prototypical Graph Regression
Self-explainable Trees (ProGReST) model, which combines prototype learning,
soft decision trees, and Graph Neural Networks. In contrast to other works, our
model can be used to address various challenging tasks, including compound
property prediction. In ProGReST, the rationale is obtained along with
prediction due to the model's built-in interpretability. Additionally, we
introduce a new graph prototype projection to accelerate model training.
Finally, we evaluate PRoGReST on a wide range of chemical datasets for
molecular property prediction and perform in-depth analysis with chemical
experts to evaluate obtained interpretations. Our method achieves competitive
results against state-of-the-art methods.",1,0,0,0,1,0,0.724838,6.0,0.774252,47
e77e635c-ceeb-4dfc-88a6-8bc27b74adfb,An Anomaly Detection Method for Satellites Using Monte Carlo Dropout,9,0.107055,0.539047,"Recently, there has been a significant amount of interest in satellite
telemetry anomaly detection (AD) using neural networks (NN). For AD purposes,
the current approaches focus on either forecasting or reconstruction of the
time series, and they cannot measure the level of reliability or the
probability of correct detection. Although the Bayesian neural network
(BNN)-based approaches are well known for time series uncertainty estimation,
they are computationally intractable. In this paper, we present a tractable
approximation for BNN based on the Monte Carlo (MC) dropout method for
capturing the uncertainty in the satellite telemetry time series, without
sacrificing accuracy. For time series forecasting, we employ an NN, which
consists of several Long Short-Term Memory (LSTM) layers followed by various
dense layers. We employ the MC dropout inside each LSTM layer and before the
dense layers for uncertainty estimation. With the proposed uncertainty region
and by utilizing a post-processing filter, we can effectively capture the
anomaly points. Numerical results show that our proposed time series AD
approach outperforms the existing methods from both prediction accuracy and AD
perspectives.",0,1,0,0,1,0,0.150789,9.0,0.619922,36
293cfe94-f890-4671-a96e-abe551b43c3a,Consistency Regularization for Domain Adaptation,1,0.0250056,0.0477416,"Collection of real world annotations for training semantic segmentation
models is an expensive process. Unsupervised domain adaptation (UDA) tries to
solve this problem by studying how more accessible data such as synthetic data
can be used to train and adapt models to real world images without requiring
their annotations. Recent UDA methods applies self-learning by training on
pixel-wise classification loss using a student and teacher network. In this
paper, we propose the addition of a consistency regularization term to
semi-supervised UDA by modelling the inter-pixel relationship between elements
in networks' output. We demonstrate the effectiveness of the proposed
consistency regularization term by applying it to the state-of-the-art DAFormer
framework and improving mIoU19 performance on the GTA5 to Cityscapes benchmark
by 0.8 and mIou16 performance on the SYNTHIA to Cityscapes benchmark by 1.2.",1,1,0,0,1,0,0.866166,12.0,0.924106,37
1e676ae6-dd3c-4434-befe-1da8f4d2c8d6,Memory-Efficient Sequential Pattern Mining with Hybrid Tries,1,0.0133806,0.105128,"As modern data sets continue to grow exponentially in size, the demand for
efficient mining algorithms capable of handling such large data sets becomes
increasingly imperative. This paper develops a memory-efficient approach for
Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery
that faces a well-known memory bottleneck for large data sets. Our methodology
involves a novel hybrid trie data structure that exploits recurring patterns to
compactly store the data set in memory; and a corresponding mining algorithm
designed to effectively extract patterns from this compact representation.
Numerical results on real-life test instances show an average improvement of
88% in memory consumption and 41% in computation time for small to medium-sized
data sets compared to the state of the art. Furthermore, our algorithm stands
out as the only capable SPM approach for large data sets within 256GB of system
memory.",0,1,0,0,1,0,0.00189299,18.0,0.562328,48
747b6464-720f-4e26-a34b-a43f23e527a6,Por Qu No Utiliser Alla Sprk? Mixed Training with Gradient Optimization in Few-Shot Cross-Lingual Transfer,10,0.196621,0.419309,"The current state-of-the-art for few-shot cross-lingual transfer learning
first trains on abundant labeled data in the source language and then
fine-tunes with a few examples on the target language, termed target-adapting.
Though this has been demonstrated to work on a variety of tasks, in this paper
we show some deficiencies of this approach and propose a one-step mixed
training method that trains on both source and target data with
\textit{stochastic gradient surgery}, a novel gradient-level optimization.
Unlike the previous studies that focus on one language at a time when
target-adapting, we use one model to handle all target languages simultaneously
to avoid excessively language-specific models. Moreover, we discuss the
unreality of utilizing large target development sets for model selection in
previous literature. We further show that our method is both development-free
for target languages, and is also able to escape from overfitting issues. We
conduct a large-scale experiment on 4 diverse NLP tasks across up to 48
languages. Our proposed method achieves state-of-the-art performance on all
tasks and outperforms target-adapting by a large margin, especially for
languages that are linguistically distant from the source language, e.g., 7.36%
F1 absolute gain on average for the NER task, up to 17.60% on Punjabi.",1,1,0,0,1,0,0.867438,6.0,0.849001,48
6702d09e-8d59-4596-b084-37ce3838a7b4,Interactive Multi-Class Tiny-Object Detection,14,0.177857,0.804051,"Annotating tens or hundreds of tiny objects in a given image is laborious yet
crucial for a multitude of Computer Vision tasks. Such imagery typically
contains objects from various categories, yet the multi-class interactive
annotation setting for the detection task has thus far been unexplored. To
address these needs, we propose a novel interactive annotation method for
multiple instances of tiny objects from multiple classes, based on a few
point-based user inputs. Our approach, C3Det, relates the full image context
with annotator inputs in a local and global manner via late-fusion and
feature-correlation, respectively. We perform experiments on the Tiny-DOTA and
LCell datasets using both two-stage and one-stage object detection
architectures to verify the efficacy of our approach. Our approach outperforms
existing approaches in interactive annotation, achieving higher mAP with fewer
clicks. Furthermore, we validate the annotation efficiency of our approach in a
user study where it is shown to be 2.85x faster and yield only 0.36x task load
(NASA-TLX, lower is better) compared to manual annotation. The code is
available at
https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection.",1,1,1,0,1,0,0.812856,8.0,0.863362,38
77d4f737-28db-4dea-a3d6-45f67288b985,"AI Technical Considerations: Data Storage, Cloud usage and AI Pipeline",2,0.125295,0.111827,"Artificial intelligence (AI), especially deep learning, requires vast amounts
of data for training, testing, and validation. Collecting these data and the
corresponding annotations requires the implementation of imaging biobanks that
provide access to these data in a standardized way. This requires careful
design and implementation based on the current standards and guidelines and
complying with the current legal restrictions. However, the realization of
proper imaging data collections is not sufficient to train, validate and deploy
AI as resource demands are high and require a careful hybrid implementation of
AI pipelines both on-premise and in the cloud. This chapter aims to help the
reader when technical considerations have to be made about the AI environment
by providing a technical background of different concepts and implementation
aspects involved in data storage, cloud usage, and AI pipelines.",0,1,0,0,0,0,0.793515,8.0,0.855803,27
1c1f8232-13fb-4146-96cb-e17a1c4e4100,Ontologically Faithful Generation of Non-Player Character Dialogues,6,0.0922475,0.441543,"We introduce a language generation task grounded in a popular video game
environment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration)
requires models to produce trees of dialogue between video game characters that
accurately reflect quest and entity specifications stated in natural language.
KNUDGE is constructed from side quest dialogues drawn directly from game data
of Obsidian Entertainment's The Outer Worlds, leading to real-world
complexities in generation: (1) dialogues are branching trees as opposed to
linear chains of utterances; (2) utterances must remain faithful to the game
lore -- character personas, backstories, and entity relationships; and (3) a
dialogue must accurately reveal new quest details to the human player. We
report results for a set of neural generation models using supervised and
in-context learning techniques; we find competent performance but room for
future work addressing the challenges of creating realistic, game-quality
dialogues.",0,1,1,1,0,0,0.3951,5.0,0.540557,176
9c179bc9-1325-4690-a242-d64558100041,AttEntropy: Segmenting Unknown Objects in Complex Scenes using the Spatial Attention Entropy of Semantic Segmentation Transformers,2,0.0145562,0.126195,"Vision transformers have emerged as powerful tools for many computer vision
tasks. It has been shown that their features and class tokens can be used for
salient object segmentation. However, the properties of segmentation
transformers remain largely unstudied. In this work we conduct an in-depth
study of the spatial attentions of different backbone layers of semantic
segmentation transformers and uncover interesting properties.
  The spatial attentions of a patch intersecting with an object tend to
concentrate within the object, whereas the attentions of larger, more uniform
image areas rather follow a diffusive behavior. In other words, vision
transformers trained to segment a fixed set of object classes generalize to
objects well beyond this set. We exploit this by extracting heatmaps that can
be used to segment unknown objects within diverse backgrounds, such as
obstacles in traffic scenes.
  Our method is training-free and its computational overhead negligible. We use
off-the-shelf transformers trained for street-scene segmentation to process
other scene types.",0,1,0,0,0,0,0.31901,6.0,0.572331,41
1683067c-f3b7-4124-b2ef-3a51c187ce99,AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation,17,0.232901,0.404982,"Crowdsourced dialogue corpora are usually limited in scale and topic coverage
due to the expensive cost of data curation. This would hinder the
generalization of downstream dialogue models to open-domain topics. In this
work, we leverage large language models for dialogue augmentation in the task
of emotional support conversation (ESC). By treating dialogue augmentation as a
dialogue completion task, we prompt a fine-tuned language model to complete
full dialogues from available dialogue posts of various topics, which are then
postprocessed based on heuristics. Applying this approach, we construct AugESC,
an augmented dataset for the ESC task, which largely extends the scale and
topic coverage of the crowdsourced ESConv corpus. Through comprehensive human
evaluation, we demonstrate that our approach is superior to strong baselines of
dialogue augmentation and that AugESC has comparable dialogue quality to the
crowdsourced corpus. We also conduct human interactive evaluation and prove
that post-training on AugESC improves downstream dialogue models'
generalization ability to open-domain topics. These results suggest the utility
of AugESC and highlight the potential of large language models in improving
data-scarce dialogue generation tasks.",1,1,0,1,0,0,0.892528,3.0,0.730933,55
865156fd-8140-4c33-97b7-92143cfd1e78,Knowledge Unlearning for Mitigating Privacy Risks in Language Models,55,0.652243,0.942763,"Pretrained Language Models (LMs) memorize a vast amount of knowledge during
initial pretraining, including information that may violate the privacy of
personal lives and identities. Previous work addressing privacy issues for
language models has mostly focused on data preprocessing and differential
privacy methods, both requiring re-training the underlying LM. We propose
knowledge unlearning as an alternative method to reduce privacy risks for LMs
post hoc. We show that simply performing gradient ascent on target token
sequences is effective at forgetting them with little to no degradation of
general language modeling performances for larger LMs; it sometimes even
substantially improves the underlying LM with just a few iterations. We also
find that sequential unlearning is better than trying to unlearn all the data
at once and that unlearning is highly dependent on which kind of data (domain)
is forgotten. By showing comparisons with a previous data preprocessing method
and a decoding method known to mitigate privacy risks for LMs, we show that
unlearning can give a stronger empirical privacy guarantee in scenarios where
the data vulnerable to extraction attacks are known a priori while being much
more efficient and robust. We release the code and dataset needed to replicate
our results at https://github.com/joeljang/knowledge-unlearning.",1,1,0,0,0,0,0.913198,5.0,0.85685,55
863ada7c-d3e5-4da4-925d-38b6111d1bef,Hyperbolic Image Segmentation,48,0.199165,0.748847,"For image segmentation, the current standard is to perform pixel-level
optimization and inference in Euclidean output embedding spaces through linear
hyperplanes. In this work, we show that hyperbolic manifolds provide a valuable
alternative for image segmentation and propose a tractable formulation of
hierarchical pixel-level classification in hyperbolic space. Hyperbolic Image
Segmentation opens up new possibilities and practical benefits for
segmentation, such as uncertainty estimation and boundary information for free,
zero-label generalization, and increased performance in low-dimensional output
embeddings.",1,0,0,0,0,0,0.223177,7.0,0.573484,54
9b3786bd-a6ec-4d98-a716-9f7a6ca4f366,ScanNeRF: a Scalable Benchmark for Neural Radiance Fields,11,0.16643,0.378637,"In this paper, we propose the first-ever real benchmark thought for
evaluating Neural Radiance Fields (NeRFs) and, in general, Neural Rendering
(NR) frameworks. We design and implement an effective pipeline for scanning
real objects in quantity and effortlessly. Our scan station is built with less
than 500$ hardware budget and can collect roughly 4000 images of a scanned
object in just 5 minutes. Such a platform is used to build ScanNeRF, a dataset
characterized by several train/val/test splits aimed at benchmarking the
performance of modern NeRF methods under different conditions. Accordingly, we
evaluate three cutting-edge NeRF variants on it to highlight their strengths
and weaknesses. The dataset is available on our project page, together with an
online benchmark to foster the development of better and better NeRFs.",1,1,0,1,0,0,0.982979,4.0,0.948754,54
388ca18f-953e-4ed8-a062-5666cb5df8f6,An Accelerator for Rule Induction in Fuzzy Rough Theory,5,0.0613712,0.561441,"Rule-based classifier, that extract a subset of induced rules to efficiently
learn/mine while preserving the discernibility information, plays a crucial
role in human-explainable artificial intelligence. However, in this era of big
data, rule induction on the whole datasets is computationally intensive. So
far, to the best of our knowledge, no known method focusing on accelerating
rule induction has been reported. This is first study to consider the
acceleration technique to reduce the scale of computation in rule induction. We
propose an accelerator for rule induction based on fuzzy rough theory; the
accelerator can avoid redundant computation and accelerate the building of a
rule classifier. First, a rule induction method based on consistence degree,
called Consistence-based Value Reduction (CVR), is proposed and used as basis
to accelerate. Second, we introduce a compacted search space termed Key Set,
which only contains the key instances required to update the induced rule, to
conduct value reduction. The monotonicity of Key Set ensures the feasibility of
our accelerator. Third, a rule-induction accelerator is designed based on Key
Set, and it is theoretically guaranteed to display the same results as the
unaccelerated version. Specifically, the rank preservation property of Key Set
ensures consistency between the rule induction achieved by the accelerator and
the unaccelerated method. Finally, extensive experiments demonstrate that the
proposed accelerator can perform remarkably faster than the unaccelerated
rule-based classifier methods, especially on datasets with numerous instances.",0,1,0,0,0,0,0.0129574,18.0,0.669499,53
9780a37b-213a-4be1-9953-9340357bd6d0,Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms,27,0.0732068,0.603861,"An increasing number of reports raise concerns about the risk that machine
learning algorithms could amplify health disparities due to biases embedded in
the training data. Seyyed-Kalantari et al. find that models trained on three
chest X-ray datasets yield disparities in false-positive rates (FPR) across
subgroups on the 'no-finding' label (indicating the absence of disease). The
models consistently yield higher FPR on subgroups known to be historically
underserved, and the study concludes that the models exhibit and potentially
even amplify systematic underdiagnosis. We argue that the experimental setup in
the study is insufficient to study algorithmic underdiagnosis. In the absence
of specific knowledge (or assumptions) about the extent and nature of the
dataset bias, it is difficult to investigate model bias. Importantly, their use
of test data exhibiting the same bias as the training data (due to random
splitting) severely complicates the interpretation of the reported disparities.",0,0,0,0,0,0,0.814647,5.0,0.782524,12
316facb3-1943-480e-9602-6f63b22a7f36,Comparison and Evaluation of Methods for a Predict+Optimize Problem in Renewable Energy,3,0.00920706,0.101093,"Algorithms that involve both forecasting and optimization are at the core of
solutions to many difficult real-world problems, such as in supply chains
(inventory optimization), traffic, and in the transition towards carbon-free
energy generation in battery/load/production scheduling in sustainable energy
systems. Typically, in these scenarios we want to solve an optimization problem
that depends on unknown future values, which therefore need to be forecast. As
both forecasting and optimization are difficult problems in their own right,
relatively few research has been done in this area. This paper presents the
findings of the ``IEEE-CIS Technical Challenge on Predict+Optimize for
Renewable Energy Scheduling,"" held in 2021. We present a comparison and
evaluation of the seven highest-ranked solutions in the competition, to provide
researchers with a benchmark problem and to establish the state of the art for
this benchmark, with the aim to foster and facilitate research in this area.
The competition used data from the Monash Microgrid, as well as weather data
and energy market data. It then focused on two main challenges: forecasting
renewable energy production and demand, and obtaining an optimal schedule for
the activities (lectures) and on-site batteries that lead to the lowest cost of
energy. The most accurate forecasts were obtained by gradient-boosted tree and
random forest models, and optimization was mostly performed using mixed integer
linear and quadratic programming. The winning method predicted different
scenarios and optimized over all scenarios jointly using a sample average
approximation method.",1,1,0,0,0,0,0.0831871,6.0,0.324502,69
209df889-3ef7-4461-a95f-fab38d8b0338,Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ),1,0.00127017,0.0205729,"A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed
for image coding in this work. The fundamental idea of image coding is to
remove correlations among pixels before quantization and entropy coding, e.g.,
the discrete cosine transform (DCT) and intra predictions, adopted by modern
image coding standards. We present a new method to remove pixel correlations.
First, by decomposing correlations into long- and short-range correlations, we
represent long-range correlations in coarser grids due to their smoothness,
thus leading to a multi-grid (MG) coding architecture. Second, we show that
short-range correlations can be effectively coded by a suite of vector
quantizers (VQs). Along this line, we argue the effectiveness of VQs of very
large block sizes and present a convenient way to implement them. It is shown
by experimental results that MGBVQ offers excellent rate-distortion (RD)
performance, which is comparable with existing image coders, at much lower
complexity. Besides, it provides a progressive coded bitstream.",0,1,0,0,0,0,0.000115282,19.0,0.438026,34
601fa20a-feaf-4e00-903c-c079c4dfa074,Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge,8,0.167431,0.39421,"We propose a novel open-domain question answering (ODQA) framework for
answering single/multi-hop questions across heterogeneous knowledge sources.
The key novelty of our method is the introduction of the intermediary modules
into the current retriever-reader pipeline. Unlike previous methods that solely
rely on the retriever for gathering all evidence in isolation, our intermediary
performs a chain of reasoning over the retrieved set. Specifically, our method
links the retrieved evidence with its related global context into graphs and
organizes them into a candidate list of evidence chains. Built upon pretrained
language models, our system achieves competitive performance on two ODQA
datasets, OTT-QA and NQ, against tables and passages from Wikipedia. In
particular, our model substantially outperforms the previous state-of-the-art
on OTT-QA with an exact match score of 47.3 (45 % relative gain).",0,1,0,0,1,1,0.784288,6.0,0.803053,48
5a2b7859-176d-49fe-ac26-c2e27737a83e,Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization,18,0.17588,0.459192,"Text summarization aims to generate a short summary for an input text. In
this work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS)
approach, which does not require parallel data for training. Our NAUS first
performs edit-based search towards a heuristically defined score, and generates
a summary as pseudo-groundtruth. Then, we train an encoder-only
non-autoregressive Transformer based on the search result. We also propose a
dynamic programming approach for length-control decoding, which is important
for the summarization task. Experiments on two datasets show that NAUS achieves
state-of-the-art performance for unsupervised summarization, yet largely
improving inference efficiency. Further, our algorithm is able to perform
explicit length-transfer summary generation.",1,1,0,0,1,0,0.552432,5.0,0.634463,55
04da2f46-2906-4971-8ee4-1273c1fafc80,Visual Programming: Compositional visual reasoning without training,195,1.0,1.0,"We present VISPROG, a neuro-symbolic approach to solving complex and
compositional visual tasks given natural language instructions. VISPROG avoids
the need for any task-specific training. Instead, it uses the in-context
learning ability of large language models to generate python-like modular
programs, which are then executed to get both the solution and a comprehensive
and interpretable rationale. Each line of the generated program may invoke one
of several off-the-shelf computer vision models, image processing routines, or
python functions to produce intermediate outputs that may be consumed by
subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4
diverse tasks - compositional visual question answering, zero-shot reasoning on
image pairs, factual knowledge object tagging, and language-guided image
editing. We believe neuro-symbolic approaches like VISPROG are an exciting
avenue to easily and effectively expand the scope of AI systems to serve the
long tail of complex tasks that people may wish to perform.",0,0,0,0,0,0,0.990816,3.0,0.978686,40
4bb6c92a-b7ce-4d29-bd31-5167939da6e8,Local Feature Swapping for Generalization in Reinforcement Learning,10,0.0151797,0.200777,"Over the past few years, the acceleration of computing resources and research
in deep learning has led to significant practical successes in a range of
tasks, including in particular in computer vision. Building on these advances,
reinforcement learning has also seen a leap forward with the emergence of
agents capable of making decisions directly from visual observations. Despite
these successes, the over-parametrization of neural architectures leads to
memorization of the data used during training and thus to a lack of
generalization. Reinforcement learning agents based on visual inputs also
suffer from this phenomenon by erroneously correlating rewards with unrelated
visual features such as background elements. To alleviate this problem, we
introduce a new regularization technique consisting of channel-consistent local
permutations (CLOP) of the feature maps. The proposed permutations induce
robustness to spatial correlations and help prevent overfitting behaviors in
RL. We demonstrate, on the OpenAI Procgen Benchmark, that RL agents trained
with the CLOP method exhibit robustness to visual changes and better
generalization properties than agents trained using other state-of-the-art
regularization techniques. We also demonstrate the effectiveness of CLOP as a
general regularization technique in supervised learning.",1,1,0,0,0,0,0.129743,10.0,0.641704,54
2cedb55c-94bb-4e32-8d1f-1e77a596cc90,Social Construction of XAI: Do We Need One Definition to Rule Them All?,3,0.0281476,0.211605,"There is a growing frustration amongst researchers and developers in
Explainable AI (XAI) around the lack of consensus around what is meant by
'explainability'. Do we need one definition of explainability to rule them all?
In this paper, we argue why a singular definition of XAI is neither feasible
nor desirable at this stage of XAI's development. We view XAI through the
lenses of Social Construction of Technology (SCOT) to explicate how diverse
stakeholders (relevant social groups) have different interpretations
(interpretative flexibility) that shape the meaning of XAI. Forcing a
standardization (closure) on the pluralistic interpretations too early can
stifle innovation and lead to premature conclusions. We share how we can
leverage the pluralism to make progress in XAI without having to wait for a
definitional consensus.",0,0,0,0,0,0,0.558661,8.0,0.7737,16
3a74d742-d8e0-4502-a07e-6b38b6015efe,Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation,27,0.411004,0.264034,"Research on Automatic Story Generation (ASG) relies heavily on human and
automatic evaluation. However, there is no consensus on which human evaluation
criteria to use, and no analysis of how well automatic criteria correlate with
them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a
set of 6 orthogonal and comprehensive human criteria, carefully motivated by
the social sciences literature. We also present HANNA, an annotated dataset of
1,056 stories produced by 10 different ASG systems. HANNA allows us to
quantitatively evaluate the correlations of 72 automatic metrics with human
criteria. Our analysis highlights the weaknesses of current metrics for ASG and
allows us to formulate practical recommendations for ASG evaluation.",1,0,1,1,0,0,0.524613,8.0,0.761793,132
997663a8-fc57-4c4e-93a2-0cd493402be5,Impact of Adversarial Training on Robustness and Generalizability of Language Models,6,0.0122655,0.343122,"Adversarial training is widely acknowledged as the most effective defense
against adversarial attacks. However, it is also well established that
achieving both robustness and generalization in adversarially trained models
involves a trade-off. The goal of this work is to provide an in depth
comparison of different approaches for adversarial training in language models.
Specifically, we study the effect of pre-training data augmentation as well as
training time input perturbations vs. embedding space perturbations on the
robustness and generalization of transformer-based language models. Our
findings suggest that better robustness can be achieved by pre-training data
augmentation or by training with input space perturbation. However, training
with embedding space perturbation significantly improves generalization. A
linguistic correlation analysis of neurons of the learned models reveals that
the improved generalization is due to 'more specialized' neurons. To the best
of our knowledge, this is the first work to carry out a deep qualitative
analysis of different methods of generating adversarial examples in adversarial
training of language models.",0,0,0,0,0,0,0.225978,8.0,0.628574,56
0fc6095f-9e62-45bc-89b9-e4faf4d139a7,Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images,59,0.408968,0.95893,"In this paper, we present a generalizable model-free 6-DoF object pose
estimator called Gen6D. Existing generalizable pose estimators either need
high-quality object models or require additional depth maps or object masks in
test time, which significantly limits their application scope. In contrast, our
pose estimator only requires some posed images of the unseen object and is able
to accurately predict the poses of the object in arbitrary environments. Gen6D
consists of an object detector, a viewpoint selector and a pose refiner, all of
which do not require the 3D object model and can generalize to unseen objects.
Experiments show that Gen6D achieves state-of-the-art results on two model-free
datasets: the MOPED dataset and a new GenMOP dataset collected by us. In
addition, on the LINEMOD dataset, Gen6D achieves competitive results compared
with instance-specific pose estimators. Project page:
https://liuyuan-pal.github.io/Gen6D/.",1,1,0,0,1,0,0.571973,4.0,0.556589,76
8dc74876-ee13-48c0-8175-5e15b77fc3e2,CD$^2$: Fine-grained 3D Mesh Reconstruction With Twice Chamfer Distance,1,0.0177991,0.0496543,"Monocular 3D reconstruction is to reconstruct the shape of object and its
other information from a single RGB image. In 3D reconstruction, polygon mesh,
with detailed surface information and low computational cost, is the most
prevalent expression form obtained from deep learning models. However, the
state-of-the-art schemes fail to directly generate well-structured meshes, and
we identify that most meshes have severe Vertices Clustering (VC) and Illegal
Twist (IT) problems. By analyzing the mesh deformation process, we pinpoint
that the inappropriate usage of Chamfer Distance (CD) loss is a root cause of
VC and IT problems in deep learning model. In this paper, we initially
demonstrate these two problems induced by CD loss with visual examples and
quantitative analyses. Then, we propose a fine-grained reconstruction method
CD$^2$ by employing Chamfer distance twice to perform a plausible and adaptive
deformation. Extensive experiments on two 3D datasets and comparisons with five
latest schemes demonstrate that our CD$^2$ directly generates a well-structured
mesh and outperforms others in terms of several quantitative metrics.",1,1,0,0,1,0,0.435273,8.0,0.728865,55
399eb08a-da16-4e61-9803-90372b72a351,Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification,11,0.0998817,0.278623,"The main question we address in this paper is how to scale up visual
recognition of unseen classes, also known as zero-shot learning, to tens of
thousands of categories as in the ImageNet-21K benchmark. At this scale,
especially with many fine-grained categories included in ImageNet-21K, it is
critical to learn quality visual semantic representations that are
discriminative enough to recognize unseen classes and distinguish them from
seen ones. We propose a \emph{H}ierarchical \emph{G}raphical knowledge
\emph{R}epresentation framework for the confidence-based classification method,
dubbed as HGR-Net. Our experimental results demonstrate that HGR-Net can grasp
class inheritance relations by utilizing hierarchical conceptual knowledge. Our
method significantly outperformed all existing techniques, boosting the
performance by 7\% compared to the runner-up approach on the ImageNet-21K
benchmark. We show that HGR-Net is learning-efficient in few-shot scenarios. We
also analyzed our method on smaller datasets like ImageNet-21K-P, 2-hops and
3-hops, demonstrating its generalization ability. Our benchmark and code are
available at https://kaiyi.me/p/hgrnet.html.",1,1,0,0,1,0,0.599338,9.0,0.81126,42
72b526ca-b3cb-40ff-a25c-cb83796fe360,Proximal PanNet: A Model-Based Deep Network for Pansharpening,8,0.326973,0.594003,"Recently, deep learning techniques have been extensively studied for
pansharpening, which aims to generate a high resolution multispectral (HRMS)
image by fusing a low resolution multispectral (LRMS) image with a high
resolution panchromatic (PAN) image. However, existing deep learning-based
pansharpening methods directly learn the mapping from LRMS and PAN to HRMS.
These network architectures always lack sufficient interpretability, which
limits further performance improvements. To alleviate this issue, we propose a
novel deep network for pansharpening by combining the model-based methodology
with the deep learning method. Firstly, we build an observation model for
pansharpening using the convolutional sparse coding (CSC) technique and design
a proximal gradient algorithm to solve this model. Secondly, we unfold the
iterative algorithm into a deep network, dubbed as Proximal PanNet, by learning
the proximal operators using convolutional neural networks. Finally, all the
learnable modules can be automatically learned in an end-to-end manner.
Experimental results on some benchmark datasets show that our network performs
better than other advanced methods both quantitatively and qualitatively.",0,1,0,0,1,0,0.948857,8.0,0.935022,44
a52dc337-db0b-46d7-a694-5b3bad76867d,An Ultra-low Power TinyML System for Real-time Visual Processing at Edge,7,0.0232147,0.430048,"Tiny machine learning (TinyML), executing AI workloads on resource and power
strictly restricted systems, is an important and challenging topic. This brief
firstly presents an extremely tiny backbone to construct high efficiency CNN
models for various visual tasks. Then, a specially designed neural co-processor
(NCP) is interconnected with MCU to build an ultra-low power TinyML system,
which stores all features and weights on chip and completely removes both of
latency and power consumption in off-chip memory access. Furthermore, an
application specific instruction-set is further presented for realizing agile
development and rapid deployment. Extensive experiments demonstrate that the
proposed TinyML system based on our model, NCP and instruction set yields
considerable accuracy and achieves a record ultra-low power of 160mW while
implementing object detection and recognition at 30FPS. The demo video is
available on \url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}.",0,1,0,0,0,0,0.044392,9.0,0.477609,32
062c2f73-2394-4f33-aba7-fe6f45c13d3b,A Comparative Attention Framework for Better Few-Shot Object Detection on Aerial Images,1,0.00846699,0.0700278,"Few-Shot Object Detection (FSOD) methods are mainly designed and evaluated on
natural image datasets such as Pascal VOC and MS COCO. However, it is not clear
whether the best methods for natural images are also the best for aerial
images. Furthermore, direct comparison of performance between FSOD methods is
difficult due to the wide variety of detection frameworks and training
strategies. Therefore, we propose a benchmarking framework that provides a
flexible environment to implement and compare attention-based FSOD methods. The
proposed framework focuses on attention mechanisms and is divided into three
modules: spatial alignment, global attention, and fusion layer. To remain
competitive with existing methods, which often leverage complex training, we
propose new augmentation techniques designed for object detection. Using this
framework, several FSOD methods are reimplemented and compared. This comparison
highlights two distinct performance regimes on aerial and natural images: FSOD
performs worse on aerial images. Our experiments suggest that small objects,
which are harder to detect in the few-shot setting, account for the poor
performance. Finally, we develop a novel multiscale alignment method,
Cross-Scales Query-Support Alignment (XQSA) for FSOD, to improve the detection
of small objects. XQSA outperforms the state-of-the-art significantly on DOTA
and DIOR.",0,1,0,0,1,0,0.566796,5.0,0.642418,69
065b494f-3151-4262-97d8-9655b9c4a491,Joint Learning Content and Degradation Aware Feature for Blind Super-Resolution,8,0.0509924,0.495167,"To achieve promising results on blind image super-resolution (SR), some
attempts leveraged the low resolution (LR) images to predict the kernel and
improve the SR performance. However, these Supervised Kernel Prediction (SKP)
methods are impractical due to the unavailable real-world blur kernels.
Although some Unsupervised Degradation Prediction (UDP) methods are proposed to
bypass this problem, the \textit{inconsistency} between degradation embedding
and SR feature is still challenging. By exploring the correlations between
degradation embedding and SR feature, we observe that jointly learning the
content and degradation aware feature is optimal. Based on this observation, a
Content and Degradation aware SR Network dubbed CDSR is proposed. Specifically,
CDSR contains three newly-established modules: (1) a Lightweight Patch-based
Encoder (LPE) is applied to jointly extract content and degradation features;
(2) a Domain Query Attention based module (DQA) is employed to adaptively
reduce the inconsistency; (3) a Codebook-based Space Compress module (CSC) that
can suppress the redundant information. Extensive experiments on several
benchmarks demonstrate that the proposed CDSR outperforms the existing UDP
models and achieves competitive performance on PSNR and SSIM even compared with
the state-of-the-art SKP methods.",0,1,0,0,1,0,0.490654,9.0,0.777441,51
44db87e4-2666-475e-92d1-a587e581c4df,User-Controllable Latent Transformer for StyleGAN Image Layout Editing,19,0.160309,0.568531,"Latent space exploration is a technique that discovers interpretable latent
directions and manipulates latent codes to edit various attributes in images
generated by generative adversarial networks (GANs). However, in previous work,
spatial control is limited to simple transformations (e.g., translation and
rotation), and it is laborious to identify appropriate latent directions and
adjust their parameters. In this paper, we tackle the problem of editing the
StyleGAN image layout by annotating the image directly. To do so, we propose an
interactive framework for manipulating latent codes in accordance with the user
inputs. In our framework, the user annotates a StyleGAN image with locations
they want to move or not and specifies a movement direction by mouse dragging.
From these user inputs and initial latent codes, our latent transformer based
on a transformer encoder-decoder architecture estimates the output latent
codes, which are fed to the StyleGAN generator to obtain a result image. To
train our latent transformer, we utilize synthetic data and pseudo-user inputs
generated by off-the-shelf StyleGAN and optical flow models, without manual
supervision. Quantitative and qualitative evaluations demonstrate the
effectiveness of our method over existing methods.",0,1,1,0,0,0,0.840739,5.0,0.799762,42
7e6b71f6-ba55-49a1-964a-07c41840a022,Physical Reasoning in an Open World,2,0.0512679,0.0683882,"Most work on physical reasoning, both in artificial intelligence and in
cognitive science, has focused on closed-world reasoning, in which it is
assumed that the problem specification specifies all relevant objects and
substance, all their relations in an initial situation, and all exogenous
events. However, in many situations, it is important to do open-world
reasoning; that is, making valid conclusions from very incomplete information.
We have implemented in Prolog an open-world reasoner for a toy microworld of
containers that can be loaded, unloaded, sealed, unsealed, carried, and dumped.",0,0,0,0,0,1,0.00908753,26.0,0.757472,34
98f2b81e-b85a-4f10-968f-243c8ad62715,NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization,2,0.0184812,0.326546,"Narrative summarization aims to produce a distilled version of a narrative to
describe its most salient events and characters. Summarizing a narrative is
challenging as it requires an understanding of event causality and character
behaviors. To encourage research in this direction, we propose NarraSum, a
large-scale narrative summarization dataset. It contains 122K narrative
documents, which are collected from plot descriptions of movies and TV episodes
with diverse genres, and their corresponding abstractive summaries. Experiments
show that there is a large performance gap between humans and the
state-of-the-art summarization models on NarraSum. We hope that this dataset
will promote future research in summarization, as well as broader studies of
natural language understanding and generation. The dataset is available at
https://github.com/zhaochaocs/narrasum.",0,0,1,1,0,0,0.463401,7.0,0.702365,70
19f9a8f2-418d-416c-bf70-53ac808867d5,Deep Surrogate Assisted Generation of Environments,22,0.0870289,0.795734,"Recent progress in reinforcement learning (RL) has started producing
generally capable agents that can solve a distribution of complex environments.
These agents are typically tested on fixed, human-authored environments. On the
other hand, quality diversity (QD) optimization has been proven to be an
effective component of environment generation algorithms, which can generate
collections of high-quality environments that are diverse in the resulting
agent behaviors. However, these algorithms require potentially expensive
simulations of agents on newly generated environments. We propose Deep
Surrogate Assisted Generation of Environments (DSAGE), a sample-efficient QD
environment generation algorithm that maintains a deep surrogate model for
predicting agent behaviors in new environments. Results in two benchmark
domains show that DSAGE significantly outperforms existing QD environment
generation algorithms in discovering collections of environments that elicit
diverse behaviors of a state-of-the-art RL agent and a planning agent. Our
source code and videos are available at https://dsagepaper.github.io/.",1,1,0,0,0,0,0.12127,6.0,0.390795,118
9c73aaa2-6902-4875-a777-9e046f7fcef1,Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation,14,0.0341661,0.644137,"Most dominant neural machine translation (NMT) models are restricted to make
predictions only according to the local context of preceding words in a
left-to-right manner. Although many previous studies try to incorporate global
information into NMT models, there still exist limitations on how to
effectively exploit bidirectional global context. In this paper, we propose a
Confidence Based Bidirectional Global Context Aware (CBBGCA) training framework
for NMT, where the NMT model is jointly trained with an auxiliary conditional
masked language model (CMLM). The training consists of two stages: (1)
multi-task joint training; (2) confidence based knowledge distillation. At the
first stage, by sharing encoder parameters, the NMT model is additionally
supervised by the signal from the CMLM decoder that contains bidirectional
global contexts. Moreover, at the second stage, using the CMLM as teacher, we
further pertinently incorporate bidirectional global context to the NMT model
on its unconfidently-predicted target words via knowledge distillation.
Experimental results show that our proposed CBBGCA training framework
significantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on
three large-scale translation datasets, namely WMT'14 English-to-German, WMT'19
Chinese-to-English and WMT'14 English-to-French, respectively.",0,0,0,0,1,1,0.142512,7.0,0.502589,37
1c0567b3-051b-4cb8-9135-c65cc38b5808,On the Effectiveness of Parameter-Efficient Fine-Tuning,51,0.714659,0.898387,"Fine-tuning pre-trained models has been ubiquitously proven to be effective
in a wide range of NLP tasks. However, fine-tuning the whole model is parameter
inefficient as it always yields an entirely new model for each task. Currently,
many research works propose to only fine-tune a small portion of the parameters
while keeping most of the parameters shared across different tasks. These
methods achieve surprisingly good performance and are shown to be more stable
than their corresponding fully fine-tuned counterparts. However, such kind of
methods is still not well understood. Some natural questions arise: How does
the parameter sparsity lead to promising performance? Why is the model more
stable than the fully fine-tuned models? How to choose the tunable parameters?
In this paper, we first categorize the existing methods into random approaches,
rule-based approaches, and projection-based approaches based on how they choose
which parameters to tune. Then, we show that all of the methods are actually
sparse fine-tuned models and conduct a novel theoretical analysis of them. We
indicate that the sparsity is actually imposing a regularization on the
original model by controlling the upper bound of the stability. Such stability
leads to better generalization capability which has been empirically observed
in a lot of recent research works. Despite the effectiveness of sparsity
grounded by our theory, it still remains an open problem of how to choose the
tunable parameters. To better choose the tunable parameters, we propose a novel
Second-order Approximation Method (SAM) which approximates the original problem
with an analytically solvable optimization function. The tunable parameters are
determined by directly optimizing the approximation function. The experimental
results show that our proposed SAM model outperforms many strong baseline
models and it also verifies our theoretical analysis.",1,0,0,0,0,0,0.947984,6.0,0.912412,91
8ef91d02-0e53-4533-a17c-65d5dcb35658,Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities,29,0.474136,0.951009,"With the advent of large language models, methods for abstractive
summarization have made great strides, creating potential for use in
applications to aid knowledge workers processing unwieldy document collections.
One such setting is the Civil Rights Litigation Clearinghouse (CRLC)
(https://clearinghouse.net),which posts information about large-scale civil
rights lawsuits, serving lawyers, scholars, and the general public. Today,
summarization in the CRLC requires extensive training of lawyers and law
students who spend hours per case understanding multiple relevant documents in
order to produce high-quality summaries of key events and outcomes. Motivated
by this ongoing real-world summarization effort, we introduce Multi-LexSum, a
collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.
Multi-LexSum presents a challenging multi-document summarization task given the
length of the source documents, often exceeding two hundred pages per case.
Furthermore, Multi-LexSum is distinct from other datasets in its multiple
target summaries, each at a different granularity (ranging from one-sentence
""extreme"" summaries to multi-paragraph narrations of over five hundred words).
We present extensive analysis demonstrating that despite the high-quality
summaries in the training data (adhering to strict content and style
guidelines), state-of-the-art summarization models perform poorly on this task.
We release Multi-LexSum for further research in summarization methods as well
as to facilitate development of applications to assist in the CRLC's mission at
https://multilexsum.github.io.",1,1,0,1,0,0,0.826877,6.0,0.825386,80
4ba4bf3c-fca3-41ea-a99d-6dde1ca4b2cd,"Perceive, Interact, Predict: Learning Dynamic and Static Clues for End-to-End Motion Prediction",14,0.0938009,0.828319,"Motion prediction is highly relevant to the perception of dynamic objects and
static map elements in the scenarios of autonomous driving. In this work, we
propose PIP, the first end-to-end Transformer-based framework which jointly and
interactively performs online mapping, object detection and motion prediction.
PIP leverages map queries, agent queries and mode queries to encode the
instance-wise information of map elements, agents and motion intentions,
respectively. Based on the unified query representation, a differentiable
multi-task interaction scheme is proposed to exploit the correlation between
perception and prediction. Even without human-annotated HD map or agent's
historical tracking trajectory as guidance information, PIP realizes end-to-end
multi-agent motion prediction and achieves better performance than
tracking-based and HD-map-based methods. PIP provides comprehensive high-level
information of the driving scene (vectorized static map and dynamic objects
with motion information), and contributes to the downstream planning and
control. Code and models will be released for facilitating further research.",0,1,0,0,1,0,0.828475,5.0,0.791519,40
8fb12ddd-8917-455e-addc-eeafec510a9d,Tutorial on Course-of-Action (COA) Attack Search Methods in Computer Networks,1,0.0116876,0.149845,"In the literature of modern network security research, deriving effective and
efficient course-of-action (COA) attach search methods are of interests in
industry and academia. As the network size grows, the traditional COA attack
search methods can suffer from the limitations to computing and communication
resources. Therefore, various methods have been developed to solve these
problems, and reinforcement learning (RL)-based intelligent algorithms are one
of the most effective solutions. Therefore, we review the RL-based COA attack
search methods for network attack scenarios in terms of the trends and their
contrib",0,0,0,0,0,0,0.113346,7.0,0.467542,34
7f9d10c7-71ba-4c98-b975-9fb711893cd9,Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography,13,0.304905,0.416418,"Although robust PCA has been increasingly adopted to extract vessels from
X-ray coronary angiography (XCA) images, challenging problems such as
inefficient vessel-sparsity modelling, noisy and dynamic background artefacts,
and high computational cost still remain unsolved. Therefore, we propose a
novel robust PCA unrolling network with sparse feature selection for
super-resolution XCA vessel imaging. Being embedded within a patch-wise
spatiotemporal super-resolution framework that is built upon a pooling layer
and a convolutional long short-term memory network, the proposed network can
not only gradually prune complex vessel-like artefacts and noisy backgrounds in
XCA during network training but also iteratively learn and select the
high-level spatiotemporal semantic information of moving contrast agents
flowing in the XCA-imaged vessels. The experimental results show that the
proposed method significantly outperforms state-of-the-art methods, especially
in the imaging of the vessel network and its distal vessels, by restoring the
intensity and geometry profiles of heterogeneous vessels against complex and
dynamic backgrounds.",0,1,0,0,1,0,0.383873,7.0,0.666502,73
cb1bd259-ced4-4175-b2cb-ec01e7c0b353,A Pixel-based Encryption Method for Privacy-Preserving Deep Learning Models,4,0.0928021,0.132686,"In the recent years, pixel-based perceptual algorithms have been successfully
applied for privacy-preserving deep learning (DL) based applications. However,
their security has been broken in subsequent works by demonstrating a
chosen-plaintext attack. In this paper, we propose an efficient pixel-based
perceptual encryption method. The method provides a necessary level of security
while preserving the intrinsic properties of the original image. Thereby, can
enable deep learning (DL) applications in the encryption domain. The method is
substitution based where pixel values are XORed with a sequence (as opposed to
a single value used in the existing methods) generated by a chaotic map. We
have used logistic maps for their low computational requirements. In addition,
to compensate for any inefficiency because of the logistic maps, we use a
second key to shuffle the sequence. We have compared the proposed method in
terms of encryption efficiency and classification accuracy of the DL models on
them. We have validated the proposed method with CIFAR datasets. The analysis
shows that when classification is performed on the cipher images, the model
preserves accuracy of the existing methods while provides better security.",0,1,0,0,0,0,0.745601,7.0,0.814934,12
26357719-a5f1-4da0-88b9-02a83648951b,On The Robustness of Offensive Language Classifiers,6,0.120806,0.354832,"Social media platforms are deploying machine learning based offensive
language classification systems to combat hateful, racist, and other forms of
offensive speech at scale. However, despite their real-world deployment, we do
not yet comprehensively understand the extent to which offensive language
classifiers are robust against adversarial attacks. Prior work in this space is
limited to studying robustness of offensive language classifiers against
primitive attacks such as misspellings and extraneous spaces. To address this
gap, we systematically analyze the robustness of state-of-the-art offensive
language classifiers against more crafty adversarial attacks that leverage
greedy- and attention-based word selection and context-aware embeddings for
word replacement. Our results on multiple datasets show that these crafty
adversarial attacks can degrade the accuracy of offensive language classifiers
by more than 50% while also being able to preserve the readability and meaning
of the modified text.",0,1,0,0,0,0,0.836843,7.0,0.855081,40
ab5cb449-958d-4869-ac07-f940be12e3a5,Contrast-Phys: Unsupervised Video-based Remote Physiological Measurement via Spatiotemporal Contrast,41,0.291053,0.999793,"Video-based remote physiological measurement utilizes face videos to measure
the blood volume change signal, which is also called remote
photoplethysmography (rPPG). Supervised methods for rPPG measurements achieve
state-of-the-art performance. However, supervised rPPG methods require face
videos and ground truth physiological signals for model training. In this
paper, we propose an unsupervised rPPG measurement method that does not require
ground truth signals for training. We use a 3DCNN model to generate multiple
rPPG signals from each video in different spatiotemporal locations and train
the model with a contrastive loss where rPPG signals from the same video are
pulled together while those from different videos are pushed away. We test on
five public datasets, including RGB videos and NIR videos. The results show
that our method outperforms the previous unsupervised baseline and achieves
accuracies very close to the current best supervised rPPG methods on all five
datasets. Furthermore, we also demonstrate that our approach can run at a much
faster speed and is more robust to noises than the previous unsupervised
baseline. Our code is available at
https://github.com/zhaodongsun/contrast-phys.",1,1,0,0,1,0,0.330909,9.0,0.719872,58
b068b793-141e-4c0e-98f2-2e013315ad31,Code-DKT: A Code-based Knowledge Tracing Model for Programming Tasks,9,0.203766,0.66959,"Knowledge tracing (KT) models are a popular approach for predicting students'
future performance at practice problems using their prior attempts. Though many
innovations have been made in KT, most models including the state-of-the-art
Deep KT (DKT) mainly leverage each student's response either as correct or
incorrect, ignoring its content. In this work, we propose Code-based Deep
Knowledge Tracing (Code-DKT), a model that uses an attention mechanism to
automatically extract and select domain-specific code features to extend DKT.
We compared the effectiveness of Code-DKT against Bayesian and Deep Knowledge
Tracing (BKT and DKT) on a dataset from a class of 50 students attempting to
solve 5 introductory programming assignments. Our results show that Code-DKT
consistently outperforms DKT by 3.07-4.00% AUC across the 5 assignments, a
comparable improvement to other state-of-the-art domain-general KT models over
DKT. Finally, we analyze problem-specific performance through a set of case
studies for one assignment to demonstrate when and how code features improve
Code-DKT's predictions.",0,1,0,0,1,0,0.211264,11.0,0.72293,63
4ef24504-4329-41f6-8e85-7821a87842ce,Better Smatch = Better Parser? AMR evaluation is not so simple anymore,9,0.0940353,0.665558,"Recently, astonishing advances have been observed in AMR parsing, as measured
by the structural Smatch metric. In fact, today's systems achieve performance
levels that seem to surpass estimates of human inter annotator agreement (IAA).
Therefore, it is unclear how well Smatch (still) relates to human estimates of
parse quality, as in this situation potentially fine-grained errors of similar
weight may impact the AMR's meaning to different degrees.
  We conduct an analysis of two popular and strong AMR parsers that --
according to Smatch -- reach quality levels on par with human IAA, and assess
how human quality ratings relate to Smatch and other AMR metrics. Our main
findings are: i) While high Smatch scores indicate otherwise, we find that AMR
parsing is far from being solved: we frequently find structurally small, but
semantically unacceptable errors that substantially distort sentence meaning.
ii) Considering high-performance parsers, better Smatch scores may not
necessarily indicate consistently better parsing quality. To obtain a
meaningful and comprehensive assessment of quality differences of parse(r)s, we
recommend augmenting evaluations with macro statistics, use of additional
metrics, and more human analysis.",1,0,0,0,0,0,0.213868,7.0,0.566581,42
e5ff6a49-ee97-4bbe-a67c-e4b31b8a6a1d,Popularity Bias in Collaborative Filtering-Based Multimedia Recommender Systems,10,0.225562,0.228374,"Multimedia recommender systems suggest media items, e.g., songs, (digital)
books and movies, to users by utilizing concepts of traditional recommender
systems such as collaborative filtering. In this paper, we investigate a
potential issue of such collaborative-filtering based multimedia recommender
systems, namely popularity bias that leads to the underrepresentation of
unpopular items in the recommendation lists. Therefore, we study four
multimedia datasets, i.e., LastFm, MovieLens, BookCrossing and MyAnimeList,
that we each split into three user groups differing in their inclination to
popularity, i.e., LowPop, MedPop and HighPop. Using these user groups, we
evaluate four collaborative filtering-based algorithms with respect to
popularity bias on the item and the user level. Our findings are three-fold:
firstly, we show that users with little interest into popular items tend to
have large user profiles and thus, are important data sources for multimedia
recommender systems. Secondly, we find that popular items are recommended more
frequently than unpopular ones. Thirdly, we find that users with little
interest into popular items receive significantly worse recommendations than
users with medium or high interest into popularity.",0,1,0,0,0,0,0.631525,10.0,0.838894,25
4c432962-725e-4425-94d8-597bcb66e101,Approaching Reflex Predictions as a Classification Problem Using Extended Phonological Alignments,1,0.00138299,0.0141751,"This work describes an implementation of the ""extended alignment"" (or
""multitiers"") approach for cognate reflex prediction, submitted to ""Prediction
of Cognate Reflexes"" shared task. Similarly to List2022d, the technique
involves an automatic extension of sequence alignments with multilayered
vectors that encode informational tiers on both site-specific traits, such as
sound classes and distinctive features, as well as contextual and
suprasegmental ones, conveyed by cross-site referrals and replication. The
method allows to generalize the problem of cognate reflex prediction as a
classification problem, with models trained using a parallel corpus of cognate
sets. A model using random forests is trained and evaluated on the shared task
for reflex prediction, and the experimental results are presented and discussed
along with some differences to other implementations.",1,1,0,0,0,0,0.00293383,11.0,0.323689,56
38afa70e-004b-455b-a6f6-c9fd11515a11,Solving Math Word Problems via Cooperative Reasoning induced Language Models,31,0.459917,0.975062,"Large-scale pre-trained language models (PLMs) bring new opportunities to
challenging problems, especially those that need high-level intelligence, such
as the math word problem (MWPs). However, directly applying existing PLMs to
MWPs can fail as the generation process lacks sufficient supervision and thus
lacks fast adaptivity as humans. We notice that human reasoning has a dual
reasoning framework that consists of an immediate reaction system (system 1)
and a delicate reasoning system (system 2), where the entire reasoning is
determined by their interaction. This inspires us to develop a cooperative
reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),
resulting in a human-like reasoning architecture with system 1 as the generator
and system 2 as the verifier. In our approach, the generator is responsible for
generating reasoning paths, and the verifiers are used to supervise the
evaluation in order to obtain reliable feedback for the generator. We evaluate
our CoRe framework on several mathematical reasoning datasets and achieve
decent improvement over state-of-the-art methods, up to 9.6% increase over best
baselines. Our codes are available at https://github.com/TianHongZXY/CoRe",0,0,0,0,0,0,0.94425,3.0,0.816913,41
3f9b1714-323c-4f4f-ae34-97fbba49f5a9,FvOR: Robust Joint Shape and Pose Optimization for Few-view Object Reconstruction,19,0.152138,0.816554,"Reconstructing an accurate 3D object model from a few image observations
remains a challenging problem in computer vision. State-of-the-art approaches
typically assume accurate camera poses as input, which could be difficult to
obtain in realistic settings. In this paper, we present FvOR, a learning-based
object reconstruction method that predicts accurate 3D models given a few
images with noisy input poses. The core of our approach is a fast and robust
multi-view reconstruction algorithm to jointly refine 3D geometry and camera
pose estimation using learnable neural network modules. We provide a thorough
benchmark of state-of-the-art approaches for this problem on ShapeNet. Our
approach achieves best-in-class results. It is also two orders of magnitude
faster than the recent optimization-based approach IDR. Our code is released at
\url{https://github.com/zhenpeiyang/FvOR/}",1,1,0,0,1,0,0.746914,9.0,0.856479,73
f540e077-c824-4f70-8646-b4a397ef2c91,End-to-end model for named entity recognition from speech without paired training data,10,0.0661938,0.753686,"Recent works showed that end-to-end neural approaches tend to become very
popular for spoken language understanding (SLU). Through the term end-to-end,
one considers the use of a single model optimized to extract semantic
information directly from the speech signal. A major issue for such models is
the lack of paired audio and textual data with semantic annotation. In this
paper, we propose an approach to build an end-to-end neural model to extract
semantic information in a scenario in which zero paired audio data is
available. Our approach is based on the use of an external model trained to
generate a sequence of vectorial representations from text. These
representations mimic the hidden representations that could be generated inside
an end-to-end automatic speech recognition (ASR) model by processing a speech
signal. An SLU neural module is then trained using these representations as
input and the annotated text as output. Last, the SLU module replaces the top
layers of the ASR model to achieve the construction of the end-to-end model.
Our experiments on named entity recognition, carried out on the QUAERO corpus,
show that this approach is very promising, getting better results than a
comparable cascade approach or than the use of synthetic voices.",0,1,0,0,0,0,0.181147,6.0,0.463397,26
26f6fb75-ca54-475d-8e1c-1024cd6a49f1,Split-U-Net: Preventing Data Leakage in Split Learning for Collaborative Multi-Modal Brain Tumor Segmentation,7,0.0351241,0.206661,"Split learning (SL) has been proposed to train deep learning models in a
decentralized manner. For decentralized healthcare applications with vertical
data partitioning, SL can be beneficial as it allows institutes with
complementary features or images for a shared set of patients to jointly
develop more robust and generalizable models. In this work, we propose
""Split-U-Net"" and successfully apply SL for collaborative biomedical image
segmentation. Nonetheless, SL requires the exchanging of intermediate
activation maps and gradients to allow training models across different feature
spaces, which might leak data and raise privacy concerns. Therefore, we also
quantify the amount of data leakage in common SL scenarios for biomedical image
segmentation and provide ways to counteract such leakage by applying
appropriate defense strategies.",0,1,0,0,0,0,0.636533,5.0,0.680511,33
62816451-a94c-4650-9766-056a03421a59,Learning to Decompose Visual Features with Latent Textual Prompts,16,0.121428,0.783576,"Recent advances in pre-training vision-language models like CLIP have shown
great potential in learning transferable visual representations. Nonetheless,
for downstream inference, CLIP-like models suffer from either 1) degraded
accuracy and robustness in the case of inaccurate text descriptions during
retrieval-based inference (the challenge for zero-shot protocol); or 2)
breaking the well-established vision-language alignment (the challenge for
linear probing). To address them, we propose Decomposed Feature Prompting
(DeFo). DeFo leverages a flexible number of learnable embeddings as textual
input while maintaining the vision-language dual-model architecture, which
enables the model to learn decomposed visual features with the help of
feature-level textual prompts. We further use an additional linear layer to
perform classification, allowing a scalable size of language inputs. Our
empirical study shows DeFo's significance in improving the vision-language
models. For example, DeFo obtains 73.2% test accuracy on ImageNet with a
ResNet-50 backbone without tuning any pretrained weights of both the vision and
language encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and
outperforming state-of-the-art vision-language prompt tuning method by 7.6%.",0,1,0,0,1,0,0.924782,5.0,0.86824,43
2ef5283c-bb6c-4412-8f33-aae9eb79a72b,"Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus",15,0.170974,0.492881,"The Abstraction and Reasoning Corpus (ARC) aims at benchmarking the
performance of general artificial intelligence algorithms. The ARC's focus on
broad generalization and few-shot learning has made it difficult to solve using
pure machine learning. A more promising approach has been to perform program
synthesis within an appropriately designed Domain Specific Language (DSL).
However, these too have seen limited success. We propose Abstract Reasoning
with Graph Abstractions (ARGA), a new object-centric framework that first
represents images using graphs and then performs a search for a correct program
in a DSL that is based on the abstracted graph space. The complexity of this
combinatorial search is tamed through the use of constraint acquisition, state
hashing, and Tabu search. An extensive set of experiments demonstrates the
promise of ARGA in tackling some of the complicated object-centric tasks of the
ARC rather efficiently, producing programs that are correct and easy to
understand.",0,0,0,0,0,0,0.000259131,19.0,0.480659,28
1f04710e-f525-4e95-b50a-465dfe0ae253,Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions,12,0.0918349,0.575186,"Pruning is one of the predominant approaches for compressing deep neural
networks (DNNs). Lately, coresets (provable data summarizations) were leveraged
for pruning DNNs, adding the advantage of theoretical guarantees on the
trade-off between the compression rate and the approximation error. However,
coresets in this domain were either data-dependent or generated under
restrictive assumptions on both the model's weights and inputs. In real-world
scenarios, such assumptions are rarely satisfied, limiting the applicability of
coresets. To this end, we suggest a novel and robust framework for computing
such coresets under mild assumptions on the model's weights and without any
assumption on the training data. The idea is to compute the importance of each
neuron in each layer with respect to the output of the following layer. This is
achieved by a combination of L\""{o}wner ellipsoid and Caratheodory theorem. Our
method is simultaneously data-independent, applicable to various networks and
datasets (due to the simplified assumptions), and theoretically supported.
Experimental results show that our method outperforms existing coreset based
neural pruning approaches across a wide range of networks and datasets. For
example, our method achieved a $62\%$ compression rate on ResNet50 on ImageNet
with $1.09\%$ drop in accuracy.",0,1,0,0,0,0,0.194245,10.0,0.685797,128
807ebd9c-b0e7-4b2b-84f6-707e00661ab7,Learning One Abstract Bit at a Time Through Self-Invented Experiments Encoded as Neural Networks,2,0.00809994,0.0618876,"There are two important things in science: (A) Finding answers to given
questions, and (B) Coming up with good questions. Our artificial scientists not
only learn to answer given questions, but also continually invent new
questions, by proposing hypotheses to be verified or falsified through
potentially complex and time-consuming experiments, including thought
experiments akin to those of mathematicians. While an artificial scientist
expands its knowledge, it remains biased towards the simplest, least costly
experiments that still have surprising outcomes, until they become boring. We
present an empirical analysis of the automatic generation of interesting
experiments. In the first setting, we investigate self-invented experiments in
a reinforcement-providing environment and show that they lead to effective
exploration. In the second setting, pure thought experiments are implemented as
the weights of recurrent neural networks generated by a neural experiment
generator. Initially interesting thought experiments may become boring over
time.",0,0,0,0,0,0,9.77768e-05,23.0,0.5286,85
f892b203-175f-4eb1-86e7-c19bdd9f7f7f,Confidence estimation of classification based on the distribution of the neural network output layer,8,0.0427553,0.365625,"One of the most common problems preventing the application of prediction
models in the real world is lack of generalization: The accuracy of models,
measured in the benchmark does repeat itself on future data, e.g. in the
settings of real business. There is relatively little methods exist that
estimate the confidence of prediction models. In this paper, we propose novel
methods that, given a neural network classification model, estimate uncertainty
of particular predictions generated by this model. Furthermore, we propose a
method that, given a model and a confidence level, calculates a threshold that
separates prediction generated by this model into two subsets, one of them
meets the given confidence level. In contrast to other methods, the proposed
methods do not require any changes on existing neural networks, because they
simply build on the output logit layer of a common neural network. In
particular, the methods infer the confidence of a particular prediction based
on the distribution of the logit values corresponding to this prediction. The
proposed methods constitute a tool that is recommended for filtering
predictions in the process of knowledge extraction, e.g. based on web
scrapping, where predictions subsets are identified that maximize the precision
on cost of the recall, which is less important due to the availability of data.
The method has been tested on different tasks including relation extraction,
named entity recognition and image classification to show the significant
increase of accuracy achieved.",0,1,0,0,0,0,0.0557159,10.0,0.55316,23
06e3f07c-2061-452e-94ab-e375c75a28c4,CADOps-Net: Jointly Learning CAD Operation Types and Steps from Boundary-Representations,5,0.202689,0.587785,"3D reverse engineering is a long sought-after, yet not completely achieved
goal in the Computer-Aided Design (CAD) industry. The objective is to recover
the construction history of a CAD model. Starting from a Boundary
Representation (B-Rep) of a CAD model, this paper proposes a new deep neural
network, CADOps-Net, that jointly learns the CAD operation types and the
decomposition into different CAD operation steps. This joint learning allows to
divide a B-Rep into parts that were created by various types of CAD operations
at the same construction step; therefore providing relevant information for
further recovery of the design history. Furthermore, we propose the novel
CC3D-Ops dataset that includes over $37k$ CAD models annotated with CAD
operation type labels and step labels. Compared to existing datasets, the
complexity and variety of CC3D-Ops models are closer to those used for
industrial purposes. Our experiments, conducted on the proposed CC3D-Ops and
the publicly available Fusion360 datasets, demonstrate the competitive
performance of CADOps-Net with respect to state-of-the-art, and confirm the
importance of the joint learning of CAD operation types and steps.",0,1,1,1,1,0,0.532207,8.0,0.764471,49
48e2481f-ff52-4c83-8bf2-88b965db5749,What do tokens know about their characters and how do they know it?,16,0.256042,0.873198,"Pre-trained language models (PLMs) that use subword tokenization schemes can
succeed at a variety of language tasks that require character-level
information, despite lacking explicit access to the character composition of
tokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe),
we probe what word pieces encode about character-level information by training
classifiers to predict the presence or absence of a particular alphabetical
character in a token, based on its embedding (e.g., probing whether the model
embedding for ""cat"" encodes that it contains the character ""a""). We find that
these models robustly encode character-level information and, in general,
larger models perform better at the task. We show that these results generalize
to characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic).
Then, through a series of experiments and analyses, we investigate the
mechanisms through which PLMs acquire English-language character information
during training and argue that this knowledge is acquired through multiple
phenomena, including a systematic relationship between particular characters
and particular parts of speech, as well as natural variability in the
tokenization of related strings.",1,0,0,0,0,1,0.521753,8.0,0.760781,83
4a6be4fa-463a-499d-87b7-19a1a4854d59,Deepfake Video Detection with Spatiotemporal Dropout Transformer,18,0.610413,0.716289,"While the abuse of deepfake technology has caused serious concerns recently,
how to detect deepfake videos is still a challenge due to the high
photo-realistic synthesis of each frame. Existing image-level approaches often
focus on single frame and ignore the spatiotemporal cues hidden in deepfake
videos, resulting in poor generalization and robustness. The key of a
video-level detector is to fully exploit the spatiotemporal inconsistency
distributed in local facial regions across different frames in deepfake videos.
Inspired by that, this paper proposes a simple yet effective patch-level
approach to facilitate deepfake video detection via spatiotemporal dropout
transformer. The approach reorganizes each input video into bag of patches that
is then fed into a vision transformer to achieve robust representation.
Specifically, a spatiotemporal dropout operation is proposed to fully explore
patch-level spatiotemporal cues and serve as effective data augmentation to
further enhance model's robustness and generalization ability. The operation is
flexible and can be easily plugged into existing vision transformers. Extensive
experiments demonstrate the effectiveness of our approach against 25
state-of-the-arts with impressive robustness, generalizability, and
representation ability.",1,1,0,0,1,0,0.987808,6.0,0.97896,52
3ef7e241-f643-4f84-afcb-5e785fc8969c,Don't Say What You Don't Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search,24,0.148498,0.684194,"Abstractive summarization systems today produce fluent and relevant output,
but often ""hallucinate"" statements not supported by the source text. We analyze
the connection between hallucinations and training data, and find evidence that
models hallucinate because they train on target summaries that are unsupported
by the source. Based on our findings, we present PINOCCHIO, a new decoding
method that improves the consistency of a transformer-based abstractive
summarizer by constraining beam search to avoid hallucinations. Given the model
states and outputs at a given step, PINOCCHIO detects likely model
hallucinations based on various measures of attribution to the source text.
PINOCCHIO backtracks to find more consistent output, and can opt to produce no
summary at all when no consistent generation can be found. In experiments, we
find that PINOCCHIO improves the consistency of generation (in terms of F1) by
an average of~67% on two abstractive summarization datasets.",1,1,0,1,0,1,0.633996,5.0,0.679132,47
26e5c346-b203-4a99-a157-8b45c6a2c1f0,End-to-End Trajectory Distribution Prediction Based on Occupancy Grid Maps,16,0.452171,0.487497,"In this paper, we aim to forecast a future trajectory distribution of a
moving agent in the real world, given the social scene images and historical
trajectories. Yet, it is a challenging task because the ground-truth
distribution is unknown and unobservable, while only one of its samples can be
applied for supervising model learning, which is prone to bias. Most recent
works focus on predicting diverse trajectories in order to cover all modes of
the real distribution, but they may despise the precision and thus give too
much credit to unrealistic predictions. To address the issue, we learn the
distribution with symmetric cross-entropy using occupancy grid maps as an
explicit and scene-compliant approximation to the ground-truth distribution,
which can effectively penalize unlikely predictions. In specific, we present an
inverse reinforcement learning based multi-modal trajectory distribution
forecasting framework that learns to plan by an approximate value iteration
network in an end-to-end manner. Besides, based on the predicted distribution,
we generate a small set of representative trajectories through a differentiable
Transformer-based network, whose attention mechanism helps to model the
relations of trajectories. In experiments, our method achieves state-of-the-art
performance on the Stanford Drone Dataset and Intersection Drone Dataset.",1,0,0,0,0,0,0.949822,6.0,0.914427,57
360cabc9-2682-4871-bc93-2d825b9df956,An integrated Auto Encoder-Block Switching defense approach to prevent adversarial attacks,4,0.0290091,0.187896,"According to recent studies, the vulnerability of state-of-the-art Neural
Networks to adversarial input samples has increased drastically. A neural
network is an intermediate path or technique by which a computer learns to
perform tasks using Machine learning algorithms. Machine Learning and
Artificial Intelligence model has become a fundamental aspect of life, such as
self-driving cars [1], smart home devices, so any vulnerability is a
significant concern. The smallest input deviations can fool these extremely
literal systems and deceive their users as well as administrator into
precarious situations. This article proposes a defense algorithm that utilizes
the combination of an auto-encoder [3] and block-switching architecture.
Auto-coder is intended to remove any perturbations found in input images
whereas the block switching method is used to make it more robust against
White-box attacks. The attack is planned using FGSM [9] model, and the
subsequent counter-attack by the proposed architecture will take place thereby
demonstrating the feasibility and security delivered by the algorithm.",0,1,0,0,0,0,0.203553,5.0,0.382075,27
661dbd9c-7ae4-431c-a58c-375dea35180c,Tele-Knowledge Pre-training for Fault Analysis,9,0.126043,0.543068,"In this work, we share our experience on tele-knowledge pre-training for
fault analysis, a crucial task in telecommunication applications that requires
a wide range of knowledge normally found in both machine log data and product
documents. To organize this knowledge from experts uniformly, we propose to
create a Tele-KG (tele-knowledge graph). Using this valuable data, we further
propose a tele-domain language pre-training model TeleBERT and its
knowledge-enhanced version, a tele-knowledge re-training model KTeleBERT. which
includes effective prompt hints, adaptive numerical data encoding, and two
knowledge injection paradigms. Concretely, our proposal includes two stages:
first, pre-training TeleBERT on 20 million tele-related corpora, and then
re-training it on 1 million causal and machine-related corpora to obtain
KTeleBERT. Our evaluation on multiple tasks related to fault analysis in
tele-applications, including root-cause analysis, event association prediction,
and fault chain tracing, shows that pre-training a language model with
tele-domain data is beneficial for downstream tasks. Moreover, the KTeleBERT
re-training further improves the performance of task models, highlighting the
effectiveness of incorporating diverse tele-knowledge into the model.",0,1,0,0,0,0,0.77373,6.0,0.797778,54
59357ca4-e025-43f4-9f2f-a010cc227d69,Robust Region Feature Synthesizer for Zero-Shot Object Detection,23,0.194762,0.642312,"Zero-shot object detection aims at incorporating class semantic vectors to
realize the detection of (both seen and) unseen classes given an unconstrained
test image. In this study, we reveal the core challenges in this research area:
how to synthesize robust region features (for unseen objects) that are as
intra-class diverse and inter-class separable as the real samples, so that
strong unseen object detectors can be trained upon them. To address these
challenges, we build a novel zero-shot object detection framework that contains
an Intra-class Semantic Diverging component and an Inter-class Structure
Preserving component. The former is used to realize the one-to-more mapping to
obtain diverse visual features from each class semantic vector, preventing
miss-classifying the real unseen objects as image backgrounds. While the latter
is used to avoid the synthesized features too scattered to mix up the
inter-class and foreground-background relationship. To demonstrate the
effectiveness of the proposed approach, comprehensive experiments on PASCAL
VOC, COCO, and DIOR datasets are conducted. Notably, our approach achieves the
new state-of-the-art performance on PASCAL VOC and COCO and it is the first
study to carry out zero-shot object detection in remote sensing imagery.",0,1,0,0,1,0,0.593497,6.0,0.71423,52
3db47f60-83ce-4efc-8d67-752aae098704,HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud,14,0.034949,0.48789,"Place recognition or loop closure detection is one of the core components in
a full SLAM system. In this paper, aiming at strengthening the relevancy of
local neighboring points and the contextual dependency among global points
simultaneously, we investigate the exploitation of transformer-based network
for feature extraction, and propose a Hierarchical Transformer for Place
Recognition (HiTPR). The HiTPR consists of four major parts: point cell
generation, short-range transformer (SRT), long-range transformer (LRT) and
global descriptor aggregation. Specifically, the point cloud is initially
divided into a sequence of small cells by downsampling and nearest neighbors
searching. In the SRT, we extract the local feature for each point cell. While
in the LRT, we build the global dependency among all of the point cells in the
whole point cloud. Experiments on several standard benchmarks demonstrate the
superiority of the HiTPR in terms of average recall rate, achieving 93.71% at
top 1% and 86.63% at top 1 on the Oxford RobotCar dataset for example.",0,1,0,0,1,0,0.336706,7.0,0.642896,36
7187240a-904b-442b-a6a8-10ba8a6b4328,GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields,7,0.0749921,0.832094,"We propose GazeNeRF, a 3D-aware method for the task of gaze redirection.
Existing gaze redirection methods operate on 2D images and struggle to generate
3D consistent results. Instead, we build on the intuition that the face region
and eyeballs are separate 3D structures that move in a coordinated yet
independent fashion. Our method leverages recent advancements in conditional
image-based neural radiance fields and proposes a two-stream architecture that
predicts volumetric features for the face and eye regions separately. Rigidly
transforming the eye features via a 3D rotation matrix provides fine-grained
control over the desired gaze angle. The final, redirected image is then
attained via differentiable volume compositing. Our experiments show that this
architecture outperforms naively conditioned NeRF baselines as well as previous
state-of-the-art 2D gaze redirection methods in terms of redirection accuracy
and identity preservation.",0,1,1,0,1,0,0.395778,8.0,0.713127,53
5486fd67-ae9d-4a1f-a847-3ce9ec91fcab,Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition,13,0.159691,0.389096,"Language model fusion helps smart assistants recognize words which are rare
in acoustic data but abundant in text-only corpora (typed search logs).
However, such corpora have properties that hinder downstream performance,
including being (1) too large, (2) beset with domain-mismatched content, and
(3) heavy-headed rather than heavy-tailed (excessively many duplicate search
queries such as ""weather""). We show that three simple strategies for selecting
language modeling data can dramatically improve rare-word recognition without
harming overall performance. First, to address the heavy-headedness, we
downsample the data according to a soft log function, which tunably reduces
high frequency (head) sentences. Second, to encourage rare-word exposure, we
explicitly filter for words rare in the acoustic data. Finally, we tackle
domain-mismatch via perplexity-based contrastive selection, filtering for
examples matched to the target domain. We down-select a large corpus of web
search queries by a factor of 53x and achieve better LM perplexities than
without down-selection. When shallow-fused with a state-of-the-art, production
speech engine, our LM achieves WER reductions of up to 24% relative on
rare-word sentences (without changing overall WER) compared to a baseline LM
trained on the raw corpus. These gains are further validated through favorable
side-by-side evaluations on live voice search traffic.",0,1,0,0,1,0,0.633967,5.0,0.679116,18
10b7bd11-e93b-4ac9-bc99-838deb34a0c0,Generalizing to New Physical Systems via Context-Informed Dynamics Model,18,0.624457,0.904573,"Data-driven approaches to modeling physical systems fail to generalize to
unseen systems that share the same general dynamics with the learning domain,
but correspond to different physical contexts. We propose a new framework for
this key problem, context-informed dynamics adaptation (CoDA), which takes into
account the distributional shift across systems for fast and efficient
adaptation to new dynamics. CoDA leverages multiple environments, each
associated to a different dynamic, and learns to condition the dynamics model
on contextual parameters, specific to each environment. The conditioning is
performed via a hypernetwork, learned jointly with a context vector from
observed data. The proposed formulation constrains the search hypothesis space
to foster fast adaptation and better generalization across environments. We
theoretically motivate our approach and show state-of-the-art generalization
results on a set of nonlinear dynamics, representative of a variety of
application domains. We also show, on these systems, that new system parameters
can be inferred from context vectors with minimal supervision. Code is
available at https://github.com/yuan-yin/CoDA .",1,0,0,0,1,0,0.984609,8.0,0.977429,65
f3a819c0-4196-4b69-84e5-e6106be5cd98,A Proposal for Foley Sound Synthesis Challenge,7,0.113889,0.838957,"""Foley"" refers to sound effects that are added to multimedia during
post-production to enhance its perceived acoustic properties, e.g., by
simulating the sounds of footsteps, ambient environmental sounds, or visible
objects on the screen. While foley is traditionally produced by foley artists,
there is increasing interest in automatic or machine-assisted techniques
building upon recent advances in sound synthesis and generative models. To
foster more participation in this growing research area, we propose a challenge
for automatic foley synthesis. Through case studies on successful previous
challenges in audio and machine learning, we set the goals of the proposed
challenge: rigorous, unified, and efficient evaluation of different foley
synthesis systems, with an overarching goal of drawing active participation
from the research community. We outline the details and design considerations
of a foley sound synthesis challenge, including task definition, dataset
requirements, and evaluation criteria.",0,1,0,0,0,0,0.784357,6.0,0.803088,39
4bbe4736-9fde-4716-9b98-6d113afe54c3,Aggregate effects of advertising decisions: a complex systems look at search engine advertising via an experimental study,15,0.378257,0.449051,"Purpose: We model group advertising decisions, which are the collective
decisions of every single advertiser within the set of advertisers who are
competing in the same auction or vertical industry, and examine resulting
market outcomes, via a proposed simulation framework named EXP-SEA
(Experimental Platform for Search Engine Advertising) supporting experimental
studies of collective behaviors in the context of search engine advertising.
Design: We implement the EXP-SEA to validate the proposed simulation framework,
also conduct three experimental studies on the aggregate impact of electronic
word-of-mouth, the competition level, and strategic bidding behaviors. EXP-SEA
supports heterogeneous participants, various auction mechanisms, and also
ranking and pricing algorithms. Findings: Findings from our three experiments
show that (a) both the market profit and advertising indexes such as number of
impressions and number of clicks are larger when the eWOM effect presents,
meaning social media certainly has some effect on search engine advertising
outcomes, (b) the competition level has a monotonic increasing effect on the
market performance, thus search engines have an incentive to encourage both the
eWOM among search users and competition among advertisers, and (c) given the
market-level effect of the percentage of advertisers employing a dynamic greedy
bidding strategy, there is a cut-off point for strategic bidding behaviors.
Originality: This is one of the first research works to explore collective
group decisions and resulting phenomena in the complex context of search engine
advertising via developing and validating a simulation framework that supports
assessments of various advertising strategies and estimations of the impact of
mechanisms on the search market.",0,0,1,0,0,0,0.0878972,15.0,0.733642,79
e2bb01d6-a2ee-4164-8c29-9cdfc9447dc6,Finding the optimal human strategy for Wordle using maximum correct letter probabilities and reinforcement learning,18,0.0229759,0.297163,"Wordle is an online word puzzle game that gained viral popularity in January
2022. The goal is to guess a hidden five letter word. After each guess, the
player gains information about whether the letters they guessed are present in
the word, and whether they are in the correct position. Numerous blogs have
suggested guessing strategies and starting word lists that improve the chance
of winning. Optimized algorithms can win 100% of games within five of the six
allowed trials. However, it is infeasible for human players to use these
algorithms due to an inability to perfectly recall all known 5-letter words and
perform complex calculations that optimize information gain. Here, we present
two different methods for choosing starting words along with a framework for
discovering the optimal human strategy based on reinforcement learning. Human
Wordle players can use the rules we discover to optimize their chance of
winning.",1,1,0,0,0,0,0.963553,13.0,0.968314,8
8f1bafe6-ac1e-4d9f-ae8d-78e1de139066,Planning Assembly Sequence with Graph Transformer,6,0.263297,0.597554,"Assembly sequence planning (ASP) is the essential process for modern
manufacturing, proven to be NP-complete thus its effective and efficient
solution has been a challenge for researchers in the field. In this paper, we
present a graph-transformer based framework for the ASP problem which is
trained and demonstrated on a self-collected ASP database. The ASP database
contains a self-collected set of LEGO models. The LEGO model is abstracted to a
heterogeneous graph structure after a thorough analysis of the original
structure and feature extraction. The ground truth assembly sequence is first
generated by brute-force search and then adjusted manually to in line with
human rational habits. Based on this self-collected ASP dataset, we propose a
heterogeneous graph-transformer framework to learn the latent rules for
assembly planning. We evaluated the proposed framework in a series of
experiment. The results show that the similarity of the predicted and ground
truth sequences can reach 0.44, a medium correlation measured by Kendall's
$\tau$. Meanwhile, we compared the different effects of node features and edge
features and generated a feasible and reasonable assembly sequence as a
benchmark for further research. Our data set and code is available on
https://github.com/AIR-DISCOVER/ICRA\_ASP.",1,1,1,1,0,0,0.539661,6.0,0.689448,34
7dfe8bab-75fa-4c9c-b204-2589fb94fa9d,MuSCLe: A Multi-Strategy Contrastive Learning Framework for Weakly Supervised Semantic Segmentation,1,0.0232965,0.0447497,"Weakly supervised semantic segmentation (WSSS) has gained significant
popularity since it relies only on weak labels such as image level annotations
rather than pixel level annotations required by supervised semantic
segmentation (SSS) methods. Despite drastically reduced annotation costs,
typical feature representations learned from WSSS are only representative of
some salient parts of objects and less reliable compared to SSS due to the weak
guidance during training. In this paper, we propose a novel Multi-Strategy
Contrastive Learning (MuSCLe) framework to obtain enhanced feature
representations and improve WSSS performance by exploiting similarity and
dissimilarity of contrastive sample pairs at image, region, pixel and object
boundary levels. Extensive experiments demonstrate the effectiveness of our
method and show that MuSCLe outperforms the current state-of-the-art on the
widely used PASCAL VOC 2012 dataset.",0,1,0,0,1,0,0.991951,8.0,0.995475,49
8cf53e97-7fe9-4d9c-95da-9f058fad8fad,Probabilistic Representations for Video Contrastive Learning,27,0.166255,0.716789,"This paper presents Probabilistic Video Contrastive Learning, a
self-supervised representation learning method that bridges contrastive
learning with probabilistic representation. We hypothesize that the clips
composing the video have different distributions in short-term duration, but
can represent the complicated and sophisticated video distribution through
combination in a common embedding space. Thus, the proposed method represents
video clips as normal distributions and combines them into a Mixture of
Gaussians to model the whole video distribution. By sampling embeddings from
the whole video distribution, we can circumvent the careful sampling strategy
or transformations to generate augmented views of the clips, unlike previous
deterministic methods that have mainly focused on such sample generation
strategies for contrastive learning. We further propose a stochastic
contrastive loss to learn proper video distributions and handle the inherent
uncertainty from the nature of the raw video. Experimental results verify that
our probabilistic embedding stands as a state-of-the-art video representation
learning for action recognition and video retrieval on the most popular
benchmarks, including UCF101 and HMDB51.",0,0,0,0,1,0,0.541029,7.0,0.73436,92
bd220877-29fd-471a-ad29-ee1d52799456,A Graph Convolution for Signed Directed Graphs,2,0.00559618,0.0678065,"A signed directed graph is a graph with sign and direction information on the
edges. Even though signed directed graphs are more informative than unsigned or
undirected graphs, they are more complicated to analyze and have received less
research attention. This paper investigates a spectral graph convolution model
to fully utilize the information embedded in signed directed edges. We propose
a novel complex Hermitian adjacency matrix that encodes graph information via
complex numbers. Compared to a simple connection-based adjacency matrix, the
complex Hermitian can represent edge direction, sign, and connectivity via its
phases and magnitudes. Then, we define a magnetic Laplacian of the proposed
adjacency matrix and prove that it is positive semi-definite (PSD) for the
analyses using spectral graph convolution. We perform extensive experiments on
four real-world datasets. Our experiments show that the proposed scheme
outperforms several state-of-the-art techniques.",0,0,1,1,1,0,0.0121075,13.0,0.537132,49
ab3e534e-d20d-4c5c-b263-e8c20d8559cb,Neutral Utterances are Also Causes: Enhancing Conversational Causal Emotion Entailment with Social Commonsense Knowledge,19,0.683731,0.503415,"Conversational Causal Emotion Entailment aims to detect causal utterances for
a non-neutral targeted utterance from a conversation. In this work, we build
conversations as graphs to overcome implicit contextual modelling of the
original entailment style. Following the previous work, we further introduce
the emotion information into graphs. Emotion information can markedly promote
the detection of causal utterances whose emotion is the same as the targeted
utterance. However, it is still hard to detect causal utterances with different
emotions, especially neutral ones. The reason is that models are limited in
reasoning causal clues and passing them between utterances. To alleviate this
problem, we introduce social commonsense knowledge (CSK) and propose a
Knowledge Enhanced Conversation graph (KEC). KEC propagates the CSK between two
utterances. As not all CSK is emotionally suitable for utterances, we therefore
propose a sentiment-realized knowledge selecting strategy to filter CSK. To
process KEC, we further construct the Knowledge Enhanced Directed Acyclic Graph
networks. Experimental results show that our method outperforms baselines and
infers more causes with different emotions from the targeted utterance.",1,1,0,0,0,0,0.982411,4.0,0.94673,26
37b5de63-dac3-4738-8b2f-5f8c0b2a4a12,Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms,1,0.00991642,0.0449199,"Prominent questions about the role of sensory vs. linguistic input in the way
we acquire and use language have been extensively studied in the
psycholinguistic literature. However, the relative effect of various factors in
a person's overall experience on their linguistic system remains unclear. We
study this question by making a step forward towards a better understanding of
the conceptual perception of colors by color-blind individuals, as reflected in
their spontaneous linguistic productions. Using a novel and carefully curated
dataset, we show that red-green color-blind speakers use the ""red"" and ""green""
color terms in less predictable contexts, and in linguistic environments
evoking mental image to a lower extent, when compared to their normal-sighted
counterparts. These findings shed some new and interesting light on the role of
sensory experience on our linguistic system.",0,0,0,1,0,0,0.0291433,17.0,0.698223,46
51fd959b-fa09-4c90-9917-ff21ad05ae6f,CCPT: Automatic Gameplay Testing and Validation with Curiosity-Conditioned Proximal Trajectories,7,0.332976,0.678188,"This paper proposes a novel deep reinforcement learning algorithm to perform
automatic analysis and detection of gameplay issues in complex 3D navigation
environments. The Curiosity-Conditioned Proximal Trajectories (CCPT) method
combines curiosity and imitation learning to train agents to methodically
explore in the proximity of known trajectories derived from expert
demonstrations. We show how CCPT can explore complex environments, discover
gameplay issues and design oversights in the process, and recognize and
highlight them directly to game designers. We further demonstrate the
effectiveness of the algorithm in a novel 3D navigation environment which
reflects the complexity of modern AAA video games. Our results show a higher
level of coverage and bug discovery than baselines methods, and it hence can
provide a valuable tool for game designers to identify issues in game design
automatically.",0,0,0,0,0,0,0.834931,9.0,0.886569,26
35aa6837-bf60-4559-bc69-49a97c93bb9b,Emotion Analysis using Multi-Layered Networks for Graphical Representation of Tweets,7,0.047811,0.527191,"Anticipating audience reaction towards a certain piece of text is integral to
several facets of society ranging from politics, research, and commercial
industries. Sentiment analysis (SA) is a useful natural language processing
(NLP) technique that utilizes both lexical/statistical and deep learning
methods to determine whether different sized texts exhibit a positive,
negative, or neutral emotion. However, there is currently a lack of tools that
can be used to analyse groups of independent texts and extract the primary
emotion from the whole set. Therefore, the current paper proposes a novel
algorithm referred to as the Multi-Layered Tweet Analyzer (MLTA) that
graphically models social media text using multi-layered networks (MLNs) in
order to better encode relationships across independent sets of tweets. Graph
structures are capable of capturing meaningful relationships in complex
ecosystems compared to other representation methods. State of the art Graph
Neural Networks (GNNs) are used to extract information from the Tweet-MLN and
make predictions based on the extracted graph features. Results show that not
only does the MLTA predict from a larger set of possible emotions, delivering a
more accurate sentiment compared to the standard positive, negative or neutral,
it also allows for accurate group-level predictions of Twitter data.",0,1,0,0,0,0,0.299121,7.0,0.622301,64
d2237030-83fd-4c0e-9a30-3bae5ec14887,Quantitative AI Risk Assessments: Opportunities and Challenges,8,0.158291,0.768873,"Although AI-based systems are increasingly being leveraged to provide value
to organizations, individuals, and society, significant attendant risks have
been identified. These risks have led to proposed regulations, litigation, and
general societal concerns.
  As with any promising technology, organizations want to benefit from the
positive capabilities of AI technology while reducing the risks. The best way
to reduce risks is to implement comprehensive AI lifecycle governance where
policies and procedures are described and enforced during the design,
development, deployment, and monitoring of an AI system. While support for
comprehensive governance is beginning to emerge, organizations often need to
identify the risks of deploying an already-built model without knowledge of how
it was constructed or access to its original developers.
  Such an assessment will quantitatively assess the risks of an existing model
in a manner analogous to how a home inspector might assess the energy
efficiency of an already-built home or a physician might assess overall patient
health based on a battery of tests. This paper explores the concept of a
quantitative AI Risk Assessment, exploring the opportunities, challenges, and
potential impacts of such an approach, and discussing how it might improve AI
regulations.",0,1,0,0,0,0,0.720576,6.0,0.772255,65
834f9cb8-29e4-4b63-939c-b63660201f68,MBGDT:Robust Mini-Batch Gradient Descent,1,0.0130625,0.132788,"In high dimensions, most machine learning method perform fragile even there
are a little outliers. To address this, we hope to introduce a new method with
the base learner, such as Bayesian regression or stochastic gradient descent to
solve the problem of the vulnerability in the model. Because the mini-batch
gradient descent allows for a more robust convergence than the batch gradient
descent, we work a method with the mini-batch gradient descent, called
Mini-Batch Gradient Descent with Trimming (MBGDT). Our method show state-of-art
performance and have greater robustness than several baselines when we apply
our method in designed dataset.",0,1,0,0,1,0,0.228342,9.0,0.671162,6
311d07c0-c14d-4b41-a70a-4eff9a28d8ec,Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations,35,0.220593,0.656546,"Although large language models can be prompted for both zero- and few-shot
learning, performance drops significantly when no demonstrations are available.
In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap
by constructing pseudo-demonstrations for a given test input using a raw text
corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the
nearest neighbors to the test input from the corpus and pairing them with
random task labels, and (2) applying a set of techniques to reduce the amount
of direct copying the model does from the resulting demonstrations. Evaluation
on nine classification datasets shows that Z-ICL outperforms previous zero-shot
methods by a significant margin, and is on par with in-context learning with
labeled training data in the few-shot setting. Overall, Z-ICL provides a
significantly higher estimate of the zero-shot performance levels of a model,
and supports future efforts to develop better pseudo-demonstrations that
further improve zero-shot results.",1,1,1,0,1,0,0.938506,4.0,0.854045,27
75bbb2af-a7a3-47be-a6b2-9c77367517c9,Co-design of Embodied Neural Intelligence via Constrained Evolution,1,0.0176807,0.0570519,"We introduce a novel co-design method for autonomous moving agents' shape
attributes and locomotion by combining deep reinforcement learning and
evolution with user control. Our main inspiration comes from evolution, which
has led to wide variability and adaptation in Nature and has the potential to
significantly improve design and behavior simultaneously. Our method takes an
input agent with optional simple constraints such as leg parts that should not
evolve or allowed ranges of changes. It uses physics-based simulation to
determine its locomotion and finds a behavior policy for the input design,
later used as a baseline for comparison. The agent is then randomly modified
within the allowed ranges creating a new generation of several hundred agents.
The generation is trained by transferring the previous policy, which
significantly speeds up the training. The best-performing agents are selected,
and a new generation is formed using their crossover and mutations. The next
generations are then trained until satisfactory results are reached. We show a
wide variety of evolved agents, and our results show that even with only 10% of
changes, the overall performance of the evolved agents improves 50%. If more
significant changes to the initial design are allowed, our experiments'
performance improves even more to 150%. Contrary to related work, our co-design
works on a single GPU and provides satisfactory results by training thousands
of agents within one hour.",0,0,1,0,0,0,0.162851,14.0,0.761657,43
261087bf-4179-4ddb-9587-b73be97b0fa1,On Explaining Multimodal Hateful Meme Detection Models,22,0.89919,0.928746,"Hateful meme detection is a new multimodal task that has gained significant
traction in academic and industry research communities. Recently, researchers
have applied pre-trained visual-linguistic models to perform the multimodal
classification task, and some of these solutions have yielded promising
results. However, what these visual-linguistic models learn for the hateful
meme classification task remains unclear. For instance, it is unclear if these
models are able to capture the derogatory or slurs references in multimodality
(i.e., image and text) of the hateful memes. To fill this research gap, this
paper propose three research questions to improve our understanding of these
visual-linguistic models performing the hateful meme classification task. We
found that the image modality contributes more to the hateful meme
classification task, and the visual-linguistic models are able to perform
visual-text slurs grounding to a certain extent. Our error analysis also shows
that the visual-linguistic models have acquired biases, which resulted in
false-positive predictions.",1,0,0,0,0,0,0.991668,6.0,0.992768,36
b7fb2f1a-cd4e-4d7e-b6d1-151b5b6e6050,More Interpretable Graph Similarity Computation via Maximum Common Subgraph Inference,9,0.0427747,0.675987,"Graph similarity measurement, which computes the distance/similarity between
two graphs, arises in various graph-related tasks. Recent learning-based
methods lack interpretability, as they directly transform interaction
information between two graphs into one hidden vector and then map it to
similarity. To cope with this problem, this study proposes a more interpretable
end-to-end paradigm for graph similarity learning, named Similarity Computation
via Maximum Common Subgraph Inference (INFMCS). Our critical insight into
INFMCS is the strong correlation between similarity score and Maximum Common
Subgraph (MCS). We implicitly infer MCS to obtain the normalized MCS size, with
the supervision information being only the similarity score during training. To
capture more global information, we also stack some vanilla transformer encoder
layers with graph convolution layers and propose a novel permutation-invariant
node Positional Encoding. The entire model is quite simple yet effective.
Comprehensive experiments demonstrate that INFMCS consistently outperforms
state-of-the-art baselines for graph-graph classification and regression tasks.
Ablation experiments verify the effectiveness of the proposed computation
paradigm and other components. Also, visualization and statistics of results
reveal the interpretability of INFMCS.",0,0,0,0,1,0,0.0698644,11.0,0.615032,46
57d36da5-7438-4db7-832a-a54eb4e0abb1,Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features,15,0.0577148,0.660481,"We consider the problem of category-level 6D pose estimation from a single
RGB image. Our approach represents an object category as a cuboid mesh and
learns a generative model of the neural feature activations at each mesh vertex
to perform pose estimation through differentiable rendering. A common problem
of rendering-based approaches is that they rely on bounding box proposals,
which do not convey information about the 3D rotation of the object and are not
reliable when objects are partially occluded. Instead, we introduce a
coarse-to-fine optimization strategy that utilizes the rendering process to
estimate a sparse set of 6D object proposals, which are subsequently refined
with gradient-based optimization. The key to enabling the convergence of our
approach is a neural feature representation that is trained to be scale- and
rotation-invariant using contrastive learning. Our experiments demonstrate an
enhanced category-level 6D pose estimation performance compared to prior work,
particularly under strong partial occlusion.",0,1,0,0,1,0,0.048052,9.0,0.486624,42
21719211-0756-46d8-bf99-337400f9064f,A Two Parameters Equation for Word Rank-Frequency Relation,1,0.00578823,0.0609859,"Let $f (\cdot)$ be the absolute frequency of words and $r$ be the rank of
words in decreasing order of frequency, then the following function can fit the
rank-frequency relation \[ f (r;s,t) = \left(\frac{r_{\tt max}}{r}\right)^{1-s}
\left(\frac{r_{\tt max}+t \cdot r_{\tt exp}}{r+t \cdot r_{\tt
exp}}\right)^{1+(1+t)s} \] where $r_{\tt max}$ and $r_{\tt exp}$ are the
maximum and the expectation of the rank, respectively; $s>0$ and $t>0$ are
parameters estimated from data. On well-behaved data, there should be $s<1$ and
$s \cdot t < 1$.",0,0,0,0,0,0,0.000200531,34.0,0.702239,8
b6f6f700-777f-441e-9f0e-0db092a212a6,Watermark Vaccine: Adversarial Attacks to Prevent Watermark Removal,16,0.103022,0.857967,"As a common security tool, visible watermarking has been widely applied to
protect copyrights of digital images. However, recent works have shown that
visible watermarks can be removed by DNNs without damaging their host images.
Such watermark-removal techniques pose a great threat to the ownership of
images. Inspired by the vulnerability of DNNs on adversarial perturbations, we
propose a novel defence mechanism by adversarial machine learning for good.
From the perspective of the adversary, blind watermark-removal networks can be
posed as our target models; then we actually optimize an imperceptible
adversarial perturbation on the host images to proactively attack against
watermark-removal networks, dubbed Watermark Vaccine. Specifically, two types
of vaccines are proposed. Disrupting Watermark Vaccine (DWV) induces to ruin
the host image along with watermark after passing through watermark-removal
networks. In contrast, Inerasable Watermark Vaccine (IWV) works in another
fashion of trying to keep the watermark not removed and still noticeable.
Extensive experiments demonstrate the effectiveness of our DWV/IWV in
preventing watermark removal, especially on various watermark removal networks.",1,1,0,0,0,0,0.0820288,10.0,0.593237,57
92cc7aa9-ff98-4c56-aae7-6acc8ec54c0b,A Multimodal Corpus for Emotion Recognition in Sarcasm,17,0.102754,0.757536,"While sentiment and emotion analysis have been studied extensively, the
relationship between sarcasm and emotion has largely remained unexplored. A
sarcastic expression may have a variety of underlying emotions. For example, ""I
love being ignored"" belies sadness, while ""my mobile is fabulous with a battery
backup of only 15 minutes!"" expresses frustration. Detecting the emotion behind
a sarcastic expression is non-trivial yet an important task. We undertake the
task of detecting the emotion in a sarcastic statement, which to the best of
our knowledge, is hitherto unexplored. We start with the recently released
multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions.
We identify and correct 343 incorrect emotion labels (out of 690). We double
the size of the dataset, label it with emotions along with valence and arousal
which are important indicators of emotional intensity. Finally, we label each
sarcastic utterance with one of the four sarcasm types-Propositional, Embedded,
Likeprefixed and Illocutionary, with the goal of advancing sarcasm detection
research. Exhaustive experimentation with multimodal (text, audio, and video)
fusion models establishes a benchmark for exact emotion recognition in sarcasm
and outperforms the state-of-art sarcasm detection. We release the dataset
enriched with various annotations and the code for research purposes:
https://github.com/apoorva-nunna/MUStARD_Plus_Plus",1,1,1,1,1,0,0.238486,9.0,0.676695,38
7bd04d70-b4cf-4078-885e-01ae466b28b7,Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss,16,0.77577,0.626417,"In this paper, we study color image inpainting as a pure quaternion matrix
completion problem. In the literature, the theoretical guarantee for quaternion
matrix completion is not well-established. Our main aim is to propose a new
minimization problem with an objective combining nuclear norm and a quadratic
loss weighted among three channels. To fill the theoretical vacancy, we obtain
the error bound in both clean and corrupted regimes, which relies on some new
results of quaternion matrices. A general Gaussian noise is considered in
robust completion where all observations are corrupted. Motivated by the error
bound, we propose to handle unbalanced or correlated noise via a cross-channel
weight in the quadratic loss, with the main purpose of rebalancing noise level,
or removing noise correlation. Extensive experimental results on synthetic and
color image data are presented to confirm and demonstrate our theoretical
findings.",0,0,0,0,0,0,0.730259,16.0,0.916301,76
bd43f58f-7395-4db7-9deb-c9fe47487542,Swin MAE: Masked Autoencoders for Small Datasets,10,0.0485155,0.6089,"The development of deep learning models in medical image analysis is majorly
limited by the lack of large-sized and well-annotated datasets. Unsupervised
learning does not require labels and is more suitable for solving medical image
analysis problems. However, most of the current unsupervised learning methods
need to be applied to large datasets. To make unsupervised learning applicable
to small datasets, we proposed Swin MAE, which is a masked autoencoder with
Swin Transformer as its backbone. Even on a dataset of only a few thousand
medical images and without using any pre-trained models, Swin MAE is still able
to learn useful semantic features purely from images. It can equal or even
slightly outperform the supervised model obtained by Swin Transformer trained
on ImageNet in terms of the transfer learning results of downstream tasks. The
code is publicly available at https://github.com/Zian-Xu/Swin-MAE.",1,1,0,0,0,0,0.592138,4.0,0.570417,41
734cc178-5ead-421d-a3b7-17307bda9dcf,A high-precision self-supervised monocular visual odometry in foggy weather based on robust cycled generative adversarial networks and multi-task learning aided depth estimation,1,0.014842,0.155536,"This paper proposes a high-precision self-supervised monocular VO, which is
specifically designed for navigation in foggy weather. A cycled generative
adversarial network is designed to obtain high-quality self-supervised loss via
forcing the forward and backward half-cycle to output consistent estimation.
Moreover, gradient-based loss and perceptual loss are introduced to eliminate
the interference of complex photometric change on self-supervised loss in foggy
weather. To solve the ill-posed problem of depth estimation, a self-supervised
multi-task learning aided depth estimation module is designed based on the
strong correlation between the depth estimation and transmission map
calculation of hazy images in foggy weather. The experimental results on the
synthetic foggy KITTI dataset show that the proposed self-supervised monocular
VO performs better in depth and pose estimation than other state-of-the-art
monocular VO in the literature, indicating the designed method is more suitable
for foggy weather.",0,1,0,0,1,0,0.310074,9.0,0.711052,54
4eb1e160-d8e8-41d5-85e9-277175a90fda,Sequential Manipulation Planning on Scene Graph,13,0.165672,0.839497,"We devise a 3D scene graph representation, contact graph+ (cg+), for
efficient sequential task planning. Augmented with predicate-like attributes,
this contact graph-based representation abstracts scene layouts with succinct
geometric information and valid robot-scene interactions. Goal configurations,
naturally specified on contact graphs, can be produced by a genetic algorithm
with a stochastic optimization method. A task plan is then initialized by
computing the Graph Editing Distance (GED) between the initial contact graphs
and the goal configurations, which generates graph edit operations
corresponding to possible robot actions. We finalize the task plan by imposing
constraints to regulate the temporal feasibility of graph edit operations,
ensuring valid task and motion correspondences. In a series of simulations and
experiments, robots successfully complete complex sequential object
rearrangement tasks that are difficult to specify using conventional planning
language like Planning Domain Definition Language (PDDL), demonstrating the
high feasibility and potential of robot sequential task planning on contact
graph.",1,1,0,0,0,0,0.253315,9.0,0.684438,56
e5dba8d3-7253-4f81-a1ce-ae63fc4b7c97,MISm: A Medical Image Segmentation Metric for Evaluation of weak labeled Data,1,0.0153531,0.0868917,"Performance measures are an important tool for assessing and comparing
different medical image segmentation algorithms. Unfortunately, the current
measures have their weaknesses when it comes to assessing certain edge cases.
These limitations arouse when images with a very small region of interest or
without a region of interest at all are assessed. As a solution for these
limitations, we propose a new medical image segmentation metric: MISm. To
evaluate MISm, the popular metrics in the medical image segmentation and MISm
were compared using images of magnet resonance tomography from several
scenarios. In order to allow application in the community and reproducibility
of experimental results, we included MISm in the publicly available evaluation
framework MISeval:
https://github.com/frankkramer-lab/miseval/tree/master/miseval",1,1,0,0,0,0,0.418285,7.0,0.682522,25
36767f8c-a120-4a4d-bd3b-9103da170780,Roadmap for Cybersecurity in Autonomous Vehicles,16,0.703498,0.824327,"Autonomous vehicles are on the horizon and will be transforming
transportation safety and comfort. These vehicles will be connected to various
external systems and utilize advanced embedded systems to perceive their
environment and make intelligent decisions. However, this increased
connectivity makes these vehicles vulnerable to various cyber-attacks that can
have catastrophic effects. Attacks on automotive systems are already on the
rise in today's vehicles and are expected to become more commonplace in future
autonomous vehicles. Thus, there is a need to strengthen cybersecurity in
future autonomous vehicles. In this article, we discuss major automotive
cyber-attacks over the past decade and present state-of-the-art solutions that
leverage artificial intelligence (AI). We propose a roadmap towards building
secure autonomous vehicles and highlight key open challenges that need to be
addressed.",0,1,0,0,0,0,0.904173,4.0,0.810734,41
89d1a2a0-3c39-4206-b154-a81e21d030b7,Automated speech tools for helping communities process restricted-access corpora for language revival efforts,3,0.0142829,0.179464,"Many archival recordings of speech from endangered languages remain
unannotated and inaccessible to community members and language learning
programs. One bottleneck is the time-intensive nature of annotation. An even
narrower bottleneck occurs for recordings with access constraints, such as
language that must be vetted or filtered by authorised community members before
annotation can begin. We propose a privacy-preserving workflow to widen both
bottlenecks for recordings where speech in the endangered language is
intermixed with a more widely-used language such as English for meta-linguistic
commentary and questions (e.g. What is the word for 'tree'?). We integrate
voice activity detection (VAD), spoken language identification (SLI), and
automatic speech recognition (ASR) to transcribe the metalinguistic content,
which an authorised person can quickly scan to triage recordings that can be
annotated by people with lower levels of access. We report work-in-progress
processing 136 hours archival audio containing a mix of English and Muruwari.
Our collaborative work with the Muruwari custodian of the archival materials
show that this workflow reduces metalanguage transcription time by 20% even
given only minimal amounts of annotated training data: 10 utterances per
language for SLI and for ASR at most 39 minutes, and possibly as little as 39
seconds.",1,1,0,0,0,0,0.0389606,8.0,0.395645,27
e77faae5-6d64-4a50-af77-4f472198c963,FabKG: A Knowledge graph of Manufacturing Science domain utilizing structured and unconventional unstructured knowledge source,3,0.00653862,0.146252,"As the demands for large-scale information processing have grown, knowledge
graph-based approaches have gained prominence for representing general and
domain knowledge. The development of such general representations is essential,
particularly in domains such as manufacturing which intelligent processes and
adaptive education can enhance. Despite the continuous accumulation of text in
these domains, the lack of structured data has created information extraction
and knowledge transfer barriers. In this paper, we report on work towards
developing robust knowledge graphs based upon entity and relation data for both
commercial and educational uses. To create the FabKG (Manufacturing knowledge
graph), we have utilized textbook index words, research paper keywords, FabNER
(manufacturing NER), to extract a sub knowledge base contained within Wikidata.
Moreover, we propose a novel crowdsourcing method for KG creation by leveraging
student notes, which contain invaluable information but are not captured as
meaningful information, excluding their use in personal preparation for
learning and written exams. We have created a knowledge graph containing 65000+
triples using all data sources. We have also shown the use case of
domain-specific question answering and expression/formula-based question
answering for educational purposes.",0,1,0,0,0,0,0.0474777,8.0,0.420911,44
ae67d3b7-8941-4ea3-b68d-e4ecbcea72ca,Densely Constrained Depth Estimator for Monocular 3D Object Detection,18,0.151286,0.687387,"Estimating accurate 3D locations of objects from monocular images is a
challenging problem because of lacking depth. Previous work shows that
utilizing the object's keypoint projection constraints to estimate multiple
depth candidates boosts the detection performance. However, the existing
methods can only utilize vertical edges as projection constraints for depth
estimation. So these methods only use a small number of projection constraints
and produce insufficient depth candidates, leading to inaccurate depth
estimation. In this paper, we propose a method that utilizes dense projection
constraints from edges of any direction. In this way, we employ much more
projection constraints and produce considerable depth candidates. Besides, we
present a graph matching weighting module to merge the depth candidates. The
proposed method DCD (Densely Constrained Detector) achieves state-of-the-art
performance on the KITTI and WOD benchmarks. Code is released at
https://github.com/BraveGroup/DCD.",1,1,0,0,1,0,0.719775,5.0,0.726256,57
a5ea9f8c-dba5-463b-bc43-63e8a5d91ef6,Learning Across Domains and Devices: Style-Driven Source-Free Domain Adaptation in Clustered Federated Learning,22,0.101674,0.649403,"Federated Learning (FL) has recently emerged as a possible way to tackle the
domain shift in real-world Semantic Segmentation (SS) without compromising the
private nature of the collected data. However, most of the existing works on FL
unrealistically assume labeled data in the remote clients. Here we propose a
novel task (FFREEDA) in which the clients' data is unlabeled and the server
accesses a source labeled dataset for pre-training only. To solve FFREEDA, we
propose LADD, which leverages the knowledge of the pre-trained model by
employing self-supervision with ad-hoc regularization techniques for local
training and introducing a novel federated clustered aggregation scheme based
on the clients' style. Our experiments show that our algorithm is able to
efficiently tackle the new task outperforming existing approaches. The code is
available at https://github.com/Erosinho13/LADD.",1,1,1,0,1,0,0.63589,6.0,0.733468,76
826fb492-6329-4a5b-ada8-62dca8293734,Zero-shot Domain Adaptation for Neural Machine Translation with Retrieved Phrase-level Prompts,7,0.0755191,0.310904,"Domain adaptation is an important challenge for neural machine translation.
However, the traditional fine-tuning solution requires multiple extra training
and yields a high cost. In this paper, we propose a non-tuning paradigm,
resolving domain adaptation with a prompt-based method. Specifically, we
construct a bilingual phrase-level database and retrieve relevant pairs from it
as a prompt for the input sentences. By utilizing Retrieved Phrase-level
Prompts (RePP), we effectively boost the translation quality. Experiments show
that our method improves domain-specific machine translation for 6.2 BLEU
scores and improves translation constraints for 11.5% accuracy without
additional training.",1,1,0,1,0,0,0.671439,6.0,0.749608,32
7d40a347-4adf-41e3-be6b-e68c301c78f2,MABEL: Attenuating Gender Bias using Textual Entailment Data,16,0.222226,0.651926,"Pre-trained language models encode undesirable social biases, which are
further exacerbated in downstream use. To this end, we propose MABEL (a Method
for Attenuating Gender Bias using Entailment Labels), an intermediate
pre-training approach for mitigating gender bias in contextualized
representations. Key to our approach is the use of a contrastive learning
objective on counterfactually augmented, gender-balanced entailment pairs from
natural language inference (NLI) datasets. We also introduce an alignment
regularizer that pulls identical entailment pairs along opposite gender
directions closer. We extensively evaluate our approach on intrinsic and
extrinsic metrics, and show that MABEL outperforms previous task-agnostic
debiasing approaches in terms of fairness. It also preserves task performance
after fine-tuning on downstream tasks. Together, these findings demonstrate the
suitability of NLI data as an effective means of bias mitigation, as opposed to
only using unlabeled sentences in the literature. Finally, we identify that
existing approaches often use evaluation settings that are insufficient or
inconsistent. We make an effort to reproduce and compare previous methods, and
call for unifying the evaluation settings across gender debiasing methods for
better future comparison.",1,1,0,0,0,0,0.758489,7.0,0.820259,78
1552e543-087e-4d94-ace1-0d4a720e8a51,Dense Prediction Transformer for Scale Estimation in Monocular Visual Odometry,4,0.0244461,0.142444,"Monocular visual odometry consists of the estimation of the position of an
agent through images of a single camera, and it is applied in autonomous
vehicles, medical robots, and augmented reality. However, monocular systems
suffer from the scale ambiguity problem due to the lack of depth information in
2D frames. This paper contributes by showing an application of the dense
prediction transformer model for scale estimation in monocular visual odometry
systems. Experimental results show that the scale drift problem of monocular
systems can be reduced through the accurate estimation of the depth map by this
model, achieving competitive state-of-the-art performance on a visual odometry
benchmark.",1,1,0,0,1,0,0.0646701,16.0,0.730334,23
0a2ec037-1f99-4d4c-a1ea-e200826fa242,Lightweight Structure-Aware Attention for Visual Understanding,1,0.0191144,0.0509052,"Vision Transformers (ViTs) have become a dominant paradigm for visual
representation learning with self-attention operators. Although these operators
provide flexibility to the model with their adjustable attention kernels, they
suffer from inherent limitations: (1) the attention kernel is not
discriminative enough, resulting in high redundancy of the ViT layers, and (2)
the complexity in computation and memory is quadratic in the sequence length.
In this paper, we propose a novel attention operator, called lightweight
structure-aware attention (LiSA), which has a better representation power with
log-linear complexity. Our operator learns structural patterns by using a set
of relative position embeddings (RPEs). To achieve log-linear complexity, the
RPEs are approximated with fast Fourier transforms. Our experiments and
ablation studies demonstrate that ViTs based on the proposed operator
outperform self-attention and other existing operators, achieving
state-of-the-art results on ImageNet, and competitive results on other visual
understanding benchmarks such as COCO and Something-Something-V2. The source
code of our approach will be released online.",0,1,0,0,1,0,0.975994,5.0,0.941364,63
3e93e5de-7fd0-4bcb-9077-dfab3dc359b6,Multi-sensor large-scale dataset for multi-view 3D reconstruction,8,0.0605077,0.293736,"We present a new multi-sensor dataset for multi-view 3D surface
reconstruction. It includes registered RGB and depth data from sensors of
different resolutions and modalities: smartphones, Intel RealSense, Microsoft
Kinect, industrial cameras, and structured-light scanner. The scenes are
selected to emphasize a diverse set of material properties challenging for
existing algorithms. We provide around 1.4 million images of 107 different
scenes acquired from 100 viewing directions under 14 lighting conditions. We
expect our dataset will be useful for evaluation and training of 3D
reconstruction algorithms and for related tasks. The dataset is available at
skoltech3d.appliedai.tech.",0,1,0,1,0,0,0.330491,8.0,0.684662,106
3cc3d539-518c-4c36-aec7-d2c0f7ea41d5,ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection,50,0.735507,0.977506,"Existing approaches for unsupervised point cloud pre-training are constrained
to either scene-level or point/voxel-level instance discrimination. Scene-level
methods tend to lose local details that are crucial for recognizing the road
objects, while point/voxel-level methods inherently suffer from limited
receptive field that is incapable of perceiving large objects or context
environments. Considering region-level representations are more suitable for 3D
object detection, we devise a new unsupervised point cloud pre-training
framework, called ProposalContrast, that learns robust 3D representations by
contrasting region proposals. Specifically, with an exhaustive set of region
proposals sampled from each point cloud, geometric point relations within each
proposal are modeled for creating expressive proposal representations. To
better accommodate 3D detection properties, ProposalContrast optimizes with
both inter-cluster and inter-proposal separation, i.e., sharpening the
discriminativeness of proposal representations across semantic classes and
object instances. The generalizability and transferability of ProposalContrast
are verified on various 3D detectors (i.e., PV-RCNN, CenterPoint, PointPillars
and PointRCNN) and datasets (i.e., KITTI, Waymo and ONCE).",1,0,1,0,0,0,0.963826,6.0,0.931724,47
5a0e54e4-0b72-4ed3-9d9a-ef272ba5ac67,Collaborative Propagation on Multiple Instance Graphs for 3D Instance Segmentation with Single-point Supervision,1,0.0184957,0.0461746,"Instance segmentation on 3D point clouds has been attracting increasing
attention due to its wide applications, especially in scene understanding
areas. However, most existing methods operate on fully annotated data while
manually preparing ground-truth labels at point-level is very cumbersome and
labor-intensive. To address this issue, we propose a novel weakly supervised
method RWSeg that only requires labeling one object with one point. With these
sparse weak labels, we introduce a unified framework with two branches to
propagate semantic and instance information respectively to unknown regions
using self-attention and a cross-graph random walk method. Specifically, we
propose a Cross-graph Competing Random Walks (CRW) algorithm that encourages
competition among different instance graphs to resolve ambiguities in closely
placed objects, improving instance assignment accuracy. RWSeg generates
high-quality instance-level pseudo labels. Experimental results on ScanNet-v2
and S3DIS datasets show that our approach achieves comparable performance with
fully-supervised methods and outperforms previous weakly-supervised methods by
a substantial margin.",0,0,0,0,0,0,0.863333,7.0,0.8684,53
d0c2a34f-eba0-4d77-9dc9-81f1f485f108,IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding Alignment,9,0.0739705,0.362645,"This paper investigates the problem of temporally interpolating dynamic 3D
point clouds with large non-rigid deformation. We formulate the problem as
estimation of point-wise trajectories (i.e., smooth curves) and further reason
that temporal irregularity and under-sampling are two major challenges. To
tackle the challenges, we propose IDEA-Net, an end-to-end deep learning
framework, which disentangles the problem under the assistance of the
explicitly learned temporal consistency. Specifically, we propose a temporal
consistency learning module to align two consecutive point cloud frames
point-wisely, based on which we can employ linear interpolation to obtain
coarse trajectories/in-between frames. To compensate the high-order nonlinear
components of trajectories, we apply aligned feature embeddings that encode
local geometry properties to regress point-wise increments, which are combined
with the coarse estimations. We demonstrate the effectiveness of our method on
various point cloud sequences and observe large improvement over
state-of-the-art methods both quantitatively and visually. Our framework can
bring benefits to 3D motion data acquisition. The source code is publicly
available at https://github.com/ZENGYIMING-EAMON/IDEA-Net.git.",1,1,0,0,1,0,0.392463,8.0,0.711762,42
e46974db-db06-4c9c-a2b6-3b0d2117e240,Learning of Structurally Unambiguous Probabilistic Grammars,5,0.0704469,0.417669,"The problem of identifying a probabilistic context free grammar has two
aspects: the first is determining the grammar's topology (the rules of the
grammar) and the second is estimating probabilistic weights for each rule.
Given the hardness results for learning context-free grammars in general, and
probabilistic grammars in particular, most of the literature has concentrated
on the second problem. In this work we address the first problem. We restrict
attention to structurally unambiguous weighted context-free grammars (SUWCFG)
and provide a query learning algorithm for \structurally unambiguous
probabilistic context-free grammars (SUPCFG). We show that SUWCFG can be
represented using \emph{co-linear multiplicity tree automata} (CMTA), and
provide a polynomial learning algorithm that learns CMTAs. We show that the
learned CMTA can be converted into a probabilistic grammar, thus providing a
complete algorithm for learning a structurally unambiguous probabilistic
context free grammar (both the grammar topology and the probabilistic weights)
using structured membership queries and structured equivalence queries. A
summarized version of this work was published at AAAI 21.",0,0,0,0,0,0,0.0002841,31.0,0.684662,53
1b1ad592-7ffc-4d5e-abee-88caad768d64,Demystifying Prompts in Language Models via Perplexity Estimation,96,0.753727,0.953758,"Language models can be prompted to perform a wide variety of zero- and
few-shot learning problems. However, performance varies significantly with the
choice of prompt, and we do not yet understand why this happens or how to pick
the best prompts. In this work, we analyze the factors that contribute to this
variance and establish a new empirical hypothesis: the performance of a prompt
is coupled with the extent to which the model is familiar with the language it
contains. Over a wide range of tasks, we show that the lower the perplexity of
the prompt is, the better the prompt is able to perform the task. As a result,
we devise a method for creating prompts: (1) automatically extend a small seed
set of manually written prompts by paraphrasing using GPT3 and backtranslation
and (2) choose the lowest perplexity prompts to get significant gains in
performance.",0,0,0,0,0,1,0.907101,4.0,0.81402,47
6b7d334c-c9a9-4c6e-9682-4171a7756cf6,HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation,7,0.048313,0.835253,"Language models with the Transformers structure have shown great performance
in natural language processing. However, there still poses problems when
fine-tuning pre-trained language models on downstream tasks, such as
over-fitting or representation collapse. In this work, we propose HyPe, a
simple yet effective fine-tuning technique to alleviate such problems by
perturbing hidden representations of Transformers layers. Unlike previous works
that only add noise to inputs or parameters, we argue that the hidden
representations of Transformers layers convey more diverse and meaningful
language information. Therefore, making the Transformers layers more robust to
hidden representation perturbations can further benefit the fine-tuning of PLMs
en bloc. We conduct extensive experiments and analyses on GLUE and other
natural language inference datasets. Results demonstrate that HyPe outperforms
vanilla fine-tuning and enhances generalization of hidden representations from
different layers. In addition, HyPe acquires negligible computational
overheads, and is better than and compatible with previous state-of-the-art
fine-tuning techniques.",1,1,0,0,1,0,0.595805,7.0,0.755956,48
f33dcd74-f508-4a2c-b7bf-d85f20d76408,"Homomorphisms Between Transfer, Multi-Task, and Meta-Learning Systems",5,0.0527064,0.889802,"Transfer learning, multi-task learning, and meta-learning are well-studied
topics concerned with the generalization of knowledge across learning tasks and
are closely related to general intelligence. But, the formal, general systems
differences between them are underexplored in the literature. This lack of
systems-level formalism leads to difficulties in coordinating related,
inter-disciplinary engineering efforts. This manuscript formalizes transfer
learning, multi-task learning, and meta-learning as abstract learning systems,
consistent with the formal-minimalist abstract systems theory of Mesarovic and
Takahara. Moreover, it uses the presented formalism to relate the three
concepts of learning in terms of composition, hierarchy, and structural
homomorphism. Findings are readily depicted in terms of input-output systems,
highlighting the ease of delineating formal, general systems differences
between transfer, multi-task, and meta-learning.",0,0,0,0,0,1,0.0541553,10.0,0.550237,11
47727e5b-441c-4fe2-8d45-2307266b1e00,Multilevel Hierarchical Network with Multiscale Sampling for Video Question Answering,20,0.21644,0.655592,"Video question answering (VideoQA) is challenging given its multimodal
combination of visual understanding and natural language processing. While most
existing approaches ignore the visual appearance-motion information at
different temporal scales, it is unknown how to incorporate the multilevel
processing capacity of a deep learning model with such multiscale information.
Targeting these issues, this paper proposes a novel Multilevel Hierarchical
Network (MHN) with multiscale sampling for VideoQA. MHN comprises two modules,
namely Recurrent Multimodal Interaction (RMI) and Parallel Visual Reasoning
(PVR). With a multiscale sampling, RMI iterates the interaction of
appearance-motion information at each scale and the question embeddings to
build the multilevel question-guided visual representations. Thereon, with a
shared transformer encoder, PVR infers the visual cues at each level in
parallel to fit with answering different question types that may rely on the
visual information at relevant levels. Through extensive experiments on three
VideoQA datasets, we demonstrate improved performances than previous
state-of-the-arts and justify the effectiveness of each part of our method.",0,0,0,0,1,0,0.67541,7.0,0.786931,29
96af97ab-785b-4c0c-a8c5-f4f55446800a,Generalized Strategic Classification and the Case of Aligned Incentives,15,0.0530539,0.609014,"Strategic classification studies learning in settings where self-interested
users can strategically modify their features to obtain favorable predictive
outcomes. A key working assumption, however, is that ""favorable"" always means
""positive""; this may be appropriate in some applications (e.g., loan approval),
but reduces to a fairly narrow view of what user interests can be. In this work
we argue for a broader perspective on what accounts for strategic user
behavior, and propose and study a flexible model of generalized strategic
classification. Our generalized model subsumes most current models but includes
other novel settings; among these, we identify and target one intriguing
sub-class of problems in which the interests of users and the system are
aligned. This setting reveals a surprising fact: that standard max-margin
losses are ill-suited for strategic inputs. Returning to our fully generalized
model, we propose a novel max-margin framework for strategic learning that is
practical and effective, and which we analyze theoretically. We conclude with a
set of experiments that empirically demonstrate the utility of our approach.",0,0,1,0,0,0,0.265867,4.0,0.304089,37
b2033aac-4643-401f-aaf2-463e523b8aa6,On the Effectiveness of Compact Biomedical Transformers,16,0.453166,0.664962,"Language models pre-trained on biomedical corpora, such as BioBERT, have
recently shown promising results on downstream biomedical tasks. Many existing
pre-trained models, on the other hand, are resource-intensive and
computationally heavy owing to factors such as embedding size, hidden
dimension, and number of layers. The natural language processing (NLP)
community has developed numerous strategies to compress these models utilising
techniques such as pruning, quantisation, and knowledge distillation, resulting
in models that are considerably faster, smaller, and subsequently easier to use
in practice. By the same token, in this paper we introduce six lightweight
models, namely, BioDistilBERT, BioTinyBERT, BioMobileBERT, DistilBioBERT,
TinyBioBERT, and CompactBioBERT which are obtained either by knowledge
distillation from a biomedical teacher or continual learning on the Pubmed
dataset via the Masked Language Modelling (MLM) objective. We evaluate all of
our models on three biomedical tasks and compare them with BioBERT-v1.1 to
create efficient lightweight models that perform on par with their larger
counterparts. All the models will be publicly available on our Huggingface
profile at https://huggingface.co/nlpie and the codes used to run the
experiments will be available at
https://github.com/nlpie-research/Compact-Biomedical-Transformers.",1,1,0,0,0,0,0.983488,7.0,0.971778,50
221108f1-234b-404a-bcb1-ea51c6be75e9,Polarimetric Multi-View Inverse Rendering,21,0.249239,0.434152,"A polarization camera has great potential for 3D reconstruction since the
angle of polarization (AoP) and the degree of polarization (DoP) of reflected
light are related to an object's surface normal. In this paper, we propose a
novel 3D reconstruction method called Polarimetric Multi-View Inverse Rendering
(Polarimetric MVIR) that effectively exploits geometric, photometric, and
polarimetric cues extracted from input multi-view color-polarization images. We
first estimate camera poses and an initial 3D model by geometric reconstruction
with a standard structure-from-motion and multi-view stereo pipeline. We then
refine the initial model by optimizing photometric rendering errors and
polarimetric errors using multi-view RGB, AoP, and DoP images, where we propose
a novel polarimetric cost function that enables an effective constraint on the
estimated surface normal of each vertex, while considering four possible
ambiguous azimuth angles revealed from the AoP measurement. The weight for the
polarimetric cost is effectively determined based on the DoP measurement, which
is regarded as the reliability of polarimetric information. Experimental
results using both synthetic and real data demonstrate that our Polarimetric
MVIR can reconstruct a detailed 3D shape without assuming a specific surface
material and lighting condition.",0,0,0,0,0,0,0.0765187,11.0,0.623625,70
d5ef96fc-b4e5-41e5-b895-53be21d7e4c1,Policy Optimization with Advantage Regularization for Long-Term Fairness in Decision Systems,8,0.0239799,0.368922,"Long-term fairness is an important factor of consideration in designing and
deploying learning-based decision systems in high-stake decision-making
contexts. Recent work has proposed the use of Markov Decision Processes (MDPs)
to formulate decision-making with long-term fairness requirements in
dynamically changing environments, and demonstrated major challenges in
directly deploying heuristic and rule-based policies that worked well in static
environments. We show that policy optimization methods from deep reinforcement
learning can be used to find strictly better decision policies that can often
achieve both higher overall utility and less violation of the fairness
requirements, compared to previously-known strategies. In particular, we
propose new methods for imposing fairness requirements in policy optimization
by regularizing the advantage evaluation of different actions. Our proposed
methods make it easy to impose fairness constraints without reward engineering
or sacrificing training efficiency. We perform detailed analyses in three
established case studies, including attention allocation in incident
monitoring, bank loan approval, and vaccine distribution in population
networks.",0,0,0,0,0,0,0.055439,8.0,0.440809,35
7729be9f-2c8a-4e29-b7e0-867b826f9d68,Attention-based Random Forest and Contamination Model,18,0.0419757,0.512115,"A new approach called ABRF (the attention-based random forest) and its
modifications for applying the attention mechanism to the random forest (RF)
for regression and classification are proposed. The main idea behind the
proposed ABRF models is to assign attention weights with trainable parameters
to decision trees in a specific way. The weights depend on the distance between
an instance, which falls into a corresponding leaf of a tree, and instances,
which fall in the same leaf. This idea stems from representation of the
Nadaraya-Watson kernel regression in the form of a RF. Three modifications of
the general approach are proposed. The first one is based on applying the
Huber's contamination model and on computing the attention weights by solving
quadratic or linear optimization problems. The second and the third
modifications use the gradient-based algorithms for computing trainable
parameters. Numerical experiments with various regression and classification
datasets illustrate the proposed method.",0,0,0,0,0,0,0.149065,7.0,0.509545,44
0fecc4c7-09af-4b1f-b843-829ec93e1401,Near-Optimal Multi-Agent Learning for Safe Coverage Control,9,0.504426,0.836703,"In multi-agent coverage control problems, agents navigate their environment
to reach locations that maximize the coverage of some density. In practice, the
density is rarely known $\textit{a priori}$, further complicating the original
NP-hard problem. Moreover, in many applications, agents cannot visit arbitrary
locations due to $\textit{a priori}$ unknown safety constraints. In this paper,
we aim to efficiently learn the density to approximately solve the coverage
problem while preserving the agents' safety. We first propose a conditionally
linear submodular coverage function that facilitates theoretical analysis.
Utilizing this structure, we develop MacOpt, a novel algorithm that efficiently
trades off the exploration-exploitation dilemma due to partial observability,
and show that it achieves sublinear regret. Next, we extend results on
single-agent safe exploration to our multi-agent setting and propose SafeMac
for safe coverage and exploration. We analyze SafeMac and give first of its
kind results: near optimal coverage in finite time while provably guaranteeing
safety. We extensively evaluate our algorithms on synthetic and real problems,
including a bio-diversity monitoring task under safety constraints, where
SafeMac outperforms competing methods.",1,0,0,0,0,0,0.853722,10.0,0.904406,75
74648a8a-0102-4bc7-9232-afed4f3a1e35,Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer,124,0.755393,0.876453,"Videos are created to express emotion, exchange information, and share
experiences. Video synthesis has intrigued researchers for a long time. Despite
the rapid progress driven by advances in visual synthesis, most existing
studies focus on improving the frames' quality and the transitions between
them, while little progress has been made in generating longer videos. In this
paper, we present a method that builds on 3D-VQGAN and transformers to generate
videos with thousands of frames. Our evaluation shows that our model trained on
16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,
and Taichi-HD datasets can generate diverse, coherent, and high-quality long
videos. We also showcase conditional extensions of our approach for generating
meaningful long videos by incorporating temporal information with text and
audio. Videos and code can be found at
https://songweige.github.io/projects/tats/index.html.",1,1,0,0,0,0,0.792061,6.0,0.806994,87
84aad538-8961-4105-ad9f-9eb555cee5ec,"3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos",7,0.0969732,0.660262,"We present 3MASSIV, a multilingual, multimodal and multi-aspect,
expertly-annotated dataset of diverse short videos extracted from short-video
social media platform - Moj. 3MASSIV comprises of 50k short videos (20 seconds
average duration) and 100K unlabeled videos in 11 different languages and
captures popular short video trends like pranks, fails, romance, comedy
expressed via unique audio-visual formats like self-shot videos, reaction
videos, lip-synching, self-sung songs, etc. 3MASSIV presents an opportunity for
multimodal and multilingual semantic understanding on these unique videos by
annotating them for concepts, affective states, media types, and audio
language. We present a thorough analysis of 3MASSIV and highlight the variety
and unique aspects of our dataset compared to other contemporary popular
datasets with strong baselines. We also show how the social media content in
3MASSIV is dynamic and temporal in nature, which can be used for semantic
understanding tasks and cross-lingual analysis.",1,1,1,1,0,0,0.305623,9.0,0.70911,83
833d1c10-c160-483e-be7e-1bc754a97e5e,Asking the Right Questions in Low Resource Template Extraction,4,0.21696,0.41027,"Information Extraction (IE) researchers are mapping tasks to Question
Answering (QA) in order to leverage existing large QA resources, and thereby
improve data efficiency. Especially in template extraction (TE), mapping an
ontology to a set of questions can be more time-efficient than collecting
labeled examples. We ask whether end users of TE systems can design these
questions, and whether it is beneficial to involve an NLP practitioner in the
process. We compare questions to other ways of phrasing natural language
prompts for TE. We propose a novel model to perform TE with prompts, and find
it benefits from questions over other styles of prompts, and that they do not
require an NLP background to author.",0,1,0,0,0,0,0.987362,6.0,0.977594,33
40b33ea9-b6cc-4163-84d4-2e3acccccc98,Endowing Language Models with Multimodal Knowledge Graph Representations,7,0.0911072,0.513313,"We propose a method to make natural language understanding models more
parameter efficient by storing knowledge in an external knowledge graph (KG)
and retrieving from this KG using a dense index. Given (possibly multilingual)
downstream task data, e.g., sentences in German, we retrieve entities from the
KG and use their multimodal representations to improve downstream task
performance. We use the recently released VisualSem KG as our external
knowledge repository, which covers a subset of Wikipedia and WordNet entities,
and compare a mix of tuple-based and graph-based algorithms to learn entity and
relation representations that are grounded on the KG multimodal information. We
demonstrate the usefulness of the learned entity representations on two
downstream tasks, and show improved performance on the multilingual named
entity recognition task by $0.3\%$--$0.7\%$ F1, while we achieve up to $2.5\%$
improvement in accuracy on the visual sense disambiguation task. All our code
and data are available in: \url{https://github.com/iacercalixto/visualsem-kg}.",0,1,0,0,0,0,0.912681,8.0,0.910227,49
4bbec74a-2fbb-43ec-b7db-9c9dbd992884,RED-ACE: Robust Error Detection for ASR using Confidence Embeddings,4,0.0185216,0.0753025,"ASR Error Detection (AED) models aim to post-process the output of Automatic
Speech Recognition (ASR) systems, in order to detect transcription errors.
Modern approaches usually use text-based input, comprised solely of the ASR
transcription hypothesis, disregarding additional signals from the ASR model.
Instead, we propose to utilize the ASR system's word-level confidence scores
for improving AED performance. Specifically, we add an ASR Confidence Embedding
(ACE) layer to the AED model's encoder, allowing us to jointly encode the
confidence scores and the transcribed text into a contextualized
representation. Our experiments show the benefits of ASR confidence scores for
AED, their complementary effect over the textual signal, as well as the
effectiveness and robustness of ACE for combining these signals. To foster
further research, we publish a novel AED dataset consisting of ASR outputs on
the LibriSpeech corpus with annotated transcription errors.",1,1,0,1,0,0,0.0122501,10.0,0.39945,24
7c133b5d-3109-4ba2-b5bc-186b6db087c3,Controllable Text Generation via Probability Density Estimation in the Latent Space,8,0.0522652,0.147638,"Previous work on controllable text generation has explored the idea of
control from the latent space, such as optimizing a representation with
attribute-related classifiers or sampling a representation from relevant
discrete samples. However, they are not effective enough in modeling both the
latent space and the control, leaving controlled text with low quality and
diversity. In this work, we propose a novel control framework using probability
density estimation in the latent space. Our method utilizes an invertible
transformation function, the Normalizing Flow, that maps the complex
distributions in the latent space to simple Gaussian distributions in the prior
space. Thus, we can perform sophisticated and flexible control in the prior
space and feed the control effects back into the latent space owing to the
one-one-mapping property of invertible transformations. Experiments on
single-attribute controls and multi-attribute control reveal that our method
outperforms several strong baselines on attribute relevance and text quality
and achieves the SOTA. Further analysis of control strength adjustment
demonstrates the flexibility of our control strategy.",1,0,0,0,1,0,0.449085,5.0,0.574668,54
9c35008d-e8f0-4a78-9b33-8bf57cc705c2,Generating Natural Language Proofs with Verifier-Guided Search,44,0.413909,0.396917,"Reasoning over natural language is a challenging problem in NLP. In this
work, we focus on proof generation: Given a hypothesis and a set of supporting
facts, the model generates a proof tree indicating how to derive the hypothesis
from supporting facts. Compared to generating the entire proof in one shot,
stepwise generation can better exploit the compositionality and generalize to
longer proofs but has achieved limited success on real-world data. Existing
stepwise methods struggle to generate proof steps that are both logically valid
and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps
given the hypothesis. In this paper, we present a novel stepwise method,
NLProofS (Natural Language Proof Search), which learns to generate relevant
steps conditioning on the hypothesis. At the core of our approach, we train an
independent verifier to check the validity of the proof steps to prevent
hallucination. Instead of generating steps greedily, we search for proofs
maximizing a global proof score judged by the verifier. NLProofS achieves
state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it
improves the correctness of predicted proofs from 27.7% to 33.3% in the
distractor setting of EntailmentBank, demonstrating the effectiveness of
NLProofS in generating challenging human-authored proofs.",1,0,0,0,1,0,0.708144,4.0,0.649698,70
72f079cf-e8d0-48b7-b0a3-564535f357ea,Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models,25,0.714391,0.304225,"We explore the idea of compressing the prompts used to condition language
models, and show that compressed prompts can retain a substantive amount of
information about the original prompt. For severely compressed prompts, while
fine-grained information is lost, abstract information and general sentiments
can be retained with surprisingly few parameters, which can be useful in the
context of decode-time algorithms for controllability and toxicity reduction.
We explore contrastive conditioning to steer language model generation towards
desirable text and away from undesirable text, and find that some complex
prompts can be effectively compressed into a single token to guide generation.
We also show that compressed prompts are largely compositional, and can be
constructed such that they can be used to control independent aspects of
generated text.",1,0,0,0,0,0,0.993151,6.0,0.999456,36
e8e06637-9588-49a2-a659-4c9aad7ea5eb,Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training,36,0.324948,0.45366,"Large-scale vision-language pre-trained (VLP) models are prone to hallucinate
non-existent visual objects when generating text based on visual information.
In this paper, we systematically study the object hallucination problem from
three aspects. First, we examine recent state-of-the-art VLP models, showing
that they still hallucinate frequently, and models achieving better scores on
standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate
how different types of image encoding in VLP influence hallucination, including
region-based, grid-based, and patch-based. Surprisingly, we find that
patch-based features perform the best and smaller patch resolution yields a
non-trivial reduction in object hallucination. Third, we decouple various VLP
objectives and demonstrate that token-level image-text alignment and controlled
generation are crucial to reducing hallucination. Based on that, we propose a
simple yet effective VLP loss named ObjMLM to further mitigate object
hallucination. Results show that it reduces object hallucination by up to 17.4%
when tested on two benchmarks (COCO Caption for in-domain and NoCaps for
out-of-domain evaluation).",1,0,0,0,0,0,0.882776,6.0,0.858847,75
7e921b30-f125-4ef2-a606-989ac4a8eb20,MCTNet: A Multi-Scale CNN-Transformer Network for Change Detection in Optical Remote Sensing Images,5,0.0170168,0.308145,"For the task of change detection (CD) in remote sensing images, deep
convolution neural networks (CNNs)-based methods have recently aggregated
transformer modules to improve the capability of global feature extraction.
However, they suffer degraded CD performance on small changed areas due to the
simple single-scale integration of deep CNNs and transformer modules. To
address this issue, we propose a hybrid network based on multi-scale
CNN-transformer structure, termed MCTNet, where the multi-scale global and
local information are exploited to enhance the robustness of the CD performance
on changed areas with different sizes. Especially, we design the ConvTrans
block to adaptively aggregate global features from transformer modules and
local features from CNN layers, which provides abundant global-local features
with different scales. Experimental results demonstrate that our MCTNet
achieves better detection performance than existing state-of-the-art CD
methods.",0,1,0,0,1,0,0.772834,7.0,0.826286,24
615d778c-fbdd-4954-bc55-bcb869775ee0,Asynchronous Distributed Bilevel Optimization,7,0.0866542,0.503241,"Bilevel optimization plays an essential role in many machine learning tasks,
ranging from hyperparameter optimization to meta-learning. Existing studies on
bilevel optimization, however, focus on either centralized or synchronous
distributed setting. The centralized bilevel optimization approaches require
collecting massive amount of data to a single server, which inevitably incur
significant communication expenses and may give rise to data privacy risks.
Synchronous distributed bilevel optimization algorithms, on the other hand,
often face the straggler problem and will immediately stop working if a few
workers fail to respond. As a remedy, we propose Asynchronous Distributed
Bilevel Optimization (ADBO) algorithm. The proposed ADBO can tackle bilevel
optimization problems with both nonconvex upper-level and lower-level objective
functions, and its convergence is theoretically guaranteed. Furthermore, it is
revealed through theoretic analysis that the iteration complexity of ADBO to
obtain the $\epsilon$-stationary point is upper bounded by
$\mathcal{O}(\frac{1}{{{\epsilon ^2}}})$. Thorough empirical studies on public
datasets have been conducted to elucidate the effectiveness and efficiency of
the proposed ADBO.",1,0,0,0,1,0,0.356161,7.0,0.652897,69
0f61e374-4734-48d1-8280-d8a30aec25c9,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,114,0.477162,0.999935,"Social intelligence and Theory of Mind (ToM), i.e., the ability to reason
about the different mental states, intents, and reactions of all people
involved, allow humans to effectively navigate and understand everyday social
interactions. As NLP systems are used in increasingly complex social
situations, their ability to grasp social dynamics becomes crucial. In this
work, we examine the open question of social intelligence and Theory of Mind in
modern NLP systems from an empirical and theory-based perspective. We show that
one of today's largest language models (GPT-3; Brown et al., 2020) lacks this
kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et
al., 2019), which measures models' ability to understand intents and reactions
of participants of social interactions, and ToMi (Le et al., 2019), which
measures whether models can infer mental states and realities of participants
of situations. Our results show that models struggle substantially at these
Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on
SocialIQa and ToMi, respectively. To conclude, we draw on theories from
pragmatics to contextualize this shortcoming of large language models, by
examining the limitations stemming from their data, neural architecture, and
training paradigms. Challenging the prevalent narrative that only scale is
needed, we posit that person-centric NLP approaches might be more effective
towards neural Theory of Mind.
  In our updated version, we also analyze newer instruction tuned and RLFH
models for neural ToM. We find that even ChatGPT and GPT-4 do not display
emergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on
the ToMi questions related to mental states and realities.",1,0,0,0,0,0,0.247674,6.0,0.522307,138
78179e15-be7c-4906-b4e2-762b1bdcad36,Interpolated SelectionConv for Spherical Images and Surfaces,2,0.00327712,0.0812377,"We present a new and general framework for convolutional neural network
operations on spherical (or omnidirectional) images. Our approach represents
the surface as a graph of connected points that doesn't rely on a particular
sampling strategy. Additionally, by using an interpolated version of
SelectionConv, we can operate on the sphere while using existing 2D CNNs and
their weights. Since our method leverages existing graph implementations, it is
also fast and can be fine-tuned efficiently. Our method is also general enough
to be applied to any surface type, even those that are topologically
non-simple. We demonstrate the effectiveness of our technique on the tasks of
style transfer and segmentation for spheres as well as stylization for 3D
meshes. We provide a thorough ablation study of the performance of various
spherical sampling strategies.",0,0,0,0,0,0,0.0070048,9.0,0.270324,33
8d4cd5cf-28f2-484f-84fa-a8c306458024,MMES: Mixture Model based Evolution Strategy for Large-Scale Optimization,19,0.0303164,0.371766,"This work provides an efficient sampling method for the covariance matrix
adaptation evolution strategy (CMA-ES) in large-scale settings. In contract to
the Gaussian sampling in CMA-ES, the proposed method generates mutation vectors
from a mixture model, which facilitates exploiting the rich variable
correlations of the problem landscape within a limited time budget. We analyze
the probability distribution of this mixture model and show that it
approximates the Gaussian distribution of CMA-ES with a controllable accuracy.
We use this sampling method, coupled with a novel method for mutation strength
adaptation, to formulate the mixture model based evolution strategy (MMES) -- a
CMA-ES variant for large-scale optimization. The numerical simulations show
that, while significantly reducing the time complexity of CMA-ES, MMES
preserves the rotational invariance, is scalable to high dimensional problems,
and is competitive against the state-of-the-arts in performing global
optimization.",1,1,0,0,0,0,0.000569863,14.0,0.351481,64
1d1706d4-8f2f-46d7-9936-44d4c4326c14,StyleT2F: Generating Human Faces from Textual Description Using StyleGAN2,2,0.0111901,0.237265,"AI-driven image generation has improved significantly in recent years.
Generative adversarial networks (GANs), like StyleGAN, are able to generate
high-quality realistic data and have artistic control over the output, as well.
In this work, we present StyleT2F, a method of controlling the output of
StyleGAN2 using text, in order to be able to generate a detailed human face
from textual description. We utilize StyleGAN's latent space to manipulate
different facial features and conditionally sample the required latent code,
which embeds the facial features mentioned in the input text. Our method proves
to capture the required features correctly and shows consistency between the
input text and the output images. Moreover, our method guarantees
disentanglement on manipulating a wide range of facial features that
sufficiently describes a human face.",1,1,0,0,0,0,0.970843,7.0,0.950471,17
f575b10d-1ee5-4829-a070-c5419d4f78f6,Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities,87,0.673145,0.999974,"Assembly101 is a new procedural activity dataset featuring 4321 videos of
people assembling and disassembling 101 ""take-apart"" toy vehicles. Participants
work without fixed instructions, and the sequences feature rich and natural
variations in action ordering, mistakes, and corrections. Assembly101 is the
first multi-view action dataset, with simultaneous static (8) and egocentric
(4) recordings. Sequences are annotated with more than 100K coarse and 1M
fine-grained action segments, and 18M 3D hand poses. We benchmark on three
action understanding tasks: recognition, anticipation and temporal
segmentation. Additionally, we propose a novel task of detecting mistakes. The
unique recording format and rich set of annotations allow us to investigate
generalization to new toys, cross-view transfer, long-tailed distributions, and
pose vs. appearance. We envision that Assembly101 will serve as a new challenge
to investigate various activity understanding problems.",1,0,0,0,0,0,0.569795,9.0,0.802262,70
142696c4-843f-4a46-a4e4-733b480892cc,Patch-Craft Self-Supervised Training for Correlated Image Denoising,4,0.0121664,0.234659,"Supervised neural networks are known to achieve excellent results in various
image restoration tasks. However, such training requires datasets composed of
pairs of corrupted images and their corresponding ground truth targets.
Unfortunately, such data is not available in many applications. For the task of
image denoising in which the noise statistics is unknown, several
self-supervised training methods have been proposed for overcoming this
difficulty. Some of these require knowledge of the noise model, while others
assume that the contaminating noise is uncorrelated, both assumptions are too
limiting for many practical needs. This work proposes a novel self-supervised
training technique suitable for the removal of unknown correlated noise. The
proposed approach neither requires knowledge of the noise model nor access to
ground truth targets. The input to our algorithm consists of easily captured
bursts of noisy shots. Our algorithm constructs artificial patch-craft images
from these bursts by patch matching and stitching, and the obtained crafted
images are used as targets for the training. Our method does not require
registration of the images within the burst. We evaluate the proposed framework
through extensive experiments with synthetic and real image noise.",1,1,0,0,0,0,0.0976638,10.0,0.611528,51
3d02eb8e-fe2f-4e1b-b5e2-ffa5c8a93757,Integrating Prior Knowledge in Contrastive Learning with Kernel,3,0.0126947,0.078564,"Data augmentation is a crucial component in unsupervised contrastive learning
(CL). It determines how positive samples are defined and, ultimately, the
quality of the learned representation. In this work, we open the door to new
perspectives for CL by integrating prior knowledge, given either by generative
models -- viewed as prior representations -- or weak attributes in the positive
and negative sampling. To this end, we use kernel theory to propose a novel
loss, called decoupled uniformity, that i) allows the integration of prior
knowledge and ii) removes the negative-positive coupling in the original
InfoNCE loss. We draw a connection between contrastive learning and conditional
mean embedding theory to derive tight bounds on the downstream classification
loss. In an unsupervised setting, we empirically demonstrate that CL benefits
from generative models to improve its representation both on natural and
medical images. In a weakly supervised scenario, our framework outperforms
other unconditional and conditional CL approaches.",0,0,0,0,0,0,0.544291,5.0,0.629928,62
774df8c7-6a9f-4d90-a834-34c70c24f23f,Rethinking the Role of Pre-Trained Networks in Source-Free Domain Adaptation,6,0.0608319,0.552257,"Source-free domain adaptation (SFDA) aims to adapt a source model trained on
a fully-labeled source domain to an unlabeled target domain. Large-data
pre-trained networks are used to initialize source models during source
training, and subsequently discarded. However, source training can cause the
model to overfit to source data distribution and lose applicable target domain
knowledge. We propose to integrate the pre-trained network into the target
adaptation process as it has diversified features important for generalization
and provides an alternate view of features and classification decisions
different from the source model. We propose to distil useful target domain
information through a co-learning strategy to improve target pseudolabel
quality for finetuning the source model. Evaluation on 4 benchmark datasets
show that our proposed strategy improves adaptation performance and can be
successfully integrated with existing SFDA methods. Leveraging modern
pre-trained networks that have stronger representation learning ability in the
co-learning strategy further boosts performance.",0,1,0,0,0,0,0.590039,6.0,0.712655,57
7e8a66a4-09be-4d98-86f7-f74a6b66533c,2nd Place Solution to Google Universal Image Embedding,1,0.00226385,0.0432625,"Image representations are a critical building block of computer vision
applications. This paper presents the 2nd place solution to the Google
Universal Image Embedding Competition, which is part of the ECCV2022
instance-level recognition workshops. We use the instance-level fine-grained
image classification method to complete this competition. We focus on data
building and processing, model structure, and training strategies. Finally, the
solution scored 0.713 on the public leaderboard and 0.709 on the private
leaderboard.",1,1,0,0,0,0,0.925756,3.0,0.782075,9
06e9469c-ee4a-4e3c-aa15-8cf37e2ef69e,RustSEG -- Automated segmentation of corrosion using deep learning,3,0.0229517,0.298259,"The inspection of infrastructure for corrosion remains a task that is
typically performed manually by qualified engineers or inspectors. This task of
inspection is laborious, slow, and often requires complex access. Recently,
deep learning based algorithms have revealed promise and performance in the
automatic detection of corrosion. However, to date, research regarding the
segmentation of images for automated corrosion detection has been limited, due
to the lack of availability of per-pixel labelled data sets which are required
for model training. Herein, a novel deep learning approach (termed RustSEG) is
presented, that can accurately segment images for automated corrosion
detection, without the requirement of per-pixel labelled data sets for
training. The RustSEG method will first, using deep learning techniques,
determine if corrosion is present in an image (i.e. a classification task), and
then if corrosion is present, the model will examine what pixels in the
original image contributed to that classification decision. Finally, the method
can refine its predictions into a pixel-level segmentation mask. In ideal
cases, the method is able to generate precise masks of corrosion in images,
demonstrating that the automated segmentation of corrosion without per-pixel
training data is possible, addressing a significant hurdle in automated
infrastructure inspection.",0,1,0,0,0,0,0.0165716,13.0,0.561449,45
46a31e35-0c4d-4cc7-9526-64d487f30303,Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning,19,0.191259,0.348399,"Impressive results in natural language processing (NLP) based on the
Transformer neural network architecture have inspired researchers to explore
viewing offline reinforcement learning (RL) as a generic sequence modeling
problem. Recent works based on this paradigm have achieved state-of-the-art
results in several of the mostly deterministic offline Atari and D4RL
benchmarks. However, because these methods jointly model the states and actions
as a single sequencing problem, they struggle to disentangle the effects of the
policy and world dynamics on the return. Thus, in adversarial or stochastic
environments, these methods lead to overly optimistic behavior that can be
dangerous in safety-critical systems like autonomous driving. In this work, we
propose a method that addresses this optimism bias by explicitly disentangling
the policy and world models, which allows us at test time to search for
policies that are robust to multiple possible futures in the environment. We
demonstrate our method's superior performance on a variety of autonomous
driving tasks in simulation.",1,1,0,0,0,0,0.942482,6.0,0.906644,38
e08d1e7b-f626-4c9c-804d-3f7cdc03509b,Polysemanticity and Capacity in Neural Networks,11,0.0462315,0.664065,"Individual neurons in neural networks often represent a mixture of unrelated
features. This phenomenon, called polysemanticity, can make interpreting neural
networks more difficult and so we aim to understand its causes. We propose
doing so through the lens of feature \emph{capacity}, which is the fractional
dimension each feature consumes in the embedding space. We show that in a toy
model the optimal capacity allocation tends to monosemantically represent the
most important features, polysemantically represent less important features (in
proportion to their impact on the loss), and entirely ignore the least
important features. Polysemanticity is more prevalent when the inputs have
higher kurtosis or sparsity and more prevalent in some architectures than
others. Given an optimal allocation of capacity, we go on to study the geometry
of the embedding space. We find a block-semi-orthogonal structure, with
differing block sizes in different models, highlighting the impact of model
architecture on the interpretability of its neurons.",0,0,0,0,0,0,0.53391,6.0,0.68676,5
66871836-01c8-4e27-b3b4-023a2ce249f2,DCVQE: A Hierarchical Transformer for Video Quality Assessment,1,0.0253074,0.0690705,"The explosion of user-generated videos stimulates a great demand for
no-reference video quality assessment (NR-VQA). Inspired by our observation on
the actions of human annotation, we put forward a Divide and Conquer Video
Quality Estimator (DCVQE) for NR-VQA. Starting from extracting the frame-level
quality embeddings (QE), our proposal splits the whole sequence into a number
of clips and applies Transformers to learn the clip-level QE and update the
frame-level QE simultaneously; another Transformer is introduced to combine the
clip-level QE to generate the video-level QE. We call this hierarchical
combination of Transformers as a Divide and Conquer Transformer (DCTr) layer.
An accurate video quality feature extraction can be achieved by repeating the
process of this DCTr layer several times. Taking the order relationship among
the annotated data into account, we also propose a novel correlation loss term
for model training. Experiments on various datasets confirm the effectiveness
and robustness of our DCVQE model.",0,1,0,0,0,0,0.610896,11.0,0.84844,71
f5f09302-a932-47da-b213-416c03a7e11b,Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder,1,0.0175448,0.0863018,"It is common practice to reuse models initially trained on different data to
increase downstream task performance. Especially in the computer vision domain,
ImageNet-pretrained weights have been successfully used for various tasks. In
this work, we investigate the impact of transfer learning for segmentation
problems, being pixel-wise classification problems that can be tackled with
encoder-decoder architectures. We find that transfer learning the decoder does
not help downstream segmentation tasks, while transfer learning the encoder is
truly beneficial. We demonstrate that pretrained weights for a decoder may
yield faster convergence, but they do not improve the overall model performance
as one can obtain equivalent results with randomly initialized decoders.
However, we show that it is more effective to reuse encoder weights trained on
a segmentation or reconstruction task than reusing encoder weights trained on
classification tasks. This finding implicates that using ImageNet-pretrained
encoders for downstream segmentation problems is suboptimal. We also propose a
contrastive self-supervised approach with multiple self-reconstruction tasks,
which provides encoders that are suitable for transfer learning in segmentation
problems in the absence of segmentation labels.",0,0,0,0,0,0,0.901738,5.0,0.846436,24
114c1a56-466a-4ec2-9846-b1ee9da70883,Generalized Inter-class Loss for Gait Recognition,5,0.0827429,0.435356,"Gait recognition is a unique biometric technique that can be performed at a
long distance non-cooperatively and has broad applications in public safety and
intelligent traffic systems. Previous gait works focus more on minimizing the
intra-class variance while ignoring the significance in constraining
inter-class variance. To this end, we propose a generalized inter-class loss
which resolves the inter-class variance from both sample-level feature
distribution and class-level feature distribution. Instead of equal penalty
strength on pair scores, the proposed loss optimizes sample-level inter-class
feature distribution by dynamically adjusting the pairwise weight. Further, in
class-level distribution, generalized inter-class loss adds a constraint on the
uniformity of inter-class feature distribution, which forces the feature
representations to approximate a hypersphere and keep maximal inter-class
variance. In addition, the proposed method automatically adjusts the margin
between classes which enables the inter-class feature distribution to be more
flexible. The proposed method can be generalized to different gait recognition
networks and achieves significant improvements. We conduct a series of
experiments on CASIA-B and OUMVLP, and the experimental results show that the
proposed loss can significantly improve the performance and achieves the
state-of-the-art performances.",1,1,0,0,1,0,0.434255,8.0,0.72847,71
72188aa2-5952-4583-9ce5-f596464eb638,RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states,13,0.0922366,0.583682,"Multi-agent deep reinforcement learning makes optimal decisions dependent on
system states observed by agents, but any uncertainty on the observations may
mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement
learning (MFAC) is well-known in the multi-agent field since it can effectively
handle a scalability problem. However, it is sensitive to state perturbations
that can significantly degrade the team rewards. This work proposes a Robust
Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two
innovations: 1) a new objective function of training actors, composed of a
\emph{policy gradient function} that is related to the expected cumulative
discount reward on sampled clean states and an \emph{action loss function} that
represents the difference between actions taken on clean and adversarial
states; and 2) a repetitive regularization of the action loss, ensuring the
trained actors to obtain excellent performance. Furthermore, this work proposes
a game model named a State-Adversarial Stochastic Game (SASG). Despite the Nash
equilibrium of SASG may not exist, adversarial perturbations to states in the
RoMFAC are proven to be defensible based on SASG. Experimental results show
that RoMFAC is robust against adversarial perturbations while maintaining its
competitive performance in environments without perturbations.",0,0,0,0,0,0,0.809832,6.0,0.816215,27
1429e517-477d-455d-8cb1-f73b40d95ca9,AdaptKeyBERT: An Attention-Based approach towards Few-Shot & Zero-Shot Domain Adaptation of KeyBERT,5,0.0927445,0.287244,"Keyword extraction has been an important topic for modern natural language
processing. With its applications ranging from ontology generation, fact
verification in summarized text, and recommendation systems. While it has had
significant data-intensive applications, it is often hampered when the data set
is small. Downstream training for keyword extractors is a lengthy process and
requires a significant amount of data. Recently, Few-shot Learning (FSL) and
Zero-Shot Learning (ZSL) have been proposed to tackle this problem. Therefore,
we propose AdaptKeyBERT, a pipeline for training keyword extractors with LLM
bases by incorporating the concept of regularized attention into a pre-training
phase for downstream domain adaptation. As we believe our work has implications
to be utilized in the pipeline of FSL/ZSL and keyword extraction, we
open-source our code as well as provide the fine-tuning library of the same
name AdaptKeyBERT at https://github.com/AmanPriyanshu/AdaptKeyBERT.",1,1,0,0,0,0,0.108663,10.0,0.622802,18
ef161bf8-b155-42fe-9791-b0e47bdb4021,Learning from Drivers to Tackle the Amazon Last Mile Routing Research Challenge,4,0.0231044,0.332901,"The goal of the Amazon Last Mile Routing Research Challenge is to integrate
the real-life experience of Amazon drivers into the solution of optimal route
planning and optimization. This paper presents our method that tackles this
challenge by hierarchically combining machine learning and conventional
Traveling Salesperson Problem (TSP) solvers. Our method reaps the benefits from
both worlds. On the one hand, our method encodes driver know-how by learning a
sequential probability model from historical routes at the zone level, where
each zone contains a few parcel stops. It then uses a single step policy
iteration method, known as the Rollout algorithm, to generate plausible zone
sequences sampled from the learned probability model. On the other hand, our
method utilizes proven methods developed in the rich TSP literature to sequence
stops within each zone efficiently. The outcome of such a combination appeared
to be promising. Our method obtained an evaluation score of $0.0374$, which is
comparable to what the top three teams have achieved on the official Challenge
leaderboard. Moreover, our learning-based method is applicable to driving
routes that may exhibit distinct sequential patterns beyond the scope of this
Challenge. The source code of our method is publicly available at
https://github.com/aws-samples/amazon-sagemaker-amazon-routing-challenge-sol",0,1,0,0,0,0,2.6536e-05,41.0,0.703746,27
67948564-809c-4841-8683-cb5d72a02af1,Learning Deep Sensorimotor Policies for Vision-based Autonomous Drone Racing,7,0.456829,0.717279,"Autonomous drones can operate in remote and unstructured environments,
enabling various real-world applications. However, the lack of effective
vision-based algorithms has been a stumbling block to achieving this goal.
Existing systems often require hand-engineered components for state estimation,
planning, and control. Such a sequential design involves laborious tuning,
human heuristics, and compounding delays and errors. This paper tackles the
vision-based autonomous-drone-racing problem by learning deep sensorimotor
policies. We use contrastive learning to extract robust feature representations
from the input images and leverage a two-stage learning-by-cheating framework
for training a neural network policy. The resulting policy directly infers
control commands with feature representations learned from raw images, forgoing
the need for globally-consistent state estimation, trajectory planning, and
handcrafted control design. Our experimental results indicate that our
vision-based policy can achieve the same level of racing performance as the
state-based policy while being robust against different visual disturbances and
distractors. We believe this work serves as a stepping-stone toward developing
intelligent vision-based autonomous systems that control the drone purely from
image inputs, like human pilots.",0,1,0,0,0,0,0.959403,6.0,0.92583,47
b2f6e0a7-cc7a-4586-9fd4-cb5e7a38a264,Real-time Online Video Detection with Temporal Smoothing Transformers,21,0.337862,0.749344,"Streaming video recognition reasons about objects and their actions in every
frame of a video. A good streaming recognition model captures both long-term
dynamics and short-term changes of video. Unfortunately, in most existing
methods, the computational complexity grows linearly or quadratically with the
length of the considered dynamics. This issue is particularly pronounced in
transformer-based architectures. To address this issue, we reformulate the
cross-attention in a video transformer through the lens of kernel and apply two
kinds of temporal smoothing kernel: A box kernel or a Laplace kernel. The
resulting streaming attention reuses much of the computation from frame to
frame, and only requires a constant time update each frame. Based on this idea,
we build TeSTra, a Temporal Smoothing Transformer, that takes in arbitrarily
long inputs with constant caching and computing overhead. Specifically, it runs
$6\times$ faster than equivalent sliding-window based transformers with 2,048
frames in a streaming setting. Furthermore, thanks to the increased temporal
span, TeSTra achieves state-of-the-art results on THUMOS'14 and
EPIC-Kitchen-100, two standard online action detection and action anticipation
datasets. A real-time version of TeSTra outperforms all but one prior
approaches on the THUMOS'14 dataset.",1,1,0,0,1,0,0.921857,8.0,0.915794,61
89457a8e-df6e-4387-8ea8-701099a229c6,Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation,10,0.0433115,0.246956,"We introduce a novel setup for low-resource task-oriented semantic parsing
which incorporates several constraints that may arise in real-world scenarios:
(1) lack of similar datasets/models from a related domain, (2) inability to
sample useful logical forms directly from a grammar, and (3) privacy
requirements for unlabeled natural utterances. Our goal is to improve a
low-resource semantic parser using utterances collected through user
interactions. In this highly challenging but realistic setting, we investigate
data augmentation approaches involving generating a set of structured canonical
utterances corresponding to logical forms, before simulating corresponding
natural language and filtering the resulting pairs. We find that such
approaches are effective despite our restrictive setup: in a low-resource
setting on the complex SMCalFlow calendaring dataset (Andreas et al., 2020), we
observe 33% relative improvement over a non-data-augmented baseline in top-1
match.",0,1,0,0,0,0,0.324364,5.0,0.490865,33
1734ac26-8790-4215-9801-9f7504dbad73,Pixel is All You Need: Adversarial Trajectory-Ensemble Active Learning for Salient Object Detection,1,0.00238576,0.068232,"Although weakly-supervised techniques can reduce the labeling effort, it is
unclear whether a saliency model trained with weakly-supervised data (e.g.,
point annotation) can achieve the equivalent performance of its
fully-supervised version. This paper attempts to answer this unexplored
question by proving a hypothesis: there is a point-labeled dataset where
saliency models trained on it can achieve equivalent performance when trained
on the densely annotated dataset. To prove this conjecture, we proposed a novel
yet effective adversarial trajectory-ensemble active learning (ATAL). Our
contributions are three-fold: 1) Our proposed adversarial attack triggering
uncertainty can conquer the overconfidence of existing active learning methods
and accurately locate these uncertain pixels. {2)} Our proposed
trajectory-ensemble uncertainty estimation method maintains the advantages of
the ensemble networks while significantly reducing the computational cost. {3)}
Our proposed relationship-aware diversity sampling algorithm can conquer
oversampling while boosting performance. Experimental results show that our
ATAL can find such a point-labeled dataset, where a saliency model trained on
it obtained $97\%$ -- $99\%$ performance of its fully-supervised version with
only ten annotated points per image.",0,1,1,0,0,0,0.128527,7.0,0.486706,53
93618487-a14f-429d-a2fb-8608af7a20ea,Multi-Agent Deep Reinforcement Learning for Cost- and Delay-Sensitive Virtual Network Function Placement and Routing,11,0.191236,0.6559,"This paper proposes an effective and novel multiagent deep reinforcement
learning (MADRL)-based method for solving the joint virtual network function
(VNF) placement and routing (P&R), where multiple service requests with
differentiated demands are delivered at the same time. The differentiated
demands of the service requests are reflected by their delay- and
cost-sensitive factors. We first construct a VNF P&R problem to jointly
minimize a weighted sum of service delay and resource consumption cost, which
is NP-complete. Then, the joint VNF P&R problem is decoupled into two iterative
subtasks: placement subtask and routing subtask. Each subtask consists of
multiple concurrent parallel sequential decision processes. By invoking the
deep deterministic policy gradient method and multi-agent technique, an
MADRL-P&R framework is designed to perform the two subtasks. The new joint
reward and internal rewards mechanism is proposed to match the goals and
constraints of the placement and routing subtasks. We also propose the
parameter migration-based model-retraining method to deal with changing network
topologies. Corroborated by experiments, the proposed MADRL-P&R framework is
superior to its alternatives in terms of service cost and delay, and offers
higher flexibility for personalized service demands. The parameter
migration-based model-retraining method can efficiently accelerate convergence
under moderate network topology changes.",0,1,0,0,1,0,0.294553,9.0,0.704184,51
a903e4a2-492c-426c-bb0b-02cded55b5d9,ELQA: A Corpus of Metalinguistic Questions and Answers about English,2,0.00995561,0.273851,"We present ELQA, a corpus of questions and answers in and about the English
language. Collected from two online forums, the >70k questions (from English
learners and others) cover wide-ranging topics including grammar, meaning,
fluency, and etymology. The answers include descriptions of general properties
of English vocabulary and grammar as well as explanations about specific
(correct and incorrect) usage examples. Unlike most NLP datasets, this corpus
is metalinguistic -- it consists of language about language. As such, it can
facilitate investigations of the metalinguistic capabilities of NLU models, as
well as educational applications in the language learning domain. To study
this, we define a free-form question answering task on our dataset and conduct
evaluations on multiple LLMs (Large Language Models) to analyze their capacity
to generate metalinguistic answers.",0,1,0,1,0,0,0.0491593,7.0,0.343282,50
6289ede6-c015-457c-9cd5-50de4676e627,Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization,26,0.923806,0.814644,"Vision transformers (ViTs) are emerging with significantly improved accuracy
in computer vision tasks. However, their complex architecture and enormous
computation/storage demand impose urgent needs for new hardware accelerator
design methodology. This work proposes an FPGA-aware automatic ViT acceleration
framework based on the proposed mixed-scheme quantization. To the best of our
knowledge, this is the first FPGA-based ViT acceleration framework exploring
model quantization. Compared with state-of-the-art ViT quantization work
(algorithmic approach only without hardware acceleration), our quantization
achieves 0.47% to 1.36% higher Top-1 accuracy under the same bit-width.
Compared with the 32-bit floating-point baseline FPGA accelerator, our
accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS
vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",0,1,0,0,1,0,0.990958,7.0,0.991338,27
751d61ef-01e2-4bf7-b9af-098ef57836b1,MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation,10,0.136906,0.484537,"Commonsense reasoning tasks such as commonsense knowledge graph completion
and commonsense question answering require powerful representation learning. In
this paper, we propose to learn commonsense knowledge representation by MICO, a
Multi-alternative contrastve learning framework on COmmonsense knowledge graphs
(MICO). MICO generates the commonsense knowledge representation by contextual
interaction between entity nodes and relations with multi-alternative
contrastive learning. In MICO, the head and tail entities in an $(h,r,t)$
knowledge triple are converted to two relation-aware sequence pairs (a premise
and an alternative) in the form of natural language. Semantic representations
generated by MICO can benefit the following two tasks by simply comparing the
distance score between the representations: 1) zero-shot commonsense question
answering task; 2) inductive commonsense knowledge graph completion task.
Extensive experiments show the effectiveness of our method.",1,0,0,0,0,0,0.864558,7.0,0.869045,50
746908ee-c33d-46fd-bb68-766e1b3ef0e9,Describing Differences between Text Distributions with Natural Language,20,0.0733033,0.801211,"How do two distributions of texts differ? Humans are slow at answering this,
since discovering patterns might require tediously reading through hundreds of
samples. We propose to automatically summarize the differences by ""learning a
natural language hypothesis"": given two distributions $D_{0}$ and $D_{1}$, we
search for a description that is more often true for $D_{1}$, e.g., ""is
military-related."" To tackle this problem, we fine-tune GPT-3 to propose
descriptions with the prompt: ""[samples of $D_{0}$] + [samples of $D_{1}$] +
the difference between them is_____."" We then re-rank the descriptions by
checking how often they hold on a larger set of samples with a learned
verifier. On a benchmark of 54 real-world binary classification tasks, while
GPT-3 Curie (13B) only generates a description similar to human annotation 7%
of the time, the performance reaches 61% with fine-tuning and re-ranking, and
our best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to
describe distribution shifts, debug dataset shortcuts, summarize unknown tasks,
and label text clusters, and present analyses based on automatically generated
descriptions.",0,1,0,0,1,1,0.57477,6.0,0.705676,82
0eebf932-de8f-4e1d-994c-83aa27dbdfc0,Argus++: Robust Real-time Activity Detection for Unconstrained Video Streams with Overlapping Cube Proposals,10,0.0430341,0.502194,"Activity detection is one of the attractive computer vision tasks to exploit
the video streams captured by widely installed cameras. Although achieving
impressive performance, conventional activity detection algorithms are usually
designed under certain constraints, such as using trimmed and/or
object-centered video clips as inputs. Therefore, they failed to deal with the
multi-scale multi-instance cases in real-world unconstrained video streams,
which are untrimmed and have large field-of-views. Real-time requirements for
streaming analysis also mark brute force expansion of them unfeasible.
  To overcome these issues, we propose Argus++, a robust real-time activity
detection system for analyzing unconstrained video streams. The design of
Argus++ introduces overlapping spatio-temporal cubes as an intermediate concept
of activity proposals to ensure coverage and completeness of activity detection
through over-sampling. The overall system is optimized for real-time processing
on standalone consumer-level hardware. Extensive experiments on different
surveillance and driving scenarios demonstrated its superior performance in a
series of activity detection benchmarks, including CVPR ActivityNet ActEV 2021,
NIST ActEV SDL UF/KF, TRECVID ActEV 2020/2021, and ICCV ROAD 2021.",0,1,0,0,1,0,0.214268,7.0,0.566883,30
a17cf88e-4ff8-4048-8a10-fd2bed3c0a89,Eliminating Meta Optimization Through Self-Referential Meta Learning,6,0.114616,0.537352,"Meta Learning automates the search for learning algorithms. At the same time,
it creates a dependency on human engineering on the meta-level, where meta
learning algorithms need to be designed. In this paper, we investigate
self-referential meta learning systems that modify themselves without the need
for explicit meta optimization. We discuss the relationship of such systems to
in-context and memory-based meta learning and show that self-referential neural
networks require functionality to be reused in the form of parameter sharing.
Finally, we propose fitness monotonic execution (FME), a simple approach to
avoid explicit meta optimization. A neural network self-modifies to solve
bandit and classic control tasks, improves its self-modifications, and learns
how to learn, purely by assigning more computational resources to better
performing solutions.",0,0,0,0,0,0,0.422859,12.0,0.81601,29
7ac3d517-44da-4f2a-8085-d3f348db80b5,Face Relighting with Geometrically Consistent Shadows,24,0.303152,0.653585,"Most face relighting methods are able to handle diffuse shadows, but struggle
to handle hard shadows, such as those cast by the nose. Methods that propose
techniques for handling hard shadows often do not produce geometrically
consistent shadows since they do not directly leverage the estimated face
geometry while synthesizing them. We propose a novel differentiable algorithm
for synthesizing hard shadows based on ray tracing, which we incorporate into
training our face relighting model. Our proposed algorithm directly utilizes
the estimated face geometry to synthesize geometrically consistent hard
shadows. We demonstrate through quantitative and qualitative experiments on
Multi-PIE and FFHQ that our method produces more geometrically consistent
shadows than previous face relighting methods while also achieving
state-of-the-art face relighting performance under directional lighting. In
addition, we demonstrate that our differentiable hard shadow modeling improves
the quality of the estimated face geometry over diffuse shading models.",0,1,0,0,1,0,0.517896,9.0,0.786144,54
963f98ef-29f9-4815-ac5d-e1f896dbe30a,TrustAL: Trustworthy Active Learning using Knowledge Distillation,5,0.0534143,0.259605,"Active learning can be defined as iterations of data labeling, model
training, and data acquisition, until sufficient labels are acquired. A
traditional view of data acquisition is that, through iterations, knowledge
from human labels and models is implicitly distilled to monotonically increase
the accuracy and label consistency. Under this assumption, the most recently
trained model is a good surrogate for the current labeled data, from which data
acquisition is requested based on uncertainty/diversity. Our contribution is
debunking this myth and proposing a new objective for distillation. First, we
found example forgetting, which indicates the loss of knowledge learned across
iterations. Second, for this reason, the last model is no longer the best
teacher -- For mitigating such forgotten knowledge, we select one of its
predecessor models as a teacher, by our proposed notion of ""consistency"". We
show that this novel distillation is distinctive in the following three
aspects; First, consistency ensures to avoid forgetting labels. Second,
consistency improves both uncertainty/diversity of labeled data. Lastly,
consistency redeems defective labels produced by human annotators.",0,1,0,0,0,0,0.411053,8.0,0.719323,41
090bf06b-a613-48b9-ab1b-e99e404bf6ff,ALLSH: Active Learning Guided by Local Sensitivity and Hardness,23,0.0557145,0.704206,"Active learning, which effectively collects informative unlabeled data for
annotation, reduces the demand for labeled data. In this work, we propose to
retrieve unlabeled samples with a local sensitivity and hardness-aware
acquisition function. The proposed method generates data copies through local
perturbations and selects data points whose predictive likelihoods diverge the
most from their copies. We further empower our acquisition function by
injecting the select-worst case perturbation. Our method achieves consistent
gains over the commonly used active learning strategies in various
classification tasks. Furthermore, we observe consistent improvements over the
baselines on the study of prompt selection in prompt-based few-shot learning.
These experiments demonstrate that our acquisition guided by local sensitivity
and hardness can be effective and beneficial for many NLP tasks.",1,1,0,0,0,0,0.198875,8.0,0.610538,86
ec243ca2-86e5-4ec3-81cd-aa8d728515e0,Accelerating Shapley Explanation via Contributive Cooperator Selection,13,0.286161,0.619467,"Even though Shapley value provides an effective explanation for a DNN model
prediction, the computation relies on the enumeration of all possible input
feature coalitions, which leads to the exponentially growing complexity. To
address this problem, we propose a novel method SHEAR to significantly
accelerate the Shapley explanation for DNN models, where only a few coalitions
of input features are involved in the computation. The selection of the feature
coalitions follows our proposed Shapley chain rule to minimize the absolute
error from the ground-truth Shapley values, such that the computation can be
both efficient and accurate. To demonstrate the effectiveness, we
comprehensively evaluate SHEAR across multiple metrics including the absolute
error from the ground-truth Shapley value, the faithfulness of the
explanations, and running speed. The experimental results indicate SHEAR
consistently outperforms state-of-the-art baseline methods across different
evaluation metrics, which demonstrates its potentials in real-world
applications where the computational resource is limited.",1,1,0,0,1,0,0.871619,7.0,0.87282,35
01f148b5-9367-4c80-a95f-08d4b693433e,The Neural Race Reduction: Dynamics of Abstraction in Gated Networks,19,0.0760998,0.761019,"Our theoretical understanding of deep learning has not kept pace with its
empirical success. While network architecture is known to be critical, we do
not yet understand its effect on learned representations and network behavior,
or how this architecture should reflect task structure.In this work, we begin
to address this gap by introducing the Gated Deep Linear Network framework that
schematizes how pathways of information flow impact learning dynamics within an
architecture. Crucially, because of the gating, these networks can compute
nonlinear functions of their input. We derive an exact reduction and, for
certain cases, exact solutions to the dynamics of learning. Our analysis
demonstrates that the learning dynamics in structured networks can be
conceptualized as a neural race with an implicit bias towards shared
representations, which then govern the model's ability to systematically
generalize, multi-task, and transfer. We validate our key insights on
naturalistic datasets and with relaxed assumptions. Taken together, our work
gives rise to general hypotheses relating neural architecture to learning and
provides a mathematical approach towards understanding the design of more
complex architectures and the role of modularity and compositionality in
solving real-world problems. The code and results are available at
https://www.saxelab.org/gated-dln .",0,0,1,0,0,0,0.207658,8.0,0.616603,105
b210d1d2-de5a-4b1b-ac62-351e236c9a06,A note on the variation of geometric functionals,1,0.00853485,0.0366125,"Calculus of Variation combined with Differential Geometry as tools of
modelling and solving problems in image processing and computer vision were
introduced in the late 80's and the 90s of the 20th century. The beginning of
an extensive work in these directions was marked by works such as Geodesic
Active Contours (GAC), the Beltrami framework, level set method of Osher and
Sethian the works of Charpiat et al. and the works by Chan and Vese to name
just a few. In many cases the optimization of these functional are done by the
gradient descent method via the calculation of the Euler-Lagrange equations.
Straightforward use of the resulted EL equations in the gradient descent scheme
leads to non-geometric and in some cases non sensical equations. It is
costumary to modify these EL equations or even the functional itself in order
to obtain geometric and/or sensical equations. The aim of this note is to point
to the correct way to derive the EL and the gradient descent equations such
that the resulted gradient descent equation is geometric and makes sense.",0,0,0,0,0,0,0.105315,46.0,0.91728,8
01be5212-c895-4797-8c0a-0228b1124728,Questions Are All You Need to Train a Dense Passage Retriever,28,0.718317,0.87667,"We introduce ART, a new corpus-level autoencoding approach for training dense
retrieval models that does not require any labeled training data. Dense
retrieval is a central challenge for open-domain tasks, such as Open QA, where
state-of-the-art methods typically require large supervised datasets with
custom hard-negative mining and denoising of positive examples. ART, in
contrast, only requires access to unpaired inputs and outputs (e.g. questions
and potential answer documents). It uses a new document-retrieval autoencoding
scheme, where (1) an input question is used to retrieve a set of evidence
documents, and (2) the documents are then used to compute the probability of
reconstructing the original question. Training for retrieval based on question
reconstruction enables effective unsupervised learning of both document and
question encoders, which can be later incorporated into complete Open QA
systems without any further finetuning. Extensive experiments demonstrate that
ART obtains state-of-the-art results on multiple QA retrieval benchmarks with
only generic initialization from a pre-trained language model, removing the
need for labeled data and task-specific losses.",1,1,0,0,1,0,0.987068,4.0,0.965071,52
45c1b8df-d64b-49e1-bd56-137d2e9f1914,CC-Riddle: A Question Answering Dataset of Chinese Character Riddles,1,0.0295775,0.281946,"The Chinese character riddle is a unique form of cultural entertainment
specific to the Chinese language. It typically comprises two parts: the riddle
description and the solution. The solution to the riddle is a single character,
while the riddle description primarily describes the glyph of the solution,
occasionally supplemented with its explanation and pronunciation. Solving
Chinese character riddles is a challenging task that demands understanding of
character glyph, general knowledge, and a grasp of figurative language. In this
paper, we construct a \textbf{C}hinese \textbf{C}haracter riddle dataset named
CC-Riddle, which covers the majority of common simplified Chinese characters.
The construction process is a combination of web crawling, language model
generation and manual filtering. In generation stage, we input the Chinese
phonetic alphabet, glyph and meaning of the solution character into the
generation model, which then produces multiple riddle descriptions. The
generated riddles are then manually filtered and the final CC-Riddle dataset is
composed of both human-written riddles and these filtered, generated riddles.
In order to assess the performance of language models on the task of solving
character riddles, we use retrieval-based, generative and multiple-choice QA
strategies to test three language models: BERT, ChatGPT and ChatGLM. The test
results reveal that current language models still struggle to solve Chinese
character riddles. CC-Riddle is publicly available at
\url{https://github.com/pku0xff/CC-Riddle}.",1,1,1,1,0,0,0.499757,6.0,0.670558,29
6c4774e1-14c1-49a1-a7da-1393a4c218b5,Discrete-Constrained Regression for Local Counting Models,17,0.180884,0.484458,"Local counts, or the number of objects in a local area, is a continuous value
by nature. Yet recent state-of-the-art methods show that formulating counting
as a classification task performs better than regression. Through a series of
experiments on carefully controlled synthetic data, we show that this
counter-intuitive result is caused by imprecise ground truth local counts.
Factors such as biased dot annotations and incorrectly matched Gaussian kernels
used to generate ground truth counts introduce deviations from the true local
counts. Standard continuous regression is highly sensitive to these errors,
explaining the performance gap between classification and regression. To
mitigate the sensitivity, we loosen the regression formulation from a
continuous scale to a discrete ordering and propose a novel
discrete-constrained (DC) regression. Applied to crowd counting, DC-regression
is more accurate than both classification and standard regression on three
public benchmarks. A similar advantage also holds for the age estimation task,
verifying the overall effectiveness of DC-regression.",0,1,0,0,1,0,0.621479,7.0,0.765947,34
9a7374a9-d464-4294-b153-25cbbe154872,Robustness Implies Generalization via Data-Dependent Generalization Bounds,16,0.0,0.272507,"This paper proves that robustness implies generalization via data-dependent
generalization bounds. As a result, robustness and generalization are shown to
be connected closely in a data-dependent manner. Our bounds improve previous
bounds in two directions, to solve an open problem that has seen little
development since 2010. The first is to reduce the dependence on the covering
number. The second is to remove the dependence on the hypothesis space. We
present several examples, including ones for lasso and deep learning, in which
our bounds are provably preferable. The experiments on real-world data and
theoretical models demonstrate near-exponential improvements in various
situations. To achieve these improvements, we do not require additional
assumptions on the unknown distribution; instead, we only incorporate an
observable and computable property of the training samples. A key technical
innovation is an improved concentration bound for multinomial random variables
that is of independent interest beyond robustness and generalization.",0,0,0,0,0,0,0.00531188,11.0,0.377765,58
9bf8a5d7-a8f8-444e-9404-94e6ba06524f,Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations,11,0.22539,0.624212,"Recently, weakly-supervised image segmentation using weak annotations like
scribbles has gained great attention, since such annotations are much easier to
obtain compared to time-consuming and label-intensive labeling at the
pixel/voxel level. However, because scribbles lack structure information of
region of interest (ROI), existing scribble-based methods suffer from poor
boundary localization. Furthermore, most current methods are designed for 2D
image segmentation, which do not fully leverage the volumetric information if
directly applied to image slices. In this paper, we propose a scribble-based
volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image
segmentation and improves boundary prediction. To achieve this, we augment a
2.5D attention UNet with a proposed label propagation module to extend semantic
information from scribbles and a combination of static and active boundary
prediction to learn ROI's boundary and regularize its shape. Extensive
experiments on three public datasets demonstrate Scribble2D5 significantly
outperforms current scribble-based methods and approaches the performance of
fully-supervised ones. Our code is available online.",1,1,0,0,1,0,0.663637,9.0,0.830704,30
8dfa0ae4-cb79-4c0c-8bd9-3a4f032fa611,GCDT: A Chinese RST Treebank for Multigenre and Multilingual Discourse Parsing,6,0.175863,0.46885,"A lack of large-scale human-annotated data has hampered the hierarchical
discourse parsing of Chinese. In this paper, we present GCDT, the largest
hierarchical discourse treebank for Mandarin Chinese in the framework of
Rhetorical Structure Theory (RST). GCDT covers over 60K tokens across five
genres of freely available text, using the same relation inventory as
contemporary RST treebanks for English. We also report on this dataset's
parsing experiments, including state-of-the-art (SOTA) scores for Chinese RST
parsing and RST parsing on the English GUM dataset, using cross-lingual
training in Chinese and English with multilingual embeddings.",1,1,1,1,1,0,0.0881129,11.0,0.637017,64
1521fe70-22fb-49a7-b0ed-3dc010220047,Anticipative Feature Fusion Transformer for Multi-Modal Action Anticipation,20,0.28372,0.86545,"Although human action anticipation is a task which is inherently multi-modal,
state-of-the-art methods on well known action anticipation datasets leverage
this data by applying ensemble methods and averaging scores of unimodal
anticipation networks. In this work we introduce transformer based modality
fusion techniques, which unify multi-modal data at an early stage. Our
Anticipative Feature Fusion Transformer (AFFT) proves to be superior to popular
score fusion approaches and presents state-of-the-art results outperforming
previous methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily
extensible and allows for adding new modalities without architectural changes.
Consequently, we extracted audio features on EpicKitchens-100 which we add to
the set of commonly used features in the community.",1,1,0,1,1,0,0.787771,9.0,0.869875,52
570bc895-7f7c-4cdb-af67-f9828e0061d4,Controllable Radiance Fields for Dynamic Face Synthesis,8,0.0737646,0.673508,"Recent work on 3D-aware image synthesis has achieved compelling results using
advances in neural rendering. However, 3D-aware synthesis of face dynamics
hasn't received much attention. Here, we study how to explicitly control
generative model synthesis of face dynamics exhibiting non-rigid motion (e.g.,
facial expression change), while simultaneously ensuring 3D-awareness. For this
we propose a Controllable Radiance Field (CoRF): 1) Motion control is achieved
by embedding motion features within the layered latent motion space of a
style-based generator; 2) To ensure consistency of background, motion features
and subject-specific attributes such as lighting, texture, shapes, albedo, and
identity, a face parsing net, a head regressor and an identity encoder are
incorporated. On head image/video data we show that CoRFs are 3D-aware while
enabling editing of identity, viewing directions, and motion.",0,0,0,0,0,0,0.9132,8.0,0.910532,77
23b8528a-83df-4f4a-aaa0-4f1d01ba6be9,Point-Level Region Contrast for Object Detection Pre-Training,43,0.284688,0.695713,"In this work we present point-level region contrast, a self-supervised
pre-training approach for the task of object detection. This approach is
motivated by the two key factors in detection: localization and recognition.
While accurate localization favors models that operate at the pixel- or
point-level, correct recognition typically relies on a more holistic,
region-level view of objects. Incorporating this perspective in pre-training,
our approach performs contrastive learning by directly sampling individual
point pairs from different regions. Compared to an aggregated representation
per region, our approach is more robust to the change in input region quality,
and further enables us to implicitly improve initial region assignments via
online knowledge distillation during training. Both advantages are important
when dealing with imperfect regions encountered in the unsupervised setting.
Experiments show point-level region contrast improves on state-of-the-art
pre-training methods for object detection and segmentation across multiple
tasks and datasets, and we provide extensive ablation studies and
visualizations to aid understanding. Code will be made available.",0,1,0,0,1,0,0.93012,5.0,0.873851,52
8cfe56ae-30f4-44e1-8bdc-c8b593f74b99,Motron: Multimodal Probabilistic Human Motion Forecasting,21,0.310277,0.693447,"Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms.",1,0,0,0,1,0,0.838992,9.0,0.888095,56
8c18b8f9-fedb-4a0c-8af1-494d082f5b68,Problem Classification for Tailored Helpdesk Auto-Replies,1,0.0880469,0.406238,"IT helpdesks are charged with the task of responding quickly to user queries.
To give the user confidence that their query matters, the helpdesk will
auto-reply to the user with confirmation that their query has been received and
logged. This auto-reply may include generic `boiler-plate' text that addresses
common problems of the day, with relevant information and links. The approach
explored here is to tailor the content of the auto-reply to the user's problem,
so as to increase the relevance of the information included. Problem
classification is achieved by training a neural network on a suitable corpus of
IT helpdesk email data. While this is no substitute for follow-up by helpdesk
agents, the aim is that this system will provide a practical stop-gap.",0,1,0,1,0,0,0.993256,24.0,0.999993,12
846a5760-31cd-42ea-b69a-5ee6c623931f,Speech Synthesis with Mixed Emotions,19,0.318672,0.954974,"Emotional speech synthesis aims to synthesize human voices with various
emotional effects. The current studies are mostly focused on imitating an
averaged style belonging to a specific emotion type. In this paper, we seek to
generate speech with a mixture of emotions at run-time. We propose a novel
formulation that measures the relative difference between the speech samples of
different emotions. We then incorporate our formulation into a
sequence-to-sequence emotional text-to-speech framework. During the training,
the framework does not only explicitly characterize emotion styles, but also
explores the ordinal nature of emotions by quantifying the differences with
other emotions. At run-time, we control the model to produce the desired
emotion mixture by manually defining an emotion attribute vector. The objective
and subjective evaluations have validated the effectiveness of the proposed
framework. To our best knowledge, this research is the first study on
modelling, synthesizing, and evaluating mixed emotions in speech.",0,0,1,0,0,0,0.263416,9.0,0.689502,136
1b89ad2a-1527-4c79-af30-ed1e75acdfee,Chunk-based Nearest Neighbor Machine Translation,21,0.118868,0.837577,"Semi-parametric models, which augment generation with retrieval, have led to
impressive results in language modeling and machine translation, due to their
ability to retrieve fine-grained information from a datastore of examples. One
of the most prominent approaches, $k$NN-MT, exhibits strong domain adaptation
capabilities by retrieving tokens from domain-specific datastores
\citep{khandelwal2020nearest}. However, $k$NN-MT requires an expensive
retrieval operation for every single generated token, leading to a very low
decoding speed (around 8 times slower than a parametric model). In this paper,
we introduce a \textit{chunk-based} $k$NN-MT model which retrieves chunks of
tokens from the datastore, instead of a single token. We propose several
strategies for incorporating the retrieved chunks into the generation process,
and for selecting the steps at which the model needs to search for neighbors in
the datastore. Experiments on machine translation in two settings, static and
``on-the-fly'' domain adaptation, show that the chunk-based $k$NN-MT model
leads to significant speed-ups (up to 4 times) with only a small drop in
translation quality.",1,1,0,0,0,0,0.360591,6.0,0.59764,51
2bc4c2b2-7ec1-4acb-a815-232556d7853f,Image Super-Resolution With Deep Variational Autoencoders,14,0.0524496,0.49849,"Image super-resolution (SR) techniques are used to generate a high-resolution
image from a low-resolution image. Until now, deep generative models such as
autoregressive models and Generative Adversarial Networks (GANs) have proven to
be effective at modelling high-resolution images. VAE-based models have often
been criticised for their feeble generative performance, but with new
advancements such as VDVAE, there is now strong evidence that deep VAEs have
the potential to outperform current state-of-the-art models for high-resolution
image generation. In this paper, we introduce VDVAE-SR, a new model that aims
to exploit the most recent deep VAE methodologies to improve upon the results
of similar models. VDVAE-SR tackles image super-resolution using transfer
learning on pretrained VDVAEs. The presented model is competitive with other
state-of-the-art models, having comparable results on image quality metrics.",0,1,0,0,0,0,0.71209,9.0,0.845532,47
1183dfe0-d3c2-4984-b70b-808bc2dc920e,What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis,12,0.0485065,0.669387,"Market sentiment analysis on social media content requires knowledge of both
financial markets and social media jargon, which makes it a challenging task
for human raters. The resulting lack of high-quality labeled data stands in the
way of conventional supervised learning methods. Instead, we approach this
problem using semi-supervised learning with a large language model (LLM). Our
pipeline generates weak financial sentiment labels for Reddit posts with an LLM
and then uses that data to train a small model that can be served in
production. We find that prompting the LLM to produce Chain-of-Thought
summaries and forcing it through several reasoning paths helps generate more
stable and accurate labels, while using a regression loss further improves
distillation quality. With only a handful of prompts, the final model performs
on par with existing supervised models. Though production applications of our
model are limited by ethical considerations, the model's competitive
performance points to the great potential of using LLMs for tasks that
otherwise require skill-intensive annotation.",0,1,0,0,0,0,0.597363,6.0,0.715991,24
