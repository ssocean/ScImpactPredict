id,title,TNCSI,abstract,OA,authors_title
2ab95d94-998c-4fb0-9100-6078682892fe,CrossRE: A Cross-Domain Dataset for Relation Extraction,0.638642,"Relation Extraction (RE) has attracted increasing attention, but current RE
evaluation is limited to in-domain evaluation setups. Little is known on how
well a RE system fares in challenging, but realistic out-of-distribution
evaluation setups. To address this gap, we propose CrossRE, a new,
freely-available cross-domain benchmark for RE, which comprises six distinct
text domains and includes multi-label annotations. An additional innovation is
that we release meta-data collected during annotation, to include explanations
and flags of difficult instances. We provide an empirical evaluation with a
state-of-the-art model for relation classification. As the meta-data enables us
to shed new light on the state-of-the-art model, we provide a comprehensive
analysis on the impact of difficult cases and find correlations between model
and human annotations. Overall, our empirical investigation highlights the
difficulty of cross-domain RE. We release our dataset, to spur more research in
this direction.",https://github.com/mainlp/CrossRE,-1
c5327846-81b0-4375-b7c1-6c1568016b24,Colonoscopy polyp detection with massive endoscopic images,0.205183,"We improved an existing end-to-end polyp detection model with better average
precision validated by different data sets with trivial cost on detection
speed. Our previous work on detecting polyps within colonoscopy provided an
efficient end-to-end solution to alleviate doctor's examination overhead.
However, our later experiments found this framework is not as robust as before
as the condition of polyp capturing varies. In this work, we conducted several
studies on data set, identifying main issues that causes low precision rate in
the task of polyp detection. We used an optimized anchor generation methods to
get better anchor box shape and more boxes are used for detection as we believe
this is necessary for small object detection. A alternative backbone is used to
compensate the heavy time cost introduced by dense anchor box regression. With
use of the attention gate module, our model can achieve state-of-the-art polyp
detection performance while still maintain real-time detection speed.",None,-1
37a09554-876a-4d1c-9fc7-eca30a95fd93,Fuzzy granular approximation classifier,0.236033,"In this article, a new Fuzzy Granular Approximation Classifier (FGAC) is
introduced. The classifier is based on the previously introduced concept of the
granular approximation and its multi-class classification case. The classifier
is instance-based and its biggest advantage is its local transparency i.e., the
ability to explain every individual prediction it makes. We first develop the
FGAC for the binary classification case and the multi-class classification case
and we discuss its variation that includes the Ordered Weighted Average (OWA)
operators. Those variations of the FGAC are then empirically compared with
other locally transparent ML methods. At the end, we discuss the transparency
of the FGAC and its advantage over other locally transparent methods. We
conclude that while the FGAC has similar predictive performance to other
locally transparent ML models, its transparency can be superior in certain
cases.",https://github.com/markopalangetic/FGAC_experiments,28553
889de63b-fbe4-46f8-9298-2862db935173,SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data,0.340137,"We present a method for the accurate 3D reconstruction of partly-symmetric
objects. We build on the strengths of recent advances in neural reconstruction
and rendering such as Neural Radiance Fields (NeRF). A major shortcoming of
such approaches is that they fail to reconstruct any part of the object which
is not clearly visible in the training image, which is often the case for
in-the-wild images and videos. When evidence is lacking, structural priors such
as symmetry can be used to complete the missing information. However,
exploiting such priors in neural rendering is highly non-trivial: while
geometry and non-reflective materials may be symmetric, shadows and reflections
from the ambient scene are not symmetric in general. To address this, we apply
a soft symmetry constraint to the 3D geometry and material properties, having
factored appearance into lighting, albedo colour and reflectivity. We evaluate
our method on the recently introduced CO3D dataset, focusing on the car
category due to the challenge of reconstructing highly-reflective materials. We
show that it can reconstruct unobserved regions with high fidelity and render
high-quality novel view images.",None,-1
e4c03f7f-df48-471a-9c6a-2cc2eb723573,C-SENN: Contrastive Self-Explaining Neural Network,0.461481,"In this study, we use a self-explaining neural network (SENN), which learns
unsupervised concepts, to acquire concepts that are easy for people to
understand automatically. In concept learning, the hidden layer retains
verbalizable features relevant to the output, which is crucial when adapting to
real-world environments where explanations are required. However, it is known
that the interpretability of concepts output by SENN is reduced in general
settings, such as autonomous driving scenarios. Thus, this study combines
contrastive learning with concept learning to improve the readability of
concepts and the accuracy of tasks. We call this model Contrastive
Self-Explaining Neural Network (C-SENN).",None,-1
586edef8-718b-449e-acf3-7752810a8f4c,Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models,0.991265,"Recent open-domain dialogue models have brought numerous breakthroughs.
However, building a chat system is not scalable since it often requires a
considerable volume of human-human dialogue data, especially when enforcing
features such as persona, style, or safety. In this work, we study the
challenge of imposing roles on open-domain dialogue systems, with the goal of
making the systems maintain consistent roles while conversing naturally with
humans. To accomplish this, the system must satisfy a role specification that
includes certain conditions on the stated features as well as a system policy
on whether or not certain types of utterances are allowed. For this, we propose
an efficient data collection framework leveraging in-context few-shot learning
of large-scale language models for building role-satisfying dialogue dataset
from scratch. We then compare various architectures for open-domain dialogue
systems in terms of meeting role specifications while maintaining
conversational abilities. Automatic and human evaluations show that our models
return few out-of-bounds utterances, keeping competitive performance on general
metrics. We release a Korean dialogue dataset we built for further research.",https://github.com/naver-ai/carecall-corpus,-1
7144e048-b6bd-4b26-a9ef-a451daac831f,LEAPMood: Light and Efficient Architecture to Predict Mood with Genetic Algorithm driven Hyperparameter Tuning,0.0484071,"Accurate and automatic detection of mood serves as a building block for use
cases like user profiling which in turn power applications such as advertising,
recommendation systems, and many more. One primary source indicative of an
individual's mood is textual data. While there has been extensive research on
emotion recognition, the field of mood prediction has been barely explored. In
addition, very little work is done in the area of on-device inferencing, which
is highly important from the user privacy point of view. In this paper, we
propose for the first time, an on-device deep learning approach for mood
prediction from textual data, LEAPMood. We use a novel on-device
deployment-focused objective function for hyperparameter tuning based on the
Genetic Algorithm (GA) and optimize the parameters concerning both performance
and size. LEAPMood consists of Emotion Recognition in Conversion (ERC) as the
first building block followed by mood prediction using K-means clustering. We
show that using a combination of character embedding, phonetic hashing, and
attention along with Conditional Random Fields (CRF), results in a performance
closely comparable to that of the current State-Of-the-Art with a significant
reduction in model size (> 90%) for the task of ERC. We achieve a Micro F1
score of 62.05% with a memory footprint of a mere 1.67MB on the DailyDialog
dataset. Furthermore, we curate a dataset for the task of mood prediction
achieving a Macro F1-score of 72.12% with LEAPMood.",None,-1
9dfb805d-ad87-4e45-8578-ab18b48ab83c,Pre-training Language Models with Deterministic Factual Knowledge,0.408802,"Previous works show that Pre-trained Language Models (PLMs) can capture
factual knowledge. However, some analyses reveal that PLMs fail to perform it
robustly, e.g., being sensitive to the changes of prompts when extracting
factual knowledge. To mitigate this issue, we propose to let PLMs learn the
deterministic relationship between the remaining context and the masked
content. The deterministic relationship ensures that the masked factual content
can be deterministically inferable based on the existing clues in the context.
That would provide more stable patterns for PLMs to capture factual knowledge
than randomly masking. Two pre-training tasks are further introduced to
motivate PLMs to rely on the deterministic relationship when filling masks.
Specifically, we use an external Knowledge Base (KB) to identify deterministic
relationships and continuously pre-train PLMs with the proposed methods. The
factual knowledge probing experiments indicate that the continuously
pre-trained PLMs achieve better robustness in factual knowledge capturing.
Further experiments on question-answering datasets show that trying to learn a
deterministic relationship with the proposed methods can also help other
knowledge-intensive tasks.",https://github.com/informagi/REL,-1
522e8809-b875-46f6-a9e1-082844cd05a5,"Global Counterfactual Explanations: Investigations, Implementations and Improvements",0.568708,"Counterfactual explanations have been widely studied in explainability, with
a range of application dependent methods emerging in fairness, recourse and
model understanding. However, the major shortcoming associated with these
methods is their inability to provide explanations beyond the local or
instance-level. While some works touch upon the notion of a global explanation,
typically suggesting to aggregate masses of local explanations in the hope of
ascertaining global properties, few provide frameworks that are either reliable
or computationally tractable. Meanwhile, practitioners are requesting more
efficient and interactive explainability tools. We take this opportunity to
investigate existing global methods, with a focus on implementing and improving
Actionable Recourse Summaries (AReS), the only known global counterfactual
explanation framework for recourse.",None,-1
d7666ed0-11f1-41bc-b911-717d4a678fa7,Hybrid Multimodal Fusion for Humor Detection,0.675339,"In this paper, we present our solution to the MuSe-Humor sub-challenge of the
Multimodal Emotional Challenge (MuSe) 2022. The goal of the MuSe-Humor
sub-challenge is to detect humor and calculate AUC from audiovisual recordings
of German football Bundesliga press conferences. It is annotated for humor
displayed by the coaches. For this sub-challenge, we first build a discriminant
model using the transformer module and BiLSTM module, and then propose a hybrid
fusion strategy to use the prediction results of each modality to improve the
performance of the model. Our experiments demonstrate the effectiveness of our
proposed model and hybrid fusion strategy on multimodal fusion, and the AUC of
our proposed model on the test set is 0.8972.",None,-1
a87c18ba-6067-4528-a9a2-0033f58ef020,Learning Non-target Knowledge for Few-shot Semantic Segmentation,0.977019,"Existing studies in few-shot semantic segmentation only focus on mining the
target object information, however, often are hard to tell ambiguous regions,
especially in non-target regions, which include background (BG) and Distracting
Objects (DOs). To alleviate this problem, we propose a novel framework, namely
Non-Target Region Eliminating (NTRE) network, to explicitly mine and eliminate
BG and DO regions in the query. First, a BG Mining Module (BGMM) is proposed to
extract the BG region via learning a general BG prototype. To this end, we
design a BG loss to supervise the learning of BGMM only using the known target
object segmentation ground truth. Then, a BG Eliminating Module and a DO
Eliminating Module are proposed to successively filter out the BG and DO
information from the query feature, based on which we can obtain a BG and
DO-free target object segmentation result. Furthermore, we propose a
prototypical contrastive learning algorithm to improve the model ability of
distinguishing the target object from DOs. Extensive experiments on both
PASCAL-5i and COCO-20i datasets show that our approach is effective despite its
simplicity.",https://github.com/LIUYUANWEI98/NERTNet,-1
9c92932d-df2f-411f-b21e-105a7cb1f2d1,Contrastive Boundary Learning for Point Cloud Segmentation,0.970563,"Point cloud segmentation is fundamental in understanding 3D environments.
However, current 3D point cloud segmentation methods usually perform poorly on
scene boundaries, which degenerates the overall segmentation performance. In
this paper, we focus on the segmentation of scene boundaries. Accordingly, we
first explore metrics to evaluate the segmentation performance on scene
boundaries. To address the unsatisfactory performance on boundaries, we then
propose a novel contrastive boundary learning (CBL) framework for point cloud
segmentation. Specifically, the proposed CBL enhances feature discrimination
between points across boundaries by contrasting their representations with the
assistance of scene contexts at multiple scales. By applying CBL on three
different baseline methods, we experimentally show that CBL consistently
improves different baselines and assists them to achieve compelling performance
on boundaries, as well as the overall performance, eg in mIoU. The experimental
results demonstrate the effectiveness of our method and the importance of
boundaries for 3D point cloud segmentation. Code and model will be made
publicly available at https://github.com/LiyaoTang/contrastBoundary.",https://github.com/LiyaoTang/contrastBoundary,-1
0683430d-e080-422b-82d4-ba3cc649d3b7,Rethinking Knowledge Graph Evaluation Under the Open-World Assumption,0.486036,"Most knowledge graphs (KGs) are incomplete, which motivates one important
research topic on automatically complementing knowledge graphs. However,
evaluation of knowledge graph completion (KGC) models often ignores the
incompleteness -- facts in the test set are ranked against all unknown triplets
which may contain a large number of missing facts not included in the KG yet.
Treating all unknown triplets as false is called the closed-world assumption.
This closed-world assumption might negatively affect the fairness and
consistency of the evaluation metrics. In this paper, we study KGC evaluation
under a more realistic setting, namely the open-world assumption, where unknown
triplets are considered to include many missing facts not included in the
training or test sets. For the currently most used metrics such as mean
reciprocal rank (MRR) and Hits@K, we point out that their behavior may be
unexpected under the open-world assumption. Specifically, with not many missing
facts, their numbers show a logarithmic trend with respect to the true strength
of the model, and thus, the metric increase could be insignificant in terms of
reflecting the true model improvement. Further, considering the variance, we
show that the degradation in the reported numbers may result in incorrect
comparisons between different models, where stronger models may have lower
metric numbers. We validate the phenomenon both theoretically and
experimentally. Finally, we suggest possible causes and solutions for this
problem. Our code and data are available at
https://github.com/GraphPKU/Open-World-KG .",https://github.com/GraphPKU/Open-World-KG,-1
6c4b4aa6-0830-4b50-a1f3-a2092ce92ebf,SPE: Symmetrical Prompt Enhancement for Fact Probing,0.145904,"Pretrained language models (PLMs) have been shown to accumulate factual
knowledge during pretrainingng (Petroni et al., 2019). Recent works probe PLMs
for the extent of this knowledge through prompts either in discrete or
continuous forms. However, these methods do not consider symmetry of the task:
object prediction and subject prediction. In this work, we propose Symmetrical
Prompt Enhancement (SPE), a continuous prompt-based method for factual probing
in PLMs that leverages the symmetry of the task by constructing symmetrical
prompts for subject and object prediction. Our results on a popular factual
probing dataset, LAMA, show significant improvement of SPE over previous
probing methods.",None,-1
5b530edb-e3c5-4a99-ba4a-cbc5ebe64de8,Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness,0.867076,"A notable challenge in Multi-Document Summarization (MDS) is the
extremely-long length of the input. In this paper, we present an
extract-then-abstract Transformer framework to overcome the problem.
Specifically, we leverage pre-trained language models to construct a
hierarchical extractor for salient sentence selection across documents and an
abstractor for rewriting the selected contents as summaries. However, learning
such a framework is challenging since the optimal contents for the abstractor
are generally unknown. Previous works typically create pseudo extraction oracle
to enable the supervised learning for both the extractor and the abstractor.
Nevertheless, we argue that the performance of such methods could be restricted
due to the insufficient information for prediction and inconsistent objectives
between training and testing. To this end, we propose a loss weighting
mechanism that makes the model aware of the unequal importance for the
sentences not in the pseudo extraction oracle, and leverage the fine-tuned
abstractor to generate summary references as auxiliary signals for learning the
extractor. Moreover, we propose a reinforcement learning method that can
efficiently apply to the extractor for harmonizing the optimization between
training and testing. Experiment results show that our framework substantially
outperforms strong baselines with comparable model sizes and achieves the best
results on the Multi-News, Multi-XScience, and WikiCatSum corpora.",None,-1
a0aa0107-087b-4c35-8f29-63ef8e30a21e,Route Planning for Last-Mile Deliveries Using Mobile Parcel Lockers: A Hybrid Q-Learning Network Approach,0.482134,"Mobile parcel lockers have been recently proposed by logistics operators as a
technology that could help reduce traffic congestion and operational costs in
urban freight distribution. Given their ability to relocate throughout their
area of deployment, they hold the potential to improve customer accessibility
and convenience. In this study, we formulate the Mobile Parcel Locker Problem
(MPLP) , a special case of the Location-Routing Problem (LRP) which determines
the optimal stopover location for MPLs throughout the day and plans
corresponding delivery routes. A Hybrid Q Learning Network based Method (HQM)
is developed to resolve the computational complexity of the resulting large
problem instances while escaping local optima. In addition, the HQM is
integrated with global and local search mechanisms to resolve the dilemma of
exploration and exploitation faced by classic reinforcement learning methods.
We examine the performance of HQM under different problem sizes (up to 200
nodes) and benchmarked it against the exact approach and Genetic Algorithm
(GA). Our results indicate that HQM achieves better optimisation performance
with shorter computation time than the exact approach solved by the Gurobi
solver in large problem instances. Additionally, the average reward obtained by
HQM is 1.96 times greater than GA, which demonstrates that HQM has a better
optimisation ability. Further, we identify critical factors that contribute to
fleet size requirements, travel distances, and service delays. Our findings
outline that the efficiency of MPLs is mainly contingent on the length of time
windows and the deployment of MPL stopovers. Finally, we highlight managerial
implications based on parametric analysis to provide guidance for logistics
operators in the context of efficient last-mile distribution operations.",None,-1
2c2e4482-1b87-437f-8d2f-c70e918607bc,Pre-Avatar: An Automatic Presentation Generation Framework Leveraging Talking Avatar,0.0459139,"Since the beginning of the COVID-19 pandemic, remote conferencing and
school-teaching have become important tools. The previous applications aim to
save the commuting cost with real-time interactions. However, our application
is going to lower the production and reproduction costs when preparing the
communication materials. This paper proposes a system called Pre-Avatar,
generating a presentation video with a talking face of a target speaker with 1
front-face photo and a 3-minute voice recording. Technically, the system
consists of three main modules, user experience interface (UEI), talking face
module and few-shot text-to-speech (TTS) module. The system firstly clones the
target speaker's voice, and then generates the speech, and finally generate an
avatar with appropriate lip and head movements. Under any scenario, users only
need to replace slides with different notes to generate another new video. The
demo has been released here and will be published as free software for use.",None,11661
8e88f6da-6b05-407e-a544-41aab514996b,Using Human Perception to Regularize Transfer Learning,0.266951,"Recent trends in the machine learning community show that models with
fidelity toward human perceptual measurements perform strongly on vision tasks.
Likewise, human behavioral measurements have been used to regularize model
performance. But can we transfer latent knowledge gained from this across
different learning objectives? In this work, we introduce PERCEP-TL (Perceptual
Transfer Learning), a methodology for improving transfer learning with the
regularization power of psychophysical labels in models. We demonstrate which
models are affected the most by perceptual transfer learning and find that
models with high behavioral fidelity -- including vision transformers --
improve the most from this regularization by as much as 1.9\% Top@1 accuracy
points. These findings suggest that biologically inspired learning agents can
benefit from human behavioral measurements as regularizers and psychophysical
learned representations can be transferred to independent evaluation tasks.",None,-1
13a21932-0a53-451f-bc12-dc6e1ae7f9d7,The Primacy Bias in Deep Reinforcement Learning,0.971712,"This work identifies a common flaw of deep reinforcement learning (RL)
algorithms: a tendency to rely on early interactions and ignore useful evidence
encountered later. Because of training on progressively growing datasets, deep
RL agents incur a risk of overfitting to earlier experiences, negatively
affecting the rest of the learning process. Inspired by cognitive science, we
refer to this effect as the primacy bias. Through a series of experiments, we
dissect the algorithmic aspects of deep RL that exacerbate this bias. We then
propose a simple yet generally-applicable mechanism that tackles the primacy
bias by periodically resetting a part of the agent. We apply this mechanism to
algorithms in both discrete (Atari 100k) and continuous action (DeepMind
Control Suite) domains, consistently improving their performance.",https://github.com/MaxASchwarzer/dopamine/tree/atari100k_spr,-1
de5afe43-15e3-42cc-b0c5-1a0404f4aa58,RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering,0.500266,"Finding accurate correspondences among different views is the Achilles' heel
of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the
assumption that corresponding pixels share similar photometric features.
However, multi-view images in real scenarios observe non-Lambertian surfaces
and experience occlusions. In this work, we propose a novel approach with
neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences
among views. Specifically, we impose a depth rendering consistency loss to
constrain the geometry features close to the object surface to alleviate
occlusions. Concurrently, we introduce a reference view synthesis loss to
generate consistent supervision, even for non-Lambertian surfaces. Extensive
experiments on DTU and Tanks\&Temples benchmarks demonstrate that our RC-MVSNet
approach achieves state-of-the-art performance over unsupervised MVS frameworks
and competitive performance to many supervised methods.The code is released at
https://github.com/Boese0601/RC-MVSNet",https://github.com/Boese0601/RC-MVSNet,-1
01f148b5-9367-4c80-a95f-08d4b693433e,The Neural Race Reduction: Dynamics of Abstraction in Gated Networks,0.761019,"Our theoretical understanding of deep learning has not kept pace with its
empirical success. While network architecture is known to be critical, we do
not yet understand its effect on learned representations and network behavior,
or how this architecture should reflect task structure.In this work, we begin
to address this gap by introducing the Gated Deep Linear Network framework that
schematizes how pathways of information flow impact learning dynamics within an
architecture. Crucially, because of the gating, these networks can compute
nonlinear functions of their input. We derive an exact reduction and, for
certain cases, exact solutions to the dynamics of learning. Our analysis
demonstrates that the learning dynamics in structured networks can be
conceptualized as a neural race with an implicit bias towards shared
representations, which then govern the model's ability to systematically
generalize, multi-task, and transfer. We validate our key insights on
naturalistic datasets and with relaxed assumptions. Taken together, our work
gives rise to general hypotheses relating neural architecture to learning and
provides a mathematical approach towards understanding the design of more
complex architectures and the role of modularity and compositionality in
solving real-world problems. The code and results are available at
https://www.saxelab.org/gated-dln .",None,1844
6b7d334c-c9a9-4c6e-9682-4171a7756cf6,HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation,0.835253,"Language models with the Transformers structure have shown great performance
in natural language processing. However, there still poses problems when
fine-tuning pre-trained language models on downstream tasks, such as
over-fitting or representation collapse. In this work, we propose HyPe, a
simple yet effective fine-tuning technique to alleviate such problems by
perturbing hidden representations of Transformers layers. Unlike previous works
that only add noise to inputs or parameters, we argue that the hidden
representations of Transformers layers convey more diverse and meaningful
language information. Therefore, making the Transformers layers more robust to
hidden representation perturbations can further benefit the fine-tuning of PLMs
en bloc. We conduct extensive experiments and analyses on GLUE and other
natural language inference datasets. Results demonstrate that HyPe outperforms
vanilla fine-tuning and enhances generalization of hidden representations from
different layers. In addition, HyPe acquires negligible computational
overheads, and is better than and compatible with previous state-of-the-art
fine-tuning techniques.",https://github.com/Yuanhy1997/HyPe,-1
8d34b306-d9ee-40d9-bb8e-8d66b64c95e5,Tencent AI Lab - Shanghai Jiao Tong University Low-Resource Translation System for the WMT22 Translation Task,0.515415,"This paper describes Tencent AI Lab - Shanghai Jiao Tong University
(TAL-SJTU) Low-Resource Translation systems for the WMT22 shared task. We
participate in the general translation task on
English$\Leftrightarrow$Livonian. Our system is based on M2M100 with novel
techniques that adapt it to the target language pair. (1) Cross-model word
embedding alignment: inspired by cross-lingual word embedding alignment, we
successfully transfer a pre-trained word embedding to M2M100, enabling it to
support Livonian. (2) Gradual adaptation strategy: we exploit Estonian and
Latvian as auxiliary languages for many-to-many translation training and then
adapt to English-Livonian. (3) Data augmentation: to enlarge the parallel data
for English-Livonian, we construct pseudo-parallel data with Estonian and
Latvian as pivot languages. (4) Fine-tuning: to make the most of all available
data, we fine-tune the model with the validation set and online
back-translation, further boosting the performance. In model evaluation: (1) We
find that previous work underestimated the translation performance of Livonian
due to inconsistent Unicode normalization, which may cause a discrepancy of up
to 14.9 BLEU score. (2) In addition to the standard validation set, we also
employ round-trip BLEU to evaluate the models, which we find more appropriate
for this task. Finally, our unconstrained system achieves BLEU scores of 17.0
and 30.4 for English to/from Livonian.",https://github.com/zwhe99/WMT22-En-Liv,-1
0d9e1551-71ea-49ec-88b4-c544d9783839,WinoDict: Probing language models for in-context word acquisition,0.71724,"We introduce a new in-context learning paradigm to measure Large Language
Models' (LLMs) ability to learn novel words during inference. In particular, we
rewrite Winograd-style co-reference resolution problems by replacing the key
concept word with a synthetic but plausible word that the model must understand
to complete the task. Solving this task requires the model to make use of the
dictionary definition of the new word given in the prompt. This benchmark
addresses word acquisition, one important aspect of the diachronic degradation
known to afflict LLMs. As LLMs are frozen in time at the moment they are
trained, they are normally unable to reflect the way language changes over
time. We show that the accuracy of LLMs compared to the original Winograd tasks
decreases radically in our benchmark, thus identifying a limitation of current
models and providing a benchmark to measure future improvements in LLMs ability
to do in-context learning.",https://github.com/google-research/language/tree/master/language/wino_dict,2175
f1c7d3e7-06a6-4edf-adcd-ff398c69d92c,"Early Recall, Late Precision: Multi-Robot Semantic Object Mapping under Operational Constraints in Perceptually-Degraded Environments",0.394469,"Semantic object mapping in uncertain, perceptually degraded environments
during long-range multi-robot autonomous exploration tasks such as
search-and-rescue is important and challenging. During such missions, high
recall is desirable to avoid missing true target objects and high precision is
also critical to avoid wasting valuable operational time on false positives.
Given recent advancements in visual perception algorithms, the former is
largely solvable autonomously, but the latter is difficult to address without
the supervision of a human operator. However, operational constraints such as
mission time, computational requirements, mesh network bandwidth and so on, can
make the operator's task infeasible unless properly managed. We propose the
Early Recall, Late Precision (EaRLaP) semantic object mapping pipeline to solve
this problem. EaRLaP was used by Team CoSTAR in DARPA Subterranean Challenge,
where it successfully detected all the artifacts encountered by the team of
robots. We will discuss these results and performance of the EaRLaP on various
datasets.",None,-1
ea99a902-6935-48d5-b3d7-962c90331e6a,FewGAN: Generating from the Joint Distribution of a Few Images,0.0269612,"We introduce FewGAN, a generative model for generating novel, high-quality
and diverse images whose patch distribution lies in the joint patch
distribution of a small number of N>1 training samples. The method is, in
essence, a hierarchical patch-GAN that applies quantization at the first coarse
scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully
convolutional GANs at finer scales. Our key idea is to first use quantization
to learn a fixed set of patch embeddings for training images. We then use a
separate set of side images to model the structure of generated images using an
autoregressive model trained on the learned patch embeddings of training
images. Using quantization at the coarsest scale allows the model to generate
both conditional and unconditional novel images. Subsequently, a patch-GAN
renders the fine details, resulting in high-quality images. In an extensive set
of experiments, it is shown that FewGAN outperforms baselines both
quantitatively and qualitatively.",None,-1
cca70b9b-4812-4c99-81ba-41febe45c4a6,FreGAN: Exploiting Frequency Components for Training GANs under Limited Data,0.862824,"Training GANs under limited data often leads to discriminator overfitting and
memorization issues, causing divergent training. Existing approaches mitigate
the overfitting by employing data augmentations, model regularization, or
attention mechanisms. However, they ignore the frequency bias of GANs and take
poor consideration towards frequency information, especially high-frequency
signals that contain rich details. To fully utilize the frequency information
of limited data, this paper proposes FreGAN, which raises the model's frequency
awareness and draws more attention to producing high-frequency signals,
facilitating high-quality generation. In addition to exploiting both real and
generated images' frequency information, we also involve the frequency signals
of real images as a self-supervised constraint, which alleviates the GAN
disequilibrium and encourages the generator to synthesize adequate rather than
arbitrary frequency signals. Extensive results demonstrate the superiority and
effectiveness of our FreGAN in ameliorating generation quality in the low-data
regime (especially when training data is less than 100). Besides, FreGAN can be
seamlessly applied to existing regularization and attention mechanism models to
further boost the performance.",https://github.com/kobeshegu/FreGAN_NeurIPS2022,-1
9a29bf4f-072a-4ea6-a10f-29a1c22658e5,Cost Volume Pyramid Network with Multi-strategies Range Searching for Multi-view Stereo,0.190835,"Multi-view stereo is an important research task in computer vision while
still keeping challenging. In recent years, deep learning-based methods have
shown superior performance on this task. Cost volume pyramid network-based
methods which progressively refine depth map in coarse-to-fine manner, have
yielded promising results while consuming less memory. However, these methods
fail to take fully consideration of the characteristics of the cost volumes in
each stage, leading to adopt similar range search strategies for each cost
volume stage. In this work, we present a novel cost volume pyramid based
network with different searching strategies for multi-view stereo. By choosing
different depth range sampling strategies and applying adaptive unimodal
filtering, we are able to obtain more accurate depth estimation in low
resolution stages and iteratively upsample depth map to arbitrary resolution.
We conducted extensive experiments on both DTU and BlendedMVS datasets, and
results show that our method outperforms most state-of-the-art methods.",https://github.com/SibylGao/MSCVP-MVSNet.git,-1
987b1fbb-93eb-4c1f-b8ec-bf7860b58bb9,Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer,0.445919,"We exploit the pre-trained seq2seq model mBART for multilingual text style
transfer. Using machine translated data as well as gold aligned English
sentences yields state-of-the-art results in the three target languages we
consider. Besides, in view of the general scarcity of parallel data, we propose
a modular approach for multilingual formality transfer, which consists of two
training strategies that target adaptation to both language and task. Our
approach achieves competitive performance without monolingual task-specific
parallel data and can be applied to other style transfer tasks as well as to
other languages.",https://github.com/laihuiyuan/multilingual-tst,-1
036b758f-c6c1-40a7-9275-1c41c8d3e75d,Socially Intelligent Genetic Agents for the Emergence of Explicit Norms,0.320399,"Norms help regulate a society. Norms may be explicit (represented in
structured form) or implicit. We address the emergence of explicit norms by
developing agents who provide and reason about explanations for norm violations
in deciding sanctions and identifying alternative norms. These agents use a
genetic algorithm to produce norms and reinforcement learning to learn the
values of these norms. We find that applying explanations leads to norms that
provide better cohesion and goal satisfaction for the agents. Our results are
stable for societies with differing attitudes of generosity.",https://github.com/niravajmeri/,31895
8b0c8530-13b8-42db-9cbb-c843b440c375,Toward Multi-Service Edge-Intelligence Paradigm: Temporal-Adaptive Prediction for Time-Critical Control over Wireless,0.250668,"Time-critical control applications typically pose stringent connectivity
requirements for communication networks. The imperfections associated with the
wireless medium such as packet losses, synchronization errors, and varying
delays have a detrimental effect on performance of real-time control, often
with safety implications. This paper introduces multi-service edge-intelligence
as a new paradigm for realizing time-critical control over wireless. It
presents the concept of multi-service edge-intelligence which revolves around
tight integration of wireless access, edge-computing and machine learning
techniques, in order to provide stability guarantees under wireless
imperfections. The paper articulates some of the key system design aspects of
multi-service edge-intelligence. It also presents a temporal-adaptive
prediction technique to cope with dynamically changing wireless environments.
It provides performance results in a robotic teleoperation scenario. Finally,
it discusses some open research and design challenges for multi-service
edge-intelligence.",None,-1
d2d3998e-75dd-4b2b-9001-d1f0a445b17a,Gollum: A Gold Standard for Large Scale Multi Source Knowledge Graph Matching,0.0909922,"The number of Knowledge Graphs (KGs) generated with automatic and manual
approaches is constantly growing. For an integrated view and usage, an
alignment between these KGs is necessary on the schema as well as instance
level. While there are approaches that try to tackle this multi source
knowledge graph matching problem, large gold standards are missing to evaluate
their effectiveness and scalability. We close this gap by presenting Gollum --
a gold standard for large-scale multi source knowledge graph matching with over
275,000 correspondences between 4,149 different KGs. They originate from
knowledge graphs derived by applying the DBpedia extraction framework to a
large wiki farm. Three variations of the gold standard are made available: (1)
a version with all correspondences for evaluating unsupervised matching
approaches, and two versions for evaluating supervised matching: (2) one where
each KG is contained both in the train and test set, and (3) one where each KG
is exclusively contained in the train or the test set.",None,-1
5f73878e-48e9-4231-a0af-1e5b3b7e172d,Human-centric Image Cropping with Partition-aware and Content-preserving Features,0.511555,"Image cropping aims to find visually appealing crops in an image, which is an
important yet challenging task. In this paper, we consider a specific and
practical application: human-centric image cropping, which focuses on the
depiction of a person. To this end, we propose a human-centric image cropping
method with two novel feature designs for the candidate crop: partition-aware
feature and content-preserving feature. For partition-aware feature, we divide
the whole image into nine partitions based on the human bounding box and treat
different partitions in a candidate crop differently conditioned on the human
information. For content-preserving feature, we predict a heatmap indicating
the important content to be included in a good crop, and extract the geometric
relation between the heatmap and a candidate crop. Extensive experiments
demonstrate that our method can perform favorably against state-of-the-art
image cropping methods on human-centric image cropping task. Code is available
at https://github.com/bcmi/Human-Centric-Image-Cropping.",https://github.com/bcmi/Human-Centric-Image-Cropping,-1
11d6cd06-7acc-4b85-b270-75dfe138b646,Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,0.362526,"We present Expected Statistic Regularization (ESR), a novel regularization
technique that utilizes low-order multi-task structural statistics to shape
model distributions for semi-supervised learning on low-resource datasets. We
study ESR in the context of cross-lingual transfer for syntactic analysis (POS
tagging and labeled dependency parsing) and present several classes of
low-order statistic functions that bear on model behavior. Experimentally, we
evaluate the proposed statistics with ESR for unsupervised transfer on 5
diverse target languages and show that all statistics, when estimated
accurately, yield improvements to both POS and LAS, with the best statistic
improving POS by +7.0 and LAS by +8.5 on average. We also present
semi-supervised transfer and learning curve experiments that show ESR provides
significant gains over strong cross-lingual-transfer-plus-fine-tuning baselines
for modest amounts of label data. These results indicate that ESR is a
promising and complementary approach to model-transfer approaches for
cross-lingual parsing.",https://github.com/teffland/expected-statistic-regularization,-1
66e1fdf8-6e58-4ff9-a3ba-ee78cf6b5381,Towards Continuous Consistency Axiom,0.127448,"Development of new algorithms in the area of machine learning, especially
clustering, comparative studies of such algorithms as well as testing according
to software engineering principles requires availability of labeled data sets.
While standard benchmarks are made available, a broader range of such data sets
is necessary in order to avoid the problem of overfitting. In this context,
theoretical works on axiomatization of clustering algorithms, especially axioms
on clustering preserving transformations are quite a cheap way to produce
labeled data sets from existing ones. However, the frequently cited axiomatic
system of Kleinberg:2002, as we show in this paper, is not applicable for
finite dimensional Euclidean spaces, in which many algorithms like $k$-means,
operate. In particular, the so-called outer-consistency axiom fails upon making
small changes in datapoint positions and inner-consistency axiom is valid only
for identity transformation in general settings.
  Hence we propose an alternative axiomatic system, in which Kleinberg's inner
consistency axiom is replaced by a centric consistency axiom and outer
consistency axiom is replaced by motion consistency axiom. We demonstrate that
the new system is satisfiable for a hierarchical version of $k$-means with
auto-adjusted $k$, hence it is not contradictory. Additionally, as $k$-means
creates convex clusters only, we demonstrate that it is possible to create a
version detecting concave clusters and still the axiomatic system can be
satisfied. The practical application area of such an axiomatic system may be
the generation of new labeled test data from existent ones for clustering
algorithm testing. %We propose the gravitational consistency as a replacement
which does not have this deficiency.",None,-1
5b1ef45e-7f9f-4d4f-a4f8-35b38d532212,CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation,0.431999,"Existing reference-free metrics have obvious limitations for evaluating
controlled text generation models. Unsupervised metrics can only provide a
task-agnostic evaluation result which correlates weakly with human judgments,
whereas supervised ones may overfit task-specific data with poor generalization
ability to other datasets. In this paper, we propose an unsupervised
reference-free metric called CTRLEval, which evaluates controlled text
generation from different aspects by formulating each aspect into multiple text
infilling tasks. On top of these tasks, the metric assembles the generation
probabilities from a pre-trained language model without any model training.
Experimental results show that our metric has higher correlations with human
judgments than other baselines, while obtaining better generalization of
evaluating generated texts from different models and with different qualities.",https://github.com/thu-coai/CTRLEval,-1
588f8245-ad84-479f-9605-9629ba924614,MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations,0.865856,"Poor sample efficiency continues to be the primary challenge for deployment
of deep Reinforcement Learning (RL) algorithms for real-world applications, and
in particular for visuo-motor control. Model-based RL has the potential to be
highly sample efficient by concurrently learning a world model and using
synthetic rollouts for planning and policy improvement. However, in practice,
sample-efficient learning with model-based RL is bottlenecked by the
exploration challenge. In this work, we find that leveraging just a handful of
demonstrations can dramatically improve the sample-efficiency of model-based
RL. Simply appending demonstrations to the interaction dataset, however, does
not suffice. We identify key ingredients for leveraging demonstrations in model
learning -- policy pretraining, targeted exploration, and oversampling of
demonstration data -- which forms the three phases of our model-based RL
framework. We empirically study three complex visuo-motor control domains and
find that our method is 150%-250% more successful in completing sparse reward
tasks compared to prior approaches in the low data regime (100K interaction
steps, 5 demonstrations). Code and videos are available at:
https://nicklashansen.github.io/modemrl",https://github.com/facebookresearch/modem,-1
9693700c-8d0a-4f49-8034-9f08cd91be7e,Probing Contextual Diversity for Dense Out-of-Distribution Detection,0.267342,"Detection of out-of-distribution (OoD) samples in the context of image
classification has recently become an area of interest and active study, along
with the topic of uncertainty estimation, to which it is closely related. In
this paper we explore the task of OoD segmentation, which has been studied less
than its classification counterpart and presents additional challenges.
Segmentation is a dense prediction task for which the model's outcome for each
pixel depends on its surroundings. The receptive field and the reliance on
context play a role for distinguishing different classes and, correspondingly,
for spotting OoD entities. We introduce MOoSe, an efficient strategy to
leverage the various levels of context represented within semantic segmentation
models and show that even a simple aggregation of multi-scale representations
has consistently positive effects on OoD detection and uncertainty estimation.",https://github.com/MOoSe-ECCV22/moose_eccv2022,-1
5901bf2c-9b6f-4511-bec6-18bfca44e7f8,Policy-Based Bayesian Experimental Design for Non-Differentiable Implicit Models,0.151339,"For applications in healthcare, physics, energy, robotics, and many other
fields, designing maximally informative experiments is valuable, particularly
when experiments are expensive, time-consuming, or pose safety hazards. While
existing approaches can sequentially design experiments based on prior
observation history, many of these methods do not extend to implicit models,
where simulation is possible but computing the likelihood is intractable.
Furthermore, they often require either significant online computation during
deployment or a differentiable simulation system. We introduce Reinforcement
Learning for Deep Adaptive Design (RL-DAD), a method for simulation-based
optimal experimental design for non-differentiable implicit models. RL-DAD
extends prior work in policy-based Bayesian Optimal Experimental Design (BOED)
by reformulating it as a Markov Decision Process with a reward function based
on likelihood-free information lower bounds, which is used to learn a policy
via deep reinforcement learning. The learned design policy maps prior histories
to experiment designs offline and can be quickly deployed during online
execution. We evaluate RL-DAD and find that it performs competitively with
baselines on three benchmarks.",None,-1
be767968-04ab-4da7-921a-727315394181,GUSOT: Green and Unsupervised Single Object Tracking for Long Video Sequences,0.426632,"Supervised and unsupervised deep trackers that rely on deep learning
technologies are popular in recent years. Yet, they demand high computational
complexity and a high memory cost. A green unsupervised single-object tracker,
called GUSOT, that aims at object tracking for long videos under a
resource-constrained environment is proposed in this work. Built upon a
baseline tracker, UHP-SOT++, which works well for short-term tracking, GUSOT
contains two additional new modules: 1) lost object recovery, and 2)
color-saliency-based shape proposal. They help resolve the tracking loss
problem and offer a more flexible object proposal, respectively. Thus, they
enable GUSOT to achieve higher tracking accuracy in the long run. We conduct
experiments on the large-scale dataset LaSOT with long video sequences, and
show that GUSOT offers a lightweight high-performance tracking solution that
finds applications in mobile and edge computing platforms.",None,-1
8a656159-9904-423b-841e-b3be00cb66d5,Domain Adaptation in Neural Machine Translation using a Qualia-Enriched FrameNet,0.0875263,"In this paper we present Scylla, a methodology for domain adaptation of
Neural Machine Translation (NMT) systems that make use of a multilingual
FrameNet enriched with qualia relations as an external knowledge base. Domain
adaptation techniques used in NMT usually require fine-tuning and in-domain
training data, which may pose difficulties for those working with
lesser-resourced languages and may also lead to performance decay of the NMT
system for out-of-domain sentences. Scylla does not require fine-tuning of the
NMT model, avoiding the risk of model over-fitting and consequent decrease in
performance for out-of-domain translations. Two versions of Scylla are
presented: one using the source sentence as input, and another one using the
target sentence. We evaluate Scylla in comparison to a state-of-the-art
commercial NMT system in an experiment in which 50 sentences from the Sports
domain are translated from Brazilian Portuguese to English. The two versions of
Scylla significantly outperform the baseline commercial system in HTER.",https://github.com/FrameNetBrasil/scylla_lr,-1
dbb9fe99-e5a3-43ed-b2d7-b0272e80f0ef,PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions,0.922778,"Cross-entropy loss and focal loss are the most common choices when training
deep neural networks for classification problems. Generally speaking, however,
a good loss function can take on much more flexible forms, and should be
tailored for different tasks and datasets. Motivated by how functions can be
approximated via Taylor expansion, we propose a simple framework, named
PolyLoss, to view and design loss functions as a linear combination of
polynomial functions. Our PolyLoss allows the importance of different
polynomial bases to be easily adjusted depending on the targeting tasks and
datasets, while naturally subsuming the aforementioned cross-entropy loss and
focal loss as special cases. Extensive experimental results show that the
optimal choice within the PolyLoss is indeed dependent on the task and dataset.
Simply by introducing one extra hyperparameter and adding one line of code, our
Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D
image classification, instance segmentation, object detection, and 3D object
detection tasks, sometimes by a large margin.",https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/car,-1
69574bb4-ce05-4822-a06d-02114c2a44e4,VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction,0.804549,"The success of the Neural Radiance Fields (NeRF) in novel view synthesis has
inspired researchers to propose neural implicit scene reconstruction. However,
most existing neural implicit reconstruction methods optimize per-scene
parameters and therefore lack generalizability to new scenes. We introduce
VolRecon, a novel generalizable implicit reconstruction method with Signed Ray
Distance Function (SRDF). To reconstruct the scene with fine details and little
noise, VolRecon combines projection features aggregated from multi-view
features, and volume features interpolated from a coarse global feature volume.
Using a ray transformer, we compute SRDF values of sampled points on a ray and
then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by
about 30% in sparse view reconstruction and achieves comparable accuracy as
MVSNet in full view reconstruction. Furthermore, our approach exhibits good
generalization performance on the large-scale ETH3D benchmark.",https://github.com/IVRL/VolRecon/,-1
1aa4200a-c11b-4f96-b425-bec47bce056e,InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds,0.884219,"In this paper, we take a significant step towards real-world applicability of
monocular neural avatar reconstruction by contributing InstantAvatar, a system
that can reconstruct human avatars from a monocular video within seconds, and
these avatars can be animated and rendered at an interactive rate. To achieve
this efficiency we propose a carefully designed and engineered system, that
leverages emerging acceleration structures for neural fields, in combination
with an efficient empty space-skipping strategy for dynamic scenes. We also
contribute an efficient implementation that we will make available for research
purposes. Compared to existing methods, InstantAvatar converges 130x faster and
can be trained in minutes instead of hours. It achieves comparable or even
better reconstruction quality and novel pose synthesis results. When given the
same time budget, our method significantly outperforms SoTA methods.
InstantAvatar can yield acceptable visual quality in as little as 10 seconds
training time.",None,-1
6fafdf88-9098-4ced-9055-7f971adcffda,Channel Pruned YOLOv5-based Deep Learning Approach for Rapid and Accurate Outdoor Obstacles Detection,0.32111,"One-stage algorithm have been widely used in target detection systems that
need to be trained with massive data. Most of them perform well both in
real-time and accuracy. However, due to their convolutional structure, they
need more computing power and greater memory consumption. Hence, we applied
pruning strategy to target detection networks to reduce the number of
parameters and the size of model. To demonstrate the practicality of the
pruning method, we select the YOLOv5 model for experiments and provide a data
set of outdoor obstacles to show the effect of model. In this specific data
set, in the best circumstances, the volume of the network model is reduced by
49.7% compared with the original model, and the reasoning time is reduced by
52.5%. Meanwhile, it also uses data processing methods to compensate for the
drop in accuracy caused by pruning.",None,-1
4e14ba26-18d0-4f84-8158-22f7634f361f,Addressing Segmentation Ambiguity in Neural Linguistic Steganography,0.205354,"Previous studies on neural linguistic steganography, except Ueoka et al.
(2021), overlook the fact that the sender must detokenize cover texts to avoid
arousing the eavesdropper's suspicion. In this paper, we demonstrate that
segmentation ambiguity indeed causes occasional decoding failures at the
receiver's side. With the near-ubiquity of subwords, this problem now affects
any language. We propose simple tricks to overcome this problem, which are even
applicable to languages without explicit word boundaries.",https://github.com/jumon/himitsu,-1
0ff1501f-8bfd-4896-8a20-dc0915391c33,On the Typicality of Musical Sequences,0.0257181,"It has been shown in a recent publication that words in human-produced
English language tend to have an information content close to the conditional
entropy. In this paper, we show that the same is true for events in
human-produced monophonic musical sequences. We also show how ""typical
sampling"" influences the distribution of information around the entropy for
single events and sequences.",None,4373
0990cdfa-c03e-41b5-9428-8c346a9db821,Towards Explainable Motion Prediction using Heterogeneous Graph Representations,0.250579,"Motion prediction systems aim to capture the future behavior of traffic
scenarios enabling autonomous vehicles to perform safe and efficient planning.
The evolution of these scenarios is highly uncertain and depends on the
interactions of agents with static and dynamic objects in the scene. GNN-based
approaches have recently gained attention as they are well suited to naturally
model these interactions. However, one of the main challenges that remains
unexplored is how to address the complexity and opacity of these models in
order to deal with the transparency requirements for autonomous driving
systems, which includes aspects such as interpretability and explainability. In
this work, we aim to improve the explainability of motion prediction systems by
using different approaches. First, we propose a new Explainable Heterogeneous
Graph-based Policy (XHGP) model based on an heterograph representation of the
traffic scene and lane-graph traversals, which learns interaction behaviors
using object-level and type-level attention. This learned attention provides
information about the most important agents and interactions in the scene.
Second, we explore this same idea with the explanations provided by
GNNExplainer. Third, we apply counterfactual reasoning to provide explanations
of selected individual scenarios by exploring the sensitivity of the trained
model to changes made to the input data, i.e., masking some elements of the
scene, modifying trajectories, and adding or removing dynamic agents. The
explainability analysis provided in this paper is a first step towards more
transparent and reliable motion prediction systems, important from the
perspective of the user, developers and regulatory agencies. The code to
reproduce this work is publicly available at
https://github.com/sancarlim/Explainable-MP/tree/v1.1.",https://github.com/sancarlim/Explainable-MP/tree/v1.1,-1
29bcea78-0a74-4dce-9c8d-043f3845bade,Applying Automatic Text Summarization for Fake News Detection,0.562473,"The distribution of fake news is not a new but a rapidly growing problem. The
shift to news consumption via social media has been one of the drivers for the
spread of misleading and deliberately wrong information, as in addition to it
of easy use there is rarely any veracity monitoring. Due to the harmful effects
of such fake news on society, the detection of these has become increasingly
important. We present an approach to the problem that combines the power of
transformer-based language models while simultaneously addressing one of their
inherent problems. Our framework, CMTR-BERT, combines multiple text
representations, with the goal of circumventing sequential limits and related
loss of information the underlying transformer architecture typically suffers
from. Additionally, it enables the incorporation of contextual information.
Extensive experiments on two very different, publicly available datasets
demonstrates that our approach is able to set new state-of-the-art performance
benchmarks. Apart from the benefit of using automatic text summarization
techniques we also find that the incorporation of contextual information
contributes to performance gains.",https://github.com/phHartl/lrec_2022,-1
08cb1604-2de3-499f-8ed8-71b1e78b03d5,Unfooling Perturbation-Based Post Hoc Explainers,0.778764,"Monumental advancements in artificial intelligence (AI) have lured the
interest of doctors, lenders, judges, and other professionals. While these
high-stakes decision-makers are optimistic about the technology, those familiar
with AI systems are wary about the lack of transparency of its decision-making
processes. Perturbation-based post hoc explainers offer a model agnostic means
of interpreting these systems while only requiring query-level access. However,
recent work demonstrates that these explainers can be fooled adversarially.
This discovery has adverse implications for auditors, regulators, and other
sentinels. With this in mind, several natural questions arise - how can we
audit these black box systems? And how can we ascertain that the auditee is
complying with the audit in good faith? In this work, we rigorously formalize
this problem and devise a defense against adversarial attacks on
perturbation-based explainers. We propose algorithms for the detection
(CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our
novel conditional anomaly detection approach, KNN-CAD. We demonstrate that our
approach successfully detects whether a black box system adversarially conceals
its decision-making process and mitigates the adversarial attack on real-world
data for the prevalent explainers, LIME and SHAP.",https://github.com/craymichael/unfooling,-1
078b8eed-f9e6-4930-ae99-5098a8a0a46b,DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering,0.705743,"Spoken Question Answering (SQA) is to find the answer from a spoken document
given a question, which is crucial for personal assistants when replying to the
queries from the users. Existing SQA methods all rely on Automatic Speech
Recognition (ASR) transcripts. Not only does ASR need to be trained with
massive annotated data that are time and cost-prohibitive to collect for
low-resourced languages, but more importantly, very often the answers to the
questions include name entities or out-of-vocabulary words that cannot be
recognized correctly. Also, ASR aims to minimize recognition errors equally
over all words, including many function words irrelevant to the SQA task.
Therefore, SQA without ASR transcripts (textless) is always highly desired,
although known to be very difficult.
  This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging
unlabeled data for pre-training and fine-tuned by the SQA downstream task. The
time intervals of spoken answers can be directly predicted from spoken
documents. We also release a new SQA benchmark corpus, NMSQA, for data with
more realistic scenarios. We empirically showed that DUAL yields results
comparable to those obtained by cascading ASR and text QA model and robust to
real-world data. Our code and model will be open-sourced.",None,-1
fcbb8e1d-0b36-431e-83db-f4a20f9cbf1b,Model Agnostic Interpretability for Multiple Instance Learning,0.112071,"In Multiple Instance Learning (MIL), models are trained using bags of
instances, where only a single label is provided for each bag. A bag label is
often only determined by a handful of key instances within a bag, making it
difficult to interpret what information a classifier is using to make
decisions. In this work, we establish the key requirements for interpreting MIL
models. We then go on to develop several model-agnostic approaches that meet
these requirements. Our methods are compared against existing inherently
interpretable MIL models on several datasets, and achieve an increase in
interpretability accuracy of up to 30%. We also examine the ability of the
methods to identify interactions between instances and scale to larger
datasets, improving their applicability to real-world problems.",https://github.com/JAEarly/MILLI,-1
ad6640fc-cb11-49ac-be6c-8272a9c9d32b,TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction,0.701998,"This technical report presents an effective method for motion prediction in
autonomous driving. We develop a Transformer-based method for input encoding
and trajectory prediction. Besides, we propose the Temporal Flow Header to
enhance the trajectory encoding. In the end, an efficient K-means ensemble
method is used. Using our Transformer network and ensemble method, we win the
first place of Argoverse 2 Motion Forecasting Challenge with the
state-of-the-art brier-minFDE score of 1.90.",None,-1
ff80e544-faf5-4972-86d0-497802ba57a1,Long-Tail Prediction Uncertainty Aware Trajectory Planning for Self-driving Vehicles,0.666931,"A typical trajectory planner of autonomous driving commonly relies on
predicting the future behavior of surrounding obstacles. Recently, deep
learning technology has been widely adopted to design prediction models due to
their impressive performance. However, such models may fail in the ""long-tail""
driving cases where the training data is sparse or unavailable, leading to
planner failures. To this end, this work proposes a trajectory planner to
consider the prediction model uncertainty arising from insufficient data for
safer performance. Firstly, an ensemble network structure estimates the
prediction model's uncertainty due to insufficient training data. Then a
trajectory planner is designed to consider the worst-case arising from
prediction uncertainty. The results show that the proposed method can improve
the safety of trajectory planning under the prediction uncertainty caused by
insufficient data. At the same time, with sufficient data, the framework will
not lead to overly conservative results. This technology helps to improve the
safety and reliability of autonomous vehicles under the long-tail data
distribution of the real world.",None,-1
2a965375-c4c9-40e7-b2a2-0dfd7548d4fd,Chemotaxis of sea urchin sperm cells through deep reinforcement learning,0.540249,"By imitating biological microswimmers, microrobots can be designed to
accomplish targeted delivery of cargos and biomedical manipulations at
microscale. However, it is still a great challenge to enable microrobots to
maneuver in a complex environment. Machine learning algorithms offer a tool to
boost mobility and flexibility of a synthetic microswimmer, hence could help us
design truly smart microrobots. In this work, we investigate how a model of sea
urchin sperm cell can self-learn chemotactic motion in a chemoattractant
concentration field. We employ an artificial neural network to act as a
decision-making agent and facilitate the sperm cell to discover efficient
maneuver strategies through a deep reinforcement learning (DRL) algorithm. Our
results show that chemotactic behaviours, very similar to the realistic ones,
can be achieved by the DRL utilizing only limited environmental information. In
most cases, the DRL algorithm discovers more efficient strategies than the
human-devised one. Furthermore, the DRL can even utilize an external
disturbance to facilitate the chemotactic motion if the extra flow information
is also taken into account by the artificial neural network. Our results
provide insights to the chemotactic process of sea urchin sperm cells and also
prepare guidance for the intelligent maneuver of microrobots.",None,1256
3f50e6c2-18f3-405e-9a80-592edd6e072d,Unsupervised Change Detection Based on Image Reconstruction Loss,0.683269,"To train the change detector, bi-temporal images taken at different times in
the same area are used. However, collecting labeled bi-temporal images is
expensive and time consuming. To solve this problem, various unsupervised
change detection methods have been proposed, but they still require unlabeled
bi-temporal images. In this paper, we propose unsupervised change detection
based on image reconstruction loss using only unlabeled single temporal single
image. The image reconstruction model is trained to reconstruct the original
source image by receiving the source image and the photometrically transformed
source image as a pair. During inference, the model receives bi-temporal images
as the input, and tries to reconstruct one of the inputs. The changed region
between bi-temporal images shows high reconstruction loss. Our change detector
showed significant performance in various change detection benchmark datasets
even though only a single temporal single source image was used. The code and
trained models will be publicly available for reproducibility.",https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix,-1
0d2520db-67e3-44c7-b570-5e5ce90aa648,Towards Self-Supervised Category-Level Object Pose and Size Estimation,0.532729,"In this work, we tackle the challenging problem of category-level object pose
and size estimation from a single depth image. Although previous
fully-supervised works have demonstrated promising performance, collecting
ground-truth pose labels is generally time-consuming and labor-intensive.
Instead, we propose a label-free method that learns to enforce the geometric
consistency between category template mesh and observed object point cloud
under a self-supervision manner. Specifically, our method consists of three key
components: differentiable shape deformation, registration, and rendering. In
particular, shape deformation and registration are applied to the template mesh
to eliminate the differences in shape, pose and scale. A differentiable
renderer is then deployed to enforce geometric consistency between point clouds
lifted from the rendered depth and the observed scene for self-supervision. We
evaluate our approach on real-world datasets and find that our approach
outperforms the simple traditional baseline by large margins while being
competitive with some fully-supervised approaches.",https://github.com/mentian/object-deformnet,-1
b7253f8f-e40c-4fb2-a069-d1e4d6d304ca,Patch-wise Contrastive Style Learning for Instagram Filter Removal,0.811488,"Image-level corruptions and perturbations degrade the performance of CNNs on
different downstream vision tasks. Social media filters are one of the most
common resources of various corruptions and perturbations for real-world visual
analysis applications. The negative effects of these distractive factors can be
alleviated by recovering the original images with their pure style for the
inference of the downstream vision tasks. Assuming these filters substantially
inject a piece of additional style information to the social media images, we
can formulate the problem of recovering the original versions as a reverse
style transfer problem. We introduce Contrastive Instagram Filter Removal
Network (CIFR), which enhances this idea for Instagram filter removal by
employing a novel multi-layer patch-wise contrastive style learning mechanism.
Experiments show our proposed strategy produces better qualitative and
quantitative results than the previous studies. Moreover, we present the
results of our additional experiments for proposed architecture within
different settings. Finally, we present the inference outputs and quantitative
comparison of filtered and recovered images on localization and segmentation
tasks to encourage the main motivation for this problem.",https://github.com/birdortyedi/cifr-pytorch,-1
5d35185e-725a-4a85-9a62-425ef8de6395,Graph Reasoning Transformer for Image Parsing,0.614242,"Capturing the long-range dependencies has empirically proven to be effective
on a wide range of computer vision tasks. The progressive advances on this
topic have been made through the employment of the transformer framework with
the help of the multi-head attention mechanism. However, the attention-based
image patch interaction potentially suffers from problems of redundant
interactions of intra-class patches and unoriented interactions of inter-class
patches. In this paper, we propose a novel Graph Reasoning Transformer (GReaT)
for image parsing to enable image patches to interact following a relation
reasoning pattern. Specifically, the linearly embedded image patches are first
projected into the graph space, where each node represents the implicit visual
center for a cluster of image patches and each edge reflects the relation
weight between two adjacent nodes. After that, global relation reasoning is
performed on this graph accordingly. Finally, all nodes including the relation
information are mapped back into the original space for subsequent processes.
Compared to the conventional transformer, GReaT has higher interaction
efficiency and a more purposeful interaction pattern. Experiments are carried
out on the challenging Cityscapes and ADE20K datasets. Results show that GReaT
achieves consistent performance gains with slight computational overheads on
the state-of-the-art transformer baselines.",https://github.com/open-mmlab/mmsegmentation,-1
087cb799-4822-44a6-b170-c242d5138b28,"An Exploratory Study of Tweets about the SARS-CoV-2 Omicron Variant: Insights from Sentiment Analysis, Language Interpretation, Source Tracking, Type Classification, and Embedded URL Detection",0.98796,"This paper presents the findings of an exploratory study on the continuously
generating Big Data on Twitter related to the sharing of information, news,
views, opinions, ideas, feedback, and experiences about the COVID-19 pandemic,
with a specific focus on the Omicron variant, which is the globally dominant
variant of SARS-CoV-2 at this time. A total of 12028 tweets about the Omicron
variant were studied, and the specific characteristics of tweets that were
analyzed include - sentiment, language, source, type, and embedded URLs. The
findings of this study are manifold. First, from sentiment analysis, it was
observed that 50.5% of tweets had a neutral emotion. The other emotions - bad,
good, terrible, and great were found in 15.6%, 14.0%, 12.5%, and 7.5% of the
tweets, respectively. Second, the findings of language interpretation showed
that 65.9% of the tweets were posted in English. It was followed by Spanish,
French, Italian, and other languages. Third, the findings from source tracking
showed that Twitter for Android was associated with 35.2% of tweets. It was
followed by Twitter Web App, Twitter for iPhone, Twitter for iPad, and other
sources. Fourth, studying the type of tweets revealed that retweets accounted
for 60.8% of the tweets, it was followed by original tweets and replies that
accounted for 19.8% and 19.4% of the tweets, respectively. Fifth, in terms of
embedded URL analysis, the most common domain embedded in the tweets was found
to be twitter.com, which was followed by biorxiv.org, nature.com, and other
domains. Finally, to support similar research in this field, we have developed
a Twitter dataset that comprises more than 500,000 tweets about the SARS-CoV-2
omicron variant since the first detected case of this variant on November 24,
2021.",None,-1
86a9b69d-7173-4dfb-a406-fdf3094d9ec8,"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends",0.90895,"This paper surveys vision-language pre-training (VLP) methods for multimodal
intelligence that have been developed in the last few years. We group these
approaches into three categories: ($i$) VLP for image-text tasks, such as image
captioning, image-text retrieval, visual question answering, and visual
grounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image
classification, object detection, and segmentation; and ($iii$) VLP for
video-text tasks, such as video captioning, video-text retrieval, and video
question answering. For each category, we present a comprehensive review of
state-of-the-art methods, and discuss the progress that has been made and
challenges still being faced, using specific systems and models as case
studies. In addition, for each category, we discuss advanced topics being
actively explored in the research community, such as big foundation models,
unified modeling, in-context few-shot learning, knowledge, robustness, and
computer vision in the wild, to name a few.",None,-1
36401f5e-656c-47c0-a662-e8c9f0dfbffe,Attention Distraction: Watermark Removal Through Continual Learning with Selective Forgetting,0.370719,"Fine-tuning attacks are effective in removing the embedded watermarks in deep
learning models. However, when the source data is unavailable, it is
challenging to just erase the watermark without jeopardizing the model
performance. In this context, we introduce Attention Distraction (AD), a novel
source data-free watermark removal attack, to make the model selectively forget
the embedded watermarks by customizing continual learning. In particular, AD
first anchors the model's attention on the main task using some unlabeled data.
Then, through continual learning, a small number of \textit{lures} (randomly
selected natural images) that are assigned a new label distract the model's
attention away from the watermarks. Experimental results from different
datasets and networks corroborate that AD can thoroughly remove the watermark
with a small resource budget without compromising the model's performance on
the main task, which outperforms the state-of-the-art works.",None,14329
4972d729-a08b-4440-8f82-944df8d79e40,Few-Max: Few-Shot Domain Adaptation for Unsupervised Contrastive Representation Learning,0.068973,"Contrastive self-supervised learning methods learn to map data points such as
images into non-parametric representation space without requiring labels. While
highly successful, current methods require a large amount of data in the
training phase. In situations where the target training set is limited in size,
generalization is known to be poor. Pretraining on a large source data set and
fine-tuning on the target samples is prone to overfitting in the few-shot
regime, where only a small number of target samples are available. Motivated by
this, we propose a domain adaption method for self-supervised contrastive
learning, termed Few-Max, to address the issue of adaptation to a target
distribution under few-shot learning. To quantify the representation quality,
we evaluate Few-Max on a range of source and target datasets, including
ImageNet, VisDA, and fastMRI, on which Few-Max consistently outperforms other
approaches.",https://github.com/utcsilab/fewmax,-1
4190866c-fb45-4978-9f32-65382c2d5527,AdaPrompt: Adaptive Model Training for Prompt-based NLP,0.598129,"Prompt-based learning, with its capability to tackle zero-shot and few-shot
NLP tasks, has gained much attention in community. The main idea is to bridge
the gap between NLP downstream tasks and language modeling (LM), by mapping
these tasks into natural language prompts, which are then filled by pre-trained
language models (PLMs). However, for prompt learning, there are still two
salient gaps between NLP tasks and pretraining. First, prompt information is
not necessarily sufficiently present during LM pretraining. Second,
task-specific data are not necessarily well represented during pretraining. We
address these two issues by proposing AdaPrompt, adaptively retrieving external
data for continual pretraining of PLMs by making use of both task and prompt
characteristics. In addition, we make use of knowledge in Natural Language
Inference models for deriving adaptive verbalizers. Experimental results on
five NLP benchmarks show that AdaPrompt can improve over standard PLMs in
few-shot settings. In addition, in zero-shot settings, our method outperforms
standard prompt-based methods by up to 26.35\% relative error reduction.",https://github.com/cylnlp/AdaPrompt,-1
a35ab0ec-72c5-456c-885c-6b1303f65c55,Graph-Text Multi-Modal Pre-training for Medical Representation Learning,0.35139,"As the volume of Electronic Health Records (EHR) sharply grows, there has
been emerging interest in learning the representation of EHR for healthcare
applications. Representation learning of EHR requires appropriate modeling of
the two dominant modalities in EHR: structured data and unstructured text. In
this paper, we present MedGTX, a pre-trained model for multi-modal
representation learning of the structured and textual EHR data. MedGTX uses a
novel graph encoder to exploit the graphical nature of structured EHR data, and
a text encoder to handle unstructured text, and a cross-modal encoder to learn
a joint representation space. We pre-train our model through four proxy tasks
on MIMIC-III, an open-source EHR data, and evaluate our model on two clinical
benchmarks and three novel downstream tasks which tackle real-world problems in
EHR data. The results consistently show the effectiveness of pre-training the
model for joint representation of both structured and unstructured information
from EHR. Given the promising performance of MedGTX, we believe this work opens
a new door to jointly understanding the two fundamental modalities of EHR data.",https://github.com/sjpark9503/kg_txt_multimodal,-1
d44be552-8f92-4164-b69d-399bc80d9b67,Pre-training to Match for Unified Low-shot Relation Extraction,0.588604,"Low-shot relation extraction~(RE) aims to recognize novel relations with very
few or even no samples, which is critical in real scenario application.
Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem
to be with similar target but require totally different underlying abilities.
In this paper, we propose Multi-Choice Matching Networks to unify low-shot
relation extraction. To fill in the gap between zero-shot and few-shot RE, we
propose the triplet-paraphrase meta-training, which leverages triplet
paraphrase to pre-train zero-shot label matching ability and uses meta-learning
paradigm to learn few-shot instance summarizing ability. Experimental results
on three different low-shot RE tasks show that the proposed method outperforms
strong baselines by a large margin, and achieve the best performance on
few-shot RE leaderboard.",https://github.com/fc-liu/MCMN,-1
96369a8f-98f2-4ae7-b878-0322c28dccf8,Semi-supervised New Event Type Induction and Description via Contrastive Loss-Enforced Batch Attention,0.575568,"Most event extraction methods have traditionally relied on an annotated set
of event types. However, creating event ontologies and annotating supervised
training data are expensive and time-consuming. Previous work has proposed
semi-supervised approaches which leverage seen (annotated) types to learn how
to automatically discover new event types. State-of-the-art methods, both
semi-supervised or fully unsupervised, use a form of reconstruction loss on
specific tokens in a context. In contrast, we present a novel approach to
semi-supervised new event type induction using a masked contrastive loss, which
learns similarities between event mentions by enforcing an attention mechanism
over the data minibatch. We further disentangle the discovered clusters by
approximating the underlying manifolds in the data, which allows us to increase
normalized mutual information and Fowlkes-Mallows scores by over 20% absolute.
Building on these clustering results, we extend our approach to two new tasks:
predicting the type name of the discovered clusters and linking them to
FrameNet frames.",None,-1
abb66d9b-4952-487d-a01b-36c1f5528140,Relational Message Passing for Fully Inductive Knowledge Graph Completion,0.93954,"In knowledge graph completion (KGC), predicting triples involving emerging
entities and/or relations, which are unseen when the KG embeddings are learned,
has become a critical challenge. Subgraph reasoning with message passing is a
promising and popular solution. Some recent methods have achieved good
performance, but they (i) usually can only predict triples involving unseen
entities alone, failing to address more realistic fully inductive situations
with both unseen entities and unseen relations, and (ii) often conduct message
passing over the entities with the relation patterns not fully utilized. In
this study, we propose a new method named RMPI which uses a novel Relational
Message Passing network for fully Inductive KGC. It passes messages directly
between relations to make full use of the relation patterns for subgraph
reasoning with new techniques on graph transformation, graph pruning,
relation-aware neighborhood attention, addressing empty subgraphs, etc., and
can utilize the relation semantics defined in the ontological schema of KG.
Extensive evaluation on multiple benchmarks has shown the effectiveness of
techniques involved in RMPI and its better performance compared with the
existing methods that support fully inductive KGC. RMPI is also comparable to
the state-of-the-art partially inductive KGC methods with very promising
results achieved. Our codes and data are available at
https://github.com/zjukg/RMPI.",https://github.com/zjukg/RMPI,-1
8dfddcc7-500c-4e60-8c10-c3168cb77c82,Stochastic Market Games,0.109485,"Some of the most relevant future applications of multi-agent systems like
autonomous driving or factories as a service display mixed-motive scenarios,
where agents might have conflicting goals. In these settings agents are likely
to learn undesirable outcomes in terms of cooperation under independent
learning, such as overly greedy behavior. Motivated from real world societies,
in this work we propose to utilize market forces to provide incentives for
agents to become cooperative. As demonstrated in an iterated version of the
Prisoner's Dilemma, the proposed market formulation can change the dynamics of
the game to consistently learn cooperative policies. Further we evaluate our
approach in spatially and temporally extended settings for varying numbers of
agents. We empirically find that the presence of markets can improve both the
overall result and agent individual returns via their trading activities.",None,-1
a9fc0d61-294c-4469-86a3-a2865f4904cb,GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost,0.56207,"Large pre-trained models have revolutionized natural language processing
(NLP) research and applications, but high training costs and limited data
resources have prevented their benefits from being shared equally amongst
speakers of all the world's languages. To address issues of cross-linguistic
access to such models and reduce energy consumption for sustainability during
large-scale model training, this study proposes an effective and
energy-efficient framework called GreenPLM that uses bilingual lexicons to
directly ""translate"" pre-trained language models of one language into another
at almost no additional cost. We validate this approach in 18 languages' BERT
models and show that this framework is comparable to, if not better than, other
heuristics with high training costs. In addition, given lightweight continued
pre-training on limited data where available, this framework outperforms the
original monolingual language models in six out of seven tested languages with
up to 200x less pre-training efforts. Aiming at the Leave No One Behind
Principle (LNOB), our approach manages to reduce inequalities between languages
and energy consumption greatly. We make our codes and models publicly available
here: \url{https://github.com/qcznlp/GreenPLMs}",https://github.com/qcznlp/GreenPLMs,-1
421ec993-fc1a-4686-be80-4fd63d967f77,Weakly Supervised 3D Multi-person Pose Estimation for Large-scale Scenes based on Monocular Camera and Single LiDAR,0.896511,"Depth estimation is usually ill-posed and ambiguous for monocular
camera-based 3D multi-person pose estimation. Since LiDAR can capture accurate
depth information in long-range scenes, it can benefit both the global
localization of individuals and the 3D pose estimation by providing rich
geometry features. Motivated by this, we propose a monocular camera and single
LiDAR-based method for 3D multi-person pose estimation in large-scale scenes,
which is easy to deploy and insensitive to light. Specifically, we design an
effective fusion strategy to take advantage of multi-modal input data,
including images and point cloud, and make full use of temporal information to
guide the network to learn natural and coherent human motions. Without relying
on any 3D pose annotations, our method exploits the inherent geometry
constraints of point cloud for self-supervision and utilizes 2D keypoints on
images for weak supervision. Extensive experiments on public datasets and our
newly collected dataset demonstrate the superiority and generalization
capability of our proposed method.",https://github.com/4DVLab/FusionPose.git,-1
451260e2-4f57-4038-8523-6476f7cffc69,A Dual-Contrastive Framework for Low-Resource Cross-Lingual Named Entity Recognition,0.324102,"Cross-lingual Named Entity Recognition (NER) has recently become a research
hotspot because it can alleviate the data-hungry problem for low-resource
languages. However, few researches have focused on the scenario where the
source-language labeled data is also limited in some specific domains. A common
approach for this scenario is to generate more training data through
translation or generation-based data augmentation method. Unfortunately, we
find that simply combining source-language data and the corresponding
translation cannot fully exploit the translated data and the improvements
obtained are somewhat limited. In this paper, we describe our novel
dual-contrastive framework ConCNER for cross-lingual NER under the scenario of
limited source-language labeled data. Specifically, based on the
source-language samples and their translations, we design two contrastive
objectives for cross-language NER at different grammatical levels, namely
Translation Contrastive Learning (TCL) to close sentence representations
between translated sentence pairs and Label Contrastive Learning (LCL) to close
token representations within the same labels. Furthermore, we utilize knowledge
distillation method where the NER model trained above is used as the teacher to
train a student model on unlabeled target-language data to better fit the
target language. We conduct extensive experiments on a wide variety of target
languages, and the results demonstrate that ConCNER tends to outperform
multiple baseline methods. For reproducibility, our code for this paper is
available at https://github.com/GKLMIP/ConCNER.",None,-1
9c1ef89d-aa1a-41db-a28e-d1a8c159dd57,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",0.994691,"We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io.",https://instructor-embedding.github.io,-1
65d2d232-c9e0-477a-8faf-056b29545506,Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL,0.474923,"Multi-agent Reinforcement Learning (MARL) is a powerful tool for training
autonomous agents acting independently in a common environment. However, it can
lead to sub-optimal behavior when individual incentives and group incentives
diverge. Humans are remarkably capable at solving these social dilemmas. It is
an open problem in MARL to replicate such cooperative behaviors in selfish
agents. In this work, we draw upon the idea of formal contracting from
economics to overcome diverging incentives between agents in MARL. We propose
an augmentation to a Markov game where agents voluntarily agree to binding
transfers of reward, under pre-specified conditions. Our contributions are
theoretical and empirical. First, we show that this augmentation makes all
subgame-perfect equilibria of all Fully Observable Markov Games exhibit
socially optimal behavior, given a sufficiently rich space of contracts. Next,
we show that for general contract spaces, and even under partial observability,
richer contract spaces lead to higher welfare. Hence, contract space design
solves an exploration-exploitation tradeoff, sidestepping incentive issues. We
complement our theoretical analysis with experiments. Issues of exploration in
the contracting augmentation are mitigated using a training methodology
inspired by multi-objective reinforcement learning: Multi-Objective Contract
Augmentation Learning (MOCA). We test our methodology in static, single-move
games, as well as dynamic domains that simulate traffic, pollution management
and common pool resource management.",https://github.com/Algorithmic-Alignment-Lab/contracts,-1
f8b9076d-99d5-41c7-a965-d6ef5246fdc0,ExPoSe: Combining State-Based Exploration with Gradient-Based Online Search,0.235478,"Online tree-based search algorithms iteratively simulate trajectories and
update action-values for a set of states stored in a tree structure. It works
reasonably well in practice but fails to effectively utilise the information
gathered from similar states. Depending upon the smoothness of the action-value
function, one approach to overcoming this issue is through online learning,
where information is interpolated among similar states; Policy Gradient Search
provides a practical algorithm to achieve this. However, Policy Gradient Search
lacks an explicit exploration mechanism, which is a key feature of tree-based
online search algorithms. In this paper, we propose an efficient and effective
online search algorithm called Exploratory Policy Gradient Search (ExPoSe),
which leverages information sharing among states by updating the search policy
parameters directly, while incorporating a well-defined exploration mechanism
during the online search process. We evaluate ExPoSe on a range of
decision-making problems, including Atari games, Sokoban, and Hamiltonian cycle
search in sparse graphs. The results demonstrate that ExPoSe consistently
outperforms other popular online search algorithms across all domains. The
ExPoSe source code is available at
\textit{\url{https://github.com/dixantmittal/ExPoSe}}.",https://github.com/dixantmittal/ExPoSe,-1
aa477ff8-eb51-478e-8328-97e604ed93dd,TraClets: Harnessing the power of computer vision for trajectory classification,0.0789057,"Due to the advent of new mobile devices and tracking sensors in recent years,
huge amounts of data are being produced every day. Therefore, novel
methodologies need to emerge that dive through this vast sea of information and
generate insights and meaningful information. To this end, researchers have
developed several trajectory classification algorithms over the years that are
able to annotate tracking data. Similarly, in this research, a novel
methodology is presented that exploits image representations of trajectories,
called TraClets, in order to classify trajectories in an intuitive humans way,
through computer vision techniques. Several real-world datasets are used to
evaluate the proposed approach and compare its classification performance to
other state-of-the-art trajectory classification algorithms. Experimental
results demonstrate that TraClets achieves a classification performance that is
comparable to, or in most cases, better than the state-of-the-art, acting as a
universal, high-accuracy approach for trajectory classification.",None,-1
02e5be36-bb6f-451c-b327-0697c9aca81b,Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability,0.119526,"Great endeavors have been made to study AI's ability in abstract reasoning,
along with which different versions of RAVEN's progressive matrices (RPM) are
proposed as benchmarks. Previous works give inkling that without sophisticated
design or extra meta-data containing semantic information, neural networks may
still be indecisive in making decisions regarding RPM problems, after
relentless training. Evidenced by thorough experiments and ablation studies, we
showcase that end-to-end neural networks embodied with felicitous inductive
bias, intentionally design or serendipitously match, can solve RPM problems
elegantly, without the augment of any extra meta-data or preferences of any
specific backbone. Our work also reveals that multi-viewpoint with
multi-evaluation is a key learning strategy for successful reasoning. Finally,
potential explanations for the failure of connectionist models in
generalization are provided. We hope that these results will serve as
inspections of AI's ability beyond perception and toward abstract reasoning.
Source code can be found in https://github.com/QinglaiWeiCASIA/RavenSolver.",https://github.com/QinglaiWeiCASIA/RavenSolver,13516
4b21a82f-39bf-43b2-912e-e8c070ec700f,BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets,0.737404,"The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article.",None,-1
81717a08-bcd2-4d35-93be-7f2de408122f,Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior,0.365189,"Model visualizations provide information that outputs alone might miss. But
can we trust that model visualizations reflect model behavior? For instance,
can they diagnose abnormal behavior such as planted backdoors or
overregularization? To evaluate visualization methods, we test whether they
assign different visualizations to anomalously trained models and normal
models. We find that while existing methods can detect models with starkly
anomalous behavior, they struggle to identify more subtle anomalies. Moreover,
they often fail to recognize the inputs that induce anomalous behavior, e.g.
images containing a spurious cue. These results reveal blind spots and
limitations of some popular model visualizations. By introducing a novel
evaluation framework for visualizations, our work paves the way for developing
more reliable model transparency methods in the future.",https://github.com/js-d/auditing-vis,-1
2fbe69bc-d1a4-4d82-9368-5e06f08b617d,MM-Claims: A Dataset for Multimodal Claim Detection in Social Media,0.44479,"In recent years, the problem of misinformation on the web has become
widespread across languages, countries, and various social media platforms.
Although there has been much work on automated fake news detection, the role of
images and their variety are not well explored. In this paper, we investigate
the roles of image and text at an earlier stage of the fake news detection
pipeline, called claim detection. For this purpose, we introduce a novel
dataset, MM-Claims, which consists of tweets and corresponding images over
three topics: COVID-19, Climate Change and broadly Technology. The dataset
contains roughly 86000 tweets, out of which 3400 are labeled manually by
multiple annotators for the training and evaluation of multimodal models. We
describe the dataset in detail, evaluate strong unimodal and multimodal
baselines, and analyze the potential and drawbacks of current models.",https://github.com/TIBHannover/MM_Claims,-1
d26e5546-6d6c-4431-8567-c4a3c0293f0c,KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model for Financial Reports,0.985052,"We present KPI-BERT, a system which employs novel methods of named entity
recognition (NER) and relation extraction (RE) to extract and link key
performance indicators (KPIs), e.g. ""revenue"" or ""interest expenses"", of
companies from real-world German financial documents. Specifically, we
introduce an end-to-end trainable architecture that is based on Bidirectional
Encoder Representations from Transformers (BERT) combining a recurrent neural
network (RNN) with conditional label masking to sequentially tag entities
before it classifies their relations. Our model also introduces a learnable
RNN-based pooling mechanism and incorporates domain expert knowledge by
explicitly filtering impossible relations. We achieve a substantially higher
prediction performance on a new practical dataset of German financial reports,
outperforming several strong baselines including a competing state-of-the-art
span-based entity tagging approach.",None,-1
92aa2c5a-0ac3-4857-8511-21ad7197ef84,SOCIOFILLMORE: A Tool for Discovering Perspectives,0.17356,"SOCIOFILLMORE is a multilingual tool which helps to bring to the fore the
focus or the perspective that a text expresses in depicting an event. Our tool,
whose rationale we also support through a large collection of human judgements,
is theoretically grounded on frame semantics and cognitive linguistics, and
implemented using the LOME frame semantic parser. We describe SOCIOFILLMORE's
development and functionalities, show how non-NLP researchers can easily
interact with the tool, and present some example case studies which are already
incorporated in the system, together with the kind of analysis that can be
visualised.",None,-1
29ea0493-0993-4e9e-8e0c-a1c6e6e33106,"Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities",0.469513,"As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.",None,34651
35a8d24e-125a-41f8-a641-96c0c8e09077,StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes,0.274136,"Analyzing ethnic or religious bias is important for improving fairness,
accountability, and transparency of natural language processing models.
However, many techniques rely on human-compiled lists of bias terms, which are
expensive to create and are limited in coverage. In this study, we present a
fully data-driven pipeline for generating a knowledge graph (KG) of cultural
knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5
nationalities and can easily be extended to include more entities. Our human
evaluation shows that the majority (59.2%) of non-singleton entries are
coherent and complete stereotypes. We further show that performing intermediate
masked language model training on the verbalized KG leads to a higher level of
cultural awareness in the model and has the potential to increase
classification performance on knowledge-crucial samples on a related task,
i.e., hate speech detection.",https://github.com/uds-lsv/StereoKG,4373
7bf94a58-2594-498e-816a-fcbf8f4da1d4,Multi-channel CNN to classify nepali covid-19 related tweets using hybrid features,0.486079,"Because of the current COVID-19 pandemic with its increasing fears among
people, it has triggered several health complications such as depression and
anxiety. Such complications have not only affected the developed countries but
also developing countries such as Nepal. These complications can be understood
from peoples' tweets/comments posted online after their proper analysis and
sentiment classification. Nevertheless, owing to the limited number of
tokens/words in each tweet, it is always crucial to capture multiple
information associated with them for their better understanding. In this study,
we, first, represent each tweet by combining both syntactic and semantic
information, called hybrid features. The syntactic information is generated
from the bag of words method, whereas the semantic information is generated
from the combination of the fastText-based (ft) and domain-specific (ds)
methods. Second, we design a novel multi-channel convolutional neural network
(MCNN), which ensembles the multiple CNNs, to capture multi-scale information
for better classification. Last, we evaluate the efficacy of both the proposed
feature extraction method and the MCNN model classifying tweets into three
sentiment classes (positive, neutral and negative) on NepCOV19Tweets dataset,
which is the only public COVID-19 tweets dataset in Nepali language. The
evaluation results show that the proposed hybrid features outperform individual
feature extraction methods with the highest classification accuracy of 69.7%
and the MCNN model outperforms the existing methods with the highest
classification accuracy of 71.3% during classification.",None,-1
eb1bcb9e-65e7-4fda-a2ef-03c9cbc3e96b,Phenomenological Causality,0.338203,"Discussions on causal relations in real life often consider variables for
which the definition of causality is unclear since the notion of interventions
on the respective variables is obscure. Asking 'what qualifies an action for
being an intervention on the variable X' raises the question whether the action
impacted all other variables only through X or directly, which implicitly
refers to a causal model.
  To avoid this known circularity, we instead suggest a notion of
'phenomenological causality' whose basic concept is a set of elementary
actions. Then the causal structure is defined such that elementary actions
change only the causal mechanism at one node (e.g. one of the causal
conditionals in the Markov factorization). This way, the Principle of
Independent Mechanisms becomes the defining property of causal structure in
domains where causality is a more abstract phenomenon rather than being an
objective fact relying on hard-wired causal links between tangible objects. We
describe this phenomenological approach to causality for toy and hypothetical
real-world examples and argue that it is consistent with the causal Markov
condition when the system under consideration interacts with other variables
that control the elementary actions.",None,-1
6017263e-b40d-413c-a15d-d1a936810fb6,Linear Array Network for Low-light Image Enhancement,0.163402,"Convolution neural networks (CNNs) based methods have dominated the low-light
image enhancement tasks due to their outstanding performance. However, the
convolution operation is based on a local sliding window mechanism, which is
difficult to construct the long-range dependencies of the feature maps.
Meanwhile, the self-attention based global relationship aggregation methods
have been widely used in computer vision, but these methods are difficult to
handle high-resolution images because of the high computational complexity. To
solve this problem, this paper proposes a Linear Array Self-attention (LASA)
mechanism, which uses only two 2-D feature encodings to construct 3-D global
weights and then refines feature maps generated by convolution layers. Based on
LASA, Linear Array Network (LAN) is proposed, which is superior to the existing
state-of-the-art (SOTA) methods in both RGB and RAW based low-light enhancement
tasks with a smaller amount of parameters. The code is released in
https://github.com/cuiziteng/LASA_enhancement.",https://github.com/cuiziteng/LASA,-1
a6939f3d-40d0-4f72-b918-30cfa7def209,AI Ethics Issues in Real World: Evidence from AI Incident Database,0.680139,"With the powerful performance of Artificial Intelligence (AI) also comes
prevalent ethical issues. Though governments and corporations have curated
multiple AI ethics guidelines to curb unethical behavior of AI, the effect has
been limited, probably due to the vagueness of the guidelines. In this paper,
we take a closer look at how AI ethics issues take place in real world, in
order to have a more in-depth and nuanced understanding of different ethical
issues as well as their social impact. With a content analysis of AI Incident
Database, which is an effort to prevent repeated real world AI failures by
cataloging incidents, we identified 13 application areas which often see
unethical use of AI, with intelligent service robots, language/vision models
and autonomous driving taking the lead. Ethical issues appear in 8 different
forms, from inappropriate use and racial discrimination, to physical safety and
unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI
practitioners with a practical guideline when trying to deploy AI applications
ethically.",None,-1
0d214505-5acf-48de-9579-90e75c17296a,Mitigating shortage of labeled data using clustering-based active learning with diversity exploration,0.0547145,"In this paper, we proposed a new clustering-based active learning framework,
namely Active Learning using a Clustering-based Sampling (ALCS), to address the
shortage of labeled data. ALCS employs a density-based clustering approach to
explore the cluster structure from the data without requiring exhaustive
parameter tuning. A bi-cluster boundary-based sample query procedure is
introduced to improve the learning performance for classifying highly
overlapped classes. Additionally, we developed an effective diversity
exploration strategy to address the redundancy among queried samples. Our
experimental results justified the efficacy of the ALCS approach.",https://github.com/XuyangAbert/ALCS,-1
26171536-7f7c-4a86-9caf-951b23e71f3a,Traceable and Authenticable Image Tagging for Fake News Detection,0.255494,"To prevent fake news images from misleading the public, it is desirable not
only to verify the authenticity of news images but also to trace the source of
fake news, so as to provide a complete forensic chain for reliable fake news
detection. To simultaneously achieve the goals of authenticity verification and
source tracing, we propose a traceable and authenticable image tagging approach
that is based on a design of Decoupled Invertible Neural Network (DINN). The
designed DINN can simultaneously embed the dual-tags, \textit{i.e.},
authenticable tag and traceable tag, into each news image before publishing,
and then separately extract them for authenticity verification and source
tracing. Moreover, to improve the accuracy of dual-tags extraction, we design a
parallel Feature Aware Projection Model (FAPM) to help the DINN preserve
essential tag information. In addition, we define a Distance Metric-Guided
Module (DMGM) that learns asymmetric one-class representations to enable the
dual-tags to achieve different robustness performances under malicious
manipulations. Extensive experiments, on diverse datasets and unseen
manipulations, demonstrate that the proposed tagging approach achieves
excellent performance in the aspects of both authenticity verification and
source tracing for reliable fake news detection and outperforms the prior
works.",None,18428
bc57b919-cc0c-485e-a492-3eadd342ccf0,"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",0.728393,"The past decade has witnessed dramatic gains in natural language processing
and an unprecedented scaling of large language models. These developments have
been accelerated by the advent of few-shot techniques such as chain of thought
(CoT) prompting. Specifically, CoT pushes the performance of large language
models in a few-shot setup by augmenting the prompts with intermediate steps.
Despite impressive results across various tasks, the reasons behind their
success have not been explored. This work uses counterfactual prompting to
develop a deeper understanding of CoT-based few-shot prompting mechanisms in
large language models. We first systematically identify and define the key
components of a prompt: symbols, patterns, and text. Then, we devise and
conduct an exhaustive set of experiments across four different tasks, by
querying the model with counterfactual prompts where only one of these
components is altered. Our experiments across three models (PaLM, GPT-3, and
CODEX) reveal several surprising findings and brings into question the
conventional wisdom around few-shot prompting. First, the presence of factual
patterns in a prompt is practically immaterial to the success of CoT. Second,
our results conclude that the primary role of intermediate steps may not be to
facilitate learning how to solve a task. The intermediate steps are rather a
beacon for the model to realize what symbols to replicate in the output to form
a factual answer. Further, text imbues patterns with commonsense knowledge and
meaning. Our empirical and qualitative analysis reveals that a symbiotic
relationship between text and patterns explains the success of few-shot
prompting: text helps extract commonsense from the question to help patterns,
and patterns enforce task understanding and direct text generation.",https://github.com/google-research/google-research/tree/master/l2da/learned2design,-1
f19ed2b3-955a-462d-a971-b7623bbb5c17,Automated Parkinson's Disease Detection and Affective Analysis from Emotional EEG Signals,0.0571008,"While Parkinson's disease (PD) is typically characterized by motor disorder,
there is evidence of diminished emotion perception in PD patients. This study
examines the utility of affective Electroencephalography (EEG) signals to
understand emotional differences between PD vs Healthy Controls (HC), and for
automated PD detection. Employing traditional machine learning and deep
learning methods, we explore (a) dimensional and categorical emotion
recognition, and (b) PD vs HC classification from emotional EEG signals. Our
results reveal that PD patients comprehend arousal better than valence, and
amongst emotion categories, \textit{fear}, \textit{disgust} and
\textit{surprise} less accurately, and \textit{sadness} most accurately.
Mislabeling analyses confirm confounds among opposite-valence emotions with PD
data. Emotional EEG responses also achieve near-perfect PD vs HC recognition.
{Cumulatively, our study demonstrates that (a) examining \textit{implicit}
responses alone enables (i) discovery of valence-related impairments in PD
patients, and (ii) differentiation of PD from HC, and (b) emotional EEG
analysis is an ecologically-valid, effective, facile and sustainable tool for
PD diagnosis vis-\'a-vis self reports, expert assessments and resting-state
analysis.}",https://github.com/ravikiranrao/PD-EEG,-1
9b3786bd-a6ec-4d98-a716-9f7a6ca4f366,ScanNeRF: a Scalable Benchmark for Neural Radiance Fields,0.378637,"In this paper, we propose the first-ever real benchmark thought for
evaluating Neural Radiance Fields (NeRFs) and, in general, Neural Rendering
(NR) frameworks. We design and implement an effective pipeline for scanning
real objects in quantity and effortlessly. Our scan station is built with less
than 500$ hardware budget and can collect roughly 4000 images of a scanned
object in just 5 minutes. Such a platform is used to build ScanNeRF, a dataset
characterized by several train/val/test splits aimed at benchmarking the
performance of modern NeRF methods under different conditions. Accordingly, we
evaluate three cutting-edge NeRF variants on it to highlight their strengths
and weaknesses. The dataset is available on our project page, together with an
online benchmark to foster the development of better and better NeRFs.",https://github.com/yenchenlin/awesome-NeRF,-1
0ba0061e-83bb-4ee5-a3d5-f6770ac62899,Block-Segmentation Vectors for Arousal Prediction using Semi-supervised Learning,0.0157475,"To handle emotional expressions in computer applications, Russell's circum-
plex model has been useful for representing emotions according to valence and
arousal. In SentiWordNet, the level of valence is automatically assigned to a
large number of synsets (groups of synonyms in WordNet) using semi-supervised
learning. However, when assigning the level of arousal, the existing method
proposed for SentiWordNet reduces the accuracy of sentiment prediction. In this
paper, we propose a block-segmentation vector for predicting the arousal levels
of many synsets from a small number of labeled words using semi-supervised
learning. We analyze the distribution of arousal and non-arousal words in a
corpus of sentences by comparing it with the distribution of valence words. We
address the problem that arousal level prediction fails when arousal and
non-arousal words are mixed together in some sentences. To capture the features
of such arousal and non-arousal words, we generate word vectors based on
inverted indexes by block IDs, where the corpus is divided into blocks in the
flow of sentences. In the evaluation experiment, we show that the results of
arousal prediction with the block-segmentation vectors outperform the results
of the previous method in SentiWordNet.",None,-1
1be6fd32-a077-4753-9e05-6ff963b3bf7e,Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents,0.715892,"At the heart of improving conversational AI is the open problem of how to
evaluate conversations. Issues with automatic metrics are well known (Liu et
al., 2016, arXiv:1603.08023), with human evaluations still considered the gold
standard. Unfortunately, how to perform human evaluations is also an open
problem: differing data collection methods have varying levels of human
agreement and statistical sensitivity, resulting in differing amounts of human
annotation hours and labor costs. In this work we compare five different
crowdworker-based human evaluation methods and find that different methods are
best depending on the types of models compared, with no clear winner across the
board. While this highlights the open problems in the area, our analysis leads
to advice of when to use which one, and possible future directions.",None,7769
c2c815cc-4005-4ec5-a9d7-711a85ff34b5,Capitalization Normalization for Language Modeling with an Accurate and Efficient Hierarchical RNN Model,0.208077,"Capitalization normalization (truecasing) is the task of restoring the
correct case (uppercase or lowercase) of noisy text. We propose a fast,
accurate and compact two-level hierarchical word-and-character-based recurrent
neural network model. We use the truecaser to normalize user-generated text in
a Federated Learning framework for language modeling. A case-aware language
model trained on this normalized text achieves the same perplexity as a model
trained on text with gold capitalization. In a real user A/B experiment, we
demonstrate that the improvement translates to reduced prediction error rates
in a virtual keyboard application. Similarly, in an ASR language model fusion
experiment, we show reduction in uppercase character error rate and word error
rate.",https://github.com/google-research-datasets/wikipedia-intrinsic-capitalization,-1
27b78b16-bdf6-45f9-917c-953d56e1bdb6,Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model,0.0410551,"Recently, deep learning-based algorithms are widely adopted due to the
advantage of being able to establish anomaly detection models without or with
minimal domain knowledge of the task. Instead, to train the artificial neural
network more stable, it should be better to define the appropriate neural
network structure or the loss function. For the training anomaly detection
model, the mean squared error (MSE) function is adopted widely. On the other
hand, the novel loss function, logarithmic mean squared error (LMSE), is
proposed in this paper to train the neural network more stable. This study
covers a variety of comparisons from mathematical comparisons, visualization in
the differential domain for backpropagation, loss convergence in the training
process, and anomaly detection performance. In an overall view, LMSE is
superior to the existing MSE function in terms of strongness of loss
convergence, anomaly detection performance. The LMSE function is expected to be
applicable for training not only the anomaly detection model but also the
general generative neural network.",None,-1
46addc12-5a56-478c-9abf-1d05e10829fd,"What's Different between Visual Question Answering for Machine ""Understanding"" Versus for Accessibility?",0.106056,"In visual question answering (VQA), a machine must answer a question given an
associated image. Recently, accessibility researchers have explored whether VQA
can be deployed in a real-world setting where users with visual impairments
learn about their environment by capturing their visual surroundings and asking
questions. However, most of the existing benchmarking datasets for VQA focus on
machine ""understanding"" and it remains unclear how progress on those datasets
corresponds to improvements in this real-world use case. We aim to answer this
question by evaluating discrepancies between machine ""understanding"" datasets
(VQA-v2) and accessibility datasets (VizWiz) by evaluating a variety of VQA
models. Based on our findings, we discuss opportunities and challenges in VQA
for accessibility and suggest directions for future work.",https://github.com/kyleseelman/vqa_accessibility,-1
8954fa3c-ae16-4f93-b6d2-b9614c990979,Positive-Unlabeled Learning with Adversarial Data Augmentation for Knowledge Graph Completion,0.812222,"Most real-world knowledge graphs (KG) are far from complete and
comprehensive. This problem has motivated efforts in predicting the most
plausible missing facts to complete a given KG, i.e., knowledge graph
completion (KGC). However, existing KGC methods suffer from two main issues, 1)
the false negative issue, i.e., the sampled negative training instances may
include potential true facts; and 2) the data sparsity issue, i.e., true facts
account for only a tiny part of all possible facts. To this end, we propose
positive-unlabeled learning with adversarial data augmentation (PUDA) for KGC.
In particular, PUDA tailors positive-unlabeled risk estimator for the KGC task
to deal with the false negative issue. Furthermore, to address the data
sparsity issue, PUDA achieves a data augmentation strategy by unifying
adversarial training and positive-unlabeled learning under the
positive-unlabeled minimax game. Extensive experimental results on real-world
benchmark datasets demonstrate the effectiveness and compatibility of our
proposed method.",https://github.com/lilv98/PUDA-IJCAI22,-1
3ddcdb75-2477-4c3a-8a41-86c19b80a224,"Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",0.536982,"While the problem of hallucinations in neural machine translation has long
been recognized, so far the progress on its alleviation is very little. Indeed,
recently it turned out that without artificially encouraging models to
hallucinate, previously existing methods fall short and even the standard
sequence log-probability is more informative. It means that characteristics
internal to the model can give much more information than we expect, and before
using external models and measures, we first need to ask: how far can we go if
we use nothing but the translation model itself ? We propose to use a method
that evaluates the percentage of the source contribution to a generated
translation. Intuitively, hallucinations are translations ""detached"" from the
source, hence they can be identified by low source contribution. This method
improves detection accuracy for the most severe hallucinations by a factor of 2
and is able to alleviate hallucinations at test time on par with the previous
best approach that relies on external models. Next, if we move away from
internal model characteristics and allow external tools, we show that using
sentence similarity from cross-lingual embeddings further improves these
results.",https://github.com/facebookresearch/stopes,2821
4ef24504-4329-41f6-8e85-7821a87842ce,Better Smatch = Better Parser? AMR evaluation is not so simple anymore,0.665558,"Recently, astonishing advances have been observed in AMR parsing, as measured
by the structural Smatch metric. In fact, today's systems achieve performance
levels that seem to surpass estimates of human inter annotator agreement (IAA).
Therefore, it is unclear how well Smatch (still) relates to human estimates of
parse quality, as in this situation potentially fine-grained errors of similar
weight may impact the AMR's meaning to different degrees.
  We conduct an analysis of two popular and strong AMR parsers that --
according to Smatch -- reach quality levels on par with human IAA, and assess
how human quality ratings relate to Smatch and other AMR metrics. Our main
findings are: i) While high Smatch scores indicate otherwise, we find that AMR
parsing is far from being solved: we frequently find structurally small, but
semantically unacceptable errors that substantially distort sentence meaning.
ii) Considering high-performance parsers, better Smatch scores may not
necessarily indicate consistently better parsing quality. To obtain a
meaningful and comprehensive assessment of quality differences of parse(r)s, we
recommend augmenting evaluations with macro statistics, use of additional
metrics, and more human analysis.",https://github.com/Heidelberg-nlp/AMRParseEval,-1
a7185e4d-9e48-4752-acd9-b19a75905e21,UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue,0.809392,"The goal-oriented document-grounded dialogue aims at responding to the user
query based on the dialogue context and supporting document. Existing studies
tackle this problem by decomposing it into two sub-tasks: knowledge
identification and response generation. However, such pipeline methods would
unavoidably suffer from the error propagation issue. This paper proposes to
unify these two sub-tasks via sequentially generating the grounding knowledge
and the response. We further develop a prompt-connected multi-task learning
strategy to model the characteristics and connections of different tasks and
introduce linear temperature scheduling to reduce the negative effect of
irrelevant document information. Experimental results demonstrate the
effectiveness of our framework.",https://github.com/doc2dial/sharedtask-dialdoc2021,-1
c8e7935f-a910-4302-a03f-2f1516f925c9,Visual Acoustic Matching,0.983029,"We introduce the visual acoustic matching task, in which an audio clip is
transformed to sound like it was recorded in a target environment. Given an
image of the target environment and a waveform for the source audio, the goal
is to re-synthesize the audio to match the target room acoustics as suggested
by its visible geometry and materials. To address this novel task, we propose a
cross-modal transformer model that uses audio-visual attention to inject visual
properties into the audio and generate realistic audio output. In addition, we
devise a self-supervised training objective that can learn acoustic matching
from in-the-wild Web videos, despite their lack of acoustically mismatched
audio. We demonstrate that our approach successfully translates human speech to
a variety of real-world environments depicted in images, outperforming both
traditional acoustic matching and more heavily supervised baselines.",None,-1
8e7576a2-1147-4daa-bd45-29efbc5704de,Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning,0.691008,"In this paper, we elaborate upon recipes for building multilingual
representation models that are not only competitive with existing
state-of-the-art models but are also more parameter efficient, thereby
promoting better adoption in resource-constrained scenarios and practical
applications. We show that going beyond English-centric bitexts, coupled with a
novel sampling strategy aimed at reducing under-utilization of training data,
substantially boosts performance across model sizes for both Electra and MLM
pre-training objectives. We introduce XY-LENT: X-Y bitext enhanced Language
ENcodings using Transformers which not only achieves state-of-the-art
performance over 5 cross-lingual tasks within all model size bands, is also
competitive across bands. Our XY-LENT XL variant outperforms XLM-RXXL and
exhibits competitive performance with mT5 XXL while being 5x and 6x smaller
respectively. We then show that our proposed method helps ameliorate the curse
of multilinguality, with the XY-LENT XL achieving 99.3% GLUE performance and
98.5% SQuAD 2.0 performance compared to a SoTA English only model in the same
size band. We then analyze our models performance on extremely low resource
languages and posit that scaling alone may not be sufficient for improving the
performance in this scenario",None,-1
8445e0c3-e5ec-4456-bec7-cc6caf4e1535,A Mask Attention Interaction and Scale Enhancement Network for SAR Ship Instance Segmentation,0.991808,"Most of existing synthetic aperture radar (SAR) ship in-stance segmentation
models do not achieve mask interac-tion or offer limited interaction
performance. Besides, their multi-scale ship instance segmentation performance
is moderate especially for small ships. To solve these problems, we propose a
mask attention interaction and scale enhancement network (MAI-SE-Net) for SAR
ship instance segmentation. MAI uses an atrous spatial pyra-mid pooling (ASPP)
to gain multi-resolution feature re-sponses, a non-local block (NLB) to model
long-range spa-tial dependencies, and a concatenation shuffle attention block
(CSAB) to improve interaction benefits. SE uses a content-aware reassembly of
features block (CARAFEB) to generate an extra pyramid bottom-level to boost
small ship performance, a feature balance operation (FBO) to improve scale
feature description, and a global context block (GCB) to refine features.
Experimental results on two public SSDD and HRSID datasets reveal that
MAI-SE-Net outperforms the other nine competitive models, better than the
suboptimal model by 4.7% detec-tion AP and 3.4% segmentation AP on SSDD and by
3.0% detection AP and 2.4% segmentation AP on HRSID.",None,-1
21471c0e-f403-4713-9a9c-145693279fff,Self-Knowledge Distillation via Dropout,0.22361,"To boost the performance, deep neural networks require deeper or wider
network structures that involve massive computational and memory costs. To
alleviate this issue, the self-knowledge distillation method regularizes the
model by distilling the internal knowledge of the model itself. Conventional
self-knowledge distillation methods require additional trainable parameters or
are dependent on the data. In this paper, we propose a simple and effective
self-knowledge distillation method using a dropout (SD-Dropout). SD-Dropout
distills the posterior distributions of multiple models through a dropout
sampling. Our method does not require any additional trainable modules, does
not rely on data, and requires only simple operations. Furthermore, this simple
method can be easily combined with various self-knowledge distillation
approaches. We provide a theoretical and experimental analysis of the effect of
forward and reverse KL-divergences in our work. Extensive experiments on
various vision tasks, i.e., image classification, object detection, and
distribution shift, demonstrate that the proposed method can effectively
improve the generalization of a single network. Further experiments show that
the proposed method also improves calibration performance, adversarial
robustness, and out-of-distribution detection ability.",None,295
87c60df3-a4ab-40f7-a8f3-7fb998f764e7,Human-to-Robot Imitation in the Wild,0.877126,"We approach the problem of learning by watching humans in the wild. While
traditional approaches in Imitation and Reinforcement Learning are promising
for learning in the real world, they are either sample inefficient or are
constrained to lab settings. Meanwhile, there has been a lot of success in
processing passive, unstructured human data. We propose tackling this problem
via an efficient one-shot robot learning algorithm, centered around learning
from a third-person perspective. We call our method WHIRL: In-the-Wild Human
Imitating Robot Learning. WHIRL extracts a prior over the intent of the human
demonstrator, using it to initialize our agent's policy. We introduce an
efficient real-world policy learning scheme that improves using interactions.
Our key contributions are a simple sampling-based policy optimization approach,
a novel objective function for aligning human and robot videos as well as an
exploration method to boost sample efficiency. We show one-shot generalization
and success in real-world settings, including 20 different manipulation tasks
in the wild. Videos and talk at https://human2robot.github.io",https://github.com/hello-robot,-1
012f0bf5-9666-498d-b95c-10b0379ac8ea,Challenges and Opportunities of Edge AI for Next-Generation Implantable BMIs,0.570493,"Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.",None,-1
3a47accd-9dc4-4f7c-8366-af22794b5195,Spectral Probing,0.133584,"Linguistic information is encoded at varying timescales (subwords, phrases,
etc.) and communicative levels, such as syntax and semantics. Contextualized
embeddings have analogously been found to capture these phenomena at
distinctive layers and frequencies. Leveraging these findings, we develop a
fully learnable frequency filter to identify spectral profiles for any given
task. It enables vastly more granular analyses than prior handcrafted filters,
and improves on efficiency. After demonstrating the informativeness of spectral
probing over manual filters in a monolingual setting, we investigate its
multilingual characteristics across seven diverse NLP tasks in six languages.
Our analyses identify distinctive spectral profiles which quantify cross-task
similarity in a linguistically intuitive manner, while remaining consistent
across languages-highlighting their potential as robust, lightweight task
descriptors.",https://github.com/mainlp/spectral-probing,6816
ded3efd1-b37a-4b87-9bea-fb76f7a7d656,Diversity-aware social robots meet people: beyond context-aware embodied AI,0.254747,"The article introduces the concept of ""diversity-aware"" robotics and
discusses the need to develop computational models to embed robots with
diversity-awareness: that is, robots capable of adapting and re-configuring
their behavior to recognize, respect, and value the uniqueness of the person
they interact with to promote inclusion regardless of their age, race, gender,
cognitive or physical capabilities, etc. Finally, the article discusses
possible technical solutions based on Ontologies and Bayesian Networks,
starting from previous experience with culturally competent robots.",None,-1
303385d4-7c00-4ac3-ada0-7012d15f8ea8,Boosting Entity Mention Detection for Targetted Twitter Streams with Global Contextual Embeddings,0.0931433,"Microblogging sites, like Twitter, have emerged as ubiquitous sources of
information. Two important tasks related to the automatic extraction and
analysis of information in Microblogs are Entity Mention Detection (EMD) and
Entity Detection (ED). The state-of-the-art EMD systems aim to model the
non-literary nature of microblog text by training upon offline static datasets.
They extract a combination of surface-level features -- orthographic, lexical,
and semantic -- from individual messages for noisy text modeling and entity
extraction. But given the constantly evolving nature of microblog streams,
detecting all entity mentions from such varying yet limited context of short
messages remains a difficult problem. To this end, we propose a framework named
EMD Globalizer, better suited for the execution of EMD learners on microblog
streams. It deviates from the processing of isolated microblog messages by
existing EMD systems, where learned knowledge from the immediate context of a
message is used to suggest entities. After an initial extraction of entity
candidates by an EMD system, the proposed framework leverages occurrence mining
to find additional candidate mentions that are missed during this first
detection. Aggregating the local contextual representations of these mentions,
a global embedding is drawn from the collective context of an entity candidate
within a stream. The global embeddings are then utilized to separate entities
within the candidates from false positives. All mentions of said entities from
the stream are produced in the framework's final outputs. Our experiments show
that EMD Globalizer can enhance the effectiveness of all existing EMD systems
that we tested (on average by 25.61%) with a small additional computational
overhead.",https://github.com/satadisha/collective-EMD-framework,-1
150b46b0-bb45-438f-a814-3d493a918657,Auto Machine Learning for Medical Image Analysis by Unifying the Search on Data Augmentation and Neural Architecture,0.0835333,"Automated data augmentation, which aims at engineering augmentation policy
automatically, recently draw a growing research interest. Many previous
auto-augmentation methods utilized a Density Matching strategy by evaluating
policies in terms of the test-time augmentation performance. In this paper, we
theoretically and empirically demonstrated the inconsistency between the train
and validation set of small-scale medical image datasets, referred to as
in-domain sampling bias. Next, we demonstrated that the in-domain sampling bias
might cause the inefficiency of Density Matching. To address the problem, an
improved augmentation search strategy, named Augmented Density Matching, was
proposed by randomly sampling policies from a prior distribution for training.
Moreover, an efficient automatical machine learning(AutoML) algorithm was
proposed by unifying the search on data augmentation and neural architecture.
Experimental results indicated that the proposed methods outperformed
state-of-the-art approaches on MedMNIST, a pioneering benchmark designed for
AutoML in medical image analysis.",None,456
5f324795-99b0-4296-bea1-49dd0e6835f2,Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations,0.726289,"In this paper, we show that recent advances in self-supervised feature
learning enable unsupervised object discovery and semantic segmentation with a
performance that matches the state of the field on supervised semantic
segmentation 10 years ago. We propose a methodology based on unsupervised
saliency masks and self-supervised feature clustering to kickstart object
discovery followed by training a semantic segmentation network on pseudo-labels
to bootstrap the system on images with multiple objects. We present results on
PASCAL VOC that go far beyond the current state of the art (50.0 mIoU), and we
report for the first time results on MS COCO for the whole set of 81 classes:
our method discovers 34 categories with more than $20\%$ IoU, while obtaining
an average IoU of 19.6 for all 81 categories.",https://github.com/zadaianchuk/comus,-1
1ff1a2ed-4e0f-4193-a6a5-a5140c9f32d5,Conservative Distributional Reinforcement Learning with Safety Constraints,0.0988142,"Safety exploration can be regarded as a constrained Markov decision problem
where the expected long-term cost is constrained. Previous off-policy
algorithms convert the constrained optimization problem into the corresponding
unconstrained dual problem by introducing the Lagrangian relaxation technique.
However, the cost function of the above algorithms provides inaccurate
estimations and causes the instability of the Lagrange multiplier learning. In
this paper, we present a novel off-policy reinforcement learning algorithm
called Conservative Distributional Maximum a Posteriori Policy Optimization
(CDMPO). At first, to accurately judge whether the current situation satisfies
the constraints, CDMPO adapts distributional reinforcement learning method to
estimate the Q-function and C-function. Then, CDMPO uses a conservative value
function loss to reduce the number of violations of constraints during the
exploration process. In addition, we utilize Weighted Average Proportional
Integral Derivative (WAPID) to update the Lagrange multiplier stably. Empirical
results show that the proposed method has fewer violations of constraints in
the early exploration process. The final test results also illustrate that our
method has better risk control.",None,6541
75730eef-200b-43c0-873c-6e8e7c3e818d,Multi-Objective Coordination Graphs for the Expected Scalarised Returns with Generative Flow Models,0.27393,"Many real-world problems contain multiple objectives and agents, where a
trade-off exists between objectives. Key to solving such problems is to exploit
sparse dependency structures that exist between agents. For example, in wind
farm control a trade-off exists between maximising power and minimising stress
on the systems components. Dependencies between turbines arise due to the wake
effect. We model such sparse dependencies between agents as a multi-objective
coordination graph (MO-CoG). In multi-objective reinforcement learning a
utility function is typically used to model a users preferences over
objectives, which may be unknown a priori. In such settings a set of optimal
policies must be computed. Which policies are optimal depends on which
optimality criterion applies. If the utility function of a user is derived from
multiple executions of a policy, the scalarised expected returns (SER) must be
optimised. If the utility of a user is derived from a single execution of a
policy, the expected scalarised returns (ESR) criterion must be optimised. For
example, wind farms are subjected to constraints and regulations that must be
adhered to at all times, therefore the ESR criterion must be optimised. For
MO-CoGs, the state-of-the-art algorithms can only compute a set of optimal
policies for the SER criterion, leaving the ESR criterion understudied. To
compute a set of optimal polices under the ESR criterion, also known as the ESR
set, distributions over the returns must be maintained. Therefore, to compute a
set of optimal policies under the ESR criterion for MO-CoGs, we present a novel
distributional multi-objective variable elimination (DMOVE) algorithm. We
evaluate DMOVE in realistic wind farm simulations. Given the returns in
real-world wind farm settings are continuous, we utilise a model known as
real-NVP to learn the continuous return distributions to calculate the ESR set.",None,-1
b4949ef8-6b58-4a1e-8726-da9e0d430d58,Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation,0.447913,"We present Referee, a novel framework for sentence summarization that can be
trained reference-free (i.e., requiring no gold summaries for supervision),
while allowing direct control for compression ratio. Our work is the first to
demonstrate that reference-free, controlled sentence summarization is feasible
via the conceptual framework of Symbolic Knowledge Distillation (West et al.,
2022), where latent knowledge in pre-trained language models is distilled via
explicit examples sampled from the teacher models, further purified with three
types of filters: length, fidelity, and Information Bottleneck. Moreover, we
uniquely propose iterative distillation of knowledge, where student models from
the previous iteration of distillation serve as teacher models in the next
iteration. Starting off from a relatively modest set of GPT3-generated
summaries, we demonstrate how iterative knowledge distillation can lead to
considerably smaller, but better summarizers with sharper controllability. A
useful by-product of this iterative distillation process is a high-quality
dataset of sentence-summary pairs with varying degrees of compression ratios.
Empirical results demonstrate that the final student models vastly outperform
the much larger GPT3-Instruct model in terms of the controllability of
compression ratios, without compromising the quality of resulting
summarization.",https://github.com/msclar/referee,-1
36e17ce4-5171-41b7-bd72-a75614ccad1b,PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch,0.820822,"Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a local region. Patch attacks can be highly effective in a
variety of tasks and physically realizable via attachment (e.g. a sticker) to
the real-world objects. Despite the diversity in attack patterns, adversarial
patches tend to be highly textured and different in appearance from natural
images. We exploit this property and present PatchZero, a general defense
pipeline against white-box adversarial patches without retraining the
downstream classifier or detector. Specifically, our defense detects
adversaries at the pixel-level and ""zeros out"" the patch region by repainting
with mean pixel values. We further design a two-stage adversarial training
scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA
defense performance on the image classification (ImageNet, RESISC45), object
detection (PASCAL VOC), and video classification (UCF101) tasks with little
degradation in benign performance. In addition, PatchZero transfers to
different patch shapes and attack types.",https://github.com/Trusted-AI/adversarial-robustness-toolbox,36875
f308d051-ea73-46ac-9467-067ceeb703c2,A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,0.784893,"Deep reinforcement learning is a promising approach to learning policies in
uncontrolled environments that do not require domain knowledge. Unfortunately,
due to sample inefficiency, deep RL applications have primarily focused on
simulated environments. In this work, we demonstrate that the recent
advancements in machine learning algorithms and libraries combined with a
carefully tuned robot controller lead to learning quadruped locomotion in only
20 minutes in the real world. We evaluate our approach on several indoor and
outdoor terrains which are known to be challenging for classical model-based
controllers. We observe the robot to be able to learn walking gait consistently
on all of these terrains. Finally, we evaluate our design decisions in a
simulated environment.",None,-1
a10eba08-7115-4e02-9128-248f7f7113e7,There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with Adversarial Activated Multi-Reference Learning,0.0464634,"Knowledge-grounded conversation (KGC) shows excellent potential to deliver an
engaging and informative response. However, existing approaches emphasize
selecting one golden knowledge given a particular dialogue context, overlooking
the one-to-many phenomenon in dialogue. As a result, the existing paradigm
limits the diversity of knowledge selection and generation. To this end, we
establish a multi-reference KGC dataset and propose a series of metrics to
systematically assess the one-to-many efficacy of existing KGC models.
Furthermore, to extend the hypothesis space of knowledge selection to enhance
the mapping relationship between multiple knowledge and multiple responses, we
devise a span-based variational model and optimize the model in a wake-sleep
style with an ameliorated evidence lower bound objective to learn the
one-to-many generalization. Both automatic and human evaluations demonstrate
the efficacy of our approach.",https://github.com/TingchenFu/MultiRefKGC,-1
91868934-cb28-4bdc-be97-98cae223e341,Spectral Adversarial Training for Robust Graph Neural Network,0.793988,"Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable
to slight but adversarially designed perturbations, known as adversarial
examples. To address this issue, robust training methods against adversarial
examples have received considerable attention in the literature.
\emph{Adversarial Training (AT)} is a successful approach to learning a robust
model using adversarially perturbed training samples. Existing AT methods on
GNNs typically construct adversarial perturbations in terms of graph structures
or node features. However, they are less effective and fraught with challenges
on graph data due to the discreteness of graph structure and the relationships
between connected examples. In this work, we seek to address these challenges
and propose Spectral Adversarial Training (SAT), a simple yet effective
adversarial training approach for GNNs. SAT first adopts a low-rank
approximation of the graph structure based on spectral decomposition, and then
constructs adversarial perturbations in the spectral domain rather than
directly manipulating the original graph structure. To investigate its
effectiveness, we employ SAT on three widely used GNNs. Experimental results on
four public graph datasets demonstrate that SAT significantly improves the
robustness of GNNs against adversarial attacks without sacrificing
classification accuracy and training efficiency.",https://github.com/EdisonLeeeee/SAT,-1
8d9f4245-d097-483d-882b-50e45c1aaf3e,Write It Like You See It: Detectable Differences in Clinical Notes By Race Lead To Differential Model Recommendations,0.834701,"Clinical notes are becoming an increasingly important data source for machine
learning (ML) applications in healthcare. Prior research has shown that
deploying ML models can perpetuate existing biases against racial minorities,
as bias can be implicitly embedded in data. In this study, we investigate the
level of implicit race information available to ML models and human experts and
the implications of model-detectable differences in clinical notes. Our work
makes three key contributions. First, we find that models can identify patient
self-reported race from clinical notes even when the notes are stripped of
explicit indicators of race. Second, we determine that human experts are not
able to accurately predict patient race from the same redacted clinical notes.
Finally, we demonstrate the potential harm of this implicit information in a
simulation study, and show that models trained on these race-redacted clinical
notes can still perpetuate existing biases in clinical treatment decisions.",None,-1
9fce4f9f-87b1-4896-9e07-76b92666aa69,A Generalist Framework for Panoptic Segmentation of Images and Videos,0.820693,"Panoptic segmentation assigns semantic and instance ID labels to every pixel
of an image. As permutations of instance IDs are also valid solutions, the task
requires learning of high-dimensional one-to-many mapping. As a result,
state-of-the-art approaches use customized architectures and task-specific loss
functions. We formulate panoptic segmentation as a discrete data generation
problem, without relying on inductive bias of the task. A diffusion model is
proposed to model panoptic masks, with a simple architecture and generic loss
function. By simply adding past predictions as a conditioning signal, our
method is capable of modeling video (in a streaming setting) and thereby learns
to track object instances automatically. With extensive experiments, we
demonstrate that our simple approach can perform competitively to
state-of-the-art specialist methods in similar settings.",https://github.com/google-research/pix2seq,-1
f570ed84-05c9-4a5a-b670-597b6e1e90d3,LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection,0.580291,"Visual question answering (VQA) often requires an understanding of visual
concepts and language semantics, which relies on external knowledge. Most
existing methods exploit pre-trained language models or/and unstructured text,
but the knowledge in these resources are often incomplete and noisy. Some other
methods prefer to use knowledge graphs (KGs) which often have intensive
structured knowledge, but the research is still quite preliminary. In this
paper, we propose LaKo, a knowledge-driven VQA method via Late
Knowledge-to-text Injection. To effectively incorporate an external KG, we
transfer triples into textual format and propose a late injection mechanism for
knowledge fusion. Finally we address VQA as a text generation task with an
effective encoder-decoder paradigm, which achieves state-of-the-art results on
OKVQA dataset.",https://github.com/hackerchenzhuo/LaKo,-1
72ffd20c-82d7-4366-81b9-eefb2b8fa68c,ImPosing: Implicit Pose Encoding for Efficient Visual Localization,0.339391,"We propose a novel learning-based formulation for visual localization of
vehicles that can operate in real-time in city-scale environments. Visual
localization algorithms determine the position and orientation from which an
image has been captured, using a set of geo-referenced images or a 3D scene
representation. Our new localization paradigm, named Implicit Pose Encoding
(ImPosing), embeds images and camera poses into a common latent representation
with 2 separate neural networks, such that we can compute a similarity score
for each image-pose pair. By evaluating candidates through the latent space in
a hierarchical manner, the camera position and orientation are not directly
regressed but incrementally refined. Very large environments force competitors
to store gigabytes of map data, whereas our method is very compact
independently of the reference database size. In this paper, we describe how to
effectively optimize our learned modules, how to combine them to achieve
real-time localization, and demonstrate results on diverse large scale
scenarios that significantly outperform prior work in accuracy and
computational efficiency.",https://github.com/Nanne/pytorch-NetVlad,-1
605f9129-082f-4b3b-80a1-2675ade94f95,Does human speech follow Benford's Law?,0.0959655,"Researchers have observed that the frequencies of leading digits in many
man-made and naturally occurring datasets follow a logarithmic curve, with
digits that start with the number 1 accounting for $\sim 30\%$ of all numbers
in the dataset and digits that start with the number 9 accounting for $\sim
5\%$ of all numbers in the dataset. This phenomenon, known as Benford's Law, is
highly repeatable and appears in lists of numbers from electricity bills, stock
prices, tax returns, house prices, death rates, lengths of rivers, and
naturally occurring images. In this paper we demonstrate that human speech
spectra also follow Benford's Law on average. That is, when averaged over many
speakers, the frequencies of leading digits in speech magnitude spectra follow
this distribution, although with some variability at the individual sample
level. We use this observation to motivate a new set of features that can be
efficiently extracted from speech and demonstrate that these features can be
used to classify between human speech and synthetic speech.",None,-1
8c6908a9-ee43-4131-b025-751b780cee98,Fingerprint Liveness Detection Based on Quality Measures,0.773064,"A new fingerprint parameterization for liveness detection based on quality
measures is presented. The novel feature set is used in a complete liveness
detection system and tested on the development set of the LivDET competition,
comprising over 4,500 real and fake images acquired with three different
optical sensors. The proposed solution proves to be robust to the multi-sensor
scenario, and presents an overall rate of 93% of correctly classified samples.
Furthermore, the liveness detection method presented has the added advantage
over previously studied techniques of needing just one image from a finger to
decide whether it is real or fake.",None,-1
831d288a-cd9b-46e5-a880-6edba9e387d9,Learning Audio-Visual embedding for Person Verification in the Wild,0.450167,"It has already been observed that audio-visual embedding is more robust than
uni-modality embedding for person verification. Here, we proposed a novel
audio-visual strategy that considers aggregators from a fusion perspective.
First, we introduced weight-enhanced attentive statistics pooling for the first
time in face verification. We find that a strong correlation exists between
modalities during pooling, so joint attentive pooling is proposed which
contains cycle consistency to learn the implicit inter-frame weight. Finally,
each modality is fused with a gated attention mechanism to gain robust
audio-visual embedding. All the proposed models are trained on the VoxCeleb2
dev dataset and the best system obtains 0.18%, 0.27%, and 0.49% EER on three
official trial lists of VoxCeleb1 respectively, which is to our knowledge the
best-published results for person verification.",None,-1
78bc5cf4-92e0-4576-8eaf-92a99e00a1c1,Image Super-resolution with An Enhanced Group Convolutional Neural Network,0.913283,"CNNs with strong learning abilities are widely chosen to resolve
super-resolution problem. However, CNNs depend on deeper network architectures
to improve performance of image super-resolution, which may increase
computational cost in general. In this paper, we present an enhanced
super-resolution group CNN (ESRGCNN) with a shallow architecture by fully
fusing deep and wide channel features to extract more accurate low-frequency
information in terms of correlations of different channels in single image
super-resolution (SISR). Also, a signal enhancement operation in the ESRGCNN is
useful to inherit more long-distance contextual information for resolving
long-term dependency. An adaptive up-sampling operation is gathered into a CNN
to obtain an image super-resolution model with low-resolution images of
different sizes. Extensive experiments report that our ESRGCNN surpasses the
state-of-the-arts in terms of SISR performance, complexity, execution speed,
image quality evaluation and visual effect in SISR. Code is found at
https://github.com/hellloxiaotian/ESRGCNN.",https://github.com/hellloxiaotian/ESRGCNN,-1
0b6a6d2c-725d-428d-934f-eb6110bb37c8,Visual Comparison of Language Model Adaptation,0.147126,"Neural language models are widely used; however, their model parameters often
need to be adapted to the specific domains and tasks of an application, which
is time- and resource-consuming. Thus, adapters have recently been introduced
as a lightweight alternative for model adaptation. They consist of a small set
of task-specific parameters with a reduced training time and simple parameter
composition. The simplicity of adapter training and composition comes along
with new challenges, such as maintaining an overview of adapter properties and
effectively comparing their produced embedding spaces. To help developers
overcome these challenges, we provide a twofold contribution. First, in close
collaboration with NLP researchers, we conducted a requirement analysis for an
approach supporting adapter evaluation and detected, among others, the need for
both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e.,
prediction-based) explanation methods. Second, motivated by the gathered
requirements, we designed a flexible visual analytics workspace that enables
the comparison of adapter properties. In this paper, we discuss several design
iterations and alternatives for interactive, comparative visual explanation
methods. Our comparative visualizations show the differences in the adapted
embedding vectors and prediction outcomes for diverse human-interpretable
concepts (e.g., person names, human qualities). We evaluate our workspace
through case studies and show that, for instance, an adapter trained on the
language debiasing task according to context-0 (decontextualized) embeddings
introduces a new type of bias where words (even gender-independent words such
as countries) become more similar to female than male pronouns. We demonstrate
that these are artifacts of context-0 embeddings.",https://github.com/Adapter-Hub/adapter-transformers,-1
28bba39c-b264-4e42-be5f-4c7de6055597,Underspecification in Scene Description-to-Depiction Tasks,0.908343,"Questions regarding implicitness, ambiguity and underspecification are
crucial for understanding the task validity and ethical concerns of multimodal
image+text systems, yet have received little attention to date. This position
paper maps out a conceptual framework to address this gap, focusing on systems
which generate images depicting scenes from scene descriptions. In doing so, we
account for how texts and images convey meaning differently. We outline a set
of core challenges concerning textual and visual ambiguity, as well as risks
that may be amplified by ambiguous and underspecified elements. We propose and
discuss strategies for addressing these challenges, including generating
visually ambiguous images, and generating a set of diverse images.",None,-1
f5f4e0ac-8273-4033-9898-3321a4a1e453,"Data Smells: Categories, Causes and Consequences, and Detection of Suspicious Data in AI-based Systems",0.544129,"High data quality is fundamental for today's AI-based systems. However,
although data quality has been an object of research for decades, there is a
clear lack of research on potential data quality issues (e.g., ambiguous,
extraneous values). These kinds of issues are latent in nature and thus often
not obvious. Nevertheless, they can be associated with an increased risk of
future problems in AI-based systems (e.g., technical debt, data-induced
faults). As a counterpart to code smells in software engineering, we refer to
such issues as Data Smells. This article conceptualizes data smells and
elaborates on their causes, consequences, detection, and use in the context of
AI-based systems. In addition, a catalogue of 36 data smells divided into three
categories (i.e., Believability Smells, Understandability Smells, Consistency
Smells) is presented. Moreover, the article outlines tool support for detecting
data smells and presents the result of an initial smell detection on more than
240 real-world datasets.",https://github.com/mkerschbaumer/rb-data-smell-detection,-1
164f2b5a-9952-4991-b19c-853fb5e3a93e,Bridging POMDPs and Bayesian decision making for robust maintenance planning under model uncertainty: An application to railway systems,0.734882,"Structural Health Monitoring (SHM) describes a process for inferring
quantifiable metrics of structural condition, which can serve as input to
support decisions on the operation and maintenance of infrastructure assets.
Given the long lifespan of critical structures, this problem can be cast as a
sequential decision making problem over prescribed horizons. Partially
Observable Markov Decision Processes (POMDPs) offer a formal framework to solve
the underlying optimal planning task. However, two issues can undermine the
POMDP solutions. Firstly, the need for a model that can adequately describe the
evolution of the structural condition under deterioration or corrective actions
and, secondly, the non-trivial task of recovery of the observation process
parameters from available monitoring data. Despite these potential challenges,
the adopted POMDP models do not typically account for uncertainty on model
parameters, leading to solutions which can be unrealistically confident. In
this work, we address both key issues. We present a framework to estimate POMDP
transition and observation model parameters directly from available data, via
Markov Chain Monte Carlo (MCMC) sampling of a Hidden Markov Model (HMM)
conditioned on actions. The MCMC inference estimates distributions of the
involved model parameters. We then form and solve the POMDP problem by
exploiting the inferred distributions, to derive solutions that are robust to
model uncertainty. We successfully apply our approach on maintenance planning
for railway track assets on the basis of a ""fractal value"" indicator, which is
computed from actual railway monitoring data.",http://github.com/google/jax,9608
3408f368-a629-4d16-bde8-2b1572d04c3b,Sequence-aware multimodal page classification of Brazilian legal documents,0.202749,"The Brazilian Supreme Court receives tens of thousands of cases each
semester. Court employees spend thousands of hours to execute the initial
analysis and classification of those cases -- which takes effort away from
posterior, more complex stages of the case management workflow. In this paper,
we explore multimodal classification of documents from Brazil's Supreme Court.
We train and evaluate our methods on a novel multimodal dataset of 6,510
lawsuits (339,478 pages) with manual annotation assigning each page to one of
six classes. Each lawsuit is an ordered sequence of pages, which are stored
both as an image and as a corresponding text extracted through optical
character recognition. We first train two unimodal classifiers: a ResNet
pre-trained on ImageNet is fine-tuned on the images, and a convolutional
network with filters of multiple kernel sizes is trained from scratch on
document texts. We use them as extractors of visual and textual features, which
are then combined through our proposed Fusion Module. Our Fusion Module can
handle missing textual or visual input by using learned embeddings for missing
data. Moreover, we experiment with bi-directional Long Short-Term Memory
(biLSTM) networks and linear-chain conditional random fields to model the
sequential nature of the pages. The multimodal approaches outperform both
textual and visual classifiers, especially when leveraging the sequential
nature of the pages.",https://github.com/peluz/victor-visual-text,733
33342f4a-1958-4233-8dbe-cac864cbc973,Efficient Visual Tracking via Hierarchical Cross-Attention Transformer,0.832323,"In recent years, target tracking has made great progress in accuracy. This
development is mainly attributed to powerful networks (such as transformers)
and additional modules (such as online update and refinement modules). However,
less attention has been paid to tracking speed. Most state-of-the-art trackers
are satisfied with the real-time speed on powerful GPUs. However, practical
applications necessitate higher requirements for tracking speed, especially
when edge platforms with limited resources are used. In this work, we present
an efficient tracking method via a hierarchical cross-attention transformer
named HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on
the edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our
HCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100,
UAV123, and VOT2020. Code and models are available at
https://github.com/chenxin-dlut/HCAT.",https://github.com/chenxin-dlut/HCAT,48744
0ebc8d90-cfd4-48e4-a98c-e156e0f61093,WeDef: Weakly Supervised Backdoor Defense for Text Classification,0.241099,"Existing backdoor defense methods are only effective for limited trigger
types. To defend different trigger types at once, we start from the
class-irrelevant nature of the poisoning process and propose a novel weakly
supervised backdoor defense framework WeDef. Recent advances in weak
supervision make it possible to train a reasonably accurate text classifier
using only a small number of user-provided, class-indicative seed words. Such
seed words shall be considered independent of the triggers. Therefore, a weakly
supervised text classifier trained by only the poisoned documents without their
labels will likely have no backdoor. Inspired by this observation, in WeDef, we
define the reliability of samples based on whether the predictions of the weak
classifier agree with their labels in the poisoned training set. We further
improve the results through a two-phase sanitization: (1) iteratively refine
the weak classifier based on the reliable samples and (2) train a binary poison
classifier by distinguishing the most unreliable samples from the most reliable
samples. Finally, we train the sanitized model on the samples that the poison
classifier predicts as benign. Extensive experiments show that WeDefis
effective against popular trigger-based attacks (e.g., words, sentences, and
paraphrases), outperforming existing defense methods.",https://github.com/LeshengJin/WeDef,4518
8cfe56ae-30f4-44e1-8bdc-c8b593f74b99,Motron: Multimodal Probabilistic Human Motion Forecasting,0.693447,"Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms.",https://github.com/TUM-AAS/motron-cvpr22,-1
e61b5001-8673-4ae4-a32b-eda86ec47346,Is Lip Region-of-Interest Sufficient for Lipreading?,0.32461,"Lip region-of-interest (ROI) is conventionally used for visual input in the
lipreading task. Few works have adopted the entire face as visual input because
lip-excluded parts of the face are usually considered to be redundant and
irrelevant to visual speech recognition. However, faces contain much more
detailed information than lips, such as speakers' head pose, emotion, identity
etc. We argue that such information might benefit visual speech recognition if
a powerful feature extractor employing the entire face is trained. In this
work, we propose to adopt the entire face for lipreading with self-supervised
learning. AV-HuBERT, an audio-visual multi-modal self-supervised learning
framework, was adopted in our experiments. Our experimental results showed that
adopting the entire face achieved 16% relative word error rate (WER) reduction
on the lipreading task, compared with the baseline method using lip as visual
input. Without self-supervised pretraining, the model with face input achieved
a higher WER than that using lip input in the case of limited training data (30
hours), while a slightly lower WER when using large amount of training data
(433 hours).",None,-1
213e01ae-455f-42a7-b9cc-c71120a82383,Classification of Distraction Levels Using Hybrid Deep Neural Networks From EEG Signals,0.20264,"Non-invasive brain-computer interface technology has been developed for
detecting human mental states with high performances. Detection of the pilots'
mental states is particularly critical because their abnormal mental states
could cause catastrophic accidents. In this study, we presented the feasibility
of classifying distraction levels (namely, normal state, low distraction, and
high distraction) by applying the deep learning method. To the best of our
knowledge, this study is the first attempt to classify distraction levels under
a flight environment. We proposed a model for classifying distraction levels. A
total of ten pilots conducted the experiment in a simulated flight environment.
The grand-average accuracy was 0.8437 for classifying distraction levels across
all subjects. Hence, we believe that it will contribute significantly to
autonomous driving or flight based on artificial intelligence technology in the
future.",None,-1
f5f09302-a932-47da-b213-416c03a7e11b,Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder,0.0863018,"It is common practice to reuse models initially trained on different data to
increase downstream task performance. Especially in the computer vision domain,
ImageNet-pretrained weights have been successfully used for various tasks. In
this work, we investigate the impact of transfer learning for segmentation
problems, being pixel-wise classification problems that can be tackled with
encoder-decoder architectures. We find that transfer learning the decoder does
not help downstream segmentation tasks, while transfer learning the encoder is
truly beneficial. We demonstrate that pretrained weights for a decoder may
yield faster convergence, but they do not improve the overall model performance
as one can obtain equivalent results with randomly initialized decoders.
However, we show that it is more effective to reuse encoder weights trained on
a segmentation or reconstruction task than reusing encoder weights trained on
classification tasks. This finding implicates that using ImageNet-pretrained
encoders for downstream segmentation problems is suboptimal. We also propose a
contrastive self-supervised approach with multiple self-reconstruction tasks,
which provides encoders that are suitable for transfer learning in segmentation
problems in the absence of segmentation labels.",https://github.com/bayer-science-for-a-better-life/contrastive-reconstruction4,-1
67b751b7-c58f-4fe7-b698-f7b765944dda,RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures,0.504984,"We introduce RIM-Net, a neural network which learns recursive implicit fields
for unsupervised inference of hierarchical shape structures. Our network
recursively decomposes an input 3D shape into two parts, resulting in a binary
tree hierarchy. Each level of the tree corresponds to an assembly of shape
parts, represented as implicit functions, to reconstruct the input shape. At
each node of the tree, simultaneous feature decoding and shape decomposition
are carried out by their respective feature and part decoders, with weight
sharing across the same hierarchy level. As an implicit field decoder, the part
decoder is designed to decompose a sub-shape, via a two-way branched
reconstruction, where each branch predicts a set of parameters defining a
Gaussian to serve as a local point distribution for shape reconstruction. With
reconstruction losses accounted for at each hierarchy level and a decomposition
loss at each node, our network training does not require any ground-truth
segmentations, let alone hierarchies. Through extensive experiments and
comparisons to state-of-the-art alternatives, we demonstrate the quality,
consistency, and interpretability of hierarchical structural inference by
RIM-Net.",None,-1
36018ecf-eb27-4de4-bbfc-9599fe9e3981,When one Logic is Not Enough: Integrating First-order Annotations in OWL Ontologies,0.486355,"In ontology development, there is a gap between domain ontologies which
mostly use the web ontology language, OWL, and foundational ontologies written
in first-order logic, FOL. To bridge this gap, we present Gavel, a tool that
supports the development of heterogeneous 'FOWL' ontologies that extend OWL
with FOL annotations, and is able to reason over the combined set of axioms.
Since FOL annotations are stored in OWL annotations, FOWL ontologies remain
compatible with the existing OWL infrastructure. We show that for the OWL
domain ontology OBI, the stronger integration with its FOL top-level ontology
BFO via our approach enables us to detect several inconsistencies. Furthermore,
existing OWL ontologies can benefit from FOL annotations. We illustrate this
with FOWL ontologies containing mereotopological axioms that enable new
meaningful inferences. Finally, we show that even for large domain ontologies
such as ChEBI, automatic reasoning with FOL annotations can be used to detect
previously unnoticed errors in the classification.",https://github.com/gavel-tool/python-gavel-owl,-1
a5be1d17-3a2e-4280-abc1-537e95cf1cc6,ATST: Audio Representation Learning with Teacher-Student Transformer,0.364117,"Self-supervised learning (SSL) learns knowledge from a large amount of
unlabeled data, and then transfers the knowledge to a specific problem with a
limited number of labeled data. SSL has achieved promising results in various
domains. This work addresses the problem of segment-level general audio SSL,
and proposes a new transformer-based teacher-student SSL model, named ATST. A
transformer encoder is developed on a recently emerged teacher-student baseline
scheme, which largely improves the modeling capability of pre-training. In
addition, a new strategy for positive pair creation is designed to fully
leverage the capability of transformer. Extensive experiments have been
conducted, and the proposed model achieves the new state-of-the-art results on
almost all of the downstream tasks.",https://github.com/Audio-WestlakeU/audiossl,-1
cef3d039-e935-4120-b6c3-6dddfb9969ce,SATr: Slice Attention with Transformer for Universal Lesion Detection,0.845477,"Universal Lesion Detection (ULD) in computed tomography plays an essential
role in computer-aided diagnosis. Promising ULD results have been reported by
multi-slice-input detection approaches which model 3D context from multiple
adjacent CT slices, but such methods still experience difficulty in obtaining a
global representation among different slices and within each individual slice
since they only use convolution-based fusion operations. In this paper, we
propose a novel Slice Attention Transformer (SATr) block which can be easily
plugged into convolution-based ULD backbones to form hybrid network structures.
Such newly formed hybrid backbones can better model long-distance feature
dependency via the cascaded self-attention modules in the Transformer block
while still holding a strong power of modeling local features with the
convolutional operations in the original backbone. Experiments with five
state-of-the-art methods show that the proposed SATr block can provide an
almost free boost to lesion detection accuracy without extra hyperparameters or
special network designs.",https://github.com/jacobgil/pytorch-grad-cam,-1
e1cb0862-3c7b-4e0d-98f2-e327e134b1be,GreenKGC: A Lightweight Knowledge Graph Completion Method,0.529388,"Knowledge graph completion (KGC) aims to discover missing relationships
between entities in knowledge graphs (KGs). Most prior KGC work focuses on
learning embeddings for entities and relations through a simple scoring
function. Yet, a higher-dimensional embedding space is usually required for a
better reasoning capability, which leads to a larger model size and hinders
applicability to real-world problems (e.g., large-scale KGs or mobile/edge
computing). A lightweight modularized KGC solution, called GreenKGC, is
proposed in this work to address this issue. GreenKGC consists of three
modules: representation learning, feature pruning, and decision learning, to
extract discriminant KG features and make accurate predictions on missing
relationships using classifiers and negative sampling. Experimental results
demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in
most datasets. In addition, low-dimensional GreenKGC can achieve competitive or
even better performance against high-dimensional models with a much smaller
model size.",https://github.com/yunchengwang/GreenKGC,-1
4b561664-406e-4cdc-981c-65cd83354e57,Question Answering and Question Generation for Finnish,0.128474,"Recent advances in the field of language modeling have improved the
state-of-the-art in question answering (QA) and question generation (QG).
However, the development of modern neural models, their benchmarks, and
datasets for training them has mainly focused on English. Finnish, like many
other languages, faces a shortage of large QA/QG model training resources,
which has prevented experimenting with state-of-the-art QA/QG fine-tuning
methods. We present the first neural QA and QG models that work with Finnish.
To train the models, we automatically translate the SQuAD dataset and then use
normalization methods to reduce the amount of problematic data created during
the translation. Using the synthetic data, together with the Finnish partition
of the TyDi-QA dataset, we fine-tune several transformer-based models to both
QA and QG and evaluate their performance. To the best of our knowledge, the
resulting dataset is the first large-scale QA/QG resource for Finnish. This
paper also sets the initial benchmarks for Finnish-language QA and QG.",https://github.com/huggingface/transformers,-1
f76fd05d-f5f8-401e-be4e-45736413a462,A Framework for Multi-stage Bonus Allocation in meal delivery Platform,0.714977,"Online meal delivery is undergoing explosive growth, as this service is
becoming increasingly popular. A meal delivery platform aims to provide
excellent and stable services for customers and restaurants. However, in
reality, several hundred thousand orders are canceled per day in the Meituan
meal delivery platform since they are not accepted by the crowd soucing
drivers. The cancellation of the orders is incredibly detrimental to the
customer's repurchase rate and the reputation of the Meituan meal delivery
platform. To solve this problem, a certain amount of specific funds is provided
by Meituan's business managers to encourage the crowdsourcing drivers to accept
more orders. To make better use of the funds, in this work, we propose a
framework to deal with the multi-stage bonus allocation problem for a meal
delivery platform. The objective of this framework is to maximize the number of
accepted orders within a limited bonus budget. This framework consists of a
semi-black-box acceptance probability model, a Lagrangian dual-based dynamic
programming algorithm, and an online allocation algorithm. The semi-black-box
acceptance probability model is employed to forecast the relationship between
the bonus allocated to order and its acceptance probability, the Lagrangian
dual-based dynamic programming algorithm aims to calculate the empirical
Lagrangian multiplier for each allocation stage offline based on the historical
data set, and the online allocation algorithm uses the results attained in the
offline part to calculate a proper delivery bonus for each order. To verify the
effectiveness and efficiency of our framework, both offline experiments on a
real-world data set and online A/B tests on the Meituan meal delivery platform
are conducted. Our results show that using the proposed framework, the total
order cancellations can be decreased by more than 25\% in reality.",None,-1
67736c89-c7b4-4dca-8cbb-89a18a548c26,HINT: Hierarchical Neuron Concept Explainer,0.885059,"To interpret deep networks, one main approach is to associate neurons with
human-understandable concepts. However, existing methods often ignore the
inherent relationships of different concepts (e.g., dog and cat both belong to
animals), and thus lose the chance to explain neurons responsible for
higher-level concepts (e.g., animal). In this paper, we study hierarchical
concepts inspired by the hierarchical cognition process of human beings. To
this end, we propose HIerarchical Neuron concepT explainer (HINT) to
effectively build bidirectional associations between neurons and hierarchical
concepts in a low-cost and scalable manner. HINT enables us to systematically
and quantitatively study whether and how the implicit hierarchical
relationships of concepts are embedded into neurons, such as identifying
collaborative neurons responsible to one concept and multimodal neurons for
different concepts, at different semantic levels from concrete concepts (e.g.,
dog) to more abstract ones (e.g., animal). Finally, we verify the faithfulness
of the associations using Weakly Supervised Object Localization, and
demonstrate its applicability in various tasks such as discovering saliency
regions and explaining adversarial attacks. Code is available on
https://github.com/AntonotnaWang/HINT.",https://github.com/AntonotnaWang/HINT,-1
94281cab-435d-4e2a-9fec-c866ac194886,Effective Image Tampering Localization with Multi-Scale ConvNeXt Feature Fusion,0.431287,"With the widespread use of powerful image editing tools, image tampering
becomes easy and realistic. Existing image forensic methods still face
challenges of low generalization performance and robustness. In this letter, we
propose an effective image tampering localization scheme based on ConvNeXt
network and multi-scale feature fusion. Stacked ConvNeXt blocks are used as an
encoder to capture hierarchical multi-scale features, which are then fused in
decoder for locating tampered pixels accurately. Combined loss and effective
data augmentation are adopted to further improve the model performance.
Extensive experimental results show that localization performance of our
proposed scheme outperforms other state-of-the-art ones. The source code will
be available at https://github.com/ZhuHC98/ITL-SSN.",https://github.com/ZhuHC98/ITL-SSN,-1
3e56dc19-c7db-43dd-a511-b8c8fd7828fe,Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness,0.176527,"Machine learning risks reinforcing biases present in data, and, as we argue
in this work, in what is absent from data. In healthcare, biases have marked
medical history, leading to unequal care affecting marginalised groups.
Patterns in missing data often reflect these group discrepancies, but the
algorithmic fairness implications of group-specific missingness are not well
understood. Despite its potential impact, imputation is often an overlooked
preprocessing step, with attention placed on the reduction of reconstruction
error and overall performance, ignoring how imputation can affect groups
differently. Our work studies how imputation choices affect reconstruction
errors across groups and algorithmic fairness properties of downstream
predictions.",https://github.com/Jeanselme/ClinicalPresenceFairness,-1
005007df-a68f-4c01-bd21-f9eca72562d0,TwistSLAM++: Fusing multiple modalities for accurate dynamic semantic SLAM,0.24785,"Most classical SLAM systems rely on the static scene assumption, which limits
their applicability in real world scenarios. Recent SLAM frameworks have been
proposed to simultaneously track the camera and moving objects. However they
are often unable to estimate the canonical pose of the objects and exhibit a
low object tracking accuracy. To solve this problem we propose TwistSLAM++, a
semantic, dynamic, SLAM system that fuses stereo images and LiDAR information.
Using semantic information, we track potentially moving objects and associate
them to 3D object detections in LiDAR scans to obtain their pose and size.
Then, we perform registration on consecutive object scans to refine object pose
estimation. Finally, object scans are used to estimate the shape of the object
and constrain map points to lie on the estimated surface within the BA. We show
on classical benchmarks that this fusion approach based on multimodal
information improves the accuracy of object tracking.",https://github.com/facebookresearch/detectron2,1442
be6280df-b5da-4c4c-b143-3e7c9ba69f32,P-Transformer: Towards Better Document-to-Document Neural Machine Translation,0.354798,"Directly training a document-to-document (Doc2Doc) neural machine translation
(NMT) via Transformer from scratch, especially on small datasets usually fails
to converge. Our dedicated probing tasks show that 1) both the absolute
position and relative position information gets gradually weakened or even
vanished once it reaches the upper encoder layers, and 2) the vanishing of
absolute position information in encoder output causes the training failure of
Doc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer
(P-Transformer) to enhance both the absolute and relative position information
in both self-attention and cross-attention. Specifically, we integrate absolute
positional information, i.e., position embeddings, into the query-key pairs
both in self-attention and cross-attention through a simple yet effective
addition operation. Moreover, we also integrate relative position encoding in
self-attention. The proposed P-Transformer utilizes sinusoidal position
encoding and does not require any task-specified position embedding, segment
embedding, or attention mechanism. Through the above methods, we build a
Doc2Doc NMT model with P-Transformer, which ingests the source document and
completely generates the target document in a sequence-to-sequence (seq2seq)
way. In addition, P-Transformer can be applied to seq2seq-based
document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)
translation. Extensive experimental results of Doc2Doc NMT show that
P-Transformer significantly outperforms strong baselines on widely-used 9
document-level datasets in 7 language pairs, covering small-, middle-, and
large-scales, and achieves a new state-of-the-art. Experimentation on discourse
phenomena shows that our Doc2Doc NMT models improve the translation quality in
both BLEU and discourse coherence. We make our code available on Github.",None,-1
059a0bca-5d2c-4306-ab5b-9d588b2ac501,Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations,0.684499,"We formulated non-speech vocalization (NSV) modeling as a text-to-speech task
and verified its viability. Specifically, we evaluated the phonetic
expressivity of HUBERT speech units on NSVs and verified our model's ability to
control over speaker timbre even though the training data is speaker few-shot.
In addition, we substantiated that the heterogeneity in recording conditions is
the major obstacle for NSV modeling. Finally, we discussed five improvements
over our method for future research. Audio samples of synthesized NSVs are
available on our demo page: https://resemble-ai.github.io/reLaugh.",None,4373
034dc8a1-7d30-4d40-924c-ddc7e9dba8a0,Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue,0.457297,"Complex conversation settings such as persuasion involve communicating
changes in attitude or behavior, so users' perspectives need to be addressed,
even when not directly related to the topic. In this work, we contribute a
novel modular dialogue system framework that seamlessly integrates factual
information and social content into persuasive dialogue. Our framework is
generalizable to any dialogue tasks that have mixed social and task contents.
We conducted a study that compared user evaluations of our framework versus a
baseline end-to-end generation model. We found our framework was evaluated more
favorably in all dimensions including competence and friendliness, compared to
the end-to-end model which does not explicitly handle social content or factual
questions.",None,-1
adfc6fd8-2410-4417-a074-98247afde9e3,TransBoost: Improving the Best ImageNet Performance using Deep Transduction,0.0220703,"This paper deals with deep transductive learning, and proposes TransBoost as
a procedure for fine-tuning any deep neural model to improve its performance on
any (unlabeled) test set provided at training time. TransBoost is inspired by a
large margin principle and is efficient and simple to use. Our method
significantly improves the ImageNet classification performance on a wide range
of architectures, such as ResNets, MobileNetV3-L, EfficientNetB0, ViT-S, and
ConvNext-T, leading to state-of-the-art transductive performance. Additionally
we show that TransBoost is effective on a wide variety of image classification
datasets. The implementation of TransBoost is provided at:
https://github.com/omerb01/TransBoost .",https://github.com/omerb01/TransBoost,-1
7b37f2a1-718d-41d9-aacd-65898da96326,Deep Probabilistic Graph Matching,0.189323,"Most previous learning-based graph matching algorithms solve the
\textit{quadratic assignment problem} (QAP) by dropping one or more of the
matching constraints and adopting a relaxed assignment solver to obtain
sub-optimal correspondences. Such relaxation may actually weaken the original
graph matching problem, and in turn hurt the matching performance. In this
paper we propose a deep learning-based graph matching framework that works for
the original QAP without compromising on the matching constraints. In
particular, we design an affinity-assignment prediction network to jointly
learn the pairwise affinity and estimate the node assignments, and we then
develop a differentiable solver inspired by the probabilistic perspective of
the pairwise affinities. Aiming to obtain better matching results, the
probabilistic solver refines the estimated assignments in an iterative manner
to impose both discrete and one-to-one matching constraints. The proposed
method is evaluated on three popularly tested benchmarks (Pascal VOC, Willow
Object and SPair-71k), and it outperforms all previous state-of-the-arts on all
benchmarks.",https://github.com/Thinklab-SJTU/ThinkMatch,-1
3fa63036-eff0-415b-ae32-7ea6d1eb09f2,CodeT: Code Generation with Generated Tests,0.99466,"The task of generating code solutions for a given programming problem can
benefit from the use of pre-trained language models such as Codex, which can
produce multiple diverse samples. However, a major challenge for this task is
to select the most appropriate solution from the multiple samples generated by
the pre-trained language models. A natural way to evaluate the quality and
correctness of a code solution is to run it against a set of test cases, but
the manual creation of such test cases is often costly and time-consuming. In
this paper, we propose a novel method, CodeT, that leverages the same
pre-trained language models to automatically generate test cases for the code
samples, thus reducing the human effort and increasing the coverage of the test
scenarios. CodeT then executes the code samples using the generated test cases,
and performs a dual execution agreement, which considers both the consistency
of the outputs against the generated test cases and the agreement of the
outputs with other code samples. We conduct comprehensive experiments on four
benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different
pre-trained language models with varying sizes and capabilities. Our results
show that CodeT can significantly improve the performance of code solution
selection over previous methods, achieving remarkable and consistent gains
across different models and benchmarks. For instance, CodeT improves the pass@1
metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8%
over the code-davinci-002 model, and an absolute improvement of more than 20%
over the previous state-of-the-art results.",https://github.com/microsoft/CodeT,-1
acae0db4-bd4d-4f55-bae0-2359db8d1721,Sparse Probabilistic Circuits via Pruning and Growing,0.836845,"Probabilistic circuits (PCs) are a tractable representation of probability
distributions allowing for exact and efficient computation of likelihoods and
marginals. There has been significant recent progress on improving the scale
and expressiveness of PCs. However, PC training performance plateaus as model
size increases. We discover that most capacity in existing large PC structures
is wasted: fully-connected parameter layers are only sparsely used. We propose
two operations: pruning and growing, that exploit the sparsity of PC
structures. Specifically, the pruning operation removes unimportant
sub-networks of the PC for model compression and comes with theoretical
guarantees. The growing operation increases model capacity by increasing the
size of the latent space. By alternatingly applying pruning and growing, we
increase the capacity that is meaningfully used, allowing us to significantly
scale up PC learning. Empirically, our learner achieves state-of-the-art
likelihoods on MNIST-family image datasets and on Penn Tree Bank language data
compared to other PC learners and less tractable deep generative models such as
flow-based models and variational autoencoders (VAEs).",https://github.com/UCLA-StarAI/SparsePC,-1
963f98ef-29f9-4815-ac5d-e1f896dbe30a,TrustAL: Trustworthy Active Learning using Knowledge Distillation,0.259605,"Active learning can be defined as iterations of data labeling, model
training, and data acquisition, until sufficient labels are acquired. A
traditional view of data acquisition is that, through iterations, knowledge
from human labels and models is implicitly distilled to monotonically increase
the accuracy and label consistency. Under this assumption, the most recently
trained model is a good surrogate for the current labeled data, from which data
acquisition is requested based on uncertainty/diversity. Our contribution is
debunking this myth and proposing a new objective for distillation. First, we
found example forgetting, which indicates the loss of knowledge learned across
iterations. Second, for this reason, the last model is no longer the best
teacher -- For mitigating such forgotten knowledge, we select one of its
predecessor models as a teacher, by our proposed notion of ""consistency"". We
show that this novel distillation is distinctive in the following three
aspects; First, consistency ensures to avoid forgetting labels. Second,
consistency improves both uncertainty/diversity of labeled data. Lastly,
consistency redeems defective labels produced by human annotators.",https://github.com/huggingface/transformers,-1
328cf194-51dd-4ee5-a21c-61704fb724e0,Learning Options via Compression,0.26867,"Identifying statistical regularities in solutions to some tasks in multi-task
reinforcement learning can accelerate the learning of new tasks. Skill learning
offers one way of identifying these regularities by decomposing pre-collected
experiences into a sequence of skills. A popular approach to skill learning is
maximizing the likelihood of the pre-collected experience with latent variable
models, where the latent variables represent the skills. However, there are
often many solutions that maximize the likelihood equally well, including
degenerate solutions. To address this underspecification, we propose a new
objective that combines the maximum likelihood objective with a penalty on the
description length of the skills. This penalty incentivizes the skills to
maximally extract common structures from the experiences. Empirically, our
objective learns skills that solve downstream tasks in fewer samples compared
to skills learned from only maximizing likelihood. Further, while most prior
works in the offline multi-task setting focus on tasks with low-dimensional
observations, our objective can scale to challenging tasks with
high-dimensional image observations.",https://github.com/yidingjiang/love,52102
c38fb8bd-d862-4620-b398-db9fe2469d14,DuAT: Dual-Aggregation Transformer Network for Medical Image Segmentation,0.802303,"Transformer-based models have been widely demonstrated to be successful in
computer vision tasks by modelling long-range dependencies and capturing global
representations. However, they are often dominated by features of large
patterns leading to the loss of local details (e.g., boundaries and small
objects), which are critical in medical image segmentation. To alleviate this
problem, we propose a Dual-Aggregation Transformer Network called DuAT, which
is characterized by two innovative designs, namely, the Global-to-Local Spatial
Aggregation (GLSA) and Selective Boundary Aggregation (SBA) modules. The GLSA
has the ability to aggregate and represent both global and local spatial
features, which are beneficial for locating large and small objects,
respectively. The SBA module is used to aggregate the boundary characteristic
from low-level features and semantic information from high-level features for
better preserving boundary details and locating the re-calibration objects.
Extensive experiments in six benchmark datasets demonstrate that our proposed
model outperforms state-of-the-art methods in the segmentation of skin lesion
images, and polyps in colonoscopy images. In addition, our approach is more
robust than existing methods in various challenging situations such as small
object segmentation and ambiguous object boundaries.",None,-1
61780441-6812-48e3-b194-bc1ce08b779f,Detecting Context-Aware Deviations in Process Executions,0.251042,"A deviation detection aims to detect deviating process instances, e.g.,
patients in the healthcare process and products in the manufacturing process. A
business process of an organization is executed in various contextual
situations, e.g., a COVID-19 pandemic in the case of hospitals and a lack of
semiconductor chip shortage in the case of automobile companies. Thus,
context-aware deviation detection is essential to provide relevant insights.
However, existing work 1) does not provide a systematic way of incorporating
various contexts, 2) is tailored to a specific approach without using an
extensive pool of existing deviation detection techniques, and 3) does not
distinguish positive and negative contexts that justify and refute deviation,
respectively. In this work, we provide a framework to bridge the aforementioned
gaps. We have implemented the proposed framework as a web service that can be
extended to various contexts and deviation detection methods. We have evaluated
the effectiveness of the proposed framework by conducting experiments using 255
different contextual scenarios.",https://github.com/janikbenzin/contect,-1
cbad9711-4396-48c4-8e45-d2676f0d2eba,Adversarially-Aware Robust Object Detector,0.641784,"Object detection, as a fundamental computer vision task, has achieved a
remarkable progress with the emergence of deep neural networks. Nevertheless,
few works explore the adversarial robustness of object detectors to resist
adversarial attacks for practical applications in various real-world scenarios.
Detectors have been greatly challenged by unnoticeable perturbation, with sharp
performance drop on clean images and extremely poor performance on adversarial
images. In this work, we empirically explore the model training for adversarial
robustness in object detection, which greatly attributes to the conflict
between learning clean images and adversarial images. To mitigate this issue,
we propose a Robust Detector (RobustDet) based on adversarially-aware
convolution to disentangle gradients for model learning on clean and
adversarial images. RobustDet also employs the Adversarial Image Discriminator
(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable
robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that
our model effectively disentangles gradients and significantly enhances the
detection robustness with maintaining the detection ability on clean images.",https://github.com/7eu7d7/RobustDet,-1
2b7b6efc-2deb-45f7-83e6-566d4a99ec9e,"When classifying grammatical role, BERT doesn't care about word order... except when it matters",0.742317,"Because meaning can often be inferred from lexical semantics alone, word
order is often a redundant cue in natural language. For example, the words
chopped, chef, and onion are more likely used to convey ""The chef chopped the
onion,"" not ""The onion chopped the chef."" Recent work has shown large language
models to be surprisingly word order invariant, but crucially has largely
considered natural prototypical inputs, where compositional meaning mostly
matches lexical expectations. To overcome this confound, we probe grammatical
role representation in English BERT and GPT-2, on instances where lexical
expectations are not sufficient, and word order knowledge is necessary for
correct classification. Such non-prototypical instances are naturally occurring
English sentences with inanimate subjects or animate objects, or sentences
where we systematically swap the arguments to make sentences like ""The onion
chopped the chef"". We find that, while early layer embeddings are largely
lexical, word order is in fact crucial in defining the later-layer
representations of words in semantically non-prototypical positions. Our
experiments isolate the effect of word order on the contextualization process,
and highlight how models use context in the uncommon, but critical, instances
where it matters.",https://github.com/toizzy/except-when-it-matters,4825
6d738a39-5a7c-4a15-9fb6-d80b7a90f8d0,Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning,0.708538,"We analyze the growth of dataset sizes used in machine learning for natural
language processing and computer vision, and extrapolate these using two
methods; using the historical growth rate and estimating the compute-optimal
dataset size for future predicted compute budgets. We investigate the growth in
data usage by estimating the total stock of unlabeled data available on the
internet over the coming decades. Our analysis indicates that the stock of
high-quality language data will be exhausted soon; likely before 2026. By
contrast, the stock of low-quality language data and image data will be
exhausted only much later; between 2030 and 2050 (for low-quality language) and
between 2030 and 2060 (for images). Our work suggests that the current trend of
ever-growing ML models that rely on enormous datasets might slow down if data
efficiency is not drastically improved or new sources of data become available.",https://github.com/epoch-research/data-stock,-1
3a08c969-9289-49fa-8a33-7d3df6f1e17d,AI Art in Architecture,0.38374,"Recent diffusion-based AI art platforms are able to create impressive images
from simple text descriptions. This makes them powerful tools for concept
design in any discipline that requires creativity in visual design tasks. This
is also true for early stages of architectural design with multiple stages of
ideation, sketching and modelling. In this paper, we investigate how applicable
diffusion-based models already are to these tasks. We research the
applicability of the platforms Midjourney, DALL-E 2 and StableDiffusion to a
series of common use cases in architectural design to determine which are
already solvable or might soon be. We also analyze how they are already being
used by analyzing a data set of 40 million Midjourney queries with NLP methods
to extract common usage patterns. With this insights we derived a workflow to
interior and exterior design that combines the strengths of the individual
platforms.",None,3533
869bf496-aaa9-4b9b-b6e7-8f80264c6844,Self-Training Vision Language BERTs with a Unified Conditional Model,0.14119,"Natural language BERTs are trained with language corpus in a self-supervised
manner. Unlike natural language BERTs, vision language BERTs need paired data
to train, which restricts the scale of VL-BERT pretraining. We propose a
self-training approach that allows training VL-BERTs from unlabeled image data.
The proposed method starts with our unified conditional model -- a vision
language BERT model that can perform zero-shot conditional generation. Given
different conditions, the unified conditional model can generate captions,
dense captions, and even questions. We use the labeled image data to train a
teacher model and use the trained model to generate pseudo captions on
unlabeled image data. We then combine the labeled data and pseudo labeled data
to train a student model. The process is iterated by putting the student model
as a new teacher. By using the proposed self-training approach and only 300k
unlabeled extra data, we are able to get competitive or even better
performances compared to the models of similar model size trained with 3
million extra image data.",None,-1
2148fc1e-5faa-4c4c-9bdf-fc7423628dbd,EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points,0.58224,"Neural radiance fields (NeRF) achieve highly photo-realistic novel-view
synthesis, but it's a challenging problem to edit the scenes modeled by
NeRF-based methods, especially for dynamic scenes. We propose editable neural
radiance fields that enable end-users to easily edit dynamic scenes and even
support topological changes. Input with an image sequence from a single camera,
our network is trained fully automatically and models topologically varying
dynamics using our picked-out surface key points. Then end-users can edit the
scene by easily dragging the key points to desired new positions. To achieve
this, we propose a scene analysis method to detect and initialize key points by
considering the dynamics in the scene, and a weighted key points strategy to
model topologically varying dynamics by joint key points and weights
optimization. Our method supports intuitive multi-dimensional (up to 3D)
editing and can generate novel scenes that are unseen in the input sequence.
Experiments demonstrate that our method achieves high-quality editing on
various dynamic scenes and outperforms the state-of-the-art. Our code and
captured data are available at https://chengwei-zheng.github.io/EditableNeRF/.",None,3333
26518c4d-daa0-405e-807b-d5bf62c06740,Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild,0.960751,"Previous methods for dynamic facial expression in the wild are mainly based
on Convolutional Neural Networks (CNNs), whose local operations ignore the
long-range dependencies in videos. To solve this problem, we propose the
spatio-temporal Transformer (STT) to capture discriminative features within
each frame and model contextual relationships among frames. Spatio-temporal
dependencies are captured and integrated by our unified Transformer.
Specifically, given an image sequence consisting of multiple frames as input,
we utilize the CNN backbone to translate each frame into a visual feature
sequence. Subsequently, the spatial attention and the temporal attention within
each block are jointly applied for learning spatio-temporal representations at
the sequence level. In addition, we propose the compact softmax cross entropy
loss to further encourage the learned features have the minimum intra-class
distance and the maximum inter-class distance. Experiments on two in-the-wild
dynamic facial expression datasets (i.e., DFEW and AFEW) indicate that our
method provides an effective way to make use of the spatial and temporal
dependencies for dynamic facial expression recognition. The source code and the
training logs will be made publicly available.",None,36241
3cfd3f3e-f284-4a51-84dd-4c70b776ca31,Informal Persian Universal Dependency Treebank,0.0297252,"This paper presents the phonological, morphological, and syntactic
distinctions between formal and informal Persian, showing that these two
variants have fundamental differences that cannot be attributed solely to
pronunciation discrepancies. Given that informal Persian exhibits particular
characteristics, any computational model trained on formal Persian is unlikely
to transfer well to informal Persian, necessitating the creation of dedicated
treebanks for this variety. We thus detail the development of the open-source
Informal Persian Universal Dependency Treebank, a new treebank annotated within
the Universal Dependencies scheme. We then investigate the parsing of informal
Persian by training two dependency parsers on existing formal treebanks and
evaluating them on out-of-domain data, i.e. the development set of our informal
treebank. Our results show that parsers experience a substantial performance
drop when we move across the two domains, as they face more unknown tokens and
structures and fail to generalize well. Furthermore, the dependency relations
whose performance deteriorates the most represent the unique properties of the
informal variant. The ultimate goal of this study that demonstrates a broader
impact is to provide a stepping-stone to reveal the significance of informal
variants of languages, which have been widely overlooked in natural language
processing tools across languages.",https://github.com/royakabiri/iPerUDT,-1
abebe689-a055-49fe-a298-9930f60ba44f,CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion,0.784557,"Knowledge graphs store a large number of factual triples while they are still
incomplete, inevitably. The previous knowledge graph completion (KGC) models
predict missing links between entities merely relying on fact-view data,
ignoring the valuable commonsense knowledge. The previous knowledge graph
embedding (KGE) techniques suffer from invalid negative sampling and the
uncertainty of fact-view link prediction, limiting KGC's performance. To
address the above challenges, we propose a novel and scalable Commonsense-Aware
Knowledge Embedding (CAKE) framework to automatically extract commonsense from
factual triples with entity concepts. The generated commonsense augments
effective self-supervision to facilitate both high-quality negative sampling
(NS) and joint commonsense and fact-view link prediction. Experimental results
on the KGC task demonstrate that assembling our framework could enhance the
performance of the original KGE models, and the proposed commonsense-aware NS
module is superior to other NS techniques. Besides, our proposed framework
could be easily adaptive to various KGE models and explain the predicted
results.",https://github.com/ngl567/CAKE,-1
3a92a602-168f-46b7-a14c-038cadd1e3ad,GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering,0.552016,"Content-based collaborative filtering (CCF) predicts user-item interactions
based on both users' interaction history and items' content information.
Recently, pre-trained language models (PLM) have been used to extract
high-quality item encodings for CCF. However, it is resource-intensive to train
a PLM-based CCF model in an end-to-end (E2E) manner, since optimization
involves back-propagating through every content encoding within a given user
interaction sequence. To tackle this issue, we propose GRAM (GRadient
Accumulation for Multi-modality in CCF), which exploits the fact that a given
item often appears multiple times within a batch of interaction histories.
Specifically, Single-step GRAM aggregates each item encoding's gradients for
back-propagation, with theoretic equivalence to the standard E2E training. As
an extension of Single-step GRAM, we propose Multi-step GRAM, which increases
the gradient update latency, achieving a further speedup with drastically less
GPU memory. GRAM significantly improves training efficiency (up to 146x) on
five datasets from two task domains of Knowledge Tracing and News
Recommendation. Our code is available at https://github.com/yoonseok312/GRAM.",https://github.com/yoonseok312/GRAM,-1
64223718-0251-4936-a16d-b8dd0f500d96,Generative Pretraining for Black-Box Optimization,0.566485,"Many problems in science and engineering involve optimizing an expensive
black-box function over a high-dimensional space. For such black-box
optimization (BBO) problems, we typically assume a small budget for online
function evaluations, but also often have access to a fixed, offline dataset
for pretraining. Prior approaches seek to utilize the offline data to
approximate the function or its inverse but are not sufficiently accurate far
from the data distribution. We propose BONET, a generative framework for
pretraining a novel black-box optimizer using offline datasets. In BONET, we
train an autoregressive model on fixed-length trajectories derived from an
offline dataset. We design a sampling strategy to synthesize trajectories from
offline data using a simple heuristic of rolling out monotonic transitions from
low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using
a causally masked Transformer and evaluate it on Design-Bench, where we rank
the best on average, outperforming state-of-the-art baselines.",https://github.com/siddarthk97/bonet,-1
e4886392-ee39-475a-bdc1-a7442e3d145b,Physics-aware Differentiable Discrete Codesign for Diffractive Optical Neural Networks,0.733773,"Diffractive optical neural networks (DONNs) have attracted lots of attention
as they bring significant advantages in terms of power efficiency, parallelism,
and computational speed compared with conventional deep neural networks (DNNs),
which have intrinsic limitations when implemented on digital platforms.
However, inversely mapping algorithm-trained physical model parameters onto
real-world optical devices with discrete values is a non-trivial task as
existing optical devices have non-unified discrete levels and non-monotonic
properties. This work proposes a novel device-to-system hardware-software
codesign framework, which enables efficient physics-aware training of DONNs
w.r.t arbitrary experimental measured optical devices across layers.
Specifically, Gumbel-Softmax is employed to enable differentiable discrete
mapping from real-world device parameters into the forward function of DONNs,
where the physical parameters in DONNs can be trained by simply minimizing the
loss function of the ML task. The results have demonstrated that our proposed
framework offers significant advantages over conventional quantization-based
methods, especially with low-precision optical devices. Finally, the proposed
algorithm is fully verified with physical experimental optical systems in
low-precision settings.",None,-1
6702d09e-8d59-4596-b084-37ce3838a7b4,Interactive Multi-Class Tiny-Object Detection,0.804051,"Annotating tens or hundreds of tiny objects in a given image is laborious yet
crucial for a multitude of Computer Vision tasks. Such imagery typically
contains objects from various categories, yet the multi-class interactive
annotation setting for the detection task has thus far been unexplored. To
address these needs, we propose a novel interactive annotation method for
multiple instances of tiny objects from multiple classes, based on a few
point-based user inputs. Our approach, C3Det, relates the full image context
with annotator inputs in a local and global manner via late-fusion and
feature-correlation, respectively. We perform experiments on the Tiny-DOTA and
LCell datasets using both two-stage and one-stage object detection
architectures to verify the efficacy of our approach. Our approach outperforms
existing approaches in interactive annotation, achieving higher mAP with fewer
clicks. Furthermore, we validate the annotation efficiency of our approach in a
user study where it is shown to be 2.85x faster and yield only 0.36x task load
(NASA-TLX, lower is better) compared to manual annotation. The code is
available at
https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection.",https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection,-1
3c26fe3d-2da4-4037-b2bc-a902840b768a,Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning,0.940425,"In this paper, we study the named entity recognition (NER) problem under
distant supervision. Due to the incompleteness of the external dictionaries
and/or knowledge bases, such distantly annotated training data usually suffer
from a high false negative rate. To this end, we formulate the Distantly
Supervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU)
learning and propose a theoretically and practically novel CONFidence-based MPU
(Conf-MPU) approach. To handle the incomplete annotations, Conf-MPU consists of
two steps. First, a confidence score is estimated for each token of being an
entity token. Then, the proposed Conf-MPU risk estimation is applied to train a
multi-class classifier for the NER task. Thorough experiments on two benchmark
datasets labeled by various external knowledge demonstrate the superiority of
the proposed Conf-MPU over existing DS-NER methods.",https://github.com/shangjingbo1226/AutoNER,3269
bdcf963e-3f76-401f-9e47-d74b3dc3efce,Hyperbolic Vision Transformers: Combining Improvements in Metric Learning,0.819661,"Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric.",https://github.com/htdt/hyp_metric,-1
904fe38e-76d9-4163-9223-6a15ea94138e,Combining Attention Module and Pixel Shuffle for License Plate Super-Resolution,0.895923,"The License Plate Recognition (LPR) field has made impressive advances in the
last decade due to novel deep learning approaches combined with the increased
availability of training data. However, it still has some open issues,
especially when the data come from low-resolution (LR) and low-quality
images/videos, as in surveillance systems. This work focuses on license plate
(LP) reconstruction in LR and low-quality images. We present a Single-Image
Super-Resolution (SISR) approach that extends the attention/transformer module
concept by exploiting the capabilities of PixelShuffle layers and that has an
improved loss function based on LPR predictions. For training the proposed
architecture, we use synthetic images generated by applying heavy Gaussian
noise in terms of Structural Similarity Index Measure (SSIM) to the original
high-resolution (HR) images. In our experiments, the proposed method
outperformed the baselines both quantitatively and qualitatively. The datasets
we created for this work are publicly available to the research community at
https://github.com/valfride/lpr-rsr/",https://github.com/valfride/lpr-rsr/,-1
dbbd7299-7ffb-4c8e-ac79-366a5febb19c,Trajectory-dependent Generalization Bounds for Deep Neural Networks via Fractional Brownian Motion,0.0439488,"Despite being tremendously overparameterized, it is appreciated that deep
neural networks trained by stochastic gradient descent (SGD) generalize
surprisingly well. Based on the Rademacher complexity of a pre-specified
hypothesis set, different norm-based generalization bounds have been developed
to explain this phenomenon. However, recent studies suggest these bounds might
be problematic as they increase with the training set size, which is contrary
to empirical evidence. In this study, we argue that the hypothesis set SGD
explores is trajectory-dependent and thus may provide a tighter bound over its
Rademacher complexity. To this end, we characterize the SGD recursion via a
stochastic differential equation by assuming the incurred stochastic gradient
noise follows the fractional Brownian motion. We then identify the Rademacher
complexity in terms of the covering numbers and relate it to the Hausdorff
dimension of the optimization trajectory. By invoking the hypothesis set
stability, we derive a novel generalization bound for deep neural networks.
Extensive experiments demonstrate that it predicts well the generalization gap
over several common experimental interventions. We further show that the Hurst
parameter of the fractional Brownian motion is more informative than existing
generalization indicators such as the power-law index and the upper
Blumenthal-Getoor index.",https://github.com/cltan023/gen2021,-1
fbe81432-7eee-459b-8933-4872b40476e2,Exemplar-free Online Continual Learning,0.752234,"Targeted for real world scenarios, online continual learning aims to learn
new tasks from sequentially available data under the condition that each data
is observed only once by the learner. Though recent works have made remarkable
achievements by storing part of learned task data as exemplars for knowledge
replay, the performance is greatly relied on the size of stored exemplars while
the storage consumption is a significant constraint in continual learning. In
addition, storing exemplars may not always be feasible for certain applications
due to privacy concerns. In this work, we propose a novel exemplar-free method
by leveraging nearest-class-mean (NCM) classifier where the class mean is
estimated during training phase on all data seen so far through online mean
update criteria. We focus on image classification task and conduct extensive
experiments on benchmark datasets including CIFAR-100 and Food-1k. The results
demonstrate that our method without using any exemplar outperforms
state-of-the-art exemplar-based approaches with large margins under standard
protocol (20 exemplars per class) and is able to achieve competitive
performance even with larger exemplar size (100 exemplars per class).",None,-1
892631ec-ef43-4f61-ab04-1b28c86d637d,DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning,0.10793,"In this paper, we propose a multi-objective camera ISP framework that
utilizes Deep Reinforcement Learning (DRL) and camera ISP toolbox that consist
of network-based and conventional ISP tools. The proposed DRL-based camera ISP
framework iteratively selects a proper tool from the toolbox and applies it to
the image to maximize a given vision task-specific reward function. For this
purpose, we implement total 51 ISP tools that include exposure correction,
color-and-tone correction, white balance, sharpening, denoising, and the
others. We also propose an efficient DRL network architecture that can extract
the various aspects of an image and make a rigid mapping relationship between
images and a large number of actions. Our proposed DRL-based ISP framework
effectively improves the image quality according to each vision task such as
RAW-to-RGB image restoration, 2D object detection, and monocular depth
estimation.",None,-1
5e60d03a-cad2-4b6a-9b31-3d663e214c61,Lexical Generalization Improves with Larger Models and Longer Training,0.24435,"While fine-tuned language models perform well on many tasks, they were also
shown to rely on superficial surface features such as lexical overlap.
Excessive utilization of such heuristics can lead to failure on challenging
inputs. We analyze the use of lexical overlap heuristics in natural language
inference, paraphrase detection, and reading comprehension (using a novel
contrastive dataset), and find that larger models are much less susceptible to
adopting lexical overlap heuristics. We also find that longer training leads
models to abandon lexical overlap heuristics. Finally, we provide evidence that
the disparity between models size has its source in the pre-trained model",https://github.com/elronbandel/lexical-generalization,-1
f8829d4c-3cdd-430d-b92c-9bcb8e2bfdd7,Ultrahyperbolic Knowledge Graph Embeddings,0.727421,"Recent knowledge graph (KG) embeddings have been advanced by hyperbolic
geometry due to its superior capability for representing hierarchies. The
topological structures of real-world KGs, however, are rather heterogeneous,
i.e., a KG is composed of multiple distinct hierarchies and non-hierarchical
graph structures. Therefore, a homogeneous (either Euclidean or hyperbolic)
geometry is not sufficient for fairly representing such heterogeneous
structures. To capture the topological heterogeneity of KGs, we present an
ultrahyperbolic KG embedding (UltraE) in an ultrahyperbolic (or
pseudo-Riemannian) manifold that seamlessly interleaves hyperbolic and
spherical manifolds. In particular, we model each relation as a
pseudo-orthogonal transformation that preserves the pseudo-Riemannian bilinear
form. The pseudo-orthogonal transformation is decomposed into various operators
(i.e., circular rotations, reflections and hyperbolic rotations), allowing for
simultaneously modeling heterogeneous structures as well as complex relational
patterns. Experimental results on three standard KGs show that UltraE
outperforms previous Euclidean- and hyperbolic-based approaches.",None,-1
69f353a8-895d-41aa-a75f-8d06edd2cb6b,MonteBoxFinder: Detecting and Filtering Primitives to Fit a Noisy Point Cloud,0.4726,"We present MonteBoxFinder, a method that, given a noisy input point cloud,
fits cuboids to the input scene. Our primary contribution is a discrete
optimization algorithm that, from a dense set of initially detected cuboids, is
able to efficiently filter good boxes from the noisy ones. Inspired by recent
applications of MCTS to scene understanding problems, we develop a stochastic
algorithm that is, by design, more efficient for our task. Indeed, the quality
of a fit for a cuboid arrangement is invariant to the order in which the
cuboids are added into the scene. We develop several search baselines for our
problem and demonstrate, on the ScanNet dataset, that our approach is more
efficient and precise. Finally, we strongly believe that our core algorithm is
very general and that it could be extended to many other problems in 3D scene
understanding.",None,-1
90733bcc-90a2-4c65-8108-a16cd77eaf21,Resolving Copycat Problems in Visual Imitation Learning via Residual Action Prediction,0.220817,"Imitation learning is a widely used policy learning method that enables
intelligent agents to acquire complex skills from expert demonstrations. The
input to the imitation learning algorithm is usually composed of both the
current observation and historical observations since the most recent
observation might not contain enough information. This is especially the case
with image observations, where a single image only includes one view of the
scene, and it suffers from a lack of motion information and object occlusions.
In theory, providing multiple observations to the imitation learning agent will
lead to better performance. However, surprisingly people find that sometimes
imitation from observation histories performs worse than imitation from the
most recent observation. In this paper, we explain this phenomenon from the
information flow within the neural network perspective. We also propose a novel
imitation learning neural network architecture that does not suffer from this
issue by design. Furthermore, our method scales to high-dimensional image
observations. Finally, we benchmark our approach on two widely used simulators,
CARLA and MuJoCo, and it successfully alleviates the copycat problem and
surpasses the existing solutions.",None,-1
700cd16a-d8ac-4199-8123-cf8a90f98ab9,Detecting the Role of an Entity in Harmful Memes: Techniques and Their Limitations,0.245924,"Harmful or abusive online content has been increasing over time, raising
concerns for social media platforms, government agencies, and policymakers.
Such harmful or abusive content can have major negative impact on society,
e.g., cyberbullying can lead to suicides, rumors about COVID-19 can cause
vaccine hesitance, promotion of fake cures for COVID-19 can cause health harms
and deaths. The content that is posted and shared online can be textual,
visual, or a combination of both, e.g., in a meme. Here, we describe our
experiments in detecting the roles of the entities (hero, villain, victim) in
harmful memes, which is part of the CONSTRAINT-2022 shared task, as well as our
system for the task. We further provide a comparative analysis of different
experimental settings (i.e., unimodal, multimodal, attention, and
augmentation). For reproducibility, we make our experimental code publicly
available. \url{https://github.com/robi56/harmful_memes_block_fusion}",https://github.com/robi56/harmful_memes_block_fusion,-1
e8cef396-04d6-4342-a121-65e630927c21,Sequential Transformer for End-to-End Person Search,0.0411007,"Person Search aims to simultaneously localize and recognize a target person
from realistic and uncropped gallery images. One major challenge of person
search comes from the contradictory goals of the two sub-tasks, i.e., person
detection focuses on finding the commonness of all persons so as to distinguish
persons from the background, while person re-identification (re-ID) focuses on
the differences among different persons. In this paper, we propose a novel
Sequential Transformer (SeqTR) for end-to-end person search to deal with this
challenge. Our SeqTR contains a detection transformer and a novel re-ID
transformer that sequentially addresses detection and re-ID tasks. The re-ID
transformer comprises the self-attention layer that utilizes contextual
information and the cross-attention layer that learns local fine-grained
discriminative features of the human body. Moreover, the re-ID transformer is
shared and supervised by multi-scale features to improve the robustness of
learned person representations. Extensive experiments on two widely-used person
search benchmarks, CUHK-SYSU and PRW, show that our proposed SeqTR not only
outperforms all existing person search methods with a 59.3% mAP on PRW but also
achieves comparable performance to the state-of-the-art results with an mAP of
94.8% on CUHK-SYSU.",None,-1
006cb6f6-50cb-47bf-80d0-6a1d4f6bbe94,Automatic Severity Classification of Dysarthric speech by using Self-supervised Model with Multi-task Learning,0.349641,"Automatic assessment of dysarthric speech is essential for sustained
treatments and rehabilitation. However, obtaining atypical speech is
challenging, often leading to data scarcity issues. To tackle the problem, we
propose a novel automatic severity assessment method for dysarthric speech,
using the self-supervised model in conjunction with multi-task learning.
Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity
classification and auxiliary automatic speech recognition (ASR). For the
baseline experiments, we employ hand-crafted acoustic features and machine
learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean
dysarthric speech QoLT database, our model outperforms the traditional baseline
methods, with a relative percentage increase of 1.25% for F1-score. In
addition, the proposed model surpasses the model trained without ASR head,
achieving 10.61% relative percentage improvements. Furthermore, we present how
multi-task learning affects the severity classification performance by
analyzing the latent representations and regularization effect.",https://github.com/juice500ml/dysarthria-mtl,654
85f29d91-f3bd-4238-9041-030e8f531f18,A Fully Memristive Spiking Neural Network with Unsupervised Learning,0.371531,"We present a fully memristive spiking neural network (MSNN) consisting of
physically-realizable memristive neurons and memristive synapses to implement
an unsupervised Spiking Time Dependent Plasticity (STDP) learning rule. The
system is fully memristive in that both neuronal and synaptic dynamics can be
realized by using memristors. The neuron is implemented using the SPICE-level
memristive integrate-and-fire (MIF) model, which consists of a minimal number
of circuit elements necessary to achieve distinct depolarization,
hyperpolarization, and repolarization voltage waveforms. The proposed MSNN
uniquely implements STDP learning by using cumulative weight changes in
memristive synapses from the voltage waveform changes across the synapses,
which arise from the presynaptic and postsynaptic spiking voltage signals
during the training process. Two types of MSNN architectures are investigated:
1) a biologically plausible memory retrieval system, and 2) a multi-class
classification system. Our circuit simulation results verify the MSNN's
unsupervised learning efficacy by replicating biological memory retrieval
mechanisms, and achieving 97.5% accuracy in a 4-pattern recognition problem in
a large scale discriminative MSNN.",None,-1
43fea4b2-9d78-45fc-a1d1-81d2b4efc638,On the Importance of Asymmetry for Siamese Representation Learning,0.642189,"Many recent self-supervised frameworks for visual representation learning are
based on certain forms of Siamese networks. Such networks are conceptually
symmetric with two parallel encoders, but often practically asymmetric as
numerous mechanisms are devised to break the symmetry. In this work, we conduct
a formal study on the importance of asymmetry by explicitly distinguishing the
two encoders within the network -- one produces source encodings and the other
targets. Our key insight is keeping a relatively lower variance in target than
source generally benefits learning. This is empirically justified by our
results from five case studies covering different variance-oriented designs,
and is aligned with our preliminary theoretical analysis on the baseline.
Moreover, we find the improvements from asymmetric designs generalize well to
longer training schedules, multiple other frameworks and newer backbones.
Finally, the combined effect of several asymmetric designs achieves a
state-of-the-art accuracy on ImageNet linear probing and competitive results on
downstream transfer. We hope our exploration will inspire more research in
exploiting asymmetry for Siamese representation learning.",https://github.com/facebookresearch/asym-siam,-1
bb782ef2-22d9-457b-887a-7dc6b701c50c,Shift-tolerant Perceptual Similarity Metric,0.96691,"Existing perceptual similarity metrics assume an image and its reference are
well aligned. As a result, these metrics are often sensitive to a small
alignment error that is imperceptible to the human eyes. This paper studies the
effect of small misalignment, specifically a small shift between the input and
reference image, on existing metrics, and accordingly develops a shift-tolerant
similarity metric. This paper builds upon LPIPS, a widely used learned
perceptual similarity metric, and explores architectural design considerations
to make it robust against imperceptible misalignment. Specifically, we study a
wide spectrum of neural network elements, such as anti-aliasing filtering,
pooling, striding, padding, and skip connection, and discuss their roles in
making a robust metric. Based on our studies, we develop a new deep neural
network-based perceptual similarity metric. Our experiments show that our
metric is tolerant to imperceptible shifts while being consistent with the
human similarity judgment.",https://tinyurl.com/5n85r28r,-1
31c9d5c8-37af-48d9-9765-e7773257d97f,Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning,0.801799,"Advances in artificial intelligence often stem from the development of new
environments that abstract real-world situations into a form where research can
be done conveniently. This paper contributes such an environment based on ideas
inspired by elementary Microeconomics. Agents learn to produce resources in a
spatially complex world, trade them with one another, and consume those that
they prefer. We show that the emergent production, consumption, and pricing
behaviors respond to environmental conditions in the directions predicted by
supply and demand shifts in Microeconomics. We also demonstrate settings where
the agents' emergent prices for goods vary over space, reflecting the local
abundance of goods. After the price disparities emerge, some agents then
discover a niche of transporting goods between regions with different
prevailing prices -- a profitable strategy because they can buy goods where
they are cheap and sell them where they are expensive. Finally, in a series of
ablation experiments, we investigate how choices in the environmental rewards,
bartering actions, agent architecture, and ability to consume tradable goods
can either aid or inhibit the emergence of this economic behavior. This work is
part of the environment development branch of a research program that aims to
build human-like artificial general intelligence through multi-agent
interactions in simulated societies. By exploring which environment features
are needed for the basic phenomena of elementary microeconomics to emerge
automatically from learning, we arrive at an environment that differs from
those studied in prior multi-agent reinforcement learning work along several
dimensions. For example, the model incorporates heterogeneous tastes and
physical abilities, and agents negotiate with one another as a grounded form of
communication.",https://github.com/deepmind/meltingpot,-1
877e9d25-7aa4-48ca-b067-fa7eaafde023,Accelerating DETR Convergence via Semantic-Aligned Matching,0.993459,"The recently developed DEtection TRansformer (DETR) establishes a new object
detection paradigm by eliminating a series of hand-crafted components. However,
DETR suffers from extremely slow convergence, which increases the training cost
significantly. We observe that the slow convergence is largely attributed to
the complication in matching object queries with target features in different
feature embedding spaces. This paper presents SAM-DETR, a
Semantic-Aligned-Matching DETR that greatly accelerates DETR's convergence
without sacrificing its accuracy. SAM-DETR addresses the convergence issue from
two perspectives. First, it projects object queries into the same embedding
space as encoded image features, where the matching can be accomplished
efficiently with aligned semantics. Second, it explicitly searches salient
points with the most discriminative features for semantic-aligned matching,
which further speeds up the convergence and boosts detection accuracy as well.
Being like a plug and play, SAM-DETR complements existing convergence solutions
well yet only introduces slight computational overhead. Extensive experiments
show that the proposed SAM-DETR achieves superior convergence as well as
competitive detection accuracy. The implementation codes are available at
https://github.com/ZhangGongjie/SAM-DETR.",https://github.com/ZhangGongjie/SAM-DETR,-1
854f19c9-cbfd-470e-aad2-e33fd0cf59af,XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence,0.953206,"Recent advances in machine learning have significantly improved the
understanding of source code data and achieved good performance on a number of
downstream tasks. Open source repositories like GitHub enable this process with
rich unlabeled code data. However, the lack of high quality labeled data has
largely hindered the progress of several code related tasks, such as program
translation, summarization, synthesis, and code search. This paper introduces
XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for
cross-lingual code intelligence. Our dataset contains fine-grained parallel
data from 8 languages (7 commonly used programming languages and English), and
supports 10 cross-lingual code tasks. To the best of our knowledge, it is the
largest parallel dataset for source code both in terms of size and the number
of languages. We also provide the performance of several state-of-the-art
baseline models for each task. We believe this new dataset can be a valuable
asset for the research community and facilitate the development and validation
of new methods for cross-lingual code intelligence.",https://github.com/reddy-lab-code-research/XLCoST,-1
94ba2c87-9d76-4df1-a67a-e8f013b27628,Accurate Polygonal Mapping of Buildings in Satellite Imagery,0.454484,"This paper studies the problem of polygonal mapping of buildings by tackling
the issue of mask reversibility that leads to a notable performance gap between
the predicted masks and polygons from the learning-based methods. We addressed
such an issue by exploiting the hierarchical supervision (of bottom-level
vertices, mid-level line segments and the high-level regional masks) and
proposed a novel interaction mechanism of feature embedding sourced from
different levels of supervision signals to obtain reversible building masks for
polygonal mapping of buildings. As a result, we show that the learned
reversible building masks take all the merits of the advances of deep
convolutional neural networks for high-performing polygonal mapping of
buildings. In the experiments, we evaluated our method on the two public
benchmarks of AICrowd and Inria. On the AICrowd dataset, our proposed method
obtains unanimous improvements on the metrics of AP, APboundary and PoLiS. For
the Inria dataset, our proposed method also obtains very competitive results on
the metrics of IoU and Accuracy. The models and source code are available at
https://github.com/SarahwXU.",https://github.com/SarahwXU/HiSup,-1
7088039f-676b-4fba-be8d-d8f87a7d4b46,"How ""Multi"" is Multi-Document Summarization?",0.69657,"The task of multi-document summarization (MDS) aims at models that, given
multiple documents as input, are able to generate a summary that combines
disperse information, originally spread across these documents. Accordingly, it
is expected that both reference summaries in MDS datasets, as well as system
summaries, would indeed be based on such dispersed information. In this paper,
we argue for quantifying and assessing this expectation. To that end, we
propose an automated measure for evaluating the degree to which a summary is
``disperse'', in the sense of the number of source documents needed to cover
its content. We apply our measure to empirically analyze several popular MDS
datasets, with respect to their reference summaries, as well as the output of
state-of-the-art systems. Our results show that certain MDS datasets barely
require combining information from multiple documents, where a single document
often covers the full summary content. Overall, we advocate using our metric
for assessing and improving the degree to which summarization datasets require
combining multi-document information, and similarly how summarization models
actually meet this challenge. Our code is available in
https://github.com/ariecattan/multi_mds.",https://github.com/ariecattan/multi_mds,-1
2754c8f2-6388-4b41-b3bc-62ec168ce5f6,GaitTAKE: Gait Recognition by Temporal Attention and Keypoint-guided Embedding,0.547193,"Gait recognition, which refers to the recognition or identification of a
person based on their body shape and walking styles, derived from video data
captured from a distance, is widely used in crime prevention, forensic
identification, and social security. However, to the best of our knowledge,
most of the existing methods use appearance, posture and temporal feautures
without considering a learned temporal attention mechanism for global and local
information fusion. In this paper, we propose a novel gait recognition
framework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),
which effectively fuses temporal-attention-based global and local appearance
feature and temporal aggregated human pose feature. Experimental results show
that our proposed method achieves a new SOTA in gait recognition with rank-1
accuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait
dataset; 90.4% accuracy on the OU-MVLP gait dataset.",None,17667
77ce9502-4cd2-42c0-be8f-ab7898584846,Jury Learning: Integrating Dissenting Voices into Machine Learning Models,0.80373,"Whose labels should a machine learning (ML) algorithm learn to emulate? For
ML tasks ranging from online comment toxicity to misinformation detection to
medical diagnosis, different groups in society may have irreconcilable
disagreements about ground truth labels. Supervised ML today resolves these
label disagreements implicitly using majority vote, which overrides minority
groups' labels. We introduce jury learning, a supervised ML approach that
resolves these disagreements explicitly through the metaphor of a jury:
defining which people or groups, in what proportion, determine the classifier's
prediction. For example, a jury learning model for online toxicity might
centrally feature women and Black jurors, who are commonly targets of online
harassment. To enable jury learning, we contribute a deep learning architecture
that models every annotator in a dataset, samples from annotators' models to
populate the jury, then runs inference to classify. Our architecture enables
juries that dynamically adapt their composition, explore counterfactuals, and
visualize dissent.",None,-1
8c922d62-a662-449a-9310-d2b372ce229a,Hidden Agenda: a Social Deduction Game with Diverse Learned Equilibria,0.581517,"A key challenge in the study of multiagent cooperation is the need for
individual agents not only to cooperate effectively, but to decide with whom to
cooperate. This is particularly critical in situations when other agents have
hidden, possibly misaligned motivations and goals. Social deduction games offer
an avenue to study how individuals might learn to synthesize potentially
unreliable information about others, and elucidate their true motivations. In
this work, we present Hidden Agenda, a two-team social deduction game that
provides a 2D environment for studying learning agents in scenarios of unknown
team alignment. The environment admits a rich set of strategies for both teams.
Reinforcement learning agents trained in Hidden Agenda show that agents can
learn a variety of behaviors, including partnering and voting without need for
communication in natural language.",None,-1
6e64a6bd-069a-43a7-97f1-77199b4060bd,Test-Time Adaptation for Visual Document Understanding,0.273317,"For visual document understanding (VDU), self-supervised pretraining has been
shown to successfully generate transferable representations, yet, effective
adaptation of such representations to distribution shifts at test-time remains
to be an unexplored area. We propose DocTTA, a novel test-time adaptation
method for documents, that does source-free domain adaptation using unlabeled
target document data. DocTTA leverages cross-modality self-supervised learning
via masked visual language modeling, as well as pseudo labeling to adapt models
learned on a \textit{source} domain to an unlabeled \textit{target} domain at
test time. We introduce new benchmarks using existing public datasets for
various VDU tasks, including entity recognition, key-value extraction, and
document visual question answering. DocTTA shows significant improvements on
these compared to the source model performance, up to 1.89\% in (F1 score),
3.43\% (F1 score), and 17.68\% (ANLS score), respectively. Our benchmark
datasets are available at \url{https://saynaebrahimi.github.io/DocTTA.html}.",None,-1
1e611b82-3626-4a9b-b5df-523995b5ded7,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,0.376032,"While both extractive and generative readers have been successfully applied
to the Question Answering (QA) task, little attention has been paid toward the
systematic comparison of them. Characterizing the strengths and weaknesses of
the two readers is crucial not only for making a more informed reader selection
in practice but also for developing a deeper understanding to foster further
research on improving readers in a principled manner. Motivated by this goal,
we make the first attempt to systematically study the comparison of extractive
and generative readers for question answering. To be aligned with the
state-of-the-art, we explore nine transformer-based large pre-trained language
models (PrLMs) as backbone architectures. Furthermore, we organize our findings
under two main categories: (1) keeping the architecture invariant, and (2)
varying the underlying PrLMs. Among several interesting findings, it is
important to highlight that (1) the generative readers perform better in long
context QA, (2) the extractive readers perform better in short context while
also showing better out-of-domain generalization, and (3) the encoder of
encoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and
outperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also
study the effect of multi-task learning on the two types of readers varying the
underlying PrLMs and perform qualitative and quantitative diagnosis to provide
further insights into future directions in modeling better readers.",None,14059
090bf06b-a613-48b9-ab1b-e99e404bf6ff,ALLSH: Active Learning Guided by Local Sensitivity and Hardness,0.704206,"Active learning, which effectively collects informative unlabeled data for
annotation, reduces the demand for labeled data. In this work, we propose to
retrieve unlabeled samples with a local sensitivity and hardness-aware
acquisition function. The proposed method generates data copies through local
perturbations and selects data points whose predictive likelihoods diverge the
most from their copies. We further empower our acquisition function by
injecting the select-worst case perturbation. Our method achieves consistent
gains over the commonly used active learning strategies in various
classification tasks. Furthermore, we observe consistent improvements over the
baselines on the study of prompt selection in prompt-based few-shot learning.
These experiments demonstrate that our acquisition guided by local sensitivity
and hardness can be effective and beneficial for many NLP tasks.",https://github.com/szhang42/allsh,19975
708a4ea8-967c-4ba0-8569-24e4d0ec8eb3,GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks,0.438227,"Time series synthesis is an important research topic in the field of deep
learning, which can be used for data augmentation. Time series data types can
be broadly classified into regular or irregular. However, there are no existing
generative models that show good performance for both types without any model
changes. Therefore, we present a general purpose model capable of synthesizing
regular and irregular time series data. To our knowledge, we are the first
designing a general purpose time series synthesis model, which is one of the
most challenging settings for time series synthesis. To this end, we design a
generative adversarial network-based method, where many related techniques are
carefully integrated into a single framework, ranging from neural
ordinary/controlled differential equations to continuous time-flow processes.
Our method outperforms all existing methods.",None,-1
a0e22556-8296-4293-8c29-5c78e12f983b,4DenoiseNet: Adverse Weather Denoising from Adjacent Point Clouds,0.902202,"Reliable point cloud data is essential for perception tasks \textit{e.g.} in
robotics and autonomous driving applications. Adverse weather causes a specific
type of noise to light detection and ranging (LiDAR) sensor data, which
degrades the quality of the point clouds significantly. To address this issue,
this letter presents a novel point cloud adverse weather denoising deep
learning algorithm (4DenoiseNet). Our algorithm takes advantage of the time
dimension unlike deep learning adverse weather denoising methods in the
literature. It performs about 10\% better in terms of intersection over union
metric compared to the previous work and is more computationally efficient.
These results are achieved on our novel SnowyKITTI dataset, which has over
40000 adverse weather annotated point clouds. Moreover, strong qualitative
results on the Canadian Adverse Driving Conditions dataset indicate good
generalizability to domain shifts and to different sensor intrinsics.",https://github.com/alvariseppanen/4DenoiseNet,-1
61df4f62-58b1-47b7-92a8-5ae048d1c15f,PromptBoosting: Black-Box Text Classification with Ten Forward Passes,0.715876,"We describe PromptBoosting, a query-efficient procedure for building a text
classifier from a neural language model (LM) without access to the LM's
parameters, gradients, or hidden representations. This form of ""black-box""
classifier training has become increasingly important as the cost of training
and inference in large-scale LMs grows. But existing black-box LM classifier
learning approaches are themselves computationally inefficient, typically
specializing LMs to the target task by searching in a large space of (discrete
or continuous) prompts using zeroth-order optimization methods. Instead of
directly optimizing in prompt space, PromptBoosting obtains a small pool of
prompts via a gradient-free approach and then constructs a large pool of weak
learners by pairing these prompts with different elements of the LM's output
distribution. These weak learners are then ensembled using the AdaBoost
algorithm. The entire learning process requires only a small number of forward
passes and no backward pass. Experiments show that PromptBoosting achieves
state-of-the-art performance in multiple black-box few-shot classification
tasks, and matches or outperforms full fine-tuning in both few-shot and
standard learning paradigms, while training 10x faster than existing black-box
methods.",https://github.com/UCSB-NLP-Chang/PromptBoosting,-1
b0354b72-bc7c-4c45-ac3a-d320c19a1d3f,Combining Static and Contextualised Multilingual Embeddings,0.162167,"Static and contextual multilingual embeddings have complementary strengths.
Static embeddings, while less expressive than contextual language models, can
be more straightforwardly aligned across multiple languages. We combine the
strengths of static and contextual models to improve multilingual
representations. We extract static embeddings for 40 languages from XLM-R,
validate those embeddings with cross-lingual word retrieval, and then align
them using VecMap. This results in high-quality, highly multilingual static
embeddings. Then we apply a novel continued pre-training approach to XLM-R,
leveraging the high quality alignment of our static embeddings to better align
the representation space of XLM-R. We show positive results for multiple
complex semantic tasks. We release the static embeddings and the continued
pre-training code. Unlike most previous work, our continued pre-training
approach does not require parallel text.",https://github.com/KathyHaem/combining-static-contextual,-1
33fa5ba3-7337-4c2e-b23f-3a7b21313257,Neural PCA for Flow-Based Representation Learning,0.0239131,"Of particular interest is to discover useful representations solely from
observations in an unsupervised generative manner. However, the question of
whether existing normalizing flows provide effective representations for
downstream tasks remains mostly unanswered despite their strong ability for
sample generation and density estimation. This paper investigates this problem
for such a family of generative models that admits exact invertibility. We
propose Neural Principal Component Analysis (Neural-PCA) that operates in full
dimensionality while capturing principal components in \emph{descending} order.
Without exploiting any label information, the principal components recovered
store the most informative elements in their \emph{leading} dimensions and
leave the negligible in the \emph{trailing} ones, allowing for clear
performance improvements of $5\%$-$10\%$ in downstream tasks. Such improvements
are empirically found consistent irrespective of the number of latent trailing
dimensions dropped. Our work suggests that necessary inductive bias be
introduced into generative modelling when representation quality is of
interest.",https://github.com/MathsShen/Neural-PCA,-1
9d37b7e3-62a9-437b-bb4a-0766a78402cf,A Song of (Dis)agreement: Evaluating the Evaluation of Explainable Artificial Intelligence in Natural Language Processing,0.570345,"There has been significant debate in the NLP community about whether or not
attention weights can be used as an explanation - a mechanism for interpreting
how important each input token is for a particular prediction. The validity of
""attention as explanation"" has so far been evaluated by computing the rank
correlation between attention-based explanations and existing feature
attribution explanations using LSTM-based models. In our work, we (i) compare
the rank correlation between five more recent feature attribution methods and
two attention-based methods, on two types of NLP tasks, and (ii) extend this
analysis to also include transformer-based models. We find that attention-based
explanations do not correlate strongly with any recent feature attribution
methods, regardless of the model or task. Furthermore, we find that none of the
tested explanations correlate strongly with one another for the
transformer-based model, leading us to question the underlying assumption that
we should measure the validity of attention-based explanations based on how
well they correlate with existing feature attribution explanation methods.
After conducting experiments on five datasets using two different models, we
argue that the community should stop using rank correlation as an evaluation
metric for attention-based explanations. We suggest that researchers and
practitioners should instead test various explanation methods and employ a
human-in-the-loop process to determine if the explanations align with human
intuition for the particular use case at hand.",None,-1
0c731477-d9ef-49ec-b510-7c7dcb520446,ViWOZ: A Multi-Domain Task-Oriented Dialogue Systems Dataset For Low-resource Language,0.10532,"Most of the current task-oriented dialogue systems (ToD), despite having
interesting results, are designed for a handful of languages like Chinese and
English. Therefore, their performance in low-resource languages is still a
significant problem due to the absence of a standard dataset and evaluation
policy. To address this problem, we proposed ViWOZ, a fully-annotated
Vietnamese task-oriented dialogue dataset. ViWOZ is the first multi-turn,
multi-domain tasked oriented dataset in Vietnamese, a low-resource language.
The dataset consists of a total of 5,000 dialogues, including 60,946 fully
annotated utterances. Furthermore, we provide a comprehensive benchmark of both
modular and end-to-end models in low-resource language scenarios. With those
characteristics, the ViWOZ dataset enables future studies on creating a
multilingual task-oriented dialogue system.",None,897
0bc973f9-79cc-4f59-96ae-5e31e878bb3d,Adapters for Enhanced Modeling of Multilingual Knowledge and Text,0.692198,"Large language models appear to learn facts from the large text corpora they
are trained on. Such facts are encoded implicitly within their many parameters,
making it difficult to verify or manipulate what knowledge has been learned.
Language models have recently been extended to multilingual language models
(MLLMs), enabling knowledge to be learned across hundreds of languages.
Meanwhile, knowledge graphs contain facts in an explicit triple format, which
require careful and costly curation and are only available in a few
high-resource languages, restricting their research and application. To address
these issues, we propose to enhance MLLMs with knowledge from multilingual
knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks
across many languages, including low-resource ones. Specifically, we introduce
a lightweight adapter set to enhance MLLMs with cross-lingual entity alignment
and facts from MLKGs for many languages. Experiments on common benchmarks show
that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable
or improved performance for knowledge graph completion and entity alignment
relative to baselines, especially for low-resource languages (for which
knowledge graphs are unavailable); and (2) improved MLLM performance on
language understanding tasks that require multilingual factual knowledge; all
while maintaining performance on other general language tasks.",https://github.com/yifan-h/Multilingual_Space,-1
1a063aff-3e43-44c9-8561-fc3fd5557407,Neural Cloth Simulation,0.887304,"We present a general framework for the garment animation problem through
unsupervised deep learning inspired in physically based simulation. Existing
trends in the literature already explore this possibility. Nonetheless, these
approaches do not handle cloth dynamics. Here, we propose the first methodology
able to learn realistic cloth dynamics unsupervisedly, and henceforth, a
general formulation for neural cloth simulation. The key to achieve this is to
adapt an existing optimization scheme for motion from simulation based
methodologies to deep learning. Then, analyzing the nature of the problem, we
devise an architecture able to automatically disentangle static and dynamic
cloth subspaces by design. We will show how this improves model performance.
Additionally, this opens the possibility of a novel motion augmentation
technique that greatly improves generalization. Finally, we show it also allows
to control the level of motion in the predictions. This is a useful, never seen
before, tool for artists. We provide of detailed analysis of the problem to
establish the bases of neural cloth simulation and guide future research into
the specifics of this domain.",None,16698
5a9c748d-ed35-4a79-9658-5dda5387c173,Commonsense and Named Entity Aware Knowledge Grounded Dialogue Generation,0.133075,"Grounding dialogue on external knowledge and interpreting linguistic patterns
in dialogue history context, such as ellipsis, anaphora, and co-references is
critical for dialogue comprehension and generation. In this paper, we present a
novel open-domain dialogue generation model which effectively utilizes the
large-scale commonsense and named entity based knowledge in addition to the
unstructured topic-specific knowledge associated with each utterance. We
enhance the commonsense knowledge with named entity-aware structures using
co-references. Our proposed model utilizes a multi-hop attention layer to
preserve the most accurate and critical parts of the dialogue history and the
associated knowledge. In addition, we employ a Commonsense and Named Entity
Enhanced Attention Module, which starts with the extracted triples from various
sources and gradually finds the relevant supporting set of triples using
multi-hop attention with the query vector obtained from the interactive
dialogue-knowledge module. Empirical results on two benchmark dataset
demonstrate that our model significantly outperforms the state-of-the-art
methods in terms of both automatic evaluation metrics and human judgment. Our
code is publicly available at
\href{https://github.com/deekshaVarshney/CNTF}{https://github.com/deekshaVarshney/CNTF};
\href{https://www.iitp.ac.in/~ai-nlp-ml/resources/codes/CNTF.zip}{https://www.iitp.ac.in/-ai-nlp-ml/resources/
codes/CNTF.zip}.",https://github.com/deekshaVarshney/CNTF,-1
80ebb927-64cb-4b63-85ae-c99c93b40e1f,Meta Self-Refinement for Robust Learning with Weak Supervision,0.316988,"Training deep neural networks (DNNs) under weak supervision has attracted
increasing research attention as it can significantly reduce the annotation
cost. However, labels from weak supervision can be noisy, and the high capacity
of DNNs enables them to easily overfit the label noise, resulting in poor
generalization. Recent methods leverage self-training to build noise-resistant
models, in which a teacher trained under weak supervision is used to provide
highly confident labels for teaching the students. Nevertheless, the teacher
derived from such frameworks may have fitted a substantial amount of noise and
therefore produce incorrect pseudo-labels with high confidence, leading to
severe error propagation. In this work, we propose Meta Self-Refinement (MSR),
a noise-resistant learning framework, to effectively combat label noise from
weak supervision. Instead of relying on a fixed teacher trained with noisy
labels, we encourage the teacher to refine its pseudo-labels. At each training
step, MSR performs a meta gradient descent on the current mini-batch to
maximize the student performance on a clean validation set. Extensive
experimentation on eight NLP benchmarks demonstrates that MSR is robust against
label noise in all settings and outperforms state-of-the-art methods by up to
11.4% in accuracy and 9.26% in F1 score.",https://github.com/uds-lsv/msr,-1
03235b73-3694-4246-aafb-9951db96f641,One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones,0.499185,"We study the problem of developing autonomous agents that can follow human
instructions to infer and perform a sequence of actions to complete the
underlying task. Significant progress has been made in recent years, especially
for tasks with short horizons. However, when it comes to long-horizon tasks
with extended sequences of actions, an agent can easily ignore some
instructions or get stuck in the middle of the long instructions and eventually
fail the task. To address this challenge, we propose a model-agnostic
milestone-based task tracker (M-TRACK) to guide the agent and monitor its
progress. Specifically, we propose a milestone builder that tags the
instructions with navigation and interaction milestones which the agent needs
to complete step by step, and a milestone checker that systemically checks the
agent's progress in its current milestone and determines when to proceed to the
next. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and
52% relative improvement in unseen success rate over two competitive base
models.",https://github.com/chanhee-luke/M-Track,8346
a098d8d7-1e7b-43f1-8506-39f219ba609b,INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold,0.218143,"Binary Neural Networks (BNNs) have emerged as a promising solution for
reducing the memory footprint and compute costs of deep neural networks, but
they suffer from quality degradation due to the lack of freedom as activations
and weights are constrained to the binary values. To compensate for the
accuracy drop, we propose a novel BNN design called Binary Neural Network with
INSTAnce-aware threshold (INSTA-BNN), which controls the quantization threshold
dynamically in an input-dependent or instance-aware manner. According to our
observation, higher-order statistics can be a representative metric to estimate
the characteristics of the input distribution. INSTA-BNN is designed to adjust
the threshold dynamically considering various information, including
higher-order statistics, but it is also optimized judiciously to realize
minimal overhead on a real device. Our extensive study shows that INSTA-BNN
outperforms the baseline by 3.0% and 2.8% on the ImageNet classification task
with comparable computing cost, achieving 68.5% and 72.2% top-1 accuracy on
ResNet-18 and MobileNetV1 based models, respectively.",None,-1
9c603c1c-f09b-450d-8cff-3bcf5e28b18a,Contextual road lane and symbol generation for autonomous driving,0.025488,"In this paper we present a novel approach for lane detection and segmentation
using generative models. Traditionally discriminative models have been employed
to classify pixels semantically on a road. We model the probability
distribution of lanes and road symbols by training a generative adversarial
network. Based on the learned probability distribution, context-aware lanes and
road signs are generated for a given image which are further quantized for
nearest class label. Proposed method has been tested on BDD100K and Baidu
ApolloScape datasets and performs better than state of the art and exhibits
robustness to adverse conditions by generating lanes in faded out and occluded
scenarios.",None,-1
a15aea73-7736-4f97-8908-1f3be6f3ff35,The GatedTabTransformer. An enhanced deep learning architecture for tabular modeling,0.421026,"There is an increasing interest in the application of deep learning
architectures to tabular data. One of the state-of-the-art solutions is
TabTransformer which incorporates an attention mechanism to better track
relationships between categorical features and then makes use of a standard MLP
to output its final logits. In this paper we propose multiple modifications to
the original TabTransformer performing better on binary classification tasks
for three separate datasets with more than 1% AUROC gains. Inspired by gated
MLP, linear projections are implemented in the MLP block and multiple
activation functions are tested. We also evaluate the importance of specific
hyper parameters during training.",https://github.com/radi-cho/GatedTabTransformer,-1
48975078-7932-43e5-9454-7bf6466b0230,Federated Learning with Position-Aware Neurons,0.657481,"Federated Learning (FL) fuses collaborative models from local nodes without
centralizing users' data. The permutation invariance property of neural
networks and the non-i.i.d. data across clients make the locally updated
parameters imprecisely aligned, disabling the coordinate-based parameter
averaging. Traditional neurons do not explicitly consider position information.
Hence, we propose Position-Aware Neurons (PANs) as an alternative, fusing
position-related values (i.e., position encodings) into neuron outputs. PANs
couple themselves to their positions and minimize the possibility of
dislocation, even updating on heterogeneous data. We turn on/off PANs to
disable/enable the permutation invariance property of neural networks. PANs are
tightly coupled with positions when applied to FL, making parameters across
clients pre-aligned and facilitating coordinate-based parameter averaging. PANs
are algorithm-agnostic and could universally improve existing FL algorithms.
Furthermore, ""FL with PANs"" is simple to implement and computationally
friendly.",https://github.com/IBM/FedMA,-1
b50e3a46-79f3-45f5-9bf7-cbeb687a5e92,Monte Carlo Tree Search based Variable Selection for High Dimensional Bayesian Optimization,0.806896,"Bayesian optimization (BO) is a class of popular methods for expensive
black-box optimization, and has been widely applied to many scenarios. However,
BO suffers from the curse of dimensionality, and scaling it to high-dimensional
problems is still a challenge. In this paper, we propose a variable selection
method MCTS-VS based on Monte Carlo tree search (MCTS), to iteratively select
and optimize a subset of variables. That is, MCTS-VS constructs a
low-dimensional subspace via MCTS and optimizes in the subspace with any BO
algorithm. We give a theoretical analysis of the general variable selection
method to reveal how it can work. Experiments on high-dimensional synthetic
functions and real-world problems (i.e., NAS-bench problems and MuJoCo
locomotion tasks) show that MCTS-VS equipped with a proper BO optimizer can
achieve state-of-the-art performance.",https://github.com/lamda-bbo/MCTS-VS,-1
832c1732-0950-493a-a613-560eacbedbac,FlowFormer: A Transformer Architecture for Optical Flow,0.999999,"We introduce optical Flow transFormer, dubbed as FlowFormer, a
transformer-based neural network architecture for learning optical flow.
FlowFormer tokenizes the 4D cost volume built from an image pair, encodes the
cost tokens into a cost memory with alternate-group transformer (AGT) layers in
a novel latent space, and decodes the cost memory via a recurrent transformer
decoder with dynamic positional cost queries. On the Sintel benchmark,
FlowFormer achieves 1.159 and 2.088 average end-point-error (AEPE) on the clean
and final pass, a 16.5% and 15.5% error reduction from the best published
result (1.388 and 2.47). Besides, FlowFormer also achieves strong
generalization performance. Without being trained on Sintel, FlowFormer
achieves 1.01 AEPE on the clean pass of Sintel training set, outperforming the
best published result (1.29) by 21.7%.",None,-1
6fa824cd-5df0-4494-9b21-21d6e64682cf,Augmenting Knowledge Graphs for Better Link Prediction,0.374277,"Embedding methods have demonstrated robust performance on the task of link
prediction in knowledge graphs, by mostly encoding entity relationships. Recent
methods propose to enhance the loss function with a literal-aware term. In this
paper, we propose KGA: a knowledge graph augmentation method that incorporates
literals in an embedding model without modifying its loss function. KGA
discretizes quantity and year values into bins, and chains these bins both
horizontally, modeling neighboring values, and vertically, modeling multiple
levels of granularity. KGA is scalable and can be used as a pre-processing step
for any existing knowledge graph embedding model. Experiments on legacy
benchmarks and a new large benchmark, DWD, show that augmenting the knowledge
graph with quantities and years is beneficial for predicting both entities and
numbers, as KGA outperforms the vanilla models and other relevant baselines.
Our ablation studies confirm that both quantities and years contribute to KGA's
performance, and that its performance depends on the discretization and binning
settings. We make the code, models, and the DWD benchmark publicly available to
facilitate reproducibility and future research.",https://github.com/Otamio/KGA/,-1
37470a62-4931-4322-bb3d-f5a7ed2b0fcd,Differentiable Point-Based Radiance Fields for Efficient View Synthesis,0.556855,"We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.",None,97412
382554ca-51f2-4325-ba7e-850c55488aac,Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations,0.102238,"Due to the huge amount of parameters, fine-tuning of pretrained language
models (PLMs) is prone to overfitting in the low resource scenarios. In this
work, we present a novel method that operates on the hidden representations of
a PLM to reduce overfitting. During fine-tuning, our method inserts random
autoencoders between the hidden layers of a PLM, which transform activations
from the previous layers into multi-view compressed representations before
feeding them into the upper layers. The autoencoders are plugged out after
fine-tuning, so our method does not add extra parameters or increase
computation cost during inference. Our method demonstrates promising
performance improvement across a wide range of sequence- and token-level
low-resource NLP tasks.",https://github.com/DAMO-NLP-SG/MVCR,11730
8a1a53da-9c6f-40ff-90a3-84bc16f7d458,Heterogeneous-Agent Mirror Learning: A Continuum of Solutions to Cooperative MARL,0.674645,"The necessity for cooperation among intelligent machines has popularised
cooperative multi-agent reinforcement learning (MARL) in the artificial
intelligence (AI) research community. However, many research endeavors have
been focused on developing practical MARL algorithms whose effectiveness has
been studied only empirically, thereby lacking theoretical guarantees. As
recent studies have revealed, MARL methods often achieve performance that is
unstable in terms of reward monotonicity or suboptimal at convergence. To
resolve these issues, in this paper, we introduce a novel framework named
Heterogeneous-Agent Mirror Learning (HAML) that provides a general template for
MARL algorithmic designs. We prove that algorithms derived from the HAML
template satisfy the desired properties of the monotonic improvement of the
joint reward and the convergence to Nash equilibrium. We verify the
practicality of HAML by proving that the current state-of-the-art cooperative
MARL algorithms, HATRPO and HAPPO, are in fact HAML instances. Next, as a
natural outcome of our theory, we propose HAML extensions of two well-known RL
algorithms, HAA2C (for A2C) and HADDPG (for DDPG), and demonstrate their
effectiveness against strong baselines on StarCraftII and Multi-Agent MuJoCo
tasks.",https://github.com/anonymouswater/HAML,-1
072b8d31-1ef5-452c-bf33-1106c019f994,Multi-View Dreaming: Multi-View World Model with Contrastive Learning,0.298509,"In this paper, we propose Multi-View Dreaming, a novel reinforcement learning
agent for integrated recognition and control from multi-view observations by
extending Dreaming. Most current reinforcement learning method assumes a
single-view observation space, and this imposes limitations on the observed
data, such as lack of spatial information and occlusions. This makes obtaining
ideal observational information from the environment difficult and is a
bottleneck for real-world robotics applications. In this paper, we use
contrastive learning to train a shared latent space between different
viewpoints, and show how the Products of Experts approach can be used to
integrate and control the probability distributions of latent states for
multiple viewpoints. We also propose Multi-View DreamingV2, a variant of
Multi-View Dreaming that uses a categorical distribution to model the latent
state instead of the Gaussian distribution. Experiments show that the proposed
method outperforms simple extensions of existing methods in a realistic robot
control task.",None,3802
77b19317-aa5d-449d-9655-aaa6ab8fee09,A Variational Hierarchical Model for Neural Cross-Lingual Summarization,0.895683,"The goal of the cross-lingual summarization (CLS) is to convert a document in
one language (e.g., English) to a summary in another one (e.g., Chinese).
Essentially, the CLS task is the combination of machine translation (MT) and
monolingual summarization (MS), and thus there exists the hierarchical
relationship between MT\&MS and CLS. Existing studies on CLS mainly focus on
utilizing pipeline methods or jointly training an end-to-end model through an
auxiliary MT or MS objective. However, it is very challenging for the model to
directly conduct CLS as it requires both the abilities to translate and
summarize. To address this issue, we propose a hierarchical model for the CLS
task, based on the conditional variational auto-encoder. The hierarchical model
contains two kinds of latent variables at the local and global levels,
respectively. At the local level, there are two latent variables, one for
translation and the other for summarization. As for the global level, there is
another latent variable for cross-lingual summarization conditioned on the two
local-level variables. Experiments on two language directions (English-Chinese)
verify the effectiveness and superiority of the proposed approach. In addition,
we show that our model is able to generate better cross-lingual summaries than
comparison models in the few-shot setting.",https://github.com/XL2248/VHM,-1
41a0bc08-5dbc-493d-b558-0fdc9c56ce57,Multifaceted Improvements for Conversational Open-Domain Question Answering,0.187122,"Open-domain question answering (OpenQA) is an important branch of textual QA
which discovers answers for the given questions based on a large number of
unstructured documents. Effectively mining correct answers from the open-domain
sources still has a fair way to go. Existing OpenQA systems might suffer from
the issues of question complexity and ambiguity, as well as insufficient
background knowledge. Recently, conversational OpenQA is proposed to address
these issues with the abundant contextual information in the conversation.
Promising as it might be, there exist several fundamental limitations including
the inaccurate question understanding, the coarse ranking for passage
selection, and the inconsistent usage of golden passage in the training and
inference phases. To alleviate these limitations, in this paper, we propose a
framework with Multifaceted Improvements for Conversational open-domain
Question Answering (MICQA). Specifically, MICQA has three significant
advantages. First, the proposed KL-divergence based regularization is able to
lead to a better question understanding for retrieval and answer reading.
Second, the added post-ranker module can push more relevant passages to the top
placements and be selected for reader with a two-aspect constrains. Third, the
well designed curriculum learning strategy effectively narrows the gap between
the golden passage settings of training and inference, and encourages the
reader to find true answer without the golden passage assistance. Extensive
experiments conducted on the publicly available dataset OR-QuAC demonstrate the
superiority of MICQA over the state-of-the-art model in conversational OpenQA
task.",None,-1
0c8367df-1bd1-4d07-ab6b-b5c2d5e4cc7a,Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization,0.856856,"Neural abstractive summarization models are prone to generate summaries which
are factually inconsistent with their source documents. Previous work has
introduced the task of recognizing such factual inconsistency as a downstream
application of natural language inference (NLI). However, state-of-the-art NLI
models perform poorly in this context due to their inability to generalize to
the target task. In this work, we show that NLI models can be effective for
this task when the training data is augmented with high-quality task-oriented
examples. We introduce Falsesum, a data generation pipeline leveraging a
controllable text generation model to perturb human-annotated summaries,
introducing varying types of factual inconsistencies. Unlike previously
introduced document-level NLI datasets, our generated dataset contains examples
that are diverse and inconsistent yet plausible. We show that models trained on
a Falsesum-augmented NLI dataset improve the state-of-the-art performance
across four benchmarks for detecting factual inconsistency in summarization.
  The code to obtain the dataset is available online at
https://github.com/joshbambrick/Falsesum",https://github.com/joshbambrick/Falsesum,-1
f8e207f2-9965-424d-9792-35789e5f73ab,BITE: Textual Backdoor Attacks with Iterative Trigger Injection,0.831134,"Backdoor attacks have become an emerging threat to NLP systems. By providing
poisoned training data, the adversary can embed a ""backdoor"" into the victim
model, which allows input instances satisfying certain textual patterns (e.g.,
containing a keyword) to be predicted as a target label of the adversary's
choice. In this paper, we demonstrate that it is possible to design a backdoor
attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a
high attack success rate). We propose BITE, a backdoor attack that poisons the
training data to establish strong correlations between the target label and a
set of ""trigger words"". These trigger words are iteratively identified and
injected into the target-label instances through natural word-level
perturbations. The poisoned training data instruct the victim model to predict
the target label on inputs containing trigger words, forming the backdoor.
Experiments on four text classification datasets show that our proposed attack
is significantly more effective than baseline methods while maintaining decent
stealthiness, raising alarm on the usage of untrusted training data. We further
propose a defense method named DeBITE based on potential trigger word removal,
which outperforms existing methods in defending against BITE and generalizes
well to handling other backdoor attacks.",https://github.com/INK-USC/BITE,-1
2f216707-34a6-4269-880e-a47f40989d0e,Outpainting by Queries,0.393231,"Image outpainting, which is well studied with Convolution Neural Network
(CNN) based framework, has recently drawn more attention in computer vision.
However, CNNs rely on inherent inductive biases to achieve effective sample
learning, which may degrade the performance ceiling. In this paper, motivated
by the flexible self-attention mechanism with minimal inductive biases in
transformer architecture, we reframe the generalised image outpainting problem
as a patch-wise sequence-to-sequence autoregression problem, enabling
query-based image outpainting. Specifically, we propose a novel hybrid
vision-transformer-based encoder-decoder framework, named \textbf{Query}
\textbf{O}utpainting \textbf{TR}ansformer (\textbf{QueryOTR}), for
extrapolating visual context all-side around a given image. Patch-wise mode's
global modeling capacity allows us to extrapolate images from the attention
mechanism's query standpoint. A novel Query Expansion Module (QEM) is designed
to integrate information from the predicted queries based on the encoder's
output, hence accelerating the convergence of the pure transformer even with a
relatively small dataset. To further enhance connectivity between each patch,
the proposed Patch Smoothing Module (PSM) re-allocates and averages the
overlapped regions, thus providing seamless predicted images. We experimentally
show that QueryOTR could generate visually appealing results smoothly and
realistically against the state-of-the-art image outpainting approaches.",https://github.com/Kaiseem/QueryOTR,-1
d26cbcb3-685a-42d9-bc2e-0e9a6f7cbe80,Analyzing Gender Representation in Multilingual Models,0.125003,"Multilingual language models were shown to allow for nontrivial transfer
across scripts and languages. In this work, we study the structure of the
internal representations that enable this transfer. We focus on the
representation of gender distinctions as a practical case study, and examine
the extent to which the gender concept is encoded in shared subspaces across
different languages. Our analysis shows that gender representations consist of
several prominent components that are shared across languages, alongside
language-specific components. The existence of language-independent and
language-specific components provides an explanation for an intriguing
empirical observation we make: while gender classification transfers well
across languages, interventions for gender removal, trained on a single
language, do not transfer easily to others.",https://github.com/gonenhila/multilingual_gender,-1
e50ada6d-f65a-4011-bc6f-97ac0aff2a73,TextHacker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack,0.285864,"Existing textual adversarial attacks usually utilize the gradient or
prediction confidence to generate adversarial examples, making it hard to be
deployed in real-world applications. To this end, we consider a rarely
investigated but more rigorous setting, namely hard-label attack, in which the
attacker can only access the prediction label. In particular, we find we can
learn the importance of different words via the change on prediction label
caused by word substitutions on the adversarial examples. Based on this
observation, we propose a novel adversarial attack, termed Text Hard-label
attacker (TextHacker). TextHacker randomly perturbs lots of words to craft an
adversarial example. Then, TextHacker adopts a hybrid local search algorithm
with the estimation of word importance from the attack history to minimize the
adversarial perturbation. Extensive evaluations for text classification and
textual entailment show that TextHacker significantly outperforms existing
hard-label attacks regarding the attack performance as well as adversary
quality.",https://github.com/JHL-HUST/TextHacker,-1
5a41a3f5-3389-4e80-83a0-a9f360e7fc3c,Language Model Pre-training on True Negatives,0.0645596,"Discriminative pre-trained language models (PLMs) learn to predict original
texts from intentionally corrupted ones. Taking the former text as positive and
the latter as negative samples, the PLM can be trained effectively for
contextualized representation. However, the training of such a type of PLMs
highly relies on the quality of the automatically constructed samples. Existing
PLMs simply treat all corrupted texts as equal negative without any
examination, which actually lets the resulting model inevitably suffer from the
false negative issue where training is carried out on pseudo-negative data and
leads to less efficiency and less robustness in the resulting PLMs. In this
work, on the basis of defining the false negative issue in discriminative PLMs
that has been ignored for a long time, we design enhanced pre-training methods
to counteract false negative predictions and encourage pre-training language
models on true negatives by correcting the harmful gradient updates subject to
false negative predictions. Experimental results on GLUE and SQuAD benchmarks
show that our counter-false-negative pre-training methods indeed bring about
better performance together with stronger robustness.",None,-1
021d632f-798e-46a5-9990-5eaeba93ad97,Representation Uncertainty in Self-Supervised Learning as Variational Inference,0.146583,"In this study, a novel self-supervised learning (SSL) method is proposed,
which considers SSL in terms of variational inference to learn not only
representation but also representation uncertainties. SSL is a method of
learning representations without labels by maximizing the similarity between
image representations of different augmented views of an image. Meanwhile,
variational autoencoder (VAE) is an unsupervised representation learning method
that trains a probabilistic generative model with variational inference. Both
VAE and SSL can learn representations without labels, but their relationship
has not been investigated in the past. Herein, the theoretical relationship
between SSL and variational inference has been clarified. Furthermore, a novel
method, namely variational inference SimSiam (VI-SimSiam), has been proposed.
VI-SimSiam can predict the representation uncertainty by interpreting SimSiam
with variational inference and defining the latent space distribution. The
present experiments qualitatively show that VI- SimSiam could learn uncertainty
by comparing input images and predicted uncertainties. Additionally, we
described a relationship between estimated uncertainty and classification
accuracy.",None,-1
058bb486-62c8-44f9-98bc-90cafae8eee0,LittleBird: Efficient Faster & Longer Transformer for Question Answering,0.105141,"BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a
limitation dealing with long inputs due to its attention mechanism. Longformer,
ETC and BigBird addressed this issue and effectively solved the quadratic
dependency problem. However we find that these models are not sufficient, and
propose LittleBird, a novel model based on BigBird with improved speed and
memory footprint while maintaining accuracy. In particular, we devise a more
flexible and efficient position representation method based on Attention with
Linear Biases (ALiBi). We also show that replacing the method of global
information represented in the BigBird with pack and unpack attention is more
effective. The proposed model can work on long inputs even after being
pre-trained on short inputs, and can be trained efficiently reusing existing
pre-trained language model for short inputs. This is a significant benefit for
low-resource languages where large amounts of long text data are difficult to
obtain. As a result, our experiments show that LittleBird works very well in a
variety of languages, achieving high performance in question answering tasks,
particularly in KorQuAD2.0, Korean Question Answering Dataset for long
paragraphs.",https://github.com/monologg/KoELECTRA,1342
6ba07fcf-611a-4551-bd77-0d092e5aabdf,Mix and Localize: Localizing Sound Sources in Mixtures,0.753976,"We present a method for simultaneously localizing multiple sound sources
within a visual scene. This task requires a model to both group a sound mixture
into individual sources, and to associate them with a visual signal. Our method
jointly solves both tasks at once, using a formulation inspired by the
contrastive random walk of Jabri et al. We create a graph in which images and
separated sounds correspond to nodes, and train a random walker to transition
between nodes from different modalities with high return probability. The
transition probabilities for this walk are determined by an audio-visual
similarity metric that is learned by our model. We show through experiments
with musical instruments and human speech that our model can successfully
localize multiple sounds, outperforming other self-supervised methods. Project
site: https://hxixixh.github.io/mix-and-localize",https://hxixixh.github.io/mix-and-localize,-1
9d446686-eb21-4152-981f-00ef028b3361,General Incremental Learning with Domain-aware Categorical Representations,0.63111,"Continual learning is an important problem for achieving human-level
intelligence in real-world applications as an agent must continuously
accumulate knowledge in response to streaming data/tasks. In this work, we
consider a general and yet under-explored incremental learning problem in which
both the class distribution and class-specific domain distribution change over
time. In addition to the typical challenges in class incremental learning, this
setting also faces the intra-class stability-plasticity dilemma and intra-class
domain imbalance problems. To address above issues, we develop a novel
domain-aware continual learning method based on the EM framework. Specifically,
we introduce a flexible class representation based on the von Mises-Fisher
mixture model to capture the intra-class structure, using an
expansion-and-reduction strategy to dynamically increase the number of
components according to the class complexity. Moreover, we design a bi-level
balanced memory to cope with data imbalances within and across classes, which
combines with a distillation loss to achieve better inter- and intra-class
stability-plasticity trade-off. We conduct exhaustive experiments on three
benchmarks: iDigits, iDomainNet and iCIFAR-20. The results show that our
approach consistently outperforms previous methods by a significant margin,
demonstrating its superiority.",None,-1
4a582e62-05b0-43a2-93e7-86199c2aa459,Estimating Social Influence from Observational Data,0.172184,"We consider the problem of estimating social influence, the effect that a
person's behavior has on the future behavior of their peers. The key challenge
is that shared behavior between friends could be equally explained by influence
or by two other confounding factors: 1) latent traits that caused people to
both become friends and engage in the behavior, and 2) latent preferences for
the behavior. This paper addresses the challenges of estimating social
influence with three contributions. First, we formalize social influence as a
causal effect, one which requires inferences about hypothetical interventions.
Second, we develop Poisson Influence Factorization (PIF), a method for
estimating social influence from observational data. PIF fits probabilistic
factor models to networks and behavior data to infer variables that serve as
substitutes for the confounding latent traits. Third, we develop assumptions
under which PIF recovers estimates of social influence. We empirically study
PIF with semi-synthetic and real data from Last.fm, and conduct a sensitivity
analysis. We find that PIF estimates social influence most accurately compared
to related methods and remains robust under some violations of its assumptions.",https://github.com/blei-lab/poisson-influence-factorization,-1
6932aa1a-5ff3-4113-abd8-c577ca893adc,Monant Medical Misinformation Dataset: Mapping Articles to Fact-Checked Claims,0.604618,"False information has a significant negative influence on individuals as well
as on the whole society. Especially in the current COVID-19 era, we witness an
unprecedented growth of medical misinformation. To help tackle this problem
with machine learning approaches, we are publishing a feature-rich dataset of
approx. 317k medical news articles/blogs and 3.5k fact-checked claims. It also
contains 573 manually and more than 51k automatically labelled mappings between
claims and articles. Mappings consist of claim presence, i.e., whether a claim
is contained in a given article, and article stance towards the claim. We
provide several baselines for these two tasks and evaluate them on the manually
labelled part of the dataset. The dataset enables a number of additional tasks
related to medical misinformation, such as misinformation characterisation
studies or studies of misinformation diffusion between sources.",None,-1
41402635-1f84-4c20-8ff5-110c0f7e881c,Whodunit? Learning to Contrast for Authorship Attribution,0.773398,"Authorship attribution is the task of identifying the author of a given text.
The key is finding representations that can differentiate between authors.
Existing approaches typically use manually designed features that capture a
dataset's content and style, but these approaches are dataset-dependent and
yield inconsistent performance across corpora. In this work, we propose
\textit{learning} author-specific representations by fine-tuning pre-trained
generic language representations with a contrastive objective (Contra-X). We
show that Contra-X learns representations that form highly separable clusters
for different authors. It advances the state-of-the-art on multiple human and
machine authorship attribution benchmarks, enabling improvements of up to 6.8%
over cross-entropy fine-tuning. However, we find that Contra-X improves overall
accuracy at the cost of sacrificing performance for some authors. Resolving
this tension will be an important direction for future work. To the best of our
knowledge, we are the first to integrate contrastive learning with pre-trained
language model fine-tuning for authorship attribution.",https://github.com/BoAi01/Contra-X.git,-1
0127ef36-d1d9-4074-a68e-c29c1630dc4f,Globally Optimal Event-Based Divergence Estimation for Ventral Landing,0.661295,"Event sensing is a major component in bio-inspired flight guidance and
control systems. We explore the usage of event cameras for predicting
time-to-contact (TTC) with the surface during ventral landing. This is achieved
by estimating divergence (inverse TTC), which is the rate of radial optic flow,
from the event stream generated during landing. Our core contributions are a
novel contrast maximisation formulation for event-based divergence estimation,
and a branch-and-bound algorithm to exactly maximise contrast and find the
optimal divergence value. GPU acceleration is conducted to speed up the global
algorithm. Another contribution is a new dataset containing real event streams
from ventral landing that was employed to test and benchmark our method. Owing
to global optimisation, our algorithm is much more capable at recovering the
true divergence, compared to other heuristic divergence estimators or
event-based optic flow methods. With GPU acceleration, our method also achieves
competitive runtimes.",https://github.com/s-mcleod/ventral-landing-event-dataset,-1
a0759b15-1960-4570-89ab-9ddb0e8a3bb1,BESS: Balanced Entity Sampling and Sharing for Large-Scale Knowledge Graph Completion,0.205606,"We present the award-winning submission to the WikiKG90Mv2 track of
OGB-LSC@NeurIPS 2022. The task is link-prediction on the large-scale knowledge
graph WikiKG90Mv2, consisting of 90M+ nodes and 600M+ edges. Our solution uses
a diverse ensemble of $85$ Knowledge Graph Embedding models combining five
different scoring functions (TransE, TransH, RotatE, DistMult, ComplEx) and two
different loss functions (log-sigmoid, sampled softmax cross-entropy). Each
individual model is trained in parallel on a Graphcore Bow Pod$_{16}$ using
BESS (Balanced Entity Sampling and Sharing), a new distribution framework for
KGE training and inference based on balanced collective communications between
workers. Our final model achieves a validation MRR of 0.2922 and a
test-challenge MRR of 0.2562, winning the first place in the competition. The
code is publicly available at:
https://github.com/graphcore/distributed-kge-poplar/tree/2022-ogb-submission.",https://github.com/graphcore/distributed-kge-poplar/tree/,-1
312a1d15-a25d-4297-a80f-bee0f0973762,Sustainability using Renewable Electricity (SuRE) towards NetZero Emissions,0.0707859,"Demand for energy has increased significantly across the globe due to
increase in population and economic growth. Growth in energy demand poses
serious threat to the environment since majority of the energy sources are
non-renewable and based on fossil fuels, which leads to emission of harmful
greenhouse gases. Organizations across the world are facing challenges in
transitioning from fossil fuels-based sources to greener sources to reduce
their carbon footprint. As a step towards achieving Net-Zero emission target,
we present a scalable AI based solution that can be used by organizations to
increase their overall renewable electricity share in total energy consumption.
Our solution provides facilities with accurate energy demand forecast,
recommendation for procurement of renewable electricity to optimize cost and
carbon offset recommendations to compensate for Greenhouse Gas (GHG) emissions.
This solution has been used in production for more than a year for four
facilities and has increased their renewable electricity share significantly.",None,-1
c58a91c5-c706-43b8-81b6-8ea9e65e525d,HYRR: Hybrid Infused Reranking for Passage Retrieval,0.168563,"We present Hybrid Infused Reranking for Passages Retrieval (HYRR), a
framework for training rerankers based on a hybrid of BM25 and neural retrieval
models. Retrievers based on hybrid models have been shown to outperform both
BM25 and neural models alone. Our approach exploits this improved performance
when training a reranker, leading to a robust reranking model. The reranker, a
cross-attention neural model, is shown to be robust to different first-stage
retrieval systems, achieving better performance than rerankers simply trained
upon the first-stage retrievers in the multi-stage systems. We present
evaluations on a supervised passage retrieval task using MS MARCO and zero-shot
retrieval tasks using BEIR. The empirical results show strong performance on
both evaluations.",https://github.com/usnistgov/trec_eval,-1
f1b8d700-4910-451e-ac12-d37d1cfed43c,AAM-Gym: Artificial Intelligence Testbed for Advanced Air Mobility,0.616116,"We introduce AAM-Gym, a research and development testbed for Advanced Air
Mobility (AAM). AAM has the potential to revolutionize travel by reducing
ground traffic and emissions by leveraging new types of aircraft such as
electric vertical take-off and landing (eVTOL) aircraft and new advanced
artificial intelligence (AI) algorithms. Validation of AI algorithms require
representative AAM scenarios, as well as a fast time simulation testbed to
evaluate their performance. Until now, there has been no such testbed available
for AAM to enable a common research platform for individuals in government,
industry, or academia. MIT Lincoln Laboratory has developed AAM-Gym to address
this gap by providing an ecosystem to develop, train, and validate new and
established AI algorithms across a wide variety of AAM use-cases. In this
paper, we use AAM-Gym to study the performance of two reinforcement learning
algorithms on an AAM use-case, separation assurance in AAM corridors. The
performance of the two algorithms is demonstrated based on a series of metrics
provided by AAM-Gym, showing the testbed's utility to AAM research.",None,-1
c88cf6a4-93c3-4750-ae5e-7ecbc51de368,Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization,0.775762,"Neural architecture search (NAS) and network pruning are widely studied
efficient AI techniques, but not yet perfect. NAS performs exhaustive candidate
architecture search, incurring tremendous search cost. Though (structured)
pruning can simply shrink model dimension, it remains unclear how to decide the
per-layer sparsity automatically and optimally. In this work, we revisit the
problem of layer-width optimization and propose Pruning-as-Search (PaS), an
end-to-end channel pruning method to search out desired sub-network
automatically and efficiently. Specifically, we add a depth-wise binary
convolution to learn pruning policies directly through gradient descent. By
combining the structural reparameterization and PaS, we successfully searched
out a new family of VGG-like and lightweight networks, which enable the
flexibility of arbitrary width with respect to each layer instead of each
stage. Experimental results show that our proposed architecture outperforms
prior arts by around $1.0\%$ top-1 accuracy under similar inference speed on
ImageNet-1000 classification task. Furthermore, we demonstrate the
effectiveness of our width search on complex tasks including instance
segmentation and image translation. Code and models are released.",None,-1
3fefd5a0-9095-414d-a62b-c3fd6718ef28,Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks,0.999016,"The wide adoption and application of Masked language models~(MLMs) on
sensitive data (from legal to medical) necessitates a thorough quantitative
investigation into their privacy vulnerabilities -- to what extent do MLMs leak
information about their training data? Prior attempts at measuring leakage of
MLMs via membership inference attacks have been inconclusive, implying the
potential robustness of MLMs to privacy attacks. In this work, we posit that
prior attempts were inconclusive because they based their attack solely on the
MLM's model score. We devise a stronger membership inference attack based on
likelihood ratio hypothesis testing that involves an additional reference MLM
to more accurately quantify the privacy risks of memorization in MLMs. We show
that masked language models are extremely susceptible to likelihood ratio
membership inference attacks: Our empirical results, on models trained on
medical notes, show that our attack improves the AUC of prior membership
inference attacks from 0.66 to an alarmingly high 0.90 level, with a
significant improvement in the low-error region: at 1% false positive rate, our
attack is 51X more powerful than prior work.",None,-1
022a6860-1935-4e98-a364-950b78015431,In-Vehicle Interface Adaptation to Environment-Induced Cognitive Workload,0.521885,"Many car accidents are caused by human distractions, including cognitive
distractions. In-vehicle human-machine interfaces (HMIs) have evolved
throughout the years, providing more and more functions. Interaction with the
HMIs can, however, also lead to further distractions and, as a consequence,
accidents. To tackle this problem, we propose using adaptive HMIs that change
according to the mental workload of the driver. In this work, we present the
current status as well as preliminary results of a user study using
naturalistic secondary tasks while driving (i.e., the primary task) that
attempt to understand the effects of one such interface.",None,67
5c70ccf9-0f01-45b7-a9eb-edd7b7fd55d2,Context-Dependent Anomaly Detection with Knowledge Graph Embedding Models,0.126169,"Increasing the semantic understanding and contextual awareness of machine
learning models is important for improving robustness and reducing
susceptibility to data shifts. In this work, we leverage contextual awareness
for the anomaly detection problem. Although graphed-based anomaly detection has
been widely studied, context-dependent anomaly detection is an open problem and
without much current research. We develop a general framework for converting a
context-dependent anomaly detection problem to a link prediction problem,
allowing well-established techniques from this domain to be applied. We
implement a system based on our framework that utilizes knowledge graph
embedding models and demonstrates the ability to detect outliers using context
provided by a semantic knowledge base. We show that our method can detect
context-dependent anomalies with a high degree of accuracy and show that
current object detectors can detect enough classes to provide the needed
context for good performance within our example domain.",None,-1
c5953aed-f94c-4a94-a8e9-a8340d744af8,ELLE: Efficient Lifelong Pre-training for Emerging Data,0.478303,"Current pre-trained language models (PLM) are typically trained with static
data, ignoring that in real-world scenarios, streaming data of various sources
may continuously grow. This requires PLMs to integrate the information from all
the sources in a lifelong manner. Although this goal could be achieved by
exhaustive pre-training on all the existing data, such a process is known to be
computationally expensive. To this end, we propose ELLE, aiming at efficient
lifelong pre-training for emerging data. Specifically, ELLE consists of (1)
function preserved model expansion, which flexibly expands an existing PLM's
width and depth to improve the efficiency of knowledge acquisition; and (2)
pre-trained domain prompts, which disentangle the versatile knowledge learned
during pre-training and stimulate the proper knowledge for downstream tasks. We
experiment ELLE with streaming data from 5 domains on BERT and GPT. The results
show the superiority of ELLE over various lifelong learning baselines in both
pre-training efficiency and downstream performances. The codes are publicly
available at https://github.com/thunlp/ELLE.",https://github.com/thunlp/ELLE,-1
7729be9f-2c8a-4e29-b7e0-867b826f9d68,Attention-based Random Forest and Contamination Model,0.512115,"A new approach called ABRF (the attention-based random forest) and its
modifications for applying the attention mechanism to the random forest (RF)
for regression and classification are proposed. The main idea behind the
proposed ABRF models is to assign attention weights with trainable parameters
to decision trees in a specific way. The weights depend on the distance between
an instance, which falls into a corresponding leaf of a tree, and instances,
which fall in the same leaf. This idea stems from representation of the
Nadaraya-Watson kernel regression in the form of a RF. Three modifications of
the general approach are proposed. The first one is based on applying the
Huber's contamination model and on computing the attention weights by solving
quadratic or linear optimization problems. The second and the third
modifications use the gradient-based algorithms for computing trainable
parameters. Numerical experiments with various regression and classification
datasets illustrate the proposed method.",None,-1
a7243b5d-e31e-40b4-bb85-ff30db019190,CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI,0.5966,"Human language expression is based on the subjective construal of the
situation instead of the objective truth conditions, which means that speakers'
personalities and emotions after cognitive processing have an important
influence on conversation. However, most existing datasets for conversational
AI ignore human personalities and emotions, or only consider part of them. It's
difficult for dialogue systems to understand speakers' personalities and
emotions although large-scale pre-training language models have been widely
used. In order to consider both personalities and emotions in the process of
conversation generation, we propose CPED, a large-scale Chinese personalized
and emotional dialogue dataset, which consists of multi-source knowledge
related to empathy and personal characteristic. These knowledge covers gender,
Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPED
contains more than 12K dialogues of 392 speakers from 40 TV shows. We release
the textual dataset with audio features and video features according to the
copyright claims, privacy issues, terms of service of video platforms. We
provide detailed description of the CPED construction process and introduce
three tasks for conversational AI, including personality recognition, emotion
recognition in conversations as well as personalized and emotional conversation
generation. Finally, we provide baseline systems for these tasks and consider
the function of speakers' personalities and emotions on conversation. Our
motivation is to propose a dataset to be widely adopted by the NLP community as
a new open benchmark for conversational AI research. The full dataset is
available at https://github.com/scutcyr/CPED.",None,-1
43a4882c-2830-484e-aa2d-ffb46151eae4,Optimizing Multiple Simultaneous Objectives for Voting and Facility Location,0.230901,"We study the classic facility location setting, where we are given $n$
clients and $m$ possible facility locations in some arbitrary metric space, and
want to choose a location to build a facility. The exact same setting also
arises in spatial social choice, where voters are the clients and the goal is
to choose a candidate or outcome, with the distance from a voter to an outcome
representing the cost of this outcome for the voter (e.g., based on their
ideological differences). Unlike most previous work, we do not focus on a
single objective to optimize (e.g., the total distance from clients to the
facility, or the maximum distance, etc.), but instead attempt to optimize
several different objectives simultaneously. More specifically, we consider the
$l$-centrum family of objectives, which includes the total distance, max
distance, and many others. We present tight bounds on how well any pair of such
objectives (e.g., max and sum) can be simultaneously approximated compared to
their optimum outcomes. In particular, we show that for any such pair of
objectives, it is always possible to choose an outcome which simultaneously
approximates both objectives within a factor of $1+\sqrt{2}$, and give a
precise characterization of how this factor improves as the two objectives
being optimized become more similar. For $q>2$ different centrum objectives, we
show that it is always possible to approximate all $q$ of these objectives
within a small constant, and that this constant approaches 3 as $q\rightarrow
\infty$. Our results show that when optimizing only a few simultaneous
objectives, it is always possible to form an outcome which is a significantly
better than 3 approximation for all of these objectives.",None,-1
b0f45728-7b08-48d3-84bc-76e0bce257f6,Auction-Based Ex-Post-Payment Incentive Mechanism Design for Horizontal Federated Learning with Reputation and Contribution Measurement,0.337991,"Federated learning trains models across devices with distributed data, while
protecting the privacy and obtaining a model similar to that of centralized ML.
A large number of workers with data and computing power are the foundation of
federal learning. However, the inevitable costs prevent self-interested workers
from serving for free. Moreover, due to data isolation, task publishers lack
effective methods to select, evaluate and pay reliable workers with
high-quality data. Therefore, we design an auction-based incentive mechanism
for horizontal federated learning with reputation and contribution measurement.
By designing a reasonable method of measuring contribution, we establish the
reputation of workers, which is easy to decline and difficult to improve.
Through reverse auctions, workers bid for tasks, and the task publisher selects
workers combining reputation and bid price. With the budget constraint, winning
workers are paid based on performance. We proved that our mechanism satisfies
the individual rationality of the honest worker, budget feasibility,
truthfulness, and computational efficiency.",None,-1
dfd1f2eb-7166-4676-8668-5ace01a3b18b,RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization,0.870943,"6-DoF object pose estimation from a monocular image is challenging, and a
post-refinement procedure is generally needed for high-precision estimation. In
this paper, we propose a framework based on a recurrent neural network (RNN)
for object pose refinement, which is robust to erroneous initial poses and
occlusions. During the recurrent iterations, object pose refinement is
formulated as a non-linear least squares problem based on the estimated
correspondence field (between a rendered image and the observed image). The
problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm
enabling end-to-end training. The correspondence field estimation and pose
refinement are conducted alternatively in each iteration to recover the object
poses. Furthermore, to improve the robustness to occlusion, we introduce a
consistency-check mechanism based on the learned descriptors of the 3D model
and observed 2D images, which downweights the unreliable correspondences during
pose optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and
YCB-Video datasets validate the effectiveness of our method and demonstrate
state-of-the-art performance.",https://github.com/DecaYale/RNNPose,-1
837cd79e-9603-4bc0-9919-601329515093,Towards True Detail Restoration for Super-Resolution: A Benchmark and a Quality Metric,0.0809544,"Super-resolution (SR) has become a widely researched topic in recent years.
SR methods can improve overall image and video quality and create new
possibilities for further content analysis. But the SR mainstream focuses
primarily on increasing the naturalness of the resulting image despite
potentially losing context accuracy. Such methods may produce an incorrect
digit, character, face, or other structural object even though they otherwise
yield good visual quality. Incorrect detail restoration can cause errors when
detecting and identifying objects both manually and automatically. To analyze
the detail-restoration capabilities of image and video SR models, we developed
a benchmark based on our own video dataset, which contains complex patterns
that SR models generally fail to correctly restore. We assessed 32 recent SR
models using our benchmark and compared their ability to preserve scene
context. We also conducted a crowd-sourced comparison of restored details and
developed an objective assessment metric that outperforms other quality metrics
by correlation with subjective scores for this task. In conclusion, we provide
a deep analysis of benchmark results that yields insights for future SR-based
work.",None,-1
1588b47a-2f59-4941-8dbd-24a261641e3d,The Stack: 3 TB of permissively licensed source code,0.999445,"Large Language Models (LLMs) play an ever-increasing role in the field of
Artificial Intelligence (AI)--not only for natural language processing but also
for code understanding and generation. To stimulate open and responsible
research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting
of permissively licensed source code in 30 programming languages. We describe
how we collect the full dataset, construct a permissively licensed subset,
present a data governance plan, discuss limitations, and show promising results
on text2code benchmarks by training 350M-parameter decoders on different Python
subsets. We find that (1) near-deduplicating the data significantly boosts
performance across all experiments, and (2) it is possible to match previously
reported HumanEval and MBPP performance using only permissively licensed data.
We make the dataset available at https://hf.co/BigCode, provide a tool called
""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers
to search The Stack for copies of their code, and provide a process for code to
be removed from the dataset by following the instructions at
https://www.bigcode-project.org/docs/about/the-stack/.",None,84607
9d0bd43b-f0f0-4390-9585-2f2415782389,Towards Proactive Information Retrieval in Noisy Text with Wikipedia Concepts,0.178575,"Extracting useful information from the user history to clearly understand
informational needs is a crucial feature of a proactive information retrieval
system. Regarding understanding information and relevance, Wikipedia can
provide the background knowledge that an intelligent system needs. This work
explores how exploiting the context of a query using Wikipedia concepts can
improve proactive information retrieval on noisy text. We formulate two models
that use entity linking to associate Wikipedia topics with the relevance model.
Our experiments around a podcast segment retrieval task demonstrate that there
is a clear signal of relevance in Wikipedia concepts while a ranking model can
improve precision by incorporating them. We also find Wikifying the background
context of a query can help disambiguate the meaning of the query, further
helping proactive information retrieval.",None,-1
a07f3277-3d8d-4f89-8a42-5e33a20bd122,Explainable Verbal Deception Detection using Transformers,0.064352,"People are regularly confronted with potentially deceptive statements (e.g.,
fake news, misleading product reviews, or lies about activities). Only few
works on automated text-based deception detection have exploited the potential
of deep learning approaches. A critique of deep-learning methods is their lack
of interpretability, preventing us from understanding the underlying
(linguistic) mechanisms involved in deception. However, recent advancements
have made it possible to explain some aspects of such models. This paper
proposes and evaluates six deep-learning models, including combinations of BERT
(and RoBERTa), MultiHead Attention, co-attentions, and transformers. To
understand how the models reach their decisions, we then examine the model's
predictions with LIME. We then zoom in on vocabulary uniqueness and the
correlation of LIWC categories with the outcome class (truthful vs deceptive).
The findings suggest that our transformer-based models can enhance automated
deception detection performances (+2.11% in accuracy) and show significant
differences pertinent to the usage of LIWC features in truthful and deceptive
statements.",None,-1
3043e978-6b85-438e-b6ae-13eb3c2a1a06,Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting,0.898452,"We study the knowledge extrapolation problem to embed new components (i.e.,
entities and relations) that come with emerging knowledge graphs (KGs) in the
federated setting. In this problem, a model trained on an existing KG needs to
embed an emerging KG with unseen entities and relations. To solve this problem,
we introduce the meta-learning setting, where a set of tasks are sampled on the
existing KG to mimic the link prediction task on the emerging KG. Based on
sampled tasks, we meta-train a graph neural network framework that can
construct features for unseen components based on structural information and
output embeddings for them. Experimental results show that our proposed method
can effectively embed unseen components and outperforms models that consider
inductive settings for KGs and baselines that directly use conventional KG
embedding methods.",https://github.com/zjukg/MaKEr,-1
c376065d-39ac-406b-af7b-8e4eb058954d,Global memory transformer for processing long documents,0.281728,"Transformer variants dominate the state-of-the-art in different natural
language processing tasks such as translation, reading comprehension and
summarization. Our paper is more directed to use general memory slots added to
the inputs and studying the results of adding these slots. This paper is a go
on study of general memory slots rule that were added to the input of the
proposed model in previous work. We have two main tasks;1) pretraining task
using masked language modeling and b) fine tuning task using HotpotQA . This
study aims to verify the ability of the proposed model to handle chunks as if
they were one chunk comparing with the base model. As baseline we used T5
transformer. We studied the rule of memory slots augmented to each input chunk
and studied the model performance without selector. We found that adding memory
to input chunks helped the proposed model to overcome the baseline on Masked
language modeling task with specific training parameters. Ablation study
reveals the ability of using the compressed input chunks with a degradation in
performance.",https://github.com/huggingface/transformers/tree/main/examples/ax/language-modeling#train-tokenizer-2,-1
d7dc4cee-a35e-4e50-9404-837288959b8c,Zero-Shot On-the-Fly Event Schema Induction,0.810376,"What are the events involved in a pandemic outbreak? What steps should be
taken when planning a wedding? The answers to these questions can be found by
collecting many documents on the complex event of interest, extracting relevant
information, and analyzing it. We present a new approach in which large
language models are utilized to generate source documents that allow
predicting, given a high-level event definition, the specific events,
arguments, and relations between them to construct a schema that describes the
complex event in its entirety. Using our model, complete schemas on any topic
can be generated on-the-fly without any manual data collection, i.e., in a
zero-shot manner. Moreover, we develop efficient methods to extract pertinent
information from texts and demonstrate in a series of experiments that these
schemas are considered to be more complete than human-curated ones in the
majority of examined scenarios. Finally, we show that this framework is
comparable in performance with previous supervised schema induction methods
that rely on collecting real texts while being more general and flexible
without the need for a predefined ontology.",None,-1
276b3014-008e-42e4-b689-b1f3b1abdf9c,"Interventions, Where and How? Experimental Design for Causal Models at Scale",0.738378,"Causal discovery from observational and interventional data is challenging
due to limited data and non-identifiability: factors that introduce uncertainty
in estimating the underlying structural causal model (SCM). Selecting
experiments (interventions) based on the uncertainty arising from both factors
can expedite the identification of the SCM. Existing methods in experimental
design for causal discovery from limited data either rely on linear assumptions
for the SCM or select only the intervention target. This work incorporates
recent advances in Bayesian causal discovery into the Bayesian optimal
experimental design framework, allowing for active causal discovery of large,
nonlinear SCMs while selecting both the interventional target and the value. We
demonstrate the performance of the proposed method on synthetic graphs
(Erdos-R\`enyi, Scale Free) for both linear and nonlinear SCMs as well as on
the \emph{in-silico} single-cell gene regulatory network dataset, DREAM.",https://github.com/yannadani/cbed,-1
0fe3c23a-b5ac-495d-a2f6-55f9c5916fbf,Mixture of Experts for Biomedical Question Answering,0.165402,"Biomedical Question Answering (BQA) has attracted increasing attention in
recent years due to its promising application prospect. It is a challenging
task because the biomedical questions are professional and usually vary widely.
Existing question answering methods answer all questions with a homogeneous
model, leading to various types of questions competing for the shared
parameters, which will confuse the model decision for each single type of
questions. In this paper, in order to alleviate the parameter competition
problem, we propose a Mixture-of-Expert (MoE) based question answering method
called MoEBQA that decouples the computation for different types of questions
by sparse routing. To be specific, we split a pretrained Transformer model into
bottom and top blocks. The bottom blocks are shared by all the examples, aiming
to capture the general features. The top blocks are extended to an MoE version
that consists of a series of independent experts, where each example is
assigned to a few experts according to its underlying question type. MoEBQA
automatically learns the routing strategy in an end-to-end manner so that each
expert tends to deal with the question types it is expert in. We evaluate
MoEBQA on three BQA datasets constructed based on real examinations. The
results show that our MoE extension significantly boosts the performance of
question answering models and achieves new state-of-the-art performance. In
addition, we elaborately analyze our MoE modules to reveal how MoEBQA works and
find that it can automatically group the questions into human-readable
clusters.",https://github.com/huggingface/transformers,-1
160f158f-6b53-40b1-9524-4a88a1aedfdf,Unsupervised Syntactically Controlled Paraphrase Generation with Abstract Meaning Representations,0.293455,"Syntactically controlled paraphrase generation has become an emerging
research direction in recent years. Most existing approaches require annotated
paraphrase pairs for training and are thus costly to extend to new domains.
Unsupervised approaches, on the other hand, do not need paraphrase pairs but
suffer from relatively poor performance in terms of syntactic control and
quality of generated paraphrases. In this paper, we demonstrate that leveraging
Abstract Meaning Representations (AMR) can greatly improve the performance of
unsupervised syntactically controlled paraphrase generation. Our proposed
model, AMR-enhanced Paraphrase Generator (AMRPG), separately encodes the AMR
graph and the constituency parse of the input sentence into two disentangled
semantic and syntactic embeddings. A decoder is then learned to reconstruct the
input sentence from the semantic and syntactic embeddings. Our experiments show
that AMRPG generates more accurate syntactically controlled paraphrases, both
quantitatively and qualitatively, compared to the existing unsupervised
approaches. We also demonstrate that the paraphrases generated by AMRPG can be
used for data augmentation to improve the robustness of NLP models.",https://github.com/bjascob/amrlib-models,34559
04e67efb-b515-46cd-b589-73c4f399f698,InstaFormer: Instance-Aware Image-to-Image Translation with Transformer,0.885141,"We present a novel Transformer-based network architecture for instance-aware
image-to-image translation, dubbed InstaFormer, to effectively integrate
global- and instance-level information. By considering extracted content
features from an image as tokens, our networks discover global consensus of
content features by considering context information through a self-attention
module in Transformers. By augmenting such tokens with an instance-level
feature extracted from the content feature with respect to bounding box
information, our framework is capable of learning an interaction between object
instances and the global image, thus boosting the instance-awareness. We
replace layer normalization (LayerNorm) in standard Transformers with adaptive
instance normalization (AdaIN) to enable a multi-modal translation with style
codes. In addition, to improve the instance-awareness and translation quality
at object regions, we present an instance-level content contrastive loss
defined between input and translated image. We conduct experiments to
demonstrate the effectiveness of our InstaFormer over the latest methods and
provide extensive ablation studies.",None,-1
3e5fba76-67b1-421f-b491-6d7e7e1f2adb,Prompt Tuning for Discriminative Pre-trained Language Models,0.229335,"Recent works have shown promising results of prompt tuning in stimulating
pre-trained language models (PLMs) for natural language processing (NLP) tasks.
However, to the best of our knowledge, existing works focus on prompt-tuning
generative PLMs that are pre-trained to generate target tokens, such as BERT.
It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be
effectively prompt-tuned. In this work, we present DPT, the first prompt tuning
framework for discriminative PLMs, which reformulates NLP tasks into a
discriminative language modeling problem. Comprehensive experiments on text
classification and question answering show that, compared with vanilla
fine-tuning, DPT achieves significantly higher performance, and also prevents
the unstable problem in tuning large PLMs in both full-set and low-resource
settings. The source code and experiment details of this paper can be obtained
from https://github.com/thunlp/DPT.",https://github.com/thunlp/DPT,-1
20592e5f-796c-47f7-a035-317780214428,Unified Line and Paragraph Detection by Graph Convolutional Networks,0.240863,"We formulate the task of detecting lines and paragraphs in a document into a
unified two-level clustering problem. Given a set of text detection boxes that
roughly correspond to words, a text line is a cluster of boxes and a paragraph
is a cluster of lines. These clusters form a two-level tree that represents a
major part of the layout of a document. We use a graph convolutional network to
predict the relations between text detection boxes and then build both levels
of clusters from these predictions. Experimentally, we demonstrate that the
unified approach can be highly efficient while still achieving state-of-the-art
quality for detecting paragraphs in public benchmarks and real-world images.",None,1989
9b9d2764-d22b-4691-809f-4f97400070a8,IDANI: Inference-time Domain Adaptation via Neuron-level Interventions,0.459322,"Large pre-trained models are usually fine-tuned on downstream task data, and
tested on unseen data. When the train and test data come from different
domains, the model is likely to struggle, as it is not adapted to the test
domain. We propose a new approach for domain adaptation (DA), using
neuron-level interventions: We modify the representation of each test example
in specific neurons, resulting in a counterfactual example from the source
domain, which the model is more familiar with. The modified example is then fed
back into the model. While most other DA methods are applied during training
time, ours is applied during inference only, making it more efficient and
applicable. Our experiments show that our method improves performance on unseen
domains.",https://github.com/technion-cs-nlp/idani,-1
136a093b-1157-496a-9cad-0359f31c5868,RetroGraph: Retrosynthetic Planning with Graph Search,0.504831,"Retrosynthetic planning, which aims to find a reaction pathway to synthesize
a target molecule, plays an important role in chemistry and drug discovery.
This task is usually modeled as a search problem. Recently, data-driven methods
have attracted many research interests and shown promising results for
retrosynthetic planning. We observe that the same intermediate molecules are
visited many times in the searching process, and they are usually independently
treated in previous tree-based methods (e.g., AND-OR tree search, Monte Carlo
tree search). Such redundancies make the search process inefficient. We propose
a graph-based search policy that eliminates the redundant explorations of any
intermediate molecules. As searching over a graph is more complicated than over
a tree, we further adopt a graph neural network to guide the search over
graphs. Meanwhile, our method can search a batch of targets together in the
graph and remove the inter-target duplication in the tree-based search methods.
Experimental results on two datasets demonstrate the effectiveness of our
method. Especially on the widely used USPTO benchmark, we improve the search
success rate to 99.47%, advancing previous state-of-the-art performance for 2.6
points.",https://github.com/binghong-ml/retro_star,-1
65da5259-8a6f-405f-b147-aeb413990d1a,Can Language Models perform Abductive Commonsense Reasoning?,0.0240894,"Abductive Reasoning is a task of inferring the most plausible hypothesis
given a set of observations. In literature, the community has approached to
solve this challenge by classifying/generating a likely hypothesis that does
not contradict with a past observation and future observation. Some of the most
well-known benchmarks that tackle this problem are aNLI and aNLG (pronounced as
alpha-NLI and alpha-NLG). In this report, I review over some of the
methodologies that were attempted to solve this challenge, re-implement the
baseline models, and analyze some of the weaknesses that current approaches
have. The code and the re-implemented results are available at this link.",https://github.com/SeungoneKim/abductive-commonsense-reasoning,-1
3220037e-f413-4571-9c73-eb82e62fbc75,Coarse-to-Fine Sparse Sequential Recommendation,0.44761,"Sequential recommendation aims to model dynamic user behavior from historical
interactions. Self-attentive methods have proven effective at capturing
short-term dynamics and long-term preferences. Despite their success, these
approaches still struggle to model sparse data, on which they struggle to learn
high-quality item representations. We propose to model user dynamics from
shopping intents and interacted items simultaneously. The learned intents are
coarse-grained and work as prior knowledge for item recommendation. To this
end, we present a coarse-to-fine self-attention framework, namely CaFe, which
explicitly learns coarse-grained and fine-grained sequential dynamics.
Specifically, CaFe first learns intents from coarse-grained sequences which are
dense and hence provide high-quality user intent representations. Then, CaFe
fuses intent representations into item encoder outputs to obtain improved item
representations. Finally, we infer recommended items based on representations
of items and corresponding intents. Experiments on sparse datasets show that
CaFe outperforms state-of-the-art self-attentive recommenders by 44.03% NDCG@5
on average.",None,104016
7fe23e5f-6af7-4872-a224-ead91756ce26,Solving ImageNet: a Unified Scheme for Training any Backbone to Top Results,0.305368,"ImageNet serves as the primary dataset for evaluating the quality of
computer-vision models. The common practice today is training each architecture
with a tailor-made scheme, designed and tuned by an expert. In this paper, we
present a unified scheme for training any backbone on ImageNet. The scheme,
named USI (Unified Scheme for ImageNet), is based on knowledge distillation and
modern tricks. It requires no adjustments or hyper-parameters tuning between
different models, and is efficient in terms of training times. We test USI on a
wide variety of architectures, including CNNs, Transformers, Mobile-oriented
and MLP-only. On all models tested, USI outperforms previous state-of-the-art
results. Hence, we are able to transform training on ImageNet from an
expert-oriented task to an automatic seamless routine. Since USI accepts any
backbone and trains it to top results, it also enables to perform methodical
comparisons, and identify the most efficient backbones along the speed-accuracy
Pareto curve. Implementation is available
at:https://github.com/Alibaba-MIIL/Solving_ImageNet",https://github.com/Alibaba-MIIL/Solving_ImageNet,-1
4d829261-5939-4ccc-bcdb-e81cfed282e6,Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation,0.589449,"Monocular 3D human pose estimation technologies have the potential to greatly
increase the availability of human movement data. The best-performing models
for single-image 2D-3D lifting use graph convolutional networks (GCNs) that
typically require some manual input to define the relationships between
different body joints. We propose a novel transformer-based approach that uses
the more generalised self-attention mechanism to learn these relationships
within a sequence of tokens representing joints. We find that the use of
intermediate supervision, as well as residual connections between the stacked
encoders benefits performance. We also suggest that using error prediction as
part of a multi-task learning framework improves performance by allowing the
network to compensate for its confidence level. We perform extensive ablation
studies to show that each of our contributions increases performance.
Furthermore, we show that our approach outperforms the recent state of the art
for single-frame 3D human pose estimation by a large margin. Our code and
trained models are made publicly available on Github.",None,-1
cbab0309-5199-49c8-9f26-18e37c2e959f,"Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks",0.260569,"The research of open-domain dialog systems has been greatly prospered by
neural models trained on large-scale corpora, however, such corpora often
introduce various safety problems (e.g., offensive languages, biases, and toxic
behaviors) that significantly hinder the deployment of dialog systems in
practice. Among all these unsafe issues, addressing social bias is more complex
as its negative impact on marginalized populations is usually expressed
implicitly, thus requiring normative reasoning and rigorous analysis. In this
paper, we focus our investigation on social bias detection of dialog safety
problems. We first propose a novel Dial-Bias Frame for analyzing the social
bias in conversations pragmatically, which considers more comprehensive
bias-related analyses rather than simple dichotomy annotations. Based on the
proposed framework, we further introduce CDail-Bias Dataset that, to our
knowledge, is the first well-annotated Chinese social bias dialog dataset. In
addition, we establish several dialog bias detection benchmarks at different
label granularities and input types (utterance-level and context-level). We
show that the proposed in-depth analyses together with these benchmarks in our
Dial-Bias Frame are necessary and essential to bias detection tasks and can
benefit building safe dialog systems in practice.",https://github.com/para-zhou/CDial-Bias,-1
4d98b820-665e-42ef-b401-8760eaa81a63,rPPG-Toolbox: Deep Remote PPG Toolbox,0.746494,"Camera-based physiological measurement is a fast growing field of computer
vision. Remote photoplethysmography (rPPG) utilizes imaging devices (e.g.,
cameras) to measure the peripheral blood volume pulse (BVP) via
photoplethysmography, and enables cardiac measurement via webcams and
smartphones. However, the task is non-trivial with important pre-processing,
modeling, and post-processing steps required to obtain state-of-the-art
results. Replication of results and benchmarking of new models is critical for
scientific progress; however, as with many other applications of deep learning,
reliable codebases are not easy to find or use. We present a comprehensive
toolbox, rPPG-Toolbox, that contains unsupervised and supervised rPPG models
with support for public benchmark datasets, data augmentation, and systematic
evaluation: \url{https://github.com/ubicomplab/rPPG-Toolbox}",https://github.com/ubicomplab/rPPG-Toolbox,-1
ac81e294-e5d3-461b-919a-ea1f2d915dd5,Cloud-based Automatic Speech Recognition Systems for Southeast Asian Languages,0.112734,"This paper provides an overall introduction of our Automatic Speech
Recognition (ASR) systems for Southeast Asian languages. As not much existing
work has been carried out on such regional languages, a few difficulties should
be addressed before building the systems: limitation on speech and text
resources, lack of linguistic knowledge, etc. This work takes Bahasa Indonesia
and Thai as examples to illustrate the strategies of collecting various
resources required for building ASR systems.",None,-1
7a303569-e991-45e9-9d6b-2cf97fab5550,Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering,0.733156,"Retrieval augmented language models have recently become the standard for
knowledge intensive tasks. Rather than relying purely on latent semantics
within the parameters of large neural models, these methods enlist a
semi-parametric memory to encode an index of knowledge for the model to
retrieve over. Most prior work has employed text passages as the unit of
knowledge, which has high coverage at the cost of interpretability,
controllability, and efficiency. The opposite properties arise in other methods
which have instead relied on knowledge base (KB) facts. At the same time, more
recent work has demonstrated the effectiveness of storing and retrieving from
an index of Q-A pairs derived from text \citep{lewis2021paq}. This approach
yields a high coverage knowledge representation that maintains KB-like
properties due to its representations being more atomic units of information.
In this work we push this line of research further by proposing a
question-answer augmented encoder-decoder model and accompanying pretraining
strategy. This yields an end-to-end system that not only outperforms prior QA
retrieval methods on single-hop QA tasks but also enables compositional
reasoning, as demonstrated by strong performance on two multi-hop QA datasets.
Together, these methods improve the ability to interpret and control the model
while narrowing the performance gap with passage retrieval systems.",https://github.com/google-research/language/,-1
0a03d328-4fe9-4da1-afce-56e5862690ac,Controlling Extra-Textual Attributes about Dialogue Participants -- A Case Study of English-to-Polish Neural Machine Translation,0.137413,"Unlike English, morphologically rich languages can reveal characteristics of
speakers or their conversational partners, such as gender and number, via
pronouns, morphological endings of words and syntax. When translating from
English to such languages, a machine translation model needs to opt for a
certain interpretation of textual context, which may lead to serious
translation errors if extra-textual information is unavailable. We investigate
this challenge in the English-to-Polish language direction. We focus on the
underresearched problem of utilising external metadata in automatic translation
of TV dialogue, proposing a case study where a wide range of approaches for
controlling attributes in translation is employed in a multi-attribute
scenario. The best model achieves an improvement of +5.81 chrF++/+6.03 BLEU,
with other models achieving competitive performance. We additionally contribute
a novel attribute-annotated dataset of Polish TV dialogue and a morphological
analysis script used to evaluate attribute control in models.",https://github.com/st-vincent1/grammatical_agreement_eamt/,-1
644b1032-b809-43d8-995f-a9c559f742b4,Are GAN-based Morphs Threatening Face Recognition?,0.716655,"Morphing attacks are a threat to biometric systems where the biometric
reference in an identity document can be altered. This form of attack presents
an important issue in applications relying on identity documents such as border
security or access control. Research in generation of face morphs and their
detection is developing rapidly, however very few datasets with morphing
attacks and open-source detection toolkits are publicly available. This paper
bridges this gap by providing two datasets and the corresponding code for four
types of morphing attacks: two that rely on facial landmarks based on OpenCV
and FaceMorpher, and two that use StyleGAN 2 to generate synthetic morphs. We
also conduct extensive experiments to assess the vulnerability of four
state-of-the-art face recognition systems, including FaceNet, VGG-Face,
ArcFace, and ISV. Surprisingly, the experiments demonstrate that, although
visually more appealing, morphs based on StyleGAN 2 do not pose a significant
threat to the state to face recognition systems, as these morphs were
outmatched by the simple morphs that are based facial landmarks.",None,-1
28f679df-767e-4053-9ba1-4859434038c0,Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition,0.521623,"Recent studies have revealed the vulnerability of face recognition models
against physical adversarial patches, which raises security concerns about the
deployed face recognition systems. However, it is still challenging to ensure
the reproducibility for most attack algorithms under complex physical
conditions, which leads to the lack of a systematic evaluation of the existing
methods. It is therefore imperative to develop a framework that can enable a
comprehensive evaluation of the vulnerability of face recognition in the
physical world. To this end, we propose to simulate the complex transformations
of faces in the physical world via 3D-face modeling, which serves as a digital
counterpart of physical faces. The generic framework allows us to control
different face variations and physical conditions to conduct reproducible
evaluations comprehensively. With this digital simulator, we further propose a
Face3DAdv method considering the 3D face transformations and realistic physical
variations. Extensive experiments validate that Face3DAdv can significantly
improve the effectiveness of diverse physically realizable adversarial patches
in both simulated and physical environments, against various white-box and
black-box face recognition models.",None,12717
036cb429-955d-4793-96fe-4458909ae53f,Fruit Quality Assessment with Densely Connected Convolutional Neural Network,0.649101,"Accurate recognition of food items along with quality assessment is of
paramount importance in the agricultural industry. Such automated systems can
speed up the wheel of the food processing sector and save tons of manual labor.
In this connection, the recent advancement of Deep learning-based architectures
has introduced a wide variety of solutions offering remarkable performance in
several classification tasks. In this work, we have exploited the concept of
Densely Connected Convolutional Neural Networks (DenseNets) for fruit quality
assessment. The feature propagation towards the deeper layers has enabled the
network to tackle the vanishing gradient problems and ensured the reuse of
features to learn meaningful insights. Evaluating on a dataset of 19,526 images
containing six fruits having three quality grades for each, the proposed
pipeline achieved a remarkable accuracy of 99.67%. The robustness of the model
was further tested for fruit classification and quality assessment tasks where
the model produced a similar performance, which makes it suitable for real-life
applications.",None,-1
7d323792-3d0f-4b30-a3f4-c0cacfefb7bf,Modeling Multi-Granularity Hierarchical Features for Relation Extraction,0.248247,"Relation extraction is a key task in Natural Language Processing (NLP), which
aims to extract relations between entity pairs from given texts. Recently,
relation extraction (RE) has achieved remarkable progress with the development
of deep neural networks. Most existing research focuses on constructing
explicit structured features using external knowledge such as knowledge graph
and dependency tree. In this paper, we propose a novel method to extract
multi-granularity features based solely on the original input sentences. We
show that effective structured features can be attained even without external
knowledge. Three kinds of features based on the input sentences are fully
exploited, which are in entity mention level, segment level, and sentence
level. All the three are jointly and hierarchically modeled. We evaluate our
method on three public benchmarks: SemEval 2010 Task 8, Tacred, and Tacred
Revisited. To verify the effectiveness, we apply our method to different
encoders such as LSTM and BERT. Experimental results show that our method
significantly outperforms existing state-of-the-art models that even use
external knowledge. Extensive analyses demonstrate that the performance of our
model is contributed by the capture of multi-granularity features and the model
of their hierarchical structure. Code and data are available at
\url{https://github.com/xnliang98/sms}.",https://github.com/xnliang98/sms,-1
8e97948d-8bc7-4ba9-9609-e753f98cb348,Protecting President Zelenskyy against Deep Fakes,0.188399,"The 2022 Russian invasion of Ukraine is being fought on two fronts: a brutal
ground war and a duplicitous disinformation campaign designed to conceal and
justify Russia's actions. This campaign includes at least one example of a
deep-fake video purportedly showing Ukrainian President Zelenskyy admitting
defeat and surrendering. In anticipation of future attacks of this form, we
describe a facial and gestural behavioral model that captures distinctive
characteristics of Zelenskyy's speaking style. Trained on over eight hours of
authentic video from four different settings, we show that this behavioral
model can distinguish Zelenskyy from deep-fake imposters.This model can play an
important role -- particularly during the fog of war -- in distinguishing the
real from the fake.",None,-1
635e8bbc-e68f-44dc-8851-ead79b1ede30,Geolocation estimation of target vehicles using image processing and geometric computation,0.0861289,"Estimating vehicles' locations is one of the key components in intelligent
traffic management systems (ITMSs) for increasing traffic scene awareness.
Traditionally, stationary sensors have been employed in this regard. The
development of advanced sensing and communication technologies on modern
vehicles (MVs) makes it feasible to use such vehicles as mobile sensors to
estimate the traffic data of observed vehicles. This study aims to explore the
capabilities of a monocular camera mounted on an MV in order to estimate the
geolocation of the observed vehicle in a global positioning system (GPS)
coordinate system. We proposed a new methodology by integrating deep learning,
image processing, and geometric computation to address the observed-vehicle
localization problem. To evaluate our proposed methodology, we developed new
algorithms and tested them using real-world traffic data. The results indicated
that our proposed methodology and algorithms could effectively estimate the
observed vehicle's latitude and longitude dynamically.",None,-1
00fa8add-a0a5-42d2-9128-3402e170410c,DigNet: Digging Clues from Local-Global Interactive Graph for Aspect-level Sentiment Classification,0.746621,"In aspect-level sentiment classification (ASC), state-of-the-art models
encode either syntax graph or relation graph to capture the local syntactic
information or global relational information. Despite the advantages of syntax
and relation graphs, they have respective shortages which are neglected,
limiting the representation power in the graph modeling process. To resolve
their limitations, we design a novel local-global interactive graph, which
marries their advantages by stitching the two graphs via interactive edges. To
model this local-global interactive graph, we propose a novel neural network
termed DigNet, whose core module is the stacked local-global interactive (LGI)
layers performing two processes: intra-graph message passing and cross-graph
message passing. In this way, the local syntactic and global relational
information can be reconciled as a whole in understanding the aspect-level
sentiment. Concretely, we design two variants of local-global interactive
graphs with different kinds of interactive edges and three variants of LGI
layers. We conduct experiments on several public benchmark datasets and the
results show that we outperform previous best scores by 3\%, 2.32\%, and 6.33\%
in terms of Macro-F1 on Lap14, Res14, and Res15 datasets, respectively,
confirming the effectiveness and superiority of the proposed local-global
interactive graph and DigNet.",None,-1
4921e039-99ee-4f28-b9d2-0c3fe7998f3a,Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models,0.15047,"Pre-trained masked language models successfully perform few-shot learning by
formulating downstream tasks as text infilling. However, as a strong
alternative in full-shot settings, discriminative pre-trained models like
ELECTRA do not fit into the paradigm. In this work, we adapt prompt-based
few-shot learning to ELECTRA and show that it outperforms masked language
models in a wide range of tasks. ELECTRA is pre-trained to distinguish if a
token is generated or original. We naturally extend that to prompt-based
few-shot learning by training to score the originality of the target options
without introducing new parameters. Our method can be easily adapted to tasks
involving multi-token predictions without extra computation overhead. Analysis
shows that ELECTRA learns distributions that align better with downstream
tasks.",https://github.com/facebookresearch/ELECTRA-Fewshot-Learning,-1
f15fdbb4-6e4d-47fb-8657-7cb979c3c356,Too Big to Fail? Active Few-Shot Learning Guided Logic Synthesis,0.0265138,"Generating sub-optimal synthesis transformation sequences (""synthesis
recipe"") is an important problem in logic synthesis. Manually crafted synthesis
recipes have poor quality. State-of-the art machine learning (ML) works to
generate synthesis recipes do not scale to large netlists as the models need to
be trained from scratch, for which training data is collected using time
consuming synthesis runs. We propose a new approach, Bulls-Eye, that fine-tunes
a pre-trained model on past synthesis data to accurately predict the quality of
a synthesis recipe for an unseen netlist. This approach on achieves 2x-10x
run-time improvement and better quality-of-result (QoR) than state-of-the-art
machine learning approaches.",https://github.com/NYU-MLDA/OpenABC,-1
98ad6ba6-4c40-4195-9a6a-e6c3a74f4f36,Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,0.754037,"Pre-trained language models (LMs) are shown to easily generate toxic
language. In this work, we systematically explore domain-adaptive training to
reduce the toxicity of language models. We conduct this study on three
dimensions: training corpus, model size, and parameter efficiency. For the
training corpus, we propose to leverage the generative power of LMs and
generate nontoxic datasets for domain-adaptive training, which mitigates the
exposure bias and is shown to be more data-efficient than using a curated
pre-training corpus. We demonstrate that the self-generation method
consistently outperforms the existing baselines across various model sizes on
both automatic and human evaluations, even when it uses a 1/3 smaller training
corpus. We then comprehensively study detoxifying LMs with parameter sizes
ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never
been studied before. We find that i) large LMs have similar toxicity levels as
smaller ones given the same pre-training corpus, and ii) large LMs require more
endeavor to detoxify. We also explore parameter-efficient training methods for
detoxification. We demonstrate that adding and training adapter-only layers in
LMs not only saves a lot of parameters but also achieves a better trade-off
between toxicity and perplexity than whole model adaptation for the large-scale
models.",https://github.com/NVIDIA/Megatron-LM/,-1
c5a9f2f2-ba02-48ce-a9da-60d1e9e86952,ORCA: A Challenging Benchmark for Arabic Language Understanding,0.507601,"Due to their crucial role in all NLP, several benchmarks have been proposed
to evaluate pretrained language models. In spite of these efforts, no public
benchmark of diverse nature currently exists for evaluation of Arabic. This
makes it challenging to measure progress for both Arabic and multilingual
language models. This challenge is compounded by the fact that any benchmark
targeting Arabic needs to take into account the fact that Arabic is not a
single language but rather a collection of languages and varieties. In this
work, we introduce ORCA, a publicly available benchmark for Arabic language
understanding evaluation. ORCA is carefully constructed to cover diverse Arabic
varieties and a wide range of challenging Arabic understanding tasks exploiting
60 different datasets across seven NLU task clusters. To measure current
progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between
18 multilingual and Arabic language models. We also provide a public
leaderboard with a unified single-number evaluation metric (ORCA score) to
facilitate future research.",None,-1
8644dc3f-f273-4b88-af24-5a59ae13bdc7,AARGH! End-to-end Retrieval-Generation for Task-Oriented Dialog,0.473217,"We introduce AARGH, an end-to-end task-oriented dialog system combining
retrieval and generative approaches in a single model, aiming at improving
dialog management and lexical diversity of outputs. The model features a new
response selection method based on an action-aware training objective and a
simplified single-encoder retrieval architecture which allow us to build an
end-to-end retrieval-enhanced generation model where retrieval and generation
share most of the parameters. On the MultiWOZ dataset, we show that our
approach produces more diverse outputs while maintaining or improving state
tracking and context-to-response generation performance, compared to
state-of-the-art baselines.",https://github.com/Tomiinek/Aargh,-1
2e8af10c-0a05-4260-a7f0-3ddf6172c261,Hyperplane bounds for neural feature mappings,0.146227,"Deep learning methods minimise the empirical risk using loss functions such
as the cross entropy loss. When minimising the empirical risk, the
generalisation of the learnt function still depends on the performance on the
training data, the Vapnik-Chervonenkis(VC)-dimension of the function and the
number of training examples. Neural networks have a large number of parameters,
which correlates with their VC-dimension that is typically large but not
infinite, and typically a large number of training instances are needed to
effectively train them.
  In this work, we explore how to optimize feature mappings using neural
network with the intention to reduce the effective VC-dimension of the
hyperplane found in the space generated by the mapping. An interpretation of
the results of this study is that it is possible to define a loss that controls
the VC-dimension of the separating hyperplane. We evaluate this approach and
observe that the performance when using this method improves when the size of
the training set is small.",https://github.com/ajjimeno/nn-hyperplane-bounds,-1
0d918816-6c45-4ac9-9036-477a54614f03,SiNeRF: Sinusoidal Neural Radiance Fields for Joint Pose Estimation and Scene Reconstruction,0.618825,"NeRFmm is the Neural Radiance Fields (NeRF) that deal with Joint Optimization
tasks, i.e., reconstructing real-world scenes and registering camera parameters
simultaneously. Despite NeRFmm producing precise scene synthesis and pose
estimations, it still struggles to outperform the full-annotated baseline on
challenging scenes. In this work, we identify that there exists a systematic
sub-optimality in joint optimization and further identify multiple potential
sources for it. To diminish the impacts of potential sources, we propose
Sinusoidal Neural Radiance Fields (SiNeRF) that leverage sinusoidal activations
for radiance mapping and a novel Mixed Region Sampling (MRS) for selecting ray
batch efficiently. Quantitative and qualitative results show that compared to
NeRFmm, SiNeRF achieves comprehensive significant improvements in image
synthesis quality and pose estimation accuracy. Codes are available at
https://github.com/yitongx/sinerf.",https://github.com/yitongx/sinerf,-1
63642686-bcce-465e-96c8-6d5391738fa3,Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention,0.139363,"The massive amount of trainable parameters in the pre-trained language models
(PLMs) makes them hard to be deployed to multiple downstream tasks. To address
this issue, parameter-efficient transfer learning methods have been proposed to
tune only a few parameters during fine-tuning while freezing the rest. This
paper looks at existing methods along this line through the \textit{kernel
lens}. Motivated by the connection between self-attention in transformer-based
PLMs and kernel learning, we propose \textit{kernel-wise adapters}, namely
\textit{Kernel-mix}, that utilize the kernel structure in self-attention to
guide the assignment of the tunable parameters. These adapters use guidelines
found in classical kernel learning and enable separate parameter tuning for
each attention head. Our empirical results, over a diverse set of natural
language generation and understanding tasks, show that our proposed adapters
can attain or improve the strong performance of existing baselines.",None,-1
20095cc6-89a4-4c41-b506-418a66e4eb41,Data-Driven Mitigation of Adversarial Text Perturbation,0.418859,"Social networks have become an indispensable part of our lives, with billions
of people producing ever-increasing amounts of text. At such scales, content
policies and their enforcement become paramount. To automate moderation,
questionable content is detected by Natural Language Processing (NLP)
classifiers. However, high-performance classifiers are hampered by misspellings
and adversarial text perturbations. In this paper, we classify intentional and
unintentional adversarial text perturbation into ten types and propose a
deobfuscation pipeline to make NLP models robust to such perturbations. We
propose Continuous Word2Vec (CW2V), our data-driven method to learn word
embeddings that ensures that perturbations of words have embeddings similar to
those of the original words. We show that CW2V embeddings are generally more
robust to text perturbations than embeddings based on character ngrams. Our
robust classification pipeline combines deobfuscation and classification, using
proposed defense methods and word embeddings to classify whether Facebook posts
are requesting engagement such as likes. Our pipeline results in engagement
bait classification that goes from 0.70 to 0.67 AUC with adversarial text
perturbation, while character ngram-based word embedding methods result in
downstream classification that goes from 0.76 to 0.64.",https://github.com/pytorch/captum,-1
4c4f99e1-3f29-44b3-b9d8-b0a9de659ccd,What do we Really Know about State of the Art NER?,0.824037,"Named Entity Recognition (NER) is a well researched NLP task and is widely
used in real world NLP scenarios. NER research typically focuses on the
creation of new ways of training NER, with relatively less emphasis on
resources and evaluation. Further, state of the art (SOTA) NER models, trained
on standard datasets, typically report only a single performance measure
(F-score) and we don't really know how well they do for different entity types
and genres of text, or how robust are they to new, unseen entities. In this
paper, we perform a broad evaluation of NER using a popular dataset, that takes
into consideration various text genres and sources constituting the dataset at
hand. Additionally, we generate six new adversarial test sets through small
perturbations in the original test set, replacing select entities while
retaining the context. We also train and test our models on randomly generated
train/dev/test splits followed by an experiment where the models are trained on
a select set of genres but tested genres not seen in training. These
comprehensive evaluation strategies were performed using three SOTA NER models.
Based on our results, we recommend some useful reporting practices for NER
researchers, that could help in providing a better understanding of a SOTA
model's performance in future.",https://github.com/nishkalavallabhi/SOTANER/,-1
b024d150-25e0-402c-9efc-919c0ade24ba,GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language,0.385402,"Helping end users comprehend the abstract distribution shifts can greatly
facilitate AI deployment. Motivated by this, we propose a novel task, dataset
explanation. Given two image data sets, dataset explanation aims to
automatically point out their dataset-level distribution shifts with natural
language. Current techniques for monitoring distribution shifts provide
inadequate information to understand datasets with the goal of improving data
quality. Therefore, we introduce GSCLIP, a training-free framework to solve the
dataset explanation task. In GSCLIP, we propose the selector as the first
quantitative evaluation method to identify explanations that are proper to
summarize dataset shifts. Furthermore, we leverage this selector to demonstrate
the superiority of a generator based on language model generation. Systematic
evaluation on natural data shift verifies that GSCLIP, a combined system of a
hybrid generator group and an efficient selector is not only easy-to-use but
also powerful for dataset explanation at scale.",https://github.com/Weixin-Liang/MetaShift,-1
5fb16a5d-a14e-4bd6-ab6a-1db0d8bae766,Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech,0.745477,"We introduce a generic, language-independent method to collect a large
percentage of offensive and hate tweets regardless of their topics or genres.
We harness the extralinguistic information embedded in the emojis to collect a
large number of offensive tweets. We apply the proposed method on Arabic tweets
and compare it with English tweets - analysing key cultural differences. We
observed a constant usage of these emojis to represent offensiveness throughout
different timespans on Twitter. We manually annotate and publicly release the
largest Arabic dataset for offensive, fine-grained hate speech, vulgar and
violence content. Furthermore, we benchmark the dataset for detecting
offensiveness and hate speech using different transformer architectures and
perform in-depth linguistic analysis. We evaluate our models on external
datasets - a Twitter dataset collected using a completely different method, and
a multi-platform dataset containing comments from Twitter, YouTube and
Facebook, for assessing generalization capability. Competitive results on these
datasets suggest that the data collected using our method captures universal
characteristics of offensive language. Our findings also highlight the common
words used in offensive communications, common targets for hate speech,
specific patterns in violence tweets; and pinpoint common classification errors
that can be attributed to limitations of NLP models. We observe that even
state-of-the-art transformer models may fail to take into account culture,
background and context or understand nuances present in real-world data such as
sarcasm.",None,-1
1ed5d2f0-86a6-4837-bb53-8deee77ca5da,PoissonMat: Remodeling Matrix Factorization using Poisson Distribution and Solving the Cold Start Problem without Input Data,0.789991,"Matrix Factorization is one of the most successful recommender system
techniques over the past decade. However, the classic probabilistic theory
framework for matrix factorization is modeled using normal distributions. To
find better probabilistic models, algorithms such as RankMat, ZeroMat and
DotMat have been invented in recent years. In this paper, we model the user
rating behavior in recommender system as a Poisson process, and design an
algorithm that relies on no input data to solve the recommendation problem and
the cold start issue at the same time. We prove the superiority of our
algorithm in comparison with matrix factorization, random placement, Zipf
placement, ZeroMat, DotMat, etc.",None,-1
2afec887-ee35-4512-b96d-5da1a87ff98b,Adapting to Latent Subgroup Shifts via Concepts and Proxies,0.454264,"We address the problem of unsupervised domain adaptation when the source
domain differs from the target domain because of a shift in the distribution of
a latent subgroup. When this subgroup confounds all observed data, neither
covariate shift nor label shift assumptions apply. We show that the optimal
target predictor can be non-parametrically identified with the help of concept
and proxy variables available only in the source domain, and unlabeled data
from the target. The identification results are constructive, immediately
suggesting an algorithm for estimating the optimal predictor in the target. For
continuous observations, when this algorithm becomes impractical, we propose a
latent variable model specific to the data generation process at hand. We show
how the approach degrades as the size of the shift changes, and verify that it
outperforms both covariate and label shift adjustment.",None,-1
845ca3a4-80a6-44cf-af79-63f1e1cf4366,Sampling-based techniques for designing school boundaries,0.265137,"Recently, an increasing number of researchers, especially in the realm of
political redistricting, have proposed sampling-based techniques to generate a
subset of plans from the vast space of districting plans. These techniques have
been increasingly adopted by U.S. courts of law and independent commissions as
a tool for identifying partisan gerrymanders. Motivated by these recent
developments, we develop a set of similar sampling techniques for designing
school boundaries based on the flip proposal. Note that the flip proposal here
refers to the change in the districting plan by a single assignment. These
sampling-based techniques serve a dual purpose. They can be used as a baseline
for comparing redistricting algorithms based on local search. Additionally,
these techniques can help to infer the problem characteristics that may be
further used for developing efficient redistricting methods. We empirically
touch on both these aspects in regards to the problem of school redistricting.",https://github.com/subhodipbiswas/SamplingbasedSchoolRedistricting,-1
d09bc21d-4bd2-400c-8de8-0d64b1aa9384,Apport des ontologies pour le calcul de la similarit smantique au sein d'un systme de recommandation,0.635075,"Measurement of the semantic relatedness or likeness between terms, words, or
text data plays an important role in different applications dealing with
textual data such as knowledge acquisition, recommender system, and natural
language processing. Over the past few years, many ontologies have been
developed and used as a form of structured representation of knowledge bases
for information systems. The calculation of semantic similarity from ontology
has developed and depending on the context is complemented by other similarity
calculation methods. In this paper, we propose and carry on an approach for the
calculation of ontology-based semantic similarity using in the context of a
recommender system.",None,-1
a78f766e-c9a3-478c-a144-712635198936,Relational Attention: Generalizing Transformers for Graph-Structured Tasks,0.412379,"Transformers flexibly operate over sets of real-valued vectors representing
task-specific entities and their attributes, where each vector might encode one
word-piece token and its position in a sequence, or some piece of information
that carries no position at all. But as set processors, transformers are at a
disadvantage in reasoning over more general graph-structured data where nodes
represent entities and edges represent relations between entities. To address
this shortcoming, we generalize transformer attention to consider and update
edge vectors in each transformer layer. We evaluate this relational transformer
on a diverse array of graph-structured tasks, including the large and
challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically
outperforms state-of-the-art graph neural networks expressly designed to reason
over graph-structured data. Our analysis demonstrates that these gains are
attributable to relational attention's inherent ability to leverage the greater
expressivity of graphs over sets.",None,-1
bdb4e7f9-0c0b-4b82-b2ed-b4b3b873d4ee,Model Agnostic Local Explanations of Reject,0.431335,"The application of machine learning based decision making systems in safety
critical areas requires reliable high certainty predictions. Reject options are
a common way of ensuring a sufficiently high certainty of predictions made by
the system. While being able to reject uncertain samples is important, it is
also of importance to be able to explain why a particular sample was rejected.
However, explaining general reject options is still an open problem. We propose
a model agnostic method for locally explaining arbitrary reject options by
means of interpretable models and counterfactual explanations.",https://github.com/andreArtelt/LocalModelAgnosticExplanationReject,-1
905efe13-dd93-43fc-91dc-3dedcef55fc0,Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models,0.858371,"Motivations for methods in explainable artificial intelligence (XAI) often
include detecting, quantifying and mitigating bias, and contributing to making
machine learning models fairer. However, exactly how an XAI method can help in
combating biases is often left unspecified. In this paper, we briefly review
trends in explainability and fairness in NLP research, identify the current
practices in which explainability methods are applied to detect and mitigate
bias, and investigate the barriers preventing XAI methods from being used more
widely in tackling fairness issues.",None,-1
6742bd4a-72d4-4865-a6e6-95fc4b97039f,DocEnTr: An End-to-End Document Image Enhancement Transformer,0.795889,"Document images can be affected by many degradation scenarios, which cause
recognition and processing difficulties. In this age of digitization, it is
important to denoise them for proper usage. To address this challenge, we
present a new encoder-decoder architecture based on vision transformers to
enhance both machine-printed and handwritten document images, in an end-to-end
fashion. The encoder operates directly on the pixel patches with their
positional information without the use of any convolutional layers, while the
decoder reconstructs a clean image from the encoded patches. Conducted
experiments show a superiority of the proposed model compared to the state-of
the-art methods on several DIBCO benchmarks. Code and models will be publicly
available at: \url{https://github.com/dali92002/DocEnTR}.",https://github.com/dali92002/DocEnTR,-1
b1b97e1e-2e2d-4efb-874c-b9d3bb939be4,Quantum Motion Segmentation,0.353905,"Motion segmentation is a challenging problem that seeks to identify
independent motions in two or several input images. This paper introduces the
first algorithm for motion segmentation that relies on adiabatic quantum
optimization of the objective function. The proposed method achieves on-par
performance with the state of the art on problem instances which can be mapped
to modern quantum annealers.",None,-1
83eb8c81-ac99-4e66-9d71-6106c3fde023,SciFact-Open: Towards open-domain scientific claim verification,0.99634,"While research on scientific claim verification has led to the development of
powerful systems that appear to approach human performance, these approaches
have yet to be tested in a realistic setting against large corpora of
scientific literature. Moving to this open-domain evaluation setting, however,
poses unique challenges; in particular, it is infeasible to exhaustively
annotate all evidence documents. In this work, we present SciFact-Open, a new
test collection designed to evaluate the performance of scientific claim
verification systems on a corpus of 500K research abstracts. Drawing upon
pooling techniques from information retrieval, we collect evidence for
scientific claims by pooling and annotating the top predictions of four
state-of-the-art scientific claim verification models. We find that systems
developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting
performance drops of at least 15 F1. In addition, analysis of the evidence in
SciFact-Open reveals interesting phenomena likely to appear when claim
verification systems are deployed in practice, e.g., cases where the evidence
supports only a special case of the claim. Our dataset is available at
https://github.com/dwadden/scifact-open.",https://github.com/dwadden/scifact-open,-1
7d49b2a7-3004-48d2-b260-82a741ce87e9,SAL-CNN: Estimate the Remaining Useful Life of Bearings Using Time-frequency Information,0.0853306,"In modern industrial production, the prediction ability of the remaining
useful life (RUL) of bearings directly affects the safety and stability of the
system. Traditional methods require rigorous physical modeling and perform
poorly for complex systems. In this paper, an end-to-end RUL prediction method
is proposed, which uses short-time Fourier transform (STFT) as preprocessing.
Considering the time correlation of signal sequences, a long and short-term
memory network is designed in CNN, incorporating the convolutional block
attention module, and understanding the decision-making process of the network
from the interpretability level. Experiments were carried out on the 2012PHM
dataset and compared with other methods, and the results proved the
effectiveness of the method.",None,-1
21da8319-bb46-4ecc-96aa-84b933f46fc5,Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency,0.0103205,"The word embedding association test (WEAT) is an important method for
measuring linguistic biases against social groups such as ethnic minorities in
large text corpora. It does so by comparing the semantic relatedness of words
prototypical of the groups (e.g., names unique to those groups) and attribute
words (e.g., 'pleasant' and 'unpleasant' words). We show that anti-black WEAT
estimates from geo-tagged social media data at the level of metropolitan
statistical areas strongly correlate with several measures of racial
animus--even when controlling for sociodemographic covariates. However, we also
show that every one of these correlations is explained by a third variable: the
frequency of Black names in the underlying corpora relative to White names.
This occurs because word embeddings tend to group positive (negative) words and
frequent (rare) words together in the estimated semantic space. As the
frequency of Black names on social media is strongly correlated with Black
Americans' prevalence in the population, this results in spurious anti-Black
WEAT estimates wherever few Black Americans live. This suggests that research
using the WEAT to measure bias should consider term frequency, and also
demonstrates the potential consequences of using black-box models like word
embeddings to study human cognition and behavior.",None,22161
159503bd-8648-4487-87e4-2ea2f05356bb,Mixture of Input-Output Hidden Markov Models for Heterogeneous Disease Progression Modeling,0.561354,"A particular challenge for disease progression modeling is the heterogeneity
of a disease and its manifestations in the patients. Existing approaches often
assume the presence of a single disease progression characteristics which is
unlikely for neurodegenerative disorders such as Parkinson's disease. In this
paper, we propose a hierarchical time-series model that can discover multiple
disease progression dynamics. The proposed model is an extension of an
input-output hidden Markov model that takes into account the clinical
assessments of patients' health status and prescribed medications. We
illustrate the benefits of our model using a synthetically generated dataset
and a real-world longitudinal dataset for Parkinson's disease.",https://github.com/tahaceritli/mIOHMM,17554
eb64e2e6-2b40-482b-8cdc-9585d6f5e0c8,Hierarchical Multi-Label Classification of Scientific Documents,0.673935,"Automatic topic classification has been studied extensively to assist
managing and indexing scientific documents in a digital collection. With the
large number of topics being available in recent years, it has become necessary
to arrange them in a hierarchy. Therefore, the automatic classification systems
need to be able to classify the documents hierarchically. In addition, each
paper is often assigned to more than one relevant topic. For example, a paper
can be assigned to several topics in a hierarchy tree. In this paper, we
introduce a new dataset for hierarchical multi-label text classification
(HMLTC) of scientific papers called SciHTC, which contains 186,160 papers and
1,233 categories from the ACM CCS tree. We establish strong baselines for HMLTC
and propose a multi-task learning approach for topic classification with
keyword labeling as an auxiliary task. Our best model achieves a Macro-F1 score
of 34.57% which shows that this dataset provides significant research
opportunities on hierarchical scientific topic classification. We make our
dataset and code available on Github.",https://github.com/msadat3/SciHTC,-1
27a4536a-f0c4-4e06-b8fd-e59a8ebc598d,The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization,0.670134,"We present a novel benchmark and associated evaluation metrics for assessing
the performance of text anonymization methods. Text anonymization, defined as
the task of editing a text document to prevent the disclosure of personal
information, currently suffers from a shortage of privacy-oriented annotated
text resources, making it difficult to properly evaluate the level of privacy
protection offered by various anonymization methods. This paper presents TAB
(Text Anonymization Benchmark), a new, open-source annotated corpus developed
to address this shortage. The corpus comprises 1,268 English-language court
cases from the European Court of Human Rights (ECHR) enriched with
comprehensive annotations about the personal information appearing in each
document, including their semantic category, identifier type, confidential
attributes, and co-reference relations. Compared to previous work, the TAB
corpus is designed to go beyond traditional de-identification (which is limited
to the detection of predefined semantic categories), and explicitly marks which
text spans ought to be masked in order to conceal the identity of the person to
be protected. Along with presenting the corpus and its annotation layers, we
also propose a set of evaluation metrics that are specifically tailored towards
measuring the performance of text anonymization, both in terms of privacy
protection and utility preservation. We illustrate the use of the benchmark and
the proposed metrics by assessing the empirical performance of several baseline
text anonymization models. The full corpus along with its privacy-oriented
annotation guidelines, evaluation scripts and baseline models are available on:
https://github.com/NorskRegnesentral/text-anonymisation-benchmark",https://github.com/NorskRegnesentral/text-anonymization-benchmark,-1
7704e40c-3f17-4132-8dd6-bb1db82432f6,Speech Resources in the Tamasheq Language,0.715946,"In this paper we present two datasets for Tamasheq, a developing language
mainly spoken in Mali and Niger. These two datasets were made available for the
IWSLT 2022 low-resource speech translation track, and they consist of
collections of radio recordings from daily broadcast news in Niger (Studio
Kalangou) and Mali (Studio Tamani). We share (i) a massive amount of unlabeled
audio data (671 hours) in five languages: French from Niger, Fulfulde, Hausa,
Tamasheq and Zarma, and (ii) a smaller 17 hours parallel corpus of audio
recordings in Tamasheq, with utterance-level translations in the French
language. All this data is shared under the Creative Commons BY-NC-ND 3.0
license. We hope these resources will inspire the speech community to develop
and benchmark models using the Tamasheq language.",https://github.com/mzboito/IWSLT2022 Tamasheq data.,-1
4e35eaff-af48-45ca-a5dd-a00be9fc1619,RITA: Boost Driving Simulators with Realistic Interactive Traffic Flow,0.271687,"High-quality traffic flow generation is the core module in building
simulators for autonomous driving. However, the majority of available
simulators are incapable of replicating traffic patterns that accurately
reflect the various features of real-world data while also simulating
human-like reactive responses to the tested autopilot driving strategies.
Taking one step forward to addressing such a problem, we propose Realistic
Interactive TrAffic flow (RITA) as an integrated component of existing driving
simulators to provide high-quality traffic flow for the evaluation and
optimization of the tested driving strategies. RITA is developed with
consideration of three key features, i.e., fidelity, diversity, and
controllability, and consists of two core modules called RITABackend and
RITAKit. RITABackend is built to support vehicle-wise control and provide
traffic generation models from real-world datasets, while RITAKit is developed
with easy-to-use interfaces for controllable traffic generation via
RITABackend. We demonstrate RITA's capacity to create diversified and
high-fidelity traffic simulations in several highly interactive highway
scenarios. The experimental findings demonstrate that our produced RITA traffic
flows exhibit all three key features, hence enhancing the completeness of
driving strategy evaluation. Moreover, we showcase the possibility for further
improvement of baseline strategies through online fine-tuning with RITA traffic
flows.",None,-1
32f310f2-06a8-41a6-8c4c-bbf5c83bd4f3,Cross-Spectral Neural Radiance Fields,0.689794,"We propose X-NeRF, a novel method to learn a Cross-Spectral scene
representation given images captured from cameras with different light spectrum
sensitivity, based on the Neural Radiance Fields formulation. X-NeRF optimizes
camera poses across spectra during training and exploits Normalized
Cross-Device Coordinates (NXDC) to render images of different modalities from
arbitrary viewpoints, which are aligned and at the same resolution. Experiments
on 16 forward-facing scenes, featuring color, multi-spectral and infrared
images, confirm the effectiveness of X-NeRF at modeling Cross-Spectral scene
representations.",None,12441
f80c7fe8-71b4-4337-9774-d8d3acdb8331,Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization,0.214299,"In long document controllable summarization, where labeled data is scarce,
pretrained models struggle to adapt to the task and effectively respond to user
queries. In this paper, we introduce Socratic pretraining, a question-driven,
unsupervised pretraining objective specifically designed to improve
controllability in summarization tasks. By training a model to generate and
answer relevant questions in a given context, Socratic pretraining enables the
model to more effectively adhere to user-provided queries and identify relevant
content to be summarized. We demonstrate the effectiveness of this approach
through extensive experimentation on two summarization domains, short stories
and dialogue, and multiple control strategies: keywords, questions, and factoid
QA pairs. Our pretraining method relies only on unlabeled documents and a
question generation system and outperforms pre-finetuning approaches that use
additional supervised data. Furthermore, our results show that Socratic
pretraining cuts task-specific labeled data requirements in half, is more
faithful to user-provided queries, and achieves state-of-the-art performance on
QMSum and SQuALITY.",https://github.com/salesforce/socratic-pretraining,-1
31d21ee2-bb3e-49ff-a3fa-984bd66d1735,Voxel Field Fusion for 3D Object Detection,0.970606,"In this work, we present a conceptually simple yet effective framework for
cross-modality 3D object detection, named voxel field fusion. The proposed
approach aims to maintain cross-modality consistency by representing and fusing
augmented image features as a ray in the voxel field. To this end, the
learnable sampler is first designed to sample vital features from the image
plane that are projected to the voxel grid in a point-to-ray manner, which
maintains the consistency in feature representation with spatial context. In
addition, ray-wise fusion is conducted to fuse features with the supplemental
context in the constructed voxel field. We further develop mixed augmentor to
align feature-variant transformations, which bridges the modality gap in data
augmentation. The proposed framework is demonstrated to achieve consistent
gains in various benchmarks and outperforms previous fusion-based methods on
KITTI and nuScenes datasets. Code is made available at
https://github.com/dvlab-research/VFF.",https://github.com/dvlab-research/VFF.1,76411
1267352f-2412-4931-9375-0855a3e5e33a,Hierarchical Compositional Representations for Few-shot Action Recognition,0.320683,"Recently action recognition has received more and more attention for its
comprehensive and practical applications in intelligent surveillance and
human-computer interaction. However, few-shot action recognition has not been
well explored and remains challenging because of data scarcity. In this paper,
we propose a novel hierarchical compositional representations (HCR) learning
approach for few-shot action recognition. Specifically, we divide a complicated
action into several sub-actions by carefully designed hierarchical clustering
and further decompose the sub-actions into more fine-grained spatially
attentional sub-actions (SAS-actions). Although there exist large differences
between base classes and novel classes, they can share similar patterns in
sub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance in
the transportation problem to measure the similarity between video samples in
terms of sub-action representations. It computes the optimal matching flows
between sub-actions as distance metric, which is favorable for comparing
fine-grained patterns. Extensive experiments show our method achieves the
state-of-the-art results on HMDB51, UCF101 and Kinetics datasets.",None,38355
2bce53af-22d4-4cad-8df2-fcfeaabbab7e,Use of a smartphone camera to determine the focal length of a thin lens by finding the transverse magnification of the virtual image of an object,0.241674,"In this work we have determined the focal length of a concave lens by
photographing the virtual image of an object by a smartphone camera. We have
similarly determined the focal length of a convex lens by forming a virtual
image of an object keeping it within the focal distance from the lens. When a
photograph is taken by a smartphone, the transverse width of the image on the
sensor of the camera in pixels can be read off by software available freely
from the internet. By taking a photograph of the virtual image from two
positions of the camera separated by a distance along the line of sight of the
camera, we have determined the transverse width of the virtual image. From this
we find the focal lengths of the lenses knowing the transverse width and the
distance of the object from the lenses.",None,-1
8c3c8d4a-c29b-4bb7-a8d6-52b3ffdeb6aa,ASR Error Correction with Constrained Decoding on Operation Prediction,0.510625,"Error correction techniques remain effective to refine outputs from automatic
speech recognition (ASR) models. Existing end-to-end error correction methods
based on an encoder-decoder architecture process all tokens in the decoding
phase, creating undesirable latency. In this paper, we propose an ASR error
correction method utilizing the predictions of correction operations. More
specifically, we construct a predictor between the encoder and the decoder to
learn if a token should be kept (""K""), deleted (""D""), or changed (""C"") to
restrict decoding to only part of the input sequence embeddings (the ""C""
tokens) for fast inference. Experiments on three public datasets demonstrate
the effectiveness of the proposed approach in reducing the latency of the
decoding process in ASR correction. It enhances the inference speed by at least
three times (3.4 and 5.7 times) while maintaining the same level of accuracy
(with WER reductions of 0.53% and 1.69% respectively) for our two proposed
models compared to a solid encoder-decoder baseline. In the meantime, we
produce and release a benchmark dataset contributing to the ASR error
correction community to foster research along this line.",https://github.com/yangjingyuan/ConstDecoder,-1
d2237030-83fd-4c0e-9a30-3bae5ec14887,Quantitative AI Risk Assessments: Opportunities and Challenges,0.768873,"Although AI-based systems are increasingly being leveraged to provide value
to organizations, individuals, and society, significant attendant risks have
been identified. These risks have led to proposed regulations, litigation, and
general societal concerns.
  As with any promising technology, organizations want to benefit from the
positive capabilities of AI technology while reducing the risks. The best way
to reduce risks is to implement comprehensive AI lifecycle governance where
policies and procedures are described and enforced during the design,
development, deployment, and monitoring of an AI system. While support for
comprehensive governance is beginning to emerge, organizations often need to
identify the risks of deploying an already-built model without knowledge of how
it was constructed or access to its original developers.
  Such an assessment will quantitatively assess the risks of an existing model
in a manner analogous to how a home inspector might assess the energy
efficiency of an already-built home or a physician might assess overall patient
health based on a battery of tests. This paper explores the concept of a
quantitative AI Risk Assessment, exploring the opportunities, challenges, and
potential impacts of such an approach, and discussing how it might improve AI
regulations.",None,-1
ff279b75-2c88-4580-9623-82589bcad37d,Language Models of Code are Few-Shot Commonsense Learners,0.983474,"We address the general task of structured commonsense reasoning: given a
natural language input, the goal is to generate a graph such as an event -- or
a reasoning-graph. To employ large language models (LMs) for this task,
existing approaches ``serialize'' the output graph as a flat list of nodes and
edges. Although feasible, these serialized graphs strongly deviate from the
natural language corpora that LMs were pre-trained on, hindering LMs from
generating them correctly. In this paper, we show that when we instead frame
structured commonsense reasoning tasks as code generation tasks, pre-trained
LMs of code are better structured commonsense reasoners than LMs of natural
language, even when the downstream task does not involve source code at all. We
demonstrate our approach across three diverse structured commonsense reasoning
tasks. In all these natural language tasks, we show that using our approach, a
code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the
target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot
setting.",https://github.com/madaan/CoCoGen,-1
16a6b8db-d396-4de3-b169-a3ab023a164d,Predicting Customer Lifetime Value in Free-to-Play Games,0.457857,"As game companies increasingly embrace a service-oriented business model, the
need for predictive models of players becomes more pressing. Multiple
activities, such as user acquisition, live game operations or game design need
to be supported with information about the choices made by the players and the
choices they could make in the future. This is especially true in the context
of free-to-play games, where the absence of a pay wall and the erratic nature
of the players' playing and spending behavior make predictions about the
revenue and allocation of budget and resources extremely challenging. In this
chapter we will present an overview of customer lifetime value modeling across
different fields, we will introduce the challenges specific to free-to-play
games across different platforms and genres and we will discuss the
state-of-the-art solutions with practical examples and references to existing
implementations.",https://github.com/CamDavidsonPilon/lifetimes,-1
1e5329f3-6194-492a-95e4-e5d83c921581,Towards Pragmatic Production Strategies for Natural Language Generation Tasks,0.409052,"This position paper proposes a conceptual framework for the design of Natural
Language Generation (NLG) systems that follow efficient and effective
production strategies in order to achieve complex communicative goals. In this
general framework, efficiency is characterised as the parsimonious regulation
of production and comprehension costs while effectiveness is measured with
respect to task-oriented and contextually grounded communicative goals. We
provide concrete suggestions for the estimation of goals, costs, and utility
via modern statistical methods, demonstrating applications of our framework to
the classic pragmatic task of visually grounded referential games and to
abstractive text summarisation, two popular generation tasks with real-world
applications. In sum, we advocate for the development of NLG systems that learn
to make pragmatic production decisions from experience, by reasoning about
goals, costs, and utility in a human-like way.",None,-1
19f14fc3-f56e-473f-b3ff-22ef12f75246,Fourier Document Restoration for Robust Document Dewarping and Recognition,0.538112,"State-of-the-art document dewarping techniques learn to predict 3-dimensional
information of documents which are prone to errors while dealing with documents
with irregular distortions or large variations in depth. This paper presents
FDRNet, a Fourier Document Restoration Network that can restore documents with
different distortions and improve document recognition in a reliable and
simpler manner. FDRNet focuses on high-frequency components in the Fourier
space that capture most structural information but are largely free of
degradation in appearance. It dewarps documents by a flexible Thin-Plate Spline
transformation which can handle various deformations effectively without
requiring deformation annotations in training. These features allow FDRNet to
learn from a small amount of simply labeled training images, and the learned
model can dewarp documents with complex geometric distortion and recognize the
restored texts accurately. To facilitate document restoration research, we
create a benchmark dataset consisting of over one thousand camera documents
with different types of geometric and photometric distortion. Extensive
experiments show that FDRNet outperforms the state-of-the-art by large margins
on both dewarping and text recognition tasks. In addition, FDRNet requires a
small amount of simply labeled training data and is easy to deploy.",None,-1
7e9448f4-7efe-4b92-80a5-b9a3850652f7,Feature Extraction Framework based on Contrastive Learning with Adaptive Positive and Negative Samples,0.0336759,"In this study, we propose a feature extraction framework based on contrastive
learning with adaptive positive and negative samples (CL-FEFA) that is suitable
for unsupervised, supervised, and semi-supervised single-view feature
extraction. CL-FEFA constructs adaptively the positive and negative samples
from the results of feature extraction, which makes it more appropriate and
accurate. Thereafter, the discriminative features are re extracted to according
to InfoNCE loss based on previous positive and negative samples, which will
make the intra-class samples more compact and the inter-class samples more
dispersed. At the same time, using the potential structure information of
subspace samples to dynamically construct positive and negative samples can
make our framework more robust to noisy data. Furthermore, CL-FEFA considers
the mutual information between positive samples, that is, similar samples in
potential structures, which provides theoretical support for its advantages in
feature extraction. The final numerical experiments prove that the proposed
framework has a strong advantage over the traditional feature extraction
methods and contrastive learning methods.",None,-1
d9ffd8c0-9ec0-43c6-82df-7d7cf5559c92,Automatic Fine-grained Glomerular Lesion Recognition in Kidney Pathology,0.522108,"Recognition of glomeruli lesions is the key for diagnosis and treatment
planning in kidney pathology; however, the coexisting glomerular structures
such as mesangial regions exacerbate the difficulties of this task. In this
paper, we introduce a scheme to recognize fine-grained glomeruli lesions from
whole slide images. First, a focal instance structural similarity loss is
proposed to drive the model to locate all types of glomeruli precisely. Then an
Uncertainty Aided Apportionment Network is designed to carry out the
fine-grained visual classification without bounding-box annotations. This
double branch-shaped structure extracts common features of the child class from
the parent class and produces the uncertainty factor for reconstituting the
training dataset. Results of slide-wise evaluation illustrate the effectiveness
of the entire scheme, with an 8-22% improvement of the mean Average Precision
compared with remarkable detection methods. The comprehensive results clearly
demonstrate the effectiveness of the proposed method.",None,-1
da26ef92-f520-476e-b7a9-3a3f1e05519b,Representation Learning for Resource-Constrained Keyphrase Generation,0.178092,"State-of-the-art keyphrase generation methods generally depend on large
annotated datasets, limiting their performance in domains with limited
annotated data. To overcome this challenge, we design a data-oriented approach
that first identifies salient information using retrieval-based corpus-level
statistics, and then learns a task-specific intermediate representation based
on a pre-trained language model using large-scale unlabeled documents. We
introduce salient span recovery and salient span prediction as denoising
training objectives that condense the intra-article and inter-article knowledge
essential for keyphrase generation. Through experiments on multiple keyphrase
generation benchmarks, we show the effectiveness of the proposed approach for
facilitating low-resource keyphrase generation and zero-shot domain adaptation.
Our method especially benefits the generation of absent keyphrases, approaching
the performance of models trained with large training sets.",https://github.com/xiaowu0162/low-resource-kpgen,-1
14bada1b-2a4c-4ab9-a648-8a393952b84e,ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer,0.494068,"The problem of multimodal intent and trajectory prediction for human-driven
vehicles in parking lots is addressed in this paper. Using models designed with
CNN and Transformer networks, we extract temporal-spatial and contextual
information from trajectory history and local bird's eye view (BEV) semantic
images, and generate predictions about intent distribution and future
trajectory sequences. Our methods outperform existing models in accuracy, while
allowing an arbitrary number of modes, encoding complex multi-agent scenarios,
and adapting to different parking maps. To train and evaluate our method, we
present the first public 4K video dataset of human driving in parking lots with
accurate annotation, high frame rate, and rich traffic scenarios.",https://github.com/XuShenLZ/ParkSim/,29242
03d3641d-41bf-4bb7-9c85-b48a45f0c1ec,Benchmark datasets driving artificial intelligence development fail to capture the needs of medical professionals,0.373417,"Publicly accessible benchmarks that allow for assessing and comparing model
performances are important drivers of progress in artificial intelligence (AI).
While recent advances in AI capabilities hold the potential to transform
medical practice by assisting and augmenting the cognitive processes of
healthcare professionals, the coverage of clinically relevant tasks by AI
benchmarks is largely unclear. Furthermore, there is a lack of systematized
meta-information that allows clinical AI researchers to quickly determine
accessibility, scope, content and other characteristics of datasets and
benchmark datasets relevant to the clinical domain.
  To address these issues, we curated and released a comprehensive catalogue of
datasets and benchmarks pertaining to the broad domain of clinical and
biomedical natural language processing (NLP), based on a systematic review of
literature and online resources. A total of 450 NLP datasets were manually
systematized and annotated with rich metadata, such as targeted tasks, clinical
applicability, data types, performance metrics, accessibility and licensing
information, and availability of data splits. We then compared tasks covered by
AI benchmark datasets with relevant tasks that medical practitioners reported
as highly desirable targets for automation in a previous empirical study.
  Our analysis indicates that AI benchmarks of direct clinical relevance are
scarce and fail to cover most work activities that clinicians want to see
addressed. In particular, tasks associated with routine documentation and
patient data administration workflows are not represented despite significant
associated workloads. Thus, currently available AI benchmarks are improperly
aligned with desired targets for AI automation in clinical settings, and novel
benchmarks should be created to fill these gaps.",None,12
f68b0cc0-aa94-4ff5-86cb-6f8c5daeff80,RoPGen: Towards Robust Code Authorship Attribution via Automatic Coding Style Transformation,0.9463,"Source code authorship attribution is an important problem often encountered
in applications such as software forensics, bug fixing, and software quality
analysis. Recent studies show that current source code authorship attribution
methods can be compromised by attackers exploiting adversarial examples and
coding style manipulation. This calls for robust solutions to the problem of
code authorship attribution. In this paper, we initiate the study on making
Deep Learning (DL)-based code authorship attribution robust. We propose an
innovative framework called Robust coding style Patterns Generation (RoPGen),
which essentially learns authors' unique coding style patterns that are hard
for attackers to manipulate or imitate. The key idea is to combine data
augmentation and gradient augmentation at the adversarial training phase. This
effectively increases the diversity of training examples, generates meaningful
perturbations to gradients of deep neural networks, and learns diversified
representations of coding styles. We evaluate the effectiveness of RoPGen using
four datasets of programs written in C, C++, and Java. Experimental results
show that RoPGen can significantly improve the robustness of DL-based code
authorship attribution, by respectively reducing 22.8% and 41.0% of the success
rate of targeted and untargeted attacks on average.",https://github.com/RoPGen/RoPGen,-1
8ebe8ddc-3ef9-4147-9bf5-50fc42233f52,ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction,0.087404,"In this work, we propose the novel Prototypical Graph Regression
Self-explainable Trees (ProGReST) model, which combines prototype learning,
soft decision trees, and Graph Neural Networks. In contrast to other works, our
model can be used to address various challenging tasks, including compound
property prediction. In ProGReST, the rationale is obtained along with
prediction due to the model's built-in interpretability. Additionally, we
introduce a new graph prototype projection to accelerate model training.
Finally, we evaluate PRoGReST on a wide range of chemical datasets for
molecular property prediction and perform in-depth analysis with chemical
experts to evaluate obtained interpretations. Our method achieves competitive
results against state-of-the-art methods.",https://github.com/gmum/ProGReST,-1
a88dbd84-6488-4d76-88f3-e2994de97bb9,ASC me to Do Anything: Multi-task Training for Embodied AI,0.0532508,"Embodied AI has seen steady progress across a diverse set of independent
tasks. While these varied tasks have different end goals, the basic skills
required to complete them successfully overlap significantly. In this paper,
our goal is to leverage these shared skills to learn to perform multiple tasks
jointly. We propose Atomic Skill Completion (ASC), an approach for multi-task
training for Embodied AI, where a set of atomic skills shared across multiple
tasks are composed together to perform the tasks. The key to the success of
this approach is a pre-training scheme that decouples learning of the skills
from the high-level tasks making joint training effective. We use ASC to train
agents within the AI2-THOR environment to perform four interactive tasks
jointly and find it to be remarkably effective. In a multi-task setting, ASC
improves success rates by a factor of 2x on Seen scenes and 4x on Unseen scenes
compared to no pre-training. Importantly, ASC enables us to train a multi-task
agent that has a 52% higher Success Rate than training 4 independent single
task agents. Finally, our hierarchical agents are more interpretable than
traditional black-box architectures.",None,-1
7f894540-043e-4096-a946-dc943976120f,Dynamic Global Memory for Document-level Argument Extraction,0.97874,"Extracting informative arguments of events from news articles is a
challenging problem in information extraction, which requires a global
contextual understanding of each document. While recent work on document-level
extraction has gone beyond single-sentence and increased the cross-sentence
inference capability of end-to-end models, they are still restricted by certain
input sequence length constraints and usually ignore the global context between
events. To tackle this issue, we introduce a new global neural generation-based
framework for document-level event argument extraction by constructing a
document memory store to record the contextual event information and leveraging
it to implicitly and explicitly help with decoding of arguments for later
events. Empirical results show that our framework outperforms prior methods
substantially and it is more robust to adversarially annotated examples with
our constrained decoding design. (Our code and resources are available at
https://github.com/xinyadu/memory_docie for research purpose.)",https://github.com/xinyadu/memory_docie,-1
3ed0b6e3-079e-4377-bf0a-5d8234832c11,Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories,0.404387,"Obtaining photorealistic reconstructions of objects from sparse views is
inherently ambiguous and can only be achieved by learning suitable
reconstruction priors. Earlier works on sparse rigid object reconstruction
successfully learned such priors from large datasets such as CO3D. In this
paper, we extend this approach to dynamic objects. We use cats and dogs as a
representative example and introduce Common Pets in 3D (CoP3D), a collection of
crowd-sourced videos showing around 4,200 distinct pets. CoP3D is one of the
first large-scale datasets for benchmarking non-rigid 3D reconstruction ""in the
wild"". We also propose Tracker-NeRF, a method for learning 4D reconstruction
from our dataset. At test time, given a small number of video frames of an
unseen object, Tracker-NeRF predicts the trajectories of its 3D points and
generates new views, interpolating viewpoint and time. Results on CoP3D reveal
significantly better non-rigid new-view synthesis performance than existing
baselines.",None,-1
6ec2a947-5e21-4a01-9c4b-c81179736ed0,Power Grid Congestion Management via Topology Optimization with AlphaZero,0.902478,"The energy sector is facing rapid changes in the transition towards clean
renewable sources. However, the growing share of volatile, fluctuating
renewable generation such as wind or solar energy has already led to an
increase in power grid congestion and network security concerns. Grid operators
mitigate these by modifying either generation or demand (redispatching,
curtailment, flexible loads). Unfortunately, redispatching of fossil generators
leads to excessive grid operation costs and higher emissions, which is in
direct opposition to the decarbonization of the energy sector. In this paper,
we propose an AlphaZero-based grid topology optimization agent as a non-costly,
carbon-free congestion management alternative. Our experimental evaluation
confirms the potential of topology optimization for power grid operation,
achieves a reduction of the average amount of required redispatching by 60%,
and shows the interoperability with traditional congestion management methods.
Our approach also ranked 1st in the WCCI 2022 Learning to Run a Power Network
(L2RPN) competition. Based on our findings, we identify and discuss open
research problems as well as technical challenges for a productive system on a
real power grid.",https://github.com/enlite-ai/maze-l2rpn-2022-submission,-1
3412999f-6dea-4e7c-b80c-c681f2ee2f51,Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator,0.524051,"Contextual knowledge is essential for reducing speech recognition errors on
high-valued long-tail words. This paper proposes a novel tree-constrained
pointer generator (TCPGen) component that enables end-to-end ASR models to bias
towards a list of long-tail words obtained using external contextual
information. With only a small overhead in memory use and computation cost,
TCPGen can structure thousands of biasing words efficiently into a symbolic
prefix-tree and creates a neural shortcut between the tree and the final ASR
output to facilitate the recognition of the biasing words. To enhance TCPGen,
we further propose a novel minimum biasing word error (MBWE) loss that directly
optimises biasing word errors during training, along with a biasing-word-driven
language model discounting (BLMD) method during the test. All contextual ASR
systems were evaluated on the public Librispeech audiobook corpus and the data
from the dialogue state tracking challenges (DSTC) with the biasing lists
extracted from the dialogue-system ontology. Consistent word error rate (WER)
reductions were achieved with TCPGen, which were particularly significant on
the biasing words with around 40\% relative reductions in the recognition error
rates. MBWE and BLMD further improved the effectiveness of TCPGen and achieved
more significant WER reductions on the biasing words. TCPGen also achieved
zero-shot learning of words not in the audio training set with large WER
reductions on the out-of-vocabulary words in the biasing list.",None,-1
215d3a98-25b0-43cf-b38d-89a08051c05d,Speech Emotion Recognition using Self-Supervised Features,0.997567,"Self-supervised pre-trained features have consistently delivered state-of-art
results in the field of natural language processing (NLP); however, their
merits in the field of speech emotion recognition (SER) still need further
investigation. In this paper we introduce a modular End-to- End (E2E) SER
system based on an Upstream + Downstream architecture paradigm, which allows
easy use/integration of a large variety of self-supervised features. Several
SER experiments for predicting categorical emotion classes from the IEMOCAP
dataset are performed. These experiments investigate interactions among
fine-tuning of self-supervised feature models, aggregation of frame-level
features into utterance-level features and back-end classification networks.
The proposed monomodal speechonly based system not only achieves SOTA results,
but also brings light to the possibility of powerful and well finetuned
self-supervised acoustic features that reach results similar to the results
achieved by SOTA multimodal systems using both Speech and Text modalities.",None,-1
6a04b37f-b8b3-4fc9-a368-9f00205cd447,Towards a Theory of Faithfulness: Faithful Explanations of Differentiable Classifiers over Continuous Data,0.0961315,"There is broad agreement in the literature that explanation methods should be
faithful to the model that they explain, but faithfulness remains a rather
vague term. We revisit faithfulness in the context of continuous data and
propose two formal definitions of faithfulness for feature attribution methods.
Qualitative faithfulness demands that scores reflect the true qualitative
effect (positive vs. negative) of the feature on the model and quanitative
faithfulness that the magnitude of scores reflect the true quantitative effect.
We discuss under which conditions these requirements can be satisfied to which
extent (local vs global). As an application of the conceptual idea, we look at
differentiable classifiers over continuous data and characterize
Gradient-scores as follows: every qualitatively faithful feature attribution
method is qualitatively equivalent to Gradient-scores. Furthermore, if an
attribution method is quantitatively faithful in the sense that changes of the
output of the classifier are proportional to the scores of features, then it is
either equivalent to gradient-scoring or it is based on an inferior
approximation of the classifier. To illustrate the practical relevance of the
theory, we experimentally demonstrate that popular attribution methods can fail
to give faithful explanations in the setting where the data is continuous and
the classifier differentiable.",https://github.com/XiangYin2021/Revisit-Faithfulness,-1
b441fab9-6f70-4d13-9418-eabb3467cbeb,Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology,0.694489,"Tissue phenotyping is a fundamental task in learning objective
characterizations of histopathologic biomarkers within the tumor-immune
microenvironment in cancer pathology. However, whole-slide imaging (WSI) is a
complex computer vision in which: 1) WSIs have enormous image resolutions with
precludes large-scale pixel-level efforts in data curation, and 2) diversity of
morphological phenotypes results in inter- and intra-observer variability in
tissue labeling. To address these limitations, current efforts have proposed
using pretrained image encoders (transfer learning from ImageNet,
self-supervised pretraining) in extracting morphological features from
pathology, but have not been extensively validated. In this work, we conduct a
search for good representations in pathology by training a variety of
self-supervised models with validation on a variety of weakly-supervised and
patch-level tasks. Our key finding is in discovering that Vision Transformers
using DINO-based knowledge distillation are able to learn data-efficient and
interpretable features in histology images wherein the different attention
heads learn distinct morphological phenotypes. We make evaluation code and
pretrained weights publicly-available at:
https://github.com/Richarizardd/Self-Supervised-ViT-Path.",https://github.com/Richarizardd/Self-Supervised-ViT-Path,-1
79c2a4b0-5a33-4a82-92da-8ac4df7dc881,Adaptive Transformers for Robust Few-shot Cross-domain Face Anti-spoofing,0.681298,"While recent face anti-spoofing methods perform well under the intra-domain
setups, an effective approach needs to account for much larger appearance
variations of images acquired in complex scenes with different sensors for
robust performance. In this paper, we present adaptive vision transformers
(ViT) for robust cross-domain face antispoofing. Specifically, we adopt ViT as
a backbone to exploit its strength to account for long-range dependencies among
pixels. We further introduce the ensemble adapters module and feature-wise
transformation layers in the ViT to adapt to different domains for robust
performance with a few samples. Experiments on several benchmark datasets show
that the proposed models achieve both robust and competitive performance
against the state-of-the-art methods for cross-domain face anti-spoofing using
a few samples.",None,-1
1cb5d21b-bc5e-49b7-8016-2225593200b7,Feature Refinement to Improve High Resolution Image Inpainting,0.177319,"In this paper, we address the problem of degradation in inpainting quality of
neural networks operating at high resolutions. Inpainting networks are often
unable to generate globally coherent structures at resolutions higher than
their training set. This is partially attributed to the receptive field
remaining static, despite an increase in image resolution. Although downscaling
the image prior to inpainting produces coherent structure, it inherently lacks
detail present at higher resolutions. To get the best of both worlds, we
optimize the intermediate featuremaps of a network by minimizing a multiscale
consistency loss at inference. This runtime optimization improves the
inpainting results and establishes a new state-of-the-art for high resolution
inpainting. Code is available at:
https://github.com/geomagical/lama-with-refiner/tree/refinement.",https://github.com/geomagical/lama-with-refiner/tree/refinement,49
b4cd9873-d026-4e4c-af20-bac38de3ebc7,Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders,0.511428,"Knowledge distillation (KD) has been a ubiquitous method for model
compression to strengthen the capability of a lightweight model with the
transferred knowledge from the teacher. In particular, KD has been employed in
quantization-aware training (QAT) of Transformer encoders like BERT to improve
the accuracy of the student model with the reduced-precision weight parameters.
However, little is understood about which of the various KD approaches best
fits the QAT of Transformers. In this work, we provide an in-depth analysis of
the mechanism of KD on attention recovery of quantized large Transformers. In
particular, we reveal that the previously adopted MSE loss on the attention
score is insufficient for recovering the self-attention information. Therefore,
we propose two KD methods; attention-map and attention-output losses.
Furthermore, we explore the unification of both losses to address
task-dependent preference between attention-map and output losses. The
experimental results on various Transformer encoder models demonstrate that the
proposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit
weight quantization.",https://github.com/MarsJacobs/kd-qat-large-enc,-1
bd737b39-0b6b-44ba-bc70-eb1c51a5dd6f,Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers,0.298557,"Recently, RNN-Transducers have achieved remarkable results on various
automatic speech recognition tasks. However, lattice-free sequence
discriminative training methods, which obtain superior performance in hybrid
models, are rarely investigated in RNN-Transducers. In this work, we propose
three lattice-free training objectives, namely lattice-free maximum mutual
information, lattice-free segment-level minimum Bayes risk, and lattice-free
minimum Bayes risk, which are used for the final posterior output of the
phoneme-based neural transducer with a limited context dependency. Compared to
criteria using N-best lists, lattice-free methods eliminate the decoding step
for hypotheses generation during training, which leads to more efficient
training. Experimental results show that lattice-free methods gain up to 6.5%
relative improvement in word error rate compared to a sequence-level
cross-entropy trained model. Compared to the N-best-list based minimum Bayes
risk objectives, lattice-free methods gain 40% - 70% relative training time
speedup with a small degradation in performance.",None,-1
89e1a713-8216-4e65-8345-42f50733f395,Multi-focus thermal image fusion,0.513198,"This paper proposes a novel algorithm for multi-focus thermal image fusion.
The algorithm is based on local activity analysis and advanced pre-selection of
images into fusion process. The algorithm improves the object temperature
measurement error up to 5 Celsius degrees. The proposed algorithm is evaluated
by half total error rate, root mean squared error, cross correlation and visual
inspection. To the best of our knowledge, this is the first work devoted to
multi-focus thermal image fusion. For testing of proposed algorithm we acquire
six thermal image set with objects at different focal depth.",None,-1
3c489e5a-78cb-4017-a005-598ff2a5199f,CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection,0.568435,"Video anomaly detection (VAD) -- commonly formulated as a multiple-instance
learning problem in a weakly-supervised manner due to its labor-intensive
nature -- is a challenging problem in video surveillance where the frames of
anomaly need to be localized in an untrimmed video. In this paper, we first
propose to utilize the ViT-encoded visual features from CLIP, in contrast with
the conventional C3D or I3D features in the domain, to efficiently extract
discriminative representations in the novel technique. We then model temporal
dependencies and nominate the snippets of interest by leveraging our proposed
Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of
TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA
outperforms the existing state-of-the-art (SOTA) methods by a large margin on
three commonly-used benchmark datasets in the VAD problem (UCF-Crime,
ShanghaiTech Campus, and XD-Violence). Our source code is available at
https://github.com/joos2010kj/CLIP-TSA.",https://github.com/joos2010kj/CLIP-TSA,-1
833d1c10-c160-483e-be7e-1bc754a97e5e,Asking the Right Questions in Low Resource Template Extraction,0.41027,"Information Extraction (IE) researchers are mapping tasks to Question
Answering (QA) in order to leverage existing large QA resources, and thereby
improve data efficiency. Especially in template extraction (TE), mapping an
ontology to a set of questions can be more time-efficient than collecting
labeled examples. We ask whether end users of TE systems can design these
questions, and whether it is beneficial to involve an NLP practitioner in the
process. We compare questions to other ways of phrasing natural language
prompts for TE. We propose a novel model to perform TE with prompts, and find
it benefits from questions over other styles of prompts, and that they do not
require an NLP background to author.",None,-1
89e2f24f-bb32-4fc2-bb55-71fe9a2aec42,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,1.0,"We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization. We explore an iterated online mode of
training, where preference models and RL policies are updated on a weekly
cadence with fresh human feedback data, efficiently improving our datasets and
models. Finally, we investigate the robustness of RLHF training, and identify a
roughly linear relation between the RL reward and the square root of the KL
divergence between the policy and its initialization. Alongside our main
results, we perform peripheral analyses on calibration, competing objectives,
and the use of OOD detection, compare our models with human writers, and
provide samples from our models using prompts appearing in recent related work.",https://github.com/anthropics/hh-rlhf,-1
5c48a80a-ccdb-42b8-845e-8302f3ea7cee,On the Effects of Image Quality Degradation on Minutiae- and Ridge-Based Automatic Fingerprint Recognition,0.918271,"The effect of image quality degradation on the verification performance of
automatic fingerprint recognition is investigated. We study the performance of
two fingerprint matchers based on minutiae and ridge information under varying
fingerprint image quality. The ridge-based system is found to be more robust to
image quality degradation than the minutiae-based system for a number of
different image quality criteria.",None,-1
8b487b20-730b-4ca2-aa73-98b0c007ce2f,KETOD: Knowledge-Enriched Task-Oriented Dialogue,0.911552,"Existing studies in dialogue system research mostly treat task-oriented
dialogue and chit-chat as separate domains. Towards building a human-like
assistant that can converse naturally and seamlessly with users, it is
important to build a dialogue system that conducts both types of conversations
effectively. In this work, we investigate how task-oriented dialogue and
knowledge-grounded chit-chat can be effectively integrated into a single model.
To this end, we create a new dataset, KETOD (Knowledge-Enriched Task-Oriented
Dialogue), where we naturally enrich task-oriented dialogues with chit-chat
based on relevant entity knowledge. We also propose two new models,
SimpleToDPlus and Combiner, for the proposed task. Experimental results on both
automatic and human evaluations show that the proposed methods can
significantly improve the performance in knowledge-enriched response generation
while maintaining a competitive task-oriented dialog performance. We believe
our new dataset will be a valuable resource for future studies. Our dataset and
code are publicly available at \url{https://github.com/facebookresearch/ketod}.",https://github.com/facebookresearch/ketod,-1
04a9acd4-f7e5-44c5-9787-fe77f54e16df,3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization,0.617571,"Although deep-learning based methods for monocular pedestrian detection have
made great progress, they are still vulnerable to heavy occlusions. Using
multi-view information fusion is a potential solution but has limited
applications, due to the lack of annotated training samples in existing
multi-view datasets, which increases the risk of overfitting. To address this
problem, a data augmentation method is proposed to randomly generate 3D
cylinder occlusions, on the ground plane, which are of the average size of
pedestrians and projected to multiple views, to relieve the impact of
overfitting in the training. Moreover, the feature map of each view is
projected to multiple parallel planes at different heights, by using
homographies, which allows the CNNs to fully utilize the features across the
height of each pedestrian to infer the locations of pedestrians on the ground
plane. The proposed 3DROM method has a greatly improved performance in
comparison with the state-of-the-art deep-learning based methods for multi-view
pedestrian detection.",https://github.com/xjtlu-cvlab/3DROM,5075
3ff6da70-d4c8-4a18-a326-7f2b3d73449d,Visually-Augmented Language Modeling,0.70736,"Human language is grounded on multimodal knowledge including visual knowledge
like colors, sizes, and shapes. However, current large-scale pre-trained
language models rely on text-only self-supervised training with massive text
data, which precludes them from utilizing relevant visual information when
necessary. To address this, we propose a novel pre-training framework, named
VaLM, to Visually-augment text tokens with retrieved relevant images for
Language Modeling. Specifically, VaLM builds on a novel latent text-image
alignment method via an image retrieval module to fetch corresponding images
given a textual context. With the visually-augmented context, VaLM uses a
visual knowledge fusion layer to enable multimodal grounded language modeling
by attending to both text context and visual knowledge in images. We evaluate
VaLM on various visual knowledge-intensive commonsense reasoning tasks, which
require visual information to excel. The experimental results illustrate that
VaLM outperforms all strong language-only and vision-language baselines with
substantial gains in reasoning object commonsense including color, size, and
shape. Our code is available at https://github.com/Victorwz/VaLM.",https://github.com/Victorwz/VaLM,-1
990cb83b-3d0d-4a6a-a1b5-004733ea81c4,Realistic Defocus Blur for Multiplane Computer-Generated Holography,0.778965,"This paper introduces a new multiplane CGH computation method to reconstruct
artefact-free high-quality holograms with natural-looking defocus blur. Our
method introduces a new targeting scheme and a new loss function. While the
targeting scheme accounts for defocused parts of the scene at each depth plane,
the new loss function analyzes focused and defocused parts separately in
reconstructed images. Our method support phase-only CGH calculations using
various iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative
(e.g., Double Phase) CGH techniques. We achieve our best image quality using a
modified gradient descent-based optimization recipe where we introduce a
constraint inspired by the double phase method. We validate our method
experimentally using our proof-of-concept holographic display, comparing
various algorithms, including multi-depth scenes with sparse and dense
contents.",None,-1
5e71df5d-1b60-428c-a1a9-f7301af3923c,A Safety Assurable Human-Inspired Perception Architecture,0.0627888,"Although artificial intelligence-based perception (AIP) using deep neural
networks (DNN) has achieved near human level performance, its well-known
limitations are obstacles to the safety assurance needed in autonomous
applications. These include vulnerability to adversarial inputs, inability to
handle novel inputs and non-interpretability. While research in addressing
these limitations is active, in this paper, we argue that a fundamentally
different approach is needed to address them. Inspired by dual process models
of human cognition, where Type 1 thinking is fast and non-conscious while Type
2 thinking is slow and based on conscious reasoning, we propose a dual process
architecture for safe AIP. We review research on how humans address the
simplest non-trivial perception problem, image classification, and sketch a
corresponding AIP architecture for this task. We argue that this architecture
can provide a systematic way of addressing the limitations of AIP using DNNs
and an approach to assurance of human-level performance and beyond. We conclude
by discussing what components of the architecture may already be addressed by
existing work and what remains future work.",None,-1
84e87e5c-9534-45e1-a98a-344a8b4e6b92,CaMEL: Mean Teacher Learning for Image Captioning,0.463523,"Describing images in natural language is a fundamental step towards the
automatic modeling of connections between the visual and textual modalities. In
this paper we present CaMEL, a novel Transformer-based architecture for image
captioning. Our proposed approach leverages the interaction of two
interconnected language models that learn from each other during the training
phase. The interplay between the two language models follows a mean teacher
learning paradigm with knowledge distillation. Experimentally, we assess the
effectiveness of the proposed solution on the COCO dataset and in conjunction
with different visual feature extractors. When comparing with existing
proposals, we demonstrate that our model provides state-of-the-art caption
quality with a significantly reduced number of parameters. According to the
CIDEr metric, we obtain a new state of the art on COCO when training without
using external data. The source code and trained models are publicly available
at: https://github.com/aimagelab/camel.",https://github.com/aimagelab/camel,-1
510338b9-49fb-498e-856a-8e9eb073fc9a,Probing Speech Emotion Recognition Transformers for Linguistic Knowledge,0.820586,"Large, pre-trained neural networks consisting of self-attention layers
(transformers) have recently achieved state-of-the-art results on several
speech emotion recognition (SER) datasets. These models are typically
pre-trained in self-supervised manner with the goal to improve automatic speech
recognition performance -- and thus, to understand linguistic information. In
this work, we investigate the extent in which this information is exploited
during SER fine-tuning. Using a reproducible methodology based on open-source
tools, we synthesise prosodically neutral speech utterances while varying the
sentiment of the text. Valence predictions of the transformer model are very
reactive to positive and negative sentiment content, as well as negations, but
not to intensifiers or reducers, while none of those linguistic features impact
arousal or dominance. These findings show that transformers can successfully
leverage linguistic information to improve their valence predictions, and that
linguistic analysis should be included in their testing.",https://github.com/espnet/espnet,-1
924df615-d2e4-4da3-8b75-05e49d17e362,Region2Vec: Community Detection on Spatial Networks Using Graph Embedding with Node Attributes and Spatial Interactions,0.345791,"Community Detection algorithms are used to detect densely connected
components in complex networks and reveal underlying relationships among
components. As a special type of networks, spatial networks are usually
generated by the connections among geographic regions. Identifying the spatial
network communities can help reveal the spatial interaction patterns,
understand the hidden regional structures and support regional development
decision-making. Given the recent development of Graph Convolutional Networks
(GCN) and its powerful performance in identifying multi-scale spatial
interactions, we proposed an unsupervised GCN-based community detection method
""region2vec"" on spatial networks. Our method first generates node embeddings
for regions that share common attributes and have intense spatial interactions,
and then applies clustering algorithms to detect communities based on their
embedding similarity and spatial adjacency. Experimental results show that
while existing methods trade off either attribute similarities or spatial
interactions for one another, ""region2vec"" maintains a great balance between
both and performs the best when one wants to maximize both attribute
similarities and spatial interactions within communities.",None,-1
5a1b5cf0-27e7-429b-ba1e-64611d565f70,Probabilistic Volumetric Fusion for Dense Monocular SLAM,0.358958,"We present a novel method to reconstruct 3D scenes from images by leveraging
deep dense monocular SLAM and fast uncertainty propagation. The proposed
approach is able to 3D reconstruct scenes densely, accurately, and in real-time
while being robust to extremely noisy depth estimates coming from dense
monocular SLAM. Differently from previous approaches, that either use ad-hoc
depth filters, or that estimate the depth uncertainty from RGB-D cameras'
sensor models, our probabilistic depth uncertainty derives directly from the
information matrix of the underlying bundle adjustment problem in SLAM. We show
that the resulting depth uncertainty provides an excellent signal to weight the
depth-maps for volumetric fusion. Without our depth uncertainty, the resulting
mesh is noisy and with artifacts, while our approach generates an accurate 3D
mesh with significantly fewer artifacts. We provide results on the challenging
Euroc dataset, and show that our approach achieves 92% better accuracy than
directly fusing depths from monocular SLAM, and up to 90% improvements compared
to the best competing approach.",None,-1
38ed675e-e56f-46ce-92c2-d7f033b6e41f,WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution,0.55241,"Exploiting image patches instead of whole images have proved to be a powerful
approach to tackle various problems in image processing. Recently, Wasserstein
patch priors (WPP), which are based on the comparison of the patch
distributions of the unknown image and a reference image, were successfully
used as data-driven regularizers in the variational formulation of
superresolution. However, for each input image, this approach requires the
solution of a non-convex minimization problem which is computationally costly.
In this paper, we propose to learn two kind of neural networks in an
unsupervised way based on WPP loss functions. First, we show how convolutional
neural networks (CNNs) can be incorporated. Once the network, called WPPNet, is
learned, it can be very efficiently applied to any input image. Second, we
incorporate conditional normalizing flows to provide a tool for uncertainty
quantification. Numerical examples demonstrate the very good performance of
WPPNets for superresolution in various image classes even if the forward
operator is known only approximately.",https://github.com/FabianAltekrueger/WPPNets,-1
5e23df06-91d0-42a6-ad22-fd5c71646f15,Explainability in reinforcement learning: perspective and position,0.467401,"Artificial intelligence (AI) has been embedded into many aspects of people's
daily lives and it has become normal for people to have AI make decisions for
them. Reinforcement learning (RL) models increase the space of solvable
problems with respect to other machine learning paradigms. Some of the most
interesting applications are in situations with non-differentiable expected
reward function, operating in unknown or underdefined environment, as well as
for algorithmic discovery that surpasses performance of any teacher, whereby
agent learns from experimental experience through simple feedback. The range of
applications and their social impact is vast, just to name a few: genomics,
game-playing (chess, Go, etc.), general optimization, financial investment,
governmental policies, self-driving cars, recommendation systems, etc. It is
therefore essential to improve the trust and transparency of RL-based systems
through explanations. Most articles dealing with explainability in artificial
intelligence provide methods that concern supervised learning and there are
very few articles dealing with this in the area of RL. The reasons for this are
the credit assignment problem, delayed rewards, and the inability to assume
that data is independently and identically distributed (i.i.d.). This position
paper attempts to give a systematic overview of existing methods in the
explainable RL area and propose a novel unified taxonomy, building and
expanding on the existing ones. The position section describes pragmatic
aspects of how explainability can be observed. The gap between the parties
receiving and generating the explanation is especially emphasized. To reduce
the gap and achieve honesty and truthfulness of explanations, we set up three
pillars: proactivity, risk attitudes, and epistemological constraints. To this
end, we illustrate our proposal on simple variants of the shortest path
problem.",None,-1
94b45400-de3d-4d2d-bfd9-8e8d26ebd452,Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning,0.992297,"Providing Emotional Support (ES) to soothe people in emotional distress is an
essential capability in social interactions. Most existing researches on
building ES conversation systems only considered single-turn interactions with
users, which was over-simplified. In comparison, multi-turn ES conversation
systems can provide ES more effectively, but face several new technical
challenges, including: (1) how to adopt appropriate support strategies to
achieve the long-term dialogue goal of comforting the user's emotion; (2) how
to dynamically model the user's state. In this paper, we propose a novel system
MultiESC to address these issues. For strategy planning, drawing inspiration
from the A* search algorithm, we propose lookahead heuristics to estimate the
future user feedback after using particular strategies, which helps to select
strategies that can lead to the best long-term effects. For user state
modeling, MultiESC focuses on capturing users' subtle emotional expressions and
understanding their emotion causes. Extensive experiments show that MultiESC
significantly outperforms competitive baselines in both dialogue generation and
strategy planning. Our codes are available at
https://github.com/lwgkzl/MultiESC.",https://github.com/lwgkzl/MultiESC,-1
8ca7daee-a019-4554-945f-695e016d028c,Spiking Approximations of the MaxPooling Operation in Deep SNNs,0.454638,"Spiking Neural Networks (SNNs) are an emerging domain of biologically
inspired neural networks that have shown promise for low-power AI. A number of
methods exist for building deep SNNs, with Artificial Neural Network
(ANN)-to-SNN conversion being highly successful. MaxPooling layers in
Convolutional Neural Networks (CNNs) are an integral component to downsample
the intermediate feature maps and introduce translational invariance, but the
absence of their hardware-friendly spiking equivalents limits such CNNs'
conversion to deep SNNs. In this paper, we present two hardware-friendly
methods to implement Max-Pooling in deep SNNs, thus facilitating easy
conversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute
SNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware
(with MNIST, FMNIST, & CIFAR10 dataset); thus, showing the feasibility of our
approach.",None,-1
baa110cc-3ca9-4698-a017-fc16899db54f,Towards Real-time High-Definition Image Snow Removal: Efficient Pyramid Network with Asymmetrical Encoder-decoder Architecture,0.479408,"In winter scenes, the degradation of images taken under snow can be pretty
complex, where the spatial distribution of snowy degradation is varied from
image to image. Recent methods adopt deep neural networks to directly recover
clean scenes from snowy images. However, due to the paradox caused by the
variation of complex snowy degradation, achieving reliable High-Definition
image desnowing performance in real time is a considerable challenge. We
develop a novel Efficient Pyramid Network with asymmetrical encoder-decoder
architecture for real-time HD image desnowing. The general idea of our proposed
network is to utilize the multi-scale feature flow fully and implicitly mine
clean cues from features. Compared with previous state-of-the-art desnowing
methods, our approach achieves a better complexity-performance trade-off and
effectively handles the processing difficulties of HD and Ultra-HD images.
  The extensive experiments on three large-scale image desnowing datasets
demonstrate that our method surpasses all state-of-the-art approaches by a
large margin both quantitatively and qualitatively, boosting the PSNR metric
from 31.76 dB to 34.10 dB on the CSD test dataset and from 28.29 dB to 30.87 dB
on the SRRS test dataset.",None,-1
d533e4be-fc3e-4eba-932b-1a256ead9ebb,ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection,0.913479,"Hate speech detection is complex; it relies on commonsense reasoning,
knowledge of stereotypes, and an understanding of social nuance that differs
from one culture to the next. It is also difficult to collect a large-scale
hate speech annotated dataset. In this work, we frame this problem as a
few-shot learning task, and show significant gains with decomposing the task
into its ""constituent"" parts. In addition, we see that infusing knowledge from
reasoning datasets (e.g. Atomic2020) improves the performance even further.
Moreover, we observe that the trained models generalize to out-of-distribution
datasets, showing the superiority of task decomposition and knowledge infusion
compared to previously used methods. Concretely, our method outperforms the
baseline by 17.83% absolute gain in the 16-shot case.",None,-1
f72d5d14-6316-405b-8ae9-4db8730e1287,Pre-Training With Scientific Text Improves Educational Question Generation,0.187946,"With the boom of digital educational materials and scalable e-learning
systems, the potential for realising AI-assisted personalised learning has
skyrocketed. In this landscape, the automatic generation of educational
questions will play a key role, enabling scalable self-assessment when a global
population is manoeuvring their personalised learning journeys. We develop
EduQG, a novel educational question generation model built by adapting a large
language model. Our initial experiments demonstrate that EduQG can produce
superior educational questions by pre-training on scientific text.",None,-1
891b38a2-569d-437b-b13b-fef9c75a12d4,Photo-realistic 360 Head Avatars in the Wild,0.112387,"Delivering immersive, 3D experiences for human communication requires a
method to obtain 360 degree photo-realistic avatars of humans. To make these
experiences accessible to all, only commodity hardware, like mobile phone
cameras, should be necessary to capture the data needed for avatar creation.
For avatars to be rendered realistically from any viewpoint, we require
training images and camera poses from all angles. However, we cannot rely on
there being trackable features in the foreground or background of all images
for use in estimating poses, especially from the side or back of the head. To
overcome this, we propose a novel landmark detector trained on synthetic data
to estimate camera poses from 360 degree mobile phone videos of a human head
for use in a multi-stage optimization process which creates a photo-realistic
avatar. We perform validation experiments with synthetic data and showcase our
method on 360 degree avatars trained from mobile phone videos.",None,-1
7858b2e4-8800-48e1-b230-4aaeb32bf9fb,Entailment Semantics Can Be Extracted from an Ideal Language Model,0.401227,"Language models are often trained on text alone, without additional
grounding. There is debate as to how much of natural language semantics can be
inferred from such a procedure. We prove that entailment judgments between
sentences can be extracted from an ideal language model that has perfectly
learned its target distribution, assuming the training sentences are generated
by Gricean agents, i.e., agents who follow fundamental principles of
communication from the linguistic theory of pragmatics. We also show entailment
judgments can be decoded from the predictions of a language model trained on
such Gricean data. Our results reveal a pathway for understanding the semantic
information encoded in unlabeled linguistic data and a potential framework for
extracting semantics from language models.",https://github.com/viking-sudo-rm/,-1
c4535629-251a-4034-8b56-54f7c00b4e34,Controlled Language Generation for Language Learning Items,0.0551641,"This work aims to employ natural language generation (NLG) to rapidly
generate items for English language learning applications: this requires both
language models capable of generating fluent, high-quality English, and to
control the output of the generation to match the requirements of the relevant
items. We experiment with deep pretrained models for this task, developing
novel methods for controlling items for factors relevant in language learning:
diverse sentences for different proficiency levels and argument structure to
test grammar. Human evaluation demonstrates high grammatically scores for all
models (3.4 and above out of 4), and higher length (24%) and complexity (9%)
over the baseline for the advanced proficiency model. Our results show that we
can achieve strong performance while adding additional control to ensure
diverse, tailored content for individual users.",https://github.com/EducationalTestingService/concept-control-gen,-1
646f407a-acdf-4a6b-af0d-6e8e45a0b993,Robust-by-Design Classification via Unitary-Gradient Neural Networks,0.399549,"The use of neural networks in safety-critical systems requires safe and
robust models, due to the existence of adversarial attacks. Knowing the minimal
adversarial perturbation of any input x, or, equivalently, knowing the distance
of x from the classification boundary, allows evaluating the classification
robustness, providing certifiable predictions. Unfortunately, state-of-the-art
techniques for computing such a distance are computationally expensive and
hence not suited for online applications. This work proposes a novel family of
classifiers, namely Signed Distance Classifiers (SDCs), that, from a
theoretical perspective, directly output the exact distance of x from the
classification boundary, rather than a probability score (e.g., SoftMax). SDCs
represent a family of robust-by-design classifiers. To practically address the
theoretical requirements of a SDC, a novel network architecture named
Unitary-Gradient Neural Network is presented. Experimental results show that
the proposed architecture approximates a signed distance classifier, hence
allowing an online certifiable classification of x at the cost of a single
inference.",None,-1
00fb004d-d6b8-4aa6-8cd2-8a7bf88c6549,Training Language Models with Language Feedback,0.746939,"Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level summarization
ability.",None,150035
de50c004-2471-45f1-9a72-d11beaf0eb4e,Acknowledging the Unknown for Multi-label Learning with Single Positive Labels,0.90977,"Due to the difficulty of collecting exhaustive multi-label annotations,
multi-label datasets often contain partial labels. We consider an extreme of
this weakly supervised learning problem, called single positive multi-label
learning (SPML), where each multi-label training image has only one positive
label. Traditionally, all unannotated labels are assumed as negative labels in
SPML, which introduces false negative labels and causes model training to be
dominated by assumed negative labels. In this work, we choose to treat all
unannotated labels from an alternative perspective, i.e. acknowledging they are
unknown. Hence, we propose entropy-maximization (EM) loss to attain a special
gradient regime for providing proper supervision signals. Moreover, we propose
asymmetric pseudo-labeling (APL), which adopts asymmetric-tolerance strategies
and a self-paced procedure, to cooperate with EM loss and then provide more
precise supervision. Experiments show that our method significantly improves
performance and achieves state-of-the-art results on all four benchmarks. Code
is available at https://github.com/Correr-Zhou/SPML-AckTheUnknown.",https://github.com/Correr-Zhou/SPML-AckTheUnknown,-1
01ec561b-637e-4bf1-858d-0c2bafdc3aa6,BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation,0.711012,"We present BlazePose GHUM Holistic, a lightweight neural network pipeline for
3D human body landmarks and pose estimation, specifically tailored to real-time
on-device inference. BlazePose GHUM Holistic enables motion capture from a
single RGB image including avatar control, fitness tracking and AR/VR effects.
Our main contributions include i) a novel method for 3D ground truth data
acquisition, ii) updated 3D body tracking with additional hand landmarks and
iii) full body pose estimation from a monocular image.",https://mediapipe.dev,19004
b0fe0505-cfd1-44ab-a17e-2c5f0fd18992,Categorizing Semantic Representations for Neural Machine Translation,0.339193,"Modern neural machine translation (NMT) models have achieved competitive
performance in standard benchmarks. However, they have recently been shown to
suffer limitation in compositional generalization, failing to effectively learn
the translation of atoms (e.g., words) and their semantic composition (e.g.,
modification) from seen compounds (e.g., phrases), and thus suffering from
significantly weakened translation performance on unseen compounds during
inference. We address this issue by introducing categorization to the source
contextualized representations. The main idea is to enhance generalization by
reducing sparsity and overfitting, which is achieved by finding prototypes of
token representations over the training set and integrating their embeddings
into the source encoding. Experiments on a dedicated MT dataset (i.e.,
CoGnition) show that our method reduces compositional generalization error
rates by 24\% error reduction. In addition, our conceptually simple method
gives consistently better results than the Transformer baseline on a range of
general MT datasets.",https://github.com/ARIES-LM/CatMT4CG.git,-1
3d57336e-2f0e-405c-8d11-43ddd5fa561a,Deep Learning Reproducibility and Explainable AI (XAI),0.217851,"The nondeterminism of Deep Learning (DL) training algorithms and its
influence on the explainability of neural network (NN) models are investigated
in this work with the help of image classification examples. To discuss the
issue, two convolutional neural networks (CNN) have been trained and their
results compared. The comparison serves the exploration of the feasibility of
creating deterministic, robust DL models and deterministic explainable
artificial intelligence (XAI) in practice. Successes and limitation of all here
carried out efforts are described in detail. The source code of the attained
deterministic models has been listed in this work. Reproducibility is indexed
as a development-phase-component of the Model Governance Framework, proposed by
the EU within their excellence in AI approach. Furthermore, reproducibility is
a requirement for establishing causality for the interpretation of model
results and building of trust towards the overwhelming expansion of AI systems
applications. Problems that have to be solved on the way to reproducibility and
ways to deal with some of them, are examined in this work.",None,-1
ff9809e7-2a8c-4d2c-8c73-6c61272dc53c,E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning,0.44811,"The ability to recognize analogies is fundamental to human cognition.
Existing benchmarks to test word analogy do not reveal the underneath process
of analogical reasoning of neural models. Holding the belief that models
capable of reasoning should be right for the right reasons, we propose a
first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning
benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in
English) problems sourced from the Civil Service Exams, which require intensive
background knowledge to solve. More importantly, we design a free-text
explanation scheme to explain whether an analogy should be drawn, and manually
annotate them for each and every question and candidate answer. Empirical
results suggest that this benchmark is very challenging for some
state-of-the-art models for both explanation generation and analogical question
answering tasks, which invites further research in this area.",https://github.com/Tiiiger/bert_score,-1
5cb857a9-4b21-4831-8c37-c3a4ab99da18,"Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability",0.745813,"Large-scale pre-training has been proven to be crucial for various computer
vision tasks. However, with the increase of pre-training data amount, model
architecture amount, and the private/inaccessible data, it is not very
efficient or possible to pre-train all the model architectures on large-scale
datasets. In this work, we investigate an alternative strategy for
pre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP),
aiming to efficiently transfer the learned feature representation from existing
pre-trained models to new student models for future downstream tasks. We
observe that existing Knowledge Distillation (KD) methods are unsuitable
towards pre-training since they normally distill the logits that are going to
be discarded when transferred to downstream tasks. To resolve this problem, we
propose a feature-based KD method with non-parametric feature dimension
aligning. Notably, our method performs comparably with supervised pre-training
counterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less
data and 5x less pre-training time. Code is available at
https://github.com/CVMI-Lab/KDEP.",https://github.com/CVMI-Lab/KDEP,-1
510bde2c-fdc4-474a-bfab-2225f7258bbf,Learning MAX-SAT from Contextual Examples for Combinatorial Optimisation,0.411003,"Combinatorial optimisation problems are ubiquitous in artificial
intelligence. Designing the underlying models, however, requires substantial
expertise, which is a limiting factor in practice. The models typically consist
of hard and soft constraints, or combine hard constraints with an objective
function. We introduce a novel setting for learning combinatorial optimisation
problems from contextual examples. These positive and negative examples show -
in a particular context - whether the solutions are good enough or not. We
develop our framework using the MAX-SAT formalism as it is simple yet powerful
setting having these features. We study the learnability of MAX-SAT models. Our
theoretical results show that high-quality MAX-SAT models can be learned from
contextual examples in the realisable and agnostic settings, as long as the
data satisfies an intuitive ""representativeness"" condition. We also contribute
two implementations based on our theoretical results: one leverages ideas from
syntax-guided synthesis while the other makes use of stochastic local search
techniques. The two implementations are evaluated by recovering synthetic and
benchmark models from contextual examples. The experimental results support our
theoretical analysis, showing that MAX-SAT models can be learned from
contextual examples. Among the two implementations, the stochastic local search
learner scales much better than the syntax-guided implementation while
providing comparable or better models.",https://github.com/mohitKULeuven/HassleWithLocalSearch,-1
0ef57b64-2f95-4311-93a5-df553d560f23,CasNet: Investigating Channel Robustness for Speech Separation,0.166911,"Recording channel mismatch between training and testing conditions has been
shown to be a serious problem for speech separation. This situation greatly
reduces the separation performance, and cannot meet the requirement of daily
use. In this study, inheriting the use of our previously constructed TAT-2mix
corpus, we address the channel mismatch problem by proposing a channel-aware
audio separation network (CasNet), a deep learning framework for end-to-end
time-domain speech separation. CasNet is implemented on top of TasNet. Channel
embedding (characterizing channel information in a mixture of multiple
utterances) generated by Channel Encoder is introduced into the separation
module by the FiLM technique. Through two training strategies, we explore two
roles that channel embedding may play: 1) a real-life noise disturbance, making
the model more robust, or 2) a guide, instructing the separation model to
retain the desired channel information. Experimental results on TAT-2mix show
that CasNet trained with both training strategies outperforms the TasNet
baseline, which does not use channel embeddings.",https://github.com/Sinica-SLAM/CasNet,8197
5db72a08-bb4d-4637-97ef-c0ad307a9cbd,Scribble-Supervised LiDAR Semantic Segmentation,0.707702,"Densely annotating LiDAR point clouds remains too expensive and
time-consuming to keep up with the ever growing volume of data. While current
literature focuses on fully-supervised performance, developing efficient
methods that take advantage of realistic weak supervision have yet to be
explored. In this paper, we propose using scribbles to annotate LiDAR point
clouds and release ScribbleKITTI, the first scribble-annotated dataset for
LiDAR semantic segmentation. Furthermore, we present a pipeline to reduce the
performance gap that arises when using such weak annotations. Our pipeline
comprises of three stand-alone contributions that can be combined with any
LiDAR semantic segmentation model to achieve up to 95.7% of the
fully-supervised performance while using only 8% labeled points. Our scribble
annotations and code are available at github.com/ouenal/scribblekitti.",https://github.com/ouenal/scribblekitti,-1
525148e1-3418-457a-a0a4-94f69319dfe1,On the Use of Modality-Specific Large-Scale Pre-Trained Encoders for Multimodal Sentiment Analysis,0.337562,"This paper investigates the effectiveness and implementation of
modality-specific large-scale pre-trained encoders for multimodal sentiment
analysis~(MSA). Although the effectiveness of pre-trained encoders in various
fields has been reported, conventional MSA methods employ them for only
linguistic modality, and their application has not been investigated. This
paper compares the features yielded by large-scale pre-trained encoders with
conventional heuristic features. One each of the largest pre-trained encoders
publicly available for each modality are used; CLIP-ViT, WavLM, and BERT for
visual, acoustic, and linguistic modalities, respectively. Experiments on two
datasets reveal that methods with domain-specific pre-trained encoders attain
better performance than those with conventional features in both unimodal and
multimodal scenarios. We also find it better to use the outputs of the
intermediate layers of the encoders than those of the output layer. The codes
are available at https://github.com/ando-hub/MSA_Pretrain.",https://github.com/ando-hub/MSA_Pretrain,-1
2533ff1d-9667-416c-9311-70bdf6406b47,Attention Based Neural Networks for Wireless Channel Estimation,0.76809,"In this paper, we deploy the self-attention mechanism to achieve improved
channel estimation for orthogonal frequency-division multiplexing waveforms in
the downlink. Specifically, we propose a new hybrid encoder-decoder structure
(called HA02) for the first time which exploits the attention mechanism to
focus on the most important input information. In particular, we implement a
transformer encoder block as the encoder to achieve the sparsity in the input
features and a residual neural network as the decoder respectively, inspired by
the success of the attention mechanism. Using 3GPP channel models, our
simulations show superior estimation performance compared with other candidate
neural network methods for channel estimation.",None,18255
1b717b8f-0ca2-4403-9162-9ac431391ea9,Place Recognition under Occlusion and Changing Appearance via Disentangled Representations,0.100777,"Place recognition is a critical and challenging task for mobile robots,
aiming to retrieve an image captured at the same place as a query image from a
database. Existing methods tend to fail while robots move autonomously under
occlusion (e.g., car, bus, truck) and changing appearance (e.g., illumination
changes, seasonal variation). Because they encode the image into only one code,
entangling place features with appearance and occlusion features. To overcome
this limitation, we propose PROCA, an unsupervised approach to decompose the
image representation into three codes: a place code used as a descriptor to
retrieve images, an appearance code that captures appearance properties, and an
occlusion code that encodes occlusion content. Extensive experiments show that
our model outperforms the state-of-the-art methods. Our code and data are
available at https://github.com/rover-xingyu/PROCA.",https://github.com/rover-xingyu/PROCA,204
807ebd9c-b0e7-4b2b-84f6-707e00661ab7,Learning One Abstract Bit at a Time Through Self-Invented Experiments Encoded as Neural Networks,0.0618876,"There are two important things in science: (A) Finding answers to given
questions, and (B) Coming up with good questions. Our artificial scientists not
only learn to answer given questions, but also continually invent new
questions, by proposing hypotheses to be verified or falsified through
potentially complex and time-consuming experiments, including thought
experiments akin to those of mathematicians. While an artificial scientist
expands its knowledge, it remains biased towards the simplest, least costly
experiments that still have surprising outcomes, until they become boring. We
present an empirical analysis of the automatic generation of interesting
experiments. In the first setting, we investigate self-invented experiments in
a reinforcement-providing environment and show that they lead to effective
exploration. In the second setting, pure thought experiments are implemented as
the weights of recurrent neural networks generated by a neural experiment
generator. Initially interesting thought experiments may become boring over
time.",None,-1
a130c686-62d3-45d7-ac85-6c91c1622496,Cross-Architecture Self-supervised Video Representation Learning,0.612859,"In this paper, we present a new cross-architecture contrastive learning
(CACL) framework for self-supervised video representation learning. CACL
consists of a 3D CNN and a video transformer which are used in parallel to
generate diverse positive pairs for contrastive learning. This allows the model
to learn strong representations from such diverse yet meaningful pairs.
Furthermore, we introduce a temporal self-supervised learning module able to
predict an Edit distance explicitly between two video sequences in the temporal
order. This enables the model to learn a rich temporal representation that
compensates strongly to the video-level representation learned by the CACL. We
evaluate our method on the tasks of video retrieval and action recognition on
UCF101 and HMDB51 datasets, where our method achieves excellent performance,
surpassing the state-of-the-art methods such as VideoMoCo and MoCo+BE by a
large margin. The code is made available at https://github.com/guoshengcv/CACL.",https://github.com/guoshengcv/CACL,-1
5e7e08bf-5f7a-4786-b252-43a23049517c,Character-Aware Models Improve Visual Text Rendering,0.746574,"Current image generation models struggle to reliably produce well-formed
visual text. In this paper, we investigate a key contributing factor: popular
text-to-image models lack character-level input features, making it much harder
to predict a word's visual makeup as a series of glyphs. To quantify this
effect, we conduct a series of experiments comparing character-aware vs.
character-blind text encoders. In the text-only domain, we find that
character-aware models provide large gains on a novel spelling task
(WikiSpell). Applying our learnings to the visual domain, we train a suite of
image generation models, and show that character-aware variants outperform
their character-blind counterparts across a range of novel text rendering tasks
(our DrawText benchmark). Our models set a much higher state-of-the-art on
visual spelling, with 30+ point accuracy gains over competitors on rare words,
despite training on far fewer examples.",None,-1
75a5daf3-8c29-4f99-9594-6dbc2991df14,SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning,0.880727,"We present the SUPERB challenge at SLT 2022, which aims at learning
self-supervised speech representation for better performance, generalization,
and efficiency. The challenge builds upon the SUPERB benchmark and implements
metrics to measure the computation requirements of self-supervised learning
(SSL) representation and to evaluate its generalizability and performance
across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive
coverage of popular speech processing tasks, from speech and speaker
recognition to audio generation and semantic understanding. As SSL has gained
interest in the speech community and showed promising outcomes, we envision the
challenge to uplevel the impact of SSL techniques by motivating more practical
designs of techniques beyond task performance. We summarize the results of 14
submitted models in this paper. We also discuss the main findings from those
submissions and the future directions of SSL research.",https://github.com/s3prl/s3prl,-1
93c568d7-3681-44e4-b934-e5f0803406e9,Deep object detection for waterbird monitoring using aerial imagery,0.608394,"Monitoring of colonial waterbird nesting islands is essential to tracking
waterbird population trends, which are used for evaluating ecosystem health and
informing conservation management decisions. Recently, unmanned aerial
vehicles, or drones, have emerged as a viable technology to precisely monitor
waterbird colonies. However, manually counting waterbirds from hundreds, or
potentially thousands, of aerial images is both difficult and time-consuming.
In this work, we present a deep learning pipeline that can be used to precisely
detect, count, and monitor waterbirds using aerial imagery collected by a
commercial drone. By utilizing convolutional neural network-based object
detectors, we show that we can detect 16 classes of waterbird species that are
commonly found in colonial nesting islands along the Texas coast. Our
experiments using Faster R-CNN and RetinaNet object detectors give mean
interpolated average precision scores of 67.9% and 63.1% respectively.",https://github.com/RiceD2KLab/Audubon F21,-1
77381d0a-fb5c-4a61-9522-1182a59a196c,FAPM: Fast Adaptive Patch Memory for Real-time Industrial Anomaly Detection,0.923662,"Feature embedding-based methods have shown exceptional performance in
detecting industrial anomalies by comparing features of target images with
normal images. However, some methods do not meet the speed requirements of
real-time inference, which is crucial for real-world applications. To address
this issue, we propose a new method called Fast Adaptive Patch Memory (FAPM)
for real-time industrial anomaly detection. FAPM utilizes patch-wise and
layer-wise memory banks that store the embedding features of images at the
patch and layer level, respectively, which eliminates unnecessary repetitive
computations. We also propose patch-wise adaptive coreset sampling for faster
and more accurate detection. FAPM performs well in both accuracy and speed
compared to other state-of-the-art methods",None,-1
6ffa664c-1a65-42ce-84f9-268ca3fb1251,Emotional Speech Recognition with Pre-trained Deep Visual Models,0.116947,"In this paper, we propose a new methodology for emotional speech recognition
using visual deep neural network models. We employ the transfer learning
capabilities of the pre-trained computer vision deep models to have a mandate
for the emotion recognition in speech task. In order to achieve that, we
propose to use a composite set of acoustic features and a procedure to convert
them into images. Besides, we present a training paradigm for these models
taking into consideration the different characteristics between acoustic-based
images and regular ones. In our experiments, we use the pre-trained VGG-16
model and test the overall methodology on the Berlin EMO-DB dataset for
speaker-independent emotion recognition. We evaluate the proposed model on the
full list of the seven emotions and the results set a new state-of-the-art.",https://github.com/mehdi-mirzapour/Emotional_Speech_Recognition,-1
3d5dc918-b4e1-48bb-af7a-6c3bc84e09f7,N-Best Hypotheses Reranking for Text-To-SQL Systems,0.861995,"Text-to-SQL task maps natural language utterances to structured queries that
can be issued to a database. State-of-the-art (SOTA) systems rely on finetuning
large, pre-trained language models in conjunction with constrained decoding
applying a SQL parser. On the well established Spider dataset, we begin with
Oracle studies: specifically, choosing an Oracle hypothesis from a SOTA model's
10-best list, yields a $7.7\%$ absolute improvement in both exact match (EM)
and execution (EX) accuracy, showing significant potential improvements with
reranking. Identifying coherence and correctness as reranking approaches, we
design a model generating a query plan and propose a heuristic schema linking
algorithm. Combining both approaches, with T5-Large, we obtain a consistent
$1\% $ improvement in EM accuracy, and a $~2.5\%$ improvement in EX,
establishing a new SOTA for this task. Our comprehensive error studies on DEV
data show the underlying difficulty in making progress on this task.",https://github.com/microsoft/DeepSpeed,-1
c482ad23-bc50-4ab6-8769-deb64803ed96,DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization,0.804437,"Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve
state-of-the-art performance on many generative NLP tasks. However, such models
pose a great challenge in resource-constrained scenarios owing to their large
memory requirements and high latency. To alleviate this issue, we propose to
jointly distill and quantize the model, where knowledge is transferred from the
full-precision teacher model to the quantized and distilled low-precision
student model. Empirical analyses show that, despite the challenging nature of
generative tasks, we were able to achieve a 16.5x model footprint compression
ratio with little performance drop relative to the full-precision counterparts
on multiple summarization and QA datasets. We further pushed the limit of
compression ratio to 27.7x and presented the performance-efficiency trade-off
for generative tasks using pre-trained models. To the best of our knowledge,
this is the first work aiming to effectively distill and quantize
sequence-to-sequence pre-trained models for language generation tasks.",https://www.github.com/amazon-research/dq-bart/,-1
fefce975-423d-4540-b3aa-3c586b7b453a,HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing,0.45056,"Deep learning algorithms are dependent on the availability of large-scale
annotated clinical text datasets. The lack of such publicly available datasets
is the biggest bottleneck for the development of clinical Natural Language
Processing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep
learning models to classify instances from new classes of which no training
data have been seen before. Prompt-based learning is an emerging ZSL technique
where we define task-based templates for NLP tasks. We developed a novel
prompt-based clinical NLP framework called HealthPrompt and applied the
paradigm of prompt-based learning on clinical texts. In this technique, rather
than fine-tuning a Pre-trained Language Model(PLM), the task definitions are
tuned by defining a prompt template. We performed an in-depth analysis of
HealthPrompt on six different PLMs in a no-data setting. Our experiments prove
that prompts effectively capture the context of clinical texts and perform
remarkably well without any training data.",None,-1
fa3e13ef-799b-4a88-a9fa-91b49ac70ae3,Interpretable Hidden Markov Model-Based Deep Reinforcement Learning Hierarchical Framework for Predictive Maintenance of Turbofan Engines,0.676536,"An open research question in deep reinforcement learning is how to focus the
policy learning of key decisions within a sparse domain. This paper emphasizes
combining the advantages of inputoutput hidden Markov models and reinforcement
learning towards interpretable maintenance decisions. We propose a novel
hierarchical-modeling methodology that, at a high level, detects and interprets
the root cause of a failure as well as the health degradation of the turbofan
engine, while, at a low level, it provides the optimal replacement policy. It
outperforms the baseline performance of deep reinforcement learning methods
applied directly to the raw data or when using a hidden Markov model without
such a specialized hierarchy. It also provides comparable performance to prior
work, however, with the additional benefit of interpretability.",None,-1
1fa2eeae-b51c-49ee-a1f4-c1979de3e869,A Contrastive Objective for Learning Disentangled Representations,0.149972,"Learning representations of images that are invariant to sensitive or
unwanted attributes is important for many tasks including bias removal and
cross domain retrieval. Here, our objective is to learn representations that
are invariant to the domain (sensitive attribute) for which labels are
provided, while being informative over all other image attributes, which are
unlabeled. We present a new approach, proposing a new domain-wise contrastive
objective for ensuring invariant representations. This objective crucially
restricts negative image pairs to be drawn from the same domain, which enforces
domain invariance whereas the standard contrastive objective does not. This
domain-wise objective is insufficient on its own as it suffers from shortcut
solutions resulting in feature suppression. We overcome this issue by a
combination of a reconstruction constraint, image augmentations and
initialization with pre-trained weights. Our analysis shows that the choice of
augmentations is important, and that a misguided choice of augmentations can
harm the invariance and informativeness objectives. In an extensive evaluation,
our method convincingly outperforms the state-of-the-art in terms of
representation invariance, representation informativeness, and training speed.
Furthermore, we find that in some cases our method can achieve excellent
results even without the reconstruction constraint, leading to a much faster
and resource efficient training.",https://github.com/jonkahana/DCoDR,3148
0a6c6b80-ce63-43be-b0ac-276e35ba5603,Agile Maneuvers in Legged Robots: a Predictive Control Approach,0.826171,"Planning and execution of agile locomotion maneuvers have been a longstanding
challenge in legged robotics. It requires to derive motion plans and local
feedback policies in real-time to handle the nonholonomy of the kinetic
momenta. To achieve so, we propose a hybrid predictive controller that
considers the robot's actuation limits and full-body dynamics. It combines the
feedback policies with tactile information to locally predict future actions.
It converges within a few milliseconds thanks to a feasibility-driven approach.
Our predictive controller enables ANYmal robots to generate agile maneuvers in
realistic scenarios. A crucial element is to track the local feedback policies
as, in contrast to whole-body control, they achieve the desired angular
momentum. To the best of our knowledge, our predictive controller is the first
to handle actuation limits, generate agile locomotion maneuvers, and execute
optimal feedback policies for low level torque control without the use of a
separate whole-body controller.",None,4536
499ef5c4-5bf7-4108-bb3a-75d020cdaf2f,Anomaly detection optimization using big data and deep learning to reduce false-positive,0.561707,"Anomaly-based Intrusion Detection System (IDS) has been a hot research topic
because of its ability to detect new threats rather than only memorized
signatures threats of signature-based IDS. Especially after the availability of
advanced technologies that increase the number of hacking tools and increase
the risk impact of an attack. The problem of any anomaly-based model is its
high false-positive rate. The high false-positive rate is the reason why
anomaly IDS is not commonly applied in practice. Because anomaly-based models
classify an unseen pattern as a threat where it may be normal but not included
in the training dataset. This type of problem is called overfitting where the
model is not able to generalize. Optimizing Anomaly-based models by having a
big training dataset that includes all possible normal cases may be an optimal
solution but could not be applied in practice. Although we can increase the
number of training samples to include much more normal cases, still we need a
model that has more ability to generalize. In this research paper, we propose
applying deep model instead of traditional models because it has more ability
to generalize. Thus, we will obtain less false-positive by using big data and
deep model. We made a comparison between machine learning and deep learning
algorithms in the optimization of anomaly-based IDS by decreasing the
false-positive rate. We did an experiment on the NSL-KDD benchmark and compared
our results with one of the best used classifiers in traditional learning in
IDS optimization. The experiment shows 10% lower false-positive by using deep
learning instead of traditional learning.",None,-1
87e769a9-203e-4f66-aaae-a9ea3e64103b,What do navigation agents learn about their environment?,0.219913,"Today's state of the art visual navigation agents typically consist of large
deep learning models trained end to end. Such models offer little to no
interpretability about the learned skills or the actions of the agent taken in
response to its environment. While past works have explored interpreting deep
learning models, little attention has been devoted to interpreting embodied AI
systems, which often involve reasoning about the structure of the environment,
target characteristics and the outcome of one's actions. In this paper, we
introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal
and Object Goal navigation agents. We use iSEE to probe the dynamic
representations produced by these agents for the presence of information about
the agent as well as the environment. We demonstrate interesting insights about
navigation agents using iSEE, including the ability to encode reachable
locations (to avoid obstacles), visibility of the target, progress from the
initial spawn location as well as the dramatic effect on the behaviors of
agents when we mask out critical individual neurons. The code is available at:
https://github.com/allenai/iSEE",https://github.com/allenai/iSEE,-1
a97c8998-5204-4fb6-918c-ba12a21dc2e0,SmartFPS: Neural Network based Wireless-inertial fusion positioning system,0.387862,"The current fusion positioning systems are mainly based on filtering
algorithms, such as Kalman filtering or particle filtering. However, the system
complexity of practical application scenarios is often very high, such as noise
modeling in pedestrian inertial navigation systems, or environmental noise
modeling in fingerprint matching and localization algorithms. To solve this
problem, this paper proposes a fusion positioning system based on deep learning
and proposes a transfer learning strategy for improving the performance of
neural network models for samples with different distributions. The results
show that in the whole floor scenario, the average positioning accuracy of the
fusion network is 0.506m. The experiment results of transfer learning show that
the estimation accuracy of the inertial navigation positioning step size and
rotation angle of different pedestrians can be improved by 53.3% on average,
the Bluetooth positioning accuracy of different devices can be improved by
33.4%, and the fusion can be improved by 31.6%.",None,-1
1be27df6-6c17-4b7f-856c-3ff375f48ff6,Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation,0.804774,"We present a simple yet effective self-supervised framework for audio-visual
representation learning, to localize the sound source in videos. To understand
what enables to learn useful representations, we systematically investigate the
effects of data augmentations, and reveal that (1) composition of data
augmentations plays a critical role, i.e. explicitly encouraging the
audio-visual representations to be invariant to various transformations~({\em
transformation invariance}); (2) enforcing geometric consistency substantially
improves the quality of learned representations, i.e. the detected sound source
should follow the same transformation applied on input video frames~({\em
transformation equivariance}). Extensive experiments demonstrate that our model
significantly outperforms previous methods on two sound localization
benchmarks, namely, Flickr-SoundNet and VGG-Sound. Additionally, we also
evaluate audio retrieval and cross-modal retrieval tasks. In both cases, our
self-supervised models demonstrate superior retrieval performances, even
competitive with the supervised approach in audio retrieval. This reveals the
proposed framework learns strong multi-modal representations that are
beneficial to sound localisation and generalization to further applications.
\textit{All codes will be available}.",https://jinxiang-liu.github.io/SSL-TIE/,-1
55e7f9c0-c3b4-4dda-82d8-bad6a929cd1f,Abstraction not Memory: BERT and the English Article System,0.236813,"Article prediction is a task that has long defied accurate linguistic
description. As such, this task is ideally suited to evaluate models on their
ability to emulate native-speaker intuition. To this end, we compare the
performance of native English speakers and pre-trained models on the task of
article prediction set up as a three way choice (a/an, the, zero). Our
experiments with BERT show that BERT outperforms humans on this task across all
articles. In particular, BERT is far superior to humans at detecting the zero
article, possibly because we insert them using rules that the deep neural model
can easily pick up. More interestingly, we find that BERT tends to agree more
with annotators than with the corpus when inter-annotator agreement is high but
switches to agreeing more with the corpus as inter-annotator agreement drops.
We contend that this alignment with annotators, despite being trained on the
corpus, suggests that BERT is not memorising article use, but captures a high
level generalisation of article use akin to human intuition.",https://github.com/H-TayyarMadabushi/Abstraction-not-Memory-BERT-and-the-English-Article-System-NAACL-2022,-1
e64d2a2a-eccf-4205-903d-8b1624c7d971,Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning,0.0749317,"Intelligent virtual assistants are currently designed to perform tasks or
services explicitly mentioned by users, so multiple related domains or tasks
need to be performed one by one through a long conversation with many explicit
intents. Instead, human assistants are capable of reasoning (multiple) implicit
intents based on user utterances via commonsense knowledge, reducing complex
interactions and improving practicality. Therefore, this paper proposes a
framework of multi-domain dialogue systems, which can automatically infer
implicit intents based on user utterances and then perform zero-shot prompting
using a large pre-trained language model to trigger suitable single
task-oriented bots. The proposed framework is demonstrated effective to realize
implicit intents and recommend associated bots in a zero-shot manner.",http://github.com/MiuLab/ImplicitBot,-1
6a79bdda-c4bf-4dd2-8673-30f82d6ef18e,From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,0.756422,"Multi-hop question answering (QA) is a challenging task requiring QA systems
to perform complex reasoning over multiple documents and provide supporting
facts together with the exact answer. Existing works tend to utilize
graph-based reasoning and question decomposition to obtain the reasoning chain,
which inevitably introduces additional complexity and cumulative error to the
system. To address the above issue, we propose a simple yet effective novel
framework, From Easy to Hard (FE2H), to remove distracting information and
obtain better contextual representations for the multi-hop QA task. Inspired by
the iterative document selection process and the progressive learning custom of
humans, FE2H divides both the document selector and reader into two stages
following an easy-to-hard manner. Specifically, we first select the document
most relevant to the question and then utilize the question together with this
document to select other pertinent documents. As for the QA phase, our reader
is first trained on a single-hop QA dataset and then transferred into the
multi-hop QA task. We comprehensively evaluate our model on the popular
multi-hop QA benchmark HotpotQA. Experimental results demonstrate that our
method ourperforms all other methods in the leaderboard of HotpotQA (distractor
setting).",None,-1
e3aca4a2-2a0e-494a-832d-d89de9f71419,Understanding Influence Functions and Datamodels via Harmonic Analysis,0.799181,"Influence functions estimate effect of individual data points on predictions
of the model on test data and were adapted to deep learning in Koh and Liang
[2017]. They have been used for detecting data poisoning, detecting helpful and
harmful examples, influence of groups of datapoints, etc. Recently, Ilyas et
al. [2022] introduced a linear regression method they termed datamodels to
predict the effect of training points on outputs on test data. The current
paper seeks to provide a better theoretical understanding of such interesting
empirical phenomena. The primary tool is harmonic analysis and the idea of
noise stability. Contributions include: (a) Exact characterization of the
learnt datamodel in terms of Fourier coefficients. (b) An efficient method to
estimate the residual error and quality of the optimum linear datamodel without
having to train the datamodel. (c) New insights into when influences of groups
of datapoints may or may not add up linearly.",https://github.com/libffcv/ffcv/,-1
4984df6e-134c-4bd9-8589-ada6482cb2ca,Benchmarking for Public Health Surveillance tasks on Social Media with a Domain-Specific Pretrained Language Model,0.899891,"A user-generated text on social media enables health workers to keep track of
information, identify possible outbreaks, forecast disease trends, monitor
emergency cases, and ascertain disease awareness and response to official
health correspondence. This exchange of health information on social media has
been regarded as an attempt to enhance public health surveillance (PHS).
Despite its potential, the technology is still in its early stages and is not
ready for widespread application. Advancements in pretrained language models
(PLMs) have facilitated the development of several domain-specific PLMs and a
variety of downstream applications. However, there are no PLMs for social media
tasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,
to identify tasks related to public health surveillance on social media. We
compared and benchmarked the performance of PHS-BERT on 25 datasets from
different social medial platforms related to 7 different PHS tasks. Compared
with existing PLMs that are mainly evaluated on limited tasks, PHS-BERT
achieved state-of-the-art performance on all 25 tested datasets, showing that
our PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT
available, we aim to facilitate the community to reduce the computational cost
and introduce new baselines for future works across various PHS-related tasks.",https://huggingface.co/publichealthsurveillance/PHS-BERT,-1
c88e3082-0387-4c15-a590-181f83d122f8,Large Language Models Are Reasoning Teachers,0.963213,"Recent works have shown that chain-of-thought (CoT) prompting can elicit
language models to solve complex reasoning tasks, step-by-step. However,
prompt-based CoT methods are dependent on very large models such as GPT-3 175B
which are prohibitive to deploy at scale. In this paper, we use these large
models as reasoning teachers to enable complex reasoning in smaller models and
reduce model size requirements by several orders of magnitude. We propose
Fine-tune-CoT, a method that generates reasoning samples from very large
teacher models to fine-tune smaller models. We evaluate our method on a wide
range of public models and complex tasks. We find that Fine-tune-CoT enables
substantial reasoning capability in small models, far outperforming
prompt-based baselines and even the teacher model in many tasks. Additionally,
we extend our method by leveraging the teacher model's ability to generate
multiple distinct rationales for each original sample. Enriching the
fine-tuning data with such diverse reasoning results in a substantial
performance boost across datasets, even for very small models. We conduct
ablations and sample studies to understand the emergence of reasoning
capabilities of student models. Our code implementation and data are available
at https://github.com/itsnamgyu/reasoning-teacher.",https://github.com/itsnamgyu/reasoning-teacher,-1
f43dca7b-7c0a-47e8-b3d2-d6bbe6eeef4d,SCAMPS: Synthetics for Camera Measurement of Physiological Signals,0.932711,"The use of cameras and computational algorithms for noninvasive, low-cost and
scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs
is very attractive. However, diverse data representing a range of environments,
body motions, illumination conditions and physiological states is laborious,
time consuming and expensive to obtain. Synthetic data have proven a valuable
tool in several areas of machine learning, yet are not widely available for
camera measurement of physiological states. Synthetic data offer ""perfect""
labels (e.g., without noise and with precise synchronization), labels that may
not be possible to obtain otherwise (e.g., precise pixel level segmentation
maps) and provide a high degree of control over variation and diversity in the
dataset. We present SCAMPS, a dataset of synthetics containing 2,800 videos
(1.68M frames) with aligned cardiac and respiratory signals and facial action
intensities. The RGB frames are provided alongside segmentation maps. We
provide precise descriptive statistics about the underlying waveforms,
including inter-beat interval, heart rate variability, and pulse arrival time.
Finally, we present baseline results training on these synthetic data and
testing on real-world datasets to illustrate generalizability.",https://github.com/danmcduff/scampsdataset,-1
3162a222-9808-41f4-a1b9-bf55af343470,Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies,0.812857,"We consider infinite-horizon discounted Markov decision processes and study
the convergence rates of the natural policy gradient (NPG) and the Q-NPG
methods with the log-linear policy class. Using the compatible function
approximation framework, both methods with log-linear policies can be written
as inexact versions of the policy mirror descent (PMD) method. We show that
both methods attain linear convergence rates and
$\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexities using a simple,
non-adaptive geometrically increasing step size, without resorting to entropy
or other strongly convex regularization. Lastly, as a byproduct, we obtain
sublinear convergence rates for both methods with arbitrary constant step size.",None,-1
7f38a327-c9af-4731-83bb-151ca83fa92a,SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control,0.451112,"Despite the growing success of diffusion models in continuous-valued domains
(e.g., images), similar efforts for discrete domains such as text have yet to
match the performance of autoregressive language models. In this work, we
present SSD-LM -- a diffusion-based language model with two key design choices.
First, SSD-LM is semi-autoregressive, iteratively generating blocks of text,
allowing for flexible output length at decoding time while enabling local
bidirectional context updates. Second, it is simplex-based, performing
diffusion on the natural vocabulary space rather than a learned latent space,
allowing us to incorporate classifier guidance and modular control using
off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on
unconstrained text generation benchmarks, and show that it matches or
outperforms strong autoregressive GPT-2 models across standard quality and
diversity metrics, while vastly outperforming diffusion-based baselines. On
controlled text generation, SSD-LM also outperforms competitive baselines, with
an extra advantage in modularity.",https://github.com/xhan77/ssd-lm,-1
1ab90e46-6eed-4c5c-85f1-30c8f1c38891,Extreme Compression for Pre-trained Transformers Made Simple and Efficient,0.69133,"Extreme compression, particularly ultra-low bit precision (binary/ternary)
quantization, has been proposed to fit large NLP models on resource-constraint
devices. However, to preserve the accuracy for such aggressive compression
schemes, cutting-edge methods usually introduce complicated compression
pipelines, e.g., multi-stage expensive knowledge distillation with extensive
hyperparameter tuning. Also, they oftentimes focus less on smaller transformer
models that have already been heavily compressed via knowledge distillation and
lack a systematic study to show the effectiveness of their methods. In this
paper, we perform a very comprehensive systematic study to measure the impact
of many key hyperparameters and training strategies from previous works. As a
result, we find out that previous baselines for ultra-low bit precision
quantization are significantly under-trained. Based on our study, we propose a
simple yet effective compression pipeline for extreme compression, named XTC.
XTC demonstrates that (1) we can skip the pre-training knowledge distillation
to obtain a 5-layer BERT while achieving better performance than previous
state-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization
plus layer reduction is able to reduce the model size by 50x, resulting in new
state-of-the-art results on GLUE tasks.",https://github.com/microsoft/DeepSpeed,7580
1f04710e-f525-4e95-b50a-465dfe0ae253,Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions,0.575186,"Pruning is one of the predominant approaches for compressing deep neural
networks (DNNs). Lately, coresets (provable data summarizations) were leveraged
for pruning DNNs, adding the advantage of theoretical guarantees on the
trade-off between the compression rate and the approximation error. However,
coresets in this domain were either data-dependent or generated under
restrictive assumptions on both the model's weights and inputs. In real-world
scenarios, such assumptions are rarely satisfied, limiting the applicability of
coresets. To this end, we suggest a novel and robust framework for computing
such coresets under mild assumptions on the model's weights and without any
assumption on the training data. The idea is to compute the importance of each
neuron in each layer with respect to the output of the following layer. This is
achieved by a combination of L\""{o}wner ellipsoid and Caratheodory theorem. Our
method is simultaneously data-independent, applicable to various networks and
datasets (due to the simplified assumptions), and theoretically supported.
Experimental results show that our method outperforms existing coreset based
neural pruning approaches across a wide range of networks and datasets. For
example, our method achieved a $62\%$ compression rate on ResNet50 on ImageNet
with $1.09\%$ drop in accuracy.",None,506
356994ab-e8ba-4e5f-8425-499d65b595de,Equivariant Self-Supervision for Musical Tempo Estimation,0.905989,"Self-supervised methods have emerged as a promising avenue for representation
learning in the recent years since they alleviate the need for labeled
datasets, which are scarce and expensive to acquire. Contrastive methods are a
popular choice for self-supervision in the audio domain, and typically provide
a learning signal by forcing the model to be invariant to some transformations
of the input. These methods, however, require measures such as negative
sampling or some form of regularisation to be taken to prevent the model from
collapsing on trivial solutions. In this work, instead of invariance, we
propose to use equivariance as a self-supervision signal to learn audio tempo
representations from unlabelled data. We derive a simple loss function that
prevents the network from collapsing on a trivial solution during training,
without requiring any form of regularisation or negative sampling. Our
experiments show that it is possible to learn meaningful representations for
tempo estimation by solely relying on equivariant self-supervision, achieving
performance comparable with supervised methods on several benchmarks. As an
added benefit, our method only requires moderate compute resources and
therefore remains accessible to a wide research community.",https://github.com/Quint-e/equivariant-self-supervision-tempo,152
62dbdfaf-6b06-4660-8ae4-048a8c27673b,Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature,0.83929,"Literary translation is a culturally significant task, but it is bottlenecked
by the small number of qualified literary translators relative to the many
untranslated works published around the world. Machine translation (MT) holds
potential to complement the work of human translators by improving both
training procedures and their overall efficiency. Literary translation is less
constrained than more traditional MT settings since translators must balance
meaning equivalence, readability, and critical interpretability in the target
language. This property, along with the complex discourse-level context present
in literary texts, also makes literary MT more challenging to computationally
model and evaluate. To explore this task, we collect a dataset (Par3) of
non-English language novels in the public domain, each aligned at the paragraph
level to both human and automatic English translations. Using Par3, we discover
that expert literary translators prefer reference human translations over
machine-translated paragraphs at a rate of 84%, while state-of-the-art
automatic MT metrics do not correlate with those preferences. The experts note
that MT outputs contain not only mistranslations, but also discourse-disrupting
errors and stylistic inconsistencies. To address these problems, we train a
post-editing model whose output is preferred over normal MT output at a rate of
69% by experts. We publicly release Par3 at
https://github.com/katherinethai/par3/ to spur future research into literary
MT.",https://github.com/katherinethai/par3/,-1
4baa7b83-04b9-447b-bb77-51728ab876b1,Contrastive Learning for Joint Normal Estimation and Point Cloud Filtering,0.200536,"Point cloud filtering and normal estimation are two fundamental research
problems in the 3D field. Existing methods usually perform normal estimation
and filtering separately and often show sensitivity to noise and/or inability
to preserve sharp geometric features such as corners and edges. In this paper,
we propose a novel deep learning method to jointly estimate normals and filter
point clouds. We first introduce a 3D patch based contrastive learning
framework, with noise corruption as an augmentation, to train a feature encoder
capable of generating faithful representations of point cloud patches while
remaining robust to noise. These representations are consumed by a simple
regression network and supervised by a novel joint loss, simultaneously
estimating point normals and displacements that are used to filter the patch
centers. Experimental results show that our method well supports the two tasks
simultaneously and preserves sharp features and fine details. It generally
outperforms state-of-the-art techniques on both tasks. Our source code is
available at https://github.com/ddsediri/CLJNEPCF.",https://github.com/ddsediri/CLJNEPCF,-1
0cc78714-ada0-4cba-8203-f60f5a043992,DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models,0.952997,"We study the way DALLE-2 maps symbols (words) in the prompt to their
references (entities or properties of entities in the generated image). We show
that in stark contrast to the way human process language, DALLE-2 does not
follow the constraint that each word has a single role in the interpretation,
and sometimes re-use the same symbol for different purposes. We collect a set
of stimuli that reflect the phenomenon: we show that DALLE-2 depicts both
senses of nouns with multiple senses at once; and that a given word can modify
the properties of two distinct entities in the image, or can be depicted as one
object and also modify the properties of another object, creating a semantic
leakage of properties between entities. Taken together, our study highlights
the differences between DALLE-2 and human language processing and opens an
avenue for future study on the inductive biases of text-to-image models.",https://github.com/RoyiRa/,-1
224498e4-2097-452a-8b2b-ac34e25215a2,Semi-Supervised Knowledge-Grounded Pre-training for Task-Oriented Dialog Systems,0.885175,"Recent advances in neural approaches greatly improve task-oriented dialogue
(TOD) systems which assist users to accomplish their goals. However, such
systems rely on costly manually labeled dialogs which are not available in
practical scenarios. In this paper, we present our models for Track 2 of the
SereTOD 2022 challenge, which is the first challenge of building
semi-supervised and reinforced TOD systems on a large-scale real-world Chinese
TOD dataset MobileCS. We build a knowledge-grounded dialog model to formulate
dialog history and local KB as input and predict the system response. And we
perform semi-supervised pre-training both on the labeled and unlabeled data.
Our system achieves the first place both in the automatic evaluation and human
interaction, especially with higher BLEU (+7.64) and Success (+13.6\%) than the
second place.",https://github.com/Zeng-WH/S2KG,-1
492c209f-9785-4996-897b-ff5bdb1edb31,Transformer Embeddings of Irregularly Spaced Events and Their Participants,0.658745,"The neural Hawkes process (Mei & Eisner, 2017) is a generative model of
irregularly spaced sequences of discrete events. To handle complex domains with
many event types, Mei et al. (2020a) further consider a setting in which each
event in the sequence updates a deductive database of facts (via
domain-specific pattern-matching rules); future events are then conditioned on
the database contents. They show how to convert such a symbolic system into a
neuro-symbolic continuous-time generative model, in which each database fact
and the possible event has a time-varying embedding that is derived from its
symbolic provenance.
  In this paper, we modify both models, replacing their recurrent LSTM-based
architectures with flatter attention-based architectures (Vaswani et al.,
2017), which are simpler and more parallelizable. This does not appear to hurt
our accuracy, which is comparable to or better than that of the original models
as well as (where applicable) previous attention-based methods (Zuo et al.,
2020; Zhang et al., 2020a).",https://github.com/yangalan123/anhp-andtt,-1
7b2745bf-3e24-42c9-b03c-27448fcaa9d0,Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence,0.715379,"The logical negation property (LNP), which implies generating different
predictions for semantically opposite inputs, is an important property that a
trustworthy language model must satisfy. However, much recent evidence shows
that large-size pre-trained language models (PLMs) do not satisfy this
property. In this paper, we perform experiments using probing tasks to assess
PLM's LNP understanding. Unlike previous studies that only examined negation
expressions, we expand the boundary of the investigation to lexical semantics.
Through experiments, we observe that PLMs violate the LNP frequently. To
alleviate the issue, we propose a novel intermediate training task, names
meaning-matching, designed to directly learn a meaning-text correspondence,
instead of relying on the distributional hypothesis. Through multiple
experiments, we find that the task enables PLMs to learn lexical semantic
information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm
that it is a safe intermediate task that guarantees a similar or better
performance of downstream tasks. Finally, we observe that our proposed approach
outperforms our previous counterparts despite its time and resource efficiency.",https://github.com/MJ-Jang/beyond-distributional,-1
ce71dc41-ea60-412e-b33c-4e300ea6b7d8,Semi-Supervised Formality Style Transfer with Consistency Training,0.750027,"Formality style transfer (FST) is a task that involves paraphrasing an
informal sentence into a formal one without altering its meaning. To address
the data-scarcity problem of existing parallel datasets, previous studies tend
to adopt a cycle-reconstruction scheme to utilize additional unlabeled data,
where the FST model mainly benefits from target-side unlabeled sentences. In
this work, we propose a simple yet effective semi-supervised framework to
better utilize source-side unlabeled sentences based on consistency training.
Specifically, our approach augments pseudo-parallel data obtained from a
source-side informal sentence by enforcing the model to generate similar
outputs for its perturbed version. Moreover, we empirically examined the
effects of various data perturbation methods and propose effective data
filtering strategies to improve our framework. Experimental results on the
GYAFC benchmark demonstrate that our approach can achieve state-of-the-art
results, even with less than 40% of the parallel data.",https://github.com/Aolius/semi-fst,-1
cfcc793a-4c06-4dc4-a813-131191fbcdfc,SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes,0.124992,"Is it possible to train a general metric for evaluating text generation
quality without human annotated ratings? Existing learned metrics either
perform unsatisfactorily across text generation tasks or require human ratings
for training on specific tasks. In this paper, we propose SESCORE2, a
self-supervised approach for training a model-based metric for text generation
evaluation. The key concept is to synthesize realistic model mistakes by
perturbing sentences retrieved from a corpus. The primary advantage of the
SESCORE2 is its ease of extension to many other languages while providing
reliable severity estimation. We evaluate SESCORE2 and previous methods on four
text generation tasks across three languages. SESCORE2 outperforms unsupervised
metric PRISM on four text generation evaluation benchmarks, with a Kendall
improvement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised
BLEURT and COMET on multiple text generation tasks. The code and data are
available at https://github.com/xu1998hz/SEScore2.",https://github.com/xu1998hz/SEScore21,-1
a94af317-7607-4251-bc60-b7e7fbf92c38,Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers,0.193283,"Large-scale transformer models have become the de-facto architectures for
various machine learning applications, e.g., CV and NLP. However, those large
models also introduce prohibitive training costs. To mitigate this issue, we
propose a novel random and layerwise token dropping method (random-LTD), which
skips the computation of a subset of the input tokens at all middle layers.
Particularly, random-LTD achieves considerable speedups and comparable accuracy
as the standard training baseline. Compared to other token dropping methods,
random-LTD does not require (1) any importance score-based metrics, (2) any
special token treatment (e.g., [CLS]), and (3) many layers in full sequence
length training except the first and the last layers. Besides, a new LayerToken
learning rate schedule is proposed for pretraining problems that resolve the
heavy tuning requirement for our proposed training mechanism. Finally, we
demonstrate that random-LTD can be applied to broader applications, including
GPT and BERT pretraining as well as ViT and GPT finetuning tasks. Our results
show that random-LTD can save about 33.3% theoretical compute cost and 25.6%
wall-clock training time while achieving similar zero-shot evaluations on
GPT-31.3B as compared to baseline.",https://github.com/microsoft/DeepSpeed,-1
82cb002c-1471-4fca-b890-0b47cd221ef7,USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration Network for Multilingual Complex Named Entity Recognition,0.862492,"This paper describes the system developed by the USTC-NELSLIP team for
SemEval-2022 Task 11 Multilingual Complex Named Entity Recognition
(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to
improve the performance of language models for recognizing complex named
entities. The method first adapts the representations of gazetteer networks to
those of language models by minimizing the KL divergence between them. After
adaptation, these two networks are then integrated for backend supervised named
entity recognition (NER) training. The proposed method is applied to several
state-of-the-art Transformer-based NER models with a gazetteer built from
Wikidata, and shows great generalization ability across them. The final
predictions are derived from an ensemble of these trained models. Experimental
results and detailed analysis verify the effectiveness of the proposed method.
The official results show that our system ranked 1st on three tracks (Chinese,
Code-mixed and Bangla) and 2nd on the other ten tracks in this task.",https://github.com/Mckysse/GAIN,-1
51900533-4610-4eef-a421-c9e5a900981b,Boosting human decision-making with AI-generated decision aids,0.285486,"Human decision-making is plagued by many systematic errors. Many of these
errors can be avoided by providing decision aids that guide decision-makers to
attend to the important information and integrate it according to a rational
decision strategy. Designing such decision aids used to be a tedious manual
process. Advances in cognitive science might make it possible to automate this
process in the future. We recently introduced machine learning methods for
discovering optimal strategies for human decision-making automatically and an
automatic method for explaining those strategies to people. Decision aids
constructed by this method were able to improve human decision-making. However,
following the descriptions generated by this method is very tedious. We
hypothesized that this problem can be overcome by conveying the automatically
discovered decision strategy as a series of natural language instructions for
how to reach a decision. Experiment 1 showed that people do indeed understand
such procedural instructions more easily than the decision aids generated by
our previous method. Encouraged by this finding, we developed an algorithm for
translating the output of our previous method into procedural instructions. We
applied the improved method to automatically generate decision aids for a
naturalistic planning task (i.e., planning a road trip) and a naturalistic
decision task (i.e., choosing a mortgage). Experiment 2 showed that these
automatically generated decision-aids significantly improved people's
performance in planning a road trip and choosing a mortgage. These findings
suggest that AI-powered boosting might have potential for improving human
decision-making in the real world.",https://github.com/RationalityEnhancement/InterpretableStrategyDiscovery,-1
746dd4ca-2ce3-4048-8c71-31008d5c7861,Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes,0.570749,"This work concerns the development of deep networks that are certifiably
robust to adversarial attacks. Joint robust classification-detection was
recently introduced as a certified defense mechanism, where adversarial
examples are either correctly classified or assigned to the ""abstain"" class. In
this work, we show that such a provable framework can benefit by extension to
networks with multiple explicit abstain classes, where the adversarial examples
are adaptively assigned to those. We show that naively adding multiple abstain
classes can lead to ""model degeneracy"", then we propose a regularization
approach and a training method to counter this degeneracy by promoting full use
of the multiple abstain classes. Our experiments demonstrate that the proposed
approach consistently achieves favorable standard vs. robust verified accuracy
tradeoffs, outperforming state-of-the-art algorithms for various choices of
number of abstain classes.",https://github.com/sinaBaharlouei/MultipleAbstainDetection,-1
2a3f7f71-d0c4-4804-806a-10ac57bcd53a,Explainability as statistical inference,0.0428235,"A wide variety of model explanation approaches have been proposed in recent
years, all guided by very different rationales and heuristics. In this paper,
we take a new route and cast interpretability as a statistical inference
problem. We propose a general deep probabilistic model designed to produce
interpretable predictions. The model parameters can be learned via maximum
likelihood, and the method can be adapted to any predictor network architecture
and any type of prediction problem. Our method is a case of amortized
interpretability models, where a neural network is used as a selector to allow
for fast interpretation at inference time. Several popular interpretability
methods are shown to be particular cases of regularised maximum likelihood for
our general model. We propose new datasets with ground truth selection which
allow for the evaluation of the features importance map. Using these datasets,
we show experimentally that using multiple imputation provides more reasonable
interpretations.",None,1641
b1ca9554-bcd2-4412-accd-8aec506b169f,Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation,0.282393,"Label smoothing and vocabulary sharing are two widely used techniques in
neural machine translation models. However, we argue that simply applying both
techniques can be conflicting and even leads to sub-optimal performance. When
allocating smoothed probability, original label smoothing treats the
source-side words that would never appear in the target language equally to the
real target-side words, which could bias the translation model. To address this
issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the
soft label probability of source-side words to zero. Simple yet effective, MLS
manages to better integrate label smoothing with vocabulary sharing. Our
extensive experiments show that MLS consistently yields improvement over
original label smoothing on different datasets, including bilingual and
multilingual translation from both translation quality and model's calibration.
Our code is released at https://github.com/PKUnlp-icler/MLS",https://github.com/shuo-git/InfECE,-1
a566c86d-17c3-42df-84ac-ac9b8885d812,Unsupervised Domain Adaptation Based on the Predictive Uncertainty of Models,0.454371,"Unsupervised domain adaptation (UDA) aims to improve the prediction
performance in the target domain under distribution shifts from the source
domain. The key principle of UDA is to minimize the divergence between the
source and the target domains. To follow this principle, many methods employ a
domain discriminator to match the feature distributions. Some recent methods
evaluate the discrepancy between two predictions on target samples to detect
those that deviate from the source distribution. However, their performance is
limited because they either match the marginal distributions or measure the
divergence conservatively. In this paper, we present a novel UDA method that
learns domain-invariant features that minimize the domain divergence. We
propose model uncertainty as a measure of the domain divergence. Our UDA method
based on model uncertainty (MUDA) adopts a Bayesian framework and provides an
efficient way to evaluate model uncertainty by means of Monte Carlo dropout
sampling. Empirical results on image recognition tasks show that our method is
superior to existing state-of-the-art methods. We also extend MUDA to
multi-source domain adaptation problems.",https://github.com/joonholee-research/MUDA,-1
16b83b4f-c120-4494-957e-76acbe1724f0,Language Models as Inductive Reasoners,0.311559,"Inductive reasoning is a core component of human intelligence. In the past
research of inductive reasoning within computer science, formal language is
used as representations of knowledge (facts and rules, more specifically).
However, formal language can cause systematic problems for inductive reasoning
such as disability of handling raw input such as natural language,
sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To
this end, we propose a new paradigm (task) for inductive reasoning, which is to
induce natural language rules from natural language facts, and create a dataset
termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts
are written in natural language. New automatic metrics are also proposed and
analysed for the evaluation of this task. With DEER, we investigate a modern
approach for inductive reasoning where we use natural language as
representation for knowledge instead of formal language and use pretrained
language models as ''reasoners''. Moreover, we provide the first and
comprehensive analysis of how well pretrained language models can induce
natural language rules from natural language facts. We also propose a new
framework drawing insights from philosophy literature for this task, which we
show in the experiment section that surpasses baselines in both automatic and
human evaluations. We discuss about our future perspectives for inductive
reasoning in Section 7. Dataset and code are available at
https://github.com/ZonglinY/Inductive_Reasoning.",https://github.com/ZonglinY/Inductive_Reasoning,-1
95dad649-f4ca-4c8f-917b-9ea6ff5c226a,An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models,0.320632,"We propose a framework for training non-autoregressive sequence-to-sequence
models for editing tasks, where the original input sequence is iteratively
edited to produce the output. We show that the imitation learning algorithms
designed to train such models for machine translation introduces mismatches
between training and inference that lead to undertraining and poor
generalization in editing scenarios. We address this issue with two
complementary strategies: 1) a roll-in policy that exposes the model to
intermediate training sequences that it is more likely to encounter during
inference, 2) a curriculum that presents easy-to-learn edit operations first,
gradually increasing the difficulty of training samples as the model becomes
competent. We show the efficacy of these strategies on two challenging English
editing tasks: controllable text simplification and abstractive summarization.
Our approach significantly improves output quality on both tasks and controls
output complexity better on the simplification task.",https://github.com/cocoxu/,-1
6587fd98-97a7-4148-9e3b-acb16c5443a8,Semi-Supervised Imitation Learning of Team Policies from Suboptimal Demonstrations,0.202845,"We present Bayesian Team Imitation Learner (BTIL), an imitation learning
algorithm to model the behavior of teams performing sequential tasks in
Markovian domains. In contrast to existing multi-agent imitation learning
techniques, BTIL explicitly models and infers the time-varying mental states of
team members, thereby enabling learning of decentralized team policies from
demonstrations of suboptimal teamwork. Further, to allow for sample- and
label-efficient policy learning from small datasets, BTIL employs a Bayesian
perspective and is capable of learning from semi-supervised demonstrations. We
demonstrate and benchmark the performance of BTIL on synthetic multi-agent
tasks as well as a novel dataset of human-agent teamwork. Our experiments show
that BTIL can successfully learn team policies from demonstrations despite the
influence of team members' (time-varying and potentially misaligned) mental
states on their behavior.",None,-1
00b7ff26-18c1-42e3-83a1-bc84b7f12d67,cosFormer: Rethinking Softmax in Attention,0.956287,"Transformer has shown great successes in natural language processing,
computer vision, and audio processing. As one of its core components, the
softmax attention helps to capture long-range dependencies yet prohibits its
scale-up due to the quadratic space and time complexity to the sequence length.
Kernel methods are often adopted to reduce the complexity by approximating the
softmax operator. Nevertheless, due to the approximation errors, their
performances vary in different tasks/corpus and suffer crucial performance
drops when compared with the vanilla softmax attention. In this paper, we
propose a linear transformer called cosFormer that can achieve comparable or
better accuracy to the vanilla transformer in both casual and cross attentions.
cosFormer is based on two key properties of softmax attention: i).
non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme
that can concentrate the distribution of the attention matrix. As its linear
substitute, cosFormer fulfills these properties with a linear operator and a
cosine-based distance re-weighting mechanism. Extensive experiments on language
modeling and text understanding tasks demonstrate the effectiveness of our
method. We further examine our method on long sequences and achieve
state-of-the-art performance on the Long-Range Arena benchmark. The source code
is available at https://github.com/OpenNLPLab/cosFormer.",None,2443
184ec04f-687a-4da6-9abb-433439127b88,Color Invariant Skin Segmentation,0.114144,"This paper addresses the problem of automatically detecting human skin in
images without reliance on color information. A primary motivation of the work
has been to achieve results that are consistent across the full range of skin
tones, even while using a training dataset that is significantly biased toward
lighter skin tones. Previous skin-detection methods have used color cues almost
exclusively, and we present a new approach that performs well in the absence of
such information. A key aspect of the work is dataset repair through
augmentation that is applied strategically during training, with the goal of
color invariant feature learning to enhance generalization. We have
demonstrated the concept using two architectures, and experimental results show
improvements in both precision and recall for most Fitzpatrick skin tones in
the benchmark ECU dataset. We further tested the system with the RFW dataset to
show that the proposed method performs much more consistently across different
ethnicities, thereby reducing the chance of bias based on skin color. To
demonstrate the effectiveness of our work, extensive experiments were performed
on grayscale images as well as images obtained under unconstrained illumination
and with artificial filters. Source code:
https://github.com/HanXuMartin/Color-Invariant-Skin-Segmentation",https://github.com/HanXuMartin/Color-Invariant-Skin-Segmentation1,-1
b31850c4-eb18-4343-a73c-43997dc777fa,Transferability Estimation Based On Principal Gradient Expectation,0.156821,"Transfer learning aims to improve the performance of target tasks by
transferring knowledge acquired in source tasks. The standard approach is
pre-training followed by fine-tuning or linear probing. Especially, selecting a
proper source domain for a specific target domain under predefined tasks is
crucial for improving efficiency and effectiveness. It is conventional to solve
this problem via estimating transferability. However, existing methods can not
reach a trade-off between performance and cost. To comprehensively evaluate
estimation methods, we summarize three properties: stability, reliability and
efficiency. Building upon them, we propose Principal Gradient Expectation(PGE),
a simple yet effective method for assessing transferability. Specifically, we
calculate the gradient over each weight unit multiple times with a restart
scheme, and then we compute the expectation of all gradients. Finally, the
transferability between the source and target is estimated by computing the gap
of normalized principal gradients. Extensive experiments show that the proposed
metric is superior to state-of-the-art methods on all properties.",None,-1
3f4672ff-3a14-45c6-bfdf-c18aaf80265e,Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection,0.623653,"With the vigorous development of computer vision, oriented object detection
has gradually been featured. In this paper, a novel differentiable angle coder
named phase-shifting coder (PSC) is proposed to accurately predict the
orientation of objects, along with a dual-frequency version (PSCD). By mapping
the rotational periodicity of different cycles into the phase of different
frequencies, we provide a unified framework for various periodic fuzzy problems
caused by rotational symmetry in oriented object detection. Upon such a
framework, common problems in oriented object detection such as boundary
discontinuity and square-like problems are elegantly solved in a unified form.
Visual analysis and experiments on three datasets prove the effectiveness and
the potentiality of our approach. When facing scenarios requiring high-quality
bounding boxes, the proposed methods are expected to give a competitive
performance. The codes are publicly available at
https://github.com/open-mmlab/mmrotate.",https://github.com/open-mmlab/mmrotate,-1
3ab65a9a-1819-415f-b024-6a76be76b74c,TEMOS: Generating diverse human motions from textual descriptions,0.979468,"We address the problem of generating diverse 3D human motions from textual
descriptions. This challenging task requires joint modeling of both modalities:
understanding and extracting useful human-centric information from the text,
and then generating plausible and realistic sequences of human poses. In
contrast to most previous work which focuses on generating a single,
deterministic, motion from a textual description, we design a variational
approach that can produce multiple diverse human motions. We propose TEMOS, a
text-conditioned generative model leveraging variational autoencoder (VAE)
training with human motion data, in combination with a text encoder that
produces distribution parameters compatible with the VAE latent space. We show
the TEMOS framework can produce both skeleton-based animations as in prior
work, as well more expressive SMPL body motions. We evaluate our approach on
the KIT Motion-Language benchmark and, despite being relatively
straightforward, demonstrate significant improvements over the state of the
art. Code and models are available on our webpage.",None,-1
01da5db6-29c3-4516-9e4d-fd211635708c,Data-driven prediction of Air Traffic Controllers reactions to resolving conflicts,0.692534,"With the aim to enhance automation in conflict detection and resolution
(CD&R) tasks in the Air Traffic Management domain, in this paper we propose
deep learning techniques (DL) that can learn models of Air Traffic Controllers'
(ATCO) reactions in resolving conflicts that can violate separation minimum
constraints among aircraft trajectories: This implies learning when the ATCO
will react towards resolving a conflict, and how he/she will react. Timely
reactions, to which this paper aims, focus on when do reactions happen, aiming
to predict the trajectory points, as the trajectory evolves, that the ATCO
issues a conflict resolution action, while also predicting the type of
resolution action (if any). Towards this goal, the paper formulates the ATCO
reactions prediction problem for CD&R, and presents DL methods that can model
ATCO timely reactions and evaluates these methods in real-world data sets,
showing their efficacy in prediction with very high accuracy.",None,4539
ecca6894-bc53-4449-a19e-05485592040c,Amodal Panoptic Segmentation,0.637,"Humans have the remarkable ability to perceive objects as a whole, even when
parts of them are occluded. This ability of amodal perception forms the basis
of our perceptual and cognitive understanding of our world. To enable robots to
reason with this capability, we formulate and propose a novel task that we name
amodal panoptic segmentation. The goal of this task is to simultaneously
predict the pixel-wise semantic segmentation labels of the visible regions of
stuff classes and the instance segmentation labels of both the visible and
occluded regions of thing classes. To facilitate research on this new task, we
extend two established benchmark datasets with pixel-level amodal panoptic
segmentation labels that we make publicly available as KITTI-360-APS and
BDD100K-APS. We present several strong baselines, along with the amodal
panoptic quality (APQ) and amodal parsing coverage (APC) metrics to quantify
the performance in an interpretable manner. Furthermore, we propose the novel
amodal panoptic segmentation network (APSNet), as a first step towards
addressing this task by explicitly modeling the complex relationships between
the occluders and occludes. Extensive experimental evaluations demonstrate that
APSNet achieves state-of-the-art performance on both benchmarks and more
importantly exemplifies the utility of amodal recognition. The benchmarks are
available at http://amodal-panoptic.cs.uni-freiburg.de.",http://amodal-panoptic.cs.uni-freiburg.de,-1
0ede90d7-6602-4f5a-bb8c-f53ac387cedc,SPBERTQA: A Two-Stage Question Answering System Based on Sentence Transformers for Medical Texts,0.220079,"Question answering (QA) systems have gained explosive attention in recent
years. However, QA tasks in Vietnamese do not have many datasets.
Significantly, there is mostly no dataset in the medical domain. Therefore, we
built a Vietnamese Healthcare Question Answering dataset (ViHealthQA),
including 10,015 question-answer passage pairs for this task, in which
questions from health-interested users were asked on prestigious health
websites and answers from highly qualified experts. This paper proposes a
two-stage QA system based on Sentence-BERT (SBERT) using multiple negatives
ranking (MNR) loss combined with BM25. Then, we conduct diverse experiments
with many bag-of-words models to assess our system's performance. With the
obtained results, this system achieves better performance than traditional
methods.",None,2352
47831805-a861-4191-95a3-9f733d026a94,Multimodal and Explainable Internet Meme Classification,0.107133,"In the current context where online platforms have been effectively
weaponized in a variety of geo-political events and social issues, Internet
memes make fair content moderation at scale even more difficult. Existing work
on meme classification and tracking has focused on black-box methods that do
not explicitly consider the semantics of the memes or the context of their
creation. In this paper, we pursue a modular and explainable architecture for
Internet meme understanding. We design and implement multimodal classification
methods that perform example- and prototype-based reasoning over training
cases, while leveraging both textual and visual SOTA models to represent the
individual cases. We study the relevance of our modular and explainable models
in detecting harmful memes on two existing tasks: Hate Speech Detection and
Misogyny Classification. We compare the performance between example- and
prototype-based methods, and between text, vision, and multimodal models,
across different categories of harmfulness (e.g., stereotype and
objectification). We devise a user-friendly interface that facilitates the
comparative analysis of examples retrieved by all of our models for any given
meme, informing the community about the strengths and limitations of these
explainable methods.",https://github.com/usc-isi-i2/meme-understanding,-1
30c9417e-1837-42ff-a7c9-11b95bbc52e5,Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency,0.325421,"The knowledge graph (KG) stores a large amount of structural knowledge, while
it is not easy for direct human understanding. Knowledge graph-to-text
(KG-to-text) generation aims to generate easy-to-understand sentences from the
KG, and at the same time, maintains semantic consistency between generated
sentences and the KG. Existing KG-to-text generation methods phrase this task
as a sequence-to-sequence generation task with linearized KG as input and
consider the consistency issue of the generated texts and KG through a simple
selection between decoded sentence word and KG node word at each time step.
However, the linearized KG order is commonly obtained through a heuristic
search without data-driven optimization. In this paper, we optimize the
knowledge description order prediction under the order supervision extracted
from the caption and further enhance the consistency of the generated sentences
and KG through syntactic and semantic regularization. We incorporate the
Part-of-Speech (POS) syntactic tags to constrain the positions to copy words
from the KG and employ a semantic context scoring function to evaluate the
semantic fitness for each word in its local context when decoding each word in
the generated sentence. Extensive experiments are conducted on two datasets,
WebNLG and DART, and achieve state-of-the-art performances.",None,-1
49c9f5a7-2b80-4c8b-85ac-d973377b8776,Speaker adaptation for Wav2vec2 based dysarthric ASR,0.622052,"Dysarthric speech recognition has posed major challenges due to lack of
training data and heavy mismatch in speaker characteristics. Recent ASR systems
have benefited from readily available pretrained models such as wav2vec2 to
improve the recognition performance. Speaker adaptation using fMLLR and
xvectors have provided major gains for dysarthric speech with very little
adaptation data. However, integration of wav2vec2 with fMLLR features or
xvectors during wav2vec2 finetuning is yet to be explored. In this work, we
propose a simple adaptation network for fine-tuning wav2vec2 using fMLLR
features. The adaptation network is also flexible to handle other speaker
adaptive features such as xvectors. Experimental analysis show steady
improvements using our proposed approach across all impairment severity levels
and attains 57.72\% WER for high severity in UASpeech dataset. We also
performed experiments on German dataset to substantiate the consistency of our
proposed approach across diverse domains.",None,-1
2100eff6-1065-4750-b0fd-bd621381162f,Adversarial Masking for Self-Supervised Learning,0.717541,"We propose ADIOS, a masked image model (MIM) framework for self-supervised
learning, which simultaneously learns a masking function and an image encoder
using an adversarial objective. The image encoder is trained to minimise the
distance between representations of the original and that of a masked image.
The masking function, conversely, aims at maximising this distance. ADIOS
consistently improves on state-of-the-art self-supervised learning (SSL)
methods on a variety of tasks and datasets -- including classification on
ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and
iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao
et al., 2021) -- while generating semantically meaningful masks. Unlike modern
MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch
tokenisation construction of Vision Transformers, and can be implemented with
convolutional backbones. We further demonstrate that the masks learned by ADIOS
are more effective in improving representation learning of SSL methods than
masking schemes used in popular MIM models. Code is available at
https://github.com/YugeTen/adios.",None,-1
58e86c86-593f-488e-a3b4-fdaeaaf3e91c,Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,0.906471,"Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning.",None,-1
a3a3437a-bebe-4a1f-8540-16e91d7f6006,Data augmentation on graphs for table type classification,0.09767,"Tables are widely used in documents because of their compact and structured
representation of information. In particular, in scientific papers, tables can
sum up novel discoveries and summarize experimental results, making the
research comparable and easily understandable by scholars. Since the layout of
tables is highly variable, it would be useful to interpret their content and
classify them into categories. This could be helpful to directly extract
information from scientific papers, for instance comparing performance of some
models given their paper result tables. In this work, we address the
classification of tables using a Graph Neural Network, exploiting the table
structure for the message passing algorithm in use. We evaluate our model on a
subset of the Tab2Know dataset. Since it contains few examples manually
annotated, we propose data augmentation techniques directly on the table graph
structures. We achieve promising preliminary results, proposing a data
augmentation method suitable for graph-based table representation.",https://github.com/tabulapdf/tabula,-1
846a5760-31cd-42ea-b69a-5ee6c623931f,Speech Synthesis with Mixed Emotions,0.954974,"Emotional speech synthesis aims to synthesize human voices with various
emotional effects. The current studies are mostly focused on imitating an
averaged style belonging to a specific emotion type. In this paper, we seek to
generate speech with a mixture of emotions at run-time. We propose a novel
formulation that measures the relative difference between the speech samples of
different emotions. We then incorporate our formulation into a
sequence-to-sequence emotional text-to-speech framework. During the training,
the framework does not only explicitly characterize emotion styles, but also
explores the ordinal nature of emotions by quantifying the differences with
other emotions. At run-time, we control the model to produce the desired
emotion mixture by manually defining an emotion attribute vector. The objective
and subjective evaluations have validated the effectiveness of the proposed
framework. To our best knowledge, this research is the first study on
modelling, synthesizing, and evaluating mixed emotions in speech.",https://github.com/chaitanya100100/Relative-Attributes-Zero-Emotions-Demo/,-1
59357ca4-e025-43f4-9f2f-a010cc227d69,Robust Region Feature Synthesizer for Zero-Shot Object Detection,0.642312,"Zero-shot object detection aims at incorporating class semantic vectors to
realize the detection of (both seen and) unseen classes given an unconstrained
test image. In this study, we reveal the core challenges in this research area:
how to synthesize robust region features (for unseen objects) that are as
intra-class diverse and inter-class separable as the real samples, so that
strong unseen object detectors can be trained upon them. To address these
challenges, we build a novel zero-shot object detection framework that contains
an Intra-class Semantic Diverging component and an Inter-class Structure
Preserving component. The former is used to realize the one-to-more mapping to
obtain diverse visual features from each class semantic vector, preventing
miss-classifying the real unseen objects as image backgrounds. While the latter
is used to avoid the synthesized features too scattered to mix up the
inter-class and foreground-background relationship. To demonstrate the
effectiveness of the proposed approach, comprehensive experiments on PASCAL
VOC, COCO, and DIOR datasets are conducted. Notably, our approach achieves the
new state-of-the-art performance on PASCAL VOC and COCO and it is the first
study to carry out zero-shot object detection in remote sensing imagery.",None,-1
767beecb-44e4-4a30-8554-5597d344b917,iSimLoc: Visual Global Localization for Previously Unseen Environments with Simulated Images,0.219413,"The visual camera is an attractive device in beyond visual line of sight
(B-VLOS) drone operation, since they are low in size, weight, power, and cost,
and can provide redundant modality to GPS failures. However, state-of-the-art
visual localization algorithms are unable to match visual data that have a
significantly different appearance due to illuminations or viewpoints. This
paper presents iSimLoc, a condition/viewpoint consistent hierarchical global
re-localization approach. The place features of iSimLoc can be utilized to
search target images under changing appearances and viewpoints. Additionally,
our hierarchical global re-localization module refines in a coarse-to-fine
manner, allowing iSimLoc to perform a fast and accurate estimation. We evaluate
our method on one dataset with appearance variations and one dataset that
focuses on demonstrating large-scale matching over a long flight in complicated
environments. On our two datasets, iSimLoc achieves 88.7\% and 83.8\%
successful retrieval rates with 1.5s inferencing time, compared to 45.8% and
39.7% using the next best method. These results demonstrate robust localization
in a range of environments.",None,-1
b3cc0139-6269-4755-8fbd-15f6f0eb9bca,Neural Contourlet Network for Monocular 360 Depth Estimation,0.550373,"For a monocular 360 image, depth estimation is a challenging because the
distortion increases along the latitude. To perceive the distortion, existing
methods devote to designing a deep and complex network architecture. In this
paper, we provide a new perspective that constructs an interpretable and sparse
representation for a 360 image. Considering the importance of the geometric
structure in depth estimation, we utilize the contourlet transform to capture
an explicit geometric cue in the spectral domain and integrate it with an
implicit cue in the spatial domain. Specifically, we propose a neural
contourlet network consisting of a convolutional neural network and a
contourlet transform branch. In the encoder stage, we design a spatial-spectral
fusion module to effectively fuse two types of cues. Contrary to the encoder,
we employ the inverse contourlet transform with learned low-pass subbands and
band-pass directional subbands to compose the depth in the decoder. Experiments
on the three popular panoramic image datasets demonstrate that the proposed
approach outperforms the state-of-the-art schemes with faster convergence. Code
is available at
https://github.com/zhijieshen-bjtu/Neural-Contourlet-Network-for-MODE.",None,-1
1d9ed28e-345a-4fcc-91ec-a75dde854e64,Meta-Learning Priors for Safe Bayesian Optimization,0.910318,"In robotics, optimizing controller parameters under safety constraints is an
important challenge. Safe Bayesian optimization (BO) quantifies uncertainty in
the objective and constraints to safely guide exploration in such settings.
Hand-designing a suitable probabilistic model can be challenging, however. In
the presence of unknown safety constraints, it is crucial to choose reliable
model hyper-parameters to avoid safety violations. Here, we propose a
data-driven approach to this problem by meta-learning priors for safe BO from
offline data. We build on a meta-learning algorithm, F-PACOH, capable of
providing reliable uncertainty quantification in settings of data scarcity. As
core contribution, we develop a novel framework for choosing safety-compliant
priors in a data-riven manner via empirical uncertainty metrics and a frontier
search algorithm. On benchmark functions and a high-precision motion system, we
demonstrate that our meta-learned priors accelerate the convergence of safe BO
approaches while maintaining safety.",https://tinyurl.com/safe-meta-bo,-1
7eefe4c0-b229-4e73-b8dd-3ed56ca05f9f,SmoothNets: Optimizing CNN architecture design for differentially private deep learning,0.298179,"The arguably most widely employed algorithm to train deep neural networks
with Differential Privacy is DPSGD, which requires clipping and noising of
per-sample gradients. This introduces a reduction in model utility compared to
non-private training. Empirically, it can be observed that this accuracy
degradation is strongly dependent on the model architecture. We investigated
this phenomenon and, by combining components which exhibit good individual
performance, distilled a new model architecture termed SmoothNet, which is
characterised by increased robustness to the challenges of DP-SGD training.
Experimentally, we benchmark SmoothNet against standard architectures on two
benchmark datasets and observe that our architecture outperforms others,
reaching an accuracy of 73.5\% on CIFAR-10 at $\varepsilon=7.0$ and 69.2\% at
$\varepsilon=7.0$ on ImageNette, a state-of-the-art result compared to prior
architectural modifications for DP.",https://github.com/fastai/imagenette,-1
06a9e8da-8822-407f-890e-35d192946e86,Same Author or Just Same Topic? Towards Content-Independent Style Representations,0.323622,"Linguistic style is an integral component of language. Recent advances in the
development of style representations have increasingly used training objectives
from authorship verification (AV): Do two texts have the same author? The
assumption underlying the AV training task (same author approximates same
writing style) enables self-supervised and, thus, extensive training. However,
a good performance on the AV task does not ensure good ""general-purpose"" style
representations. For example, as the same author might typically write about
certain topics, representations trained on AV might also encode content
information instead of style alone. We introduce a variation of the AV training
task that controls for content using conversation or domain labels. We evaluate
whether known style dimensions are represented and preferred over content
information through an original variation to the recently proposed STEL
framework. We find that representations trained by controlling for conversation
are better than representations trained with domain or no content control at
representing style independent from content.",https://github.com/nlpsoc/Style-Embeddings,-1
5b40bdcd-7799-409d-b565-5ff5ba5d9b51,Natural Language Specifications in Proof Assistants,0.0651355,"Interactive proof assistants are computer programs carefully constructed to
check a human-designed proof of a mathematical claim with high confidence in
the implementation. However, this only validates truth of a formal claim, which
may have been mistranslated from a claim made in natural language. This is
especially problematic when using proof assistants to formally verify the
correctness of software with respect to a natural language specification. The
translation from informal to formal remains a challenging, time-consuming
process that is difficult to audit for correctness. This paper argues that it
is possible to build support for natural language specifications within
existing proof assistants, in a way that complements the principles used to
establish trust and auditability in proof assistants themselves.",None,-1
4a169a9a-9542-4e49-940a-a068f22af8c1,Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering,0.900871,"Recent works on knowledge base question answering (KBQA) retrieve subgraphs
for easier reasoning. A desired subgraph is crucial as a small one may exclude
the answer but a large one might introduce more noises. However, the existing
retrieval is either heuristic or interwoven with the reasoning, causing
reasoning on the partial subgraphs, which increases the reasoning bias when the
intermediate supervision is missing. This paper proposes a trainable subgraph
retriever (SR) decoupled from the subsequent reasoning process, which enables a
plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive
experiments demonstrate SR achieves significantly better retrieval and QA
performance than existing retrieval methods. Via weakly supervised pre-training
as well as the end-to-end fine-tuning, SRl achieves new state-of-the-art
performance when combined with NSM, a subgraph-oriented reasoner, for
embedding-based KBQA methods.",https://github.com/RUCKBReasoning/SubgraphRetrievalKBQA,-1
c037fa5f-7b9d-4be2-ab4a-12e687152609,Evaluating Explainability for Graph Neural Networks,0.887414,"As post hoc explanations are increasingly used to understand the behavior of
graph neural networks (GNNs), it becomes crucial to evaluate the quality and
reliability of GNN explanations. However, assessing the quality of GNN
explanations is challenging as existing graph datasets have no or unreliable
ground-truth explanations for a given task. Here, we introduce a synthetic
graph data generator, ShapeGGen, which can generate a variety of benchmark
datasets (e.g., varying graph sizes, degree distributions, homophilic vs.
heterophilic graphs) accompanied by ground-truth explanations. Further, the
flexibility to generate diverse synthetic datasets and corresponding
ground-truth explanations allows us to mimic the data generated by various
real-world applications. We include ShapeGGen and several real-world graph
datasets into an open-source graph explainability library, GraphXAI. In
addition to synthetic and real-world graph datasets with ground-truth
explanations, GraphXAI provides data loaders, data processing functions,
visualizers, GNN model implementations, and evaluation metrics to benchmark the
performance of GNN explainability methods.",https://github.com/mims-harvard/GraphXAI,-1
08cf02b4-0a8a-4be3-a093-41e90882f213,AdaCat: Adaptive Categorical Discretization for Autoregressive Models,0.279581,"Autoregressive generative models can estimate complex continuous data
distributions, like trajectory rollouts in an RL environment, image
intensities, and audio. Most state-of-the-art models discretize continuous data
into several bins and use categorical distributions over the bins to
approximate the continuous data distribution. The advantage is that the
categorical distribution can easily express multiple modes and are
straightforward to optimize. However, such approximation cannot express sharp
changes in density without using significantly more bins, making it parameter
inefficient. We propose an efficient, expressive, multimodal parameterization
called Adaptive Categorical Discretization (AdaCat). AdaCat discretizes each
dimension of an autoregressive model adaptively, which allows the model to
allocate density to fine intervals of interest, improving parameter efficiency.
AdaCat generalizes both categoricals and quantile-based regression. AdaCat is a
simple add-on to any discretization-based distribution estimator. In
experiments, AdaCat improves density estimation for real-world tabular data,
images, audio, and trajectories, and improves planning in model-based offline
RL.",https://github.com/ColinQiyangLi/AdaCat,-1
974bd629-3288-440f-8615-f636409e4c4f,Visual Concepts Tokenization,0.304596,"Obtaining the human-like perception ability of abstracting visual concepts
from concrete pixels has always been a fundamental and important target in
machine learning research fields such as disentangled representation learning
and scene decomposition. Towards this goal, we propose an unsupervised
transformer-based Visual Concepts Tokenization framework, dubbed VCT, to
perceive an image into a set of disentangled visual concept tokens, with each
concept token responding to one type of independent visual concept.
Particularly, to obtain these concept tokens, we only use cross-attention to
extract visual information from the image tokens layer by layer without
self-attention between concept tokens, preventing information leakage across
concept tokens. We further propose a Concept Disentangling Loss to facilitate
that different concept tokens represent independent visual concepts. The
cross-attention and disentangling loss play the role of induction and mutual
exclusion for the concept tokens, respectively. Extensive experiments on
several popular datasets verify the effectiveness of VCT on the tasks of
disentangled representation learning and scene decomposition. VCT achieves the
state of the art results by a large margin.",https://github.com/thomasmry/VCT,-1
9fabc5dd-2441-4100-8c47-8d1697cb97c6,SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model,0.873921,"Generic image inpainting aims to complete a corrupted image by borrowing
surrounding information, which barely generates novel content. By contrast,
multi-modal inpainting provides more flexible and useful controls on the
inpainted content, \eg, a text prompt can be used to describe an object with
richer attributes, and a mask can be used to constrain the shape of the
inpainted object rather than being only considered as a missing area. We
propose a new diffusion-based model named SmartBrush for completing a missing
region with an object using both text and shape-guidance. While previous work
such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not
support shape guidance and tend to modify background texture surrounding the
generated object. Our model incorporates both text and shape guidance with
precision control. To preserve the background better, we propose a novel
training and sampling strategy by augmenting the diffusion U-net with
object-mask prediction. Lastly, we introduce a multi-task training strategy by
jointly training inpainting with text-to-image generation to leverage more
training data. We conduct extensive experiments showing that our model
outperforms all baselines in terms of visual quality, mask controllability, and
background preservation.",https://github.com/CompVis/stable-diffusion,-1
9be6f02b-828d-40fc-bf05-d9ffedb2f167,CMNEROne at SemEval-2022 Task 11: Code-Mixed Named Entity Recognition by leveraging multilingual data,0.361296,"Identifying named entities is, in general, a practical and challenging task
in the field of Natural Language Processing. Named Entity Recognition on the
code-mixed text is further challenging due to the linguistic complexity
resulting from the nature of the mixing. This paper addresses the submission of
team CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER
task aimed to identify named entities on the code-mixed dataset. Our work
consists of Named Entity Recognition (NER) on the code-mixed dataset by
leveraging the multilingual data. We achieved a weighted average F1 score of
0.7044, i.e., 6% greater than the baseline.",https://github.com/scrapinghub/python-crfsuite,-1
47e2d540-ac4e-404a-b11d-f24f2bcde11e,"EMMT: A simultaneous eye-tracking, 4-electrode EEG and audio corpus for multi-modal reading and translation scenarios",0.353492,"We present the Eyetracked Multi-Modal Translation (EMMT) corpus, a dataset
containing monocular eye movement recordings, audio and 4-electrode
electroencephalogram (EEG) data of 43 participants. The objective was to
collect cognitive signals as responses of participants engaged in a number of
language intensive tasks involving different text-image stimuli settings when
translating from English to Czech.
  Each participant was exposed to 32 text-image stimuli pairs and asked to (1)
read the English sentence, (2) translate it into Czech, (3) consult the image,
(4) translate again, either updating or repeating the previous translation. The
text stimuli consisted of 200 unique sentences with 616 unique words coupled
with 200 unique images as the visual stimuli.
  The recordings were collected over a two week period and all the participants
included in the study were Czech natives with strong English skills. Due to the
nature of the tasks involved in the study and the relatively large number of
participants involved, the corpus is well suited for research in Translation
Process Studies, Cognitive Sciences among other disciplines.",https://github.com/NEUREM3/recording-code-for-eyetracked-multi-modal-translation,-1
35f7fd3e-15b7-40fc-8469-5bdf790b5e82,Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model,0.649663,"The recently released NLLB-200 is a set of multilingual Neural Machine
Translation models that cover 202 languages. The largest model is based on a
Mixture of Experts architecture and achieves SoTA results across many language
pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just
for inference. In this work, we propose a pruning method that enables the
removal of up to 80% of experts without further finetuning and with a
negligible loss in translation quality, which makes it feasible to run the
model on a single 32GB GPU. Further analysis suggests that our pruning metrics
can identify language-specific experts.",None,-1
8bc4e8fb-2964-4750-8318-5307fe3f1f05,D'ARTAGNAN: Counterfactual Video Generation,0.399764,"Causally-enabled machine learning frameworks could help clinicians to
identify the best course of treatments by answering counterfactual questions.
We explore this path for the case of echocardiograms by looking into the
variation of the Left Ventricle Ejection Fraction, the most essential clinical
metric gained from these examinations. We combine deep neural networks, twin
causal networks and generative adversarial methods for the first time to build
D'ARTAGNAN (Deep ARtificial Twin-Architecture GeNerAtive Networks), a novel
causal generative model. We demonstrate the soundness of our approach on a
synthetic dataset before applying it to cardiac ultrasound videos to answer the
question: ""What would this echocardiogram look like if the patient had a
different ejection fraction?"". To do so, we generate new ultrasound videos,
retaining the video style and anatomy of the original patient, while modifying
the Ejection Fraction conditioned on a given input. We achieve an SSIM score of
0.79 and an R2 score of 0.51 on the counterfactual videos. Code and models are
available at: https://github.com/HReynaud/dartagnan.",https://github.com/HReynaud/dartagnan,-1
7519908b-9ebf-4e93-aafe-056a9fe79925,KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,0.467197,"Relative positional embeddings (RPE) have received considerable attention
since RPEs effectively model the relative distance among tokens and enable
length extrapolation. We propose KERPLE, a framework that generalizes relative
position embedding for extrapolation by kernelizing positional differences. We
achieve this goal using conditionally positive definite (CPD) kernels, a class
of functions known for generalizing distance metrics. To maintain the inner
product interpretation of self-attention, we show that a CPD kernel can be
transformed into a PD kernel by adding a constant offset. This offset is
implicitly absorbed in the Softmax normalization during self-attention. The
diversity of CPD kernels allows us to derive various RPEs that enable length
extrapolation in a principled way. Experiments demonstrate that the logarithmic
variant achieves excellent extrapolation performance on three large language
modeling datasets. Our implementation and pretrained checkpoints are released
at https://github.com/chijames/KERPLE.git.",https://github.com/chijames/KERPLE.git,-1
96163f18-8d52-4f63-8db2-df957e193b31,CTM -- A Model for Large-Scale Multi-View Tweet Topic Classification,0.368056,"Automatically associating social media posts with topics is an important
prerequisite for effective search and recommendation on many social media
platforms. However, topic classification of such posts is quite challenging
because of (a) a large topic space (b) short text with weak topical cues, and
(c) multiple topic associations per post. In contrast to most prior work which
only focuses on post classification into a small number of topics ($10$-$20$),
we consider the task of large-scale topic classification in the context of
Twitter where the topic space is $10$ times larger with potentially multiple
topic associations per Tweet. We address the challenges above by proposing a
novel neural model, CTM that (a) supports a large topic space of $300$ topics
and (b) takes a holistic approach to tweet content modeling -- leveraging
multi-modal content, author context, and deeper semantic cues in the Tweet. Our
method offers an effective way to classify Tweets into topics at scale by
yielding superior performance to other approaches (a relative lift of
$\mathbf{20}\%$ in median average precision score) and has been successfully
deployed in production at Twitter.",None,-1
58511f1d-120d-46bb-88e5-14f05ccaa7e0,End-to-end Document Recognition and Understanding with Dessurt,0.936588,"We introduce Dessurt, a relatively simple document understanding transformer
capable of being fine-tuned on a greater variety of document tasks than prior
methods. It receives a document image and task string as input and generates
arbitrary text autoregressively as output. Because Dessurt is an end-to-end
architecture that performs text recognition in addition to the document
understanding, it does not require an external recognition model as prior
methods do. Dessurt is a more flexible model than prior methods and is able to
handle a variety of document domains and tasks. We show that this model is
effective at 9 different dataset-task combinations.",https://github.com/herobd/dessurt2,-1
6797656f-250a-4356-8ec1-cbd95003b08d,NERDA-Con: Extending NER models for Continual Learning -- Integrating Distinct Tasks and Updating Distribution Shifts,0.424776,"With increasing applications in areas such as biomedical information
extraction pipelines and social media analytics, Named Entity Recognition (NER)
has become an indispensable tool for knowledge extraction. However, with the
gradual shift in language structure and vocabulary, NERs are plagued with
distribution shifts, making them redundant or not as profitable without
re-training. Re-training NERs based on Large Language Models (LLMs) from
scratch over newly acquired data poses economic disadvantages. In contrast,
re-training only with newly acquired data will result in Catastrophic
Forgetting of previously acquired knowledge. Therefore, we propose NERDA-Con, a
pipeline for training NERs with LLM bases by incorporating the concept of
Elastic Weight Consolidation (EWC) into the NER fine-tuning NERDA pipeline. As
we believe our work has implications to be utilized in the pipeline of
continual learning and NER, we open-source our code as well as provide the
fine-tuning library of the same name NERDA-Con at
https://github.com/SupritiVijay/NERDA-Con and
https://pypi.org/project/NERDA-Con/.",https://github.com/SupritiVijay/NERDA-Con,-1
fc6cdae2-2b75-4a64-9899-0d04981a61f2,Commonsense Knowledge from Scene Graphs for Textual Environments,0.0727575,"Text-based games are becoming commonly used in reinforcement learning as
real-world simulation environments. They are usually imperfect information
games, and their interactions are only in the textual modality. To challenge
these games, it is effective to complement the missing information by providing
knowledge outside the game, such as human common sense. However, such knowledge
has only been available from textual information in previous works. In this
paper, we investigate the advantage of employing commonsense reasoning obtained
from visual datasets such as scene graph datasets. In general, images convey
more comprehensive information compared with text for humans. This property
enables to extract commonsense relationship knowledge more useful for acting
effectively in a game. We compare the statistics of spatial relationships
available in Visual Genome (a scene graph dataset) and ConceptNet (a text-based
knowledge) to analyze the effectiveness of introducing scene graph datasets. We
also conducted experiments on a text-based game task that requires commonsense
reasoning. Our experimental results demonstrated that our proposed methods have
higher and competitive performance than existing state-of-the-art methods.",None,-1
0d2436e5-ccec-4c64-b98f-f93ce589dbe5,A multi-domain virtual network embedding algorithm with delay prediction,0.0328,"Virtual network embedding (VNE) is an crucial part of network virtualization
(NV), which aims to map the virtual networks (VNs) to a shared substrate
network (SN). With the emergence of various delay-sensitive applications, how
to improve the delay performance of the system has become a hot topic in
academic circles. Based on extensive research, we proposed a multi-domain
virtual network embedding algorithm based on delay prediction (DP-VNE).
Firstly, the candidate physical nodes are selected by estimating the delay of
virtual requests, then particle swarm optimization (PSO) algorithm is used to
optimize the mapping process, so as to reduce the delay of the system. The
simulation results show that compared with the other three advanced algorithms,
the proposed algorithm can significantly reduce the system delay while keeping
other indicators unaffected.",None,9769
2eaa3aa2-59bc-4ec1-a347-2b4a3d7948a2,Copiloting Autonomous Multi-Robot Missions: A Game-inspired Supervisory Control Interface,0.33054,"Real-world deployment of new technology and capabilities can be daunting. The
recent DARPA Subterranean (SubT) Challenge, for instance, aimed at the
advancement of robotic platforms and autonomy capabilities in three one-year
development pushes. While multi-agent systems are traditionally deployed in
controlled and structured environments that allow for controlled testing (e.g.,
warehouses), the SubT challenge targeted various types of unknown underground
environments that imposed the risk of robot loss in the case of failure. In
this work, we introduce a video game-inspired interface, an autonomous mission
assistant, and test and deploy these using a heterogeneous multi-agent system
in challenging environments. This work leads to improved human-supervisory
control for a multi-agent system reducing overhead from application switching,
task planning, execution, and verification while increasing available
exploration time with this human-autonomy teaming platform.",None,4549
6d5d4c71-506b-4a77-9dd3-37efbfc326aa,Dress Well via Fashion Cognitive Learning,0.11968,"Fashion compatibility models enable online retailers to easily obtain a large
number of outfit compositions with good quality. However, effective fashion
recommendation demands precise service for each customer with a deeper
cognition of fashion. In this paper, we conduct the first study on fashion
cognitive learning, which is fashion recommendations conditioned on personal
physical information. To this end, we propose a Fashion Cognitive Network (FCN)
to learn the relationships among visual-semantic embedding of outfit
composition and appearance features of individuals. FCN contains two
submodules, namely outfit encoder and Multi-label Graph Neural Network
(ML-GCN). The outfit encoder uses a convolutional layer to encode an outfit
into an outfit embedding. The latter module learns label classifiers via
stacked GCN. We conducted extensive experiments on the newly collected O4U
dataset, and the results provide strong qualitative and quantitative evidence
that our framework outperforms alternative methods.",None,-1
34d26ea8-3f07-43d6-ae89-9afa1ede77fa,Learnable Polyphase Sampling for Shift Invariant and Equivariant Convolutional Networks,0.0807375,"We propose learnable polyphase sampling (LPS), a pair of learnable
down/upsampling layers that enable truly shift-invariant and equivariant
convolutional networks. LPS can be trained end-to-end from data and generalizes
existing handcrafted downsampling layers. It is widely applicable as it can be
integrated into any convolutional network by replacing down/upsampling layers.
We evaluate LPS on image classification and semantic segmentation. Experiments
show that LPS is on-par with or outperforms existing methods in both
performance and shift consistency. For the first time, we achieve true
shift-equivariance on semantic segmentation (PASCAL VOC), i.e., 100% shift
consistency, outperforming baselines by an absolute 3.3%.",https://raymondyeh07.github.io/learnable_polyphase_sampling/,-1
08fc8c9f-76a4-42ca-8a3b-cb319234b5bc,Keke AI Competition: Solving puzzle levels in a dynamically changing mechanic space,0.24957,"The Keke AI Competition introduces an artificial agent competition for the
game Baba is You - a Sokoban-like puzzle game where players can create rules
that influence the mechanics of the game. Altering a rule can cause temporary
or permanent effects for the rest of the level that could be part of the
solution space. The nature of these dynamic rules and the deterministic aspect
of the game creates a challenge for AI to adapt to a variety of mechanic
combinations in order to solve a level. This paper describes the framework and
evaluation metrics used to rank submitted agents and baseline results from
sample tree search agents.",https://github.com/MasterMilkX/KekeCompetition/wiki/Baba-Simulation-Code,-1
1964cf85-6b0d-4333-830f-9b74a6b2f4d8,Style Matters! Investigating Linguistic Style in Online Communities,0.2038,"Content has historically been the primary lens used to study language in
online communities. This paper instead focuses on the linguistic style of
communities. While we know that individuals have distinguishable styles, here
we ask whether communities have distinguishable styles. Additionally, while
prior work has relied on a narrow definition of style, we employ a broad
definition involving 262 features to analyze the linguistic style of 9 online
communities from 3 social media platforms discussing politics, television and
travel. We find that communities indeed have distinct styles. Also, style is an
excellent predictor of group membership (F-score 0.952 and Accuracy 96.09%).
While on average it is statistically equivalent to predictions using content
alone, it is more resilient to reductions in training data.",https://github.com/pushshift/api,8411
04417baf-d00d-4417-a990-e97e5cc934a2,The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error,0.441561,"Discourse analysis allows us to attain inferences of a text document that
extend beyond the sentence-level. The current performance of discourse models
is very low on texts outside of the training distribution's coverage,
diminishing the practical utility of existing models. There is need for a
measure that can inform us to what extent our model generalizes from the
training to the test sample when these samples may be drawn from distinct
distributions. While this can be estimated via distribution shift, we argue
that this does not directly correlate with change in the observed error of a
classifier (i.e. error-gap). Thus, we propose to use a statistic from the
theoretical domain adaptation literature which can be directly tied to
error-gap. We study the bias of this statistic as an estimator of error-gap
both theoretically and through a large-scale empirical study of over 2400
experiments on 6 discourse datasets from domains including, but not limited to:
news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not
only motivate our proposal and help us to understand its limitations, but also
provide insight on the properties of discourse models and datasets which
improve performance in domain adaptation. For instance, we find that non-news
datasets are slightly easier to transfer to than news datasets when the
training and test sets are very different. Our code and an associated Python
package are available to allow practitioners to make more informed model and
dataset choices.",https://github.com/anthonysicilia/change-that-matters-,-1
23b8528a-83df-4f4a-aaa0-4f1d01ba6be9,Point-Level Region Contrast for Object Detection Pre-Training,0.695713,"In this work we present point-level region contrast, a self-supervised
pre-training approach for the task of object detection. This approach is
motivated by the two key factors in detection: localization and recognition.
While accurate localization favors models that operate at the pixel- or
point-level, correct recognition typically relies on a more holistic,
region-level view of objects. Incorporating this perspective in pre-training,
our approach performs contrastive learning by directly sampling individual
point pairs from different regions. Compared to an aggregated representation
per region, our approach is more robust to the change in input region quality,
and further enables us to implicitly improve initial region assignments via
online knowledge distillation during training. Both advantages are important
when dealing with imperfect regions encountered in the unsupervised setting.
Experiments show point-level region contrast improves on state-of-the-art
pre-training methods for object detection and segmentation across multiple
tasks and datasets, and we provide extensive ablation studies and
visualizations to aid understanding. Code will be made available.",None,115578
8f4bd2de-e28c-494a-b349-f75dd81d8814,The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations,0.988578,"Machine learning models in safety-critical settings like healthcare are often
blackboxes: they contain a large number of parameters which are not transparent
to users. Post-hoc explainability methods where a simple, human-interpretable
model imitates the behavior of these blackbox models are often proposed to help
users trust model predictions. In this work, we audit the quality of such
explanations for different protected subgroups using real data from four
settings in finance, healthcare, college admissions, and the US justice system.
Across two different blackbox model architectures and four popular
explainability methods, we find that the approximation quality of explanation
models, also known as the fidelity, differs significantly between subgroups. We
also demonstrate that pairing explainability methods with recent advances in
robust machine learning can improve explanation fairness in some settings.
However, we highlight the importance of communicating details of non-zero
fidelity gaps to users, since a single solution might not exist across all
settings. Finally, we discuss the implications of unfair explanation models as
a challenging and understudied problem facing the machine learning community.",https://github.com/MLforHealth/ExplanationsSubpopulations,-1
2e9238af-c529-4a8c-98d7-ef9dc6665ee8,Dialogue Meaning Representation for Task-Oriented Dialogue Systems,0.404541,"Dialogue meaning representation formulates natural language utterance
semantics in their conversational context in an explicit and machine-readable
form. Previous work typically follows the intent-slot framework, which is easy
for annotation yet limited in scalability for complex linguistic expressions. A
line of works alleviates the representation issue by introducing hierarchical
structures but challenging to express complex compositional semantics, such as
negation and coreference. We propose Dialogue Meaning Representation (DMR), a
pliable and easily extendable representation for task-oriented dialogue. Our
representation contains a set of nodes and edges to represent rich
compositional semantics. Moreover, we propose an inheritance hierarchy
mechanism focusing on domain extensibility. Additionally, we annotated
DMR-FastFood, a multi-turn dialogue dataset with more than 70k utterances, with
DMR. We propose two evaluation tasks to evaluate different dialogue models and
a novel coreference resolution model GNNCoref for the graph-based coreference
resolution task. Experiments show that DMR can be parsed well with pre-trained
Seq2Seq models, and GNNCoref outperforms the baseline models by a large margin.",https://github.com/amazon-research/dialogue-meaning-representation,20692
24749b87-12b5-4693-ade3-4380b3cb55f6,Diffeomorphic Counterfactuals with Generative Models,0.510276,"Counterfactuals can explain classification decisions of neural networks in a
human interpretable way. We propose a simple but effective method to generate
such counterfactuals. More specifically, we perform a suitable diffeomorphic
coordinate transformation and then perform gradient ascent in these coordinates
to find counterfactuals which are classified with great confidence as a
specified target class. We propose two methods to leverage generative models to
construct such suitable coordinate systems that are either exactly or
approximately diffeomorphic. We analyze the generation process theoretically
using Riemannian differential geometry and validate the quality of the
generated counterfactuals using various qualitative and quantitative measures.",https://github.com/annahdo/counterfactuals,-1
9cc15ae1-3533-46c0-8cda-0d59f8df73cd,Momentum Diminishes the Effect of Spectral Bias in Physics-Informed Neural Networks,0.154703,"Physics-informed neural network (PINN) algorithms have shown promising
results in solving a wide range of problems involving partial differential
equations (PDEs). However, they often fail to converge to desirable solutions
when the target function contains high-frequency features, due to a phenomenon
known as spectral bias. In the present work, we exploit neural tangent kernels
(NTKs) to investigate the training dynamics of PINNs evolving under stochastic
gradient descent with momentum (SGDM). This demonstrates SGDM significantly
reduces the effect of spectral bias. We have also examined why training a model
via the Adam optimizer can accelerate the convergence while reducing the
spectral bias. Moreover, our numerical experiments have confirmed that
wide-enough networks using SGDM still converge to desirable solutions, even in
the presence of high-frequency features. In fact, we show that the width of a
network plays a critical role in convergence.",None,-1
ee681f79-07a2-4dc8-a5e6-0f2ae2379c8a,Improving Fine-tuning of Self-supervised Models with Contrastive Initialization,0.211209,"Self-supervised learning (SSL) has achieved remarkable performance in
pretraining the models that can be further used in downstream tasks via
fine-tuning. However, these self-supervised models may not capture meaningful
semantic information since the images belonging to the same class are always
regarded as negative pairs in the contrastive loss. Consequently, the images of
the same class are often located far away from each other in learned feature
space, which would inevitably hamper the fine-tuning process. To address this
issue, we seek to provide a better initialization for the self-supervised
models by enhancing the semantic information. To this end, we propose a
Contrastive Initialization (COIN) method that breaks the standard fine-tuning
pipeline by introducing an extra initialization stage before fine-tuning.
Extensive experiments show that, with the enriched semantics, our COIN
significantly outperforms existing methods without introducing extra training
cost and sets new state-of-the-arts on multiple downstream tasks.",https://github.com/PorientHaolin/COIN,-1
8eea630e-0b41-4d9a-852d-d4e66a9730a5,Exploring Transformer Backbones for Image Diffusion Models,0.115876,"We present an end-to-end Transformer based Latent Diffusion model for image
synthesis. On the ImageNet class conditioned generation task we show that a
Transformer based Latent Diffusion model achieves a 14.1FID which is comparable
to the 13.1FID score of a UNet based architecture. In addition to showing the
application of Transformer models for Diffusion based image synthesis this
simplification in architecture allows easy fusion and modeling of text and
image data. The multi-head attention mechanism of Transformers enables
simplified interaction between the image and text features which removes the
requirement for crossattention mechanism in UNet based Diffusion models.",None,-1
956461ef-03e5-4357-b941-1b3e67e348f1,Data Augmentation to Address Out-of-Vocabulary Problem in Low-Resource Sinhala-English Neural Machine Translation,0.186166,"Out-of-Vocabulary (OOV) is a problem for Neural Machine Translation (NMT).
OOV refers to words with a low occurrence in the training data, or to those
that are absent from the training data. To alleviate this, word or phrase-based
Data Augmentation (DA) techniques have been used. However, existing DA
techniques have addressed only one of these OOV types and limit to considering
either syntactic constraints or semantic constraints. We present a word and
phrase replacement-based DA technique that consider both types of OOV, by
augmenting (1) rare words in the existing parallel corpus, and (2) new words
from a bilingual dictionary. During augmentation, we consider both syntactic
and semantic properties of the words to guarantee fluency in the synthetic
sentences. This technique was experimented with low resource Sinhala-English
language pair. We observe with only semantic constraints in the DA, the results
are comparable with the scores obtained considering syntactic constraints, and
is favourable for low-resourced languages that lacks linguistic tool support.
Additionally, results can be further improved by considering both syntactic and
semantic constraints.",None,-1
f16dfecb-d145-4f7c-975d-9ec639e918cc,Can language models learn from explanations in context?,0.999953,"Language Models (LMs) can perform new tasks by adapting to a few in-context
examples. For humans, explanations that connect examples to task principles can
improve learning. We therefore investigate whether explanations of few-shot
examples can help LMs. We annotate questions from 40 challenging tasks with
answer explanations, and various matched control explanations. We evaluate how
different types of explanations, instructions, and controls affect zero- and
few-shot performance. We analyze these results using statistical multilevel
modeling techniques that account for the nested dependencies among conditions,
tasks, prompts, and models. We find that explanations can improve performance
-- even without tuning. Furthermore, explanations hand-tuned for performance on
a small validation set offer substantially larger benefits, and building a
prompt by selecting examples and explanations together substantially improves
performance over selecting examples alone. Finally, even untuned explanations
outperform carefully matched controls, suggesting that the benefits are due to
the link between an example and its explanation, rather than lower-level
features. However, only large models benefit. In summary, explanations can
support the in-context learning of large LMs on challenging tasks.",None,-1
4a4dd1fc-7d68-420c-a454-c289e9283db5,Mitigating Attacks on Artificial Intelligence-based Spectrum Sensing for Cellular Network Signals,0.303256,"Cellular networks (LTE, 5G, and beyond) are dramatically growing with high
demand from consumers and more promising than the other wireless networks with
advanced telecommunication technologies. The main goal of these networks is to
connect billions of devices, systems, and users with high-speed data
transmission, high cell capacity, and low latency, as well as to support a wide
range of new applications, such as virtual reality, metaverse, telehealth,
online education, autonomous and flying vehicles, advanced manufacturing, and
many more. To achieve these goals, spectrum sensing has been paid more
attention, along with new approaches using artificial intelligence (AI) methods
for spectrum management in cellular networks. This paper provides a
vulnerability analysis of spectrum sensing approaches using AI-based semantic
segmentation models for identifying cellular network signals under adversarial
attacks with and without defensive distillation methods. The results showed
that mitigation methods can significantly reduce the vulnerabilities of
AI-based spectrum sensing models against adversarial attacks.",None,-1
25a933a4-25ba-41e6-a554-06fee4feff25,Geometry Interaction Knowledge Graph Embeddings,0.957237,"Knowledge graph (KG) embeddings have shown great power in learning
representations of entities and relations for link prediction tasks. Previous
work usually embeds KGs into a single geometric space such as Euclidean space
(zero curved), hyperbolic space (negatively curved) or hyperspherical space
(positively curved) to maintain their specific geometric structures (e.g.,
chain, hierarchy and ring structures). However, the topological structure of
KGs appears to be complicated, since it may contain multiple types of geometric
structures simultaneously. Therefore, embedding KGs in a single space, no
matter the Euclidean space, hyperbolic space or hyperspheric space, cannot
capture the complex structures of KGs accurately. To overcome this challenge,
we propose Geometry Interaction knowledge graph Embeddings (GIE), which learns
spatial structures interactively between the Euclidean, hyperbolic and
hyperspherical spaces. Theoretically, our proposed GIE can capture a richer set
of relational information, model key inference patterns, and enable expressive
semantic matching across entities. Experimental results on three
well-established knowledge graph completion benchmarks show that our GIE
achieves the state-of-the-art performance with fewer parameters.",https://github.com/Lion-ZS/GIE,-1
5257b166-22b0-41c4-8217-c50ceea26521,Neural Maximum A Posteriori Estimation on Unpaired Data for Motion Deblurring,0.0666767,"Real-world dynamic scene deblurring has long been a challenging task since
paired blurry-sharp training data is unavailable. Conventional Maximum A
Posteriori estimation and deep learning-based deblurring methods are restricted
by handcrafted priors and synthetic blurry-sharp training pairs respectively,
thereby failing to generalize to real dynamic blurriness. To this end, we
propose a Neural Maximum A Posteriori (NeurMAP) estimation framework for
training neural networks to recover blind motion information and sharp content
from unpaired data. The proposed NeruMAP consists of a motion estimation
network and a deblurring network which are trained jointly to model the
(re)blurring process (i.e. likelihood function). Meanwhile, the motion
estimation network is trained to explore the motion information in images by
applying implicit dynamic motion prior, and in return enforces the deblurring
network training (i.e. providing sharp image prior). The proposed NeurMAP is an
orthogonal approach to existing deblurring neural networks, and is the first
framework that enables training image deblurring networks on unpaired datasets.
Experiments demonstrate our superiority on both quantitative metrics and visual
quality over state-of-the-art methods. Codes are available on
https://github.com/yjzhang96/NeurMAP-deblur.",https://github.com/yjzhang96/NeurMAP-deblur,-1
15a95987-3cbf-441b-bf8b-d54a341b8ebd,Cross-Lingual Knowledge Transfer for Clinical Phenotyping,0.402829,"Clinical phenotyping enables the automatic extraction of clinical conditions
from patient records, which can be beneficial to doctors and clinics worldwide.
However, current state-of-the-art models are mostly applicable to clinical
notes written in English. We therefore investigate cross-lingual knowledge
transfer strategies to execute this task for clinics that do not use the
English language and have a small amount of in-domain data available. We
evaluate these strategies for a Greek and a Spanish clinic leveraging clinical
notes from different clinical domains such as cardiology, oncology and the ICU.
Our results reveal two strategies that outperform the state-of-the-art:
Translation-based methods in combination with domain-specific encoders and
cross-lingual encoders plus adapters. We find that these strategies perform
especially well for classifying rare phenotypes and we advise on which method
to prefer in which situation. Our results show that using multilingual data
overall improves clinical phenotyping models and can compensate for data
sparseness.",https://github.com/neuron1682/cross-lingual-phenotype-prediction,20322
9244ef98-899c-47be-9056-95f777a91080,Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information,0.671592,"We describe a simple and effective method (Spectral Attribute removaL; SAL)
to remove private or guarded information from neural representations. Our
method uses matrix decomposition to project the input representations into
directions with reduced covariance with the guarded information rather than
maximal covariance as factorization methods normally use. We begin with linear
information removal and proceed to generalize our algorithm to the case of
nonlinear information removal using kernels. Our experiments demonstrate that
our algorithm retains better main task performance after removing the guarded
information compared to previous work. In addition, our experiments demonstrate
that we need a relatively small amount of guarded attribute data to remove
information about these attributes, which lowers the exposure to sensitive data
and is more suitable for low-resource scenarios. Code is available at
https://github.com/jasonshaoshun/SAL.",https://github.com/jasonshaoshun/SAL,-1
52bbf16c-086f-4702-9625-5dec01403f58,Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking,0.998752,"Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling
errors. Recent researches start from the pretrained knowledge of language
models and take multimodal information into CSC models to improve the
performance. However, they overlook the rich knowledge in the dictionary, the
reference book where one can learn how one character should be pronounced,
written, and used. In this paper, we propose the LEAD framework, which renders
the CSC model to learn heterogeneous knowledge from the dictionary in terms of
phonetics, vision, and meaning. LEAD first constructs positive and negative
samples according to the knowledge of character phonetics, glyphs, and
definitions in the dictionary. Then a unified contrastive learning-based
training scheme is employed to refine the representations of the CSC models.
Extensive experiments and detailed analyses on the SIGHAN benchmark datasets
demonstrate the effectiveness of our proposed methods.",https://github.com/geekjuruo/LEAD,-1
6e18104d-6003-4163-8ffa-fe2f143c6460,Deep Reinforcement Learning for Multi-class Imbalanced Training,0.291195,"With the rapid growth of memory and computing power, datasets are becoming
increasingly complex and imbalanced. This is especially severe in the context
of clinical data, where there may be one rare event for many cases in the
majority class. We introduce an imbalanced classification framework, based on
reinforcement learning, for training extremely imbalanced data sets, and extend
it for use in multi-class settings. We combine dueling and double deep
Q-learning architectures, and formulate a custom reward function and
episode-training procedure, specifically with the added capability of handling
multi-class imbalanced training. Using real-world clinical case studies, we
demonstrate that our proposed framework outperforms current state-of-the-art
imbalanced learning methods, achieving more fair and balanced classification,
while also significantly improving the prediction of minority classes.",None,-1
d4e62d19-f0ec-4548-87db-4f222ec07cdc,Automating Defeasible Reasoning in Law,0.0260612,"The paper studies defeasible reasoning in rule-based systems, in particular
about legal norms and contracts. We identify rule modifiers that specify how
rules interact and how they can be overridden. We then define rule
transformations that eliminate these modifiers, leading in the end to a
translation of rules to formulas. For reasoning with and about rules, we
contrast two approaches, one in a classical logic with SMT solvers as proof
engines, one in a non-monotonic logic with Answer Set Programming solvers.",None,-1
82533b04-cc6d-493f-9bb7-bff283076c39,Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks,0.712479,"Iterative text revision improves text quality by fixing grammatical errors,
rephrasing for better readability or contextual appropriateness, or
reorganizing sentence structures throughout a document. Most recent research
has focused on understanding and classifying different types of edits in the
iterative revision process from human-written text instead of building accurate
and robust systems for iterative text revision. In this work, we aim to build
an end-to-end text revision system that can iteratively generate helpful edits
by explicitly detecting editable spans (where-to-edit) with their corresponding
edit intents and then instructing a revision model to revise the detected edit
spans. Leveraging datasets from other related text editing NLP tasks, combined
with the specification of editable spans, leads our system to more accurately
model the process of iterative text refinement, as evidenced by empirical
results and human evaluations. Our system significantly outperforms previous
baselines on our text revision tasks and other standard text revision tasks,
including grammatical error correction, text simplification, sentence fusion,
and style transfer. Through extensive qualitative and quantitative analysis, we
make vital connections between edit intentions and writing quality, and better
computational modeling of iterative text revisions.",https://github.com/vipulraheja/iterater,-1
799be3af-d1ce-48ad-ac44-7597df3f7d37,Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation,0.155062,"Data augmentation is a technique to generate new training data based on
existing data. We evaluate the simple and cost-effective method of
concatenating the original data examples to build new training instances.
Continued training with such augmented data is able to improve off-the-shelf
Transformer and Conformer models that were optimized on the original data only.
We demonstrate considerable improvements on the LibriSpeech-960h test sets (WER
2.83 and 6.87 for test-clean and test-other), which carry over to models
combined with shallow fusion (WER 2.55 and 6.27). Our method of continued
training also leads to improvements of up to 0.9 WER on the ASR part of
CoVoST-2 for four non English languages, and we observe that the gains are
highly dependent on the size of the original training data. We compare
different concatenation strategies and found that our method does not need
speaker information to achieve its improvements. Finally, we demonstrate on two
datasets that our methods also works for speech translation tasks.",https://github.com/facebookresearch/fairseq,-1
05238201-3a6e-4abc-8711-a5bc930e7b78,A Spanish dataset for Targeted Sentiment Analysis of political headlines,0.297192,"Subjective texts have been studied by several works as they can induce
certain behaviours in their users. Most work focuses on user-generated texts in
social networks, but some other texts also comprise opinions on certain topics
and could influence judgement criteria during political decisions. In this
work, we address the task of Targeted Sentiment Analysis for the domain of news
headlines, published by the main outlets during the 2019 Argentinean
Presidential Elections. For this purpose, we present a polarity dataset of
1,976 headlines mentioning candidates in the 2019 elections at the target
level. Preliminary experiments with state-of-the-art classification algorithms
based on pre-trained linguistic models suggest that target information is
helpful for this task. We make our data and pre-trained models publicly
available.",None,-1
e0f286e6-695e-4e56-8951-4f54ae7f263b,Not All Errors are Equal: Learning Text Generation Metrics using Stratified Error Synthesis,0.262123,"Is it possible to build a general and automatic natural language generation
(NLG) evaluation metric? Existing learned metrics either perform
unsatisfactorily or are restricted to tasks where large human rating data is
already available. We introduce SESCORE, a model-based metric that is highly
correlated with human judgements without requiring human annotation, by
utilizing a novel, iterative error synthesis and severity scoring pipeline.
This pipeline applies a series of plausible errors to raw text and assigns
severity labels by simulating human judgements with entailment. We evaluate
SESCORE against existing metrics by comparing how their scores correlate with
human ratings. SESCORE outperforms all prior unsupervised metrics on multiple
diverse NLG tasks including machine translation, image captioning, and WebNLG
text generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average
Kendall correlation with human judgement from 0.154 to 0.195. SESCORE even
achieves comparable performance to the best supervised metric COMET, despite
receiving no human-annotated training data.",https://github.com/xu1998hz/SEScore,-1
5484159d-65cf-4c92-bfba-73bc719cb179,Earnings-22: A Practical Benchmark for Accents in the Wild,0.453664,"Modern automatic speech recognition (ASR) systems have achieved superhuman
Word Error Rate (WER) on many common corpora despite lacking adequate
performance on speech in the wild. Beyond that, there is a lack of real-world,
accented corpora to properly benchmark academic and commercial models. To
ensure this type of speech is represented in ASR benchmarking, we present
Earnings-22, a 125 file, 119 hour corpus of English-language earnings calls
gathered from global companies. We run a comparison across 4 commercial models
showing the variation in performance when taking country of origin into
consideration. Looking at hypothesis transcriptions, we explore errors common
to all ASR systems tested. By examining Individual Word Error Rate (IWER), we
find that key speech features impact model performance more for certain accents
than others. Earnings-22 provides a free-to-use benchmark of real-world,
accented audio to bridge academic and industrial research.",https://github.com/revdotcom/speech-datasets/tree/master/earnings22,-1
7a8a5899-9d81-4686-8007-7819cc21d34b,OpenEarthMap: A Benchmark Dataset for Global High-Resolution Land Cover Mapping,0.986037,"We introduce OpenEarthMap, a benchmark dataset, for global high-resolution
land cover mapping. OpenEarthMap consists of 2.2 million segments of 5000
aerial and satellite images covering 97 regions from 44 countries across 6
continents, with manually annotated 8-class land cover labels at a 0.25--0.5m
ground sampling distance. Semantic segmentation models trained on the
OpenEarthMap generalize worldwide and can be used as off-the-shelf models in a
variety of applications. We evaluate the performance of state-of-the-art
methods for unsupervised domain adaptation and present challenging problem
settings suitable for further technical development. We also investigate
lightweight models using automated neural architecture search for limited
computational resources and fast mapping. The dataset is available at
https://open-earth-map.org.",None,-1
947dc643-071e-4670-965e-ec74f28764fd,Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing,0.78174,"Generated texts from large pretrained language models have been shown to
exhibit a variety of harmful, human-like biases about various demographics.
These findings prompted large efforts aiming to understand and measure such
effects, with the goal of providing benchmarks that can guide the development
of techniques mitigating these stereotypical associations. However, as recent
research has pointed out, the current benchmarks lack a robust experimental
setup, consequently hindering the inference of meaningful conclusions from
their evaluation metrics. In this paper, we extend these arguments and
demonstrate that existing techniques and benchmarks aiming to measure
stereotypes tend to be inaccurate and consist of a high degree of experimental
noise that severely limits the knowledge we can gain from benchmarking language
models based on them. Accordingly, we propose a new framework for robustly
measuring and quantifying biases exhibited by generative language models.
Finally, we use this framework to investigate GPT-3's occupational gender bias
and propose prompting techniques for mitigating these biases without the need
for fine-tuning.",None,-1
44a569aa-c721-4347-89e7-cbbaa998df5b,Pushing the limits of fairness impossibility: Who's the fairest of them all?,0.36658,"The impossibility theorem of fairness is a foundational result in the
algorithmic fairness literature. It states that outside of special cases, one
cannot exactly and simultaneously satisfy all three common and intuitive
definitions of fairness - demographic parity, equalized odds, and predictive
rate parity. This result has driven most works to focus on solutions for one or
two of the metrics. Rather than follow suit, in this paper we present a
framework that pushes the limits of the impossibility theorem in order to
satisfy all three metrics to the best extent possible. We develop an
integer-programming based approach that can yield a certifiably optimal
post-processing method for simultaneously satisfying multiple fairness criteria
under small violations. We show experiments demonstrating that our
post-processor can improve fairness across the different definitions
simultaneously with minimal model performance reduction. We also discuss
applications of our framework for model selection and fairness explainability,
thereby attempting to answer the question: who's the fairest of them all?",None,-1
316facb3-1943-480e-9602-6f63b22a7f36,Comparison and Evaluation of Methods for a Predict+Optimize Problem in Renewable Energy,0.101093,"Algorithms that involve both forecasting and optimization are at the core of
solutions to many difficult real-world problems, such as in supply chains
(inventory optimization), traffic, and in the transition towards carbon-free
energy generation in battery/load/production scheduling in sustainable energy
systems. Typically, in these scenarios we want to solve an optimization problem
that depends on unknown future values, which therefore need to be forecast. As
both forecasting and optimization are difficult problems in their own right,
relatively few research has been done in this area. This paper presents the
findings of the ``IEEE-CIS Technical Challenge on Predict+Optimize for
Renewable Energy Scheduling,"" held in 2021. We present a comparison and
evaluation of the seven highest-ranked solutions in the competition, to provide
researchers with a benchmark problem and to establish the state of the art for
this benchmark, with the aim to foster and facilitate research in this area.
The competition used data from the Monash Microgrid, as well as weather data
and energy market data. It then focused on two main challenges: forecasting
renewable energy production and demand, and obtaining an optimal schedule for
the activities (lectures) and on-site batteries that lead to the lowest cost of
energy. The most accurate forecasts were obtained by gradient-boosted tree and
random forest models, and optimization was mostly performed using mixed integer
linear and quadratic programming. The winning method predicted different
scenarios and optimized over all scenarios jointly using a sample average
approximation method.",None,-1
302f9631-01e5-4395-93dd-f55a8b23f6d8,Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,0.763926,"Language models have been shown to perform better with an increase in scale
on a wide variety of tasks via the in-context learning paradigm. In this paper,
we investigate the hypothesis that the ability of a large language model to
in-context learn-perform a task is not uniformly spread across all of its
underlying components. Using a 66 billion parameter language model (OPT-66B)
across a diverse set of 14 downstream tasks, we find this is indeed the case:
$\sim$70% of attention heads and $\sim$20% of feed forward networks can be
removed with minimal decline in task performance. We find substantial overlap
in the set of attention heads (un)important for in-context learning across
tasks and number of in-context examples. We also address our hypothesis through
a task-agnostic lens, finding that a small set of attention heads in OPT-66B
score highly on their ability to perform primitive induction operations
associated with in-context learning, namely, prefix matching and copying. These
induction heads overlap with task-specific important heads, reinforcing
arguments by Olsson et al. (arXiv:2209.11895) regarding induction head
generality to more sophisticated behaviors associated with in-context learning.
Overall, our study provides several insights that indicate large language
models may be under-trained for in-context learning and opens up questions on
how to pre-train language models to more effectively perform in-context
learning.",github.com/amazon-science/llm-interpret,42346
a3bf09a6-35f4-4fe8-b788-47abbe54d1a8,Distance Matters in Human-Object Interaction Detection,0.53429,"Human-Object Interaction (HOI) detection has received considerable attention
in the context of scene understanding. Despite the growing progress on
benchmarks, we realize that existing methods often perform unsatisfactorily on
distant interactions, where the leading causes are two-fold: 1) Distant
interactions are by nature more difficult to recognize than close ones. A
natural scene often involves multiple humans and objects with intricate spatial
relations, making the interaction recognition for distant human-object largely
affected by complex visual context. 2) Insufficient number of distant
interactions in benchmark datasets results in under-fitting on these instances.
To address these problems, in this paper, we propose a novel two-stage method
for better handling distant interactions in HOI detection. One essential
component in our method is a novel Far Near Distance Attention module. It
enables information propagation between humans and objects, whereby the spatial
distance is skillfully taken into consideration. Besides, we devise a novel
Distance-Aware loss function which leads the model to focus more on distant yet
rare interactions. We conduct extensive experiments on two challenging datasets
- HICO-DET and V-COCO. The results demonstrate that the proposed method can
surpass existing approaches by a large margin, resulting in new
state-of-the-art performance.",None,-1
59f6520f-03a8-4c62-ad15-10b3b1267062,Prediction of Football Player Value using Bayesian Ensemble Approach,0.0600162,"The transfer fees of sports players have become astronomical. This is because
bringing players of great future value to the club is essential for their
survival. We present a case study on the key factors affecting the world's top
soccer players' transfer fees based on the FIFA data analysis. To predict each
player's market value, we propose an improved LightGBM model by optimizing its
hyperparameter using a Tree-structured Parzen Estimator (TPE) algorithm. We
identify prominent features by the SHapley Additive exPlanations (SHAP)
algorithm. The proposed method has been compared against the baseline
regression models (e.g., linear regression, lasso, elastic net, kernel ridge
regression) and gradient boosting model without hyperparameter optimization.
The optimized LightGBM model showed an excellent accuracy of approximately 3.8,
1.4, and 1.8 times on average compared to the regression baseline models, GBDT,
and LightGBM model in terms of RMSE. Our model offers interpretability in
deciding what attributes football clubs should consider in recruiting players
in the future.",None,-1
73b30ce8-f8dd-4696-a63c-f5866bddfe9c,PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN,0.102496,"Beyond topical relevance, passage ranking for open-domain factoid question
answering also requires a passage to contain an answer (answerability). While a
few recent studies have incorporated some reading capability into a ranker to
account for answerability, the ranker is still hindered by the noisy nature of
the training data typically available in this area, which considers any passage
containing an answer entity as a positive sample. However, the answer entity in
a passage is not necessarily mentioned in relation with the given question. To
address the problem, we propose an approach called \ttt{PReGAN} for Passage
Reranking based on Generative Adversarial Neural networks, which incorporates a
discriminator on answerability, in addition to a discriminator on topical
relevance. The goal is to force the generator to rank higher a passage that is
topically relevant and contains an answer. Experiments on five public datasets
show that \ttt{PReGAN} can better rank appropriate passages, which in turn,
boosts the effectiveness of QA systems, and outperforms the existing approaches
without using external data.",https://github.com/facebookresearch/DPR,-1
cd47a3b8-4e85-4a07-aa40-efa6ba35ae34,Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models,0.0476995,"Large language models (LLM) trained using the next-token-prediction
objective, such as GPT3 and PaLM, have revolutionized natural language
processing in recent years by showing impressive zero-shot and few-shot
capabilities across a wide range of tasks. In this work, we propose a simple
technique that significantly boosts the performance of LLMs without adding
computational cost. Our key observation is that, by performing the next token
prediction task with randomly selected past tokens masked out, we can improve
the quality of the learned representations for downstream language
understanding tasks. We hypothesize that randomly masking past tokens prevents
over-attending to recent tokens and encourages attention to tokens in the
distant past. We find that our method, Forgetful Causal Masking (FCM),
significantly improves both few-shot and finetuning performance of PaLM. We
further consider a simple extension, T-FCM, which introduces bidirectional
context to causal language model without altering the sequence order, and
further improves finetuning performance.",https://github.com/kingoflolz/mesh-transformer-jax,-1
34ecd008-ade9-45c4-801a-c0138543ebe8,Ham2Pose: Animating Sign Language Notation into Pose Sequences,0.89581,"Translating spoken languages into Sign languages is necessary for open
communication between the hearing and hearing-impaired communities. To achieve
this goal, we propose the first method for animating a text written in
HamNoSys, a lexical Sign language notation, into signed pose sequences. As
HamNoSys is universal by design, our proposed method offers a generic solution
invariant to the target Sign language. Our method gradually generates pose
predictions using transformer encoders that create meaningful representations
of the text and poses while considering their spatial and temporal information.
We use weak supervision for the training process and show that our method
succeeds in learning from partial and inaccurate data. Additionally, we offer a
new distance measurement that considers missing keypoints, to measure the
distance between pose sequences using DTW-MJE. We validate its correctness
using AUTSL, a large-scale Sign language dataset, show that it measures the
distance between pose sequences more accurately than existing measurements, and
use it to assess the quality of our generated pose sequences. Code for the data
pre-processing, the model, and the distance measurement is publicly released
for future research.",https://github.com/AmitMY/pose-format,3241
6a458e74-88a3-44d7-b48a-34e583c39b46,Trustworthy Social Bias Measurement,0.812324,"How do we design measures of social bias that we trust? While prior work has
introduced several measures, no measure has gained widespread trust: instead,
mounting evidence argues we should distrust these measures. In this work, we
design bias measures that warrant trust based on the cross-disciplinary theory
of measurement modeling. To combat the frequently fuzzy treatment of social
bias in NLP, we explicitly define social bias, grounded in principles drawn
from social science research. We operationalize our definition by proposing a
general bias measurement framework DivDist, which we use to instantiate 5
concrete bias measures. To validate our measures, we propose a rigorous testing
protocol with 8 testing criteria (e.g. predictive validity: do measures predict
biases in US employment?). Through our testing, we demonstrate considerable
evidence to trust our measures, showing they overcome conceptual, technical,
and empirical deficiencies present in prior measures.",https://github.com/rishibommasani/BiasMeasures,-1
efd69a82-4f4d-4486-8c9f-684344001294,Unsupervised Homography Estimation with Coplanarity-Aware GAN,0.473362,"Estimating homography from an image pair is a fundamental problem in image
alignment. Unsupervised learning methods have received increasing attention in
this field due to their promising performance and label-free training. However,
existing methods do not explicitly consider the problem of plane-induced
parallax, which will make the predicted homography compromised on multiple
planes. In this work, we propose a novel method HomoGAN to guide unsupervised
homography estimation to focus on the dominant plane. First, a multi-scale
transformer network is designed to predict homography from the feature pyramids
of input images in a coarse-to-fine fashion. Moreover, we propose an
unsupervised GAN to impose coplanarity constraint on the predicted homography,
which is realized by using a generator to predict a mask of aligned regions,
and then a discriminator to check if two masked feature maps are induced by a
single homography. To validate the effectiveness of HomoGAN and its components,
we conduct extensive experiments on a large-scale dataset, and the results show
that our matching error is 22% lower than the previous SOTA method. Code is
available at https://github.com/megvii-research/HomoGAN.",https://github.com/megvii-research/HomoGAN,-1
8ba3cf92-280f-4650-b0b4-72c02261fc58,On Label Granularity and Object Localization,0.395277,"Weakly supervised object localization (WSOL) aims to learn representations
that encode object location using only image-level category labels. However,
many objects can be labeled at different levels of granularity. Is it an
animal, a bird, or a great horned owl? Which image-level labels should we use?
In this paper we study the role of label granularity in WSOL. To facilitate
this investigation we introduce iNatLoc500, a new large-scale fine-grained
benchmark dataset for WSOL. Surprisingly, we find that choosing the right
training label granularity provides a much larger performance boost than
choosing the best WSOL algorithm. We also show that changing the label
granularity can significantly improve data efficiency.",https://github.com/tensorflow/models/blob/65407126c5adc216d606d360429fe12ed3c3f187/research/object_detection/configs/tf2/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.config,-1
492d83f8-f931-4723-a60d-034bfcef04c3,Personal Attribute Prediction from Conversations,0.335546,"Personal knowledge bases (PKBs) are critical to many applications, such as
Web-based chatbots and personalized recommendation. Conversations containing
rich personal knowledge can be regarded as a main source to populate the PKB.
Given a user, a user attribute, and user utterances from a conversational
system, we aim to predict the personal attribute value for the user, which is
helpful for the enrichment of PKBs. However, there are three issues existing in
previous studies: (1) manually labeled utterances are required for model
training; (2) personal attribute knowledge embedded in both utterances and
external resources is underutilized; (3) the performance on predicting some
difficult personal attributes is unsatisfactory. In this paper, we propose a
framework DSCGN based on the pre-trained language model with a noise-robust
loss function to predict personal attributes from conversations without
requiring any labeled utterances. We yield two categories of supervision, i.e.,
document-level supervision via a distant supervision strategy and
contextualized word-level supervision via a label guessing method, by mining
the personal attribute knowledge embedded in both unlabeled utterances and
external resources to fine-tune the language model. Extensive experiments over
two real-world data sets (i.e., a profession data set and a hobby data set)
show our framework obtains the best performance compared with all the twelve
baselines in terms of nDCG and MRR.",https://github.com/CodingPerson/DSCGN,-1
974c7588-e707-4069-8ed1-161bf5cd33d4,Revisiting AP Loss for Dense Object Detection: Adaptive Ranking Pair Selection,0.425304,"Average precision (AP) loss has recently shown promising performance on the
dense object detection task. However,a deep understanding of how AP loss
affects the detector from a pairwise ranking perspective has not yet been
developed.In this work, we revisit the average precision (AP)loss and reveal
that the crucial element is that of selecting the ranking pairs between
positive and negative samples.Based on this observation, we propose two
strategies to improve the AP loss. The first of these is a novel Adaptive
Pairwise Error (APE) loss that focusing on ranking pairs in both positive and
negative samples. Moreover,we select more accurate ranking pairs by exploiting
the normalized ranking scores and localization scores with a clustering
algorithm. Experiments conducted on the MSCOCO dataset support our analysis and
demonstrate the superiority of our proposed method compared with current
classification and ranking loss. The code is available at
https://github.com/Xudangliatiger/APE-Loss.",https://github.com/Xudangliatiger/APE-Loss,-1
8f711f2e-2906-4fc8-8dec-6a8fb28ff82c,Mildly Conservative Q-Learning for Offline Reinforcement Learning,0.854359,"Offline reinforcement learning (RL) defines the task of learning from a
static logged dataset without continually interacting with the environment. The
distribution shift between the learned policy and the behavior policy makes it
necessary for the value function to stay conservative such that
out-of-distribution (OOD) actions will not be severely overestimated. However,
existing approaches, penalizing the unseen actions or regularizing with the
behavior policy, are too pessimistic, which suppresses the generalization of
the value function and hinders the performance improvement. This paper explores
mild but enough conservatism for offline learning while not harming
generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD
actions are actively trained by assigning them proper pseudo Q values. We
theoretically show that MCQ induces a policy that behaves at least as well as
the behavior policy and no erroneous overestimation will occur for OOD actions.
Experimental results on the D4RL benchmarks demonstrate that MCQ achieves
remarkable performance compared with prior work. Furthermore, MCQ shows
superior generalization ability when transferring from offline to online, and
significantly outperforms baselines. Our code is publicly available at
https://github.com/dmksjfl/MCQ.",https://github.com/dmksjfl/MCQ,-1
9efe9995-107c-4fc0-b8b5-7b88f394925b,DeepLIIF: An Online Platform for Quantification of Clinical Pathology Slides,0.748379,"In the clinic, resected tissue samples are stained with Hematoxylin-and-Eosin
(H&E) and/or Immunhistochemistry (IHC) stains and presented to the pathologists
on glass slides or as digital scans for diagnosis and assessment of disease
progression. Cell-level quantification, e.g. in IHC protein expression scoring,
can be extremely inefficient and subjective. We present DeepLIIF
(https://deepliif.org), a first free online platform for efficient and
reproducible IHC scoring. DeepLIIF outperforms current state-of-the-art
approaches (relying on manual error-prone annotations) by virtually restaining
clinical IHC slides with more informative multiplex immunofluorescence
staining. Our DeepLIIF cloud-native platform supports (1) more than 150
proprietary/non-proprietary input formats via the Bio-Formats standard, (2)
interactive adjustment, visualization, and downloading of the IHC
quantification results and the accompanying restained images, (3) consumption
of an exposed workflow API programmatically or through interactive plugins for
open source whole slide image viewers such as QuPath/ImageJ, and (4) auto
scaling to efficiently scale GPU resources based on user demand.",https://github.com/nadeemlab/DeepLIIF,-1
eabf0fe4-9a58-4b2d-bdd9-6874a2eff926,Yet Another Format of Universal Dependencies for Korean,0.567008,"In this study, we propose a morpheme-based scheme for Korean dependency
parsing and adopt the proposed scheme to Universal Dependencies. We present the
linguistic rationale that illustrates the motivation and the necessity of
adopting the morpheme-based format, and develop scripts that convert between
the original format used by Universal Dependencies and the proposed
morpheme-based format automatically. The effectiveness of the proposed format
for Korean dependency parsing is then testified by both statistical and neural
models, including UDPipe and Stanza, with our carefully constructed
morpheme-based word embedding for Korean. morphUD outperforms parsing results
for all Korean UD treebanks, and we also present detailed error analyses.",https://github.com/jungyeul/morphUD-korean,-1
5dfc6a72-9982-460b-85cb-bb58a3fb948a,SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views,0.991195,"We introduce SparseNeuS, a novel neural rendering based method for the task
of surface reconstruction from multi-view images. This task becomes more
difficult when only sparse images are provided as input, a scenario where
existing neural reconstruction approaches usually produce incomplete or
distorted results. Moreover, their inability of generalizing to unseen new
scenes impedes their application in practice. Contrarily, SparseNeuS can
generalize to new scenes and work well with sparse images (as few as 2 or 3).
SparseNeuS adopts signed distance function (SDF) as the surface representation,
and learns generalizable priors from image features by introducing geometry
encoding volumes for generic surface prediction. Moreover, several strategies
are introduced to effectively leverage sparse views for high-quality
reconstruction, including 1) a multi-level geometry reasoning framework to
recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color
blending scheme for more reliable color prediction; 3) a consistency-aware
fine-tuning scheme to control the inconsistent regions caused by occlusion and
noise. Extensive experiments demonstrate that our approach not only outperforms
the state-of-the-art methods, but also exhibits good efficiency,
generalizability, and flexibility.",https://www.xxlong.site/SparseNeuSarXiv:2206.05737v2,-1
d6463ca0-2100-4597-895e-dd6834837127,Data Contamination: From Memorization to Exploitation,0.955182,"Pretrained language models are typically trained on massive web-based
datasets, which are often ""contaminated"" with downstream test sets. It is not
clear to what extent models exploit the contaminated data for downstream tasks.
We present a principled method to study this question. We pretrain BERT models
on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune
them on the relevant task. Comparing performance between samples seen and
unseen during pretraining enables us to define and quantify levels of
memorization and exploitation. Experiments with two models and three downstream
tasks show that exploitation exists in some cases, but in others the models
memorize the contaminated data, but do not exploit it. We show that these two
measures are affected by different factors such as the number of duplications
of the contaminated data and the model size. Our results highlight the
importance of analyzing massive web-scale datasets to verify that progress in
NLP is obtained by better language understanding and not better data
exploitation.",https://github.com/schwartz-lab-NLP/data_contamination,-1
9090cc97-74d1-4335-ae42-2cda20394d50,FEAT: Face Editing with Attention,0.32111,"Employing the latent space of pretrained generators has recently been shown
to be an effective means for GAN-based face manipulation. The success of this
approach heavily relies on the innate disentanglement of the latent space axes
of the generator. However, face manipulation often intends to affect local
regions only, while common generators do not tend to have the necessary spatial
disentanglement. In this paper, we build on the StyleGAN generator, and present
a method that explicitly encourages face manipulation to focus on the intended
regions by incorporating learned attention maps. During the generation of the
edited image, the attention map serves as a mask that guides a blending between
the original features and the modified ones. The guidance for the latent space
edits is achieved by employing CLIP, which has recently been shown to be
effective for text-driven edits. We perform extensive experiments and show that
our method can perform disentangled and controllable face manipulations based
on text descriptions by attending to the relevant regions only. Both
qualitative and quantitative experimental results demonstrate the superiority
of our method for facial region editing over alternative methods.",None,-1
68298ffa-37fc-44b9-a11e-b43f590c6b97,Linear programming word problems formulation using EnsembleCRF NER labeler and T5 text generator with data augmentations,0.306079,"We propose an ensemble approach to predict the labels in linear programming
word problems. The entity identification and the meaning representation are two
types of tasks to be solved in the NL4Opt competition. We propose the
ensembleCRF method to identify the named entities for the first task. We found
that single models didn't improve for the given task in our analysis. A set of
prediction models predict the entities. The generated results are combined to
form a consensus result in the ensembleCRF method. We present an ensemble text
generator to produce the representation sentences for the second task. We
thought of dividing the problem into multiple small tasks due to the overflow
in the output. A single model generates different representations based on the
prompt. All the generated text is combined to form an ensemble and produce a
mathematical meaning of a linear programming problem.",None,-1
abfe7816-bac2-4de7-9ba0-961ccdf5c0ac,Community Question Answering Entity Linking via Leveraging Auxiliary Data,0.659144,"Community Question Answering (CQA) platforms contain plenty of CQA texts
(i.e., questions and answers corresponding to the question) where named
entities appear ubiquitously. In this paper, we define a new task of CQA entity
linking (CQAEL) as linking the textual entity mentions detected from CQA texts
with their corresponding entities in a knowledge base. This task can facilitate
many downstream applications including expert finding and knowledge base
enrichment. Traditional entity linking methods mainly focus on linking entities
in news documents, and are suboptimal over this new task of CQAEL since they
cannot effectively leverage various informative auxiliary data involved in the
CQA platform to aid entity linking, such as parallel answers and two types of
meta-data (i.e., topic tags and users). To remedy this crucial issue, we
propose a novel transformer-based framework to effectively harness the
knowledge delivered by different kinds of auxiliary data to promote the linking
performance. We validate the superiority of our framework through extensive
experiments over a newly released CQAEL data set against state-of-the-art
entity linking methods.",https://github.com/yhLeeee/CQA EntityLinking,-1
59855050-1faf-418a-a97e-a7f8c7a861f0,NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles,0.304544,"In this paper, we present the fifth installment of the NELA-GT datasets,
NELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between
January 1st, 2022 and December 31st, 2022. Just as in past releases of the
dataset, NELA-GT-2022 includes outlet-level veracity labels from Media
Bias/Fact Check and tweets embedded in collected news articles. The
NELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H",https://github.com/MELALab/nela-gt,-1
7607802b-7788-43e9-aa3f-32912ad45145,Automatic Recognition and Classification of Future Work Sentences from Academic Articles in a Specific Domain,0.182514,"Future work sentences (FWS) are the particular sentences in academic papers
that contain the author's description of their proposed follow-up research
direction. This paper presents methods to automatically extract FWS from
academic papers and classify them according to the different future directions
embodied in the paper's content. FWS recognition methods will enable subsequent
researchers to locate future work sentences more accurately and quickly and
reduce the time and cost of acquiring the corpus. The current work on automatic
identification of future work sentences is relatively small, and the existing
research cannot accurately identify FWS from academic papers, and thus cannot
conduct data mining on a large scale. Furthermore, there are many aspects to
the content of future work, and the subdivision of the content is conducive to
the analysis of specific development directions. In this paper, Nature Language
Processing (NLP) is used as a case study, and FWS are extracted from academic
papers and classified into different types. We manually build an annotated
corpus with six different types of FWS. Then, automatic recognition and
classification of FWS are implemented using machine learning models, and the
performance of these models is compared based on the evaluation metrics. The
results show that the Bernoulli Bayesian model has the best performance in the
automatic recognition task, with the Macro F1 reaching 90.73%, and the SCIBERT
model has the best performance in the automatic classification task, with the
weighted average F1 reaching 72.63%. Finally, we extract keywords from FWS and
gain a deep understanding of the key content described in FWS, and we also
demonstrate that content determination in FWS will be reflected in the
subsequent research work by measuring the similarity between future work
sentences and the abstracts.",https://github.com/xiangyi-njust/FWS,-1
0d7975aa-7bbc-4b3c-914c-fcc94c0315f8,Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality,0.999972,"We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.",None,20526
bcc1667d-cabd-4552-a435-1fdf5a6e5642,Hybrid-Regressive Neural Machine Translation,0.637648,"In this work, we empirically confirm that non-autoregressive translation with
an iterative refinement mechanism (IR-NAT) suffers from poor acceleration
robustness because it is more sensitive to decoding batch size and computing
device setting than autoregressive translation (AT). Inspired by it, we attempt
to investigate how to combine the strengths of autoregressive and
non-autoregressive translation paradigms better. To this end, we demonstrate
through synthetic experiments that prompting a small number of AT's predictions
can promote one-shot non-autoregressive translation to achieve the equivalent
performance of IR-NAT. Following this line, we propose a new two-stage
translation prototype called hybrid-regressive translation (HRT). Specifically,
HRT first generates discontinuous sequences via autoregression (e.g., make a
prediction every k tokens, k>1) and then fills in all previously skipped tokens
at once in a non-autoregressive manner. We also propose a bag of techniques to
effectively and efficiently train HRT without adding any model parameters. HRT
achieves the state-of-the-art BLEU score of 28.49 on the WMT En-De task and is
at least 1.5x faster than AT, regardless of batch size and device. In addition,
another bonus of HRT is that it successfully inherits the good characteristics
of AT in the deep-encoder-shallow-decoder architecture. Concretely, compared to
the vanilla HRT with a 6-layer encoder and 6-layer decoder, the inference speed
of HRT with a 12-layer encoder and 1-layer decoder is further doubled on both
GPU and CPU without BLEU loss.",None,-1
c58397b0-f373-4556-9969-e7cb83bd7cc9,Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation,0.922771,"Non-autoregressive translation (NAT) models are typically trained with the
cross-entropy loss, which forces the model outputs to be aligned verbatim with
the target sentence and will highly penalize small shifts in word positions.
Latent alignment models relax the explicit alignment by marginalizing out all
monotonic latent alignments with the CTC loss. However, they cannot handle
non-monotonic alignments, which is non-negligible as there is typically global
word reordering in machine translation. In this work, we explore non-monotonic
latent alignments for NAT. We extend the alignment space to non-monotonic
alignments to allow for the global word reordering and further consider all
alignments that overlap with the target sentence. We non-monotonically match
the alignments to the target sentence and train the latent alignment model to
maximize the F1 score of non-monotonic matching. Extensive experiments on major
WMT benchmarks show that our method substantially improves the translation
performance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14
En-De with only one-iteration decoding, closing the gap between
non-autoregressive and autoregressive models.",None,-1
4b3ded05-0bbf-4e00-9411-416fb6d71dce,A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning,0.513942,"Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.",None,-1
6e1d2b7e-df96-4a14-b3e7-0c267c3bae49,Regionalized Optimization,0.0244367,"We propose a theoretical framework for non redundant reconstruction of a
global loss from a collection of local ones under constraints given by a
functor; we call this loss the regionalized loss in honor to Yedidia, Freeman,
Weiss' celebrated article `Constructing free-energy approximations and
generalized belief propagation algorithms' where a first example of
regionalized loss, for entropy and the marginal functor, is built. We show how
one can associate to these regionalized losses message passing algorithms for
finding their critical points. It is a natural mathematical framework for
optimization problems where there are multiple points of views on a dataset and
replaces message passing algorithms as canonical ways of finding the optima of
these problems. We explain how Generalized Belief propagation algorithms fall
into the framework we propose and propose novel message passing algorithms for
noisy channel networks.",None,-1
55a6c2f5-3a4b-44a6-a8be-8558f274e573,To Answer or Not to Answer? Improving Machine Reading Comprehension Model with Span-based Contrastive Learning,0.716549,"Machine Reading Comprehension with Unanswerable Questions is a difficult NLP
task, challenged by the questions which can not be answered from passages. It
is observed that subtle literal changes often make an answerable question
unanswerable, however, most MRC models fail to recognize such changes. To
address this problem, in this paper, we propose a span-based method of
Contrastive Learning (spanCL) which explicitly contrast answerable questions
with their answerable and unanswerable counterparts at the answer span level.
With spanCL, MRC models are forced to perceive crucial semantic changes from
slight literal differences. Experiments on SQuAD 2.0 dataset show that spanCL
can improve baselines significantly, yielding 0.86-2.14 absolute EM
improvements. Additional experiments also show that spanCL is an effective way
to utilize generated questions.",https://github.com/explosion/spaCy,-1
15b852c6-1cbd-4afc-893b-6ffd6411a3a6,Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations,0.999692,"We present Neural Feature Fusion Fields (N3F), a method that improves dense
2D image feature extractors when the latter are applied to the analysis of
multiple images reconstructible as a 3D scene. Given an image feature
extractor, for example pre-trained using self-supervision, N3F uses it as a
teacher to learn a student network defined in 3D space. The 3D student network
is similar to a neural radiance field that distills said features and can be
trained with the usual differentiable rendering machinery. As a consequence,
N3F is readily applicable to most neural rendering formulations, including
vanilla NeRF and its extensions to complex dynamic scenes. We show that our
method not only enables semantic understanding in the context of scene-specific
neural fields without the use of manual labels, but also consistently improves
over the self-supervised 2D baselines. This is demonstrated by considering
various tasks, such as 2D object retrieval, 3D segmentation, and scene editing,
in diverse sequences, including long egocentric videos in the EPIC-KITCHENS
benchmark.",None,96096
4f779920-588b-4c09-a7f4-c9561747ef0b,Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning,0.767272,"Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions
formed from seen state and object during training. Since the same state may be
various in the visual appearance while entangled with different objects, CZSL
is still a challenging task. Some methods recognize state and object with two
trained classifiers, ignoring the impact of the interaction between object and
state; the other methods try to learn the joint representation of the
state-object compositions, leading to the domain gap between seen and unseen
composition sets. In this paper, we propose a novel Siamese Contrastive
Embedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for
unseen composition recognition. Considering the entanglement between state and
object, we embed the visual feature into a Siamese Contrastive Space to capture
prototypes of them separately, alleviating the interaction between state and
object. In addition, we design a State Transition Module (STM) to increase the
diversity of training compositions, improving the robustness of the recognition
model. Extensive experiments indicate that our method significantly outperforms
the state-of-the-art approaches on three challenging benchmark datasets,
including the recent proposed C-QGA dataset.",https://github.com/XDUxyLi/SCEN-master,-1
9826c48f-2b75-4f80-8b55-188fccb2a44c,Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition,0.537487,"Self-supervised learning (SSL) is a powerful tool that allows learning of
underlying representations from unlabeled data. Transformer based models such
as wav2vec 2.0 and HuBERT are leading the field in the speech domain. Generally
these models are fine-tuned on a small amount of labeled data for a downstream
task such as Automatic Speech Recognition (ASR). This involves re-training the
majority of the model for each task. Adapters are small lightweight modules
which are commonly used in Natural Language Processing (NLP) to adapt
pre-trained models to new tasks. In this paper we propose applying adapters to
wav2vec 2.0 to reduce the number of parameters required for downstream ASR
tasks, and increase scalability of the model to multiple tasks or languages.
Using adapters we can perform ASR while training fewer than 10% of parameters
per task compared to full fine-tuning with little degradation of performance.
Ablations show that applying adapters into just the top few layers of the
pre-trained network gives similar performance to full transfer, supporting the
theory that higher pre-trained layers encode more phonemic information, and
further optimizing efficiency.",None,-1
568e1748-b0a8-4307-9115-d2ef2315bbbd,Fast Event-based Optical Flow Estimation by Triplet Matching,0.887678,"Event cameras are novel bio-inspired sensors that offer advantages over
traditional cameras (low latency, high dynamic range, low power, etc.). Optical
flow estimation methods that work on packets of events trade off speed for
accuracy, while event-by-event (incremental) methods have strong assumptions
and have not been tested on common benchmarks that quantify progress in the
field. Towards applications on resource-constrained devices, it is important to
develop optical flow algorithms that are fast, light-weight and accurate. This
work leverages insights from neuroscience, and proposes a novel optical flow
estimation scheme based on triplet matching. The experiments on publicly
available benchmarks demonstrate its capability to handle complex scenes with
comparable results as prior packet-based algorithms. In addition, the proposed
method achieves the fastest execution time (> 10 kHz) on standard CPUs as it
requires only three events in estimation. We hope that our research opens the
door to real-time, incremental motion estimation methods and applications in
real-world scenarios.",None,-1
b44f3164-475e-48d3-979e-debdff39285d,Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning,0.179963,"Practising and honing skills forms a fundamental component of how humans
learn, yet artificial agents are rarely specifically trained to perform them.
Instead, they are usually trained end-to-end, with the hope being that useful
skills will be implicitly learned in order to maximise discounted return of
some extrinsic reward function. In this paper, we investigate how skills can be
incorporated into the training of reinforcement learning (RL) agents in complex
environments with large state-action spaces and sparse rewards. To this end, we
created SkillHack, a benchmark of tasks and associated skills based on the game
of NetHack. We evaluate a number of baselines on this benchmark, as well as our
own novel skill-based method Hierarchical Kickstarting (HKS), which is shown to
outperform all other evaluated methods. Our experiments show that learning with
a prior knowledge of useful skills can significantly improve the performance of
agents on complex problems. We ultimately argue that utilising predefined
skills provides a useful inductive bias for RL problems, especially those with
large state-action spaces and sparse rewards.",https://github.com/ucl-dark/skillhack,-1
7c689472-8c9a-4982-95ef-57b6cbe7bcad,Measuring and Improving the Use of Graph Information in Graph Neural Networks,0.709562,"Graph neural networks (GNNs) have been widely used for representation
learning on graph data. However, there is limited understanding on how much
performance GNNs actually gain from graph data. This paper introduces a
context-surrounding GNN framework and proposes two smoothness metrics to
measure the quantity and quality of information obtained from graph data. A new
GNN model, called CS-GNN, is then designed to improve the use of graph
information based on the smoothness values of a graph. CS-GNN is shown to
achieve better performance than existing methods in different types of real
graphs.",None,-1
345e3520-81f4-453a-b340-3f5d727ddbaf,Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution,0.108426,"Nowadays, thermal infrared satellite remote sensors enable to extract very
interesting information at large scale, in particular Land Surface Temperature
(LST). However such data are limited in spatial and/or temporal resolutions
which prevents from an analysis at fine scales. For example, MODIS satellite
provides daily acquisitions with 1Km spatial resolutions which is not
sufficient to deal with highly heterogeneous environments as agricultural
parcels. Therefore, image super-resolution is a crucial task to better exploit
MODIS LSTs. This issue is tackled in this paper. We introduce a deep
learning-based algorithm, named Multi-residual U-Net, for super-resolution of
MODIS LST single-images. Our proposed network is a modified version of U-Net
architecture, which aims at super-resolving the input LST image from 1Km to
250m per pixel. The results show that our Multi-residual U-Net outperforms
other state-of-the-art methods.",https://github.com/IMT-Project-LTS-SR/MRUNet-for-MODIS-super-resolution,3504
92f72fc5-112e-4dec-9e50-48c90a3420ab,VITA: Video Instance Segmentation via Object Token Association,0.961558,"We introduce a novel paradigm for offline Video Instance Segmentation (VIS),
based on the hypothesis that explicit object-oriented information can be a
strong clue for understanding the context of the entire sequence. To this end,
we propose VITA, a simple structure built on top of an off-the-shelf
Transformer-based image instance segmentation model. Specifically, we use an
image object detector as a means of distilling object-specific contexts into
object tokens. VITA accomplishes video-level understanding by associating
frame-level object tokens without using spatio-temporal backbone features. By
effectively building relationships between objects using the condensed
information, VITA achieves the state-of-the-art on VIS benchmarks with a
ResNet-50 backbone: 49.8 AP, 45.7 AP on YouTube-VIS 2019 & 2021, and 19.6 AP on
OVIS. Moreover, thanks to its object token-based structure that is disjoint
from the backbone features, VITA shows several practical advantages that
previous offline VIS methods have not explored - handling long and
high-resolution videos with a common GPU, and freezing a frame-level detector
trained on image domain. Code is available at
https://github.com/sukjunhwang/VITA.",https://github.com/sukjunhwang/VITA,-1
469a44bc-bd89-4333-80d6-f5427d4b8eb4,Targeted Honeyword Generation with Language Models,0.0214113,"Honeywords are fictitious passwords inserted into databases in order to
identify password breaches. The major difficulty is how to produce honeywords
that are difficult to distinguish from real passwords. Although the generation
of honeywords has been widely investigated in the past, the majority of
existing research assumes attackers have no knowledge of the users. These
honeyword generating techniques (HGTs) may utterly fail if attackers exploit
users' personally identifiable information (PII) and the real passwords include
users' PII. In this paper, we propose to build a more secure and trustworthy
authentication system that employs off-the-shelf pre-trained language models
which require no further training on real passwords to produce honeywords while
retaining the PII of the associated real password, therefore significantly
raising the bar for attackers.
  We conducted a pilot experiment in which individuals are asked to distinguish
between authentic passwords and honeywords when the username is provided for
GPT-3 and a tweaking technique. Results show that it is extremely difficult to
distinguish the real passwords from the artifical ones for both techniques. We
speculate that a larger sample size could reveal a significant difference
between the two HGT techniques, favouring our proposed approach.",None,-1
7292e902-81aa-4b27-ba0e-e295744d1ff7,CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models,0.461353,"We motivate and introduce CHARD: Clinical Health-Aware Reasoning across
Dimensions, to investigate the capability of text generation models to act as
implicit clinical knowledge bases and generate free-flow textual explanations
about various health-related conditions across several dimensions. We collect
and present an associated dataset, CHARDat, consisting of explanations about 52
health conditions across three clinical dimensions. We conduct extensive
experiments using BART and T5 along with data augmentation, and perform
automatic, human, and qualitative analyses. We show that while our models can
perform decently, CHARD is very challenging with strong potential for further
exploration.",https://github.com/styfeng/CHARD,-1
53d3ac4e-d76f-412a-a2ff-a4f34eb5ea7d,Revisiting Attention Weights as Explanations from an Information Theoretic Perspective,0.0810403,"Attention mechanisms have recently demonstrated impressive performance on a
range of NLP tasks, and attention scores are often used as a proxy for model
explainability. However, there is a debate on whether attention weights can, in
fact, be used to identify the most important inputs to a model. We approach
this question from an information theoretic perspective by measuring the mutual
information between the model output and the hidden states. From extensive
experiments, we draw the following conclusions: (i) Additive and Deep attention
mechanisms are likely to be better at preserving the information between the
hidden states and the model output (compared to Scaled Dot-product); (ii)
ablation studies indicate that Additive attention can actively learn to explain
the importance of its input hidden representations; (iii) when attention values
are nearly the same, the rank order of attention values is not consistent with
the rank order of the mutual information(iv) Using Gumbel-Softmax with a
temperature lower than one, tends to produce a more skewed attention score
distribution compared to softmax and hence is a better choice for explainable
design; (v) some building blocks are better at preserving the correlation
between the ordered list of mutual information and attention weights order (for
e.g., the combination of BiLSTM encoder and Additive attention). Our findings
indicate that attention mechanisms do have the potential to function as a
shortcut to model explanations when they are carefully combined with other
model elements.",https://github.com/successar/AttentionExplanation,-1
eb21ee74-bb89-4509-84cb-ae24e8c439d9,Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing N-gram Language Models,0.462242,"Although over 300M around the world speak Bangla, scant work has been done in
improving Bangla voice-to-text transcription due to Bangla being a low-resource
language. However, with the introduction of the Bengali Common Voice 9.0 speech
dataset, Automatic Speech Recognition (ASR) models can now be significantly
improved. With 399hrs of speech recordings, Bengali Common Voice is the largest
and most diversified open-source Bengali speech corpus in the world. In this
paper, we outperform the SOTA pretrained Bengali ASR models by finetuning a
pretrained wav2vec2 model on the common voice dataset. We also demonstrate how
to significantly improve the performance of an ASR model by adding an n-gram
language model as a post-processor. Finally, we do some experiments and
hyperparameter tuning to generate a robust Bangla ASR model that is better than
the existing ASR models.",None,-1
2db28e47-a455-44da-a599-246fa0003dd4,Perceptual Quality Assessment of Omnidirectional Images,0.993223,"Omnidirectional images and videos can provide immersive experience of
real-world scenes in Virtual Reality (VR) environment. We present a perceptual
omnidirectional image quality assessment (IQA) study in this paper since it is
extremely important to provide a good quality of experience under the VR
environment. We first establish an omnidirectional IQA (OIQA) database, which
includes 16 source images and 320 distorted images degraded by 4 commonly
encountered distortion types, namely JPEG compression, JPEG2000 compression,
Gaussian blur and Gaussian noise. Then a subjective quality evaluation study is
conducted on the OIQA database in the VR environment. Considering that humans
can only see a part of the scene at one movement in the VR environment, visual
attention becomes extremely important. Thus we also track head and eye movement
data during the quality rating experiments. The original and distorted
omnidirectional images, subjective quality ratings, and the head and eye
movement data together constitute the OIQA database. State-of-the-art
full-reference (FR) IQA measures are tested on the OIQA database, and some new
observations different from traditional IQA are made.",None,-1
d4e6ff64-f750-4349-b86b-94ab36b6cae9,RNNCTPs: A Neural Symbolic Reasoning Method Using Dynamic Knowledge Partitioning Technology,0.117683,"Although traditional symbolic reasoning methods are highly interpretable,
their application in knowledge graph link prediction is limited due to their
low computational efficiency. In this paper, we propose a new neural symbolic
reasoning method: RNNCTPs, which improves computational efficiency by
re-filtering the knowledge selection of Conditional Theorem Provers (CTPs), and
is less sensitive to the embedding size parameter. RNNCTPs are divided into
relation selectors and predictors. The relation selectors are trained
efficiently and interpretably, so that the whole model can dynamically generate
knowledge for the inference of the predictor. In all four datasets, the method
shows competitive performance against traditional methods on the link
prediction task, and can have higher applicability to the selection of datasets
relative to CTPs.",None,-1
3021f713-6064-4f7c-a52c-8c8e244dd783,kogito: A Commonsense Knowledge Inference Toolkit,0.387911,"In this paper, we present kogito, an open-source tool for generating
commonsense inferences about situations described in text. kogito provides an
intuitive and extensible interface to interact with natural language generation
models that can be used for hypothesizing commonsense knowledge inference from
a textual input. In particular, kogito offers several features for targeted,
multi-granularity knowledge generation. These include a standardized API for
training and evaluating knowledge models, and generating and filtering
inferences from them. We also include helper functions for converting natural
language texts into a format ingestible by knowledge models - intermediate
pipeline stages such as knowledge head extraction from text, heuristic and
model-based knowledge head-relation matching, and an ability to define and use
custom knowledge relations. We make the code for kogito available at
https://github.com/epfl-nlp/kogito along with thorough documentation at
https://kogito.readthedocs.io.",https://github.com/epfl-nlp/kogito,-1
64646df3-d228-4919-b8bb-9ba9800112f6,Rank-N-Contrast: Learning Continuous Representations for Regression,0.708021,"Deep regression models typically learn in an end-to-end fashion without
explicitly emphasizing a regression-aware representation. Consequently, the
learned representations exhibit fragmentation and fail to capture the
continuous nature of sample orders, inducing suboptimal results across a wide
range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a
framework that learns continuous representations for regression by contrasting
samples against each other based on their rankings in the target space. We
demonstrate, theoretically and empirically, that RNC guarantees the desired
order of learned representations in accordance with the target orders, enjoying
not only better performance but also significantly improved robustness,
efficiency, and generalization. Extensive experiments using five real-world
regression datasets that span computer vision, human-computer interaction, and
healthcare verify that RNC achieves state-of-the-art performance, highlighting
its intriguing properties including better data efficiency, robustness to
spurious targets and data corruptions, and generalization to distribution
shifts. Code is available at: https://github.com/kaiwenzha/Rank-N-Contrast.",https://github.com/kaiwenzha/Rank-N-Contrast,-1
a5d8caae-2b24-4197-8b16-b4cef8532dd1,Localized Vision-Language Matching for Open-vocabulary Object Detection,0.614434,"In this work, we propose an open-vocabulary object detection method that,
based on image-caption pairs, learns to detect novel object classes along with
a given set of known classes. It is a two-stage training approach that first
uses a location-guided image-caption matching technique to learn class labels
for both novel and known classes in a weakly-supervised manner and second
specializes the model for the object detection task using known class
annotations. We show that a simple language model fits better than a large
contextualized language model for detecting novel objects. Moreover, we
introduce a consistency-regularization technique to better exploit
image-caption pair information. Our method compares favorably to existing
open-vocabulary detection approaches while being data-efficient. Source code is
available at https://github.com/lmb-freiburg/locov .",https://github.com/lmb-freiburg/locov,-1
91320d83-4748-44a3-bf6d-0dac79d7834f,KSG: Knowledge and Skill Graph,0.972676,"The knowledge graph (KG) is an essential form of knowledge representation
that has grown in prominence in recent years. Because it concentrates on
nominal entities and their relationships, traditional knowledge graphs are
static and encyclopedic in nature. On this basis, event knowledge graph (Event
KG) models the temporal and spatial dynamics by text processing to facilitate
downstream applications, such as question-answering, recommendation and
intelligent search. Existing KG research, on the other hand, mostly focuses on
text processing and static facts, ignoring the vast quantity of dynamic
behavioral information included in photos, movies, and pre-trained neural
networks. In addition, no effort has been done to include behavioral
intelligence information into the knowledge graph for deep reinforcement
learning (DRL) and robot learning. In this paper, we propose a novel dynamic
knowledge and skill graph (KSG), and then we develop a basic and specific KSG
based on CN-DBpedia. The nodes are divided into entity and attribute nodes,
with entity nodes containing the agent, environment, and skill (DRL policy or
policy representation), and attribute nodes containing the entity description,
pre-train network, and offline dataset. KSG can search for different agents'
skills in various environments and provide transferable information for
acquiring new skills. This is the first study that we are aware of that looks
into dynamic KSG for skill retrieval and learning. Extensive experimental
results on new skill learning show that KSG boosts new skill learning
efficiency.",None,-1
3a82b121-3c38-4bf5-bc75-51fc9ecc6ee7,Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,0.925727,"Large language models (LLMs) have shown remarkable reasoning capabilities
given chain-of-thought prompts (examples with intermediate reasoning steps).
Existing benchmarks measure reasoning ability indirectly, by evaluating
accuracy on downstream tasks such as mathematical reasoning. However, it is
unclear how these models obtain the answers and whether they rely on simple
heuristics rather than the generated chain-of-thought. To enable systematic
exploration of the reasoning ability of LLMs, we present a new synthetic
question-answering dataset called PrOntoQA, where each example is generated
from a synthetic world model represented in first-order logic. This allows us
to parse the generated chain-of-thought into symbolic proofs for formal
analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite
capable of making correct individual deduction steps, and so are generally
capable of reasoning, even in fictional contexts. However, they have difficulty
with proof planning: When multiple valid deduction steps are available, they
are not able to systematically explore the different options.",None,84287
6f666293-f27b-476f-8e59-0ced3e8ae43d,Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection,0.334273,"Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating
preventive care and to delay further progression. Speech based automatic AD
screening systems provide a non-intrusive and more scalable alternative to
other clinical screening techniques. Textual embedding features produced by
pre-trained language models (PLMs) such as BERT are widely used in such
systems. However, PLM domain fine-tuning is commonly based on the masked word
or sentence prediction costs that are inconsistent with the back-end AD
detection task. To this end, this paper investigates the use of prompt-based
fine-tuning of PLMs that consistently uses AD classification errors as the
training objective function. Disfluency features based on hesitation or pause
filler token frequencies are further incorporated into prompt phrases during
PLM fine-tuning. The decision voting based combination among systems using
different PLMs (BERT and RoBERTa) or systems with different fine-tuning
paradigms (conventional masked-language modelling fine-tuning and prompt-based
fine-tuning) is further applied. Mean, standard deviation and the maximum among
accuracy scores over 15 experiment runs are adopted as performance measurements
for the AD detection system. Mean detection accuracy of 84.20% (with std 2.09%,
best 87.5%) and 82.64% (with std 4.0%, best 89.58%) were obtained using manual
and ASR speech transcripts respectively on the ADReSS20 test set consisting of
48 elderly speakers.",https://github.com/yiwang454/prompt-ad-code,-1
0baf7a80-cc0b-4655-ba0e-0498e7189154,TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation,0.597776,"Knowledge graph embedding (KGE) aims to learn continuous vectors of relations
and entities in knowledge graph. Recently, transition-based KGE methods have
achieved promising performance, where the single relation vector learns to
translate head entity to tail entity. However, this scoring pattern is not
suitable for complex scenarios where the same entity pair has different
relations. Previous models usually focus on the improvement of entity
representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single
relation vector. In this paper, we propose a novel transition-based method,
TranS, for knowledge graph embedding. The single relation vector in traditional
scoring patterns is replaced with synthetic relation representation, which can
solve these issues effectively and efficiently. Experiments on a large
knowledge graph dataset, ogbl-wikikg2, show that our model achieves
state-of-the-art results.",None,-1
ce10972f-09b5-433f-a1a2-0bf772bb407c,Pronunciation Modeling of Foreign Words for Mandarin ASR by Considering the Effect of Language Transfer,0.49142,"One of the challenges in automatic speech recognition is foreign words
recognition. It is observed that a speaker's pronunciation of a foreign word is
influenced by his native language knowledge, and such phenomenon is known as
the effect of language transfer. This paper focuses on examining the phonetic
effect of language transfer in automatic speech recognition. A set of lexical
rules is proposed to convert an English word into Mandarin phonetic
representation. In this way, a Mandarin lexicon can be augmented by including
English words. Hence, the Mandarin ASR system becomes capable to recognize
English words without retraining or re-estimation of the acoustic model
parameters. Using the lexicon that derived from the proposed rules, the ASR
performance of Mandarin English mixed speech is improved without harming the
accuracy of Mandarin only speech. The proposed lexical rules are generalized
and they can be directly applied to unseen English words.",None,-1
a0ef30a3-661c-4633-a0e2-f66a5c7c51ff,Model-based Multi-agent Reinforcement Learning: Recent Progress and Prospects,0.519939,"Significant advances have recently been achieved in Multi-Agent Reinforcement
Learning (MARL) which tackles sequential decision-making problems involving
multiple participants. However, MARL requires a tremendous number of samples
for effective training. On the other hand, model-based methods have been shown
to achieve provable advantages of sample efficiency. However, the attempts of
model-based methods to MARL have just started very recently. This paper
presents a review of the existing research on model-based MARL, including
theoretical analyses, algorithms, and applications, and analyzes the advantages
and potential of model-based MARL. Specifically, we provide a detailed taxonomy
of the algorithms and point out the pros and cons for each algorithm according
to the challenges inherent to multi-agent scenarios. We also outline promising
directions for future development of this field.",None,-1
ddb92509-ed64-476e-9acc-0989d81f4db5,MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering,0.783259,"Visual language data such as plots, charts, and infographics are ubiquitous
in the human world. However, state-of-the-art vision-language models do not
perform well on these data. We propose MatCha (Math reasoning and Chart
derendering pretraining) to enhance visual language models' capabilities in
jointly modeling charts/plots and language data. Specifically, we propose
several pretraining tasks that cover plot deconstruction and numerical
reasoning which are the key capabilities in visual language modeling.
  We perform the MatCha pretraining starting from Pix2Struct, a recently
proposed image-to-text visual language model. On standard benchmarks such as
PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as
much as nearly 20%. We also examine how well MatCha pretraining transfers to
domains such as screenshots, textbook diagrams, and document figures and
observe overall improvement, verifying the usefulness of MatCha pretraining on
broader visual language tasks.",https://github.com/google-research/google-research/tree/master/deplot,-1
42acb600-27bb-4570-9305-aef68597bb22,MultiEarth 2022 Deforestation Challenge -- ForestGump,0.0955867,"The estimation of deforestation in the Amazon Forest is challenge task
because of the vast size of the area and the difficulty of direct human access.
However, it is a crucial problem in that deforestation results in serious
environmental problems such as global climate change, reduced biodiversity,
etc. In order to effectively solve the problems, satellite imagery would be a
good alternative to estimate the deforestation of the Amazon. With a
combination of optical images and Synthetic aperture radar (SAR) images,
observation of such a massive area regardless of weather conditions become
possible. In this paper, we present an accurate deforestation estimation method
with conventional UNet and comprehensive data processing. The diverse channels
of Sentinel-1, Sentinel-2 and Landsat 8 are carefully selected and utilized to
train deep neural networks. With the proposed method, deforestation status for
novel queries are successfully estimated with high accuracy.",None,-1
96e2c774-2480-4836-ac2e-71d8e42bda0a,Interactive Concept Bottleneck Models,0.98592,"Concept bottleneck models (CBMs) are interpretable neural networks that first
predict labels for human-interpretable concepts relevant to the prediction
task, and then predict the final label based on the concept label predictions.
We extend CBMs to interactive prediction settings where the model can query a
human collaborator for the label to some concepts. We develop an interaction
policy that, at prediction time, chooses which concepts to request a label for
so as to maximally improve the final prediction. We demonstrate that a simple
policy combining concept prediction uncertainty and influence of the concept on
the final prediction achieves strong performance and outperforms static
approaches as well as active feature acquisition methods proposed in the
literature. We show that the interactive CBM can achieve accuracy gains of
5-10% with only 5 interactions over competitive baselines on the Caltech-UCSD
Birds, CheXpert and OAI datasets.",https://github.com/google-research/google-research/tree/master/interactive_cbms,-1
292fa82f-badb-442f-ae81-e5b179db9f6b,QUIC-FL: Quick Unbiased Compression for Federated Learning,0.256254,"Distributed Mean Estimation (DME), in which $n$ clients communicate vectors
to a parameter server that estimates their average, is a fundamental building
block in communication-efficient federated learning. In this paper, we improve
on previous DME techniques that achieve the optimal $O(1/n)$ Normalized Mean
Squared Error (NMSE) guarantee by asymptotically improving the complexity for
either encoding or decoding (or both). To achieve this, we formalize the
problem in a novel way that allows us to use off-the-shelf mathematical solvers
to design the quantization.",None,-1
905f157f-3672-4f4f-a244-eefadfc45657,Disentangling Visual Embeddings for Attributes and Objects,0.658334,"We study the problem of compositional zero-shot learning for object-attribute
recognition. Prior works use visual features extracted with a backbone network,
pre-trained for object classification and thus do not capture the subtly
distinct features associated with attributes. To overcome this challenge, these
studies employ supervision from the linguistic space, and use pre-trained word
embeddings to better separate and compose attribute-object pairs for
recognition. Analogous to linguistic embedding space, which already has unique
and agnostic embeddings for object and attribute, we shift the focus back to
the visual space and propose a novel architecture that can disentangle
attribute and object features in the visual space. We use visual decomposed
features to hallucinate embeddings that are representative for the seen and
novel compositions to better regularize the learning of our model. Extensive
experiments show that our method outperforms existing work with significant
margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created
based on VAW. The code, models, and dataset splits are publicly available at
https://github.com/nirat1606/OADis.",https://github.com/nirat1606/OADis,-1
87217bef-4099-482b-8f8a-9d0f602be3a7,AugRmixAT: A Data Processing and Training Method for Improving Multiple Robustness and Generalization Performance,0.0527134,"Deep neural networks are powerful, but they also have shortcomings such as
their sensitivity to adversarial examples, noise, blur, occlusion, etc.
Moreover, ensuring the reliability and robustness of deep neural network models
is crucial for their application in safety-critical areas. Much previous work
has been proposed to improve specific robustness. However, we find that the
specific robustness is often improved at the sacrifice of the additional
robustness or generalization ability of the neural network model. In
particular, adversarial training methods significantly hurt the generalization
performance on unperturbed data when improving adversarial robustness. In this
paper, we propose a new data processing and training method, called AugRmixAT,
which can simultaneously improve the generalization ability and multiple
robustness of neural network models. Finally, we validate the effectiveness of
AugRmixAT on the CIFAR-10/100 and Tiny-ImageNet datasets. The experiments
demonstrate that AugRmixAT can improve the model's generalization performance
while enhancing the white-box robustness, black-box robustness, common
corruption robustness, and partial occlusion robustness.",None,-1
81161212-f789-4270-be08-92881fc43112,DICTDIS: Dictionary Constrained Disambiguation for Improved NMT,0.205796,"Domain-specific neural machine translation (NMT) systems (\eg, in educational
applications) are socially significant with the potential to help make
information accessible to a diverse set of users in multilingual societies. It
is desirable that such NMT systems be lexically constrained and draw from
domain-specific dictionaries. Dictionaries could present multiple candidate
translations for a source word/phrase due to the polysemous nature of words.
The onus is then on the NMT model to choose the contextually most appropriate
candidate. Prior work has largely ignored this problem and focused on the
single candidate constraint setting wherein the target word or phrase is
replaced by a single constraint. In this work we present \dictdis, a lexically
constrained NMT system that disambiguates between multiple candidate
translations derived from dictionaries. We achieve this by augmenting training
data with multiple dictionary candidates to actively encourage disambiguation
during training by implicitly aligning multiple candidate constraints. We
demonstrate the utility of \dictdis\ via extensive experiments on English-Hindi
and English-German sentences in a variety of domains including regulatory,
finance, engineering. We also present comparisons on standard benchmark test
datasets. In comparison with existing approaches for lexically constrained and
unconstrained NMT, we demonstrate superior performance with respect to
constraint copy and disambiguation related measures on all domains while also
obtaining improved fluency of up to 2-3 BLEU points on some domains.",https://github.com/facebookresearch/fairseq/,-1
33d20390-66f7-4265-a385-25cb04f31527,RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs,0.855099,"Blind face restoration is to recover a high-quality face image from unknown
degradations. As face image contains abundant contextual information, we
propose a method, RestoreFormer, which explores fully-spatial attentions to
model contextual information and surpasses existing works that use local
operators. RestoreFormer has several benefits compared to prior arts. First,
unlike the conventional multi-head self-attention in previous Vision
Transformers (ViTs), RestoreFormer incorporates a multi-head cross-attention
layer to learn fully-spatial interactions between corrupted queries and
high-quality key-value pairs. Second, the key-value pairs in ResotreFormer are
sampled from a reconstruction-oriented high-quality dictionary, whose elements
are rich in high-quality facial features specifically aimed for face
reconstruction, leading to superior restoration results. Third, RestoreFormer
outperforms advanced state-of-the-art methods on one synthetic dataset and
three real-world datasets, as well as produces images with better visual
quality.",https://github.com/wzhouxiff/RestoreFormer.git,53489
7331276b-7e1c-440b-9e5f-83da33c9b7b3,Calibrated Interpretation: Confidence Estimation in Semantic Parsing,0.513746,"Sequence generation models are increasingly being used to translate natural
language into programs, i.e. to perform executable semantic parsing. The fact
that semantic parsing aims to predict programs that can lead to executed
actions in the real world motivates developing safe systems. This in turn makes
measuring calibration -- a central component to safety -- particularly
important. We investigate the calibration of popular generation models across
four popular semantic parsing datasets, finding that it varies across models
and datasets. We then analyze factors associated with calibration error and
release new confidence-based challenge splits of two parsing datasets. To
facilitate the inclusion of calibration in semantic parsing evaluations, we
release a library for computing calibration metrics.",https://github.com/esteng/calibration_metric,-1
b9453be4-fda3-4293-b9b5-4f8604cf6c4d,ViGAT: Bottom-up event recognition and explanation in video using factorized graph attention network,0.391036,"In this paper a pure-attention bottom-up approach, called ViGAT, that
utilizes an object detector together with a Vision Transformer (ViT) backbone
network to derive object and frame features, and a head network to process
these features for the task of event recognition and explanation in video, is
proposed. The ViGAT head consists of graph attention network (GAT) blocks
factorized along the spatial and temporal dimensions in order to capture
effectively both local and long-term dependencies between objects or frames.
Moreover, using the weighted in-degrees (WiDs) derived from the adjacency
matrices at the various GAT blocks, we show that the proposed architecture can
identify the most salient objects and frames that explain the decision of the
network. A comprehensive evaluation study is performed, demonstrating that the
proposed approach provides state-of-the-art results on three large, publicly
available video datasets (FCVID, Mini-Kinetics, ActivityNet).",https://github.com/bmezaris/ViGAT,-1
b33261a9-e3d0-4fe7-a7fe-2d062fedbe37,FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,0.54836,"Transformers have been shown to be able to perform deductive reasoning on a
logical rulebase containing rules and statements written in natural language.
Recent works show that such models can also produce the reasoning steps (i.e.,
the proof graph) that emulate the model's logical reasoning process. Currently,
these black-box models generate both the proof graph and intermediate
inferences within the same model and thus may be unfaithful. In this work, we
frame the deductive logical reasoning task by defining three modular
components: rule selection, fact selection, and knowledge composition. The rule
and fact selection steps select the candidate rule and facts to be used and
then the knowledge composition combines them to generate new inferences. This
ensures model faithfulness by assured causal relation from the proof step to
the inference reasoning. To test our framework, we propose FaiRR (Faithful and
Robust Reasoner) where the above three components are independently modeled by
transformers. We observe that FaiRR is robust to novel language perturbations,
and is faster at inference than previous works on existing reasoning datasets.
Additionally, in contrast to black-box generative models, the errors made by
FaiRR are more interpretable due to the modular approach.",https://github.com/INK-USC/FaiRR,-1
75e36924-b686-4e38-a68a-5c1ef3256ea9,Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric Approach for Explainability Requirements,0.508468,"With the recent proliferation of artificial intelligence systems, there has
been a surge in the demand for explainability of these systems. Explanations
help to reduce system opacity, support transparency, and increase stakeholder
trust. In this position paper, we discuss synergies between requirements
engineering (RE) and Explainable AI (XAI). We highlight challenges in the field
of XAI, and propose a framework and research directions on how RE practices can
help to mitigate these challenges.",None,-1
307e79dc-a0d0-429d-a706-249754b9cbec,ParGAN: Learning Real Parametrizable Transformations,0.0640061,"Current methods for image-to-image translation produce compelling results,
however, the applied transformation is difficult to control, since existing
mechanisms are often limited and non-intuitive. We propose ParGAN, a
generalization of the cycle-consistent GAN framework to learn image
transformations with simple and intuitive controls. The proposed generator
takes as input both an image and a parametrization of the transformation. We
train this network to preserve the content of the input image while ensuring
that the result is consistent with the given parametrization. Our approach does
not require paired data and can learn transformations across several tasks and
datasets. We show how, with disjoint image domains with no annotated
parametrization, our framework can create smooth interpolations as well as
learn multiple transformations simultaneously.",https://github.com/tensorow/gan/tree/master/tensorow_gan/examples/cyclegan,-1
43cc387b-d578-4b5e-976d-4c5aa917f470,Repairing Bugs in Python Assignments Using Large Language Models,0.996491,"Students often make mistakes on their introductory programming assignments as
part of their learning process. Unfortunately, providing custom repairs for
these mistakes can require a substantial amount of time and effort from class
instructors. Automated program repair (APR) techniques can be used to
synthesize such fixes. Prior work has explored the use of symbolic and neural
techniques for APR in the education domain. Both types of approaches require
either substantial engineering efforts or large amounts of data and training.
We propose to use a large language model trained on code, such as Codex, to
build an APR system -- MMAPR -- for introductory Python programming
assignments. Our system can fix both syntactic and semantic mistakes by
combining multi-modal prompts, iterative querying, test-case-based selection of
few-shots, and program chunking. We evaluate MMAPR on 286 real student programs
and compare to a baseline built by combining a state-of-the-art Python syntax
repair engine, BIFI, and state-of-the-art Python semantic repair engine for
student assignments, Refactory. We find that MMAPR can fix more programs and
produce smaller patches on average.",None,-1
980e5340-8446-45eb-8620-2e68fc7afbcb,AI-based Malware and Ransomware Detection Models,0.677428,"Cybercrime is one of the major digital threats of this century. In
particular, ransomware attacks have significantly increased, resulting in
global damage costs of tens of billion dollars. In this paper, we train and
test different Machine Learning and Deep Learning models for malware detection,
malware classification and ransomware detection. We introduce a novel and
flexible solution that combines two optimized models for malware and ransomware
detection. Our results demonstrate some improvements both in terms of detection
performances and flexibility. In particular, our combined models pave the way
for easier future enhancements using specialized and thus interchangeable
detection modules.",None,-1
6cc3bb65-f8c3-479a-966b-6379f1ad81be,Polish Natural Language Inference and Factivity -- an Expert-based Dataset and Benchmarks,0.0178172,"Despite recent breakthroughs in Machine Learning for Natural Language
Processing, the Natural Language Inference (NLI) problems still constitute a
challenge. To this purpose we contribute a new dataset that focuses exclusively
on the factivity phenomenon; however, our task remains the same as other NLI
tasks, i.e. prediction of entailment, contradiction or neutral (ECN). The
dataset contains entirely natural language utterances in Polish and gathers
2,432 verb-complement pairs and 309 unique verbs. The dataset is based on the
National Corpus of Polish (NKJP) and is a representative sample in regards to
frequency of main verbs and other linguistic features (e.g. occurrence of
internal negation). We found that transformer BERT-based models working on
sentences obtained relatively good results ($\approx89\%$ F1 score). Even
though better results were achieved using linguistic features ($\approx91\%$ F1
score), this model requires more human labour (humans in the loop) because
features were prepared manually by expert linguists. BERT-based models
consuming only the input sentences show that they capture most of the
complexity of NLI/factivity. Complex cases in the phenomenon - e.g. cases with
entitlement (E) and non-factive verbs - remain an open issue for further
research.",https://github.com/grant-TraDA/factivity-classification,-1
f7bf7dbc-3029-40ca-a654-966068c77df3,Local Directional Gradient Pattern: A Local Descriptor for Face Recognition,0.915713,"In this paper a local pattern descriptor in high order derivative space is
proposed for face recognition. The proposed local directional gradient pattern
(LDGP) is a 1D local micropattern computed by encoding the relationships
between the higher order derivatives of the reference pixel in four distinct
directions. The proposed descriptor identifies the relationship between the
high order derivatives of the referenced pixel in four different directions to
compute the micropattern which corresponds to the local feature. Proposed
descriptor considerably reduces the length of the micropattern which
consequently reduces the extraction time and matching time while maintaining
the recognition rate. Results of the extensive experiments conducted on
benchmark databases AT&T, Extended Yale B and CMU-PIE show that the proposed
descriptor significantly reduces the extraction as well as matching time while
the recognition rate is almost similar to the existing state of the art
methods.",None,-1
6cd3aebd-b136-496d-93ef-723c9aa2bbba,Automatic Depression Detection: An Emotional Audio-Textual Corpus and a GRU/BiLSTM-based Model,0.395149,"Depression is a global mental health problem, the worst case of which can
lead to suicide. An automatic depression detection system provides great help
in facilitating depression self-assessment and improving diagnostic accuracy.
In this work, we propose a novel depression detection approach utilizing speech
characteristics and linguistic contents from participants' interviews. In
addition, we establish an Emotional Audio-Textual Depression Corpus
(EATD-Corpus) which contains audios and extracted transcripts of responses from
depressed and non-depressed volunteers. To the best of our knowledge,
EATD-Corpus is the first and only public depression dataset that contains audio
and text data in Chinese. Evaluated on two depression datasets, the proposed
method achieves the state-of-the-art performances. The outperforming results
demonstrate the effectiveness and generalization ability of the proposed
method. The source code and EATD-Corpus are available at
https://github.com/speechandlanguageprocessing/ICASSP2022-Depression.",https://github.com/speechandlanguageprocessing/ICASSP2022-Depression,-1
ff42ead5-7adf-4512-aea5-91a27cae3103,Artificial Intelligence for Imaging Cherenkov Detectors at the EIC,0.0364218,"Imaging Cherenkov detectors form the backbone of particle identification
(PID) at the future Electron Ion Collider (EIC). Currently all the designs for
the first EIC detector proposal use a dual Ring Imaging CHerenkov (dRICH)
detector in the hadron endcap, a Detector for Internally Reflected Cherenkov
(DIRC) light in the barrel, and a modular RICH (mRICH) in the electron endcap.
These detectors involve optical processes with many photons that need to be
tracked through complex surfaces at the simulation level, while for
reconstruction they rely on pattern recognition of ring images. This proceeding
summarizes ongoing efforts and possible applications of AI for imaging
Cherenkov detectors at EIC. In particular we will provide the example of the
dRICH for the AI-assisted design and of the DIRC for simulation and particle
identification from complex patterns and discuss possible advantages of using
AI.",None,-1
700555a7-5a8e-4123-9d6d-a8597477de23,mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling,0.160136,"Zero-shot slot filling has received considerable attention to cope with the
problem of limited available data for the target domain. One of the important
factors in zero-shot learning is to make the model learn generalized and
reliable representations. For this purpose, we present mcBERT, which stands for
momentum contrastive learning with BERT, to develop a robust zero-shot slot
filling model. mcBERT uses BERT to initialize the two encoders, the query
encoder and key encoder, and is trained by applying momentum contrastive
learning. Our experimental results on the SNIPS benchmark show that mcBERT
substantially outperforms the previous models, recording a new
state-of-the-art. Besides, we also show that each component composing mcBERT
contributes to the performance improvement.",None,-1
0efbf51f-a4d6-4127-b365-68b56913211a,Deep Reinforcement Learning for Exact Combinatorial Optimization: Learning to Branch,0.592686,"Branch-and-bound is a systematic enumerative method for combinatorial
optimization, where the performance highly relies on the variable selection
strategy. State-of-the-art handcrafted heuristic strategies suffer from
relatively slow inference time for each selection, while the current machine
learning methods require a significant amount of labeled data. We propose a new
approach for solving the data labeling and inference latency issues in
combinatorial optimization based on the use of the reinforcement learning (RL)
paradigm. We use imitation learning to bootstrap an RL agent and then use
Proximal Policy Optimization (PPO) to further explore global optimal actions.
Then, a value network is used to run Monte-Carlo tree search (MCTS) to enhance
the policy network. We evaluate the performance of our method on four different
categories of combinatorial optimization problems and show that our approach
performs strongly compared to the state-of-the-art machine learning and
heuristics based methods.",None,-1
8d076cd8-6687-4c43-a536-6922a2f62150,A general-purpose method for applying Explainable AI for Anomaly Detection,0.320443,"The need for explainable AI (XAI) is well established but relatively little
has been published outside of the supervised learning paradigm. This paper
focuses on a principled approach to applying explainability and
interpretability to the task of unsupervised anomaly detection. We argue that
explainability is principally an algorithmic task and interpretability is
principally a cognitive task, and draw on insights from the cognitive sciences
to propose a general-purpose method for practical diagnosis using explained
anomalies. We define Attribution Error, and demonstrate, using real-world
labeled datasets, that our method based on Integrated Gradients (IG) yields
significantly lower attribution errors than alternative methods.",None,-1
05789d07-7cf4-4499-b66f-ccfdb3a3915d,Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,0.992143,"Human evaluation is the foundation upon which the evaluation of both
summarization systems and automatic metrics rests. However, existing human
evaluation studies for summarization either exhibit a low inter-annotator
agreement or have insufficient scale, and an in-depth analysis of human
evaluation is lacking. Therefore, we address the shortcomings of existing
summarization evaluation along the following axes: (1) We propose a modified
summarization salience protocol, Atomic Content Units (ACUs), which is based on
fine-grained semantic units and allows for a high inter-annotator agreement.
(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large
human evaluation dataset consisting of 22,000 summary-level annotations over 28
top-performing systems on three datasets. (3) We conduct a comparative study of
four human evaluation protocols, underscoring potential confounding factors in
evaluation setups. (4) We evaluate 50 automatic metrics and their variants
using the collected human annotations across evaluation protocols and
demonstrate how our benchmark leads to more statistically stable and
significant results. The metrics we benchmarked include recent methods based on
large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings
have important implications for evaluating LLMs, as we show that LLMs adjusted
by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,
which is affected by the annotators' prior, input-agnostic preferences, calling
for more robust, targeted evaluation methods.",https://github.com/Yale-LILY/ROSE,41181
1b33a33e-4f1a-44ef-96d6-21c1c5d9bdb1,A Hierarchical Bayesian Approach to Inverse Reinforcement Learning with Symbolic Reward Machines,0.596133,"A misspecified reward can degrade sample efficiency and induce undesired
behaviors in reinforcement learning (RL) problems. We propose symbolic reward
machines for incorporating high-level task knowledge when specifying the reward
signals. Symbolic reward machines augment existing reward machine formalism by
allowing transitions to carry predicates and symbolic reward outputs. This
formalism lends itself well to inverse reinforcement learning, whereby the key
challenge is determining appropriate assignments to the symbolic values from a
few expert demonstrations. We propose a hierarchical Bayesian approach for
inferring the most likely assignments such that the concretized reward machine
can discriminate expert demonstrated trajectories from other trajectories with
high accuracy. Experimental results show that learned reward machines can
significantly improve training efficiency for complex RL tasks and generalize
well across different task environment configurations.",None,2911
4dfc3cf2-3188-468a-bf95-787c2121f803,PriorLane: A Prior Knowledge Enhanced Lane Detection Approach Based on Transformer,0.253747,"Lane detection is one of the fundamental modules in self-driving. In this
paper we employ a transformer-only method for lane detection, thus it could
benefit from the blooming development of fully vision transformer and achieve
the state-of-the-art (SOTA) performance on both CULane and TuSimple benchmarks,
by fine-tuning the weight fully pre-trained on large datasets. More
importantly, this paper proposes a novel and general framework called
PriorLane, which is used to enhance the segmentation performance of the fully
vision transformer by introducing the low-cost local prior knowledge.
Specifically, PriorLane utilizes an encoder-only transformer to fuse the
feature extracted by a pre-trained segmentation model with prior knowledge
embeddings. Note that a Knowledge Embedding Alignment (KEA) module is adapted
to enhance the fusion performance by aligning the knowledge embedding.
Extensive experiments on our Zjlab dataset show that PriorLane outperforms SOTA
lane detection methods by a 2.82% mIoU when prior knowledge is employed, and
the code will be released at: https://github.com/vincentqqb/PriorLane.",https://github.com/vincentqqb/PriorLane,-1
79c8c58a-9674-4321-a59e-f54cf9090419,TAMFormer: Multi-Modal Transformer with Learned Attention Mask for Early Intent Prediction,0.135673,"Human intention prediction is a growing area of research where an activity in
a video has to be anticipated by a vision-based system. To this end, the model
creates a representation of the past, and subsequently, it produces future
hypotheses about upcoming scenarios. In this work, we focus on pedestrians'
early intention prediction in which, from a current observation of an urban
scene, the model predicts the future activity of pedestrians that approach the
street. Our method is based on a multi-modal transformer that encodes past
observations and produces multiple predictions at different anticipation times.
Moreover, we propose to learn the attention masks of our transformer-based
model (Temporal Adaptive Mask Transformer) in order to weigh differently
present and past temporal dependencies. We investigate our method on several
public benchmarks for early intention prediction, improving the prediction
performances at different anticipation times compared to the previous works.",None,-1
0a72d7ea-87a8-4682-90b2-8d4daf9d6c5c,From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer,0.939238,"Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC.",https://github.com/zjunlp/PromptKG/tree/main/research/GenKGC,8507
92751b14-239d-46ab-81b7-a3427ef77414,Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution,0.599197,"Generating agents that can achieve zero-shot coordination (ZSC) with unseen
partners is a new challenge in cooperative multi-agent reinforcement learning
(MARL). Recently, some studies have made progress in ZSC by exposing the agents
to diverse partners during the training process. They usually involve self-play
when training the partners, implicitly assuming that the tasks are homogeneous.
However, many real-world tasks are heterogeneous, and hence previous methods
may be inefficient. In this paper, we study the heterogeneous ZSC problem for
the first time and propose a general method based on coevolution, which
coevolves two populations of agents and partners through three sub-processes:
pairing, updating and selection. Experimental results on various heterogeneous
tasks highlight the necessity of considering the heterogeneous setting and
demonstrate that our proposed method is a promising solution for heterogeneous
ZSC tasks.",https://github.com/HumanCompatibleAI/human-aware-rl,-1
3d5ef2ed-39e9-4d5e-ab19-e0691d96aff3,A new Hyper-heuristic based on Adaptive Simulated Annealing and Reinforcement Learning for the Capacitated Electric Vehicle Routing Problem,0.724777,"Electric vehicles (EVs) have been adopted in urban areas to reduce
environmental pollution and global warming as a result of the increasing number
of freight vehicles. However, there are still deficiencies in routing the
trajectories of last-mile logistics that continue to impact social and economic
sustainability. For that reason, in this paper, a hyper-heuristic (HH) approach
called Hyper-heuristic Adaptive Simulated Annealing with Reinforcement Learning
(HHASA$_{RL}$) is proposed. It is composed of a multi-armed bandit method and
the self-adaptive Simulated Annealing (SA) metaheuristic algorithm for solving
the problem called Capacitated Electric Vehicle Routing Problem (CEVRP). Due to
the limited number of charging stations and the travel range of EVs, the EVs
must require battery recharging moments in advance and reduce travel times and
costs. The HH implemented improves multiple minimum best-known solutions and
obtains the best mean values for some high-dimensional instances for the
proposed benchmark for the IEEE WCCI2020 competition.",None,4806
ea7be224-8075-4885-aa3c-57227ba236fc,Benchmarking Self-Supervised Learning on Diverse Pathology Datasets,0.878163,"Computational pathology can lead to saving human lives, but models are
annotation hungry and pathology images are notoriously expensive to annotate.
Self-supervised learning has shown to be an effective method for utilizing
unlabeled data, and its application to pathology could greatly benefit its
downstream tasks. Yet, there are no principled studies that compare SSL methods
and discuss how to adapt them for pathology. To address this need, we execute
the largest-scale study of SSL pre-training on pathology image data, to date.
Our study is conducted using 4 representative SSL methods on diverse downstream
tasks. We establish that large-scale domain-aligned pre-training in pathology
consistently out-performs ImageNet pre-training in standard SSL settings such
as linear and fine-tuning evaluations, as well as in low-label regimes.
Moreover, we propose a set of domain-specific techniques that we experimentally
show leads to a performance boost. Lastly, for the first time, we apply SSL to
the challenging task of nuclei instance segmentation and show large and
consistent performance improvements under diverse settings.",https://lunit-io.github.io/research/publications/pathology_ssl,-1
1235b28b-9d48-46e9-863f-93c74c1bf802,Masked Visual Pre-training for Motor Control,0.996273,"This paper shows that self-supervised visual pre-training from real-world
images is effective for learning motor control tasks from pixels. We first
train the visual representations by masked modeling of natural images. We then
freeze the visual encoder and train neural network controllers on top with
reinforcement learning. We do not perform any task-specific fine-tuning of the
encoder; the same visual representations are used for all motor control tasks.
To the best of our knowledge, this is the first self-supervised model to
exploit real-world images at scale for motor control. To accelerate progress in
learning from pixels, we contribute a benchmark suite of hand-designed tasks
varying in movements, scenes, and robots. Without relying on labels,
state-estimation, or expert demonstrations, we consistently outperform
supervised encoders by up to 80% absolute success rate, sometimes even matching
the oracle state performance. We also find that in-the-wild images, e.g., from
YouTube or Egocentric videos, lead to better visual representations for various
manipulation tasks than ImageNet images.",None,-1
93e8b5fc-9687-43f8-b212-a7bcb4fcb58f,Perceptual Quality Assessment for Digital Human Heads,0.612491,"Digital humans are attracting more and more research interest during the last
decade, the generation, representation, rendering, and animation of which have
been put into large amounts of effort. However, the quality assessment of
digital humans has fallen behind. Therefore, to tackle the challenge of digital
human quality assessment issues, we propose the first large-scale quality
assessment database for three-dimensional (3D) scanned digital human heads
(DHHs). The constructed database consists of 55 reference DHHs and 1,540
distorted DHHs along with the subjective perceptual ratings. Then, a simple yet
effective full-reference (FR) projection-based method is proposed to evaluate
the visual quality of DHHs. The pretrained Swin Transformer tiny is employed
for hierarchical feature extraction and the multi-head attention module is
utilized for feature fusion. The experimental results reveal that the proposed
method exhibits state-of-the-art performance among the mainstream FR metrics.
The database is released at https://github.com/zzc-1998/DHHQA.",https://github.com/zzc-1998/DHHQA,-1
871ed9bc-f3f3-4cf9-9a23-405637309628,MAPS-KB: A Million-scale Probabilistic Simile Knowledge Base,0.0485269,"The ability to understand and generate similes is an imperative step to
realize human-level AI. However, there is still a considerable gap between
machine intelligence and human cognition in similes, since deep models based on
statistical distribution tend to favour high-frequency similes. Hence, a
large-scale symbolic knowledge base of similes is required, as it contributes
to the modeling of diverse yet unpopular similes while facilitating additional
evaluation and reasoning. To bridge the gap, we propose a novel framework for
large-scale simile knowledge base construction, as well as two probabilistic
metrics which enable an improved understanding of simile phenomena in natural
language. Overall, we construct MAPS-KB, a million-scale probabilistic simile
knowledge base, covering 4.3 million triplets over 0.4 million terms from 70 GB
corpora. We conduct sufficient experiments to justify the effectiveness and
necessity of the methods of our framework. We also apply MAPS-KB on three
downstream tasks to achieve state-of-the-art performance, further demonstrating
the value of MAPS-KB.",https://github.com/Abbey4799/MAPS-KB,-1
53e80dd4-074f-496c-b5e1-1facedd09039,How stable are Transferability Metrics evaluations?,0.709996,"Transferability metrics is a maturing field with increasing interest, which
aims at providing heuristics for selecting the most suitable source models to
transfer to a given target dataset, without fine-tuning them all. However,
existing works rely on custom experimental setups which differ across papers,
leading to inconsistent conclusions about which transferability metrics work
best. In this paper we conduct a large-scale study by systematically
constructing a broad range of 715k experimental setup variations. We discover
that even small variations to an experimental setup lead to different
conclusions about the superiority of a transferability metric over another.
Then we propose better evaluations by aggregating across many experiments,
enabling to reach more stable conclusions. As a result, we reveal the
superiority of LogME at selecting good source datasets to transfer from in a
semantic segmentation scenario, NLEEP at selecting good source architectures in
an image classification scenario, and GBC at determining which target task
benefits most from a given source model. Yet, no single transferability metric
works best in all scenarios.",https://github.com/google-research/google-research/tree/master/stable transfer,-1
10c1eb33-25d5-4fca-8724-14d99da2828b,The Legal Argument Reasoning Task in Civil Procedure,0.573754,"We present a new NLP task and dataset from the domain of the U.S. civil
procedure. Each instance of the dataset consists of a general introduction to
the case, a particular question, and a possible solution argument, accompanied
by a detailed analysis of why the argument applies in that case. Since the
dataset is based on a book aimed at law students, we believe that it represents
a truly complex task for benchmarking modern legal language models. Our
baseline evaluation shows that fine-tuning a legal transformer provides some
advantage over random baseline models, but our analysis reveals that the actual
ability to infer legal arguments remains a challenging open research question.",https://github.com/trusthlt/legal-argument-reasoning-task,-1
96f0aeed-f79c-4d8c-9243-b55b59a0c028,Universal adversarial perturbation for remote sensing images,0.348182,"Recently, with the application of deep learning in the remote sensing image
(RSI) field, the classification accuracy of the RSI has been dramatically
improved compared with traditional technology. However, even the
state-of-the-art object recognition convolutional neural networks are fooled by
the universal adversarial perturbation (UAP). The research on UAP is mostly
limited to ordinary images, and RSIs have not been studied. To explore the
basic characteristics of UAPs of RSIs, this paper proposes a novel method
combining an encoder-decoder network with an attention mechanism to generate
the UAP of RSIs. Firstly, the former is used to generate the UAP, which can
learn the distribution of perturbations better, and then the latter is used to
find the sensitive regions concerned by the RSI classification model. Finally,
the generated regions are used to fine-tune the perturbation making the model
misclassified with fewer perturbations. The experimental results show that the
UAP can make the classification model misclassify, and the attack success rate
of our proposed method on the RSI data set is as high as 97.09%.",None,-1
0a456794-0a3c-4fd4-9ba2-eb2c667ddb69,"For Learning in Symmetric Teams, Local Optima are Global Nash Equilibria",0.418476,"Although it has been known since the 1970s that a globally optimal strategy
profile in a common-payoff game is a Nash equilibrium, global optimality is a
strict requirement that limits the result's applicability. In this work, we
show that any locally optimal symmetric strategy profile is also a (global)
Nash equilibrium. Furthermore, we show that this result is robust to
perturbations to the common payoff and to the local optimum. Applied to machine
learning, our result provides a global guarantee for any gradient method that
finds a local optimum in symmetric strategy space. While this result indicates
stability to unilateral deviation, we nevertheless identify broad classes of
games where mixed local optima are unstable under joint, asymmetric deviations.
We analyze the prevalence of instability by running learning algorithms in a
suite of symmetric games, and we conclude by discussing the applicability of
our results to multi-agent RL, cooperative inverse RL, and decentralized
POMDPs.",https://github.com/scottemmons/coordination,117510
7da9d8f6-f27a-415b-9f72-74554b6f4878,Neural Architecture Search for Dense Prediction Tasks in Computer Vision,0.444779,"The success of deep learning in recent years has lead to a rising demand for
neural network architecture engineering. As a consequence, neural architecture
search (NAS), which aims at automatically designing neural network
architectures in a data-driven manner rather than manually, has evolved as a
popular field of research. With the advent of weight sharing strategies across
architectures, NAS has become applicable to a much wider range of problems. In
particular, there are now many publications for dense prediction tasks in
computer vision that require pixel-level predictions, such as semantic
segmentation or object detection. These tasks come with novel challenges, such
as higher memory footprints due to high-resolution data, learning multi-scale
representations, longer training times, and more complex and larger neural
architectures. In this manuscript, we provide an overview of NAS for dense
prediction tasks by elaborating on these novel challenges and surveying ways to
address them to ease future research and application of existing methods to
novel problems.",None,-1
35043258-95df-465c-aeb6-7168bcefa654,Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue,0.761775,"Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question
regarding a given video and dialogue context. Despite the recent success of
multi-modal reasoning to generate answer sentences, existing dialogue systems
still suffer from a text hallucination problem, which denotes indiscriminate
text-copying from input texts without an understanding of the question. This is
due to learning spurious correlations from the fact that answer sentences in
the dataset usually include the words of input texts, thus the VGD system
excessively relies on copying words from input texts by hoping those words to
overlap with ground-truth texts. Hence, we design Text Hallucination Mitigating
(THAM) framework, which incorporates Text Hallucination Regularization (THR)
loss derived from the proposed information-theoretic text hallucination
measurement approach. Applying THAM with current dialogue systems validates the
effectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows
enhanced interpretability.",https://github.com/dialogtekgeek/DSTC8-AVSD_ofcial,-1
20ef996e-d41c-4fbe-bf96-6a8f480fff2e,Prompting Large Pre-trained Vision-Language Models For Compositional Concept Learning,0.241998,"This work explores the zero-shot compositional learning ability of large
pre-trained vision-language models(VLMs) within the prompt-based learning
framework and propose a model (\textit{PromptCompVL}) to solve the compositonal
zero-shot learning (CZSL) problem. \textit{PromptCompVL} makes two design
choices: first, it uses a soft-prompting instead of hard-prompting to inject
learnable parameters to reprogram VLMs for compositional learning. Second, to
address the compositional challenge, it uses the soft-embedding layer to learn
primitive concepts in different combinations. By combining both soft-embedding
and soft-prompting, \textit{PromptCompVL} achieves state-of-the-art performance
on the MIT-States dataset. Furthermore, our proposed model achieves consistent
improvement compared to other CLIP-based methods which shows the effectiveness
of the proposed prompting strategies for CZSL.",None,1473
093dccfa-083e-4a9b-b155-d43e2ad339b5,Hierarchical Phrase-based Sequence-to-Sequence Learning,0.675982,"We describe a neural transducer that maintains the flexibility of standard
sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases
as a source of inductive bias during training and as explicit constraints
during inference. Our approach trains two models: a discriminative parser based
on a bracketing transduction grammar whose derivation tree hierarchically
aligns source and target phrases, and a neural seq2seq model that learns to
translate the aligned phrases one-by-one. We use the same seq2seq model to
translate at all phrase scales, which results in two inference modes: one mode
in which the parser is discarded and only the seq2seq component is used at the
sequence-level, and another in which the parser is combined with the seq2seq
model. Decoding in the latter mode is done with the cube-pruned CKY algorithm,
which is more involved but can make use of new translation rules during
inference. We formalize our model as a source-conditioned synchronous grammar
and develop an efficient variational inference algorithm for training. When
applied on top of both randomly initialized and pretrained seq2seq models, we
find that both inference modes performs well compared to baselines on small
scale machine translation benchmarks.",https://github.com/berlino/btg-seq2seq,29122
27d1eb26-55dd-433e-9ff1-db3a1fe3b6f8,LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification,0.60404,"Weakly supervised text classification methods typically train a deep neural
classifier based on pseudo-labels. The quality of pseudo-labels is crucial to
final performance but they are inevitably noisy due to their heuristic nature,
so selecting the correct ones has a huge potential for performance boost. One
straightforward solution is to select samples based on the softmax probability
scores in the neural classifier corresponding to their pseudo-labels. However,
we show through our experiments that such solutions are ineffective and
unstable due to the erroneously high-confidence predictions from poorly
calibrated models. Recent studies on the memorization effects of deep neural
models suggest that these models first memorize training samples with clean
labels and then those with noisy labels. Inspired by this observation, we
propose a novel pseudo-label selection method LOPS that takes learning order of
samples into consideration. We hypothesize that the learning order reflects the
probability of wrong annotation in terms of ranking, and therefore, propose to
select the samples that are learnt earlier. LOPS can be viewed as a strong
performance-boost plug-in to most of existing weakly-supervised text
classification methods, as confirmed in extensive experiments on four
real-world datasets.",https://github.com/dheeraj7596/LOPS,-1
288a7d8c-3122-4656-ba56-8f138524e830,Peano: Learning Formal Mathematical Reasoning,0.66256,"General mathematical reasoning is computationally undecidable, but humans
routinely solve new problems. Moreover, discoveries developed over centuries
are taught to subsequent generations quickly. What structure enables this, and
how might that inform automated mathematical reasoning? We posit that central
to both puzzles is the structure of procedural abstractions underlying
mathematics. We explore this idea in a case study on 5 sections of beginning
algebra on the Khan Academy platform. To define a computational foundation, we
introduce Peano, a theorem-proving environment where the set of valid actions
at any point is finite. We use Peano to formalize introductory algebra problems
and axioms, obtaining well-defined search problems. We observe existing
reinforcement learning methods for symbolic reasoning to be insufficient to
solve harder problems. Adding the ability to induce reusable abstractions
(""tactics"") from its own solutions allows an agent to make steady progress,
solving all problems. Furthermore, these abstractions induce an order to the
problems, seen at random during training. The recovered order has significant
agreement with the expert-designed Khan Academy curriculum, and
second-generation agents trained on the recovered curriculum learn
significantly faster. These results illustrate the synergistic role of
abstractions and curricula in the cultural transmission of mathematics.",https://github.com/gpoesia/peano,-1
8dcc1cd3-a5bc-4134-9263-c0a660b9f109,Action Conditioned Tactile Prediction: a case study on slip prediction,0.225863,"Tactile predictive models can be useful across several robotic manipulation
tasks, e.g. robotic pushing, robotic grasping, slip avoidance, and in-hand
manipulation. However, available tactile prediction models are mostly studied
for image-based tactile sensors and there is no comparison study indicating the
best performing models. In this paper, we presented two novel data-driven
action-conditioned models for predicting tactile signals during real-world
physical robot interaction tasks (1) action condition tactile prediction and
(2) action conditioned tactile-video prediction models. We use a magnetic-based
tactile sensor that is challenging to analyse and test state-of-the-art
predictive models and the only existing bespoke tactile prediction model. We
compare the performance of these models with those of our proposed models. We
perform the comparison study using our novel tactile enabled dataset containing
51,000 tactile frames of a real-world robotic manipulation task with 11
flat-surfaced household objects. Our experimental results demonstrate the
superiority of our proposed tactile prediction models in terms of qualitative,
quantitative and slip prediction scores.",https://github.com/imanlab/action-conditioned-tactile-prediction,-1
f72dc8d8-50f6-4149-a163-0fb5d579839d,A Universal Discriminator for Zero-Shot Generalization,0.377566,"Generative modeling has been the dominant approach for large-scale
pretraining and zero-shot generalization. In this work, we challenge this
convention by showing that discriminative approaches perform substantially
better than generative ones on a large number of NLP tasks. Technically, we
train a single discriminator to predict whether a text sample comes from the
true data distribution, similar to GANs. Since many NLP tasks can be formulated
as selecting from a few options, we use this discriminator to predict the
concatenation of input and which option has the highest probability of coming
from the true data distribution. This simple formulation achieves
state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by
16.0\%, 7.8\%, and 11.5\% respectively on different scales. In the finetuning
setting, our approach also achieves new state-of-the-art results on a wide
range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile,
our approach requires minimal prompting efforts, which largely improves
robustness and is essential for real-world applications. Furthermore, we also
jointly train a generalized UD in combination with generative tasks, which
maintains its advantage on discriminative tasks and simultaneously works on
generative tasks.",https://github.com/Rafa-zy/UD,-1
2218f765-2f60-4919-bb4e-d255acac5501,Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning,0.941447,"Spatiotemporal predictive learning aims to generate future frames by learning
from historical frames. In this paper, we investigate existing methods and
present a general framework of spatiotemporal predictive learning, in which the
spatial encoder and decoder capture intra-frame features and the middle
temporal module catches inter-frame correlations. While the mainstream methods
employ recurrent units to capture long-term temporal dependencies, they suffer
from low computational efficiency due to their unparallelizable architectures.
To parallelize the temporal module, we propose the Temporal Attention Unit
(TAU), which decomposes the temporal attention into intra-frame statical
attention and inter-frame dynamical attention. Moreover, while the mean squared
error loss focuses on intra-frame errors, we introduce a novel differential
divergence regularization to take inter-frame variations into account.
Extensive experiments demonstrate that the proposed method enables the derived
model to achieve competitive performance on various spatiotemporal prediction
benchmarks.",None,-1
19a07c56-9c69-4f14-93ab-d59d45a1f6db,Reachability Verification Based Reliability Assessment for Deep Reinforcement Learning Controlled Robotics and Autonomous Systems,0.493345,"Deep Reinforcement Learning (DRL) has achieved impressive performance in
robotics and autonomous systems (RAS). A key challenge to its deployment in
real-life operations is the presence of spuriously unsafe DRL policies.
Unexplored states may lead the agent to make wrong decisions that could result
in hazards, especially in applications where DRL-trained end-to-end controllers
govern the behaviour of RAS. This paper proposes a novel quantitative
reliability assessment framework for DRL-controlled RAS, leveraging
verification evidence generated from formal reliability analysis of neural
networks. A two-level verification framework is introduced to check the safety
property with respect to inaccurate observations that are due to, e.g.,
environmental noise and state changes. Reachability verification tools are
leveraged locally to generate safety evidence of trajectories. In contrast, at
the global level, we quantify the overall reliability as an aggregated metric
of local safety evidence, corresponding to a set of distinct tasks and their
occurrence probabilities. The effectiveness of the proposed verification
framework is demonstrated and validated via experiments on real RAS.",https://github.com/Solitude-SAMR,-1
2275320a-9ad3-473d-9ea7-7c9377fabc08,Monte Carlo Tree Search Algorithms for Risk-Aware and Multi-Objective Reinforcement Learning,0.533053,"In many risk-aware and multi-objective reinforcement learning settings, the
utility of the user is derived from a single execution of a policy. In these
settings, making decisions based on the average future returns is not suitable.
For example, in a medical setting a patient may only have one opportunity to
treat their illness. Making decisions using just the expected future returns --
known in reinforcement learning as the value -- cannot account for the
potential range of adverse or positive outcomes a decision may have. Therefore,
we should use the distribution over expected future returns differently to
represent the critical information that the agent requires at decision time by
taking both the future and accrued returns into consideration. In this paper,
we propose two novel Monte Carlo tree search algorithms. Firstly, we present a
Monte Carlo tree search algorithm that can compute policies for nonlinear
utility functions (NLU-MCTS) by optimising the utility of the different
possible returns attainable from individual policy executions, resulting in
good policies for both risk-aware and multi-objective settings. Secondly, we
propose a distributional Monte Carlo tree search algorithm (DMCTS) which
extends NLU-MCTS. DMCTS computes an approximate posterior distribution over the
utility of the returns, and utilises Thompson sampling during planning to
compute policies in risk-aware and multi-objective settings. Both algorithms
outperform the state-of-the-art in multi-objective reinforcement learning for
the expected utility of the returns.",None,-1
0bbdad69-1313-4a17-8625-ca89819cd084,Phrase translation using a bilingual dictionary and n-gram data: A case study from Vietnamese to English,0.010179,"Past approaches to translate a phrase in a language L1 to a language L2 using
a dictionary-based approach require grammar rules to restructure initial
translations. This paper introduces a novel method without using any grammar
rules to translate a given phrase in L1, which does not exist in the
dictionary, to L2. We require at least one L1-L2 bilingual dictionary and
n-gram data in L2. The average manual evaluation score of our translations is
4.29/5.00, which implies very high quality.",None,-1
13d870b0-fd2f-4a5a-ab04-ddcad234724d,SNaC: Coherence Error Detection for Narrative Summarization,0.878623,"Progress in summarizing long texts is inhibited by the lack of appropriate
evaluation frameworks. When a long summary must be produced to appropriately
cover the facets of that text, that summary needs to present a coherent
narrative to be understandable by a reader, but current automatic and human
evaluation methods fail to identify gaps in coherence. In this work, we
introduce SNaC, a narrative coherence evaluation framework rooted in
fine-grained annotations for long summaries. We develop a taxonomy of coherence
errors in generated narrative summaries and collect span-level annotations for
6.6k sentences across 150 book and movie screenplay summaries. Our work
provides the first characterization of coherence errors generated by
state-of-the-art summarization models and a protocol for eliciting coherence
judgments from crowd annotators. Furthermore, we show that the collected
annotations allow us to train a strong classifier for automatically localizing
coherence errors in generated summaries as well as benchmarking past work in
coherence modeling. Finally, our SNaC framework can support future work in long
document summarization and coherence evaluation, including improved
summarization modeling and post-hoc summary correction.",https://github.com/tagoyal/snac,-1
b332e456-508b-4789-b0d1-613b729bd98c,Towards Automatic Construction of Filipino WordNet: Word Sense Induction and Synset Induction Using Sentence Embeddings,0.203829,"Wordnets are indispensable tools for various natural language processing
applications. Unfortunately, wordnets get outdated, and producing or updating
wordnets can be slow and costly in terms of time and resources. This problem
intensifies for low-resource languages. This study proposes a method for word
sense induction and synset induction using only two linguistic resources,
namely, an unlabeled corpus and a sentence embeddings-based language model. The
resulting sense inventory and synonym sets can be used in automatically
creating a wordnet. We applied this method on a corpus of Filipino text. The
sense inventory and synsets were evaluated by matching them with the sense
inventory of the machine translated Princeton WordNet, as well as comparing the
synsets to the Filipino WordNet. This study empirically shows that the 30% of
the induced word senses are valid and 40% of the induced synsets are valid in
which 20% are novel synsets.",None,-1
e176bed9-bb12-4ec1-8558-26d5ca6a76b4,Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages,0.912235,"Scaling multilingual representation learning beyond the hundred most frequent
languages is challenging, in particular to cover the long tail of low-resource
languages. A promising approach has been to train one-for-all multilingual
models capable of cross-lingual transfer, but these models often suffer from
insufficient capacity and interference between unrelated languages. Instead, we
move away from this approach and focus on training multiple language (family)
specific representations, but most prominently enable all languages to still be
encoded in the same representational space. To achieve this, we focus on
teacher-student training, allowing all encoders to be mutually compatible for
bitext mining, and enabling fast learning of new languages. We introduce a new
teacher-student training scheme which combines supervised and self-supervised
training, allowing encoders to take advantage of monolingual training data,
which is valuable in the low-resource setting.
  Our approach significantly outperforms the original LASER encoder. We study
very low-resource languages and handle 50 African languages, many of which are
not covered by any other model. For these languages, we train sentence
encoders, mine bitexts, and validate the bitexts by training NMT systems.",https://github.com/facebookresearch/LASER,-1
10535b95-ec1c-465e-8ff4-0a61ecc88f34,AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,0.69306,"This paper proposes AdaTest, a novel adaptive test pattern generation
framework for efficient and reliable Hardware Trojan (HT) detection. HT is a
backdoor attack that tampers with the design of victim integrated circuits
(ICs). AdaTest improves the existing HT detection techniques in terms of
scalability and accuracy of detecting smaller Trojans in the presence of noise
and variations. To achieve high trigger coverage, AdaTest leverages
Reinforcement Learning (RL) to produce a diverse set of test inputs.
Particularly, we progressively generate test vectors with high reward values in
an iterative manner. In each iteration, the test set is evaluated and
adaptively expanded as needed. Furthermore, AdaTest integrates adaptive
sampling to prioritize test samples that provide more information for HT
detection, thus reducing the number of samples while improving the sample
quality for faster exploration. We develop AdaTest with a Software/Hardware
co-design principle and provide an optimized on-chip architecture solution.
AdaTest's architecture minimizes the hardware overhead in two ways:(i)
Deploying circuit emulation on programmable hardware to accelerate reward
evaluation of the test input; (ii) Pipelining each computation stage in AdaTest
by automatically constructing auxiliary circuit for test input generation,
reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on
various HT benchmarks and compare it with two prior works that use logic
testing for HT detection. Experimental results show that AdaTest engenders up
to two orders of test generation speedup and two orders of test set size
reduction compared to the prior works while achieving the same level or higher
Trojan detection rate.",None,31252
b88a6cdb-3cb3-4929-acb0-1a77dd5b37ea,Large Language Models Can Self-Improve,1.0,"Large Language Models (LLMs) have achieved excellent performances in various
tasks. However, fine-tuning an LLM requires extensive supervision. Human, on
the other hand, may improve their reasoning abilities by self-thinking without
external inputs. In this work, we demonstrate that an LLM is also capable of
self-improving with only unlabeled datasets. We use a pre-trained LLM to
generate ""high-confidence"" rationale-augmented answers for unlabeled questions
using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM
using those self-generated solutions as target outputs. We show that our
approach improves the general reasoning ability of a 540B-parameter LLM
(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and
63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,
without any ground truth label. We conduct ablation studies and show that
fine-tuning on reasoning is critical for self-improvement.",https://github.com/google-research/google-research/tree/master/ul2,-1
eb2ae307-1849-4dbf-8d7d-3515766d481c,Sentiment analysis on electricity twitter posts,0.13527,"In today's world, everyone is expressive in some way, and the focus of this
project is on people's opinions about rising electricity prices in United
Kingdom and India using data from Twitter, a micro-blogging platform on which
people post messages, known as tweets. Because many people's incomes are not
good and they have to pay so many taxes and bills, maintaining a home has
become a disputed issue these days. Despite the fact that Government offered
subsidy schemes to compensate people electricity bills but it is not welcomed
by people. In this project, the aim is to perform sentiment analysis on
people's expressions and opinions expressed on Twitter. In order to grasp the
electricity prices opinion, it is necessary to carry out sentiment analysis for
the government and consumers in energy market. Furthermore, text present on
these medias are unstructured in nature, so to process them we firstly need to
pre-process the data. There are so many feature extraction techniques such as
Bag of Words, TF-IDF (Term Frequency-Inverse Document Frequency), word
embedding, NLP based features like word count. In this project, we analysed the
impact of feature TF-IDF word level on electricity bills dataset of sentiment
analysis. We found that by using TF-IDF word level performance of sentiment
analysis is 3-4 higher than using N-gram features. Analysis is done using four
classification algorithms including Naive Bayes, Decision Tree, Random Forest,
and Logistic Regression and considering F-Score, Accuracy, Precision, and
Recall performance parameters.",None,-1
22e1b992-d49d-4ec3-b1b9-f546782d7a83,Black-box Few-shot Knowledge Distillation,0.429478,"Knowledge distillation (KD) is an efficient approach to transfer the
knowledge from a large ""teacher"" network to a smaller ""student"" network.
Traditional KD methods require lots of labeled training samples and a white-box
teacher (parameters are accessible) to train a good student. However, these
resources are not always available in real-world applications. The distillation
process often happens at an external party side where we do not have access to
much data, and the teacher does not disclose its parameters due to security and
privacy concerns. To overcome these challenges, we propose a black-box few-shot
KD method to train the student with few unlabeled training samples and a
black-box teacher. Our main idea is to expand the training set by generating a
diverse set of out-of-distribution synthetic images using MixUp and a
conditional variational auto-encoder. These synthetic images along with their
labels obtained from the teacher are used to train the student. We conduct
extensive experiments to show that our method significantly outperforms recent
SOTA few/zero-shot KD methods on image classification tasks. The code and
models are available at: https://github.com/nphdang/FS-BBT",https://github.com/nphdang/FS-BBT,30421
d01e0379-dca5-4e7c-a590-0de83abd0fce,DSI++: Updating Transformer Memory with New Documents,0.641119,"Differentiable Search Indices (DSIs) encode a corpus of documents in model
parameters and use the same model to answer user queries directly. Despite the
strong performance of DSI models, deploying them in situations where the corpus
changes over time is computationally expensive because reindexing the corpus
requires re-training the model. In this work, we introduce DSI++, a continual
learning challenge for DSI to incrementally index new documents while being
able to answer queries related to both previously and newly indexed documents.
Across different model scales and document identifier representations, we show
that continual indexing of new documents leads to considerable forgetting of
previously indexed documents. We also hypothesize and verify that the model
experiences forgetting events during training, leading to unstable learning. To
mitigate these issues, we investigate two approaches. The first focuses on
modifying the training dynamics. Flatter minima implicitly alleviate
forgetting, so we optimize for flatter loss basins and show that the model
stably memorizes more documents ($+12\%$). Next, we introduce a generative
memory to sample pseudo-queries for documents and supplement them during
continual indexing to prevent forgetting for the retrieval task. Extensive
experiments on novel continual indexing benchmarks based on Natural Questions
(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting
significantly. Concretely, it improves the average Hits@10 by $+21.1\%$ over
competitive baselines for NQ and requires $6$ times fewer model updates
compared to re-training the DSI model for incrementally indexing five corpora
in a sequence.",None,-1
39bf4838-e9eb-497a-987c-a7226201ca18,Compositional Law Parsing with Latent Random Functions,0.603469,"Human cognition has compositionality. We understand a scene by decomposing
the scene into different concepts (e.g., shape and position of an object) and
learning the respective laws of these concepts, which may be either natural
(e.g., laws of motion) or man-made (e.g., laws of a game). The automatic
parsing of these laws indicates the model's ability to understand the scene,
which makes law parsing play a central role in many visual tasks. This paper
proposes a deep latent variable model for Compositional LAw Parsing (CLAP),
which achieves the human-like compositionality ability through an
encoding-decoding architecture to represent concepts in the scene as latent
variables. CLAP employs concept-specific latent random functions instantiated
with Neural Processes to capture the law of concepts. Our experimental results
demonstrate that CLAP outperforms the baseline methods in multiple visual tasks
such as intuitive physics, abstract visual reasoning, and scene representation.
The law manipulation experiments illustrate CLAP's interpretability by
modifying specific latent random functions on samples. For example, CLAP learns
the laws of position-changing and appearance constancy from the moving balls in
a scene, making it possible to exchange laws between samples or compose
existing laws into novel laws.",https://github.com/FudanVI/generative-abstract-reasoning/tree/main/clap,-1
0fc6095f-9e62-45bc-89b9-e4faf4d139a7,Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images,0.95893,"In this paper, we present a generalizable model-free 6-DoF object pose
estimator called Gen6D. Existing generalizable pose estimators either need
high-quality object models or require additional depth maps or object masks in
test time, which significantly limits their application scope. In contrast, our
pose estimator only requires some posed images of the unseen object and is able
to accurately predict the poses of the object in arbitrary environments. Gen6D
consists of an object detector, a viewpoint selector and a pose refiner, all of
which do not require the 3D object model and can generalize to unseen objects.
Experiments show that Gen6D achieves state-of-the-art results on two model-free
datasets: the MOPED dataset and a new GenMOP dataset collected by us. In
addition, on the LINEMOD dataset, Gen6D achieves competitive results compared
with instance-specific pose estimators. Project page:
https://liuyuan-pal.github.io/Gen6D/.",https://liuyuan-pal.github.io/Gen6D/,-1
a8e210fc-9a57-43d1-ab98-bf2f74e77564,Towards Inter-character Relationship-driven Story Generation,0.731222,"In this paper, we introduce the task of modeling interpersonal relationships
for story generation. For addressing this task, we propose Relationships as
Latent Variables for Story Generation, (ReLiSt). ReLiSt generates stories
sentence by sentence and has two major components - a relationship selector and
a story continuer. The relationship selector specifies a latent variable to
pick the relationship to exhibit in the next sentence and the story continuer
generates the next sentence while expressing the selected relationship in a
coherent way. Our automatic and human evaluations demonstrate that ReLiSt is
able to generate stories with relationships that are more faithful to desired
relationships while maintaining the content quality. The relationship
assignments to sentences during inference bring interpretability to ReLiSt.",https://github.com/dbamman/book-nlp,-1
90f18f7b-ddd6-49c6-ab77-1b196cc0e213,CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI Models,0.174673,"As the practicality of Artificial Intelligence (AI) and Machine Learning (ML)
based techniques grow, there is an ever increasing threat of adversarial
attacks. There is a need to red team this ecosystem to identify system
vulnerabilities, potential threats, characterize properties that will enhance
system robustness, and encourage the creation of effective defenses. A
secondary need is to share this AI security threat intelligence between
different stakeholders like, model developers, users, and AI/ML security
professionals. In this paper, we create and describe a prototype system CTI4AI,
to overcome the need to methodically identify and share AI/ML specific
vulnerabilities and threat intelligence.",None,-1
65a59ec1-dd40-4ce9-8395-a22a1044818f,CorrLoss: Integrating Co-Occurrence Domain Knowledge for Affect Recognition,0.0675455,"Neural networks are widely adopted, yet the integration of domain knowledge
is still underutilized. We propose to integrate domain knowledge about
co-occurring facial movements as a constraint in the loss function to enhance
the training of neural networks for affect recognition. As the co-ccurrence
patterns tend to be similar across datasets, applying our method can lead to a
higher generalizability of models and a lower risk of overfitting. We
demonstrate this by showing performance increases in cross-dataset testing for
various datasets. We also show the applicability of our method for calibrating
neural networks to different facial expressions.",None,-1
4509c81e-fafa-48ea-8a3c-72f0523411e3,Quantum policy gradient algorithms,0.614996,"Understanding the power and limitations of quantum access to data in machine
learning tasks is primordial to assess the potential of quantum computing in
artificial intelligence. Previous works have already shown that speed-ups in
learning are possible when given quantum access to reinforcement learning
environments. Yet, the applicability of quantum algorithms in this setting
remains very limited, notably in environments with large state and action
spaces. In this work, we design quantum algorithms to train state-of-the-art
reinforcement learning policies by exploiting quantum interactions with an
environment. However, these algorithms only offer full quadratic speed-ups in
sample complexity over their classical analogs when the trained policies
satisfy some regularity conditions. Interestingly, we find that reinforcement
learning policies derived from parametrized quantum circuits are well-behaved
with respect to these conditions, which showcases the benefit of a
fully-quantum reinforcement learning framework.",None,-1
47727e5b-441c-4fe2-8d45-2307266b1e00,Multilevel Hierarchical Network with Multiscale Sampling for Video Question Answering,0.655592,"Video question answering (VideoQA) is challenging given its multimodal
combination of visual understanding and natural language processing. While most
existing approaches ignore the visual appearance-motion information at
different temporal scales, it is unknown how to incorporate the multilevel
processing capacity of a deep learning model with such multiscale information.
Targeting these issues, this paper proposes a novel Multilevel Hierarchical
Network (MHN) with multiscale sampling for VideoQA. MHN comprises two modules,
namely Recurrent Multimodal Interaction (RMI) and Parallel Visual Reasoning
(PVR). With a multiscale sampling, RMI iterates the interaction of
appearance-motion information at each scale and the question embeddings to
build the multilevel question-guided visual representations. Thereon, with a
shared transformer encoder, PVR infers the visual cues at each level in
parallel to fit with answering different question types that may rely on the
visual information at relevant levels. Through extensive experiments on three
VideoQA datasets, we demonstrate improved performances than previous
state-of-the-arts and justify the effectiveness of each part of our method.",None,-1
3303481e-edeb-46d4-a177-8a91e2416f43,Deep Virtual-to-Real Distillation for Pedestrian Crossing Prediction,0.483896,"Pedestrian crossing is one of the most typical behavior which conflicts with
natural driving behavior of vehicles. Consequently, pedestrian crossing
prediction is one of the primary task that influences the vehicle planning for
safe driving. However, current methods that rely on the practically collected
data in real driving scenes cannot depict and cover all kinds of scene
condition in real traffic world. To this end, we formulate a deep virtual to
real distillation framework by introducing the synthetic data that can be
generated conveniently, and borrow the abundant information of pedestrian
movement in synthetic videos for the pedestrian crossing prediction in real
data with a simple and lightweight implementation. In order to verify this
framework, we construct a benchmark with 4667 virtual videos owning about 745k
frames (called Virtual-PedCross-4667), and evaluate the proposed method on two
challenging datasets collected in real driving situations, i.e., JAAD and PIE
datasets. State-of-the-art performance of this framework is demonstrated by
exhaustive experiment analysis. The dataset and code can be downloaded from the
website \url{http://www.lotvs.net/code_data/}.",None,6089
3772b3d3-de32-496a-a930-5159d496217c,CNSNet: A Cleanness-Navigated-Shadow Network for Shadow Removal,0.520938,"The key to shadow removal is recovering the contents of the shadow regions
with the guidance of the non-shadow regions. Due to the inadequate long-range
modeling, the CNN-based approaches cannot thoroughly investigate the
information from the non-shadow regions. To solve this problem, we propose a
novel cleanness-navigated-shadow network (CNSNet), with a shadow-oriented
adaptive normalization (SOAN) module and a shadow-aware aggregation with
transformer (SAAT) module based on the shadow mask. Under the guidance of the
shadow mask, the SOAN module formulates the statistics from the non-shadow
region and adaptively applies them to the shadow region for region-wise
restoration. The SAAT module utilizes the shadow mask to precisely guide the
restoration of each shadowed pixel by considering the highly relevant pixels
from the shadow-free regions for global pixel-wise restoration. Extensive
experiments on three benchmark datasets (ISTD, ISTD+, and SRD) show that our
method achieves superior de-shadowing performance.",None,6653
997663a8-fc57-4c4e-93a2-0cd493402be5,Impact of Adversarial Training on Robustness and Generalizability of Language Models,0.343122,"Adversarial training is widely acknowledged as the most effective defense
against adversarial attacks. However, it is also well established that
achieving both robustness and generalization in adversarially trained models
involves a trade-off. The goal of this work is to provide an in depth
comparison of different approaches for adversarial training in language models.
Specifically, we study the effect of pre-training data augmentation as well as
training time input perturbations vs. embedding space perturbations on the
robustness and generalization of transformer-based language models. Our
findings suggest that better robustness can be achieved by pre-training data
augmentation or by training with input space perturbation. However, training
with embedding space perturbation significantly improves generalization. A
linguistic correlation analysis of neurons of the learned models reveals that
the improved generalization is due to 'more specialized' neurons. To the best
of our knowledge, this is the first work to carry out a deep qualitative
analysis of different methods of generating adversarial examples in adversarial
training of language models.",None,-1
74f151d1-3227-45e8-bdb3-3f82296f7ef7,Quality Diversity Evolutionary Learning of Decision Trees,0.210901,"Addressing the need for explainable Machine Learning has emerged as one of
the most important research directions in modern Artificial Intelligence (AI).
While the current dominant paradigm in the field is based on black-box models,
typically in the form of (deep) neural networks, these models lack direct
interpretability for human users, i.e., their outcomes (and, even more so,
their inner working) are opaque and hard to understand. This is hindering the
adoption of AI in safety-critical applications, where high interests are at
stake. In these applications, explainable by design models, such as decision
trees, may be more suitable, as they provide interpretability. Recent works
have proposed the hybridization of decision trees and Reinforcement Learning,
to combine the advantages of the two approaches. So far, however, these works
have focused on the optimization of those hybrid models. Here, we apply
MAP-Elites for diversifying hybrid models over a feature space that captures
both the model complexity and its behavioral variability. We apply our method
on two well-known control problems from the OpenAI Gym library, on which we
discuss the ""illumination"" patterns projected by MAP-Elites, comparing its
results against existing similar approaches.",None,-1
b1ff806d-f993-4a12-8157-0b81c918107f,Just-DREAM-about-it: Figurative Language Understanding with DREAM-FLUTE,0.097532,"Figurative language (e.g., ""he flew like the wind"") is challenging to
understand, as it is hard to tell what implicit information is being conveyed
from the surface form alone. We hypothesize that to perform this task well, the
reader needs to mentally elaborate the scene being described to identify a
sensible meaning of the language. We present DREAM-FLUTE, a figurative language
understanding system that does this, first forming a ""mental model"" of
situations described in a premise and hypothesis before making an
entailment/contradiction decision and generating an explanation. DREAM-FLUTE
uses an existing scene elaboration model, DREAM, for constructing its ""mental
model."" In the FigLang2022 Shared Task evaluation, DREAM-FLUTE achieved (joint)
first place (Acc@60=63.3%), and can perform even better with ensemble
techniques, demonstrating the effectiveness of this approach. More generally,
this work suggests that adding a reflective component to pretrained language
models can improve their performance beyond standard fine-tuning (3.3%
improvement in Acc@60).",https://github.com/allenai/dream,-1
9865f5b2-6795-43cb-ada0-24a75a7a1822,Amortized Inference for Heterogeneous Reconstruction in Cryo-EM,0.538722,"Cryo-electron microscopy (cryo-EM) is an imaging modality that provides
unique insights into the dynamics of proteins and other building blocks of
life. The algorithmic challenge of jointly estimating the poses, 3D structure,
and conformational heterogeneity of a biomolecule from millions of noisy and
randomly oriented 2D projections in a computationally efficient manner,
however, remains unsolved. Our method, cryoFIRE, performs ab initio
heterogeneous reconstruction with unknown poses in an amortized framework,
thereby avoiding the computationally expensive step of pose search while
enabling the analysis of conformational heterogeneity. Poses and conformation
are jointly estimated by an encoder while a physics-based decoder aggregates
the images into an implicit neural representation of the conformational space.
We show that our method can provide one order of magnitude speedup on datasets
containing millions of images without any loss of accuracy. We validate that
the joint estimation of poses and conformations can be amortized over the size
of the dataset. For the first time, we prove that an amortized method can
extract interpretable dynamic information from experimental datasets.",None,-1
d22a3bc2-d10e-435d-be8e-d49f4cadcc79,MHMS: Multimodal Hierarchical Multimedia Summarization,0.67204,"Multimedia summarization with multimodal output can play an essential role in
real-world applications, i.e., automatically generating cover images and titles
for news articles or providing introductions to online videos. In this work, we
propose a multimodal hierarchical multimedia summarization (MHMS) framework by
interacting visual and language domains to generate both video and textual
summaries. Our MHMS method contains video and textual segmentation and
summarization module, respectively. It formulates a cross-domain alignment
objective with optimal transport distance which leverages cross-domain
interaction to generate the representative keyframe and textual summary. We
evaluated MHMS on three recent multimodal datasets and demonstrated the
effectiveness of our method in producing high-quality multimodal summaries.",None,-1
4bba9a04-1ad1-46d2-98e4-2cbd64c20ad0,Interactive Language: Talking to Robots in Real Time,0.994635,"We present a framework for building interactive, real-time, natural
language-instructable robots in the real world, and we open source related
assets (dataset, environment, benchmark, and policies). Trained with behavioral
cloning on a dataset of hundreds of thousands of language-annotated
trajectories, a produced policy can proficiently execute an order of magnitude
more commands than previous works: specifically we estimate a 93.5% success
rate on a set of 87,000 unique natural language strings specifying raw
end-to-end visuo-linguo-motor skills in the real world. We find that the same
policy is capable of being guided by a human via real-time language to address
a wide range of precise long-horizon rearrangement goals, e.g. ""make a smiley
face out of blocks"". The dataset we release comprises nearly 600,000
language-labeled trajectories, an order of magnitude larger than prior
available datasets. We hope the demonstrated results and associated assets
enable further advancement of helpful, capable, natural-language-interactable
robots. See videos at https://interactive-language.github.io.",None,-1
1259f461-e3ec-4ff4-9b43-f0a69172b844,Putting AI Ethics into Practice: The Hourglass Model of Organizational AI Governance,0.543743,"The organizational use of artificial intelligence (AI) has rapidly spread
across various sectors. Alongside the awareness of the benefits brought by AI,
there is a growing consensus on the necessity of tackling the risks and
potential harms, such as bias and discrimination, brought about by advanced AI
technologies. A multitude of AI ethics principles have been proposed to tackle
these risks, but the outlines of organizational processes and practices for
ensuring socially responsible AI development are in a nascent state. To address
the paucity of comprehensive governance models, we present an AI governance
framework, the hourglass model of organizational AI governance, which targets
organizations that develop and use AI systems. The framework is designed to
help organizations deploying AI systems translate ethical AI principles into
practice and align their AI systems and processes with the forthcoming European
AI Act. The hourglass framework includes governance requirements at the
environmental, organizational, and AI system levels. At the AI system level, we
connect governance requirements to AI system life cycles to ensure governance
throughout the system's life span. The governance model highlights the systemic
nature of AI governance and opens new research avenues into its practical
implementation, the mechanisms that connect different AI governance layers, and
the dynamics between the AI governance actors. The model also offers a starting
point for organizational decision-makers to consider the governance components
needed to ensure social acceptability, mitigate risks, and realize the
potential of AI.",None,-1
abd389ba-4ef0-4bb5-84a1-7c5360d1cfa9,Data-Efficient Finetuning Using Cross-Task Nearest Neighbors,0.244016,"Obtaining labeled data to train a model for a task of interest is often
expensive. Prior work shows training models on multitask data augmented with
task descriptions (prompts) effectively transfers knowledge to new tasks.
Towards efficiently building task-specific models, we assume access to a small
number (32-1000) of unlabeled target-task examples and use those to retrieve
the most similar labeled examples from a large pool of multitask data augmented
with prompts. Compared to the current practice of finetuning models on
uniformly sampled prompted multitask data (e.g.: FLAN, T0), our approach of
finetuning on cross-task nearest neighbors is significantly more
data-efficient. Using only 2% of the data from the P3 pool without any labeled
target-task data, our models outperform strong baselines trained on all
available data by 3-30% on 12 out of 14 datasets representing held-out tasks
including legal and scientific document QA. Similarly, models trained on
cross-task nearest neighbors from SuperNaturalInstructions, representing about
5% of the pool, obtain comparable performance to state-of-the-art models on 12
held-out tasks from that pool. Moreover, the models produced by our approach
also provide a better initialization than single multitask finetuned models for
few-shot finetuning on target-task data, as shown by a 2-23% relative
improvement over few-shot finetuned T0-3B models on 8 datasets.",https://github.com/allenai/data-efficient-finetuning,-1
d2064fea-816d-44f4-a2a4-f815a531fb75,Deep Domain Adaptation for Detecting Bomb Craters in Aerial Images,0.390731,"The aftermath of air raids can still be seen for decades after the
devastating events. Unexploded ordnance (UXO) is an immense danger to human
life and the environment. Through the assessment of wartime images, experts can
infer the occurrence of a dud. The current manual analysis process is expensive
and time-consuming, thus automated detection of bomb craters by using deep
learning is a promising way to improve the UXO disposal process. However, these
methods require a large amount of manually labeled training data. This work
leverages domain adaptation with moon surface images to address the problem of
automated bomb crater detection with deep learning under the constraint of
limited training data. This paper contributes to both academia and practice (1)
by providing a solution approach for automated bomb crater detection with
limited training data and (2) by demonstrating the usability and associated
challenges of using synthetic images for domain adaptation.",None,-1
a672b220-b1d0-4057-b96e-b452106418da,Robust Anytime Learning of Markov Decision Processes,0.579167,"Markov decision processes (MDPs) are formal models commonly used in
sequential decision-making. MDPs capture the stochasticity that may arise, for
instance, from imprecise actuators via probabilities in the transition
function. However, in data-driven applications, deriving precise probabilities
from (limited) data introduces statistical errors that may lead to unexpected
or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise
probabilities but instead use so-called uncertainty sets in the transitions,
accounting for such limited data. Tools from the formal verification community
efficiently compute robust policies that provably adhere to formal
specifications, like safety constraints, under the worst-case instance in the
uncertainty set. We continuously learn the transition probabilities of an MDP
in a robust anytime-learning approach that combines a dedicated Bayesian
inference scheme with the computation of robust policies. In particular, our
method (1) approximates probabilities as intervals, (2) adapts to new data that
may be inconsistent with an intermediate model, and (3) may be stopped at any
time to compute a robust policy on the uMDP that faithfully captures the data
so far. Furthermore, our method is capable of adapting to changes in the
environment. We show the effectiveness of our approach and compare it to robust
policies computed on uMDPs learned by the UCRL2 reinforcement learning
algorithm in an experimental evaluation on several benchmarks.",https://github.com/LAVA-LAB/luiaard,-1
a5923c8b-1d16-472d-9e6e-93a8c0a43ff2,Optimal Representations for Covariate Shift,0.83161,"Machine learning systems often experience a distribution shift between
training and testing. In this paper, we introduce a simple variational
objective whose optima are exactly the set of all representations on which risk
minimizers are guaranteed to be robust to any distribution shift that preserves
the Bayes predictor, e.g., covariate shifts. Our objective has two components.
First, a representation must remain discriminative for the task, i.e., some
predictor must be able to simultaneously minimize the source and target risk.
Second, the representation's marginal support needs to be the same across
source and target. We make this practical by designing self-supervised
objectives that only use unlabelled data and augmentations to train robust
representations. Our objectives give insights into the robustness of CLIP, and
further improve CLIP's representations to achieve SOTA results on DomainBed.",https://github.com/ryoungj/optdom,-1
a405aa3d-d420-407e-9cb9-82b1419c413a,Action Languages Based Actual Causality for Computational Ethics: a Sound and Complete Implementation in ASP,0.0488551,"Although moral responsibility is not circumscribed by causality, they are
both closely intermixed. Furthermore, rationally understanding the evolution of
the physical world is inherently linked with the idea of causality. Thus, the
decision-making applications based on automated planning inevitably have to
deal with causality, especially if they consider imputability aspects or
integrate references to ethical norms. The many debates around causation in the
last decades have shown how complex this notion is and thus, how difficult is
its integration with planning. As a result, much of the work in computational
ethics relegates causality to the background, despite the considerations stated
above. This paper's contribution is to provide a complete and sound translation
into logic programming from an actual causation definition suitable for action
languages, this definition is a formalisation of Wright's NESS test. The
obtained logic program allows to deal with complex causal relations. In
addition to enabling agents to reason about causality, this contribution
specifically enables the computational ethics domain to handle situations that
were previously out of reach. In a context where ethical considerations in
decision-making are increasingly important, advances in computational ethics
can greatly benefit the entire AI community.",None,-1
b180d468-796a-4173-90fd-7bbc87476624,Unbiased Knowledge Distillation for Recommendation,0.478845,"As a promising solution for model compression, knowledge distillation (KD)
has been applied in recommender systems (RS) to reduce inference latency.
Traditional solutions first train a full teacher model from the training data,
and then transfer its knowledge (\ie \textit{soft labels}) to supervise the
learning of a compact student model. However, we find such a standard
distillation paradigm would incur serious bias issue -- popular items are more
heavily recommended after the distillation. This effect prevents the student
model from making accurate and fair recommendations, decreasing the
effectiveness of RS.
  In this work, we identify the origin of the bias in KD -- it roots in the
biased soft labels from the teacher, and is further propagated and intensified
during the distillation. To rectify this, we propose a new KD method with a
stratified distillation strategy. It first partitions items into multiple
groups according to their popularity, and then extracts the ranking knowledge
within each group to supervise the learning of the student. Our method is
simple and teacher-agnostic -- it works on distillation stage without affecting
the training of the teacher model. We conduct extensive theoretical and
empirical studies to validate the effectiveness of our proposal. We release our
code at: https://github.com/chengang95/UnKD.",None,-1
da73e46a-233d-40a5-97c9-7c7b545b5234,Neural Face Video Compression using Multiple Views,0.244429,"Recent advances in deep generative models led to the development of neural
face video compression codecs that use an order of magnitude less bandwidth
than engineered codecs. These neural codecs reconstruct the current frame by
warping a source frame and using a generative model to compensate for
imperfections in the warped source frame. Thereby, the warp is encoded and
transmitted using a small number of keypoints rather than a dense flow field,
which leads to massive savings compared to traditional codecs. However, by
relying on a single source frame only, these methods lead to inaccurate
reconstructions (e.g. one side of the head becomes unoccluded when turning the
head and has to be synthesized). Here, we aim to tackle this issue by relying
on multiple source frames (views of the face) and present encouraging results.",None,-1
3e712872-2591-492e-b74d-850de9deb383,Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning,0.485203,"Controlled table-to-text generation seeks to generate natural language
descriptions for highlighted subparts of a table. Previous SOTA systems still
employ a sequence-to-sequence generation method, which merely captures the
table as a linear structure and is brittle when table layouts change. We seek
to go beyond this paradigm by (1) effectively expressing the relations of
content pieces in the table, and (2) making our model robust to
content-invariant structural transformations. Accordingly, we propose an
equivariance learning framework, which encodes tables with a structure-aware
self-attention mechanism. This prunes the full self-attention structure into an
order-invariant graph attention that captures the connected graph structure of
cells belonging to the same row or column, and it differentiates between
relevant cells and irrelevant cells from the structural perspective. Our
framework also modifies the positional encoding mechanism to preserve the
relative position of tokens in the same cell but enforce position invariance
among different cells. Our technology is free to be plugged into existing
table-to-text generation models, and has improved T5-based models to offer
better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,
we preserve promising performance, while previous SOTA systems, even with
transformation-based data augmentation, have seen significant performance
drops. Our code is available at https://github.com/luka-group/Lattice.",https://github.com/luka-group/Lattice,-1
b7ae0da6-c81d-408b-aaad-ffc44c57d5d4,Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters,0.898056,"The growing public concerns on data privacy in face recognition can be
greatly addressed by the federated learning (FL) paradigm. However,
conventional FL methods perform poorly due to the uniqueness of the task:
broadcasting class centers among clients is crucial for recognition
performances but leads to privacy leakage. To resolve the privacy-utility
paradox, this work proposes PrivacyFace, a framework largely improves the
federated learning face recognition via communicating auxiliary and
privacy-agnostic information among clients. PrivacyFace mainly consists of two
components: First, a practical Differentially Private Local Clustering (DPLC)
mechanism is proposed to distill sanitized clusters from local class centers.
Second, a consensus-aware recognition loss subsequently encourages global
consensuses among clients, which ergo results in more discriminative features.
The proposed framework is mathematically proved to be differentially private,
introducing a lightweight overhead as well as yielding prominent performance
boosts (\textit{e.g.}, +9.63\% and +10.26\% for TAR@FAR=1e-4 on IJB-B and IJB-C
respectively). Extensive experiments and ablation studies on a large-scale
dataset have demonstrated the efficacy and practicability of our method.",https://github.com/IrvingMeng/MagFace,-1
4af18046-723f-49cc-8ca7-b6dba2929302,Learning to Execute Actions or Ask Clarification Questions,0.82288,"Collaborative tasks are ubiquitous activities where a form of communication
is required in order to reach a joint goal. Collaborative building is one of
such tasks. We wish to develop an intelligent builder agent in a simulated
building environment (Minecraft) that can build whatever users wish to build by
just talking to the agent. In order to achieve this goal, such agents need to
be able to take the initiative by asking clarification questions when further
information is needed. Existing works on Minecraft Corpus Dataset only learn to
execute instructions neglecting the importance of asking for clarifications. In
this paper, we extend the Minecraft Corpus Dataset by annotating all builder
utterances into eight types, including clarification questions, and propose a
new builder agent model capable of determining when to ask or execute
instructions. Experimental results show that our model achieves
state-of-the-art performance on the collaborative building task with a
substantial improvement. We also define two new tasks, the learning to ask task
and the joint learning task. The latter consists of solving both collaborating
building and learning to ask tasks jointly.",https://github.com/ZhengxiangShi/LearnToAsk,-1
4fdcc4c0-40dc-497a-aea1-4e144bec6008,A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning,0.0145814,"In recent years, spiking neural networks (SNNs) have been used in
reinforcement learning (RL) due to their low power consumption and event-driven
features. However, spiking reinforcement learning (SRL), which suffers from
fixed coding methods, still faces the problems of high latency and poor
versatility. In this paper, we use learnable matrix multiplication to encode
and decode spikes, improving the flexibility of the coders and thus reducing
latency. Meanwhile, we train the SNNs using the direct training method and use
two different structures for online and offline RL algorithms, which gives our
model a wider range of applications. Extensive experiments have revealed that
our method achieves optimal performance with ultra-low latency (as low as 0.8%
of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) in
different algorithms and different environments.",None,-1
f2e7c547-e386-4675-9afd-446cda5a10f1,DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models,0.569239,"We introduce DoWhy-GCM, an extension of the DoWhy Python library, that
leverages graphical causal models. Unlike existing causality libraries, which
mainly focus on effect estimation questions, with DoWhy-GCM, users can ask a
wide range of additional causal questions, such as identifying the root causes
of outliers and distributional changes, causal structure learning, attributing
causal influences, and diagnosis of causal structures. To this end, DoWhy-GCM
users first model cause-effect relations between variables in a system under
study through a graphical causal model, fit the causal mechanisms of variables
next, and then ask the causal question. All these steps take only a few lines
of code in DoWhy-GCM.
  The library is available at https://github.com/py-why/dowhy.",https://github.com/py-why/dowhy,-1
52626d56-dccf-4675-9b34-05ef09f4b292,SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation,0.425696,"Conventional point cloud semantic segmentation methods usually employ an
encoder-decoder architecture, where mid-level features are locally aggregated
to extract geometric information. However, the over-reliance on these
class-agnostic local geometric representations may raise confusion between
local parts from different categories that are similar in appearance or
spatially adjacent. To address this issue, we argue that mid-level features can
be further enhanced with semantic information, and propose semantic-affine
transformation that transforms features of mid-level points belonging to
different categories with class-specific affine parameters. Based on this
technique, we propose SemAffiNet for point cloud semantic segmentation, which
utilizes the attention mechanism in the Transformer module to implicitly and
explicitly capture global structural knowledge within local parts for overall
comprehension of each category. We conduct extensive experiments on the
ScanNetV2 and NYUv2 datasets, and evaluate semantic-affine transformation on
various 3D point cloud and 2D image segmentation baselines, where both
qualitative and quantitative results demonstrate the superiority and
generalization ability of our proposed approach. Code is available at
https://github.com/wangzy22/SemAffiNet.",https://github.com/wangzy22/SemAffiNet,-1
91c8f7d7-0ff7-4bc2-8178-1c53db29427d,Two-stream Multi-dimensional Convolutional Network for Real-time Violence Detection,0.081662,"The increasing number of surveillance cameras and security concerns have made
automatic violent activity detection from surveillance footage an active area
for research. Modern deep learning methods have achieved good accuracy in
violence detection and proved to be successful because of their applicability
in intelligent surveillance systems. However, the models are computationally
expensive and large in size because of their inefficient methods for feature
extraction. This work presents a novel architecture for violence detection
called Two-stream Multi-dimensional Convolutional Network (2s-MDCN), which uses
RGB frames and optical flow to detect violence. Our proposed method extracts
temporal and spatial information independently by 1D, 2D, and 3D convolutions.
Despite combining multi-dimensional convolutional networks, our models are
lightweight and efficient due to reduced channel capacity, yet they learn to
extract meaningful spatial and temporal information. Additionally, combining
RGB frames and optical flow yields 2.2% more accuracy than a single RGB stream.
Regardless of having less complexity, our models obtained state-of-the-art
accuracy of 89.7% on the largest violence detection benchmark dataset.",None,-1
2d5757a1-d9f5-47eb-ae83-c4f0baf9c9ba,Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale,0.522143,"We introduce Saga, a next-generation knowledge construction and serving
platform for powering knowledge-based applications at industrial scale. Saga
follows a hybrid batch-incremental design to continuously integrate billions of
facts about real-world entities and construct a central knowledge graph that
supports multiple production use cases with diverse requirements around data
freshness, accuracy, and availability. In this paper, we discuss the unique
challenges associated with knowledge graph construction at industrial scale,
and review the main components of Saga and how they address these challenges.
Finally, we share lessons-learned from a wide array of production use cases
powered by Saga.",None,3350
deff21ef-5a35-4aeb-be83-a48cf4ed1f8c,Explainability of Predictive Process Monitoring Results: Can You See My Data Issues?,0.582827,"Predictive business process monitoring (PPM) has been around for several
years as a use case of process mining. PPM enables foreseeing the future of a
business process through predicting relevant information about how a running
process instance might end, related performance indicators, and other
predictable aspects. A big share of PPM approaches adopts a Machine Learning
(ML) technique to address a prediction task, especially non-process-aware PPM
approaches. Consequently, PPM inherits the challenges faced by ML approaches.
One of these challenges concerns the need to gain user trust in the predictions
generated. The field of explainable artificial intelligence (XAI) addresses
this issue. However, the choices made, and the techniques employed in a PPM
task, in addition to ML model characteristics, influence resulting
explanations. A comparison of the influence of different settings on the
generated explanations is missing. To address this gap, we investigate the
effect of different PPM settings on resulting data fed into an ML model and
consequently to a XAI method. We study how differences in resulting
explanations may indicate several issues in underlying data. We construct a
framework for our experiments including different settings at each stage of PPM
with XAI integrated as a fundamental part. Our experiments reveal several
inconsistencies, as well as agreements, between data characteristics (and hence
expectations about these data), important data used by the ML model as a result
of querying it, and explanations of predictions of the investigated ML model.",https://github.com/GhadaElkhawaga/PPM_XAI_Comparison.git,-1
90c756f1-d1ae-4d49-880f-bebb02905e83,SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels,0.833861,"Deep neural networks are prone to overfitting noisy labels, resulting in poor
generalization performance. To overcome this problem, we present a simple and
effective method self-ensemble label correction (SELC) to progressively correct
noisy labels and refine the model. We look deeper into the memorization
behavior in training with noisy labels and observe that the network outputs are
reliable in the early stage. To retain this reliable knowledge, SELC uses
ensemble predictions formed by an exponential moving average of network outputs
to update the original noisy labels. We show that training with SELC refines
the model by gradually reducing supervision from noisy labels and increasing
supervision from ensemble predictions. Despite its simplicity, compared with
many state-of-the-art methods, SELC obtains more promising and stable results
in the presence of class-conditional, instance-dependent, and real-world label
noise. The code is available at https://github.com/MacLLL/SELC.",https://github.com/MacLLL/SELC,-1
8542e4f8-044e-45da-855a-02a9e544f965,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,0.612578,"We have recently witnessed a number of impressive results on hard
mathematical reasoning problems with language models. At the same time, the
robustness of these models has also been called into question; recent works
have shown that models can rely on shallow patterns in the problem description
when generating a solution. Building on the idea of behavioral testing, we
propose a novel framework, which pins down the causal effect of various factors
in the input, e.g., the surface form of the problem text, the operands, and
math operators on the output solution. By grounding the behavioral analysis in
a causal graph describing an intuitive reasoning process, we study the behavior
of language models in terms of robustness and sensitivity to direct
interventions in the input space. We apply our framework on a test bed of math
word problems. Our analysis shows that robustness does not appear to
continuously improve as a function of size, but the GPT-3 Davinci models (175B)
achieve a dramatic improvement in both robustness and sensitivity compared to
all other GPT variants.",https://github.com/alestolfo/causal-math,-1
d987055d-9fe1-44a7-93a1-79abc43ef8bc,Overview of The MediaEval 2022 Predicting Video Memorability Task,0.406176,"This paper describes the 5th edition of the Predicting Video Memorability
Task as part of MediaEval2022. This year we have reorganised and simplified the
task in order to lubricate a greater depth of inquiry. Similar to last year,
two datasets are provided in order to facilitate generalisation, however, this
year we have replaced the TRECVid2019 Video-to-Text dataset with the VideoMem
dataset in order to remedy underlying data quality issues, and to prioritise
short-term memorability prediction by elevating the Memento10k dataset as the
primary dataset. Additionally, a fully fledged electroencephalography
(EEG)-based prediction sub-task is introduced. In this paper, we outline the
core facets of the task and its constituent sub-tasks; describing the datasets,
evaluation metrics, and requirements for participant submissions.",None,-1
1975baeb-d65f-45f1-8fec-63388da2fde7,Exploration of the possibility of infusing Social Media Trends into generating NFT Recommendations,0.459618,"Recommendations Systems have been identified to be one of the integral
elements of driving sales in e-commerce sites. The utilization of opinion
mining data extracted from trends has been attempted to improve the
recommendations that can be provided by baseline methods in this research when
user-click data is lacking or is difficult to be collected due to privacy
concerns.
  Utilizing social trends to influence the recommendations generated for a set
of unique items has been explored with the use of a suggested scoring
mechanism. Embracing concepts from decentralized networks that are expected to
change how users interact via the internet over the next couple of decades, the
suggested Recommendations System attempts to make use of multiple sources of
information, applying coherent information retrieval techniques to extract
probable trending items.
  The proposed Recommendations Architecture in the research presents a method
to integrate social trends with recommendations to produce promising outputs.",None,-1
a5651004-9eeb-41ff-8b42-9079625598e1,Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors,0.583465,"Multi-scale features have been proven highly effective for object detection
but often come with huge and even prohibitive extra computation costs,
especially for the recent Transformer-based detectors. In this paper, we
propose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm
that enables efficient use of multi-scale features in Transformer-based object
detectors. The core idea is to exploit sparse multi-scale features from just a
few crucial locations, and it is achieved with two novel designs. First, IMFA
rearranges the Transformer encoder-decoder pipeline so that the encoded
features can be iteratively updated based on the detection predictions. Second,
IMFA sparsely samples scale-adaptive features for refined detection from just a
few keypoint locations under the guidance of prior detection predictions. As a
result, the sampled multi-scale features are sparse yet still highly beneficial
for object detection. Extensive experiments show that the proposed IMFA boosts
the performance of multiple Transformer-based object detectors significantly
yet with only slight computational overhead.",https://github.com/ZhangGongjie/IMFA,-1
11f7ca6f-4d62-4b8b-9682-503053ec0b18,Strategy Synthesis for Zero-Sum Neuro-Symbolic Concurrent Stochastic Games,0.864665,"Neuro-symbolic approaches to artificial intelligence, which combine neural
networks with classical symbolic techniques, are growing in prominence,
necessitating formal approaches to reason about their correctness. We propose a
novel modelling formalism called neuro-symbolic concurrent stochastic games
(NS-CSGs), which comprise two probabilistic finite-state agents interacting in
a shared continuous-state environment. Each agent observes the environment
using a neural perception mechanism, which converts inputs such as images into
symbolic percepts, and makes decisions symbolically. We focus on the class of
NS-CSGs with Borel state spaces and prove the existence and measurability of
the value function for zero-sum discounted cumulative rewards under
piecewise-constant restrictions on the components of this class of models. To
compute values and synthesise strategies, we present, for the first time,
practical value iteration (VI) and policy iteration (PI) algorithms to solve
this new subclass of continuous-state CSGs. These require a finite
decomposition of the environment induced by the neural perception mechanisms of
the agents and rely on finite abstract representations of value functions and
strategies closed under VI or PI. First, we introduce a Borel measurable
piecewise-constant (B-PWC) representation of value functions, extend minimax
backups to this representation and propose a value iteration algorithm called
B-PWC VI. Second, we introduce two novel representations for the value
functions and strategies, constant-piecewise-linear (CON-PWL) and
constant-piecewise-constant (CON-PWC) respectively, and propose
Minimax-action-free PI by extending a recent PI method based on alternating
player choices for finite state spaces to Borel state spaces, which does not
require normal-form games to be solved.",None,16419
8569c238-795e-4667-9111-7aaa6fdd555c,CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification,0.724332,"Existing computer vision research in artwork struggles with artwork's
fine-grained attributes recognition and lack of curated annotated datasets due
to their costly creation. To the best of our knowledge, we are one of the first
methods to use CLIP (Contrastive Language-Image Pre-Training) to train a neural
network on a variety of artwork images and text descriptions pairs. CLIP is
able to learn directly from free-form art descriptions, or, if available,
curated fine-grained labels. Model's zero-shot capability allows predicting
accurate natural language description for a given image, without directly
optimizing for the task. Our approach aims to solve 2 challenges: instance
retrieval and fine-grained artwork attribute recognition. We use the iMet
Dataset, which we consider the largest annotated artwork dataset. In this
benchmark we achieved competitive results using only self-supervision.",https://github.com/KeremTurgutlu/clip_art,-1
86b3cc65-c3e9-4b22-8790-da555db8a431,Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation,0.52127,"Multi-modal Machine Translation (MMT) enables the use of visual information
to enhance the quality of translations. The visual information can serve as a
valuable piece of context information to decrease the ambiguity of input
sentences. Despite the increasing popularity of such a technique, good and
sizeable datasets are scarce, limiting the full extent of their potential.
Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It
is estimated that about 100 to 150 million people speak the language, with more
than 80 million indigenous speakers. This is more than any of the other Chadic
languages. Despite a large number of speakers, the Hausa language is considered
low-resource in natural language processing (NLP). This is due to the absence
of sufficient resources to implement most NLP tasks. While some datasets exist,
they are either scarce, machine-generated, or in the religious domain.
Therefore, there is a need to create training and evaluation data for
implementing machine learning tasks and bridging the research gap in the
language. This work presents the Hausa Visual Genome (HaVG), a dataset that
contains the description of an image or a section within the image in Hausa and
its equivalent in English. To prepare the dataset, we started by translating
the English description of the images in the Hindi Visual Genome (HVG) into
Hausa automatically. Afterward, the synthetic Hausa data was carefully
post-edited considering the respective images. The dataset comprises 32,923
images and their descriptions that are divided into training, development,
test, and challenge test set. The Hausa Visual Genome is the first dataset of
its kind and can be used for Hausa-English machine translation, multi-modal
research, and image description, among various other natural language
processing and generation tasks.",https://github.com/abumafrim/visual-genome-dataset-creation-tool,-1
48f86cfd-0178-48ff-b9c7-a4edde57d61f,Knowledge Sharing via Domain Adaptation in Customs Fraud Detection,0.634039,"Knowledge of the changing traffic is critical in risk management. Customs
offices worldwide have traditionally relied on local resources to accumulate
knowledge and detect tax fraud. This naturally poses countries with weak
infrastructure to become tax havens of potentially illicit trades. The current
paper proposes DAS, a memory bank platform to facilitate knowledge sharing
across multi-national customs administrations to support each other. We propose
a domain adaptation method to share transferable knowledge of frauds as
prototypes while safeguarding the local trade information. Data encompassing
over 8 million import declarations have been used to test the feasibility of
this new system, which shows that participating countries may benefit up to
2-11 times in fraud detection with the help of shared knowledge. We discuss
implications for substantial tax revenue potential and strengthened policy
against illicit trades.",None,-1
524c3e10-7eb4-4a36-8e4c-411804f247b9,Abstract Interpretation on E-Graphs,0.156808,"Recent e-graph applications have typically considered concrete semantics of
expressions, where the notion of equivalence stems from concrete interpretation
of expressions. However, equivalences that hold over one interpretation may not
hold in an alternative interpretation. Such an observation can be exploited. We
consider the application of abstract interpretation to e-graphs, and show that
within an e-graph, the lattice meet operation associated with the abstract
domain has a natural interpretation for an e-class, leading to improved
precision in over-approximation. In this extended abstract, we use Interval
Arithmetic (IA) to illustrate this point.",None,-1
55530bc5-b4d9-42b0-8ce2-65e83f8efdda,Tackling Online One-Class Incremental Learning by Removing Negative Contrasts,0.355712,"Recent work studies the supervised online continual learning setting where a
learner receives a stream of data whose class distribution changes over time.
Distinct from other continual learning settings the learner is presented new
samples only once and must distinguish between all seen classes. A number of
successful methods in this setting focus on storing and replaying a subset of
samples alongside incoming data in a computationally efficient manner. One
recent proposal ER-AML achieved strong performance in this setting by applying
an asymmetric loss based on contrastive learning to the incoming data and
replayed data. However, a key ingredient of the proposed method is avoiding
contrasts between incoming data and stored data, which makes it impractical for
the setting where only one new class is introduced in each phase of the stream.
In this work we adapt a recently proposed approach (\textit{BYOL}) from
self-supervised learning to the supervised learning setting, unlocking the
constraint on contrasts. We then show that supplementing this with additional
regularization on class prototypes yields a new method that achieves strong
performance in the one-class incremental learning setting and is competitive
with the top performing methods in the multi-class incremental setting.",None,-1
388ca18f-953e-4ed8-a062-5666cb5df8f6,An Accelerator for Rule Induction in Fuzzy Rough Theory,0.561441,"Rule-based classifier, that extract a subset of induced rules to efficiently
learn/mine while preserving the discernibility information, plays a crucial
role in human-explainable artificial intelligence. However, in this era of big
data, rule induction on the whole datasets is computationally intensive. So
far, to the best of our knowledge, no known method focusing on accelerating
rule induction has been reported. This is first study to consider the
acceleration technique to reduce the scale of computation in rule induction. We
propose an accelerator for rule induction based on fuzzy rough theory; the
accelerator can avoid redundant computation and accelerate the building of a
rule classifier. First, a rule induction method based on consistence degree,
called Consistence-based Value Reduction (CVR), is proposed and used as basis
to accelerate. Second, we introduce a compacted search space termed Key Set,
which only contains the key instances required to update the induced rule, to
conduct value reduction. The monotonicity of Key Set ensures the feasibility of
our accelerator. Third, a rule-induction accelerator is designed based on Key
Set, and it is theoretically guaranteed to display the same results as the
unaccelerated version. Specifically, the rank preservation property of Key Set
ensures consistency between the rule induction achieved by the accelerator and
the unaccelerated method. Finally, extensive experiments demonstrate that the
proposed accelerator can perform remarkably faster than the unaccelerated
rule-based classifier methods, especially on datasets with numerous instances.",https://github.com/RUC-DWBI-ML/A-CVRC,15039
ce79288a-b658-4627-a4ed-589c096f7b73,WiCV 2021: The Eighth Women In Computer Vision Workshop,0.864665,"In this paper, we present the details of Women in Computer Vision Workshop -
WiCV 2021, organized alongside the virtual CVPR 2021. It provides a voice to a
minority (female) group in the computer vision community and focuses on
increasing the visibility of these researchers, both in academia and industry.
WiCV believes that such an event can play an important role in lowering the
gender imbalance in the field of computer vision. WiCV is organized each year
where it provides a)~opportunity for collaboration between researchers from
minority groups, b)~mentorship to female junior researchers, c)~financial
support to presenters to overcome monetary burden and d)~large and diverse
choice of role models, who can serve as examples to younger researchers at the
beginning of their careers. In this paper, we present a report on the workshop
program, trends over the past years, a summary of statistics regarding
presenters, attendees, and sponsorship for the WiCV 2021 workshop.",None,-1
4207356c-0a8a-47cf-999b-6cd6fe295915,CounTR: Transformer-based Generalised Visual Counting,0.701063,"In this paper, we consider the problem of generalised visual object counting,
with the goal of developing a computational model for counting the number of
objects from arbitrary semantic categories, using arbitrary number of
""exemplars"", i.e. zero-shot or few-shot counting. To this end, we make the
following four contributions: (1) We introduce a novel transformer-based
architecture for generalised visual object counting, termed as Counting
Transformer (CounTR), which explicitly capture the similarity between image
patches or with given ""exemplars"" with the attention mechanism;(2) We adopt a
two-stage training regime, that first pre-trains the model with self-supervised
learning, and followed by supervised fine-tuning;(3) We propose a simple,
scalable pipeline for synthesizing training images with a large number of
instances or that from different semantic categories, explicitly forcing the
model to make use of the given ""exemplars"";(4) We conduct thorough ablation
studies on the large-scale counting benchmark, e.g. FSC-147, and demonstrate
state-of-the-art performance on both zero and few-shot settings.",https://verg-avesta.github.io/CounTR_Webpage/,-1
28a2d8c3-a973-4c29-be3a-bcfb75f1af24,Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks,0.0334394,"Parameter-efficient tuning aims to distill knowledge for downstream tasks by
optimizing a few introduced parameters while freezing the pretrained language
models (PLMs). Continuous prompt tuning which prepends a few trainable vectors
to the embeddings of input is one of these methods and has drawn much attention
due to its effectiveness and efficiency. This family of methods can be
illustrated as exerting nonlinear transformations of hidden states inside PLMs.
However, a natural question is ignored: can the hidden states be directly used
for classification without changing them? In this paper, we aim to answer this
question by proposing a simple tuning method which only introduces three
trainable vectors. Firstly, we integrate all layers hidden states using the
introduced vectors. And then, we input the integrated hidden state(s) to a
task-specific linear classifier to predict categories. This scheme is similar
to the way ELMo utilises hidden states except that they feed the hidden states
to LSTM-based models. Although our proposed tuning scheme is simple, it
achieves comparable performance with prompt tuning methods like P-tuning and
P-tuning v2, verifying that original hidden states do contain useful
information for classification tasks. Moreover, our method has an advantage
over prompt tuning in terms of time and the number of parameters.",None,334
4911b6db-35a2-4387-9699-998a90c5b4b8,Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?,0.925171,"What can pre-trained multilingual sequence-to-sequence models like mBART
contribute to translating low-resource languages? We conduct a thorough
empirical experiment in 10 languages to ascertain this, considering five
factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning
data, (3) the amount of pre-training data in the model, (4) the impact of
domain mismatch, and (5) language typology. In addition to yielding several
heuristics, the experiments form a framework for evaluating the data
sensitivities of machine translation systems. While mBART is robust to domain
differences, its translations for unseen and typologically distant languages
remain below 3.0 BLEU. In answer to our title's question, mBART is not a
low-resource panacea; we therefore encourage shifting the emphasis from new
models to new data.",https://github.com/LRLNMT/,-1
642d3d3c-8491-4382-8327-e05236e86e0f,Mixture of basis for interpretable continual learning with distribution shifts,0.0199185,"Continual learning in environments with shifting data distributions is a
challenging problem with several real-world applications. In this paper we
consider settings in which the data distribution(task) shifts abruptly and the
timing of these shifts are not known. Furthermore, we consider a
semi-supervised task-agnostic setting in which the learning algorithm has
access to both task-segmented and unsegmented data for offline training. We
propose a novel approach called mixture of Basismodels (MoB) for addressing
this problem setting. The core idea is to learn a small set of basis models and
to construct a dynamic, task-dependent mixture of the models to predict for the
current task. We also propose a new methodology to detect observations that are
out-of-distribution with respect to the existing basis models and to
instantiate new models as needed. We test our approach in multiple domains and
show that it attains better prediction error than existing methods in most
cases while using fewer models than other multiple model approaches. Moreover,
we analyze the latent task representations learned by MoB and show that similar
tasks tend to cluster in the latent space and that the latent representation
shifts at the task boundaries when tasks are dissimilar.",None,-1
f5e77932-c9c2-4be9-bfbf-a848e023f1ce,Radial Basis Function Networks for Convolutional Neural Networks to Learn Similarity Distance Metric and Improve Interpretability,0.638604,"Radial basis function neural networks (RBFs) are prime candidates for pattern
classification and regression and have been used extensively in classical
machine learning applications. However, RBFs have not been integrated into
contemporary deep learning research and computer vision using conventional
convolutional neural networks (CNNs) due to their lack of adaptability with
modern architectures. In this paper, we adapt RBF networks as a classifier on
top of CNNs by modifying the training process and introducing a new activation
function to train modern vision architectures end-to-end for image
classification. The specific architecture of RBFs enables the learning of a
similarity distance metric to compare and find similar and dissimilar images.
Furthermore, we demonstrate that using an RBF classifier on top of any CNN
architecture provides new human-interpretable insights about the
decision-making process of the models. Finally, we successfully apply RBFs to a
range of CNN architectures and evaluate the results on benchmark computer
vision datasets.",None,-1
14f1c326-12fa-460f-a695-8ab582f068d0,DFA: Dynamic Feature Aggregation for Efficient Video Object Detection,0.622218,"Video object detection is a fundamental yet challenging task in computer
vision. One practical solution is to take advantage of temporal information
from the video and apply feature aggregation to enhance the object features in
each frame. Though effective, those existing methods always suffer from low
inference speeds because they use a fixed number of frames for feature
aggregation regardless of the input frame. Therefore, this paper aims to
improve the inference speed of the current feature aggregation-based video
object detectors while maintaining their performance. To achieve this goal, we
propose a vanilla dynamic aggregation module that adaptively selects the frames
for feature enhancement. Then, we extend the vanilla dynamic aggregation module
to a more effective and reconfigurable deformable version. Finally, we
introduce inplace distillation loss to improve the representations of objects
aggregated with fewer frames. Extensive experimental results validate the
effectiveness and efficiency of our proposed methods: On the ImageNet VID
benchmark, integrated with our proposed methods, FGFA and SELSA can improve the
inference speed by 31% and 76% respectively while getting comparable
performance on accuracy.",https://github.com/open-mmlab/mmtracking,-1
05f108de-6f83-4f87-9e63-4607eab42a49,Improving Neural Machine Translation of Indigenous Languages with Multilingual Transfer Learning,0.271443,"Machine translation (MT) involving Indigenous languages, including those
possibly endangered, is challenging due to lack of sufficient parallel data. We
describe an approach exploiting bilingual and multilingual pretrained MT models
in a transfer learning setting to translate from Spanish to ten South American
Indigenous languages. Our models set new SOTA on five out of the ten language
pairs we consider, even doubling performance on one of these five pairs. Unlike
previous SOTA that perform data augmentation to enlarge the train sets, we
retain the low-resource setting to test the effectiveness of our models under
such a constraint. In spite of the rarity of linguistic information available
about the Indigenous languages, we offer a number of quantitative and
qualitative analyses (e.g., as to morphology, tokenization, and orthography) to
contextualize our results.",None,-1
d768c5c8-54af-425a-9b8d-e36da3171924,Semantic properties of English nominal pluralization: Insights from word embeddings,0.718231,"Semantic differentiation of nominal pluralization is grammaticalized in many
languages. For example, plural markers may only be relevant for human nouns.
English does not appear to make such distinctions. Using distributional
semantics, we show that English nominal pluralization exhibits semantic
clusters. For instance, pluralization of fruit words is more similar to one
another and less similar to pluralization of other semantic classes. Therefore,
reduction of the meaning shift in plural formation to the addition of an
abstract plural meaning is too simplistic. A semantically informed method,
called CosClassAvg, is introduced that outperforms pluralization methods in
distributional semantics which assume plural formation amounts to the addition
of a fixed plural vector. In comparison with our approach, a method from
compositional distributional semantics, called FRACSS, predicted plural vectors
that were more similar to the corpus-extracted plural vectors in terms of
direction but not vector length. A modeling study reveals that the observed
difference between the two predicted semantic spaces by CosClassAvg and FRACSS
carries over to how well a computational model of the listener can understand
previously unencountered plural forms. Mappings from word forms, represented
with triphone vectors, to predicted semantic vectors are more productive when
CosClassAvg-generated semantic vectors are employed as gold standard vectors
instead of FRACSS-generated vectors.",None,-1
587bf69c-28cb-441b-b8e2-4da7dabaadd6,Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking,0.286404,"Current natural language interaction for self-tracking tools largely depends
on bespoke implementation optimized for a specific tracking theme and data
format, which is neither generalizable nor scalable to a tremendous design
space of self-tracking. However, training machine learning models in the
context of self-tracking is challenging due to the wide variety of tracking
topics and data formats. In this paper, we propose a novel NLP task for
self-tracking that extracts close- and open-ended information from a
retrospective activity log described as a plain text, and a domain-agnostic,
GPT-3-based NLU framework that performs this task. The framework augments the
prompt using synthetic samples to transform the task into 10-shot learning, to
address a cold-start problem in bootstrapping a new tracking topic. Our
preliminary evaluation suggests that our approach significantly outperforms the
baseline QA models. Going further, we discuss future application domains toward
which the NLP and HCI researchers can collaborate.",None,-1
abc0010d-f133-48b8-bba3-a697c1a27910,On-Board Pedestrian Trajectory Prediction Using Behavioral Features,0.562441,"This paper presents a novel approach to pedestrian trajectory prediction for
on-board camera systems, which utilizes behavioral features of pedestrians that
can be inferred from visual observations. Our proposed method, called
Behavior-Aware Pedestrian Trajectory Prediction (BA-PTP), processes multiple
input modalities, i.e. bounding boxes, body and head orientation of pedestrians
as well as their pose, with independent encoding streams. The encodings of each
stream are fused using a modality attention mechanism, resulting in a final
embedding that is used to predict future bounding boxes in the image.
  In experiments on two datasets for pedestrian behavior prediction, we
demonstrate the benefit of using behavioral features for pedestrian trajectory
prediction and evaluate the effectiveness of the proposed encoding strategy.
Additionally, we investigate the relevance of different behavioral features on
the prediction performance based on an ablation study.",None,-1
6d5bac3f-f39f-45be-a48f-20772119711f,KGRGRL: A User's Permission Reasoning Method Based on Knowledge Graph Reward Guidance Reinforcement Learning,0.130642,"In general, multiple domain cyberspace security assessments can be
implemented by reasoning user's permissions. However, while existing methods
include some information from the physical and social domains, they do not
provide a comprehensive representation of cyberspace. Existing reasoning
methods are also based on expert-given rules, resulting in inefficiency and a
low degree of intelligence. To address this challenge, we create a Knowledge
Graph (KG) of multiple domain cyberspace in order to provide a standard
semantic description of the multiple domain cyberspace. Following that, we
proposed a user's permissions reasoning method based on reinforcement learning.
All permissions in cyberspace are represented as nodes, and an agent is trained
to find all permissions that user can have according to user's initial
permissions and cyberspace KG. We set 10 reward setting rules based on the
features of cyberspace KG in the reinforcement learning of reward information
setting, so that the agent can better locate user's all permissions and avoid
blindly finding user's permissions. The results of the experiments showed that
the proposed method can successfully reason about user's permissions and
increase the intelligence level of the user's permissions reasoning method. At
the same time, the F1 value of the proposed method is 6% greater than that of
the Translating Embedding (TransE) method.",None,-1
2ffcc61f-63e2-4446-98d1-31dac2ecc80e,"CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework Using CNN, Visual Transformer and Multilayer Perceptron",0.768205,"Cervical cancer is the seventh most common cancer among all the cancers
worldwide and the fourth most common cancer among women. Cervical cytopathology
image classification is an important method to diagnose cervical cancer. Manual
screening of cytopathology images is time-consuming and error-prone. The
emergence of the automatic computer-aided diagnosis system solves this problem.
This paper proposes a framework called CVM-Cervix based on deep learning to
perform cervical cell classification tasks. It can analyze pap slides quickly
and accurately. CVM-Cervix first proposes a Convolutional Neural Network module
and a Visual Transformer module for local and global feature extraction
respectively, then a Multilayer Perceptron module is designed to fuse the local
and global features for the final classification. Experimental results show the
effectiveness and potential of the proposed CVM-Cervix in the field of cervical
Pap smear image classification. In addition, according to the practical needs
of clinical work, we perform a lightweight post-processing to compress the
model.",None,14255
639b313d-5869-45dd-b89e-8375f932b532,SI-GAT: A method based on improved Graph Attention Network for sonar image classification,0.0589829,"The existing sonar image classification methods based on deep learning are
often analyzed in Euclidean space, only considering the local image features.
For this reason, this paper presents a sonar classification method based on
improved Graph Attention Network (GAT), namely SI-GAT, which is applicable to
multiple types imaging sonar. This method quantifies the correlation
relationship between nodes based on the joint calculation of color proximity
and spatial proximity that represent the sonar characteristics in non-Euclidean
space, then the KNN (K-Nearest Neighbor) algorithm is used to determine the
neighborhood range and adjacency matrix in the graph attention mechanism, which
are jointly considered with the attention coefficient matrix to construct the
key part of the SI-GAT. This SI-GAT is superior to several CNN (Convolutional
Neural Network) methods based on Euclidean space through validation of real
data.",https://github.com/huoguanying/SeabedObjects-Ship-and-Airplane-dataset.git,-1
5a79edb2-3912-476a-9860-6a906e25f8f5,Improving robustness of language models from a geometry-aware perspective,0.412737,"Recent studies have found that removing the norm-bounded projection and
increasing search steps in adversarial training can significantly improve
robustness. However, we observe that a too large number of search steps can
hurt accuracy. We aim to obtain strong robustness efficiently using fewer
steps. Through a toy experiment, we find that perturbing the clean data to the
decision boundary but not crossing it does not degrade the test accuracy.
Inspired by this, we propose friendly adversarial data augmentation (FADA) to
generate friendly adversarial data. On top of FADA, we propose geometry-aware
adversarial training (GAT) to perform adversarial training on friendly
adversarial data so that we can save a large number of search steps.
Comprehensive experiments across two widely used datasets and three pre-trained
language models demonstrate that GAT can obtain stronger robustness via fewer
steps. In addition, we provide extensive empirical results and in-depth
analyses on robustness to facilitate future studies.",https://github.com/QData/TextAttack,-1
65f8c301-4fcd-4cfd-8029-98ef64a82f0e,Best of Both Worlds Model Selection,0.19768,"We study the problem of model selection in bandit scenarios in the presence
of nested policy classes, with the goal of obtaining simultaneous adversarial
and stochastic (""best of both worlds"") high-probability regret guarantees. Our
approach requires that each base learner comes with a candidate regret bound
that may or may not hold, while our meta algorithm plays each base learner
according to a schedule that keeps the base learner's candidate regret bounds
balanced until they are detected to violate their guarantees. We develop
careful mis-specification tests specifically designed to blend the above model
selection criterion with the ability to leverage the (potentially benign)
nature of the environment. We recover the model selection guarantees of the
CORRAL algorithm for adversarial environments, but with the additional benefit
of achieving high probability regret bounds, specifically in the case of nested
adversarial linear bandits. More importantly, our model selection results also
hold simultaneously in stochastic environments under gap assumptions. These are
the first theoretical results that achieve best of both world (stochastic and
adversarial) guarantees while performing model selection in (linear) bandit
scenarios.",None,-1
1232aadd-1e6f-4556-896d-0c5b0c2f2ea2,Towards Unbiased Label Distribution Learning for Facial Pose Estimation Using Anisotropic Spherical Gaussian,0.37848,"Facial pose estimation refers to the task of predicting face orientation from
a single RGB image. It is an important research topic with a wide range of
applications in computer vision. Label distribution learning (LDL) based
methods have been recently proposed for facial pose estimation, which achieve
promising results. However, there are two major issues in existing LDL methods.
First, the expectations of label distributions are biased, leading to a biased
pose estimation. Second, fixed distribution parameters are applied for all
learning samples, severely limiting the model capability. In this paper, we
propose an Anisotropic Spherical Gaussian (ASG)-based LDL approach for facial
pose estimation. In particular, our approach adopts the spherical Gaussian
distribution on a unit sphere which constantly generates unbiased expectation.
Meanwhile, we introduce a new loss function that allows the network to learn
the distribution parameter for each learning sample flexibly. Extensive
experimental results show that our method sets new state-of-the-art records on
AFLW2000 and BIWI datasets.",None,-1
bfc2a81f-e675-4f1c-9d34-4cddcb0ac2aa,Concept Bottleneck Model with Additional Unsupervised Concepts,0.645908,"With the increasing demands for accountability, interpretability is becoming
an essential capability for real-world AI applications. However, most methods
utilize post-hoc approaches rather than training the interpretable model. In
this article, we propose a novel interpretable model based on the concept
bottleneck model (CBM). CBM uses concept labels to train an intermediate layer
as the additional visible layer. However, because the number of concept labels
restricts the dimension of this layer, it is difficult to obtain high accuracy
with a small number of labels. To address this issue, we integrate supervised
concepts with unsupervised ones trained with self-explaining neural networks
(SENNs). By seamlessly training these two types of concepts while reducing the
amount of computation, we can obtain both supervised and unsupervised concepts
simultaneously, even for large-sized images. We refer to the proposed model as
the concept bottleneck model with additional unsupervised concepts (CBM-AUC).
We experimentally confirmed that the proposed model outperformed CBM and SENN.
We also visualized the saliency map of each concept and confirmed that it was
consistent with the semantic meanings.",https://github.com/yewsiang/ConceptBottleneck,-1
cb0236bb-897d-4dec-b2ff-90740427d0f2,Weight Predictor Network with Feature Selection for Small Sample Tabular Biomedical Data,0.456113,"Tabular biomedical data is often high-dimensional but with a very small
number of samples. Although recent work showed that well-regularised simple
neural networks could outperform more sophisticated architectures on tabular
data, they are still prone to overfitting on tiny datasets with many
potentially irrelevant features. To combat these issues, we propose Weight
Predictor Network with Feature Selection (WPFS) for learning neural networks
from high-dimensional and small sample data by reducing the number of learnable
parameters and simultaneously performing feature selection. In addition to the
classification network, WPFS uses two small auxiliary networks that together
output the weights of the first layer of the classification model. We evaluate
on nine real-world biomedical datasets and demonstrate that WPFS outperforms
other standard as well as more recent methods typically applied to tabular
data. Furthermore, we investigate the proposed feature selection mechanism and
show that it improves performance while providing useful insights into the
learning task.",https://github.com/andreimargeloiu/WPFS,-1
9c4a54ca-3ed8-41f3-83eb-7e6f99522f53,BiPOCO: Bi-Directional Trajectory Prediction with Pose Constraints for Pedestrian Anomaly Detection,0.316616,"We present BiPOCO, a Bi-directional trajectory predictor with POse
COnstraints, for detecting anomalous activities of pedestrians in videos. In
contrast to prior work based on feature reconstruction, our work identifies
pedestrian anomalous events by forecasting their future trajectories and
comparing the predictions with their expectations. We introduce a set of novel
compositional pose-based losses with our predictor and leverage prediction
errors of each body joint for pedestrian anomaly detection. Experimental
results show that our BiPOCO approach can detect pedestrian anomalous
activities with a high detection rate (up to 87.0%) and incorporating pose
constraints helps distinguish normal and anomalous poses in prediction. This
work extends current literature of using prediction-based methods for anomaly
detection and can benefit safety-critical applications such as autonomous
driving and surveillance. Code is available at
https://github.com/akanuasiegbu/BiPOCO.",https://github.com/akanuasiegbu/BiPOCO,-1
403f2f4d-434d-4ae1-a854-26b05ce4abb0,Thermodynamics-informed neural networks for physically realistic mixed reality,0.712728,"The imminent impact of immersive technologies in society urges for active
research in real-time and interactive physics simulation for virtual worlds to
be realistic. In this context, realistic means to be compliant to the laws of
physics. In this paper we present a method for computing the dynamic response
of (possibly non-linear and dissipative) deformable objects induced by
real-time user interactions in mixed reality using deep learning. The
graph-based architecture of the method ensures the thermodynamic consistency of
the predictions, whereas the visualization pipeline allows a natural and
realistic user experience. Two examples of virtual solids interacting with
virtual or physical solids in mixed reality scenarios are provided to prove the
performance of the method.",https://github.com/quercushernandez,-1
a09c7104-6d95-41a6-b0d1-21f2aaed4b69,Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask,0.236783,"Reading is integral to everyday life, and yet learning to read is a struggle
for many young learners. During lessons, teachers can use comprehension
questions to increase engagement, test reading skills, and improve retention.
Historically such questions were written by skilled teachers, but recently
language models have been used to generate comprehension questions. However,
many existing Question Generation (QG) systems focus on generating literal
questions from the text, and have no way to control the type of the generated
question. In this paper, we study QG for reading comprehension where
inferential questions are critical and extractive techniques cannot be used. We
propose a two-step model (HTA-WTA) that takes advantage of previous datasets,
and can generate questions for a specific targeted comprehension skill. We
propose a new reading comprehension dataset that contains questions annotated
with story-based reading comprehension skills (SBRCS), allowing for a more
complete reader assessment. Across several experiments, our results show that
HTA-WTA outperforms multiple strong baselines on this new dataset. We show that
the HTA-WTA model tests for strong SCRS by asking deep inferential questions.",https://github.com/seanie12/neural-question-generation,-1
52b410a2-a8cf-4401-9ec1-48dae1412722,READ: Large-Scale Neural Scene Rendering for Autonomous Driving,0.626653,"Synthesizing free-view photo-realistic images is an important task in
multimedia. With the development of advanced driver assistance systems~(ADAS)
and their applications in autonomous vehicles, experimenting with different
scenarios becomes a challenge. Although the photo-realistic street scenes can
be synthesized by image-to-image translation methods, which cannot produce
coherent scenes due to the lack of 3D information. In this paper, a large-scale
neural rendering method is proposed to synthesize the autonomous driving
scene~(READ), which makes it possible to synthesize large-scale driving
scenarios on a PC through a variety of sampling schemes. In order to represent
driving scenarios, we propose an {\omega} rendering network to learn neural
descriptors from sparse point clouds. Our model can not only synthesize
realistic driving scenes but also stitch and edit driving scenes. Experiments
show that our model performs well in large-scale driving scenarios.",https://github.com/JOP-Lee/READ-Large-Scale-Neural-,-1
5eaf0076-a50e-453d-86f9-407b0cdca12d,Zero-shot Cross-Linguistic Learning of Event Semantics,0.0820605,"Typologically diverse languages offer systems of lexical and grammatical
aspect that allow speakers to focus on facets of event structure in ways that
comport with the specific communicative setting and discourse constraints they
face. In this paper, we look specifically at captions of images across Arabic,
Chinese, Farsi, German, Russian, and Turkish and describe a computational model
for predicting lexical aspects. Despite the heterogeneity of these languages,
and the salient invocation of distinctive linguistic resources across their
caption corpora, speakers of these languages show surprising similarities in
the ways they frame image content. We leverage this observation for zero-shot
cross-lingual learning and show that lexical aspects can be predicted for a
given language despite not having observed any annotated data for this language
at all.",None,-1
05e35bde-02e2-4c15-89b9-bdea2bd5d67b,Mean Field Games on Weighted and Directed Graphs via Colored Digraphons,0.203591,"The field of multi-agent reinforcement learning (MARL) has made considerable
progress towards controlling challenging multi-agent systems by employing
various learning methods. Numerous of these approaches focus on empirical and
algorithmic aspects of the MARL problems and lack a rigorous theoretical
foundation. Graphon mean field games (GMFGs) on the other hand provide a
scalable and mathematically well-founded approach to learning problems that
involve a large number of connected agents. In standard GMFGs, the connections
between agents are undirected, unweighted and invariant over time. Our paper
introduces colored digraphon mean field games (CDMFGs) which allow for weighted
and directed links between agents that are also adaptive over time. Thus,
CDMFGs are able to model more complex connections than standard GMFGs. Besides
a rigorous theoretical analysis including both existence and convergence
guarantees, we provide a learning scheme and illustrate our findings with an
epidemics model and a model of the systemic risk in financial markets.",None,3777
ce4fb9f7-8735-4389-a419-8a33616125b0,Learning Sample Importance for Cross-Scenario Video Temporal Grounding,0.257343,"The task of temporal grounding aims to locate video moment in an untrimmed
video, with a given sentence query. This paper for the first time investigates
some superficial biases that are specific to the temporal grounding task, and
proposes a novel targeted solution. Most alarmingly, we observe that existing
temporal ground models heavily rely on some biases (e.g., high preference on
frequent concepts or certain temporal intervals) in the visual modal. This
leads to inferior performance when generalizing the model in cross-scenario
test setting. To this end, we propose a novel method called Debiased Temporal
Language Localizer (DebiasTLL) to prevent the model from naively memorizing the
biases and enforce it to ground the query sentence based on true inter-modal
relationship. Debias-TLL simultaneously trains two models. By our design, a
large discrepancy of these two models' predictions when judging a sample
reveals higher probability of being a biased sample. Harnessing the informative
discrepancy, we devise a data re-weighing scheme for mitigating the data
biases. We evaluate the proposed model in cross-scenario temporal grounding,
where the train / test data are heterogeneously sourced. Experiments show
large-margin superiority of the proposed method in comparison with
state-of-the-art competitors.",None,-1
11a61a75-39fb-48f1-9e25-df616cd77c99,Hierarchical Decision Transformer,0.593557,"Sequence models in reinforcement learning require task knowledge to estimate
the task policy. This paper presents a hierarchical algorithm for learning a
sequence model from demonstrations. The high-level mechanism guides the
low-level controller through the task by selecting sub-goals for the latter to
reach. This sequence replaces the returns-to-go of previous methods, improving
its performance overall, especially in tasks with longer episodes and scarcer
rewards. We validate our method in multiple tasks of OpenAIGym, D4RL and
RoboMimic benchmarks. Our method outperforms the baselines in eight out of ten
tasks of varied horizons and reward frequencies without prior task knowledge,
showing the advantages of the hierarchical model approach for learning from
demonstrations using a sequence model.",None,3
74df8a5f-ed03-4d7a-91aa-2aa5a89e7e4d,Abstraction-Refinement for Hierarchical Probabilistic Models,0.345516,"Markov decision processes are a ubiquitous formalism for modelling systems
with non-deterministic and probabilistic behavior. Verification of these models
is subject to the famous state space explosion problem. We alleviate this
problem by exploiting a hierarchical structure with repetitive parts. This
structure not only occurs naturally in robotics, but also in probabilistic
programs describing, e.g., network protocols. Such programs often repeatedly
call a subroutine with similar behavior. In this paper, we focus on a local
case, in which the subroutines have a limited effect on the overall system
state. The key ideas to accelerate analysis of such programs are (1) to treat
the behavior of the subroutine as uncertain and only remove this uncertainty by
a detailed analysis if needed, and (2) to abstract similar subroutines into a
parametric template, and then analyse this template. These two ideas are
embedded into an abstraction-refinement loop that analyses hierarchical MDPs. A
prototypical implementation shows the efficacy of the approach.",None,-1
57ff2b0c-e8db-42bb-a6d2-2aeb7db8f94c,Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning,0.858513,"We introduce Patch Aligned Contrastive Learning (PACL), a modified
compatibility function for CLIP's contrastive loss, intending to train an
alignment between the patch tokens of the vision encoder and the CLS token of
the text encoder. With such an alignment, a model can identify regions of an
image corresponding to a given text input, and therefore transfer seamlessly to
the task of open vocabulary semantic segmentation without requiring any
segmentation annotations during training. Using pre-trained CLIP encoders with
PACL, we are able to set the state-of-the-art on the task of open vocabulary
zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC,
Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also
applicable to image-level predictions and when used with a CLIP backbone,
provides a general improvement in zero-shot classification accuracy compared to
CLIP, across a suite of 12 image classification datasets.",None,-1
1c17e97c-a65b-48a5-bd92-b08741415b51,Ditto: Building Digital Twins of Articulated Objects from Interaction,0.930759,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto",https://ut-austin-rpl.github.io/Ditto/,18716
c3ab0602-98c6-41f8-b6c4-35f568eab858,LeNSE: Learning To Navigate Subgraph Embeddings for Large-Scale Combinatorial Optimisation,0.169861,"Combinatorial Optimisation problems arise in several application domains and
are often formulated in terms of graphs. Many of these problems are NP-hard,
but exact solutions are not always needed. Several heuristics have been
developed to provide near-optimal solutions; however, they do not typically
scale well with the size of the graph. We propose a low-complexity approach for
identifying a (possibly much smaller) subgraph of the original graph where the
heuristics can be run in reasonable time and with a high likelihood of finding
a global near-optimal solution. The core component of our approach is LeNSE, a
reinforcement learning algorithm that learns how to navigate the space of
possible subgraphs using an Euclidean subgraph embedding as its map. To solve
CO problems, LeNSE is provided with a discriminative embedding trained using
any existing heuristics using only on a small portion of the original graph.
When tested on three problems (vertex cover, max-cut and influence
maximisation) using real graphs with up to $10$ million edges, LeNSE identifies
small subgraphs yielding solutions comparable to those found by running the
heuristics on the entire graph, but at a fraction of the total run time.",https://github.com/davidireland3/LeNSE,-1
0ce131b1-ef04-4fd1-b205-128dac21ed1d,Vehicle-road Cooperative Simulation and 3D Visualization System,0.0983417,"The safety of single-vehicle autonomous driving technology is limited due to
the limits of perception capability of on-board sensors. In contrast,
vehicle-road collaboration technology can overcome those limits and improve the
traffic safety and efficiency, by expanding the sensing range, improving the
perception accuracy, and reducing the response time. However, such a technology
is still under development; it requires rigorous testing and verification
methods to ensure the reliability and trustworthiness of the technology. In
this thesis, we focus on three major tasks: (1) analyze the functional
characteristics related to the scenarios of vehicle-road cooperations,
highlightening the differences between vehicle-road cooperative systems and
traditional single-vehicle autonomous driving systems; (2) refine and classifiy
the functional characteristics of vehicle-road cooperative systems; (3) design
and develop a simulation system, and provide a visual interface to facilitate
development and analysis. The efficiency and effectiveness the proposed method
are verfied by experiments.",None,-1
ff827454-c92e-461d-9334-bcb419141cc2,Defending Person Detection Against Adversarial Patch Attack by using Universal Defensive Frame,0.23342,"Person detection has attracted great attention in the computer vision area
and is an imperative element in human-centric computer vision. Although the
predictive performances of person detection networks have been improved
dramatically, they are vulnerable to adversarial patch attacks. Changing the
pixels in a restricted region can easily fool the person detection network in
safety-critical applications such as autonomous driving and security systems.
Despite the necessity of countering adversarial patch attacks, very few efforts
have been dedicated to defending person detection against adversarial patch
attack. In this paper, we propose a novel defense strategy that defends against
an adversarial patch attack by optimizing a defensive frame for person
detection. The defensive frame alleviates the effect of the adversarial patch
while maintaining person detection performance with clean person. The proposed
defensive frame in the person detection is generated with a competitive
learning algorithm which makes an iterative competition between detection
threatening module and detection shielding module in person detection.
Comprehensive experimental results demonstrate that the proposed method
effectively defends person detection against adversarial patch attacks.",https://github.com/UMBCvision/Contextual-Adversarial-Patches,-1
763a7072-d35f-4b46-a855-b8df900b48d6,Massively Digitized Power Grid: Opportunities and Challenges of Use-inspired AI,0.721697,"This article presents a use-inspired perspective of the opportunities and
challenges in a massively digitized power grid. It argues that the intricate
interplay of data availability, computing capability, and artificial
intelligence (AI) algorithm development are the three key factors driving the
adoption of digitized solutions in the power grid. The impact of these three
factors on critical functions of power system operation and planning practices
are reviewed and illustrated with industrial practice case studies. Open
challenges and research opportunities for data, computing, and AI algorithms
are articulated within the context of the power industry's tremendous
decarbonization efforts.",None,-1
5b5d6798-0d92-4ded-bdd3-9c2492f10096,SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues,0.783454,"Dialogue systems are usually categorized into two types, open-domain and
task-oriented. The first one focuses on chatting with users and making them
engage in the conversations, where selecting a proper topic to fit the dialogue
context is essential for a successful dialogue. The other one focuses on a
specific task instead of casual talks, e.g., finding a movie on Friday night,
or playing a song. These two directions have been studied separately due to
their different purposes. However, how smoothly transitioning from social
chatting to task-oriented dialogues is important for triggering business
opportunities, and there is no public data focusing on such scenarios. Hence,
this paper focuses on investigating the conversations starting from open-domain
social chatting and then gradually transitioning to task-oriented purposes, and
releases a large-scale dataset with detailed annotations for encouraging this
research direction. To achieve this goal, this paper proposes a framework to
automatically generate many dialogues without human involvement, in which any
powerful open-domain dialogue generation model can be easily leveraged. The
human evaluation shows that our generated dialogue data has a natural flow at a
reasonable quality, showing that our released data has a great potential of
guiding future research directions and commercial activities. Furthermore, the
released models allow researchers to automatically generate unlimited dialogues
in the target scenarios, which can greatly benefit semi-supervised and
unsupervised approaches.",https://github.com/MiuLab/SalesBot,-1
c4d734b8-da77-4bbb-a637-5b5223200a12,Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings with Multivariate Occupancy Time Series,0.607655,"In this work, we address the problem of unsupervised moving object
segmentation (MOS) in 4D LiDAR data recorded from a stationary sensor, where no
ground truth annotations are involved. Deep learning-based state-of-the-art
methods for LiDAR MOS strongly depend on annotated ground truth data, which is
expensive to obtain and scarce in existence. To close this gap in the
stationary setting, we propose a novel 4D LiDAR representation based on
multivariate time series that relaxes the problem of unsupervised MOS to a time
series clustering problem. More specifically, we propose modeling the change in
occupancy of a voxel by a multivariate occupancy time series (MOTS), which
captures spatio-temporal occupancy changes on the voxel level and its
surrounding neighborhood. To perform unsupervised MOS, we train a neural
network in a self-supervised manner to encode MOTS into voxel-level feature
representations, which can be partitioned by a clustering algorithm into moving
or stationary. Experiments on stationary scenes from the Raw KITTI dataset show
that our fully unsupervised approach achieves performance that is comparable to
that of supervised state-of-the-art approaches.",https://github.com/thkreutz/umosmots,-1
ecabdc66-f64b-4b1c-ac58-9ce4a0386771,Discovering Transferable Forensic Features for CNN-generated Images Detection,0.727816,"Visual counterfeits are increasingly causing an existential conundrum in
mainstream media with rapid evolution in neural image synthesis methods. Though
detection of such counterfeits has been a taxing problem in the image forensics
community, a recent class of forensic detectors -- universal detectors -- are
able to surprisingly spot counterfeit images regardless of generator
architectures, loss functions, training datasets, and resolutions. This
intriguing property suggests the possible existence of transferable forensic
features (T-FF) in universal detectors. In this work, we conduct the first
analytical study to discover and understand T-FF in universal detectors. Our
contributions are 2-fold: 1) We propose a novel forensic feature relevance
statistic (FF-RS) to quantify and discover T-FF in universal detectors and, 2)
Our qualitative and quantitative investigations uncover an unexpected finding:
color is a critical T-FF in universal detectors. Code and models are available
at https://keshik6.github.io/transferable-forensic-features/",None,-1
816aaa49-f9a9-4f0d-99ea-2bd49868ff46,Incorporating Constituent Syntax for Coreference Resolution,0.0535564,"Syntax has been shown to benefit Coreference Resolution from incorporating
long-range dependencies and structured information captured by syntax trees,
either in traditional statistical machine learning based systems or recently
proposed neural models. However, most leading systems use only dependency
trees. We argue that constituent trees also encode important information, such
as explicit span-boundary signals captured by nested multi-word phrases, extra
linguistic labels and hierarchical structures useful for detecting anaphora. In
this work, we propose a simple yet effective graph-based method to incorporate
constituent syntactic structures. Moreover, we also explore to utilise
higher-order neighbourhood information to encode rich structures in constituent
trees. A novel message propagation mechanism is therefore proposed to enable
information flow among elements in syntax trees. Experiments on the English and
Chinese portions of OntoNotes 5.0 benchmark show that our proposed model either
beats a strong baseline or achieves new state-of-the-art performance. (Code is
available at https://github.com/Fantabulous-J/Coref-Constituent-Graph)",https://github.com/Fantabulous-J/Coref-Constituent-Graph,-1
5ebe8324-2625-42ac-8465-8930b761ccf2,HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks,0.534684,"Implicit neural representations (INRs) are a rapidly growing research field,
which provides alternative ways to represent multimedia signals. Recent
applications of INRs include image super-resolution, compression of
high-dimensional signals, or 3D rendering. However, these solutions usually
focus on visual data, and adapting them to the audio domain is not trivial.
Moreover, it requires a separately trained model for every data sample. To
address this limitation, we propose HyperSound, a meta-learning method
leveraging hypernetworks to produce INRs for audio signals unseen at training
time. We show that our approach can reconstruct sound waves with quality
comparable to other state-of-the-art models.",None,-1
021d1f74-3bdc-405f-ae3c-54e91fe348d8,The Role of ImageNet Classes in Frchet Inception Distance,0.99988,"Fr\'echet Inception Distance (FID) is the primary metric for ranking models
in data-driven generative modeling. While remarkably successful, the metric is
known to sometimes disagree with human judgement. We investigate a root cause
of these discrepancies, and visualize what FID ""looks at"" in generated images.
We show that the feature space that FID is (typically) computed in is so close
to the ImageNet classifications that aligning the histograms of Top-$N$
classifications between sets of generated and real images can reduce FID
substantially -- without actually improving the quality of results. Thus, we
conclude that FID is prone to intentional or accidental distortions. As a
practical example of an accidental distortion, we discuss a case where an
ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while
being worse in terms of human evaluation.",https://github.com/kynkaat/role-of-imagenet-classes-in-fid,44899
88eb2437-41a8-426e-840f-7d31d07b9e8e,All you need is feedback: Communication with block attention feedback codes,0.398667,"Deep learning based channel code designs have recently gained interest as an
alternative to conventional coding algorithms, particularly for channels for
which existing codes do not provide effective solutions. Communication over a
feedback channel is one such problem, for which promising results have recently
been obtained by employing various deep learning architectures. In this paper,
we introduce a novel learning-aided code design for feedback channels, called
generalized block attention feedback (GBAF) codes, which i) employs a modular
architecture that can be implemented using different neural network
architectures; ii) provides order-of-magnitude improvements in the probability
of error compared to existing designs; and iii) can transmit at desired code
rates.",None,-1
9c46af7f-1e02-4065-8d0a-d4b2b26015b2,Optimizing Partial Area Under the Top-k Curve: Theory and Practice,0.356966,"Top-k error has become a popular metric for large-scale classification
benchmarks due to the inevitable semantic ambiguity among classes. Existing
literature on top-k optimization generally focuses on the optimization method
of the top-k objective, while ignoring the limitations of the metric itself. In
this paper, we point out that the top-k objective lacks enough discrimination
such that the induced predictions may give a totally irrelevant label a top
rank. To fix this issue, we develop a novel metric named partial Area Under the
top-k Curve (AUTKC). Theoretical analysis shows that AUTKC has a better
discrimination ability, and its Bayes optimal score function could give a
correct top-K ranking with respect to the conditional probability. This shows
that AUTKC does not allow irrelevant labels to appear in the top list.
Furthermore, we present an empirical surrogate risk minimization framework to
optimize the proposed metric. Theoretically, we present (1) a sufficient
condition for Fisher consistency of the Bayes optimal score function; (2) a
generalization upper bound which is insensitive to the number of classes under
a simple hyperparameter setting. Finally, the experimental results on four
benchmark datasets validate the effectiveness of our proposed framework.",https://github.com/CSAILVision/places365,-1
2fb66db1-8de3-4f9c-941f-c9c0cdc80785,Near-Term Advances in Quantum Natural Language Processing,0.148897,"This paper describes experiments showing that some tasks in natural language
processing (NLP) can already be performed using quantum computers, though so
far only with small datasets.
  We demonstrate various approaches to topic classification. The first uses an
explicit word-based approach, in which word-topic scoring weights are
implemented as fractional rotations of individual qubit, and a new phrase is
classified based on the accumulation of these weights in a scoring qubit using
entangling controlled-NOT gates. This is compared with more scalable quantum
encodings of word embedding vectors, which are used in the computation of
kernel values in a quantum support vector machine: this approach achieved an
average of 62% accuracy on classification tasks involving over 10000 words,
which is the largest such quantum computing experiment to date.
  We describe a quantum probability approach to bigram modeling that can be
applied to sequences of words and formal concepts, investigating a generative
approximation to these distributions using a quantum circuit Born machine, and
an approach to ambiguity resolution in verb-noun composition using single-qubit
rotations for simple nouns and 2-qubit controlled-NOT gates for simple verbs.
  The smaller systems described have been run successfully on physical quantum
computers, and the larger ones have been simulated. We show that statistically
meaningful results can be obtained using real datasets, but this is much more
difficult to predict than with easier artificial language examples used
previously in developing quantum NLP systems.
  Other approaches to quantum NLP are compared, partly with respect to
contemporary issues including informal language, fluency, and truthfulness.",None,4087
8dc6d695-5266-4b94-90fa-cc9508e500ad,ManiFlow: Implicitly Representing Manifolds with Normalizing Flows,0.104979,"Normalizing Flows (NFs) are flexible explicit generative models that have
been shown to accurately model complex real-world data distributions. However,
their invertibility constraint imposes limitations on data distributions that
reside on lower dimensional manifolds embedded in higher dimensional space.
Practically, this shortcoming is often bypassed by adding noise to the data
which impacts the quality of the generated samples. In contrast to prior work,
we approach this problem by generating samples from the original data
distribution given full knowledge about the perturbed distribution and the
noise model. To this end, we establish that NFs trained on perturbed data
implicitly represent the manifold in regions of maximum likelihood. Then, we
propose an optimization objective that recovers the most likely point on the
manifold given a sample from the perturbed distribution. Finally, we focus on
3D point clouds for which we utilize the explicit nature of NFs, i.e. surface
normals extracted from the gradient of the log-likelihood and the
log-likelihood itself, to apply Poisson surface reconstruction to refine
generated point sets.",None,-1
346efdbf-8b34-4ad2-ab2e-988277de99f6,Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification,0.547919,"Transformer has been widely used in histopathology whole slide image (WSI)
classification for the purpose of tumor grading, prognosis analysis, etc.
However, the design of token-wise self-attention and positional embedding
strategy in the common Transformer limits the effectiveness and efficiency in
the application to gigapixel histopathology images. In this paper, we propose a
kernel attention Transformer (KAT) for histopathology WSI classification. The
information transmission of the tokens is achieved by cross-attention between
the tokens and a set of kernels related to a set of positional anchors on the
WSI. Compared to the common Transformer structure, the proposed KAT can better
describe the hierarchical context information of the local regions of the WSI
and meanwhile maintains a lower computational complexity. The proposed method
was evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset
with 2560 WSIs, and was compared with 6 state-of-the-art methods. The
experimental results have demonstrated the proposed KAT is effective and
efficient in the task of histopathology WSI classification and is superior to
the state-of-the-art methods. The code is available at
https://github.com/zhengyushan/kat.",https://github.com/zhengyushan/kat,-1
7ae58a86-14dd-4af0-8ab5-877bb86e67ce,Empowering Graph Representation Learning with Test-Time Graph Transformation,0.967751,"As powerful tools for representation learning on graphs, graph neural
networks (GNNs) have facilitated various applications from drug discovery to
recommender systems. Nevertheless, the effectiveness of GNNs is immensely
challenged by issues related to data quality, such as distribution shift,
abnormal features and adversarial attacks. Recent efforts have been made on
tackling these issues from a modeling perspective which requires additional
cost of changing model architectures or re-training model parameters. In this
work, we provide a data-centric view to tackle these issues and propose a graph
transformation framework named GTrans which adapts and refines graph data at
test time to achieve better performance. We provide theoretical analysis on the
design of the framework and discuss why adapting graph data works better than
adapting the model. Extensive experiments have demonstrated the effectiveness
of GTrans on three distinct scenarios for eight benchmark datasets where
suboptimal data is presented. Remarkably, GTrans performs the best in most
cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on
three experimental settings. Code is released at
https://github.com/ChandlerBang/GTrans.",https://github.com/ChandlerBang/GTrans,-1
f1503f99-21b9-470f-b2ce-7f5b28295f50,Robust Domain Adaptation for Pre-trained Multilingual Neural Machine Translation Models,0.129157,"Recent literature has demonstrated the potential of multilingual Neural
Machine Translation (mNMT) models. However, the most efficient models are not
well suited to specialized industries. In these cases, internal data is scarce
and expensive to find in all language pairs. Therefore, fine-tuning a mNMT
model on a specialized domain is hard. In this context, we decided to focus on
a new task: Domain Adaptation of a pre-trained mNMT model on a single pair of
language while trying to maintain model quality on generic domain data for all
language pairs. The risk of loss on generic domain and on other pairs is high.
This task is key for mNMT model adoption in the industry and is at the border
of many others. We propose a fine-tuning procedure for the generic mNMT that
combines embeddings freezing and adversarial loss. Our experiments demonstrated
that the procedure improves performances on specialized data with a minimal
loss in initial performances on generic domain for all languages pairs,
compared to a naive standard approach (+10.0 BLEU score on specialized data,
-0.01 to -0.5 BLEU on WMT and Tatoeba datasets on the other pairs with M2M100).",None,-1
6db1947f-ce96-49af-a5a8-b88805c1139f,Towards Equalised Odds as Fairness Metric in Academic Performance Prediction,0.570769,"The literature for fairness-aware machine learning knows a plethora of
different fairness notions. It is however wellknown, that it is impossible to
satisfy all of them, as certain notions contradict each other. In this paper,
we take a closer look at academic performance prediction (APP) systems and try
to distil which fairness notions suit this task most. For this, we scan recent
literature proposing guidelines as to which fairness notion to use and apply
these guidelines onto APP. Our findings suggest equalised odds as most suitable
notion for APP, based on APP's WYSIWYG worldview as well as potential long-term
improvements for the population.",None,-1
1c3ce445-c42a-4362-aa14-f75014e56522,ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations,0.70853,"Precise representations of 3D faces are beneficial to various computer vision
and graphics applications. Due to the data discretization and model linearity,
however, it remains challenging to capture accurate identity and expression
clues in current studies. This paper presents a novel 3D morphable face model,
namely ImFace, to learn a nonlinear and continuous space with implicit neural
representations. It builds two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
and designs an improved learning strategy to extend embeddings of expressions
to allow more diverse changes. We further introduce a Neural Blend-Field to
learn sophisticated details by adaptively blending a series of local fields. In
addition to ImFace, an effective preprocessing pipeline is proposed to address
the issue of watertight input requirement in implicit representations, enabling
them to work with common facial surfaces for the first time. Extensive
experiments are performed to demonstrate the superiority of ImFace.",https://github.com/MingwuZheng/ImFace,11135
c9090333-146a-410c-bef3-4e2b171b14f6,Sky Computing: Accelerating Geo-distributed Computing in Federated Learning,0.0582988,"Federated learning is proposed by Google to safeguard data privacy through
training models locally on users' devices. However, with deep learning models
growing in size to achieve better results, it becomes increasingly difficult to
accommodate the whole model on one single device. Thus, model parallelism is
then used to divide the model weights among several devices. With this logic,
the approach currently used evenly allocates weights among devices. However, in
reality, a computation bottleneck may occur resulting from variant computing
power of different users' devices. To address this problem, load balancing is
needed to allocate the model weights based on the computational capability of
the device. In this paper, we proposed Sky Computing, a load-balanced model
parallelism framework to adaptively allocate the weights to devices. Sky
Computing outperforms the baseline method by 55% in training time when training
160-layer BERT with 64 nodes. The source code can be found at
https://github.com/hpcaitech/SkyComputing.",https://github.com/hpcaitech/SkyComputing,-1
66f76036-5f69-4cad-bf84-124de69b2982,Finding and Listing Front-door Adjustment Sets,0.748572,"Identifying the effects of new interventions from data is a significant
challenge found across a wide range of the empirical sciences. A well-known
strategy for identifying such effects is Pearl's front-door (FD) criterion
(Pearl, 1995). The definition of the FD criterion is declarative, only allowing
one to decide whether a specific set satisfies the criterion. In this paper, we
present algorithms for finding and enumerating possible sets satisfying the FD
criterion in a given causal diagram. These results are useful in facilitating
the practical applications of the FD criterion for causal effects estimation
and helping scientists to select estimands with desired properties, e.g., based
on cost, feasibility of measurement, or statistical power.",https://github.com/CausalAILab/FrontdoorAdjustmentSets,-1
2334ecf9-078a-4b1d-8424-2ab241af1075,Semiconductor Defect Detection by Hybrid Classical-Quantum Deep Learning,0.934265,"With the rapid development of artificial intelligence and autonomous driving
technology, the demand for semiconductors is projected to rise substantially.
However, the massive expansion of semiconductor manufacturing and the
development of new technology will bring many defect wafers. If these defect
wafers have not been correctly inspected, the ineffective semiconductor
processing on these defect wafers will cause additional impact to our
environment, such as excessive carbon dioxide emission and energy consumption.
In this paper, we utilize the information processing advantages of quantum
computing to promote the defect learning defect review (DLDR). We propose a
classical-quantum hybrid algorithm for deep learning on near-term quantum
processors. By tuning parameters implemented on it, quantum circuit driven by
our framework learns a given DLDR task, include of wafer defect map
classification, defect pattern classification, and hotspot detection. In
addition, we explore parametrized quantum circuits with different
expressibility and entangling capacities. These results can be used to build a
future roadmap to develop circuit-based quantum deep learning for semiconductor
defect detection.",None,10320
ad7f95ad-86fe-4eaa-a488-b6ba088febca,PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence,0.432169,"Work on social media rumour verification utilises signals from posts, their
propagation and users involved. Other lines of work target identifying and
fact-checking claims based on information from Wikipedia, or trustworthy news
articles without considering social media context. However works combining the
information from social media with external evidence from the wider web are
lacking. To facilitate research in this direction, we release a novel dataset,
PHEMEPlus, an extension of the PHEME benchmark, which contains social media
conversations as well as relevant external evidence for each rumour. We
demonstrate the effectiveness of incorporating such evidence in improving
rumour verification models. Additionally, as part of the evidence collection,
we evaluate various ways of query formulation to identify the most effective
method.",https://github.com/JohnNLP/PhemePlus,-1
63c95fdc-2bbd-474d-8235-240f1ac2c87c,Learning Uncertainty with Artificial Neural Networks for Improved Predictive Process Monitoring,0.513304,"The inability of artificial neural networks to assess the uncertainty of
their predictions is an impediment to their widespread use. We distinguish two
types of learnable uncertainty: model uncertainty due to a lack of training
data and noise-induced observational uncertainty. Bayesian neural networks use
solid mathematical foundations to learn the model uncertainties of their
predictions. The observational uncertainty can be calculated by adding one
layer to these networks and augmenting their loss functions. Our contribution
is to apply these uncertainty concepts to predictive process monitoring tasks
to train uncertainty-based models to predict the remaining time and outcomes.
Our experiments show that uncertainty estimates allow more and less accurate
predictions to be differentiated and confidence intervals to be constructed in
both regression and classification tasks. These conclusions remain true even in
early stages of running processes. Moreover, the deployed techniques are fast
and produce more accurate predictions. The learned uncertainty could increase
users' confidence in their process prediction systems, promote better
cooperation between humans and these systems, and enable earlier
implementations with smaller datasets.",None,-1
98db860c-9885-4c3e-b0ca-2efd2df384fe,In-BoXBART: Get Instructions into Biomedical Multi-Task Learning,0.402515,"Single-task models have proven pivotal in solving specific tasks; however,
they have limitations in real-world applications where multi-tasking is
necessary and domain shifts are exhibited. Recently, instructional prompts have
shown significant improvement towards multi-task generalization; however, the
effect of instructional prompts and Multi-Task Learning (MTL) has not been
systematically studied in the biomedical domain. Motivated by this, this paper
explores the impact of instructional prompts for biomedical MTL. We introduce
the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)
various categories. Using this meta-dataset, we propose a unified model termed
In-BoXBART, that can jointly learn all tasks of the BoX without any
task-specific modules. To the best of our knowledge, this is the first attempt
to propose a unified model in the biomedical domain and use instructions to
achieve generalization across several biomedical tasks. Experimental results
indicate that the proposed model: 1) outperforms the single-task baseline by
~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)
shows ~23% improvement compared to the single-task baseline in few-shot
learning (i.e., 32 instances per task) on an average. Our analysis indicates
that there is significant room for improvement across tasks in the BoX,
implying the scope for future research direction.",https://github.com/Mihir3009/In-BoXBART,-1
5b2814ec-8541-4418-ab2c-939f29266d26,Sparse Adversarial Attack in Multi-agent Reinforcement Learning,0.420824,"Cooperative multi-agent reinforcement learning (cMARL) has many real
applications, but the policy trained by existing cMARL algorithms is not robust
enough when deployed. There exist also many methods about adversarial attacks
on the RL system, which implies that the RL system can suffer from adversarial
attacks, but most of them focused on single agent RL. In this paper, we propose
a \textit{sparse adversarial attack} on cMARL systems. We use (MA)RL with
regularization to train the attack policy. Our experiments show that the policy
trained by the current cMARL algorithm can obtain poor performance when only
one or a few agents in the team (e.g., 1 of 8 or 5 of 25) were attacked at a
few timesteps (e.g., attack 3 of total 40 timesteps).",None,-1
746c8f10-3bfd-4bb1-a4f6-b885c60be706,Distributed Control using Reinforcement Learning with Temporal-Logic-Based Reward Shaping,0.18831,"We present a computational framework for synthesis of distributed control
strategies for a heterogeneous team of robots in a partially observable
environment. The goal is to cooperatively satisfy specifications given as
Truncated Linear Temporal Logic (TLTL) formulas. Our approach formulates the
synthesis problem as a stochastic game and employs a policy graph method to
find a control strategy with memory for each agent. We construct the stochastic
game on the product between the team transition system and a finite state
automaton (FSA) that tracks the satisfaction of the TLTL formula. We use the
quantitative semantics of TLTL as the reward of the game, and further reshape
it using the FSA to guide and accelerate the learning process. Simulation
results demonstrate the efficacy of the proposed solution under demanding task
specifications and the effectiveness of reward shaping in significantly
accelerating the speed of learning.",None,-1
4bf45bdb-f455-4152-b5ce-803fb1d5cfd1,"CounterGeDi: A controllable approach to generate polite, detoxified and emotional counterspeech",0.787466,"Recently, many studies have tried to create generation models to assist
counter speakers by providing counterspeech suggestions for combating the
explosive proliferation of online hate. However, since these suggestions are
from a vanilla generation model, they might not include the appropriate
properties required to counter a particular hate speech instance. In this
paper, we propose CounterGeDi - an ensemble of generative discriminators (GeDi)
to guide the generation of a DialoGPT model toward more polite, detoxified, and
emotionally laden counterspeech. We generate counterspeech using three datasets
and observe significant improvement across different attribute scores. The
politeness and detoxification scores increased by around 15% and 6%
respectively, while the emotion in the counterspeech increased by at least 10%
across all the datasets. We also experiment with triple-attribute control and
observe significant improvement over single attribute results when combining
complementing attributes, e.g., politeness, joyfulness and detoxification. In
all these experiments, the relevancy of the generated text does not deteriorate
due to the application of these controls",https://github.com/hate-alert/CounterGEDI,-1
36aef0d9-4624-4f1e-a8a3-0b4ac36f3323,Generation-Augmented Query Expansion For Code Retrieval,0.327469,"Pre-trained language models have achieved promising success in code retrieval
tasks, where a natural language documentation query is given to find the most
relevant existing code snippet. However, existing models focus only on
optimizing the documentation code pairs by embedding them into latent space,
without the association of external knowledge. In this paper, we propose a
generation-augmented query expansion framework. Inspired by the human retrieval
process - sketching an answer before searching, in this work, we utilize the
powerful code generation model to benefit the code retrieval task.
Specifically, we demonstrate that rather than merely retrieving the target code
snippet according to the documentation query, it would be helpful to augment
the documentation query with its generation counterpart - generated code
snippets from the code generation model. To the best of our knowledge, this is
the first attempt that leverages the code generation model to enhance the code
retrieval task. We achieve new state-of-the-art results on the CodeSearchNet
benchmark and surpass the baselines significantly.",https://github.com/kingoflolz/mesh-transformer-jax,19975
398bc730-b745-4fd7-901c-b1a53af085a7,A Capability and Skill Model for Heterogeneous Autonomous Robots,0.204726,"Teams of heterogeneous autonomous robots become increasingly important due to
their facilitation of various complex tasks. For such heterogeneous robots,
there is currently no consistent way of describing the functions that each
robot provides. In the field of manufacturing, capability modeling is
considered a promising approach to semantically model functions provided by
different machines. This contribution investigates how to apply and extend
capability models from manufacturing to the field of autonomous robots and
presents an approach for such a capability model.",https://github.com/Miguel2617/robocap,-1
0b03ebc8-0c60-4eda-848e-d0210badf4f4,Exploring Diversity-based Active Learning for 3D Object Detection in Autonomous Driving,0.443109,"3D object detection has recently received much attention due to its great
potential in autonomous vehicle (AV). The success of deep learning based object
detectors relies on the availability of large-scale annotated datasets, which
is time-consuming and expensive to compile, especially for 3D bounding box
annotation. In this work, we investigate diversity-based active learning (AL)
as a potential solution to alleviate the annotation burden. Given limited
annotation budget, only the most informative frames and objects are
automatically selected for human to annotate. Technically, we take the
advantage of the multimodal information provided in an AV dataset, and propose
a novel acquisition function that enforces spatial and temporal diversity in
the selected samples. We benchmark the proposed method against other AL
strategies under realistic annotation cost measurement, where the realistic
costs for annotating a frame and a 3D bounding box are both taken into
consideration. We demonstrate the effectiveness of the proposed method on the
nuScenes dataset and show that it outperforms existing AL strategies
significantly.",https://github.com/poodarchu/Det3D,1317
52a3a832-e733-4206-b275-073f1c218445,OpenScene: 3D Scene Understanding with Open Vocabularies,0.999086,"Traditional 3D scene understanding approaches rely on labeled 3D datasets to
train a model for a single task with supervision. We propose OpenScene, an
alternative approach where a model predicts dense features for 3D scene points
that are co-embedded with text and image pixels in CLIP feature space. This
zero-shot approach enables task-agnostic training and open-vocabulary queries.
For example, to perform SOTA zero-shot 3D semantic segmentation it first infers
CLIP features for every 3D point and later classifies them based on
similarities to embeddings of arbitrary class labels. More interestingly, it
enables a suite of open-vocabulary scene understanding applications that have
never been done before. For example, it allows a user to enter an arbitrary
text query and then see a heat map indicating which parts of a scene match. Our
approach is effective at identifying objects, materials, affordances,
activities, and room types in complex 3D scenes, all using a single model
trained without any labeled 3D data.",https://github.com/mseg-dataset/mseg-semantic,-1
2d65d3c3-9cc1-484e-b3ae-71d1d5174349,A Length-Extrapolatable Transformer,0.997461,"Position modeling plays a critical role in Transformers. In this paper, we
focus on length extrapolation, i.e., training on short texts while evaluating
longer sequences. We define attention resolution as an indicator of
extrapolation. Then we propose two designs to improve the above metric of
Transformers. Specifically, we introduce a relative position embedding to
explicitly maximize attention resolution. Moreover, we use blockwise causal
attention during inference for better resolution. We evaluate different
Transformer variants with language modeling. Experimental results show that our
model achieves strong performance in both interpolation and extrapolation
settings. The code will be available at https://aka.ms/LeX-Transformer.",https://github.com/microsoft/torchscale,-1
10ddba2b-36d5-4ac3-a960-c9539d0c56d1,ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval,0.959352,"Neural retrievers based on pre-trained language models (PLMs), such as
dual-encoders, have achieved promising performance on the task of open-domain
question answering (QA). Their effectiveness can further reach new
state-of-the-arts by incorporating cross-architecture knowledge distillation.
However, most of the existing studies just directly apply conventional
distillation methods. They fail to consider the particular situation where the
teacher and student have different structures. In this paper, we propose a
novel distillation method that significantly advances cross-architecture
distillation for dual-encoders. Our method 1) introduces a self on-the-fly
distillation method that can effectively distill late interaction (i.e.,
ColBERT) to vanilla dual-encoder, and 2) incorporates a cascade distillation
process to further improve the performance with a cross-encoder teacher.
Extensive experiments are conducted to validate that our proposed solution
outperforms strong baselines and establish a new state-of-the-art on
open-domain QA benchmarks.",None,15024
ad9adfca-77ca-45b5-b65e-d221f9727fba,EAutoDet: Efficient Architecture Search for Object Detection,0.58587,"Training CNN for detection is time-consuming due to the large dataset and
complex network modules, making it hard to search architectures on detection
datasets directly, which usually requires vast search costs (usually tens and
even hundreds of GPU-days). In contrast, this paper introduces an efficient
framework, named EAutoDet, that can discover practical backbone and FPN
architectures for object detection in 1.4 GPU-days. Specifically, we construct
a supernet for both backbone and FPN modules and adopt the differentiable
method. To reduce the GPU memory requirement and computational cost, we propose
a kernel reusing technique by sharing the weights of candidate operations on
one edge and consolidating them into one convolution. A dynamic channel
refinement strategy is also introduced to search channel numbers. Extensive
experiments show significant efficacy and efficiency of our method. In
particular, the discovered architectures surpass state-of-the-art object
detection NAS methods and achieve 40.1 mAP with 120 FPS and 49.2 mAP with 41.3
FPS on COCO test-dev set. We also transfer the discovered architectures to
rotation detection task, which achieve 77.05 mAP$_{\text{50}}$ on DOTA-v1.0
test set with 21.1M parameters.",None,-1
8d7bf0d5-651f-4fde-9da8-19d4b55a2f8f,Correspondence Distillation from NeRF-based GAN,0.313276,"The neural radiance field (NeRF) has shown promising results in preserving
the fine details of objects and scenes. However, unlike mesh-based
representations, it remains an open problem to build dense correspondences
across different NeRFs of the same category, which is essential in many
downstream tasks. The main difficulties of this problem lie in the implicit
nature of NeRF and the lack of ground-truth correspondence annotations. In this
paper, we show it is possible to bypass these challenges by leveraging the rich
semantics and structural priors encapsulated in a pre-trained NeRF-based GAN.
Specifically, we exploit such priors from three aspects, namely 1) a dual
deformation field that takes latent codes as global structural indicators, 2) a
learning objective that regards generator features as geometric-aware local
descriptors, and 3) a source of infinite object-specific NeRF samples. Our
experiments demonstrate that such priors lead to 3D dense correspondence that
is accurate, smooth, and robust. We also show that established dense
correspondence across NeRFs can effectively enable many NeRF-based downstream
applications such as texture transfer.",None,-1
544b9da8-776a-4762-8388-f3ef44a63d52,Prompt-based Generative Approach towards Multi-Hierarchical Medical Dialogue State Tracking,0.167962,"The medical dialogue system is a promising application that can provide great
convenience for patients. The dialogue state tracking (DST) module in the
medical dialogue system which interprets utterances into the machine-readable
structure for downstream tasks is particularly challenging. Firstly, the states
need to be able to represent compound entities such as symptoms with their body
part or diseases with degrees of severity to provide enough information for
decision support. Secondly, these named entities in the utterance might be
discontinuous and scattered across sentences and speakers. These also make it
difficult to annotate a large corpus which is essential for most methods.
Therefore, we first define a multi-hierarchical state structure. We annotate
and publish a medical dialogue dataset in Chinese. To the best of our
knowledge, there are no publicly available ones before. Then we propose a
Prompt-based Generative Approach which can generate slot values with
multi-hierarchies incrementally using a top-down approach. A dialogue style
prompt is also supplemented to utilize the large unlabeled dialogue corpus to
alleviate the data scarcity problem. The experiments show that our approach
outperforms other DST methods and is rather effective in the scenario with
little data.",None,-1
087c677f-6b89-42fa-a90c-99c6b2f09c64,Dreamento: an open-source dream engineering toolbox for sleep EEG wearables,0.468914,"We introduce Dreamento (Dream engineering toolbox), an open-source Python
package for dream engineering using sleep electroencephalography (EEG)
wearables. Dreamento main functions are (1) real-time recording, monitoring,
analysis, and sensory stimulation, and (2) offline post-processing of the
resulting data, both in a graphical user interface (GUI). In real-time,
Dreamento is capable of (1) data recording, visualization, and navigation, (2)
power-spectrum analysis, (3) automatic sleep scoring, (4) sensory stimulation
(visual, auditory, tactile), (5) establishing text-to-speech communication, and
(6) managing annotations of automatic and manual events. The offline functions
aid in post-processing the acquired data with features to reformat the wearable
data and integrate it with non-wearable recorded modalities such as
electromyography (EMG). While Dreamento was primarily developed for (lucid)
dreaming studies, its applications can be extended to other areas of sleep
research such as closed-loop auditory stimulation and targeted memory
reactivation.",https://github.com/dreamento/dreamento,7176
a358b8e0-5685-43ec-ab1c-462d36d51d24,Optimizing Elimination Templates by Greedy Parameter Search,0.522688,"We propose a new method for constructing elimination templates for efficient
polynomial system solving of minimal problems in structure from motion, image
matching, and camera tracking. We first construct a particular affine
parameterization of the elimination templates for systems with a finite number
of distinct solutions. Then, we use a heuristic greedy optimization strategy
over the space of parameters to get a template with a small size. We test our
method on 34 minimal problems in computer vision. For all of them, we found the
templates either of the same or smaller size compared to the state-of-the-art.
For some difficult examples, our templates are, e.g., 2.1, 2.5, 3.8, 6.6 times
smaller. For the problem of refractive absolute pose estimation with unknown
focal length, we have found a template that is 20 times smaller. Our
experiments on synthetic data also show that the new solvers are fast and
numerically accurate. We also present a fast and numerically accurate solver
for the problem of relative pose estimation with unknown common focal length
and radial distortion.",http://github.com/martyushev/EliminationTemplates,-1
4d5e6337-6ee1-439d-843e-015113b01080,NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers,0.63117,"The complicated architecture and high training cost of vision transformers
urge the exploration of post-training quantization. However, the heavy-tailed
distribution of vision transformer activations hinders the effectiveness of
previous post-training quantization methods, even with advanced quantizer
designs. Instead of tuning the quantizer to better fit the complicated
activation distribution, this paper proposes NoisyQuant, a quantizer-agnostic
enhancement for the post-training activation quantization performance of vision
transformers. We make a surprising theoretical discovery that for a given
quantizer, adding a fixed Uniform noisy bias to the values being quantized can
significantly reduce the quantization error under provable conditions. Building
on the theoretical insight, NoisyQuant achieves the first success on actively
altering the heavy-tailed activation distribution with additive noisy bias to
fit a given quantizer. Extensive experiments show NoisyQuant largely improves
the post-training quantization performance of vision transformer with minimal
computation overhead. For instance, on linear uniform 6-bit activation
quantization, NoisyQuant improves SOTA top-1 accuracy on ImageNet by up to
1.7%, 1.1% and 0.5% for ViT, DeiT, and Swin Transformer respectively, achieving
on-par or even higher performance than previous nonlinear, mixed-precision
quantization.",https://github.com/rwightman/pytorch-image-models,-1
196d14bb-5d41-4cea-8c0f-275431c83cba,QuadTree Attention for Vision Transformers,0.766959,"Transformers have been successful in many vision tasks, thanks to their
capability of capturing long-range dependency. However, their quadratic
computational complexity poses a major obstacle for applying them to vision
tasks requiring dense predictions, such as object detection, feature matching,
stereo, etc. We introduce QuadTree Attention, which reduces the computational
complexity from quadratic to linear. Our quadtree transformer builds token
pyramids and computes attention in a coarse-to-fine manner. At each level, the
top K patches with the highest attention scores are selected, such that at the
next level, attention is only evaluated within the relevant regions
corresponding to these top K patches. We demonstrate that quadtree attention
achieves state-of-the-art performance in various vision tasks, e.g. with 4.0%
improvement in feature matching on ScanNet, about 50% flops reduction in stereo
matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification,
1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on
semantic segmentation over previous state-of-the-art transformers. The codes
are available at https://github.com/Tangshitao/QuadtreeAttention.",https://github.com/Tangshitao/QuadtreeAttention,-1
e9327491-01ad-4166-a6ce-35b6573aee0f,CD Tools -- Condensed Detachment and Structure Generating Theorem Proving (System Description),0.584615,"CD Tools is a Prolog library for experimenting with condensed detachment in
first-order ATP, which puts a recent formal view centered around proof
structures into practice. From the viewpoint of first-order ATP, condensed
detachment offers a setting that is relatively simple but with essential
features and serious applications, making it attractive as a basis for
developing and evaluating novel techniques. CD Tools includes specialized
provers based on the enumeration of proof structures. We focus here on one of
these, SGCD, which permits to blend goal- and axiom-driven proof search in
particularly flexible ways. In purely goal-driven configurations it acts
similarly to a prover of the clausal tableaux or connection method family. In
blended configurations its performance is much stronger, close to
state-of-the-art provers, while emitting relatively short proofs. Experiments
show characteristics and application possibilities of the structure generating
approach realized by that prover. For a historic problem often studied in ATP
it produced a new proof that is much shorter than any known one.",None,-1
318502a6-4c31-40ba-a2b7-39bd2ac36815,Learning to Adapt Domain Shifts of Moral Values via Instance Weighting,0.809831,"Classifying moral values in user-generated text from social media is critical
in understanding community cultures and interpreting user behaviors of social
movements. Moral values and language usage can change across the social
movements; however, text classifiers are usually trained in source domains of
existing social movements and tested in target domains of new social issues
without considering the variations. In this study, we examine domain shifts of
moral values and language usage, quantify the effects of domain shifts on the
morality classification task, and propose a neural adaptation framework via
instance weighting to improve cross-domain classification tasks. The
quantification analysis suggests a strong correlation between morality shifts,
language usage, and classification performance. We evaluate the neural
adaptation framework on a public Twitter data across 7 social movements and
gain classification improvements up to 12.1\%. Finally, we release a new data
of the COVID-19 vaccine labeled with moral values and evaluate our approach on
the new target domain. For the case study of the COVID-19 vaccine, our
adaptation framework achieves up to 5.26\% improvements over neural baselines.",None,12040
b9f19de7-3531-4172-8f94-29eb3a914970,Intent Contrastive Learning for Sequential Recommendation,0.967625,"Users' interactions with items are driven by various intents (e.g., preparing
for holiday gifts, shopping for fishing equipment, etc.).However, users'
underlying intents are often unobserved/latent, making it challenging to
leverage such latent intents forSequentialrecommendation(SR). To investigate
the benefits of latent intents and leverage them effectively for
recommendation, we proposeIntentContrastiveLearning(ICL), a general learning
paradigm that leverages a latent intent variable into SR. The core idea is to
learn users' intent distribution functions from unlabeled user behavior
sequences and optimize SR models with contrastive self-supervised learning
(SSL) by considering the learned intents to improve recommendation.
Specifically, we introduce a latent variable to represent users' intents and
learn the distribution function of the latent variable via clustering. We
propose to leverage the learned intents into SR models via contrastive SSL,
which maximizes the agreement between a view of sequence and its corresponding
intent. The training is alternated between intent representation learning and
the SR model optimization steps within the generalized expectation-maximization
(EM) framework. Fusing user intent information into SR also improves model
robustness. Experiments conducted on four real-world datasets demonstrate the
superiority of the proposed learning paradigm, which improves performance, and
robustness against data sparsity and noisy interaction issues.",https://github.com/salesforce/ICLRec,-1
42ba44df-2f26-4e5b-a673-7d6198503369,Kinematic-aware Hierarchical Attention Network for Human Pose Estimation in Videos,0.28078,"Previous video-based human pose estimation methods have shown promising
results by leveraging aggregated features of consecutive frames. However, most
approaches compromise accuracy to mitigate jitter or do not sufficiently
comprehend the temporal aspects of human motion. Furthermore, occlusion
increases uncertainty between consecutive frames, which results in unsmooth
results. To address these issues, we design an architecture that exploits the
keypoint kinematic features with the following components. First, we
effectively capture the temporal features by leveraging individual keypoint's
velocity and acceleration. Second, the proposed hierarchical transformer
encoder aggregates spatio-temporal dependencies and refines the 2D or 3D input
pose estimated from existing estimators. Finally, we provide an online
cross-supervision between the refined input pose generated from the encoder and
the final pose from our decoder to enable joint optimization. We demonstrate
comprehensive results and validate the effectiveness of our model in various
tasks: 2D pose estimation, 3D pose estimation, body mesh recovery, and sparsely
annotated multi-human pose estimation. Our code is available at
https://github.com/KyungMinJin/HANet.",https://github.com/KyungMinJin/HANet,-1
f8430f13-504e-4c01-a472-c963bc4cefd7,A Portable Multiscopic Camera for Novel View and Time Synthesis in Dynamic Scenes,0.0689087,"We present a portable multiscopic camera system with a dedicated model for
novel view and time synthesis in dynamic scenes. Our goal is to render
high-quality images for a dynamic scene from any viewpoint at any time using
our portable multiscopic camera. To achieve such novel view and time synthesis,
we develop a physical multiscopic camera equipped with five cameras to train a
neural radiance field (NeRF) in both time and spatial domains for dynamic
scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal
coordinate, and 2D viewing direction) to view-dependent and time-varying
emitted radiance and volume density. Volume rendering is applied to render a
photo-realistic image at a specified camera pose and time. To improve the
robustness of our physical camera, we propose a camera parameter optimization
module and a temporal frame interpolation module to promote information
propagation across time. We conduct experiments on both real-world and
synthetic datasets to evaluate our system, and the results show that our
approach outperforms alternative solutions qualitatively and quantitatively.
Our code and dataset are available at https://yuenfuilau.github.io.",https://github.com/avinashpaliwal/Super-SloMo,-1
3940a9db-9dd1-499b-94ab-101e0188241a,Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms,0.242476,"Learning in high dimensional continuous tasks is challenging, mainly when the
experience replay memory is very limited. We introduce a simple yet effective
experience sharing mechanism for deterministic policies in continuous action
domains for the future off-policy deep reinforcement learning applications in
which the allocated memory for the experience replay buffer is limited. To
overcome the extrapolation error induced by learning from other agents'
experiences, we facilitate our algorithm with a novel off-policy correction
technique without any action probability estimates. We test the effectiveness
of our method in challenging OpenAI Gym continuous control tasks and conclude
that it can achieve a safe experience sharing across multiple agents and
exhibits a robust performance when the replay memory is strictly limited.",https://github.com/baturaysaglam/DASE,644
2e70d0e2-b6fb-4f71-bf15-69b935990c5d,SfM-TTR: Using Structure from Motion for Test-Time Refinement of Single-View Depth Networks,0.0396631,"Estimating a dense depth map from a single view is geometrically ill-posed,
and state-of-the-art methods rely on learning depth's relation with visual
appearance using deep neural networks. On the other hand, Structure from Motion
(SfM) leverages multi-view constraints to produce very accurate but sparse
maps, as matching across images is typically limited by locally discriminative
texture. In this work, we combine the strengths of both approaches by proposing
a novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts the
performance of single-view depth networks at test time using SfM multi-view
cues. Specifically, and differently from the state of the art, we use sparse
SfM point clouds as test-time self-supervisory signal, fine-tuning the network
encoder to learn a better representation of the test scene. Our results show
how the addition of SfM-TTR to several state-of-the-art self-supervised and
supervised networks improves significantly their performance, outperforming
previous TTR baselines mainly based on photometric multi-view consistency. The
code is available at https://github.com/serizba/SfM-TTR.",https://github.com/serizba/SfM-TTR,-1
68725a5c-42b8-4356-9957-7a690eef58d9,"Deep Insights of Learning based Micro Expression Recognition: A Perspective on Promises, Challenges and Research Needs",0.547467,"Micro expression recognition (MER) is a very challenging area of research due
to its intrinsic nature and fine-grained changes. In the literature, the
problem of MER has been solved through handcrafted/descriptor-based techniques.
However, in recent times, deep learning (DL) based techniques have been adopted
to gain higher performance for MER. Also, rich survey articles on MER are
available by summarizing the datasets, experimental settings, conventional and
deep learning methods. In contrast, these studies lack the ability to convey
the impact of network design paradigms and experimental setting strategies for
DL-based MER. Therefore, this paper aims to provide a deep insight into the
DL-based MER frameworks with a perspective on promises in network model
designing, experimental strategies, challenges, and research needs. Also, the
detailed categorization of available MER frameworks is presented in various
aspects of model design and technical characteristics. Moreover, an empirical
analysis of the experimental and validation protocols adopted by MER methods is
presented. The challenges mentioned earlier and network design strategies may
assist the affective computing research community in forging ahead in MER
research. Finally, we point out the future directions, research needs, and draw
our conclusions.",None,-1
d628744c-100c-44ec-ba8e-fd4379d2bdf0,Practical Aspects of Zero-Shot Learning,0.0179586,"One of important areas of machine learning research is zero-shot learning. It
is applied when properly labeled training data set is not available. A number
of zero-shot algorithms have been proposed and experimented with. However, none
of them seems to be the ""overall winner"". In situations like this, it may be
possible to develop a meta-classifier that would combine ""best aspects"" of
individual classifiers and outperform all of them. In this context, the goal of
this contribution is twofold. First, multiple state-of-the-art zero-shot
learning methods are compared for standard benchmark datasets. Second, multiple
meta-classifiers are suggested and experimentally compared (for the same
datasets).",https://github.com/Inars/Developing-MC-for-ZSL,-1
0ac61a7b-db1f-4b83-b0d4-b006658d86af,Towards Disentangled Speech Representations,0.341046,"The careful construction of audio representations has become a dominant
feature in the design of approaches to many speech tasks. Increasingly, such
approaches have emphasized ""disentanglement"", where a representation contains
only parts of the speech signal relevant to transcription while discarding
irrelevant information. In this paper, we construct a representation learning
task based on joint modeling of ASR and TTS, and seek to learn a representation
of audio that disentangles that part of the speech signal that is relevant to
transcription from that part which is not. We present empirical evidence that
successfully finding such a representation is tied to the randomness inherent
in training. We then make the observation that these desired, disentangled
solutions to the optimization problem possess unique statistical properties.
Finally, we show that enforcing these properties during training improves WER
by 24.5% relative on average for our joint modeling task. These observations
motivate a novel approach to learning effective audio representations.",None,150035
a76afe98-9241-47b9-8112-3381bc4fa06b,Distilling a Pretrained Language Model to a Multilingual ASR Model,0.26337,"Multilingual speech data often suffer from long-tailed language distribution,
resulting in performance degradation. However, multilingual text data is much
easier to obtain, yielding a more useful general language model. Hence, we are
motivated to distill the rich knowledge embedded inside a well-trained teacher
text model to the student speech model. We propose a novel method called the
Distilling a Language model to a Speech model (Distill-L2S), which aligns the
latent representations of two different modalities. The subtle differences are
handled by the shrinking mechanism, nearest-neighbor interpolation, and a
learnable linear projection layer. We demonstrate the effectiveness of our
distillation method by applying it to the multilingual automatic speech
recognition (ASR) task. We distill the transformer-based cross-lingual language
model (InfoXLM) while fine-tuning the large-scale multilingual ASR model
(XLSR-wav2vec 2.0) for each language. We show the superiority of our method on
20 low-resource languages of the CommonVoice dataset with less than 100 hours
of speech data.",https://github.com/juice500ml/xlm_to_xlsr,-1
4966d118-6278-439a-b5cf-03f6da254ae1,Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind,0.934603,"When reading a story, humans can quickly understand new fictional characters
with a few observations, mainly by drawing analogies to fictional and real
people they already know. This reflects the few-shot and meta-learning essence
of humans' inference of characters' mental states, i.e., theory-of-mind (ToM),
which is largely ignored in existing research. We fill this gap with a novel
NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM
in a realistic narrative understanding scenario. Our dataset consists of ~1,000
parsed movie scripts, each corresponding to a few-shot character understanding
task that requires models to mimic humans' ability of fast digesting characters
with a few starting scenes in a new movie.
  We propose a novel ToM prompting approach designed to explicitly assess the
influence of multiple ToM dimensions. It surpasses existing baseline models,
underscoring the significance of modeling multiple ToM dimensions for our task.
Our extensive human study verifies that humans are capable of solving our
problem by inferring characters' mental states based on their previously seen
movies. In comparison, our systems based on either state-of-the-art large
language models (GPT-4) or meta-learning algorithms lags >20% behind,
highlighting a notable limitation in existing approaches' ToM capabilities.",None,-1
6fe57c31-14e2-4009-8b4b-a59b65436bc3,Are Face Detection Models Biased?,0.181839,"The presence of bias in deep models leads to unfair outcomes for certain
demographic subgroups. Research in bias focuses primarily on facial recognition
and attribute prediction with scarce emphasis on face detection. Existing
studies consider face detection as binary classification into 'face' and
'non-face' classes. In this work, we investigate possible bias in the domain of
face detection through facial region localization which is currently
unexplored. Since facial region localization is an essential task for all face
recognition pipelines, it is imperative to analyze the presence of such bias in
popular deep models. Most existing face detection datasets lack suitable
annotation for such analysis. Therefore, we web-curate the Fair Face
Localization with Attributes (F2LA) dataset and manually annotate more than 10
attributes per face, including facial localization information. Utilizing the
extensive annotations from F2LA, an experimental setup is designed to study the
performance of four pre-trained face detectors. We observe (i) a high disparity
in detection accuracies across gender and skin-tone, and (ii) interplay of
confounding factors beyond demography. The F2LA data and associated annotations
can be accessed at http://iab-rubric.org/index.php/F2LA.",None,-1
142696c4-843f-4a46-a4e4-733b480892cc,Patch-Craft Self-Supervised Training for Correlated Image Denoising,0.234659,"Supervised neural networks are known to achieve excellent results in various
image restoration tasks. However, such training requires datasets composed of
pairs of corrupted images and their corresponding ground truth targets.
Unfortunately, such data is not available in many applications. For the task of
image denoising in which the noise statistics is unknown, several
self-supervised training methods have been proposed for overcoming this
difficulty. Some of these require knowledge of the noise model, while others
assume that the contaminating noise is uncorrelated, both assumptions are too
limiting for many practical needs. This work proposes a novel self-supervised
training technique suitable for the removal of unknown correlated noise. The
proposed approach neither requires knowledge of the noise model nor access to
ground truth targets. The input to our algorithm consists of easily captured
bursts of noisy shots. Our algorithm constructs artificial patch-craft images
from these bursts by patch matching and stitching, and the obtained crafted
images are used as targets for the training. Our method does not require
registration of the images within the burst. We evaluate the proposed framework
through extensive experiments with synthetic and real image noise.",https://github.com/grishavak/pcst,-1
b149d9f7-e1d1-4749-953d-eae72dc1c72d,Synthetic Distracted Driving (SynDD2) dataset for analyzing distracted behaviors and various gaze zones of a driver,0.773011,"This article presents a synthetic distracted driving (SynDD2 - a continuum of
SynDD1) dataset for machine learning models to detect and analyze drivers'
various distracted behavior and different gaze zones. We collected the data in
a stationary vehicle using three in-vehicle cameras positioned at locations: on
the dashboard, near the rearview mirror, and on the top right-side window
corner. The dataset contains two activity types: distracted activities and gaze
zones for each participant, and each activity type has two sets: without
appearance blocks and with appearance blocks such as wearing a hat or
sunglasses. The order and duration of each activity for each participant are
random. In addition, the dataset contains manual annotations for each activity,
having its start and end time annotated. Researchers could use this dataset to
evaluate the performance of machine learning algorithms to classify various
distracting activities and gaze zones of drivers.",None,-1
6ac55474-1ef5-40e4-b441-fb68ba58d5a4,Interspace Pruning: Using Adaptive Filter Representations to Improve Training of Sparse CNNs,0.692795,"Unstructured pruning is well suited to reduce the memory footprint of
convolutional neural networks (CNNs), both at training and inference time. CNNs
contain parameters arranged in $K \times K$ filters. Standard unstructured
pruning (SP) reduces the memory footprint of CNNs by setting filter elements to
zero, thereby specifying a fixed subspace that constrains the filter.
Especially if pruning is applied before or during training, this induces a
strong bias. To overcome this, we introduce interspace pruning (IP), a general
tool to improve existing pruning methods. It uses filters represented in a
dynamic interspace by linear combinations of an underlying adaptive filter
basis (FB). For IP, FB coefficients are set to zero while un-pruned
coefficients and FBs are trained jointly. In this work, we provide mathematical
evidence for IP's superior performance and demonstrate that IP outperforms SP
on all tested state-of-the-art unstructured pruning methods. Especially in
challenging situations, like pruning for ImageNet or pruning to high sparsity,
IP greatly exceeds SP with equal runtime and parameter costs. Finally, we show
that advances of IP are due to improved trainability and superior
generalization ability.",None,-1
fc8f8248-24b9-4f7d-ad03-1b3144a0fc45,"AI-based artistic representation of emotions from EEG signals: a discussion on fairness, inclusion, and aesthetics",0.02007,"While Artificial Intelligence (AI) technologies are being progressively
developed, artists and researchers are investigating their role in artistic
practices. In this work, we present an AI-based Brain-Computer Interface (BCI)
in which humans and machines interact to express feelings artistically. This
system and its production of images give opportunities to reflect on the
complexities and range of human emotions and their expressions. In this
discussion, we seek to understand the dynamics of this interaction to reach
better co-existence in fairness, inclusion, and aesthetics.",None,-1
ba596697-dd0e-408d-83c6-8cb00aed4432,Unified Chinese License Plate Detection and Recognition with High Efficiency,0.997996,"Recently, deep learning-based methods have reached an excellent performance
on License Plate (LP) detection and recognition tasks. However, it is still
challenging to build a robust model for Chinese LPs since there are not enough
large and representative datasets. In this work, we propose a new dataset named
Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP
images as a supplement to the existing public benchmarks. The images are mainly
captured with electronic monitoring systems with detailed annotations. To our
knowledge, CRPD is the largest public multi-objective Chinese LP dataset with
annotations of vertices. With CRPD, a unified detection and recognition network
with high efficiency is presented as the baseline. The network is end-to-end
trainable with totally real-time inference efficiency (30 fps with 640p). The
experiments on several public benchmarks demonstrate that our method has
reached competitive performance. The code and dataset will be publicly
available at https://github.com/yxgong0/CRPD.",https://github.com/yxgong0/CRPD,-1
0a7562af-86ef-4e84-981d-481f66e0f217,A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching,0.43958,"We present a scalable combinatorial algorithm for globally optimizing over
the space of geometrically consistent mappings between 3D shapes. We use the
mathematically elegant formalism proposed by Windheuser et al. (ICCV 2011)
where 3D shape matching was formulated as an integer linear program over the
space of orientation-preserving diffeomorphisms. Until now, the resulting
formulation had limited practical applicability due to its complicated
constraint structure and its large size. We propose a novel primal heuristic
coupled with a Lagrange dual problem that is several orders of magnitudes
faster compared to previous solvers. This allows us to handle shapes with
substantially more triangles than previously solvable. We demonstrate
compelling results on diverse datasets, and, even showcase that we can address
the challenging setting of matching two partial shapes without availability of
complete shapes. Our code is publicly available at
http://github.com/paul0noah/sm-comb .",http://github.com/paul0noah/sm-comb,-1
73f57456-239f-4336-9cd3-f6f63bdd1f17,Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models,0.897133,"Machine learning based traffic forecasting models leverage sophisticated
spatiotemporal auto-correlations to provide accurate predictions of city-wide
traffic states. However, existing methods assume a reliable and unbiased
forecasting environment, which is not always available in the wild. In this
work, we investigate the vulnerability of spatiotemporal traffic forecasting
models and propose a practical adversarial spatiotemporal attack framework.
Specifically, instead of simultaneously attacking all geo-distributed data
sources, an iterative gradient-guided node saliency method is proposed to
identify the time-dependent set of victim nodes. Furthermore, we devise a
spatiotemporal gradient descent based scheme to generate real-valued
adversarial traffic states under a perturbation constraint. Meanwhile, we
theoretically demonstrate the worst performance bound of adversarial traffic
forecasting attacks. Extensive experiments on two real-world datasets show that
the proposed two-step framework achieves up to $67.8\%$ performance degradation
on various advanced spatiotemporal forecasting models. Remarkably, we also show
that adversarial training with our proposed attacks can significantly improve
the robustness of spatiotemporal traffic forecasting models. Our code is
available in \url{https://github.com/luckyfan-cs/ASTFA}.",https://github.com/kdd-hkust/Adv-ST,-1
4ba4bf3c-fca3-41ea-a99d-6dde1ca4b2cd,"Perceive, Interact, Predict: Learning Dynamic and Static Clues for End-to-End Motion Prediction",0.828319,"Motion prediction is highly relevant to the perception of dynamic objects and
static map elements in the scenarios of autonomous driving. In this work, we
propose PIP, the first end-to-end Transformer-based framework which jointly and
interactively performs online mapping, object detection and motion prediction.
PIP leverages map queries, agent queries and mode queries to encode the
instance-wise information of map elements, agents and motion intentions,
respectively. Based on the unified query representation, a differentiable
multi-task interaction scheme is proposed to exploit the correlation between
perception and prediction. Even without human-annotated HD map or agent's
historical tracking trajectory as guidance information, PIP realizes end-to-end
multi-agent motion prediction and achieves better performance than
tracking-based and HD-map-based methods. PIP provides comprehensive high-level
information of the driving scene (vectorized static map and dynamic objects
with motion information), and contributes to the downstream planning and
control. Code and models will be released for facilitating further research.",None,-1
b733504d-51ab-4ec3-8ee9-5df53567955a,"Runtime Analysis for the NSGA-II: Proving, Quantifying, and Explaining the Inefficiency For Many Objectives",0.502374,"The NSGA-II is one of the most prominent algorithms to solve multi-objective
optimization problems. Despite numerous successful applications, several
studies have shown that the NSGA-II is less effective for larger numbers of
objectives. In this work, we use mathematical runtime analyses to rigorously
demonstrate and quantify this phenomenon. We show that even on the simple
$m$-objective generalization of the discrete OneMinMax benchmark, where every
solution is Pareto optimal, the NSGA-II also with large population sizes cannot
compute the full Pareto front (objective vectors of all Pareto optima) in
sub-exponential time when the number of objectives is at least three. The
reason for this unexpected behavior lies in the fact that in the computation of
the crowding distance, the different objectives are regarded independently.
This is not a problem for two objectives, where any sorting of a pair-wise
incomparable set of solutions according to one objective is also such a sorting
according to the other objective (in the inverse order).",None,-1
b3a9f9db-b88c-4ab1-91aa-1c03f14b28f3,LISA: Learning Implicit Shape and Appearance of Hands,0.727271,"This paper proposes a do-it-all neural model of human hands, named LISA. The
model can capture accurate hand shape and appearance, generalize to arbitrary
hand subjects, provide dense surface correspondences, be reconstructed from
images in the wild and easily animated. We train LISA by minimizing the shape
and appearance losses on a large set of multi-view RGB image sequences
annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand
local coordinate, our model predicts the color and the signed distance with
respect to each hand bone independently, and then combines the per-bone
predictions using predicted skinning weights. The shape, color and pose
representations are disentangled by design, allowing to estimate or animate
only selected parameters. We experimentally demonstrate that LISA can
accurately reconstruct a dynamic hand from monocular or multi-view sequences,
achieving a noticeably higher quality of reconstructed hand shapes compared to
baseline approaches. Project page:
https://www.iri.upc.edu/people/ecorona/lisa/.",https://www.iri.upc.edu/people/ecorona/lisa/,-1
7d21ba25-d9f5-4ab5-b105-da5809a3f785,Correlation-Aware Deep Tracking,0.97059,"Robustness and discrimination power are two fundamental requirements in
visual object tracking. In most tracking paradigms, we find that the features
extracted by the popular Siamese-like networks cannot fully discriminatively
model the tracked targets and distractor objects, hindering them from
simultaneously meeting these two requirements. While most methods focus on
designing robust correlation operations, we propose a novel target-dependent
feature network inspired by the self-/cross-attention scheme. In contrast to
the Siamese-like feature extraction, our network deeply embeds cross-image
feature correlation in multiple layers of the feature network. By extensively
matching the features of the two images through multiple layers, it is able to
suppress non-target features, resulting in instance-varying feature extraction.
The output features of the search image can be directly used for predicting
target locations without extra correlation step. Moreover, our model can be
flexibly pre-trained on abundant unpaired images, leading to notably faster
convergence than the existing methods. Extensive experiments show our method
achieves the state-of-the-art results while running at real-time. Our feature
networks also can be applied to existing tracking pipelines seamlessly to raise
the tracking performance. Code will be available.",https://github.com/visionml/pytracking,-1
945780fa-7f49-424c-89b1-beefd3a65912,Knowledge Equivalence in Digital Twins of Intelligent Systems,0.297003,"A digital twin contains up-to-date data-driven models of the physical world
being studied and can use simulation to optimise the physical world. However,
the analysis made by the digital twin is valid and reliable only when the model
is equivalent to the physical world. Maintaining such an equivalent model is
challenging, especially when the physical systems being modelled are
intelligent and autonomous. The paper focuses in particular on digital twin
models of intelligent systems where the systems are knowledge-aware but with
limited capability. The digital twin improves the acting of the physical system
at a meta-level by accumulating more knowledge in the simulated environment.
The modelling of such an intelligent physical system requires replicating the
knowledge-awareness capability in the virtual space. Novel equivalence
maintaining techniques are needed, especially in synchronising the knowledge
between the model and the physical system. This paper proposes the notion of
knowledge equivalence and an equivalence maintaining approach by knowledge
comparison and updates. A quantitative analysis of the proposed approach
confirms that compared to state equivalence, knowledge equivalence maintenance
can tolerate deviation thus reducing unnecessary updates and achieve more
Pareto efficient solutions for the trade-off between update overhead and
simulation reliability.",None,-1
0885fd8a-854a-4f90-8869-3621562bcda3,Large Language Models are few(1)-shot Table Reasoners,0.794004,"Recent literature has shown that large language models (LLMs) are generally
excellent few-shot reasoners to solve text reasoning tasks. However, the
capability of LLMs on table reasoning tasks is yet to be explored. In this
paper, we aim at understanding how well LLMs can perform table-related tasks
with few-shot in-context learning. Specifically, we evaluated LLMs on popular
table QA and fact verification datasets like WikiTableQuestion, FetaQA,
TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning
over table structures, though these models are not pre-trained on any table
corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very
strong performance with only a 1-shot demonstration, even on par with some SoTA
models. We show that LLMs are even more competent at generating comprehensive
long-form answers on FetaQA than tuned T5-large. We further manually studied
the reasoning chains elicited from LLMs and found that these reasoning chains
are highly consistent with the underlying semantic form. We believe that LLMs
can serve as a simple yet generic baseline for future research. The code and
data are released in https://github.com/wenhuchen/TableCoT.",https://github.com/wenhuchen/TableCoT,-1
b8c3149c-be5a-415e-8f18-779fbadb57d0,Minimum Class Confusion based Transfer for Land Cover Segmentation in Rural and Urban Regions,0.258075,"Transfer Learning methods are widely used in satellite image segmentation
problems and improve performance upon classical supervised learning methods. In
this study, we present a semantic segmentation method that allows us to make
land cover maps by using transfer learning methods. We compare models trained
in low-resolution images with insufficient data for the targeted region or zoom
level. In order to boost performance on target data we experiment with models
trained with unsupervised, semi-supervised and supervised transfer learning
approaches, including satellite images from public datasets and other unlabeled
sources. According to experimental results, transfer learning improves
segmentation performance 3.4% MIoU (Mean Intersection over Union) in rural
regions and 12.9% MIoU in urban regions. We observed that transfer learning is
more effective when two datasets share a comparable zoom level and are labeled
with identical rules; otherwise, semi-supervised learning is more effective by
using the data as unlabeled. In addition, experiments showed that HRNet
outperformed building segmentation approaches in multi-class segmentation.",None,-1
6289ede6-c015-457c-9cd5-50de4676e627,Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization,0.814644,"Vision transformers (ViTs) are emerging with significantly improved accuracy
in computer vision tasks. However, their complex architecture and enormous
computation/storage demand impose urgent needs for new hardware accelerator
design methodology. This work proposes an FPGA-aware automatic ViT acceleration
framework based on the proposed mixed-scheme quantization. To the best of our
knowledge, this is the first FPGA-based ViT acceleration framework exploring
model quantization. Compared with state-of-the-art ViT quantization work
(algorithmic approach only without hardware acceleration), our quantization
achieves 0.47% to 1.36% higher Top-1 accuracy under the same bit-width.
Compared with the 32-bit floating-point baseline FPGA accelerator, our
accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS
vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",None,-1
8551dc63-7782-42da-95a0-ff37109d51af,Psychophysical Evaluation of Human Performance in Detecting Digital Face Image Manipulations,0.424282,"In recent years, increasing deployment of face recognition technology in
security-critical settings, such as border control or law enforcement, has led
to considerable interest in the vulnerability of face recognition systems to
attacks utilising legitimate documents, which are issued on the basis of
digitally manipulated face images. As automated manipulation and attack
detection remains a challenging task, conventional processes with human
inspectors performing identity verification remain indispensable. These
circumstances merit a closer investigation of human capabilities in detecting
manipulated face images, as previous work in this field is sparse and often
concentrated only on specific scenarios and biometric characteristics.
  This work introduces a web-based, remote visual discrimination experiment on
the basis of principles adopted from the field of psychophysics and
subsequently discusses interdisciplinary opportunities with the aim of
examining human proficiency in detecting different types of digitally
manipulated face images, specifically face swapping, morphing, and retouching.
In addition to analysing appropriate performance measures, a possible metric of
detectability is explored. Experimental data of 306 probands indicate that
detection performance is widely distributed across the population and detection
of certain types of face image manipulations is much more challenging than
others.",None,-1
5db2b96c-bc50-460e-be20-fb1b25264f26,Prototypical VoteNet for Few-Shot 3D Point Cloud Object Detection,0.351018,"Most existing 3D point cloud object detection approaches heavily rely on
large amounts of labeled training data. However, the labeling process is costly
and time-consuming. This paper considers few-shot 3D point cloud object
detection, where only a few annotated samples of novel classes are needed with
abundant samples of base classes. To this end, we propose Prototypical VoteNet
to recognize and localize novel instances, which incorporates two new modules:
Prototypical Vote Module (PVM) and Prototypical Head Module (PHM).
Specifically, as the 3D basic geometric structures can be shared among
categories, PVM is designed to leverage class-agnostic geometric prototypes,
which are learned from base classes, to refine local features of novel
categories.Then PHM is proposed to utilize class prototypes to enhance the
global feature of each object, facilitating subsequent object localization and
classification, which is trained by the episodic training strategy. To evaluate
the model in this new setting, we contribute two new benchmark datasets,
FS-ScanNet and FS-SUNRGBD. We conduct extensive experiments to demonstrate the
effectiveness of Prototypical VoteNet, and our proposed method shows
significant and consistent improvements compared to baselines on two benchmark
datasets.",https://shizhen-zhao.github.io/FS3D_page/,-1
a8a4886d-a69c-4d8a-9cd8-3528b92b884a,Ask Me Anything: A simple strategy for prompting language models,0.798629,"Large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt that demonstrates how to perform the task and
no additional training. Prompting is a brittle process wherein small
modifications to the prompt can cause large variations in the model
predictions, and therefore significant effort is dedicated towards designing a
painstakingly ""perfect prompt"" for a task. To mitigate the high degree of
effort involved in prompt-design, we instead ask whether producing multiple
effective, yet imperfect, prompts and aggregating them can lead to a high
quality prompting strategy. Our observations motivate our proposed prompting
method, ASK ME ANYTHING (AMA). We first develop an understanding of the
effective prompt formats, finding that question-answering (QA) prompts, which
encourage open-ended generation (""Who went to the park?"") tend to outperform
those that restrict the model outputs (""John went to the park. Output True or
False.""). Our approach recursively uses the LLM itself to transform task inputs
to the effective QA format. We apply the collected prompts to obtain several
noisy votes for the input's true label. We find that the prompts can have very
different accuracies and complex dependencies and thus propose to use weak
supervision, a procedure for combining the noisy predictions, to produce the
final predictions for the inputs. We evaluate AMA across open-source model
families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B
parameters), demonstrating an average performance lift of 10.2% over the
few-shot baseline. This simple strategy enables the open-source GPT-J-6B model
to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular
benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms
few-shot GPT3-175B. We release our code here:
https://github.com/HazyResearch/ama_prompting",None,-1
0acb79c0-ef30-4c90-9d2e-f3a831536f7c,PaCo: Parameter-Compositional Multi-Task Reinforcement Learning,0.656791,"The purpose of multi-task reinforcement learning (MTRL) is to train a single
policy that can be applied to a set of different tasks. Sharing parameters
allows us to take advantage of the similarities among tasks. However, the gaps
between contents and difficulties of different tasks bring us challenges on
both which tasks should share the parameters and what parameters should be
shared, as well as the optimization challenges due to parameter sharing. In
this work, we introduce a parameter-compositional approach (PaCo) as an attempt
to address these challenges. In this framework, a policy subspace represented
by a set of parameters is learned. Policies for all the single tasks lie in
this subspace and can be composed by interpolating with the learned set. It
allows not only flexible parameter sharing but also a natural way to improve
training. We demonstrate the state-of-the-art performance on Meta-World
benchmarks, verifying the effectiveness of the proposed approach.",https://github.com/facebookresearch/mtrl,30737
c9f298bc-ca76-4953-bb54-09926e5d2dbc,You Only Derive Once (YODO): Automatic Differentiation for Efficient Sensitivity Analysis in Bayesian Networks,0.281696,"Sensitivity analysis measures the influence of a Bayesian network's
parameters on a quantity of interest defined by the network, such as the
probability of a variable taking a specific value. In particular, the so-called
sensitivity value measures the quantity of interest's partial derivative with
respect to the network's conditional probabilities. However, finding such
values in large networks with thousands of parameters can become
computationally very expensive. We propose to use automatic differentiation
combined with exact inference to obtain all sensitivity values in a single
pass. Our method first marginalizes the whole network once using e.g. variable
elimination and then backpropagates this operation to obtain the gradient with
respect to all input parameters. We demonstrate our routines by ranking all
parameters by importance on a Bayesian network modeling humanitarian crises and
disasters, and then show the method's efficiency by scaling it to huge networks
with up to 100'000 parameters. An implementation of the methods using the
popular machine learning library PyTorch is freely available.",https://github.com/rballester/yodo,-1
bb00425f-e537-4dce-a7a0-494b2dfb8c29,Accurate Action Recommendation for Smart Home via Two-Level Encoders and Commonsense Knowledge,0.222751,"How can we accurately recommend actions for users to control their devices at
home? Action recommendation for smart home has attracted increasing attention
due to its potential impact on the markets of virtual assistants and Internet
of Things (IoT). However, designing an effective action recommender system for
smart home is challenging because it requires handling context correlations,
considering both queried contexts and previous histories of users, and dealing
with capricious intentions in history. In this work, we propose SmartSense, an
accurate action recommendation method for smart home. For individual action,
SmartSense summarizes its device control and its temporal contexts in a
self-attentive manner, to reflect the importance of the correlation between
them. SmartSense then summarizes sequences of users considering queried
contexts in a query-attentive manner to extract the query-related patterns from
the sequential actions. SmartSense also transfers the commonsense knowledge
from routine data to better handle intentions in action sequences. As a result,
SmartSense addresses all three main challenges of action recommendation for
smart home, and achieves the state-of-the-art performance giving up to 9.8%
higher mAP@1 than the best competitor.",https://github.com/snudatalab/SmartSense,-1
1dcf8741-a35d-4ba4-9d15-d2b5b0e392ab,"M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval",0.864665,"This work investigates the use of large-scale, English-only pre-trained
models (CLIP and HuBERT) for multilingual image-speech retrieval. For
non-English image-speech retrieval, we outperform the current state-of-the-art
performance by a wide margin both when training separate models for each
language, and with a single model which processes speech in all three
languages. We identify key differences in model behavior and performance
between English and non-English settings, attributable to the English-only
pre-training of CLIP and HuBERT, and investigate how fine-tuning the
pre-trained models impacts these differences. Finally, we show that our models
can be used for mono- and cross-lingual speech-text retrieval and cross-lingual
speech-speech retrieval, despite never having seen any parallel speech-text or
speech-speech data during training.",None,9523
4ba4bf6b-9296-4ef4-94e6-05fe1afcc1a0,ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation,0.112602,"Residual networks are an Euler discretization of solutions to Ordinary
Differential Equations (ODE). This paper explores a deeper relationship between
Transformer and numerical ODE methods. We first show that a residual block of
layers in Transformer can be described as a higher-order solution to ODE.
Inspired by this, we design a new architecture, {\it ODE Transformer}, which is
analogous to the Runge-Kutta method that is well motivated in ODE. As a natural
extension to Transformer, ODE Transformer is easy to implement and efficient to
use. Experimental results on the large-scale machine translation, abstractive
summarization, and grammar error correction tasks demonstrate the high
genericity of ODE Transformer. It can gain large improvements in model
performance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the
WMT'14 English-German and English-French benchmarks) at a slight cost in
inference efficiency.",https://github.com/libeineu/ODE-Transformer,-1
20db284b-e6ec-4fde-9f25-d838ee5844b5,Utterance Rewriting with Contrastive Learning in Multi-turn Dialogue,0.0916841,"Context modeling plays a significant role in building multi-turn dialogue
systems. In order to make full use of context information, systems can use
Incomplete Utterance Rewriting(IUR) methods to simplify the multi-turn dialogue
into single-turn by merging current utterance and context information into a
self-contained utterance. However, previous approaches ignore the intent
consistency between the original query and rewritten query. The detection of
omitted or coreferred locations in the original query can be further improved.
In this paper, we introduce contrastive learning and multi-task learning to
jointly model the problem. Our method benefits from carefully designed
self-supervised objectives, which act as auxiliary tasks to capture semantics
at both sentence-level and token-level. The experiments show that our proposed
model achieves state-of-the-art performance on several public datasets.",None,199
5794824d-9ab6-48a7-bb5d-0642d006facf,The 8-Point Algorithm as an Inductive Bias for Relative Pose Prediction by ViTs,0.873884,"We present a simple baseline for directly estimating the relative pose
(rotation and translation, including scale) between two images. Deep methods
have recently shown strong progress but often require complex or multi-stage
architectures. We show that a handful of modifications can be applied to a
Vision Transformer (ViT) to bring its computations close to the Eight-Point
Algorithm. This inductive bias enables a simple method to be competitive in
multiple settings, often substantially improving over the state of the art with
strong performance gains in limited data regimes.",https://github.com/rwightman/pytorch-image-models,-1
90d5cbc8-687c-43d5-bc64-0c93ea7a0c73,A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling,0.786775,"Document-level relation extraction (RE) aims to identify relations between
entities across multiple sentences. Most previous methods focused on
document-level RE under full supervision. However, in real-world scenario, it
is expensive and difficult to completely label all relations in a document
because the number of entity pairs in document-level RE grows quadratically
with the number of entities. To solve the common incomplete labeling problem,
we propose a unified positive-unlabeled learning framework - shift and squared
ranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled
(PU) learning on document-level RE for the first time. Considering that labeled
data of a dataset may lead to prior shift of unlabeled data, we introduce a PU
learning under prior shift of training data. Also, using none-class score as an
adaptive threshold, we propose squared ranking loss and prove its Bayesian
consistency with multi-label ranking metrics. Extensive experiments demonstrate
that our method achieves an improvement of about 14 F1 points relative to the
previous baseline with incomplete labeling. In addition, it outperforms
previous state-of-the-art results under both fully supervised and extremely
unlabeled settings as well.",https://github.com/www-Ye/SSR-PU,-1
1055cc29-a8d8-4ef5-8d44-0fc13a887230,PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation,0.824345,"In this paper, we propose a new deep learning-based method for estimating
room layout given a pair of 360 panoramas. Our system, called Position-aware
Stereo Merging Network or PSMNet, is an end-to-end joint layout-pose estimator.
PSMNet consists of a Stereo Pano Pose (SP2) transformer and a novel
Cross-Perspective Projection (CP2) layer. The stereo-view SP2 transformer is
used to implicitly infer correspondences between views, and can handle noisy
poses. The pose-aware CP2 layer is designed to render features from the
adjacent view to the anchor (reference) view, in order to perform view fusion
and estimate the visible layout. Our experiments and analysis validate our
method, which significantly outperforms the state-of-the-art layout estimators,
especially for large and complex room spaces.",None,28982
ca4288ad-9bee-4a50-990f-885d501fdd86,Are High-Resolution Event Cameras Really Needed?,0.51533,"Due to their outstanding properties in challenging conditions, event cameras
have become indispensable in a wide range of applications, ranging from
automotive, computational photography, and SLAM. However, as further
improvements are made to the sensor design, modern event cameras are trending
toward higher and higher sensor resolutions, which result in higher bandwidth
and computational requirements on downstream tasks. Despite this trend, the
benefits of using high-resolution event cameras to solve standard computer
vision tasks are still not clear. In this work, we report the surprising
discovery that, in low-illumination conditions and at high speeds,
low-resolution cameras can outperform high-resolution ones, while requiring a
significantly lower bandwidth. We provide both empirical and theoretical
evidence for this claim, which indicates that high-resolution event cameras
exhibit higher per-pixel event rates, leading to higher temporal noise in
low-illumination conditions and at high speeds. As a result, in most cases,
high-resolution event cameras show a lower task performance, compared to lower
resolution sensors in these conditions. We empirically validate our findings
across several tasks, namely image reconstruction, optical flow estimation, and
camera pose tracking, both on synthetic and real data. We believe that these
findings will provide important guidelines for future trends in event camera
development.",None,-1
5502c399-df1a-4bd7-b880-4405d7785a77,Edge-enhanced Feature Distillation Network for Efficient Super-Resolution,0.606906,"With the recently massive development in convolution neural networks,
numerous lightweight CNN-based image super-resolution methods have been
proposed for practical deployments on edge devices. However, most existing
methods focus on one specific aspect: network or loss design, which leads to
the difficulty of minimizing the model size. To address the issue, we conclude
block devising, architecture searching, and loss design to obtain a more
efficient SR structure. In this paper, we proposed an edge-enhanced feature
distillation network, named EFDN, to preserve the high-frequency information
under constrained resources. In detail, we build an edge-enhanced convolution
block based on the existing reparameterization methods. Meanwhile, we propose
edge-enhanced gradient loss to calibrate the reparameterized path training.
Experimental results show that our edge-enhanced strategies preserve the edge
and significantly improve the final restoration quality. Code is available at
https://github.com/icandle/EFDN.",https://github.com/icandle/EFDN,-1
c4f50556-bfcf-48f7-8d7f-17a5f639fd54,Few-Shot Teamwork,0.0253523,"We propose the novel few-shot teamwork (FST) problem, where skilled agents
trained in a team to complete one task are combined with skilled agents from
different tasks, and together must learn to adapt to an unseen but related
task. We discuss how the FST problem can be seen as addressing two separate
problems: one of reducing the experience required to train a team of agents to
complete a complex task; and one of collaborating with unfamiliar teammates to
complete a new task. Progress towards solving FST could lead to progress in
both multi-agent reinforcement learning and ad hoc teamwork.",None,-1
9c9b153b-caa5-4ee0-aad5-6da8ea046e4d,Symlink: A New Dataset for Scientific Symbol-Description Linking,0.584314,"Mathematical symbols and descriptions appear in various forms across document
section boundaries without explicit markup. In this paper, we present a new
large-scale dataset that emphasizes extracting symbols and descriptions in
scientific documents. Symlink annotates scientific papers of 5 different
domains (i.e., computer science, biology, physics, mathematics, and economics).
Our experiments on Symlink demonstrate the challenges of the symbol-description
linking task for existing models and call for further research effort in this
area. We will publicly release Symlink to facilitate future research.",None,-1
70e10440-5312-4e68-916c-3d1e840c9836,VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners,0.634628,"We explore an efficient approach to establish a foundational video-text
model. We present VideoCoCa that maximally reuses a pretrained image-text
contrastive captioner (CoCa) model and adapt it to video-text tasks with
minimal extra training. While previous works adapt image-text models with
various cross-frame fusion modules, we find that the generative attentional
pooling and contrastive attentional pooling layers in CoCa are instantly
adaptable to flattened frame embeddings, yielding state-of-the-art results on
zero-shot video classification and zero-shot text-to-video retrieval.
Furthermore, we explore lightweight finetuning on top of VideoCoCa, and achieve
strong results on video question-answering and video captioning.",None,-1
bbcb0c56-a8b7-4f80-b53f-e0d3d4f89663,Object Class Aware Video Anomaly Detection through Image Translation,0.102511,"Semi-supervised video anomaly detection (VAD) methods formulate the task of
anomaly detection as detection of deviations from the learned normal patterns.
Previous works in the field (reconstruction or prediction-based methods) suffer
from two drawbacks: 1) They focus on low-level features, and they (especially
holistic approaches) do not effectively consider the object classes. 2)
Object-centric approaches neglect some of the context information (such as
location). To tackle these challenges, this paper proposes a novel two-stream
object-aware VAD method that learns the normal appearance and motion patterns
through image translation tasks. The appearance branch translates the input
image to the target semantic segmentation map produced by Mask-RCNN, and the
motion branch associates each frame with its expected optical flow magnitude.
Any deviation from the expected appearance or motion in the inference stage
shows the degree of potential abnormality. We evaluated our proposed method on
the ShanghaiTech, UCSD-Ped1, and UCSD-Ped2 datasets and the results show
competitive performance compared with state-of-the-art works. Most importantly,
the results show that, as significant improvements to previous methods,
detections by our method are completely explainable and anomalies are localized
accurately in the frames.",None,-1
1422a914-0235-4af1-b161-918170c5adbb,Towards a Grounded Theory of Causation for Embodied AI,0.212008,"There exist well-developed frameworks for causal modelling, but these require
rather a lot of human domain expertise to define causal variables and perform
interventions. In order to enable autonomous agents to learn abstract causal
models through interactive experience, the existing theoretical foundations
need to be extended and clarified. Existing frameworks give no guidance
regarding variable choice / representation, and more importantly, give no
indication as to which behaviour policies or physical transformations of state
space shall count as interventions. The framework sketched in this paper
describes actions as transformations of state space, for instance induced by an
agent running a policy. This makes it possible to describe in a uniform way
both transformations of the micro-state space and abstract models thereof, and
say when the latter is veridical / grounded / natural. We then introduce
(causal) variables, define a mechanism as an invariant predictor, and say when
an action can be viewed as a ``surgical intervention'', thus bringing the
objective of causal representation \& intervention skill learning into clearer
focus.",None,-1
9a1aafa6-7151-4dd6-8640-d23d1c37916b,Maze Learning using a Hyperdimensional Predictive Processing Cognitive Architecture,0.399824,"We present the COGnitive Neural GENerative system (CogNGen), a cognitive
architecture that combines two neurobiologically-plausible, computational
models: predictive processing and hyperdimensional/vector-symbolic models. We
draw inspiration from architectures such as ACT-R and Spaun/Nengo. CogNGen is
in broad agreement with these, providing a level of detail between ACT-R's
high-level symbolic description of human cognition and Spaun's low-level
neurobiological description, furthermore creating the groundwork for designing
agents that learn continually from diverse tasks and model human performance at
larger scales than what is possible with current systems. We test CogNGen on
four maze-learning tasks, including those that test memory and planning, and
find that CogNGen matches performance of deep reinforcement learning models and
exceeds on a task designed to test memory.",None,-1
3db32823-5ed1-42f5-bd3d-3a0f080e0ca7,GloCAL: Glocalized Curriculum-Aided Learning of Multiple Tasks with Application to Robotic Grasping,0.0804053,"The domain of robotics is challenging to apply deep reinforcement learning
due to the need for large amounts of data and for ensuring safety during
learning. Curriculum learning has shown good performance in terms of sample-
efficient deep learning. In this paper, we propose an algorithm (named GloCAL)
that creates a curriculum for an agent to learn multiple discrete tasks, based
on clustering tasks according to their evaluation scores. From the
highest-performing cluster, a global task representative of the cluster is
identified for learning a global policy that transfers to subsequently formed
new clusters, while the remaining tasks in the cluster are learned as local
policies. The efficacy and efficiency of our GloCAL algorithm are compared with
other approaches in the domain of grasp learning for 49 objects with varied
object complexity and grasp difficulty from the EGAD! dataset. The results show
that GloCAL is able to learn to grasp 100% of the objects, whereas other
approaches achieve at most 86% despite being given 1.5 times longer training
time.",https://github.com/hill-a/stable-baselines,-1
812f6c05-c1ed-4589-b2bc-8e44b9a5a410,Blur Interpolation Transformer for Real-World Motion from Blur,0.789912,"This paper studies the challenging problem of recovering motion from blur,
also known as joint deblurring and interpolation or blur temporal
super-resolution. The challenges are twofold: 1) the current methods still
leave considerable room for improvement in terms of visual quality even on the
synthetic dataset, and 2) poor generalization to real-world data. To this end,
we propose a blur interpolation transformer (BiT) to effectively unravel the
underlying temporal correlation encoded in blur. Based on multi-scale residual
Swin transformer blocks, we introduce dual-end temporal supervision and
temporally symmetric ensembling strategies to generate effective features for
time-varying motion rendering. In addition, we design a hybrid camera system to
collect the first real-world dataset of one-to-many blur-sharp video pairs.
Experimental results show that BiT has a significant gain over the
state-of-the-art methods on the public dataset Adobe240. Besides, the proposed
real-world dataset effectively helps the model generalize well to real blurry
scenarios. Code and data are available at https://github.com/zzh-tech/BiT.",https://github.com/zzh-tech/BiT,-1
20268a05-dcc3-4494-8c80-c681295b553c,AutoAdversary: A Pixel Pruning Method for Sparse Adversarial Attack,0.0603421,"Deep neural networks (DNNs) have been proven to be vulnerable to adversarial
examples. A special branch of adversarial examples, namely sparse adversarial
examples, can fool the target DNNs by perturbing only a few pixels. However,
many existing sparse adversarial attacks use heuristic methods to select the
pixels to be perturbed, and regard the pixel selection and the adversarial
attack as two separate steps. From the perspective of neural network pruning,
we propose a novel end-to-end sparse adversarial attack method, namely
AutoAdversary, which can find the most important pixels automatically by
integrating the pixel selection into the adversarial attack. Specifically, our
method utilizes a trainable neural network to generate a binary mask for the
pixel selection. After jointly optimizing the adversarial perturbation and the
neural network, only the pixels corresponding to the value 1 in the mask are
perturbed. Experiments demonstrate the superiority of our proposed method over
several state-of-the-art methods. Furthermore, since AutoAdversary does not
require a heuristic pixel selection process, it does not slow down excessively
as other methods when the image size increases.",https://github.com/wubaoyuan/Sparse-Adversarial-Attack,-1
19e31adf-30be-4f59-8a83-471acf2b76d2,Features Fusion Framework for Multimodal Irregular Time-series Events,0.292714,"Some data from multiple sources can be modeled as multimodal time-series
events which have different sampling frequencies, data compositions, temporal
relations and characteristics. Different types of events have complex nonlinear
relationships, and the time of each event is irregular. Neither the classical
Recurrent Neural Network (RNN) model nor the current state-of-the-art
Transformer model can deal with these features well. In this paper, a features
fusion framework for multimodal irregular time-series events is proposed based
on the Long Short-Term Memory networks (LSTM). Firstly, the complex features
are extracted according to the irregular patterns of different events.
Secondly, the nonlinear correlation and complex temporal dependencies
relationship between complex features are captured and fused into a tensor.
Finally, a feature gate are used to control the access frequency of different
tensors. Extensive experiments on MIMIC-III dataset demonstrate that the
proposed framework significantly outperforms to the existing methods in terms
of AUC (the area under Receiver Operating Characteristic curve) and AP (Average
Precision).",None,22
ec598a52-b12d-4681-bcc3-8154cd0fbb41,MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning,0.914962,"Instruction tuning, a new learning paradigm that fine-tunes pre-trained
language models on tasks specified through instructions, has shown promising
zero-shot performance on various natural language processing tasks. However, it
has yet to be explored for vision and multimodal tasks. In this work, we
introduce MUL-TIINSTRUCT, the first multimodal instruction tuning benchmark
dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq
format covering 10 broad categories. The tasks are derived from 21 existing
open-source datasets and each task is equipped with 5 expert-written
instructions. We take OFA as the base pre-trained model for multimodal
instruction tuning, and to further improve its zero-shot performance, we
explore multiple transfer learning strategies to leverage the large-scale
NATURAL INSTRUCTIONS dataset. Experimental results demonstrate strong zero-shot
performance on various unseen multimodal tasks and the benefit of transfer
learning from a text-only instruction dataset. We also design a new evaluation
metric - Sensitivity, to evaluate how sensitive the model is to the variety of
instructions. Our results indicate that fine-tuning the model on a diverse set
of tasks and instructions leads to a reduced sensitivity to variations in
instructions for each task.",https://github.com/VT-NLP/MultiInstruct,-1
77c42547-ccbb-49ba-ad20-96089e21d882,Transformers are Adaptable Task Planners,0.628072,"Every home is different, and every person likes things done in their
particular way. Therefore, home robots of the future need to both reason about
the sequential nature of day-to-day tasks and generalize to user's preferences.
To this end, we propose a Transformer Task Planner(TTP) that learns high-level
actions from demonstrations by leveraging object attribute-based
representations. TTP can be pre-trained on multiple preferences and shows
generalization to unseen preferences using a single demonstration as a prompt
in a simulated dishwasher loading task. Further, we demonstrate real-world dish
rearrangement using TTP with a Franka Panda robotic arm, prompted using a
single human demonstration.",None,-1
dead0be4-15ed-467c-882c-ac56427043f4,Multi-Attribute Open Set Recognition,0.598247,"Open Set Recognition (OSR) extends image classification to an open-world
setting, by simultaneously classifying known classes and identifying unknown
ones. While conventional OSR approaches can detect Out-of-Distribution (OOD)
samples, they cannot provide explanations indicating which underlying visual
attribute(s) (e.g., shape, color or background) cause a specific sample to be
unknown. In this work, we introduce a novel problem setup that generalizes
conventional OSR to a multi-attribute setting, where multiple visual attributes
are simultaneously recognized. Here, OOD samples can be not only identified but
also categorized by their unknown attribute(s). We propose simple extensions of
common OSR baselines to handle this novel scenario. We show that these
baselines are vulnerable to shortcuts when spurious correlations exist in the
training dataset. This leads to poor OOD performance which, according to our
experiments, is mainly due to unintended cross-attribute correlations of the
predicted confidence scores. We provide an empirical evidence showing that this
behavior is consistent across different baselines on both synthetic and real
world datasets.",None,-1
08947664-35b1-4117-9b52-570ed84a99ca,One Weird Trick to Improve Your Semi-Weakly Supervised Semantic Segmentation Model,0.201806,"Semi-weakly supervised semantic segmentation (SWSSS) aims to train a model to
identify objects in images based on a small number of images with pixel-level
labels, and many more images with only image-level labels. Most existing SWSSS
algorithms extract pixel-level pseudo-labels from an image classifier - a very
difficult task to do well, hence requiring complicated architectures and
extensive hyperparameter tuning on fully-supervised validation sets. We propose
a method called prediction filtering, which instead of extracting
pseudo-labels, just uses the classifier as a classifier: it ignores any
segmentation predictions from classes which the classifier is confident are not
present. Adding this simple post-processing method to baselines gives results
competitive with or better than prior SWSSS algorithms. Moreover, it is
compatible with pseudo-label methods: adding prediction filtering to existing
SWSSS algorithms further improves segmentation performance.",None,-1
dd0ab0f5-e398-4115-9d70-3b8eae9a02db,Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space,0.378745,"Learning causal relationships in high-dimensional data (images, videos) is a
hard task, as they are often defined on low dimensional manifolds and must be
extracted from complex signals dominated by appearance, lighting, textures and
also spurious correlations in the data. We present a method for learning
counterfactual reasoning of physical processes in pixel space, which requires
the prediction of the impact of interventions on initial conditions. Going
beyond the identification of structural relationships, we deal with the
challenging problem of forecasting raw video over long horizons. Our method
does not require the knowledge or supervision of any ground truth positions or
other object or scene properties. Our model learns and acts on a suitable
hybrid latent representation based on a combination of dense features, sets of
2D keypoints and an additional latent vector per keypoint. We show that this
better captures the dynamics of physical processes than purely dense or sparse
representations. We introduce a new challenging and carefully designed
counterfactual benchmark for predictions in pixel space and outperform strong
baselines in physics-inspired ML and video prediction.",https://filteredcophy.github.io,-1
009887a4-ec84-4371-9edf-33e1dffb45fe,Soft Diffusion: Score Matching for General Corruptions,0.963586,"We define a broader family of corruption processes that generalizes
previously known diffusion models. To reverse these general diffusions, we
propose a new objective called Soft Score Matching that provably learns the
score function for any linear corruption process and yields state of the art
results for CelebA. Soft Score Matching incorporates the degradation process in
the network. Our new loss trains the model to predict a clean image,
\textit{that after corruption}, matches the diffused observation. We show that
our objective learns the gradient of the likelihood under suitable regularity
conditions for a family of corruption processes. We further develop a
principled way to select the corruption levels for general diffusion processes
and a novel sampling method that we call Momentum Sampler. We show
experimentally that our framework works for general linear corruption
processes, such as Gaussian blur and masking. We achieve state-of-the-art FID
score $1.85$ on CelebA-64, outperforming all previous linear diffusion models.
We also show significant computational benefits compared to vanilla denoising
diffusion.",None,28054
86db4964-cdb3-4734-8c5e-72e64d0a638e,Local Sliced-Wasserstein Feature Sets for Illumination-invariant Face Recognition,0.707265,"We present a new method for face recognition from digital images acquired
under varying illumination conditions. The method is based on mathematical
modeling of local gradient distributions using the Radon Cumulative
Distribution Transform (R-CDT). We demonstrate that lighting variations cause
certain types of deformations of local image gradient distributions which, when
expressed in R-CDT domain, can be modeled as a subspace. Face recognition is
then performed using a nearest subspace in R-CDT domain of local gradient
distributions. Experiment results demonstrate the proposed method outperforms
other alternatives in several face recognition tasks with challenging
illumination conditions. Python code implementing the proposed method is
available, which is integrated as a part of the software package PyTransKit.",https://github.com/rohdelab/drcdt face,-1
8908128c-c437-4ac5-b8b0-ef7bc9205daa,Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages,0.652001,"Multilingual Pretrained Language Models (MPLMs) have shown their strong
multilinguality in recent empirical cross-lingual transfer studies. In this
paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)
pipeline to improve the zero-shot performance on low-resource languages (LRLs)
by augmenting the context with semantically similar sentences retrieved from a
high-resource language (HRL) as prompts. PARC improves the zero-shot
performance on three downstream tasks (binary sentiment classification, topic
categorization and natural language inference) with multilingual parallel test
sets across 10 LRLs covering 6 language families in both unlabeled settings
(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the
finetuning baseline by 3.7%. We find a significant positive correlation between
cross-lingual transfer performance on one side, and the similarity between the
high- and low-resource languages as well as the amount of low-resource
pretraining data on the other side. A robustness analysis suggests that PARC
has the potential to achieve even stronger performance with more powerful
MPLMs.",https://github.com/ercong21/parc,-1
71655112-1cc6-426c-9da3-b1a92331d389,Contrastive Supervised Distillation for Continual Representation Learning,0.137315,"In this paper, we propose a novel training procedure for the continual
representation learning problem in which a neural network model is sequentially
learned to alleviate catastrophic forgetting in visual search tasks. Our
method, called Contrastive Supervised Distillation (CSD), reduces feature
forgetting while learning discriminative features. This is achieved by
leveraging labels information in a distillation setting in which the student
model is contrastively learned from the teacher model. Extensive experiments
show that CSD performs favorably in mitigating catastrophic forgetting by
outperforming current state-of-the-art methods. Our results also provide
further evidence that feature forgetting evaluated in visual retrieval tasks is
not as catastrophic as in classification tasks. Code at:
https://github.com/NiccoBiondi/ContrastiveSupervisedDistillation.",https://github.com/NiccoBiondi/ContrastiveSupervisedDistillation,-1
4cba19b2-e1fd-43f3-8779-78a63cb72369,Automatic Detection of Entity-Manipulated Text using Factual Knowledge,0.686449,"In this work, we focus on the problem of distinguishing a human written news
article from a news article that is created by manipulating entities in a human
written news article (e.g., replacing entities with factually incorrect
entities). Such manipulated articles can mislead the reader by posing as a
human written news article. We propose a neural network based detector that
detects manipulated news articles by reasoning about the facts mentioned in the
article. Our proposed detector exploits factual knowledge via graph
convolutional neural network along with the textual information in the news
article. We also create challenging datasets for this task by considering
various strategies to generate the new replacement entity (e.g., entity
generation from GPT-2). In all the settings, our proposed model either matches
or outperforms the state-of-the-art detector in terms of accuracy. Our code and
data are available at https://github.com/UBC-NLP/manipulated_entity_detection.",https://github.com/UBC-NLP/manipulated_entity_detection,-1
c3933262-b3a4-4942-95df-23d9dd278556,Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension,0.213036,"Multilingual pre-trained models are able to zero-shot transfer knowledge from
rich-resource to low-resource languages in machine reading comprehension (MRC).
However, inherent linguistic discrepancies in different languages could make
answer spans predicted by zero-shot transfer violate syntactic constraints of
the target language. In this paper, we propose a novel multilingual MRC
framework equipped with a Siamese Semantic Disentanglement Model (SSDM) to
disassociate semantics from syntax in representations learned by multilingual
pre-trained models. To explicitly transfer only semantic knowledge to the
target language, we propose two groups of losses tailored for semantic and
syntactic encoding and disentanglement. Experimental results on three
multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the
effectiveness of our proposed approach over models based on mBERT and XLM-100.
Code is available at:https://github.com/wulinjuan/SSDM_MRC.",None,-1
0591de13-8c13-463b-910e-f3b5babcf656,On the Effect of Anticipation on Reading Times,0.726785,"Over the past two decades, numerous studies have demonstrated how less
predictable (i.e., higher surprisal) words take more time to read. In general,
these studies have implicitly assumed the reading process is purely responsive:
Readers observe a new word and allocate time to process it as required. We
argue that prior results are also compatible with a reading process that is at
least partially anticipatory: Readers could make predictions about a future
word and allocate time to process it based on their expectation. In this work,
we operationalize this anticipation as a word's contextual entropy. We assess
the effect of anticipation on reading by comparing how well surprisal and
contextual entropy predict reading times on four naturalistic reading datasets:
two self-paced and two eye-tracking. Experimentally, across datasets and
analyses, we find substantial evidence for effects of contextual entropy over
surprisal on a word's reading time (RT): in fact, entropy is sometimes better
than surprisal in predicting a word's RT. Spillover effects, however, are
generally not captured by entropy, but only by surprisal. Further, we
hypothesize four cognitive mechanisms through which contextual entropy could
impact RTs -- three of which we are able to design experiments to analyze.
Overall, our results support a view of reading that is not just responsive, but
also anticipatory.",https://github.com/rycolab/anticipation-on-reading-times,-1
3d36d40b-7b9b-496e-8843-e8b98803daaa,Test-time adversarial detection and robustness for localizing humans using ultra wide band channel impulse responses,0.211872,"Keyless entry systems in cars are adopting neural networks for localizing its
operators. Using test-time adversarial defences equip such systems with the
ability to defend against adversarial attacks without prior training on
adversarial samples. We propose a test-time adversarial example detector which
detects the input adversarial example through quantifying the localized
intermediate responses of a pre-trained neural network and confidence scores of
an auxiliary softmax layer. Furthermore, in order to make the network robust,
we extenuate the non-relevant features by non-iterative input sample clipping.
Using our approach, mean performance over 15 levels of adversarial
perturbations is increased by 55.33% for the fast gradient sign method (FGSM)
and 6.3% for both the basic iterative method (BIM) and the projected gradient
method (PGD).",None,-1
56a1a6e8-d974-4df1-be0d-5340048869b9,A Critical Reflection and Forward Perspective on Empathy and Natural Language Processing,0.3564,"We review the state of research on empathy in natural language processing and
identify the following issues: (1) empathy definitions are absent or abstract,
which (2) leads to low construct validity and reproducibility. Moreover, (3)
emotional empathy is overemphasized, skewing our focus to a narrow subset of
simplified tasks. We believe these issues hinder research progress and argue
that current directions will benefit from a clear conceptualization that
includes operationalizing cognitive empathy components. Our main objectives are
to provide insight and guidance on empathy conceptualization for NLP research
objectives and to encourage researchers to pursue the overlooked opportunities
in this area, highly relevant, e.g., for clinical and educational sectors.",None,-1
1843786d-ce8e-4dda-838e-437fbd96ee89,SCALES: From Fairness Principles to Constrained Decision-Making,0.0704519,"This paper proposes SCALES, a general framework that translates
well-established fairness principles into a common representation based on the
Constraint Markov Decision Process (CMDP). With the help of causal language,
our framework can place constraints on both the procedure of decision making
(procedural fairness) as well as the outcomes resulting from decisions (outcome
fairness). Specifically, we show that well-known fairness principles can be
encoded either as a utility component, a non-causal component, or a causal
component in a SCALES-CMDP. We illustrate SCALES using a set of case studies
involving a simulated healthcare scenario and the real-world COMPAS dataset.
Experiments demonstrate that our framework produces fair policies that embody
alternative fairness principles in single-step and sequential decision-making
scenarios.",https://github.com/clear-nus/SCALES,-1
668dbf34-0816-4f0b-b4d4-aa2eb3faecca,Unobserved Local Structures Make Compositional Generalization Hard,0.506526,"While recent work has convincingly showed that sequence-to-sequence models
struggle to generalize to new compositions (termed compositional
generalization), little is known on what makes compositional generalization
hard on a particular test instance. In this work, we investigate what are the
factors that make generalization to certain test instances challenging. We
first substantiate that indeed some examples are more difficult than others by
showing that different models consistently fail or succeed on the same test
instances. Then, we propose a criterion for the difficulty of an example: a
test instance is hard if it contains a local structure that was not observed at
training time. We formulate a simple decision rule based on this criterion and
empirically show it predicts instance-level generalization well across 5
different semantic parsing datasets, substantially better than alternative
decision rules. Last, we show local structures can be leveraged for creating
difficult adversarial compositional splits and also to improve compositional
generalization under limited training budgets by strategically selecting
examples for the training set.",https://github.com/benbogin/unobserved-local-structures,-1
9c32738a-ccd1-4bb1-a9cc-337c4338b54b,The four-fifths rule is not disparate impact: a woeful tale of epistemic trespassing in algorithmic fairness,0.732937,"Computer scientists are trained to create abstractions that simplify and
generalize. However, a premature abstraction that omits crucial contextual
details creates the risk of epistemic trespassing, by falsely asserting its
relevance into other contexts. We study how the field of responsible AI has
created an imperfect synecdoche by abstracting the four-fifths rule (a.k.a. the
4/5 rule or 80% rule), a single part of disparate impact discrimination law,
into the disparate impact metric. This metric incorrectly introduces a new
deontic nuance and new potentials for ethical harms that were absent in the
original 4/5 rule. We also survey how the field has amplified the potential for
harm in codifying the 4/5 rule into popular AI fairness software toolkits. The
harmful erasure of legal nuances is a wake-up call for computer scientists to
self-critically re-evaluate the abstractions they create and use, particularly
in the interdisciplinary field of AI ethics.",None,-1
65a94cf9-0280-4742-a96f-46f4f350064a,SALTED: A Framework for SAlient Long-Tail Translation Error Detection,0.702872,"Traditional machine translation (MT) metrics provide an average measure of
translation quality that is insensitive to the long tail of behavioral problems
in MT. Examples include translation of numbers, physical units, dropped content
and hallucinations. These errors, which occur rarely and unpredictably in
Neural Machine Translation (NMT), greatly undermine the reliability of
state-of-the-art MT systems. Consequently, it is important to have visibility
into these problems during model development. Towards this direction, we
introduce SALTED, a specifications-based framework for behavioral testing of MT
models that provides fine-grained views of salient long-tail errors, permitting
trustworthy visibility into previously invisible problems. At the core of our
approach is the development of high-precision detectors that flag errors (or
alternatively, verify output correctness) between a source sentence and a
system output. We demonstrate that such detectors could be used not just to
identify salient long-tail errors in MT systems, but also for higher-recall
filtering of the training data, fixing targeted errors with model fine-tuning
in NMT and generating novel data for metamorphic testing to elicit further bugs
in models.",https://github.com/pytorch/fairseq/tree/main/examples/wmt21,-1
2107581f-8b1b-4b85-9cd0-c379173c3e18,e-CARE: a New Dataset for Exploring Explainable Causal Reasoning,0.55242,"Understanding causality has vital importance for various Natural Language
Processing (NLP) applications. Beyond the labeled instances, conceptual
explanations of the causality can provide deep understanding of the causal
facts to facilitate the causal reasoning process. However, such explanation
information still remains absent in existing causal reasoning resources. In
this paper, we fill this gap by presenting a human-annotated explainable CAusal
REasoning dataset (e-CARE), which contains over 21K causal reasoning questions,
together with natural language formed explanations of the causal questions.
Experimental results show that generating valid explanations for causal facts
still remains especially challenging for the state-of-the-art models, and the
explanation information can be helpful for promoting the accuracy and stability
of causal reasoning models.",https://github.com/Waste-Wood/e-CARE/,21012
0fd8e45e-d551-43b8-bc5a-a85d2dd1d157,Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud Analysis,0.59808,"Point clouds are characterized by irregularity and unstructuredness, which
pose challenges in efficient data exploitation and discriminative feature
extraction. In this paper, we present an unsupervised deep neural architecture
called Flattening-Net to represent irregular 3D point clouds of arbitrary
geometry and topology as a completely regular 2D point geometry image (PGI)
structure, in which coordinates of spatial points are captured in colors of
image pixels. \mr{Intuitively, Flattening-Net implicitly approximates a locally
smooth 3D-to-2D surface flattening process while effectively preserving
neighborhood consistency.} \mr{As a generic representation modality, PGI
inherently encodes the intrinsic property of the underlying manifold structure
and facilitates surface-style point feature aggregation.} To demonstrate its
potential, we construct a unified learning framework directly operating on PGIs
to achieve \mr{diverse types of high-level and low-level} downstream
applications driven by specific task networks, including classification,
segmentation, reconstruction, and upsampling. Extensive experiments demonstrate
that our methods perform favorably against the current state-of-the-art
competitors. We will make the code and data publicly available at
https://github.com/keeganhk/Flattening-Net.",https://github.com/keeganhk/Flattening-Net,9714
24d3d2b9-42bf-47bc-99aa-660ed981f2f2,Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings,0.80125,"Although contextualized embeddings generated from large-scale pre-trained
models perform well in many tasks, traditional static embeddings (e.g.,
Skip-gram, Word2Vec) still play an important role in low-resource and
lightweight settings due to their low computational cost, ease of deployment,
and stability. In this paper, we aim to improve word embeddings by 1)
incorporating more contextual information from existing pre-trained models into
the Skip-gram framework, which we call Context-to-Vec; 2) proposing a
post-processing retrofitting method for static embeddings independent of
training by employing priori synonym knowledge and weighted vector
distribution. Through extrinsic and intrinsic tasks, our methods are well
proven to outperform the baselines by a large margin.",https://github.com/binbinjiang/Context2Vector,70015
d20a1f56-d851-4377-9fc3-86fcb9ef7210,Re-Examining Calibration: The Case of Question Answering,0.701852,"For users to trust model predictions, they need to understand model outputs,
particularly their confidence - calibration aims to adjust (calibrate) models'
confidence to match expected accuracy. We argue that the traditional
calibration evaluation does not promote effective calibrations: for example, it
can encourage always assigning a mediocre confidence score to all predictions,
which does not help users distinguish correct predictions from wrong ones.
Building on those observations, we propose a new calibration metric, MacroCE,
that better captures whether the model assigns low confidence to wrong
predictions and high confidence to correct predictions. Focusing on the
practical application of open-domain question answering, we examine
conventional calibration methods applied on the widely-used retriever-reader
pipeline, all of which do not bring significant gains under our new MacroCE
metric. Toward better calibration, we propose a new calibration method
(ConsCal) that uses not just final model predictions but whether multiple model
checkpoints make consistent predictions. Altogether, we provide an alternative
view of calibration along with a new metric, re-evaluation of existing
calibration methods on our metric, and proposal of a more effective calibration
method.",https://github.com/NoviScl/calibrateQA,-1
03d1a0ed-f4e1-4914-9cda-17ab07fae04e,Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection,0.270472,"Hate speech classifiers exhibit substantial performance degradation when
evaluated on datasets different from the source. This is due to learning
spurious correlations between words that are not necessarily relevant to
hateful language, and hate speech labels from the training corpus. Previous
work has attempted to mitigate this problem by regularizing specific terms from
pre-defined static dictionaries. While this has been demonstrated to improve
the generalizability of classifiers, the coverage of such methods is limited
and the dictionaries require regular manual updates from human experts. In this
paper, we propose to automatically identify and reduce spurious correlations
using attribution methods with dynamic refinement of the list of terms that
need to be regularized during training. Our approach is flexible and improves
the cross-corpora performance over previous work independently and in
combination with pre-defined dictionaries.",https://github.com/tbose20/D-Ref,-1
fd6cc292-9c7d-42c9-bbd7-8f1233fc4b8d,CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command Recognition,0.638463,"With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.",https://github.com/HLTCHKUST/CI-AVSR,-1
2ce4fa10-7507-4b12-b50a-56945a31b482,Data-Efficient Backdoor Attacks,0.829504,"Recent studies have proven that deep neural networks are vulnerable to
backdoor attacks. Specifically, by mixing a small number of poisoned samples
into the training set, the behavior of the trained model can be maliciously
controlled. Existing attack methods construct such adversaries by randomly
selecting some clean data from the benign set and then embedding a trigger into
them. However, this selection strategy ignores the fact that each poisoned
sample contributes inequally to the backdoor injection, which reduces the
efficiency of poisoning. In this paper, we formulate improving the poisoned
data efficiency by the selection as an optimization problem and propose a
Filtering-and-Updating Strategy (FUS) to solve it. The experimental results on
CIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the
same attack success rate can be achieved with only 47% to 75% of the poisoned
sample volume compared to the random selection strategy. More importantly, the
adversaries selected according to one setting can generalize well to other
settings, exhibiting strong transferability. The prototype code of our method
is now available at https://github.com/xpf/Data-Efficient-Backdoor-Attacks.",https://github.com/xpf/Data-Efcient-Backdoor-Attacks,175520
257ff6ce-0d14-499e-bfd8-a51cf9a52868,DeepVoxNet2: Yet another CNN framework,0.0293552,"We know that both the CNN mapping function and the sampling scheme are of
paramount importance for CNN-based image analysis. It is clear that both
functions operate in the same space, with an image axis $\mathcal{I}$ and a
feature axis $\mathcal{F}$. Remarkably, we found that no frameworks existed
that unified the two and kept track of the spatial origin of the data
automatically. Based on our own practical experience, we found the latter to
often result in complex coding and pipelines that are difficult to exchange.
This article introduces our framework for 1, 2 or 3D image classification or
segmentation: DeepVoxNet2 (DVN2). This article serves as an interactive
tutorial, and a pre-compiled version, including the outputs of the code blocks,
can be found online in the public DVN2 repository. This tutorial uses data from
the multimodal Brain Tumor Image Segmentation Benchmark (BRATS) of 2018 to show
an example of a 3D segmentation pipeline.",https://github.com/JeroenBertels/deepvoxnet2,24508
9887af80-ed09-4d1c-90ed-2e0628abd3fc,Code Comment Inconsistency Detection with BERT and Longformer,0.161781,"Comments, or natural language descriptions of source code, are standard
practice among software developers. By communicating important aspects of the
code such as functionality and usage, comments help with software project
maintenance. However, when the code is modified without an accompanying
correction to the comment, an inconsistency between the comment and code can
arise, which opens up the possibility for developer confusion and bugs. In this
paper, we propose two models based on BERT (Devlin et al., 2019) and Longformer
(Beltagy et al., 2020) to detect such inconsistencies in a natural language
inference (NLI) context. Through an evaluation on a previously established
corpus of comment-method pairs both during and after code changes, we
demonstrate that our models outperform multiple baselines and yield comparable
results to the state-of-the-art models that exclude linguistic and lexical
features. We further discuss ideas for future research in using pretrained
language models for both inconsistency detection and automatic comment
updating.",https://github.com/theo2023/coco-bert-longformer,-1
be33605c-2174-4bf5-93ba-077c60b0ba70,Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions,0.638927,"Current QA systems can generate reasonable-sounding yet false answers without
explanation or evidence for the generated answer, which is especially
problematic when humans cannot readily check the model's answers. This presents
a challenge for building trust in machine learning systems. We take inspiration
from real-world situations where difficult questions are answered by
considering opposing sides (see Irving et al., 2018). For multiple-choice QA
examples, we build a dataset of single arguments for both a correct and
incorrect answer option in a debate-style set-up as an initial step in training
models to produce explanations for two candidate answers. We use long contexts
-- humans familiar with the context write convincing explanations for
pre-selected correct and incorrect answers, and we test if those explanations
allow humans who have not read the full context to more accurately determine
the correct answer. We do not find that explanations in our set-up improve
human accuracy, but a baseline condition shows that providing human-selected
text snippets does improve accuracy. We use these findings to suggest ways of
improving the debate set up for future data collection efforts.",https://github.com/nyu-mll/single_turn_debate,-1
75b543cd-4556-45ac-8937-677e170e4096,SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution,0.20924,"Dynamic convolution achieves better performance for efficient CNNs at the
cost of negligible FLOPs increase. However, the performance increase can not
match the significantly expanded number of parameters, which is the main
bottleneck in real-world applications. Contrastively, mask-based unstructured
pruning obtains a lightweight network by removing redundancy in the heavy
network. In this paper, we propose a new framework, \textbf{Sparse Dynamic
Convolution} (\textsc{SD-Conv}), to naturally integrate these two paths such
that it can inherit the advantage of dynamic mechanism and sparsity. We first
design a binary mask derived from a learnable threshold to prune static
kernels, significantly reducing the parameters and computational cost but
achieving higher performance in Imagenet-1K. We further transfer pretrained
models into a variety of downstream tasks, showing consistently better results
than baselines. We hope our SD-Conv could be an efficient alternative to
conventional dynamic convolutions.",https://github.com/weiaicunzai/pytorch-cifar100,-1
24eb2ffa-2aa0-4f87-bf65-df43f772f37d,Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT,0.0724676,"Transformer-based pre-trained models, such as BERT, have shown extraordinary
success in achieving state-of-the-art results in many natural language
processing applications. However, deploying these models can be prohibitively
costly, as the standard self-attention mechanism of the Transformer suffers
from quadratic computational cost in the input sequence length. To confront
this, we propose FCA, a fine- and coarse-granularity hybrid self-attention that
reduces the computation cost through progressively shortening the computational
sequence length in self-attention. Specifically, FCA conducts an
attention-based scoring strategy to determine the informativeness of tokens at
each layer. Then, the informative tokens serve as the fine-granularity
computing units in self-attention and the uninformative tokens are replaced
with one or several clusters as the coarse-granularity computing units in
self-attention. Experiments on GLUE and RACE datasets show that BERT with FCA
achieves 2x reduction in FLOPs over original BERT with <1% loss in accuracy. We
show that FCA offers a significantly better trade-off between accuracy and
FLOPs compared to prior methods.",https://github.com/pierre-zhao/FCA-BERT,-1
b717b0bf-80bf-4a34-b9ba-9565711b0930,Progressive with Purpose: Guiding Progressive Inpainting DNNs through Context and Structure,0.121429,"The advent of deep learning in the past decade has significantly helped
advance image inpainting. Although achieving promising performance, deep
learning-based inpainting algorithms still struggle from the distortion caused
by the fusion of structural and contextual features, which are commonly
obtained from, respectively, deep and shallow layers of a convolutional
encoder. Motivated by this observation, we propose a novel progressive
inpainting network that maintains the structural and contextual integrity of a
processed image. More specifically, inspired by the Gaussian and Laplacian
pyramids, the core of the proposed network is a feature extraction module named
GLE. Stacking GLE modules enables the network to extract image features from
different image frequency components. This ability is important to maintain
structural and contextual integrity, for high frequency components correspond
to structural information while low frequency components correspond to
contextual information. The proposed network utilizes the GLE features to
progressively fill in missing regions in a corrupted image in an iterative
manner. Our benchmarking experiments demonstrate that the proposed method
achieves clear improvement in performance over many state-of-the-art inpainting
algorithms.",None,-1
e5d5771b-0c43-491d-ab68-55876206128c,Constrained Dynamic Movement Primitives for Safe Learning of Motor Skills,0.744271,"Dynamic movement primitives are widely used for learning skills which can be
demonstrated to a robot by a skilled human or controller. While their
generalization capabilities and simple formulation make them very appealing to
use, they possess no strong guarantees to satisfy operational safety
constraints for a task. In this paper, we present constrained dynamic movement
primitives (CDMP) which can allow for constraint satisfaction in the robot
workspace. We present a formulation of a non-linear optimization to perturb the
DMP forcing weights regressed by locally-weighted regression to admit a Zeroing
Barrier Function (ZBF), which certifies workspace constraint satisfaction. We
demonstrate the proposed CDMP under different constraints on the end-effector
movement such as obstacle avoidance and workspace constraints on a physical
robot. A video showing the implementation of the proposed algorithm using
different manipulators in different environments could be found here
https://youtu.be/hJegJJkJfys.",None,-1
94db08d1-35be-4b42-bbe4-17370f526064,A Structure-guided Effective and Temporal-lag Connectivity Network for Revealing Brain Disorder Mechanisms,0.351935,"Brain network provides important insights for the diagnosis of many brain
disorders, and how to effectively model the brain structure has become one of
the core issues in the domain of brain imaging analysis. Recently, various
computational methods have been proposed to estimate the causal relationship
(i.e., effective connectivity) between brain regions. Compared with traditional
correlation-based methods, effective connectivity can provide the direction of
information flow, which may provide additional information for the diagnosis of
brain diseases. However, existing methods either ignore the fact that there is
a temporal-lag in the information transmission across brain regions, or simply
set the temporal-lag value between all brain regions to a fixed value. To
overcome these issues, we design an effective temporal-lag neural network
(termed ETLN) to simultaneously infer the causal relationships and the
temporal-lag values between brain regions, which can be trained in an
end-to-end manner. In addition, we also introduce three mechanisms to better
guide the modeling of brain networks. The evaluation results on the Alzheimer's
Disease Neuroimaging Initiative (ADNI) database demonstrate the effectiveness
of the proposed method.",None,-1
7d88d0c9-693a-4136-98da-1963a547e18c,Safe Reinforcement Learning via Shielding under Partial Observability,0.675844,"Safe exploration is a common problem in reinforcement learning (RL) that aims
to prevent agents from making disastrous decisions while exploring their
environment. A family of approaches to this problem assume domain knowledge in
the form of a (partial) model of this environment to decide upon the safety of
an action. A so-called shield forces the RL agent to select only safe actions.
However, for adoption in various applications, one must look beyond enforcing
safety and also ensure the applicability of RL with good performance. We extend
the applicability of shields via tight integration with state-of-the-art deep
RL, and provide an extensive, empirical study in challenging, sparse-reward
environments under partial observability. We show that a carefully integrated
shield ensures safety and can improve the convergence rate and final
performance of RL agents. We furthermore show that a shield can be used to
bootstrap state-of-the-art RL agents: they remain safe after initial learning
in a shielded setting, allowing us to disable a potentially too conservative
shield eventually.",https://github.com/tensorflow/agents,-1
b7b9cd66-3ecc-4b96-af3d-8c579b63bb86,Instance-Specific Image Goal Navigation: Training Embodied Agents to Find Object Instances,0.797243,"We consider the problem of embodied visual navigation given an image-goal
(ImageNav) where an agent is initialized in an unfamiliar environment and
tasked with navigating to a location 'described' by an image. Unlike related
navigation tasks, ImageNav does not have a standardized task definition which
makes comparison across methods difficult. Further, existing formulations have
two problematic properties; (1) image-goals are sampled from random locations
which can lead to ambiguity (e.g., looking at walls), and (2) image-goals match
the camera specification and embodiment of the agent; this rigidity is limiting
when considering user-driven downstream applications. We present the
Instance-specific ImageNav task (InstanceImageNav) to address these
limitations. Specifically, the goal image is 'focused' on some particular
object instance in the scene and is taken with camera parameters independent of
the agent. We instantiate InstanceImageNav in the Habitat Simulator using
scenes from the Habitat-Matterport3D dataset (HM3D) and release a standardized
benchmark to measure community progress.",https://github.com/facebookresearch/habitat-lab,-1
3f8a3ec3-3868-4d61-bd61-4477736f498c,TASTEset -- Recipe Dataset and Food Entities Recognition Benchmark,0.310313,"Food Computing is currently a fast-growing field of research. Natural
language processing (NLP) is also increasingly essential in this field,
especially for recognising food entities. However, there are still only a few
well-defined tasks that serve as benchmarks for solutions in this area. We
introduce a new dataset -- called \textit{TASTEset} -- to bridge this gap. In
this dataset, Named Entity Recognition (NER) models are expected to find or
infer various types of entities helpful in processing recipes, e.g.~food
products, quantities and their units, names of cooking processes, physical
quality of ingredients, their purpose, taste.
  The dataset consists of 700 recipes with more than 13,000 entities to
extract. We provide a few state-of-the-art baselines of named entity
recognition models, which show that our dataset poses a solid challenge to
existing models. The best model achieved, on average, 0.95 $F_1$ score,
depending on the entity type -- from 0.781 to 0.982. We share the dataset and
the task to encourage progress on more in-depth and complex information
extraction from recipes.",https://github.com/taisti/TASTEset,-1
34bba560-6413-4dd8-b4e8-5b755684977b,Grounding Aleatoric Uncertainty for Unsupervised Environment Design,0.492585,"Adaptive curricula in reinforcement learning (RL) have proven effective for
producing policies robust to discrepancies between the train and test
environment. Recently, the Unsupervised Environment Design (UED) framework
generalized RL curricula to generating sequences of entire environments,
leading to new methods with robust minimax regret properties. Problematically,
in partially-observable or stochastic settings, optimal policies may depend on
the ground-truth distribution over aleatoric parameters of the environment in
the intended deployment setting, while curriculum learning necessarily shifts
the training distribution. We formalize this phenomenon as curriculum-induced
covariate shift (CICS), and describe how its occurrence in aleatoric parameters
can lead to suboptimal policies. Directly sampling these parameters from the
ground-truth distribution avoids the issue, but thwarts curriculum learning. We
propose SAMPLR, a minimax regret UED method that optimizes the ground-truth
utility function, even when the underlying training data is biased due to CICS.
We prove, and validate on challenging domains, that our approach preserves
optimality under the ground-truth distribution, while promoting robustness
across the full range of environment settings.",https://github.com/facebookresearch/nle,18829
6d420c40-9a2e-4893-af98-fb3c57253ed4,MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages,0.424825,"We present the results of the Workshop on Multilingual Information Access
(MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question
answering (QA) systems in 16 typologically diverse languages. In this task, we
adapted two large-scale cross-lingual open-retrieval QA datasets in 14
typologically diverse languages, and newly annotated open-retrieval QA data in
2 underrepresented languages: Tagalog and Tamil. Four teams submitted their
systems. The best system leveraging iteratively mined diverse negative examples
and larger pretrained models achieves 32.2 F1, outperforming our baseline by
4.5 points. The second best system uses entity-aware contextualized
representations for document retrieval, and achieves significant improvements
in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores.",https://github.com/mia-workshop/MIA-Shared-Task-2022,-1
19dd9f56-ab0e-440a-ad45-34504d108cfb,Atari-5: Distilling the Arcade Learning Environment down to Five Games,0.791756,"The Arcade Learning Environment (ALE) has become an essential benchmark for
assessing the performance of reinforcement learning algorithms. However, the
computational cost of generating results on the entire 57-game dataset limits
ALE's use and makes the reproducibility of many results infeasible. We propose
a novel solution to this problem in the form of a principled methodology for
selecting small but representative subsets of environments within a benchmark
suite. We applied our method to identify a subset of five ALE games, called
Atari-5, which produces 57-game median score estimates within 10% of their true
values. Extending the subset to 10-games recovers 80% of the variance for
log-scores for all games within the 57-game set. We show this level of
compression is possible due to a high degree of correlation between many of the
games in ALE.",https://github.com/maitchison/Atari-5,10193
359dc1c1-2973-4600-a89c-17f72c7c7891,Towards Two-view 6D Object Pose Estimation: A Comparative Study on Fusion Strategy,0.110275,"Current RGB-based 6D object pose estimation methods have achieved noticeable
performance on datasets and real world applications. However, predicting 6D
pose from single 2D image features is susceptible to disturbance from changing
of environment and textureless or resemblant object surfaces. Hence, RGB-based
methods generally achieve less competitive results than RGBD-based methods,
which deploy both image features and 3D structure features. To narrow down this
performance gap, this paper proposes a framework for 6D object pose estimation
that learns implicit 3D information from 2 RGB images. Combining the learned 3D
information and 2D image features, we establish more stable correspondence
between the scene and the object models. To seek for the methods best utilizing
3D information from RGB inputs, we conduct an investigation on three different
approaches, including Early- Fusion, Mid-Fusion, and Late-Fusion. We ascertain
the Mid- Fusion approach is the best approach to restore the most precise 3D
keypoints useful for object pose estimation. The experiments show that our
method outperforms state-of-the-art RGB-based methods, and achieves comparable
results with RGBD-based methods.",None,19514
64117a3a-253e-45d7-a83c-f566837719bc,Improving Few-Shot Part Segmentation using Coarse Supervision,0.335946,"A significant bottleneck in training deep networks for part segmentation is
the cost of obtaining detailed annotations. We propose a framework to exploit
coarse labels such as figure-ground masks and keypoint locations that are
readily available for some categories to improve part segmentation models. A
key challenge is that these annotations were collected for different tasks and
with different labeling styles and cannot be readily mapped to the part labels.
To this end, we propose to jointly learn the dependencies between labeling
styles and the part segmentation model, allowing us to utilize supervision from
diverse labels. To evaluate our approach we develop a benchmark on the
Caltech-UCSD birds and OID Aircraft dataset. Our approach outperforms baselines
based on multi-task learning, semi-supervised learning, and competitive methods
relying on loss functions manually designed to exploit sparse-supervision.",None,-1
9e76accc-d852-41d5-97a3-50199e15e4d5,Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue,0.169317,"Embodied dialogue instruction following requires an agent to complete a
complex sequence of tasks from a natural language exchange. The recent
introduction of benchmarks (Padmakumar et al., 2022) raises the question of how
best to train and evaluate models for this multi-turn, multi-agent,
long-horizon task. This paper contributes to that conversation, by arguing that
imitation learning (IL) and related low-level metrics are actually misleading
and do not align with the goals of embodied dialogue research and may hinder
progress. We provide empirical comparisons of metrics, analysis of three
models, and make suggestions for how the field might best progress. First, we
observe that models trained with IL take spurious actions during evaluation.
Second, we find that existing models fail to ground query utterances, which are
essential for task completion. Third, we argue evaluation should focus on
higher-level semantic goals.",https://github.com/soyeonm/TEACh_FILM,-1
c91875e8-a2aa-4d9f-9e4a-eef807adf169,Learning to Ignore Adversarial Attacks,0.0602004,"Despite the strong performance of current NLP models, they can be brittle
against adversarial attacks. To enable effective learning against adversarial
inputs, we introduce the use of rationale models that can explicitly learn to
ignore attack tokens. We find that the rationale models can successfully ignore
over 90% of attack tokens. This approach leads to consistent sizable
improvements ($\sim$10%) over baseline models in robustness on three datasets
for both BERT and RoBERTa, and also reliably outperforms data augmentation with
adversarial examples alone. In many cases, we find that our method is able to
close the gap between model performance on a clean test set and an attacked
test set and hence reduce the effect of adversarial attacks.",https://github.com/ChicagoHAI/rationalization-robustness,-1
b4855bcd-8c25-49e7-a740-d4f63d740254,Learning Probabilities of Causation from Finite Population Data,0.803088,"This paper deals with the problem of learning the probabilities of causation
of subpopulations given finite population data. The tight bounds of three basic
probabilities of causation, the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN),
were derived by Tian and Pearl. However, obtaining the bounds for each
subpopulation requires experimental and observational distributions of each
subpopulation, which is usually impractical to estimate given finite population
data. We propose a machine learning model that helps to learn the bounds of the
probabilities of causation for subpopulations given finite population data. We
further show by a simulated study that the machine learning model is able to
learn the bounds of PNS for 32768 subpopulations with only knowing roughly 500
of them from the finite population data.",None,-1
06676330-c1cb-49ed-8134-8fd33766affe,Cross-modal Contrastive Learning for Speech Translation,0.93309,"How can we learn unified representations for spoken utterances and their
written text? Learning similar representations for semantically similar speech
and text is important for speech translation. To this end, we propose ConST, a
cross-modal contrastive learning method for end-to-end speech-to-text
translation. We evaluate ConST and a variety of previous baselines on a popular
benchmark MuST-C. Experiments show that the proposed ConST consistently
outperforms the previous methods on, and achieves an average BLEU of 29.4. The
analysis further verifies that ConST indeed closes the representation gap of
different modalities -- its learned representation improves the accuracy of
cross-modal speech-text retrieval from 4% to 88%. Code and models are available
at https://github.com/ReneeYe/ConST.",https://github.com/ReneeYe/ConST,3502
a5524c62-8604-460b-a4d2-584f8dba2d52,Panini-Net: GAN Prior Based Degradation-Aware Feature Interpolation for Face Restoration,0.380888,"Emerging high-quality face restoration (FR) methods often utilize pre-trained
GAN models (\textit{i.e.}, StyleGAN2) as GAN Prior. However, these methods
usually struggle to balance realness and fidelity when facing various
degradation levels. Besides, there is still a noticeable visual quality gap
compared with pre-trained GAN models. In this paper, we propose a novel GAN
Prior based degradation-aware feature interpolation network, dubbed Panini-Net,
for FR tasks by explicitly learning the abstract representations to distinguish
various degradations. Specifically, an unsupervised degradation representation
learning (UDRL) strategy is first developed to extract degradation
representations (DR) of the input degraded images. Then, a degradation-aware
feature interpolation (DAFI) module is proposed to dynamically fuse the two
types of informative features (\textit{i.e.}, features from input images and
features from GAN Prior) with flexible adaption to various degradations based
on DR. Ablation studies reveal the working mechanism of DAFI and its potential
for editable FR. Extensive experiments demonstrate that our Panini-Net achieves
state-of-the-art performance for multi-degradation face restoration and face
super-resolution. The source code is available at
https://github.com/jianzhangcs/panini.",https://github.com/jianzhangcs/panini,-1
fb3eb02f-e7d2-4f6b-b93b-d8f4e2b314da,Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network,0.236197,"In recent years, with the development of computing resources and LiDAR, point
cloud semantic segmentation has attracted many researchers. For the sparsity of
point clouds, although there is already a way to deal with sparse convolution,
multi-scale features are not considered. In this letter, we propose a feature
extraction module based on multi-scale sparse convolution and a feature
selection module based on channel attention and build a point cloud
segmentation network framework based on this. By introducing multi-scale sparse
convolution, the network could capture richer feature information based on
convolution kernels with different sizes, improving the segmentation result of
point cloud segmentation. Experimental results on Stanford large-scale 3-D
Indoor Spaces(S3DIS) dataset and outdoor dataset(SemanticKITTI), demonstrate
effectiveness and superiority of the proposed mothod.",None,-1
6dbda123-abbf-4e26-8208-2df98a184b18,SVG Vector Font Generation for Chinese Characters with Transformer,0.615773,"Designing fonts for Chinese characters is highly labor-intensive and
time-consuming. While the latest methods successfully generate the English
alphabet vector font, despite the high demand for automatic font generation,
Chinese vector font generation has been an unsolved problem owing to its
complex shape and numerous characters. This study addressed the problem of
automatically generating Chinese vector fonts from only a single style and
content reference. We proposed a novel network architecture with Transformer
and loss functions to capture structural features without differentiable
rendering. Although the dataset range was still limited to the sans-serif
family, we successfully generated the Chinese vector font for the first time
using the proposed method.",None,-1
b3383a5a-d822-45ce-81f9-efd777d3247d,Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks,0.161703,"The increasing importance of both deep neural networks (DNNs) and cloud
services for training them means that bad actors have more incentive and
opportunity to insert backdoors to alter the behavior of trained models. In
this paper, we introduce a novel method for backdoor detection that extracts
features from pre-trained DNN's weights using independent vector analysis (IVA)
followed by a machine learning classifier. In comparison to other detection
techniques, this has a number of benefits, such as not requiring any training
data, being applicable across domains, operating with a wide range of network
architectures, not assuming the nature of the triggers used to change network
behavior, and being highly scalable. We discuss the detection pipeline, and
then demonstrate the results on two computer vision datasets regarding image
classification and object detection. Our method outperforms the competing
algorithms in terms of efficiency and is more accurate, helping to ensure the
safe application of deep learning and AI.",None,-1
7cde8ef9-c3e9-4b16-814a-1d995491df31,Exploring Temporal Information Dynamics in Spiking Neural Networks,0.920185,"Most existing Spiking Neural Network (SNN) works state that SNNs may utilize
temporal information dynamics of spikes. However, an explicit analysis of
temporal information dynamics is still missing. In this paper, we ask several
important questions for providing a fundamental understanding of SNNs: What are
temporal information dynamics inside SNNs? How can we measure the temporal
information dynamics? How do the temporal information dynamics affect the
overall learning performance? To answer these questions, we estimate the Fisher
Information of the weights to measure the distribution of temporal information
during training in an empirical manner. Surprisingly, as training goes on,
Fisher information starts to concentrate in the early timesteps. After
training, we observe that information becomes highly concentrated in earlier
few timesteps, a phenomenon we refer to as temporal information concentration.
We observe that the temporal information concentration phenomenon is a common
learning feature of SNNs by conducting extensive experiments on various
configurations such as architecture, dataset, optimization strategy, time
constant, and timesteps. Furthermore, to reveal how temporal information
concentration affects the performance of SNNs, we design a loss function to
change the trend of temporal information. We find that temporal information
concentration is crucial to building a robust SNN but has little effect on
classification accuracy. Finally, we propose an efficient iterative pruning
method based on our observation on temporal information concentration. Code is
available at
https://github.com/Intelligent-Computing-Lab-Yale/Exploring-Temporal-Information-Dynamics-in-Spiking-Neural-Networks.",https://github.com/Intelligent-Computing-Lab-Yale/Exploring-Temporal-Information-Dynamics-in-Spiking-Neural-Networks,-1
b2f6e0a7-cc7a-4586-9fd4-cb5e7a38a264,Real-time Online Video Detection with Temporal Smoothing Transformers,0.749344,"Streaming video recognition reasons about objects and their actions in every
frame of a video. A good streaming recognition model captures both long-term
dynamics and short-term changes of video. Unfortunately, in most existing
methods, the computational complexity grows linearly or quadratically with the
length of the considered dynamics. This issue is particularly pronounced in
transformer-based architectures. To address this issue, we reformulate the
cross-attention in a video transformer through the lens of kernel and apply two
kinds of temporal smoothing kernel: A box kernel or a Laplace kernel. The
resulting streaming attention reuses much of the computation from frame to
frame, and only requires a constant time update each frame. Based on this idea,
we build TeSTra, a Temporal Smoothing Transformer, that takes in arbitrarily
long inputs with constant caching and computing overhead. Specifically, it runs
$6\times$ faster than equivalent sliding-window based transformers with 2,048
frames in a streaming setting. Furthermore, thanks to the increased temporal
span, TeSTra achieves state-of-the-art results on THUMOS'14 and
EPIC-Kitchen-100, two standard online action detection and action anticipation
datasets. A real-time version of TeSTra outperforms all but one prior
approaches on the THUMOS'14 dataset.",https://github.com/zhaoyue-zephyrus/TeSTra/,-1
82f203d1-dd19-4113-84dd-0b9d5fbb69b7,A Simple Temporal Information Matching Mechanism for Entity Alignment Between Temporal Knowledge Graphs,0.420242,"Entity alignment (EA) aims to find entities in different knowledge graphs
(KGs) that refer to the same object in the real world. Recent studies
incorporate temporal information to augment the representations of KGs. The
existing methods for EA between temporal KGs (TKGs) utilize a time-aware
attention mechanism to incorporate relational and temporal information into
entity embeddings. The approaches outperform the previous methods by using
temporal information. However, we believe that it is not necessary to learn the
embeddings of temporal information in KGs since most TKGs have uniform temporal
representations. Therefore, we propose a simple graph neural network (GNN)
model combined with a temporal information matching mechanism, which achieves
better performance with less time and fewer parameters. Furthermore, since
alignment seeds are difficult to label in real-world applications, we also
propose a method to generate unsupervised alignment seeds via the temporal
information of TKG. Extensive experiments on public datasets indicate that our
supervised method significantly outperforms the previous methods and the
unsupervised one has competitive performance.",https://github.com/lcai2/STEA,-1
1a6c0a05-ef70-407d-b180-3475b62d859a,The Pump Scheduling Problem: A Real-World Scenario for Reinforcement Learning,0.0325333,"Deep Reinforcement Learning (DRL) has achieved remarkable success in
scenarios such as games and has emerged as a potential solution for control
tasks. That is due to its ability to leverage scalability and handle complex
dynamics. However, few works have targeted environments grounded in real-world
settings. Indeed, real-world scenarios can be challenging, especially when
faced with the high dimensionality of the state space and unknown reward
function. We release a testbed consisting of an environment simulator and
demonstrations of human operation concerning pump scheduling of a real-world
water distribution facility to facilitate research. The pump scheduling problem
can be viewed as a decision process to decide when to operate pumps to supply
water while limiting electricity consumption and meeting system constraints. To
provide a starting point, we release a well-documented codebase, present an
overview of some challenges that can be addressed and provide a baseline
representation of the problem. The code and dataset are available at
https://gitlab.com/hdonancio/pumpscheduling.",https://gitlab.com/hdonancio/pumpscheduling,-1
9e237042-f7a8-48e5-98cf-82258e1d9657,Low Complexity Channel estimation with Neural Network Solutions,0.542754,"Research on machine learning for channel estimation, especially neural
network solutions for wireless communications, is attracting significant
current interest. This is because conventional methods cannot meet the present
demands of the high speed communication. In the paper, we deploy a general
residual convolutional neural network to achieve channel estimation for the
orthogonal frequency-division multiplexing (OFDM) signals in a downlink
scenario. Our method also deploys a simple interpolation layer to replace the
transposed convolutional layer used in other networks to reduce the computation
cost. The proposed method is more easily adapted to different pilot patterns
and packet sizes. Compared with other deep learning methods for channel
estimation, our results for 3GPP channel models suggest improved mean squared
error performance for our approach.",None,-1
fb57a366-bd97-4cef-b0c8-7b009a579e8e,B-SMART: A Reference Architecture for Artificially Intelligent Autonomic Smart Buildings,0.884224,"The pervasive application of artificial intelligence and machine learning
algorithms is transforming many industries and aspects of the human experience.
One very important industry trend is the move to convert existing human
dwellings to smart buildings, and to create new smart buildings. Smart
buildings aim to mitigate climate change by reducing energy consumption and
associated carbon emissions. To accomplish this, they leverage artificial
intelligence, big data, and machine learning algorithms to learn and optimize
system performance. These fields of research are currently very rapidly
evolving and advancing, but there has been very little guidance to help
engineers and architects working on smart buildings apply artificial
intelligence algorithms and technologies in a systematic and effective manner.
In this paper we present B-SMART: the first reference architecture for
autonomic smart buildings. B-SMART facilitates the application of artificial
intelligence techniques and technologies to smart buildings by decoupling
conceptually distinct layers of functionality and organizing them into an
autonomic control loop. We also present a case study illustrating how B-SMART
can be applied to accelerate the introduction of artificial intelligence into
an existing smart building.",None,-1
56338680-8997-4aee-ad56-746720bda879,Modeling Information Change in Science Communication with Semantically Matched Paraphrases,0.797138,"Whether the media faithfully communicate scientific information has long been
a core issue to the science community. Automatically identifying paraphrased
scientific findings could enable large-scale tracking and analysis of
information changes in the science communication process, but this requires
systems to understand the similarity between scientific information across
multiple domains. To this end, we present the SCIENTIFIC PARAPHRASE AND
INFORMATION CHANGE DATASET (SPICED), the first paraphrase dataset of scientific
findings annotated for degree of information change. SPICED contains 6,000
scientific finding pairs extracted from news stories, social media discussions,
and full texts of original papers. We demonstrate that SPICED poses a
challenging task and that models trained on SPICED improve downstream
performance on evidence retrieval for fact checking of real-world scientific
claims. Finally, we show that models trained on SPICED can reveal large-scale
trends in the degrees to which people and organizations faithfully communicate
new scientific findings. Data, code, and pre-trained models are available at
http://www.copenlu.com/publication/2022_emnlp_wright/.",http://www.copenlu.com/publication/2022_emnlp_wright/,-1
1bc83f1e-99d1-44c8-8f9c-6d63afed7f1b,Booster-SHOT: Boosting Stacked Homography Transformations for Multiview Pedestrian Detection with Attention,0.205477,"Improving multi-view aggregation is integral for multi-view pedestrian
detection, which aims to obtain a bird's-eye-view pedestrian occupancy map from
images captured through a set of calibrated cameras. Inspired by the success of
attention modules for deep neural networks, we first propose a Homography
Attention Module (HAM) which is shown to boost the performance of existing
end-to-end multiview detection approaches by utilizing a novel channel gate and
spatial gate. Additionally, we propose Booster-SHOT, an end-to-end
convolutional approach to multiview pedestrian detection incorporating our
proposed HAM as well as elements from previous approaches such as view-coherent
augmentation or stacked homography transformations. Booster-SHOT achieves 92.9%
and 94.2% for MODA on Wildtrack and MultiviewX respectively, outperforming the
state-of-the-art by 1.4% on Wildtrack and 0.5% on MultiviewX, achieving
state-of-the-art performance overall for standard evaluation metrics used in
multi-view pedestrian detection.",None,188
335bfdb7-df7d-492b-a80b-eb700742ea44,"Markov categories, causal theories, and the do-calculus",0.07146,"We give a category-theoretic treatment of causal models that formalizes the
syntax for causal reasoning over a directed acyclic graph (DAG) by associating
a free Markov category with the DAG in a canonical way. This framework enables
us to define and study important concepts in causal reasoning from an abstract
and ""purely causal"" point of view, such as causal independence/separation,
causal conditionals, and decomposition of intervention effects. Our results
regarding these concepts abstract away from the details of the commonly adopted
causal models such as (recursive) structural equation models or causal Bayesian
networks. They are therefore more widely applicable and in a way conceptually
clearer. Our results are also intimately related to Judea Pearl's celebrated
do-calculus, and yield a syntactic version of a core part of the calculus that
is inherited in all causal models. In particular, it induces a simpler and
specialized version of Pearl's do-calculus in the context of causal Bayesian
networks, which we show is as strong as the full version.",None,-1
241d4961-104d-4803-8c80-1123f40cadbb,M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction,0.981273,"Predicting future motions of road participants is an important task for
driving autonomously in urban scenes. Existing models excel at predicting
marginal trajectories for single agents, yet it remains an open question to
jointly predict scene compliant trajectories over multiple agents. The
challenge is due to exponentially increasing prediction space as a function of
the number of agents. In this work, we exploit the underlying relations between
interacting agents and decouple the joint prediction problem into marginal
prediction problems. Our proposed approach M2I first classifies interacting
agents as pairs of influencers and reactors, and then leverages a marginal
prediction model and a conditional prediction model to predict trajectories for
the influencers and reactors, respectively. The predictions from interacting
agents are combined and selected according to their joint likelihoods.
Experiments show that our simple but effective approach achieves
state-of-the-art performance on the Waymo Open Motion Dataset interactive
prediction benchmark.",None,18787
7b7e7638-9a44-426e-8fc4-52c1c3deabe4,Multi-Figurative Language Generation,0.0242874,"Figurative language generation is the task of reformulating a given text in
the desired figure of speech while still being faithful to the original
context. We take the first step towards multi-figurative language modelling by
providing a benchmark for the automatic generation of five common figurative
forms in English. We train mFLAG employing a scheme for multi-figurative
language pre-training on top of BART, and a mechanism for injecting the target
figurative information into the encoder; this enables the generation of text
with the target figurative form from another figurative form without parallel
figurative-figurative sentence pairs. Our approach outperforms all strong
baselines. We also offer some qualitative analysis and reflections on the
relationship between the different figures of speech.",https://github.com/laihuiyuan/mflag,-1
392b431c-02b6-4dc7-9125-2b6b48fba61a,LogiGAN: Learning Logical Reasoning via Adversarial Pre-training,0.315414,"We present LogiGAN, an unsupervised adversarial pre-training framework for
improving logical reasoning abilities of language models. Upon automatic
identifying logical reasoning phenomena in massive text corpus via detection
heuristics, we train language models to predict the masked-out logical
statements. Inspired by the facilitation effect of reflective thinking in human
learning, we analogically simulate the learning-thinking process with an
adversarial Generator-Verifier architecture to assist logic learning. LogiGAN
implements a novel sequential GAN approach that (a) circumvents the
non-differentiable challenge of the sequential GAN by leveraging the Generator
as a sentence-level generative likelihood scorer with a learning objective of
reaching scoring consensus with the Verifier; (b) is computationally feasible
for large-scale pre-training with arbitrary target length. Both base and large
size language models pre-trained with LogiGAN demonstrate obvious performance
improvement on 12 datasets requiring general reasoning abilities, revealing the
fundamental role of logic in broad reasoning, as well as the effectiveness of
LogiGAN. Ablation studies on LogiGAN components reveal the relative
orthogonality between linguistic and logic abilities and suggest that
reflective thinking's facilitation effect might also generalize to machine
learning.",None,-1
416e576b-9336-4980-a8da-3910ea347079,Improving Monocular Visual Odometry Using Learned Depth,0.616519,"Monocular visual odometry (VO) is an important task in robotics and computer
vision. Thus far, how to build accurate and robust monocular VO systems that
can work well in diverse scenarios remains largely unsolved. In this paper, we
propose a framework to exploit monocular depth estimation for improving VO. The
core of our framework is a monocular depth estimation module with a strong
generalization capability for diverse scenes. It consists of two separate
working modes to assist the localization and mapping. With a single monocular
image input, the depth estimation module predicts a relative depth to help the
localization module on improving the accuracy. With a sparse depth map and an
RGB image input, the depth estimation module can generate accurate
scale-consistent depth for dense mapping. Compared with current learning-based
VO methods, our method demonstrates a stronger generalization ability to
diverse scenes. More significantly, our framework is able to boost the
performances of existing geometry-based VO methods by a large margin.",None,-1
f85b2dd8-4d90-4980-b6c9-7819ef80d57b,On the Effect of Information Asymmetry in Human-AI Teams,0.518506,"Over the last years, the rising capabilities of artificial intelligence (AI)
have improved human decision-making in many application areas. Teaming between
AI and humans may even lead to complementary team performance (CTP), i.e., a
level of performance beyond the ones that can be reached by AI or humans
individually. Many researchers have proposed using explainable AI (XAI) to
enable humans to rely on AI advice appropriately and thereby reach CTP.
However, CTP is rarely demonstrated in previous work as often the focus is on
the design of explainability, while a fundamental prerequisite -- the presence
of complementarity potential between humans and AI -- is often neglected.
Therefore, we focus on the existence of this potential for effective human-AI
decision-making. Specifically, we identify information asymmetry as an
essential source of complementarity potential, as in many real-world
situations, humans have access to different contextual information. By
conducting an online experiment, we demonstrate that humans can use such
contextual information to adjust the AI's decision, finally resulting in CTP.",None,-1
003e02e0-e2f3-475d-9fae-3e648f26e0ca,SAFER: Safe Collision Avoidance using Focused and Efficient Trajectory Search with Reinforcement Learning,0.446559,"Collision avoidance is key for mobile robots and agents to operate safely in
the real world. In this work we present SAFER, an efficient and effective
collision avoidance system that is able to improve safety by correcting the
control commands sent by an operator. It combines real-world reinforcement
learning (RL), search-based online trajectory planning, and automatic emergency
intervention, e.g. automatic emergency braking (AEB). The goal of the RL is to
learn an effective corrective control action that is used in a focused search
for collision-free trajectories, and to reduce the frequency of triggering
automatic emergency braking. This novel setup enables the RL policy to learn
safely and directly on mobile robots in a real-world indoor environment,
minimizing actual crashes even during training. Our real-world experiments show
that, when compared with several baselines, our approach enjoys a higher
average speed, lower crash rate, less emergency intervention, smaller
computation overhead, and smoother overall control.",None,-1
5549a5ee-832f-49be-922d-985e0c3c507d,A Memory Transformer Network for Incremental Learning,0.679776,"We study class-incremental learning, a training setup in which new classes of
data are observed over time for the model to learn from. Despite the
straightforward problem formulation, the naive application of classification
models to class-incremental learning results in the ""catastrophic forgetting""
of previously seen classes. One of the most successful existing methods has
been the use of a memory of exemplars, which overcomes the issue of
catastrophic forgetting by saving a subset of past data into a memory bank and
utilizing it to prevent forgetting when training future tasks. In our paper, we
propose to enhance the utilization of this memory bank: we not only use it as a
source of additional training data like existing works but also integrate it in
the prediction process explicitly.Our method, the Memory Transformer Network
(MTN), learns how to combine and aggregate the information from the nearest
neighbors in the memory with a transformer to make more accurate predictions.
We conduct extensive experiments and ablations to evaluate our approach. We
show that MTN achieves state-of-the-art performance on the challenging
ImageNet-1k and Google-Landmarks-1k incremental learning benchmarks.",None,-1
1bac05e8-76fc-43ce-951d-999f0f78e0d0,MrSARP: A Hierarchical Deep Generative Prior for SAR Image Super-resolution,0.098759,"Generative models learned from training using deep learning methods can be
used as priors in inverse under-determined inverse problems, including imaging
from sparse set of measurements. In this paper, we present a novel hierarchical
deep-generative model MrSARP for SAR imagery that can synthesize SAR images of
a target at different resolutions jointly. MrSARP is trained in conjunction
with a critic that scores multi resolution images jointly to decide if they are
realistic images of a target at different resolutions. We show how this deep
generative model can be used to retrieve the high spatial resolution image from
low resolution images of the same target. The cost function of the generator is
modified to improve its capability to retrieve the input parameters for a given
set of resolution images. We evaluate the model's performance using the three
standard error metrics used for evaluating super-resolution performance on
simulated data and compare it to upsampling and sparsity based image sharpening
approaches.",None,-1
bc45c70e-4adb-4738-b2ed-2c251c593121,Talking About Large Language Models,0.490064,"Thanks to rapid progress in artificial intelligence, we have entered an era
when technology and philosophy intersect in interesting ways. Sitting squarely
at the centre of this intersection are large language models (LLMs). The more
adept LLMs become at mimicking human language, the more vulnerable we become to
anthropomorphism, to seeing the systems in which they are embedded as more
human-like than they really are. This trend is amplified by the natural
tendency to use philosophically loaded terms, such as ""knows"", ""believes"", and
""thinks"", when describing these systems. To mitigate this trend, this paper
advocates the practice of repeatedly stepping back to remind ourselves of how
LLMs, and the systems of which they form a part, actually work. The hope is
that increased scientific precision will encourage more philosophical nuance in
the discourse around artificial intelligence, both within the field and in the
public sphere.",None,-1
7e2f13de-c604-4a81-a8a3-e957612d9e7f,GMMSeg: Gaussian Mixture based Generative Semantic Segmentation Models,0.871951,"Prevalent semantic segmentation solutions are, in essence, a dense
discriminative classifier of p(class|pixel feature). Though straightforward,
this de facto paradigm neglects the underlying data distribution p(pixel
feature|class), and struggles to identify out-of-distribution data. Going
beyond this, we propose GMMSeg, a new family of segmentation models that rely
on a dense generative classifier for the joint distribution p(pixel
feature,class). For each class, GMMSeg builds Gaussian Mixture Models (GMMs)
via Expectation-Maximization (EM), so as to capture class-conditional
densities. Meanwhile, the deep dense representation is end-to-end trained in a
discriminative manner, i.e., maximizing p(class|pixel feature). This endows
GMMSeg with the strengths of both generative and discriminative models. With a
variety of segmentation architectures and backbones, GMMSeg outperforms the
discriminative counterparts on three closed-set datasets. More impressively,
without any modification, GMMSeg even performs well on open-world datasets. We
believe this work brings fundamental insights into the related fields.",https://github.com/leonnnop/GMMSeg,-1
38f60649-034d-4504-b45b-bc30b804751c,M2-Net: Multi-stages Specular Highlight Detection and Removal in Multi-scenes,0.182597,"In this paper, we propose a novel uniformity framework for highlight
detection and removal in multi-scenes, including synthetic images, face images,
natural images, and text images. The framework consists of three main
components, highlight feature extractor module, highlight coarse removal
module, and highlight refine removal module. Firstly, the highlight feature
extractor module can directly separate the highlight feature and non-highlight
feature from the original highlight image. Then highlight removal image is
obtained using a coarse highlight removal network. To further improve the
highlight removal effect, the refined highlight removal image is finally
obtained using refine highlight removal module based on contextual highlight
attention mechanisms. Extensive experimental results in multiple scenes
indicate that the proposed framework can obtain excellent visual effects of
highlight removal and achieve state-of-the-art results in several quantitative
evaluation metrics. Our algorithm is applied for the first time in video
highlight removal with promising results.",https://github.com/hzzzyf/specular-removal,-1
9531ff98-3400-4ee9-9a2a-b68c2a963380,ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs,0.286323,"Knowledge Graph Question Answering (KGQA) involves retrieving entities as
answers from a Knowledge Graph (KG) using natural language queries. The
challenge is to learn to reason over question-relevant KG facts that traverse
KG entities and lead to the question answers. To facilitate reasoning, the
question is decoded into instructions, which are dense question representations
used to guide the KG traversals. However, if the derived instructions do not
exactly match the underlying KG information, they may lead to reasoning under
irrelevant context. Our method, termed ReaRev, introduces a new way to KGQA
reasoning with respect to both instruction decoding and execution. To improve
instruction decoding, we perform reasoning in an adaptive manner, where
KG-aware information is used to iteratively update the initial instructions. To
improve instruction execution, we emulate breadth-first search (BFS) with graph
neural networks (GNNs). The BFS strategy treats the instructions as a set and
allows our method to decide on their execution order on the fly. Experimental
results on three KGQA benchmarks demonstrate the ReaRev's effectiveness
compared with previous state-of-the-art, especially when the KG is incomplete
or when we tackle complex questions. Our code is publicly available at
https://github.com/cmavro/ReaRev_KGQA.",https://github.com/cmavro/ReaRev_KGQA,-1
4a8ae470-9ae0-4351-861e-862b274d832d,Korean-Specific Dataset for Table Question Answering,0.0642198,"Existing question answering systems mainly focus on dealing with text data.
However, much of the data produced daily is stored in the form of tables that
can be found in documents and relational databases, or on the web. To solve the
task of question answering over tables, there exist many datasets for table
question answering written in English, but few Korean datasets. In this paper,
we demonstrate how we construct Korean-specific datasets for table question
answering: Korean tabular dataset is a collection of 1.4M tables with
corresponding descriptions for unsupervised pre-training language models.
Korean table question answering corpus consists of 70k pairs of questions and
answers created by crowd-sourced workers. Subsequently, we then build a
pre-trained language model based on Transformer and fine-tune the model for
table question answering with these datasets. We then report the evaluation
results of our model. We make our datasets publicly available via our GitHub
repository and hope that those datasets will help further studies for question
answering over tables, and for the transformation of table formats.",https://github.com/LG-NLP/KorWikiTableQuestions,-1
58b38021-d37f-4f45-9b14-12d29777b4f8,DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning,0.155086,"Dialogue State Tracking (DST), a key component of task-oriented conversation
systems, represents user intentions by determining the values of pre-defined
slots in an ongoing dialogue. Existing approaches use hand-crafted templates
and additional slot information to fine-tune and prompt large pre-trained
language models and elicit slot values from the dialogue context. Significant
manual effort and domain knowledge is required to design effective prompts,
limiting the generalizability of these approaches to new domains and tasks. In
this work, we propose DiSTRICT, a generalizable in-context tuning approach for
DST that retrieves highly relevant training examples for a given dialogue to
fine-tune the model without any hand-crafted templates. Experiments with the
MultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches
in various zero-shot and few-shot settings using a much smaller model, thereby
providing an important advantage for real-world deployments that often have
limited resource availability.",None,-1
31b1fd97-1a33-4930-8f61-87c7bf7dedfa,Information Extraction from Scanned Invoice Images using Text Analysis and Layout Features,0.414656,"While storing invoice content as metadata to avoid paper document processing
may be the future trend, almost all of daily issued invoices are still printed
on paper or generated in digital formats such as PDFs. In this paper, we
introduce the OCRMiner system for information extraction from scanned document
images which is based on text analysis techniques in combination with layout
features to extract indexing metadata of (semi-)structured documents. The
system is designed to process the document in a similar way a human reader
uses, i.e. to employ different layout and text attributes in a coordinated
decision. The system consists of a set of interconnected modules that start
with (possibly erroneous) character-based output from a standard OCR system and
allow to apply different techniques and to expand the extracted knowledge at
each step. Using an open source OCR, the system is able to recover the invoice
data in 90% for English and in 88% for the Czech set.",https://github.com/naiveHobo/InvoiceNet,-1
44f05010-07e7-4241-a203-5a12fb851dbd,Measuring Geographic Performance Disparities of Offensive Language Classifiers,0.516392,"Text classifiers are applied at scale in the form of one-size-fits-all
solutions. Nevertheless, many studies show that classifiers are biased
regarding different languages and dialects. When measuring and discovering
these biases, some gaps present themselves and should be addressed. First,
``Does language, dialect, and topical content vary across geographical
regions?'' and secondly ``If there are differences across the regions, do they
impact model performance?''. We introduce a novel dataset called GeoOLID with
more than 14 thousand examples across 15 geographically and demographically
diverse cities to address these questions. We perform a comprehensive analysis
of geographical-related content and their impact on performance disparities of
offensive language detection models. Overall, we find that current models do
not generalize across locations. Likewise, we show that while offensive
language models produce false positives on African American English, model
performance is not correlated with each city's minority population proportions.
Warning: This paper contains offensive language.",None,-1
7382bcf5-7fee-483e-83b0-f46c15c20ce8,MMNet: Muscle motion-guided network for micro-expression recognition,0.754336,"Facial micro-expressions (MEs) are involuntary facial motions revealing
peoples real feelings and play an important role in the early intervention of
mental illness, the national security, and many human-computer interaction
systems. However, existing micro-expression datasets are limited and usually
pose some challenges for training good classifiers. To model the subtle facial
muscle motions, we propose a robust micro-expression recognition (MER)
framework, namely muscle motion-guided network (MMNet). Specifically, a
continuous attention (CA) block is introduced to focus on modeling local subtle
muscle motion patterns with little identity information, which is different
from most previous methods that directly extract features from complete video
frames with much identity information. Besides, we design a position
calibration (PC) module based on the vision transformer. By adding the position
embeddings of the face generated by PC module at the end of the two branches,
the PC module can help to add position information to facial muscle motion
pattern features for the MER. Extensive experiments on three public
micro-expression datasets demonstrate that our approach outperforms
state-of-the-art methods by a large margin.",https://github.com/muse1998/MMNet,-1
f0a41b77-0bce-452f-adec-f3b6643c4c7b,All You Need Is Supervised Learning: From Imitation Learning to Meta-RL With Upside Down RL,0.169402,"Upside down reinforcement learning (UDRL) flips the conventional use of the
return in the objective function in RL upside down, by taking returns as input
and predicting actions. UDRL is based purely on supervised learning, and
bypasses some prominent issues in RL: bootstrapping, off-policy corrections,
and discount factors. While previous work with UDRL demonstrated it in a
traditional online RL setting, here we show that this single algorithm can also
work in the imitation learning and offline RL settings, be extended to the
goal-conditioned RL setting, and even the meta-RL setting. With a general agent
architecture, a single UDRL agent can learn across all paradigms.",https://github.com/Kaixhin/GUDRL,-1
9b6e1656-918d-4b43-9a4a-5736765169da,Finding Dataset Shortcuts with Grammar Induction,0.43653,"Many NLP datasets have been found to contain shortcuts: simple decision rules
that achieve surprisingly high accuracy. However, it is difficult to discover
shortcuts automatically. Prior work on automatic shortcut detection has focused
on enumerating features like unigrams or bigrams, which can find only low-level
shortcuts, or relied on post-hoc model interpretability methods like saliency
maps, which reveal qualitative patterns without a clear statistical
interpretation. In this work, we propose to use probabilistic grammars to
characterize and discover shortcuts in NLP datasets. Specifically, we use a
context-free grammar to model patterns in sentence classification datasets and
use a synchronous context-free grammar to model datasets involving sentence
pairs. The resulting grammars reveal interesting shortcut features in a number
of datasets, including both simple and high-level features, and automatically
identify groups of test examples on which conventional classifiers fail.
Finally, we show that the features we discover can be used to generate
diagnostic contrast examples and incorporated into standard robust optimization
methods to improve worst-group accuracy.",https://github.com/princeton-nlp/ShortcutGrammar,-1
f4c89dd5-2755-44b0-994f-5c3cbaea43c7,"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",0.788986,"Humans excel at continually learning from an ever-changing environment
whereas it remains a challenge for deep neural networks which exhibit
catastrophic forgetting. The complementary learning system (CLS) theory
suggests that the interplay between rapid instance-based learning and slow
structured learning in the brain is crucial for accumulating and retaining
knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)
method which maintains short-term and long-term semantic memories that interact
with the episodic memory. Our method employs an effective replay mechanism
whereby new knowledge is acquired while aligning the decision boundaries with
the semantic memories. CLS-ER does not utilize the task boundaries or make any
assumption about the distribution of the data which makes it versatile and
suited for ""general continual learning"". Our approach achieves state-of-the-art
performance on standard benchmarks as well as more realistic general continual
learning settings.",https://github.com/NeurAI-Lab/CLS-ER,-1
6522d3ae-56dd-448f-bd88-a451df7e35f9,Memorizing Transformers,0.746492,"Language models typically need to be trained or finetuned in order to acquire
new knowledge, which involves updating their weights. We instead envision
language models that can simply read and memorize new data at inference time,
thus acquiring new knowledge immediately. In this work, we extend language
models with the ability to memorize the internal representations of past
inputs. We demonstrate that an approximate kNN lookup into a non-differentiable
memory of recent (key, value) pairs improves language modeling across various
benchmarks and tasks, including generic webtext (C4), math papers (arXiv),
books (PG-19), code (Github), as well as formal theorems (Isabelle). We show
that the performance steadily improves when we increase the size of memory up
to 262K tokens. On benchmarks including code and mathematics, we find that the
model is capable of making use of newly defined functions and theorems during
test time.",None,-1
643b0f08-c9f9-4a68-8546-6089ee60abe1,Finding patterns in Knowledge Attribution for Transformers,0.115831,"We analyze the Knowledge Neurons framework for the attribution of factual and
relational knowledge to particular neurons in the transformer network. We use a
12-layer multi-lingual BERT model for our experiments. Our study reveals
various interesting phenomena. We observe that mostly factual knowledge can be
attributed to middle and higher layers of the network($\ge 6$). Further
analysis reveals that the middle layers($6-9$) are mostly responsible for
relational information, which is further refined into actual factual knowledge
or the ""correct answer"" in the last few layers($10-12$). Our experiments also
show that the model handles prompts in different languages, but representing
the same fact, similarly, providing further evidence for effectiveness of
multi-lingual pre-training. Applying the attribution scheme for grammatical
knowledge, we find that grammatical knowledge is far more dispersed among the
neurons than factual knowledge.",https://github.com/EleutherAI/knowledge-neurons,-1
5cb74da3-a2d9-4bfb-aacf-ef7cc1f8d8df,Classification on Sentence Embeddings for Legal Assistance,0.0355164,"Legal proceedings take plenty of time and also cost a lot. The lawyers have
to do a lot of work in order to identify the different sections of prior cases
and statutes. The paper tries to solve the first tasks in AILA2021 (Artificial
Intelligence for Legal Assistance) that will be held in FIRE2021 (Forum for
Information Retrieval Evaluation). The task is to semantically segment the
document into different assigned one of the 7 predefined labels or ""rhetorical
roles."" The paper uses BERT to obtain the sentence embeddings from a sentence,
and then a linear classifier is used to output the final prediction. The
experiments show that when more weightage is assigned to the class with the
highest frequency, the results are better than those when more weightage is
given to the class with a lower frequency. In task 1, the team legalNLP
obtained a F1 score of 0.22.",https://github.com/thearkamitra/LegalNLP,-1
9392c43f-d32e-40d8-b290-6d05434b05ac,MIAD: A Maintenance Inspection Dataset for Unsupervised Anomaly Detection,0.884824,"Visual anomaly detection plays a crucial role in not only manufacturing
inspection to find defects of products during manufacturing processes, but also
maintenance inspection to keep equipment in optimum working condition
particularly outdoors. Due to the scarcity of the defective samples,
unsupervised anomaly detection has attracted great attention in recent years.
However, existing datasets for unsupervised anomaly detection are biased
towards manufacturing inspection, not considering maintenance inspection which
is usually conducted under outdoor uncontrolled environment such as varying
camera viewpoints, messy background and degradation of object surface after
long-term working. We focus on outdoor maintenance inspection and contribute a
comprehensive Maintenance Inspection Anomaly Detection (MIAD) dataset which
contains more than 100K high-resolution color images in various outdoor
industrial scenarios. This dataset is generated by a 3D graphics software and
covers both surface and logical anomalies with pixel-precise ground truth.
Extensive evaluations of representative algorithms for unsupervised anomaly
detection are conducted, and we expect MIAD and corresponding experimental
results can inspire research community in outdoor unsupervised anomaly
detection tasks. Worthwhile and related future work can be spawned from our new
dataset.",https://github.com/vitjanz/draem,-1
5f082014-c33b-44cd-b959-049d2d191b3e,Fast Point Cloud Generation with Straight Flows,0.708762,"Diffusion models have emerged as a powerful tool for point cloud generation.
A key component that drives the impressive performance for generating
high-quality samples from noise is iteratively denoise for thousands of steps.
While beneficial, the complexity of learning steps has limited its applications
to many 3D real-world. To address this limitation, we propose Point Straight
Flow (PSF), a model that exhibits impressive performance using one step. Our
idea is based on the reformulation of the standard diffusion model, which
optimizes the curvy learning trajectory into a straight path. Further, we
develop a distillation strategy to shorten the straight path into one step
without a performance loss, enabling applications to 3D real-world with latency
constraints. We perform evaluations on multiple 3D tasks and find that our PSF
performs comparably to the standard diffusion model, outperforming other
efficient 3D point cloud generation methods. On real-world applications such as
point cloud completion and training-free text-guided generation in a
low-latency setup, PSF performs favorably.",None,-1
6dc0cf13-9729-432a-9b8c-4634ac1904c0,A-Eye: Driving with the Eyes of AI for Corner Case Generation,0.344656,"The overall goal of this work is to enrich training data for automated
driving with so called corner cases. In road traffic, corner cases are
critical, rare and unusual situations that challenge the perception by AI
algorithms. For this purpose, we present the design of a test rig to generate
synthetic corner cases using a human-in-the-loop approach. For the test rig, a
real-time semantic segmentation network is trained and integrated into the
driving simulation software CARLA in such a way that a human can drive on the
network's prediction. In addition, a second person gets to see the same scene
from the original CARLA output and is supposed to intervene with the help of a
second control unit as soon as the semantic driver shows dangerous driving
behavior. Interventions potentially indicate poor recognition of a critical
scene by the segmentation network and then represents a corner case. In our
experiments, we show that targeted enrichment of training data with corner
cases leads to improvements in pedestrian detection in safety relevant episodes
in road traffic.",None,-1
3e6c26f5-bbd3-4b0b-9c8f-02057854dd97,Emergent Computations in Trained Artificial Neural Networks and Real Brains,0.0441447,"Synaptic plasticity allows cortical circuits to learn new tasks and to adapt
to changing environments. How do cortical circuits use plasticity to acquire
functions such as decision-making or working memory? Neurons are connected in
complex ways, forming recurrent neural networks, and learning modifies the
strength of their connections. Moreover, neurons communicate emitting brief
discrete electric signals. Here we describe how to train recurrent neural
networks in tasks like those used to train animals in neuroscience
laboratories, and how computations emerge in the trained networks.
Surprisingly, artificial networks and real brains can use similar computational
strategies.",None,-1
86fd0831-593a-430e-85b0-edc70b3270bc,Dynamic Dialogue Policy for Continual Reinforcement Learning,0.7011,"Continual learning is one of the key components of human learning and a
necessary requirement of artificial intelligence. As dialogue can potentially
span infinitely many topics and tasks, a task-oriented dialogue system must
have the capability to continually learn, dynamically adapting to new
challenges while preserving the knowledge it already acquired. Despite the
importance, continual reinforcement learning of the dialogue policy has
remained largely unaddressed. The lack of a framework with training protocols,
baseline models and suitable metrics, has so far hindered research in this
direction. In this work we fill precisely this gap, enabling research in
dialogue policy optimisation to go from static to dynamic learning. We provide
a continual learning algorithm, baseline architectures and metrics for
assessing continual learning models. Moreover, we propose the dynamic dialogue
policy transformer (DDPT), a novel dynamic architecture that can integrate new
knowledge seamlessly, is capable of handling large state spaces and obtains
significant zero-shot performance when being exposed to unseen domains, without
any growth in network parameter size.",None,-1
36f59043-bd02-47a4-8e59-d526e2cc2240,CSS: Combining Self-training and Self-supervised Learning for Few-shot Dialogue State Tracking,0.0597938,"Few-shot dialogue state tracking (DST) is a realistic problem that trains the
DST model with limited labeled data. Existing few-shot methods mainly transfer
knowledge learned from external labeled dialogue data (e.g., from question
answering, dialogue summarization, machine reading comprehension tasks, etc.)
into DST, whereas collecting a large amount of external labeled data is
laborious, and the external data may not effectively contribute to the
DST-specific task. In this paper, we propose a few-shot DST framework called
CSS, which Combines Self-training and Self-supervised learning methods. The
unlabeled data of the DST task is incorporated into the self-training
iterations, where the pseudo labels are predicted by a DST model trained on
limited labeled data in advance. Besides, a contrastive self-supervised method
is used to learn better representations, where the data is augmented by the
dropout operation to train the model. Experimental results on the MultiWOZ
dataset show that our proposed CSS achieves competitive performance in several
few-shot scenarios.",None,30637
4279c715-4c21-49fb-b5bc-7eb0afbbd22c,Can Pre-trained Language Models Interpret Similes as Smart as Human?,0.644622,"Simile interpretation is a crucial task in natural language processing.
Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art
performance on many tasks. However, it remains under-explored whether PLMs can
interpret similes or not. In this paper, we investigate the ability of PLMs in
simile interpretation by designing a novel task named Simile Property Probing,
i.e., to let the PLMs infer the shared properties of similes. We construct our
simile property probing datasets from both general textual corpora and
human-designed questions, containing 1,633 examples covering seven main
categories. Our empirical study based on the constructed datasets shows that
PLMs can infer similes' shared properties while still underperforming humans.
To bridge the gap with human performance, we additionally design a
knowledge-enhanced training objective by incorporating the simile knowledge
into PLMs via knowledge embedding methods. Our method results in a gain of
8.58% in the probing task and 1.37% in the downstream task of sentiment
classification. The datasets and code are publicly available at
https://github.com/Abbey4799/PLMs-Interpret-Simile.",https://github.com/Abbey4799/PLMs-Interpret-Simile,-1
ccbb43da-9845-412a-aaae-478986f0833d,Learning Symmetric Rules with SATNet,0.0494921,"SATNet is a differentiable constraint solver with a custom backpropagation
algorithm, which can be used as a layer in a deep-learning system. It is a
promising proposal for bridging deep learning and logical reasoning. In fact,
SATNet has been successfully applied to learn, among others, the rules of a
complex logical puzzle, such as Sudoku, just from input and output pairs where
inputs are given as images. In this paper, we show how to improve the learning
of SATNet by exploiting symmetries in the target rules of a given but unknown
logical puzzle or more generally a logical formula. We present SymSATNet, a
variant of SATNet that translates the given symmetries of the target rules to a
condition on the parameters of SATNet and requires that the parameters should
have a particular parametric form that guarantees the condition. The
requirement dramatically reduces the number of parameters to learn for the
rules with enough symmetries, and makes the parameter learning of SymSATNet
much easier than that of SATNet. We also describe a technique for automatically
discovering symmetries of the target rules from examples. Our experiments with
Sudoku and Rubik's cube show the substantial improvement of SymSATNet over the
baseline SATNet.",https://github.com/locuslab/SATNet.git,-1
57d79066-d1fa-4866-9b4b-e1010db0a8b7,Towards Neural Numeric-To-Text Generation From Temporal Personal Health Data,0.0606951,"With an increased interest in the production of personal health technologies
designed to track user data (e.g., nutrient intake, step counts), there is now
more opportunity than ever to surface meaningful behavioral insights to
everyday users in the form of natural language. This knowledge can increase
their behavioral awareness and allow them to take action to meet their health
goals. It can also bridge the gap between the vast collection of personal
health data and the summary generation required to describe an individual's
behavioral tendencies. Previous work has focused on rule-based time-series data
summarization methods designed to generate natural language summaries of
interesting patterns found within temporal personal health data. We examine
recurrent, convolutional, and Transformer-based encoder-decoder models to
automatically generate natural language summaries from numeric temporal
personal health data. We showcase the effectiveness of our models on real user
health data logged in MyFitnessPal and show that we can automatically generate
high-quality natural language summaries. Our work serves as a first step
towards the ambitious goal of automatically generating novel and meaningful
temporal summaries from personal health data.",https://github.com/neato47/Neural-Numeric-To-Text-Generation,-1
5081d9fa-3fc5-4fcb-8975-01ef3025a20a,Non-Isometric Shape Matching via Functional Maps on Landmark-Adapted Bases,0.397394,"We propose a principled approach for non-isometric landmark-preserving
non-rigid shape matching. Our method is based on the functional maps framework,
but rather than promoting isometries we focus instead on near-conformal maps
that preserve landmarks exactly. We achieve this, first, by introducing a novel
landmark-adapted basis using an intrinsic Dirichlet-Steklov eigenproblem.
Second, we establish the functional decomposition of conformal maps expressed
in this basis. Finally, we formulate a conformally-invariant energy that
promotes high-quality landmark-preserving maps, and show how it can be solved
via a variant of the recently proposed ZoomOut method that we extend to our
setting. Our method is descriptor-free, efficient and robust to significant
mesh variability. We evaluate our approach on a range of benchmark datasets and
demonstrate state-of-the-art performance on non-isometric benchmarks and near
state-of-the-art performance on isometric ones.",https://github.com/mpanine/DirichletSteklovLandmarkMatching,-1
9b239cff-890c-45b9-904d-1e877b71dea5,"Learning the Ordering of Coordinate Compounds and Elaborate Expressions in Hmong, Lahu, and Chinese",0.0487848,"Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate
constructions common in languages of East and Southeast Asia. Mortensen (2006)
claims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese
can be predicted via phonological hierarchies and (2) these phonological
hierarchies lack a clear phonetic rationale. These claims are significant
because morphosyntax has often been seen as in a feed-forward relationship with
phonology, and phonological generalizations have often been assumed to be
phonetically ""natural"". We investigate whether the ordering of CCs and EEs can
be learned empirically and whether computational models (classifiers and
sequence labeling models) learn unnatural hierarchies similar to those posited
by Mortensen (2006). We find that decision trees and SVMs learn to predict the
order of CCs/EEs on the basis of phonology, with DTs learning hierarchies
strikingly similar to those proposed by Mortensen. However, we also find that a
neural sequence labeling model is able to learn the ordering of elaborate
expressions in Hmong very effectively without using any phonological
information. We argue that EE ordering can be learned through two independent
routes: phonology and lexical distribution, presenting a more nuanced picture
than previous work. [ISO 639-3:hmn, lhu, cmn]",https://github.com/dmort27/elab-order,-1
5d471b4f-6a8b-4e99-bfdd-1f6738cad26c,NeRF-RPN: A general framework for object detection in NeRFs,0.81361,"This paper presents the first significant object detection framework,
NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,
NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting
a novel voxel representation that incorporates multi-scale 3D neural volumetric
features, we demonstrate it is possible to regress the 3D bounding boxes of
objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN
is a general framework and can be applied to detect objects without class
labels. We experimented NeRF-RPN with various backbone architectures, RPN head
designs and loss functions. All of them can be trained in an end-to-end manner
to estimate high quality 3D bounding boxes. To facilitate future research in
object detection for NeRF, we built a new benchmark dataset which consists of
both synthetic and real-world data with careful labeling and clean up. Code and
dataset are available at https://github.com/lyclyc52/NeRF_RPN.",https://github.com/lyclyc52/NeRF_RPN,-1
7127f8c0-9048-4f42-97ff-6b2facdfc514,CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation,0.873409,"Practical dialog systems need to deal with various knowledge sources, noisy
user expressions, and the shortage of annotated data. To better solve the above
problems, we propose CGoDial, new challenging and comprehensive Chinese
benchmark for multi-domain Goal-oriented Dialog evaluation. It contains 96,763
dialog sessions and 574,949 dialog turns totally, covering three datasets with
different knowledge sources: 1) a slot-based dialog (SBD) dataset with
table-formed knowledge, 2) a flow-based dialog (FBD) dataset with tree-formed
knowledge, and a retrieval-based dialog (RBD) dataset with candidate-formed
knowledge. To bridge the gap between academic benchmarks and spoken dialog
scenarios, we either collect data from real conversations or add spoken
features to existing datasets via crowd-sourcing. The proposed experimental
settings include the combinations of training with either the entire training
set or a few-shot training set, and testing with either the standard test set
or a hard test subset, which can assess model capabilities in terms of general
prediction, fast adaptability and reliable robustness.",https://github.com/Alibaba/DAMO-ConvAI/cgodial,-1
3e822085-8d36-4480-b07e-fde0c8eb71ab,Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs,0.0329622,"Answering factual questions with temporal intent over knowledge graphs
(temporal KGQA) attracts rising attention in recent years. In the generation of
temporal queries, existing KGQA methods ignore the fact that some intrinsic
connections between events can make them temporally related, which may limit
their capability. We systematically analyze the possible interpretation of
temporal constraints and conclude the interpretation structures as the Semantic
Framework of Temporal Constraints, SF-TCons. Based on the semantic framework,
we propose a temporal question answering method, SF-TQA, which generates query
graphs by exploring the relevant facts of mentioned entities, where the
exploring process is restricted by SF-TCons. Our evaluations show that SF-TQA
significantly outperforms existing methods on two benchmarks over different
knowledge graphs.",None,-1
f649e4ed-3192-4104-8acd-d77894f4a6d3,Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection,0.802509,"We study the selection of transfer languages for automatic abusive language
detection. Instead of preparing a dataset for every language, we demonstrate
the effectiveness of cross-lingual transfer learning for zero-shot abusive
language detection. This way we can use existing data from higher-resource
languages to build better detection systems for low-resource languages. Our
datasets are from seven different languages from three language families. We
measure the distance between the languages using several language similarity
measures, especially by quantifying the World Atlas of Language Structures. We
show that there is a correlation between linguistic similarity and classifier
performance. This discovery allows us to choose an optimal transfer language
for zero shot abusive language detection.",None,-1
01c11285-22bc-4846-93d7-2e824e2dba8f,Latent Evolution Model for Change Point Detection in Time-varying Networks,0.337876,"Graph-based change point detection (CPD) play an irreplaceable role in
discovering anomalous graphs in the time-varying network. While several
techniques have been proposed to detect change points by identifying whether
there is a significant difference between the target network and successive
previous ones, they neglect the natural evolution of the network. In practice,
real-world graphs such as social networks, traffic networks, and rating
networks are constantly evolving over time. Considering this problem, we treat
the problem as a prediction task and propose a novel CPD method for dynamic
graphs via a latent evolution model. Our method focuses on learning the
low-dimensional representations of networks and capturing the evolving patterns
of these learned latent representations simultaneously. After having the
evolving patterns, a prediction of the target network can be achieved. Then, we
can detect the change points by comparing the prediction and the actual network
by leveraging a trade-off strategy, which balances the importance between the
prediction network and the normal graph pattern extracted from previous
networks. Intensive experiments conducted on both synthetic and real-world
datasets show the effectiveness and superiority of our model.",None,-1
83f157d5-b6f9-4091-8d4a-10ee8195ad62,An Interpretability Evaluation Benchmark for Pre-trained Language Models,0.0802763,"While pre-trained language models (LMs) have brought great improvements in
many NLP tasks, there is increasing attention to explore capabilities of LMs
and interpret their predictions. However, existing works usually focus only on
a certain capability with some downstream tasks. There is a lack of datasets
for directly evaluating the masked word prediction performance and the
interpretability of pre-trained LMs. To fill in the gap, we propose a novel
evaluation benchmark providing with both English and Chinese annotated data. It
tests LMs abilities in multiple dimensions, i.e., grammar, semantics,
knowledge, reasoning and computation. In addition, it provides carefully
annotated token-level rationales that satisfy sufficiency and compactness. It
contains perturbed instances for each original instance, so as to use the
rationale consistency under perturbations as the metric for faithfulness, a
perspective of interpretability. We conduct experiments on several widely-used
pre-trained LMs. The results show that they perform very poorly on the
dimensions of knowledge and computation. And their plausibility in all
dimensions is far from satisfactory, especially when the rationale is short. In
addition, the pre-trained LMs we evaluated are not robust on syntax-aware data.
We will release this evaluation benchmark at \url{http://xyz}, and hope it can
facilitate the research progress of pre-trained LMs.",None,-1
a8290d2d-bb95-4cd3-ae4c-3539c412e5a5,Masked Summarization to Generate Factually Inconsistent Summaries for Improved Factual Consistency Checking,0.204497,"Despite the recent advances in abstractive summarization systems, it is still
difficult to determine whether a generated summary is factual consistent with
the source text. To this end, the latest approach is to train a factual
consistency classifier on factually consistent and inconsistent summaries.
Luckily, the former is readily available as reference summaries in existing
summarization datasets. However, generating the latter remains a challenge, as
they need to be factually inconsistent, yet closely relevant to the source text
to be effective. In this paper, we propose to generate factually inconsistent
summaries using source texts and reference summaries with key information
masked. Experiments on seven benchmark datasets demonstrate that factual
consistency classifiers trained on summaries generated using our method
generally outperform existing models and show a competitive correlation with
human judgments. We also analyze the characteristics of the summaries generated
using our method. We will release the pre-trained model and the code at
https://github.com/hwanheelee1993/MFMA.",https://github.com/hwanheelee1993/MFMA,-1
04f7291b-2b99-4b87-a775-6dfca5be10c1,On the Paradox of Learning to Reason from Data,0.787685,"Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be
trained end-to-end to solve logical reasoning problems presented in natural
language? We attempt to answer this question in a confined problem space where
there exists a set of parameters that perfectly simulates logical reasoning. We
make observations that seem to contradict each other: BERT attains near-perfect
accuracy on in-distribution test examples while failing to generalize to other
data distributions over the exact same problem space. Our study provides an
explanation for this paradox: instead of learning to emulate the correct
reasoning function, BERT has in fact learned statistical features that
inherently exist in logical reasoning problems. We also show that it is
infeasible to jointly remove statistical features from data, illustrating the
difficulty of learning to reason in general. Our result naturally extends to
other neural models and unveils the fundamental difference between learning to
reason and learning to achieve high performance on NLP benchmarks using
statistical features.",https://github.com/joshuacnf/paradox-learning2reason,34559
4f0e9897-b6ce-4306-96be-0b3c2cbe59f9,RELIC: Retrieving Evidence for Literary Claims,0.768384,"Humanities scholars commonly provide evidence for claims that they make about
a work of literature (e.g., a novel) in the form of quotations from the work.
We collect a large-scale dataset (RELiC) of 78K literary quotations and
surrounding critical analysis and use it to formulate the novel task of
literary evidence retrieval, in which models are given an excerpt of literary
analysis surrounding a masked quotation and asked to retrieve the quoted
passage from the set of all passages in the work. Solving this retrieval task
requires a deep understanding of complex literary and linguistic phenomena,
which proves challenging to methods that overwhelmingly rely on lexical and
semantic similarity matching. We implement a RoBERTa-based dense passage
retriever for this task that outperforms existing pretrained information
retrieval baselines; however, experiments and analysis by human domain experts
indicate that there is substantial room for improvement over our dense
retriever.",None,-1
8302c488-3cdd-4653-a2d4-2e63f4cab0bf,PREME: Preference-based Meeting Exploration through an Interactive Questionnaire,0.336197,"The recent increase in the volume of online meetings necessitates automated
tools for managing and organizing the material, especially when an attendee has
missed the discussion and needs assistance in quickly exploring it. In this
work, we propose a novel end-to-end framework for generating interactive
questionnaires for preference-based meeting exploration. As a result, users are
supplied with a list of suggested questions reflecting their preferences. Since
the task is new, we introduce an automatic evaluation strategy. Namely, it
measures how much the generated questions via questionnaire are answerable to
ensure factual correctness and covers the source meeting for the depth of
possible exploration.",https://github.com/microsoft/preme,-1
59a9af27-7e8f-4664-872c-cd35f8b1ba59,Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents,0.475873,"Text semantic matching is a fundamental task that has been widely used in
various scenarios, such as community question answering, information retrieval,
and recommendation. Most state-of-the-art matching models, e.g., BERT, directly
perform text comparison by processing each word uniformly. However, a query
sentence generally comprises content that calls for different levels of
matching granularity. Specifically, keywords represent factual information such
as action, entity, and event that should be strictly matched, while intents
convey abstract concepts and ideas that can be paraphrased into various
expressions. In this work, we propose a simple yet effective training strategy
for text semantic matching in a divide-and-conquer manner by disentangling
keywords from intents. Our approach can be easily combined with pre-trained
language models (PLM) without influencing their inference efficiency, achieving
stable performance improvements against a wide range of PLMs on three
benchmarks.",https://github.com/RowitZou/DC-Match,-1
a872af26-07b0-48bd-a675-c9b85f386901,Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021,0.794019,"Human intelligence has the remarkable ability to quickly adapt to new tasks
and environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research in
this direction, we propose \emph{IGLU: Interactive Grounded Language
Understanding in a Collaborative Environment}.
  The primary goal of the competition is to approach the problem of how to
build interactive agents that learn to solve a task while provided with
grounded natural language instructions in a collaborative environment.
Understanding the complexity of the challenge, we split it into sub-tasks to
make it feasible for participants.",None,-1
be2920bb-e9d5-4e1d-9a63-91573c2f1b7b,RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks,0.729292,"Temporal/spatial receptive fields of models play an important role in
sequential/spatial tasks. Large receptive fields facilitate long-term
relations, while small receptive fields help to capture the local details.
Existing methods construct models with hand-designed receptive fields in
layers. Can we effectively search for receptive field combinations to replace
hand-designed patterns? To answer this question, we propose to find better
receptive field combinations through a global-to-local search scheme. Our
search scheme exploits both global search to find the coarse combinations and
local search to get the refined receptive field combinations further. The
global search finds possible coarse combinations other than human-designed
patterns. On top of the global search, we propose an expectation-guided
iterative local search scheme to refine combinations effectively. Our RF-Next
models, plugging receptive field search to various models, boost the
performance on many tasks, e.g., temporal action segmentation, object
detection, instance segmentation, and speech synthesis. The source code is
publicly available on http://mmcheng.net/rfnext.",http://mmcheng.net/rfnext,-1
2bd86f18-4d0f-4db0-9486-d037e9d2373c,Omnigrok: Grokking Beyond Algorithmic Data,0.53064,"Grokking, the unusual phenomenon for algorithmic datasets where
generalization happens long after overfitting the training data, has remained
elusive. We aim to understand grokking by analyzing the loss landscapes of
neural networks, identifying the mismatch between training and test losses as
the cause for grokking. We refer to this as the ""LU mechanism"" because training
and test losses (against model weight norm) typically resemble ""L"" and ""U"",
respectively. This simple mechanism can nicely explain many aspects of
grokking: data size dependence, weight decay dependence, the emergence of
representations, etc. Guided by the intuitive picture, we are able to induce
grokking on tasks involving images, language and molecules. In the reverse
direction, we are able to eliminate grokking for algorithmic datasets. We
attribute the dramatic nature of grokking for algorithmic datasets to
representation learning.",https://github.com/KindXiaoming/Omnigrok,-1
43a3e68f-f399-41be-b890-05c36a7be4c7,TorchMD-NET: Equivariant Transformers for Neural Network based Molecular Potentials,0.309998,"The prediction of quantum mechanical properties is historically plagued by a
trade-off between accuracy and speed. Machine learning potentials have
previously shown great success in this domain, reaching increasingly better
accuracy while maintaining computational efficiency comparable with classical
force fields. In this work we propose TorchMD-NET, a novel equivariant
transformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1,
and many QM9 targets in both accuracy and computational efficiency. Through an
extensive attention weight analysis, we gain valuable insights into the black
box predictor and show differences in the learned representation of conformers
versus conformations sampled from molecular dynamics or normal modes.
Furthermore, we highlight the importance of datasets including off-equilibrium
conformations for the evaluation of molecular potentials.",None,-1
2c33d67c-d8f3-4af9-a932-8703326b4d97,Improving Low-Resource Question Answering using Active Learning in Multiple Stages,0.128272,"Neural approaches have become very popular in the domain of Question
Answering, however they require a large amount of annotated data. Furthermore,
they often yield very good performance but only in the domain they were trained
on. In this work we propose a novel approach that combines data augmentation
via question-answer generation with Active Learning to improve performance in
low resource settings, where the target domains are diverse in terms of
difficulty and similarity to the source domain. We also investigate Active
Learning for question answering in different stages, overall reducing the
annotation effort of humans. For this purpose, we consider target domains in
realistic settings, with an extremely low amount of annotated samples but with
many unlabeled documents, which we assume can be obtained with little effort.
Additionally, we assume sufficient amount of labeled data from the source
domain is available. We perform extensive experiments to find the best setup
for incorporating domain experts. Our findings show that our novel approach,
where humans are incorporated as early as possible in the process, boosts
performance in the low-resource, domain-specific setting, allowing for
low-labeling-effort question answering systems in new, specialized domains.
They further demonstrate how human annotation affects the performance of QA
depending on the stage it is performed.",https://github.com/primeqa/primeqa,1402
097443f4-45fd-437d-b5f5-49154af7177e,PixelGame: Infrared small target segmentation as a Nash equilibrium,0.580486,"A key challenge of infrared small target segmentation (ISTS) is to balance
false negative pixels (FNs) and false positive pixels (FPs). Traditional
methods combine FNs and FPs into a single objective by weighted sum, and the
optimization process is decided by one actor. Minimizing FNs and FPs with the
same strategy leads to antagonistic decisions. To address this problem, we
propose a competitive game framework (pixelGame) from a novel perspective for
ISTS. In pixelGame, FNs and FPs are controlled by different player whose goal
is to minimize their own utility function. FNs-player and FPs-player are
designed with different strategies: One is to minimize FNs and the other is to
minimize FPs. The utility function drives the evolution of the two participants
in competition. We consider the Nash equilibrium of pixelGame as the optimal
solution. In addition, we propose maximum information modulation (MIM) to
highlight the tar-get information. MIM effectively focuses on the salient
region including small targets. Extensive experiments on two standard public
datasets prove the effectiveness of our method. Compared with other
state-of-the-art methods, our method achieves better performance in terms of
F1-measure (F1) and the intersection of union (IoU).",None,-1
b56e075a-1f78-447a-be3d-b5a678a6fe32,"""This is Fake! Shared it by Mistake"": Assessing the Intent of Fake News Spreaders",0.887701,"Individuals can be misled by fake news and spread it unintentionally without
knowing it is false. This phenomenon has been frequently observed but has not
been investigated. Our aim in this work is to assess the intent of fake news
spreaders. To distinguish between intentional versus unintentional spreading,
we study the psychological explanations of unintentional spreading. With this
foundation, we then propose an influence graph, using which we assess the
intent of fake news spreaders. Our extensive experiments show that the assessed
intent can help significantly differentiate between intentional and
unintentional fake news spreaders. Furthermore, the estimated intent can
significantly improve the current techniques that detect fake news. To our best
knowledge, this is the first work to model individuals' intent in fake news
spreading.",https://github.com/flairNLP/flair,-1
9a729348-906e-429b-adf3-59a8f1ce6549,Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,0.983318,"Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create ""latent saliency maps"" that can help explain predictions
in human terms.",None,-1
57b59add-6122-46f9-9d43-a73c60e89c21,Deconstructed Generation-Based Zero-Shot Model,0.0356589,"Recent research on Generalized Zero-Shot Learning (GZSL) has focused
primarily on generation-based methods. However, current literature has
overlooked the fundamental principles of these methods and has made limited
progress in a complex manner. In this paper, we aim to deconstruct the
generator-classifier framework and provide guidance for its improvement and
extension. We begin by breaking down the generator-learned unseen class
distribution into class-level and instance-level distributions. Through our
analysis of the role of these two types of distributions in solving the GZSL
problem, we generalize the focus of the generation-based approach, emphasizing
the importance of (i) attribute generalization in generator learning and (ii)
independent classifier learning with partially biased data. We present a simple
method based on this analysis that outperforms SotAs on four public GZSL
datasets, demonstrating the validity of our deconstruction. Furthermore, our
proposed method remains effective even without a generative model, representing
a step towards simplifying the generator-classifier structure. Our code is
available at \url{https://github.com/cdb342/DGZ}.",https://github.com/cdb342/DGZ,-1
9f93c85c-a45e-428f-841f-a43037b39bd0,MetaEMS: A Meta Reinforcement Learning-based Control Framework for Building Energy Management System,0.390883,"The building sector has been recognized as one of the primary sectors for
worldwide energy consumption. Improving the energy efficiency of the building
sector can help reduce the operation cost and reduce the greenhouse gas
emission. The energy management system (EMS) can monitor and control the
operations of built-in appliances in buildings, so an efficient EMS is of
crucial importance to improve the building operation efficiency and maintain
safe operations. With the growing penetration of renewable energy and
electrical appliances, increasing attention has been paid to the development of
intelligent building EMS. Recently, reinforcement learning (RL) has been
applied for building EMS and has shown promising potential. However, most of
the current RL-based EMS solutions would need a large amount of data to learn a
reliable control policy, which limits the applicability of these solutions in
the real world. In this work, we propose MetaEMS, which can help achieve better
energy management performance with the benefits of RL and meta-learning.
Experiment results showcase that our proposed MetaEMS can adapt faster to
environment changes and perform better in most situations compared with other
baselines.",None,-1
78189e68-74bd-4899-a9dc-a239c5caee61,Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance,0.516922,"Score-based generative models are shown to achieve remarkable empirical
performances in various applications such as image generation and audio
synthesis. However, a theoretical understanding of score-based diffusion models
is still incomplete. Recently, Song et al. showed that the training objective
of score-based generative models is equivalent to minimizing the
Kullback-Leibler divergence of the generated distribution from the data
distribution. In this work, we show that score-based models also minimize the
Wasserstein distance between them under suitable assumptions on the model.
Specifically, we prove that the Wasserstein distance is upper bounded by the
square root of the objective function up to multiplicative constants and a
fixed constant offset. Our proof is based on a novel application of the theory
of optimal transport, which can be of independent interest to the society. Our
numerical experiments support our findings. By analyzing our upper bounds, we
provide a few techniques to obtain tighter upper bounds.",https://github.com/UW-Madison-Lee-Lab/score-wasserstein,-1
71bc4b52-7db8-4fbe-af29-1a38c0ea6fba,Introducing Randomized High Order Fuzzy Cognitive Maps as Reservoir Computing Models: A Case Study in Solar Energy and Load Forecasting,0.432556,"Fuzzy Cognitive Maps (FCMs) have emerged as an interpretable signed weighted
digraph method consisting of nodes (concepts) and weights which represent the
dependencies among the concepts. Although FCMs have attained considerable
achievements in various time series prediction applications, designing an FCM
model with time-efficient training method is still an open challenge. Thus,
this paper introduces a novel univariate time series forecasting technique,
which is composed of a group of randomized high order FCM models labeled
R-HFCM. The novelty of the proposed R-HFCM model is relevant to merging the
concepts of FCM and Echo State Network (ESN) as an efficient and particular
family of Reservoir Computing (RC) models, where the least squares algorithm is
applied to train the model. From another perspective, the structure of R-HFCM
consists of the input layer, reservoir layer, and output layer in which only
the output layer is trainable while the weights of each sub-reservoir
components are selected randomly and keep constant during the training process.
As case studies, this model considers solar energy forecasting with public data
for Brazilian solar stations as well as Malaysia dataset, which includes hourly
electric load and temperature data of the power supply company of the city of
Johor in Malaysia. The experiment also includes the effect of the map size,
activation function, the presence of bias and the size of the reservoir on the
accuracy of R-HFCM method. The obtained results confirm the outperformance of
the proposed R-HFCM model in comparison to the other methods. This study
provides evidence that FCM can be a new way to implement a reservoir of
dynamics in time series modelling.",None,-1
e686c0bf-398e-4b70-9731-f9182c1c3304,Improving Robustness to Out-of-Distribution Data by Frequency-based Augmentation,0.314281,"Although Convolutional Neural Networks (CNNs) have high accuracy in image
recognition, they are vulnerable to adversarial examples and
out-of-distribution data, and the difference from human recognition has been
pointed out. In order to improve the robustness against out-of-distribution
data, we present a frequency-based data augmentation technique that replaces
the frequency components with other images of the same class. When the training
data are CIFAR10 and the out-of-distribution data are SVHN, the Area Under
Receiver Operating Characteristic (AUROC) curve of the model trained with the
proposed method increases from 89.22\% to 98.15\%, and further increased to
98.59\% when combined with another data augmentation method. Furthermore, we
experimentally demonstrate that the robust model for out-of-distribution data
uses a lot of high-frequency components of the image.",None,-1
e23a56b2-0e7a-40dd-95f8-e44dc209712b,Mixture-Net: Low-Rank Deep Image Prior Inspired by Mixture Models for Spectral Image Recovery,0.387464,"This paper proposes a non-data-driven deep neural network for spectral image
recovery problems such as denoising, single hyperspectral image
super-resolution, and compressive spectral imaging reconstruction. Unlike
previous methods, the proposed approach, dubbed Mixture-Net, implicitly learns
the prior information through the network. Mixture-Net consists of a deep
generative model whose layers are inspired by the linear and non-linear
low-rank mixture models, where the recovered image is composed of a weighted
sum between the linear and non-linear decomposition. Mixture-Net also provides
a low-rank decomposition interpreted as the spectral image abundances and
endmembers, helpful in achieving remote sensing tasks without running
additional routines. The experiments show the MixtureNet effectiveness
outperforming state-of-the-art methods in recovery quality with the advantage
of architecture interpretability.",None,-1
34413a03-cda8-4c8c-9ffa-fb10eb7d8f5c,Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models,0.438925,"Our native language influences the way we perceive speech sounds, affecting
our ability to discriminate non-native sounds. We compare two ideas about the
influence of the native language on speech perception: the Perceptual
Assimilation Model, which appeals to a mental classification of sounds into
native phoneme categories, versus the idea that rich, fine-grained phonetic
representations tuned to the statistics of the native language, are sufficient.
We operationalize this idea using representations from two state-of-the-art
speech models, a Dirichlet process Gaussian mixture model and the more recent
wav2vec 2.0 model. We present a new, open dataset of French- and
English-speaking participants' speech perception behaviour for 61 vowel sounds
from six languages. We show that phoneme assimilation is a better predictor
than fine-grained phonetic modelling, both for the discrimination behaviour as
a whole, and for predicting differences in discriminability associated with
differences in native language background. We also show that wav2vec 2.0, while
not good at capturing the effects of native language on speech perception, is
complementary to information about native phoneme assimilation, and provides a
good model of low-level phonetic representations, supporting the idea that both
categorical and fine-grained perception are used during speech perception.",https://github.com/JAMJU/CONLL_2021_nonnative_speech_perception,-1
a05517a9-6413-41d5-a091-c14f68902879,Towards Abstractive Grounded Summarization of Podcast Transcripts,0.472924,"Podcasts have recently shown a rapid rise in popularity. Summarization of
podcast transcripts is of practical benefit to both content providers and
consumers. It helps consumers to quickly decide whether they will listen to the
podcasts and reduces the cognitive load of content providers to write
summaries. Nevertheless, podcast summarization faces significant challenges
including factual inconsistencies with respect to the inputs. The problem is
exacerbated by speech disfluencies and recognition errors in transcripts of
spoken language. In this paper, we explore a novel abstractive summarization
method to alleviate these challenges. Specifically, our approach learns to
produce an abstractive summary while grounding summary segments in specific
portions of the transcript to allow for full inspection of summary details. We
conduct a series of analyses of the proposed approach on a large podcast
dataset and show that the approach can achieve promising results. Grounded
summaries bring clear benefits in locating the summary and transcript segments
that contain inconsistent information, and hence significantly improve
summarization quality in both automatic and human evaluation metrics.",https://github.com/tencent-ailab/GrndPodcastSum,-1
b39236f3-4abb-4eb9-94fd-8ef07b40fdef,A Hazard Analysis Framework for Code Synthesis Large Language Models,0.907124,"Codex, a large language model (LLM) trained on a variety of codebases,
exceeds the previous state of the art in its capacity to synthesize and
generate code. Although Codex provides a plethora of benefits, models that may
generate code on such scale have significant limitations, alignment problems,
the potential to be misused, and the possibility to increase the rate of
progress in technical fields that may themselves have destabilizing impacts or
have misuse potential. Yet such safety impacts are not yet known or remain to
be explored. In this paper, we outline a hazard analysis framework constructed
at OpenAI to uncover hazards or safety risks that the deployment of models like
Codex may impose technically, socially, politically, and economically. The
analysis is informed by a novel evaluation framework that determines the
capacity of advanced code generation techniques against the complexity and
expressivity of specification prompts, and their capability to understand and
execute them relative to human ability.",None,-1
62e57313-b0e6-44fb-8f44-0d9f5dd1f48c,Online Hybrid Lightweight Representations Learning: Its Application to Visual Tracking,0.11996,"This paper presents a novel hybrid representation learning framework for
streaming data, where an image frame in a video is modeled by an ensemble of
two distinct deep neural networks; one is a low-bit quantized network and the
other is a lightweight full-precision network. The former learns coarse primary
information with low cost while the latter conveys residual information for
high fidelity to original representations. The proposed parallel architecture
is effective to maintain complementary information since fixed-point arithmetic
can be utilized in the quantized network and the lightweight model provides
precise representations given by a compact channel-pruned network. We
incorporate the hybrid representation technique into an online visual tracking
task, where deep neural networks need to handle temporal variations of target
appearances in real-time. Compared to the state-of-the-art real-time trackers
based on conventional deep neural networks, our tracking algorithm demonstrates
competitive accuracy on the standard benchmarks with a small fraction of
computational cost and memory footprint.",None,-1
3f51a76d-c5d6-4239-851b-8ab0f9d4346a,Registering Explicit to Implicit: Towards High-Fidelity Garment mesh Reconstruction from Single Images,0.869975,"Fueled by the power of deep learning techniques and implicit shape learning,
recent advances in single-image human digitalization have reached unprecedented
accuracy and could recover fine-grained surface details such as garment
wrinkles. However, a common problem for the implicit-based methods is that they
cannot produce separated and topology-consistent mesh for each garment piece,
which is crucial for the current 3D content creation pipeline. To address this
issue, we proposed a novel geometry inference framework ReEF that reconstructs
topology-consistent layered garment mesh by registering the explicit garment
template to the whole-body implicit fields predicted from single images.
Experiments demonstrate that our method notably outperforms its counterparts on
single-image layered garment reconstruction and could bring high-quality
digital assets for further content creation.",None,-1
dbb7870c-f1f2-453a-91e2-32332b6196fc,Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP,0.576935,"Textual adversarial samples play important roles in multiple subfields of NLP
research, including security, evaluation, explainability, and data
augmentation. However, most work mixes all these roles, obscuring the problem
definitions and research goals of the security role that aims to reveal the
practical concerns of NLP models. In this paper, we rethink the research
paradigm of textual adversarial samples in security scenarios. We discuss the
deficiencies in previous work and propose our suggestions that the research on
the Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their
methods on security tasks to demonstrate the real-world concerns; (2) consider
real-world attackers' goals, instead of developing impractical methods. To this
end, we first collect, process, and release a security datasets collection
Advbench. Then, we reformalize the task and adjust the emphasis on different
goals in SoadNLP. Next, we propose a simple method based on heuristic rules
that can easily fulfill the actual adversarial goals to simulate real-world
attack methods. We conduct experiments on both the attack and the defense sides
on Advbench. Experimental results show that our method has higher practical
value, indicating that the research paradigm in SoadNLP may start from our new
benchmark. All the code and data of Advbench can be obtained at
\url{https://github.com/thunlp/Advbench}.",https://github.com/thunlp/Advbench,-1
2b97ff4a-ad9b-41ac-ba7e-e66b5522a376,Data Augmentation Vision Transformer for Fine-grained Image Classification,0.116995,"Recently, the vision transformer (ViT) has made breakthroughs in image
recognition. Its self-attention mechanism (MSA) can extract discriminative
labeling information of different pixel blocks to improve image classification
accuracy. However, the classification marks in their deep layers tend to ignore
local features between layers. In addition, the embedding layer will be
fixed-size pixel blocks. Input network Inevitably introduces additional image
noise. To this end, we study a data augmentation vision transformer (DAVT)
based on data augmentation and proposes a data augmentation method for
attention cropping, which uses attention weights as the guide to crop images
and improve the ability of the network to learn critical features. Secondly, we
also propose a hierarchical attention selection (HAS) method, which improves
the ability of discriminative markers between levels of learning by filtering
and fusing labels between levels. Experimental results show that the accuracy
of this method on the two general datasets, CUB-200-2011, and Stanford Dogs, is
better than the existing mainstream methods, and its accuracy is 1.4\% and
1.6\% higher than the original ViT, respectively",None,-1
1b977bc2-9976-451c-807f-c0119cbc69e5,GrASP: Gradient-Based Affordance Selection for Planning,0.0921994,"Planning with a learned model is arguably a key component of intelligence.
There are several challenges in realizing such a component in large-scale
reinforcement learning (RL) problems. One such challenge is dealing effectively
with continuous action spaces when using tree-search planning (e.g., it is not
feasible to consider every action even at just the root node of the tree). In
this paper we present a method for selecting affordances useful for planning --
for learning which small number of actions/options from a continuous space of
actions/options to consider in the tree-expansion process during planning. We
consider affordances that are goal-and-state-conditional mappings to
actions/options as well as unconditional affordances that simply select
actions/options available in all states. Our selection method is gradient
based: we compute gradients through the planning procedure to update the
parameters of the function that represents affordances. Our empirical work
shows that it is feasible to learn to select both primitive-action and option
affordances, and that simultaneously learning to select affordances and
planning with a learned value-equivalent model can outperform model-free RL.",None,-1
8a43500a-3725-4cd0-b6b4-2d08be215d13,MINER: Multiscale Implicit Neural Representations,0.412058,"We introduce a new neural signal model designed for efficient high-resolution
representation of large-scale signals. The key innovation in our multiscale
implicit neural representation (MINER) is an internal representation via a
Laplacian pyramid, which provides a sparse multiscale decomposition of the
signal that captures orthogonal parts of the signal across scales. We leverage
the advantages of the Laplacian pyramid by representing small disjoint patches
of the pyramid at each scale with a small MLP. This enables the capacity of the
network to adaptively increase from coarse to fine scales, and only represent
parts of the signal with strong signal energy. The parameters of each MLP are
optimized from coarse-to-fine scale which results in faster approximations at
coarser scales, thereby ultimately an extremely fast training process. We apply
MINER to a range of large-scale signal representation tasks, including
gigapixel images and very large point clouds, and demonstrate that it requires
fewer than 25% of the parameters, 33% of the memory footprint, and 10% of the
computation time of competing techniques such as ACORN to reach the same
representation accuracy.",https://vishwa91.github.io/miner,-1
89e6de5a-8a1d-4833-a11e-07b03b4a4099,Multi-Task Learning Framework for Extracting Emotion Cause Span and Entailment in Conversations,0.696856,"Predicting emotions expressed in text is a well-studied problem in the NLP
community. Recently there has been active research in extracting the cause of
an emotion expressed in text. Most of the previous work has done causal emotion
entailment in documents. In this work, we propose neural models to extract
emotion cause span and entailment in conversations. For learning such models,
we use RECCON dataset, which is annotated with cause spans at the utterance
level. In particular, we propose MuTEC, an end-to-end Multi-Task learning
framework for extracting emotions, emotion cause, and entailment in
conversations. This is in contrast to existing baseline models that use ground
truth emotions to extract the cause. MuTEC performs better than the baselines
for most of the data folds provided in the dataset.",https://github.com/Exploration-Lab/MuTEC,-1
2a3717d4-cb0f-4991-84f0-00c854fd30d6,VeriDark: A Large-Scale Benchmark for Authorship Verification on the Dark Web,0.522717,"The DarkWeb represents a hotbed for illicit activity, where users communicate
on different market forums in order to exchange goods and services. Law
enforcement agencies benefit from forensic tools that perform authorship
analysis, in order to identify and profile users based on their textual
content. However, authorship analysis has been traditionally studied using
corpora featuring literary texts such as fragments from novels or fan fiction,
which may not be suitable in a cybercrime context. Moreover, the few works that
employ authorship analysis tools for cybercrime prevention usually employ
ad-hoc experimental setups and datasets. To address these issues, we release
VeriDark: a benchmark comprised of three large scale authorship verification
datasets and one authorship identification dataset obtained from user activity
from either Dark Web related Reddit communities or popular illicit Dark Web
market forums. We evaluate competitive NLP baselines on the three datasets and
perform an analysis of the predictions to better understand the limitations of
such approaches. We make the datasets and baselines publicly available at
https://github.com/bit-ml/VeriDark",None,7673
78d7c1e6-6982-4775-a8b5-18dc5966d6b2,Enhanced Teaching-Learning-based Optimization for 3D Path Planning of Multicopter UAVs,0.121216,"This paper introduces a new path planning algorithm for unmanned aerial
vehicles (UAVs) based on the teaching-learning-based optimization (TLBO)
technique. We first define an objective function that incorporates requirements
on the path length and constraints on the movement and safe operation of UAVs
to convert the path planning into an optimization problem. The optimization
algorithm named Multi-subject TLBO is then proposed to minimize the formulated
objective function. The algorithm is developed based on TLBO but enhanced with
new operations including mutation, elite selection and multi-subject training
to improve the solution quality and speed up the convergence rate. Comparison
with state-of-the-art algorithms and experiments with real UAVs have been
conducted to evaluate the performance of the proposed algorithm. The results
confirm its validity and effectiveness in generating optimal, collision-free
and flyable paths for UAVs in complex operating environments.",None,-1
743f9007-e57e-46c0-b383-db0fb565108c,Practical Phase Retrieval Using Double Deep Image Priors,0.414824,"Phase retrieval (PR) concerns the recovery of complex phases from complex
magnitudes. We identify the connection between the difficulty level and the
number and variety of symmetries in PR problems. We focus on the most difficult
far-field PR (FFPR), and propose a novel method using double deep image priors.
In realistic evaluation, our method outperforms all competing methods by large
margins. As a single-instance method, our method requires no training data and
minimal hyperparameter tuning, and hence enjoys good practicality.",None,-1
77e28296-5960-409d-81ff-862ee5620125,Seeing Like a Toolkit: How Toolkits Envision the Work of AI Ethics,0.946931,"Numerous toolkits have been developed to support ethical AI development.
However, toolkits, like all tools, encode assumptions in their design about
what work should be done and how. In this paper, we conduct a qualitative
analysis of 27 AI ethics toolkits to critically examine how the work of ethics
is imagined and how it is supported by these toolkits. Specifically, we examine
the discourses toolkits rely on when talking about ethical issues, who they
imagine should do the work of ethics, and how they envision the work practices
involved in addressing ethics. Among the toolkits, we identify a mismatch
between the imagined work of ethics and the support the toolkits provide for
doing that work. In particular, we identify a lack of guidance around how to
navigate labor, organizational, and institutional power dynamics as they relate
to performing ethical work. We use these omissions to chart future work for
researchers and designers of AI ethics toolkits.",None,-1
7ffc3841-347f-4e6c-9c04-b6fb568a9bfe,What are the best systems? New perspectives on NLP Benchmarking,0.262474,"In Machine Learning, a benchmark refers to an ensemble of datasets associated
with one or multiple metrics together with a way to aggregate different systems
performances. They are instrumental in (i) assessing the progress of new
methods along different axes and (ii) selecting the best systems for practical
use. This is particularly the case for NLP with the development of large
pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a
variety of tasks. While the community mainly focused on developing new datasets
and metrics, there has been little interest in the aggregation procedure, which
is often reduced to a simple average over various performance measures.
However, this procedure can be problematic when the metrics are on a different
scale, which may lead to spurious conclusions. This paper proposes a new
procedure to rank systems based on their performance across different tasks.
Motivated by the social choice theory, the final system ordering is obtained
through aggregating the rankings induced by each task and is theoretically
grounded. We conduct extensive numerical experiments (on over 270k scores) to
assess the soundness of our approach both on synthetic and real scores (e.g.
GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method
yields different conclusions on state-of-the-art systems than the
mean-aggregation procedure while being both more reliable and robust.",None,-1
9907b102-64a7-4125-9c45-723cb7cf6a78,Graph Coloring with Physics-Inspired Graph Neural Networks,0.358783,"We show how graph neural networks can be used to solve the canonical graph
coloring problem. We frame graph coloring as a multi-class node classification
problem and utilize an unsupervised training strategy based on the statistical
physics Potts model. Generalizations to other multi-class problems such as
community detection, data clustering, and the minimum clique cover problem are
straightforward. We provide numerical benchmark results and illustrate our
approach with an end-to-end application for a real-world scheduling use case
within a comprehensive encode-process-decode framework. Our optimization
approach performs on par or outperforms existing solvers, with the ability to
scale to problems with millions of variables.",https://github.com/amazon-research/gcp-with-gnns-example,-1
6f192659-0726-4ea3-b76a-fa2bb612b0ab,WikiWhy: Answering and Explaining Cause-and-Effect Questions,0.191428,"As large language models (LLMs) grow larger and more sophisticated, assessing
their ""reasoning"" capabilities in natural language grows more challenging.
Recent question answering (QA) benchmarks that attempt to assess reasoning are
often limited by a narrow scope of covered situations and subject matters. We
introduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining
why an answer is true in natural language. WikiWhy contains over 9,000 ""why""
question-answer-rationale triples, grounded on Wikipedia facts across a diverse
set of topics. Each rationale is a set of supporting statements connecting the
question to the answer. WikiWhy serves as a benchmark for the reasoning
capabilities of LLMs because it demands rigorous explicit rationales for each
answer to demonstrate the acquisition of implicit commonsense knowledge, which
is unlikely to be easily memorized. GPT-3 baselines achieve only 38.7%
human-evaluated correctness in the end-to-end answer & explain condition,
leaving significant room for future improvements.",None,-1
cc4c0167-acda-4fc9-9cea-efbbdd24d38a,Holistic Sentence Embeddings for Better Out-of-Distribution Detection,0.308835,"Detecting out-of-distribution (OOD) instances is significant for the safe
deployment of NLP models. Among recent textual OOD detection works based on
pretrained language models (PLMs), distance-based methods have shown superior
performance. However, they estimate sample distance scores in the last-layer
CLS embedding space and thus do not make full use of linguistic information
underlying in PLMs. To address the issue, we propose to boost OOD detection by
deriving more holistic sentence embeddings. On the basis of the observations
that token averaging and layer combination contribute to improving OOD
detection, we propose a simple embedding approach named Avg-Avg, which averages
all token representations from each intermediate layer as the sentence
embedding and significantly surpasses the state-of-the-art on a comprehensive
suite of benchmarks by a 9.33% FAR95 margin. Furthermore, our analysis
demonstrates that it indeed helps preserve general linguistic knowledge in
fine-tuned PLMs and substantially benefits detecting background shifts. The
simple yet effective embedding method can be applied to fine-tuned PLMs with
negligible extra costs, providing a free gain in OOD detection. Our code is
available at https://github.com/lancopku/Avg-Avg.",https://github.com/lancopku/Avg-Avg,-1
5b917f6b-9457-408f-930b-65270d138b3a,Longtonotes: OntoNotes with Longer Coreference Chains,0.0336702,"Ontonotes has served as the most important benchmark for coreference
resolution. However, for ease of annotation, several long documents in
Ontonotes were split into smaller parts. In this work, we build a corpus of
coreference-annotated documents of significantly longer length than what is
currently available. We do so by providing an accurate, manually-curated,
merging of annotations from documents that were split into multiple parts in
the original Ontonotes annotation process. The resulting corpus, which we call
LongtoNotes contains documents in multiple genres of the English language with
varying lengths, the longest of which are up to 8x the length of documents in
Ontonotes, and 2x those in Litbank. We evaluate state-of-the-art neural
coreference systems on this new corpus, analyze the relationships between model
architectures/hyperparameters and document length on performance and efficiency
of the models, and demonstrate areas of improvement in long-document
coreference modeling revealed by our new corpus. Our data and code is available
at: https://github.com/kumar-shridhar/LongtoNotes.",https://github.com/kumar-shridhar/LongtoNotes,-1
64bcf43c-ac0e-4920-89c8-587e294e10e5,Multiview Stereo with Cascaded Epipolar RAFT,0.877271,"We address multiview stereo (MVS), an important 3D vision task that
reconstructs a 3D model such as a dense point cloud from multiple calibrated
images. We propose CER-MVS (Cascaded Epipolar RAFT Multiview Stereo), a new
approach based on the RAFT (Recurrent All-Pairs Field Transforms) architecture
developed for optical flow. CER-MVS introduces five new changes to RAFT:
epipolar cost volumes, cost volume cascading, multiview fusion of cost volumes,
dynamic supervision, and multiresolution fusion of depth maps. CER-MVS is
significantly different from prior work in multiview stereo. Unlike prior work,
which operates by updating a 3D cost volume, CER-MVS operates by updating a
disparity field. Furthermore, we propose an adaptive thresholding method to
balance the completeness and accuracy of the reconstructed point clouds.
Experiments show that our approach achieves competitive performance on DTU (the
second best among known results) and state-of-the-art performance on the
Tanks-and-Temples benchmark (both the intermediate and advanced set). Code is
available at https://github.com/princeton-vl/CER-MVS",https://github.com/princeton-vl/CER-MVS,-1
858c027f-fcab-429d-91ae-c70402febf85,DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines,0.514357,"Dialogue models are able to generate coherent and fluent responses, but they
can still be challenging to control and may produce non-engaging, unsafe
results. This unpredictability diminishes user trust and can hinder the use of
the models in the real world. To address this, we introduce DialGuide, a novel
framework for controlling dialogue model behavior using natural language rules,
or guidelines. These guidelines provide information about the context they are
applicable to and what should be included in the response, allowing the models
to generate responses that are more closely aligned with the developer's
expectations and intent. We evaluate DialGuide on three tasks in open-domain
dialogue response generation: guideline selection, response generation, and
response entailment verification. Our dataset contains 10,737 positive and
15,467 negative dialogue context-response-guideline triplets across two domains
- chit-chat and safety. We provide baseline models for the tasks and benchmark
their performance. We also demonstrate that DialGuide is effective in the
dialogue safety domain, producing safe and engaging responses that follow
developer guidelines.",https://github.com/alexa/dial-guide,-1
ab20f26d-5562-4487-8771-999d9aaa01b3,SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks,0.996267,"We present SkexGen, a novel autoregressive generative model for
computer-aided design (CAD) construction sequences containing
sketch-and-extrude modeling operations. Our model utilizes distinct Transformer
architectures to encode topological, geometric, and extrusion variations of
construction sequences into disentangled codebooks. Autoregressive Transformer
decoders generate CAD construction sequences sharing certain properties
specified by the codebook vectors. Extensive experiments demonstrate that our
disentangled codebook representation generates diverse and high-quality CAD
models, enhances user control, and enables efficient exploration of the design
space. The code is available at https://samxuxiang.github.io/skexgen.",https://samxuxiang.github.io/skexgen,-1
f38ef1b2-971d-4e9e-bca6-1774f6627e1a,Object-Guided Day-Night Visual Localization in Urban Scenes,0.436467,"We introduce Object-Guided Localization (OGuL) based on a novel method of
local-feature matching. Direct matching of local features is sensitive to
significant changes in illumination. In contrast, object detection often
survives severe changes in lighting conditions. The proposed method first
detects semantic objects and establishes correspondences of those objects
between images. Object correspondences provide local coarse alignment of the
images in the form of a planar homography. These homographies are consequently
used to guide the matching of local features. Experiments on standard urban
localization datasets (Aachen, Extended-CMU-Season, RobotCar-Season) show that
OGuL significantly improves localization results with as simple local features
as SIFT, and its performance competes with the state-of-the-art CNN-based
methods trained for day-to-night localization.",None,-1
d5b94a73-a8ae-4727-ac36-3c5998d901ce,A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank,0.527381,"We show that the choice of pretraining languages affects downstream
cross-lingual transfer for BERT-based models. We inspect zero-shot performance
in balanced data conditions to mitigate data size confounds, classifying
pretraining languages that improve downstream performance as donors, and
languages that are improved in zero-shot performance as recipients. We develop
a method of quadratic time complexity in the number of languages to estimate
these relations, instead of an exponential exhaustive computation of all
possible combinations. We find that our method is effective on a diverse set of
languages spanning different linguistic features and two downstream tasks. Our
findings can inform developers of large-scale multilingual language models in
choosing better pretraining configurations.",https://github.com/SLAB-NLP/linguistic-blood-bank,-1
42e5fb01-3f02-4ae2-9899-cdf8ebc35009,Transform your Smartphone into a DSLR Camera: Learning the ISP in the Wild,0.353294,"We propose a trainable Image Signal Processing (ISP) framework that produces
DSLR quality images given RAW images captured by a smartphone. To address the
color misalignments between training image pairs, we employ a color-conditional
ISP network and optimize a novel parametric color mapping between each input
RAW and reference DSLR image. During inference, we predict the target color
image by designing a color prediction network with efficient Global Context
Transformer modules. The latter effectively leverage global information to
learn consistent color and tone mappings. We further propose a robust masked
aligned loss to identify and discard regions with inaccurate motion estimation
during training. Lastly, we introduce the ISP in the Wild (ISPW) dataset,
consisting of weakly paired phone RAW and DSLR sRGB images. We extensively
evaluate our method, setting a new state-of-the-art on two datasets.",None,-1
9a1b1eea-73ff-44f6-b736-1b424ac44ae8,Transformer-Based Named Entity Recognition for French Using Adversarial Adaptation to Similar Domain Corpora,0.549094,"Named Entity Recognition (NER) involves the identification and classification
of named entities in unstructured text into predefined classes. NER in
languages with limited resources, like French, is still an open problem due to
the lack of large, robust, labelled datasets. In this paper, we propose a
transformer-based NER approach for French using adversarial adaptation to
similar domain or general corpora for improved feature extraction and better
generalization. We evaluate our approach on three labelled datasets and show
that our adaptation framework outperforms the corresponding non-adaptive models
for various combinations of transformer models, source datasets and target
corpora.",None,-1
ad70d2cd-3e4c-4737-9fc0-d9861eca2c2b,Generative Design Ideation: A Natural Language Generation Approach,0.332314,"This paper aims to explore a generative approach for knowledge-based design
ideation by applying the latest pre-trained language models in artificial
intelligence (AI). Specifically, a method of fine-tuning the generative
pre-trained transformer using the USPTO patent database is proposed. The
AI-generated ideas are not only in concise and understandable language but also
able to synthesize the target design with external knowledge sources with
controllable knowledge distance. The method is tested in a case study of
rolling toy design and the results show good performance in generating ideas of
varied novelty with near-field and far-field source knowledge.",None,-1
8b0b415f-5fe1-410b-a1a0-beee6a2c0fad,Cognitive Modeling of Semantic Fluency Using Transformers,0.0275761,"Can deep language models be explanatory models of human cognition? If so,
what are their limits? In order to explore this question, we propose an
approach called hyperparameter hypothesization that uses predictive
hyperparameter tuning in order to find individuating descriptors of
cognitive-behavioral profiles. We take the first step in this approach by
predicting human performance in the semantic fluency task (SFT), a well-studied
task in cognitive science that has never before been modeled using
transformer-based language models (TLMs). In our task setup, we compare several
approaches to predicting which word an individual performing SFT will utter
next. We report preliminary evidence suggesting that, despite obvious
implementational differences in how people and TLMs learn and use language,
TLMs can be used to identify individual differences in human fluency task
behaviors better than existing computational models, and may offer insights
into human memory retrieval strategies -- cognitive process not typically
considered to be the kinds of things TLMs can model. Finally, we discuss the
implications of this work for cognitive modeling of knowledge representations.",None,-1
21719211-0756-46d8-bf99-337400f9064f,A Two Parameters Equation for Word Rank-Frequency Relation,0.0609859,"Let $f (\cdot)$ be the absolute frequency of words and $r$ be the rank of
words in decreasing order of frequency, then the following function can fit the
rank-frequency relation \[ f (r;s,t) = \left(\frac{r_{\tt max}}{r}\right)^{1-s}
\left(\frac{r_{\tt max}+t \cdot r_{\tt exp}}{r+t \cdot r_{\tt
exp}}\right)^{1+(1+t)s} \] where $r_{\tt max}$ and $r_{\tt exp}$ are the
maximum and the expectation of the rank, respectively; $s>0$ and $t>0$ are
parameters estimated from data. On well-behaved data, there should be $s<1$ and
$s \cdot t < 1$.",None,-1
858ba835-f461-4bff-a7fa-b56eda9eed5d,FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs,0.851459,"Predicting the future motion of road agents is a critical task in an
autonomous driving pipeline. In this work, we address the problem of generating
a set of scene-level, or joint, future trajectory predictions in multi-agent
driving scenarios. To this end, we propose FJMP, a Factorized Joint Motion
Prediction framework for multi-agent interactive driving scenarios. FJMP models
the future scene interaction dynamics as a sparse directed interaction graph,
where edges denote explicit interactions between agents. We then prune the
graph into a directed acyclic graph (DAG) and decompose the joint prediction
task into a sequence of marginal and conditional predictions according to the
partial ordering of the DAG, where joint future trajectories are decoded using
a directed acyclic graph neural network (DAGNN). We conduct experiments on the
INTERACTION and Argoverse 2 datasets and demonstrate that FJMP produces more
accurate and scene-consistent joint trajectory predictions than non-factorized
approaches, especially on the most interactive and kinematically interesting
agents. FJMP ranks 1st on the multi-agent test leaderboard of the INTERACTION
dataset.",None,-1
4ecbf7fc-6a20-4a71-8838-683a9950c653,Xplique: A Deep Learning Explainability Toolbox,0.418994,"Today's most advanced machine-learning models are hardly scrutable. The key
challenge for explainability methods is to help assisting researchers in
opening up these black boxes, by revealing the strategy that led to a given
decision, by characterizing their internal states or by studying the underlying
data representation. To address this challenge, we have developed Xplique: a
software library for explainability which includes representative
explainability methods as well as associated evaluation metrics. It interfaces
with one of the most popular learning libraries: Tensorflow as well as other
libraries including PyTorch, scikit-learn and Theano. The code is licensed
under the MIT license and is freely available at github.com/deel-ai/xplique.",https://github.com/deel-ai/xplique,-1
d77c22bf-357a-4b42-98d8-8eaabd122dd7,Accelerating Code Search with Deep Hashing and Code Classification,0.47169,"Code search is to search reusable code snippets from source code corpus based
on natural languages queries. Deep learning-based methods of code search have
shown promising results. However, previous methods focus on retrieval accuracy
but lacked attention to the efficiency of the retrieval process. We propose a
novel method CoSHC to accelerate code search with deep hashing and code
classification, aiming to perform an efficient code search without sacrificing
too much accuracy. To evaluate the effectiveness of CoSHC, we apply our method
to five code search models. Extensive experimental results indicate that
compared with previous code search baselines, CoSHC can save more than 90% of
retrieval time meanwhile preserving at least 99% of retrieval accuracy.",None,-1
9a8fb4b3-4606-4c93-9f93-4efd0e9b5da4,BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment,0.731738,"This work addresses the Burst Super-Resolution (BurstSR) task using a new
architecture, which requires restoring a high-quality image from a sequence of
noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in
BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can
significantly improve the capability of extracting inter-frame information and
reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided
Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin
Transformer Blocks and Groups as our main backbone. More specifically, we
combine optical flows and deformable convolutions, hence our BSRT can handle
misalignment and aggregate the potential texture information in multi-frames
more efficiently. In addition, our Transformer-based structure can capture
long-range dependency to further improve the performance. The evaluation on
both synthetic and real-world tracks demonstrates that our approach achieves a
new state-of-the-art in BurstSR task. Further, our BSRT wins the championship
in the NTIRE2022 Burst Super-Resolution Challenge.",https://github.com/Algolzw/BSRT,-1
4e76cc0a-d55d-445f-81bb-2681bd0f9983,Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure,0.86535,"Generative models have demonstrated impressive results on Aspect-based
Sentiment Analysis (ABSA) tasks, particularly for the emerging task of
extracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these
models struggle with implicit sentiment expressions, which are commonly
observed in opinionated content such as online reviews. In this work, we
introduce GEN-SCL-NAT, which consists of two techniques for improved structured
generation for ACOS quadruple extraction. First, we propose GEN-SCL, a
supervised contrastive learning objective that aids quadruple prediction by
encouraging the model to produce input representations that are discriminable
across key input attributes, such as sentiment polarity and the existence of
implicit opinions and aspects. Second, we introduce GEN-NAT, a new structured
generation format that better adapts autoregressive encoder-decoder models to
extract quadruples in a generative fashion. Experimental results show that
GEN-SCL-NAT achieves top performance across three ACOS datasets, averaging
1.48% F1 improvement, with a maximum 1.73% increase on the LAPTOP-L1 dataset.
Additionally, we see significant gains on implicit aspect and opinion splits
that have been shown as challenging for existing ACOS approaches.",https://github.com/jpeper/GEN_SCL_NAT,-1
863ada7c-d3e5-4da4-925d-38b6111d1bef,Hyperbolic Image Segmentation,0.748847,"For image segmentation, the current standard is to perform pixel-level
optimization and inference in Euclidean output embedding spaces through linear
hyperplanes. In this work, we show that hyperbolic manifolds provide a valuable
alternative for image segmentation and propose a tractable formulation of
hierarchical pixel-level classification in hyperbolic space. Hyperbolic Image
Segmentation opens up new possibilities and practical benefits for
segmentation, such as uncertainty estimation and boundary information for free,
zero-label generalization, and increased performance in low-dimensional output
embeddings.",https://github.com/MinaGhadimiAtigh/HyperbolicImageSegmentation,-1
ef8ea6f7-e7da-4605-aa79-67abb4145f71,Graph-in-Graph Network for Automatic Gene Ontology Description Generation,0.460365,"Gene Ontology (GO) is the primary gene function knowledge base that enables
computational tasks in biomedicine. The basic element of GO is a term, which
includes a set of genes with the same function. Existing research efforts of GO
mainly focus on predicting gene term associations. Other tasks, such as
generating descriptions of new terms, are rarely pursued. In this paper, we
propose a novel task: GO term description generation. This task aims to
automatically generate a sentence that describes the function of a GO term
belonging to one of the three categories, i.e., molecular function, biological
process, and cellular component. To address this task, we propose a
Graph-in-Graph network that can efficiently leverage the structural information
of GO. The proposed network introduces a two-layer graph: the first layer is a
graph of GO terms where each node is also a graph (gene graph). Such a
Graph-in-Graph network can derive the biological functions of GO terms and
generate proper descriptions. To validate the effectiveness of the proposed
network, we build three large-scale benchmark datasets. By incorporating the
proposed Graph-in-Graph network, the performances of seven different
sequence-to-sequence models can be substantially boosted across all evaluation
metrics, with up to 34.7%, 14.5%, and 39.1% relative improvements in BLEU,
ROUGE-L, and METEOR, respectively.",None,-1
71671215-c49d-4e4f-bef8-0dcb518c43a0,Large-Field Contextual Feature Learning for Glass Detection,0.773657,"Glass is very common in our daily life. Existing computer vision systems
neglect it and thus may have severe consequences, e.g., a robot may crash into
a glass wall. However, sensing the presence of glass is not straightforward.
The key challenge is that arbitrary objects/scenes can appear behind the glass.
In this paper, we propose an important problem of detecting glass surfaces from
a single RGB image. To address this problem, we construct the first large-scale
glass detection dataset (GDD) and propose a novel glass detection network,
called GDNet-B, which explores abundant contextual cues in a large
field-of-view via a novel large-field contextual feature integration (LCFI)
module and integrates both high-level and low-level boundary features with a
boundary feature enhancement (BFE) module. Extensive experiments demonstrate
that our GDNet-B achieves satisfying glass detection results on the images
within and beyond the GDD testing set. We further validate the effectiveness
and generalization capability of our proposed GDNet-B by applying it to other
vision tasks, including mirror segmentation and salient object detection.
Finally, we show the potential applications of glass detection and discuss
possible future research directions.",None,-1
b2f75d53-2aeb-4eb6-a944-a1b0c4b9fe64,Flow-Guided Transformer for Video Inpainting,0.617272,"We propose a flow-guided transformer, which innovatively leverage the motion
discrepancy exposed by optical flows to instruct the attention retrieval in
transformer for high fidelity video inpainting. More specially, we design a
novel flow completion network to complete the corrupted flows by exploiting the
relevant flow features in a local temporal window. With the completed flows, we
propagate the content across video frames, and adopt the flow-guided
transformer to synthesize the rest corrupted regions. We decouple transformers
along temporal and spatial dimension, so that we can easily integrate the
locally relevant completed flows to instruct spatial attention only.
Furthermore, we design a flow-reweight module to precisely control the impact
of completed flows on each spatial transformer. For the sake of efficiency, we
introduce window partition strategy to both spatial and temporal transformers.
Especially in spatial transformer, we design a dual perspective spatial MHSA,
which integrates the global tokens to the window-based attention. Extensive
experiments demonstrate the effectiveness of the proposed method qualitatively
and quantitatively. Codes are available at https://github.com/hitachinsk/FGT.",https://github.com/hitachinsk/FGT,-1
00fa2060-c0cf-49ad-86ec-ec203b979eb8,Learning a Pedestrian Social Behavior Dictionary,0.226494,"Understanding pedestrian behavior patterns is a key component to building
autonomous agents that can navigate among humans. We seek a learned dictionary
of pedestrian behavior to obtain a semantic description of pedestrian
trajectories. Supervised methods for dictionary learning are impractical since
pedestrian behaviors may be unknown a priori and the process of manually
generating behavior labels is prohibitively time consuming. We instead utilize
a novel, unsupervised framework to create a taxonomy of pedestrian behavior
observed in a specific space. First, we learn a trajectory latent space that
enables unsupervised clustering to create an interpretable pedestrian behavior
dictionary. We show the utility of this dictionary for building pedestrian
behavior maps to visualize space usage patterns and for computing the
distributions of behaviors. We demonstrate a simple but effective trajectory
prediction by conditioning on these behavior labels. While many trajectory
analysis methods rely on RNNs or transformers, we develop a lightweight,
low-parameter approach and show results comparable to SOTA on the ETH and UCY
datasets.",None,-1
79448429-7471-4d08-b65f-f0a6529ae2d8,Boundary-Guided Camouflaged Object Detection,0.764698,"Camouflaged object detection (COD), segmenting objects that are elegantly
blended into their surroundings, is a valuable yet challenging task. Existing
deep-learning methods often fall into the difficulty of accurately identifying
the camouflaged object with complete and fine object structure. To this end, in
this paper, we propose a novel boundary-guided network (BGNet) for camouflaged
object detection. Our method explores valuable and extra object-related edge
semantics to guide representation learning of COD, which forces the model to
generate features that highlight object structure, thereby promoting
camouflaged object detection of accurate boundary localization. Extensive
experiments on three challenging benchmark datasets demonstrate that our BGNet
significantly outperforms the existing 18 state-of-the-art methods under four
widely-used evaluation metrics. Our code is publicly available at:
https://github.com/thograce/BGNet.",None,-1
340caa25-454f-443d-a6a2-7ea2d7d607a8,DialCrowd 2.0: A Quality-Focused Dialog System Crowdsourcing Toolkit,0.437403,"Dialog system developers need high-quality data to train, fine-tune and
assess their systems. They often use crowdsourcing for this since it provides
large quantities of data from many workers. However, the data may not be of
sufficiently good quality. This can be due to the way that the requester
presents a task and how they interact with the workers. This paper introduces
DialCrowd 2.0 to help requesters obtain higher quality data by, for example,
presenting tasks more clearly and facilitating effective communication with
workers. DialCrowd 2.0 guides developers in creating improved Human
Intelligence Tasks (HITs) and is directly applicable to the workflows used
currently by developers and researchers.",None,14484
8cfff110-d82d-4286-bc82-c391971defb7,T4DT: Tensorizing Time for Learning Temporal 3D Visual Data,0.288407,"Unlike 2D raster images, there is no single dominant representation for 3D
visual data processing. Different formats like point clouds, meshes, or
implicit functions each have their strengths and weaknesses. Still, grid
representations such as signed distance functions have attractive properties
also in 3D. In particular, they offer constant-time random access and are
eminently suitable for modern machine learning. Unfortunately, the storage size
of a grid grows exponentially with its dimension. Hence they often exceed
memory limits even at moderate resolution. This work proposes using low-rank
tensor formats, including the Tucker, tensor train, and quantics tensor train
decompositions, to compress time-varying 3D data. Our method iteratively
computes, voxelizes, and compresses each frame's truncated signed distance
function and applies tensor rank truncation to condense all frames into a
single, compressed tensor that represents the entire 4D scene. We show that
low-rank tensor compression is extremely compact to store and query
time-varying signed distance functions. It significantly reduces the memory
footprint of 4D scenes while remarkably preserving their geometric quality.
Unlike existing, iterative learning-based approaches like DeepSDF and NeRF, our
method uses a closed-form algorithm with theoretical guarantees.",https://github.com/prs-eth/T4DT,-1
a42b649d-fe95-451d-a3da-cb5927be33e5,Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust,0.503229,"Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact
descriptions according to rule of law, serves as legal assistance to mitigate
the great work burden of limited legal practitioners. Most existing methods
apply various large-scale pre-trained language models (PLMs) finetuned in LJP
tasks to obtain consistent improvements. However, we discover the fact that the
state-of-the-art (SOTA) model makes judgment predictions according to
irrelevant (or non-casual) information. The violation of rule of law not only
weakens the robustness and generalization ability of models but also results in
severe social problems like discrimination. In this paper, we use causal
structural models (SCMs) to theoretically analyze how LJP models learn to make
decisions and why they can succeed in passing the traditional testing paradigm
without learning causality. According to our analysis, we provide two solutions
intervening on data and model by causality, respectively. In detail, we first
distinguish non-causal information by applying the open information extraction
(OIE) technique. Then, we propose a method named the Causal Information
Enhanced SAmpling Method (CIESAM) to eliminate the non-causal information from
data. To validate our theoretical analysis, we further propose another method
using our proposed Causality-Aware Self-Attention Mechanism (CASAM) to guide
the model to learn the underlying causality knowledge in legal texts. The
confidence of CASAM in learning causal information is higher than that of
CIESAM. The extensive experimental results show that both our proposed methods
achieve state-of-the-art (SOTA) performance on three commonly used
legal-specific datasets. The stronger performance of CASAM further demonstrates
that causality is the key to the robustness and generalization ability of
models.",None,-1
c08c74ad-aeab-4d59-adb9-02bc8ada192e,Bayes risk CTC: Controllable CTC alignment in Sequence-to-Sequence tasks,0.793711,"Sequence-to-Sequence (seq2seq) tasks transcribe the input sequence to a
target sequence. The Connectionist Temporal Classification (CTC) criterion is
widely used in multiple seq2seq tasks. Besides predicting the target sequence,
a side product of CTC is to predict the alignment, which is the most probable
input-long sequence that specifies a hard aligning relationship between the
input and target units. As there are multiple potential aligning sequences
(called paths) that are equally considered in CTC formulation, the choice of
which path will be most probable and become the predicted alignment is always
uncertain. In addition, it is usually observed that the alignment predicted by
vanilla CTC will drift compared with its reference and rarely provides
practical functionalities. Thus, the motivation of this work is to make the CTC
alignment prediction controllable and thus equip CTC with extra
functionalities. The Bayes risk CTC (BRCTC) criterion is then proposed in this
work, in which a customizable Bayes risk function is adopted to enforce the
desired characteristics of the predicted alignment. With the risk function, the
BRCTC is a general framework to adopt some customizable preference over the
paths in order to concentrate the posterior into a particular subset of the
paths. In applications, we explore one particular preference which yields
models with the down-sampling ability and reduced inference costs. By using
BRCTC with another preference for early emissions, we obtain an improved
performance-latency trade-off for online models. Experimentally, the proposed
BRCTC reduces the inference cost of offline models by up to 47% without
performance degradation and cuts down the overall latency of online systems to
an unseen level.",https://github.com/k2-fsa/k2,-1
a980515c-7b6b-40fe-8391-85805fd821fe,Synthesis and Reconstruction of Fingerprints using Generative Adversarial Networks,0.205379,"Deep learning-based models have been shown to improve the accuracy of
fingerprint recognition. While these algorithms show exceptional performance,
they require large-scale fingerprint datasets for training and evaluation. In
this work, we propose a novel fingerprint synthesis and reconstruction
framework based on the StyleGan2 architecture, to address the privacy issues
related to the acquisition of such large-scale datasets. We also derive a
computational approach to modify the attributes of the generated fingerprint
while preserving their identity. This allows synthesizing multiple different
fingerprint images per finger. In particular, we introduce the SynFing
synthetic fingerprints dataset consisting of 100K image pairs, each pair
corresponding to the same identity. The proposed framework was experimentally
shown to outperform contemporary state-of-the-art approaches for both
fingerprint synthesis and reconstruction. It significantly improved the realism
of the generated fingerprints, both visually and in terms of their ability to
spoof fingerprint-based verification systems. The code and fingerprints dataset
are publicly available: https://github.com/rafaelbou/fingerprint_generator.",https://github.com/rafaelbou/fingerprint-generator/,-1
df305a25-6145-419d-b079-627817c4bbce,Dual-Geometric Space Embedding Model for Two-View Knowledge Graphs,0.692489,"Two-view knowledge graphs (KGs) jointly represent two components: an ontology
view for abstract and commonsense concepts, and an instance view for specific
entities that are instantiated from ontological concepts. As such, these KGs
contain heterogeneous structures that are hierarchical, from the ontology-view,
and cyclical, from the instance-view. Despite these various structures in KGs,
most recent works on embedding KGs assume that the entire KG belongs to only
one of the two views but not both simultaneously. For works that seek to put
both views of the KG together, the instance and ontology views are assumed to
belong to the same geometric space, such as all nodes embedded in the same
Euclidean space or non-Euclidean product space, an assumption no longer
reasonable for two-view KGs where different portions of the graph exhibit
different structures. To address this issue, we define and construct a
dual-geometric space embedding model (DGS) that models two-view KGs using a
complex non-Euclidean geometric space, by embedding different portions of the
KG in different geometric spaces. DGS utilizes the spherical space, hyperbolic
space, and their intersecting space in a unified framework for learning
embeddings. Furthermore, for the spherical space, we propose novel closed
spherical space operators that directly operate in the spherical space without
the need for mapping to an approximate tangent space. Experiments on public
datasets show that DGS significantly outperforms previous state-of-the-art
baseline models on KG completion tasks, demonstrating its ability to better
model heterogeneous structures in KGs.",https://github.com/roshnigiyer/dgs,-1
9e0d93d7-1a18-45a1-8b88-fdbe36f1bd3e,Debiasing Word Embeddings with Nonlinear Geometry,0.423781,"Debiasing word embeddings has been largely limited to individual and
independent social categories. However, real-world corpora typically present
multiple social categories that possibly correlate or intersect with each
other. For instance, ""hair weaves"" is stereotypically associated with African
American females, but neither African American nor females alone. Therefore,
this work studies biases associated with multiple social categories: joint
biases induced by the union of different categories and intersectional biases
that do not overlap with the biases of the constituent categories. We first
empirically observe that individual biases intersect non-trivially (i.e., over
a one-dimensional subspace). Drawing from the intersectional theory in social
science and the linguistic theory, we then construct an intersectional subspace
to debias for multiple social categories using the nonlinear geometry of
individual biases. Empirical evaluations corroborate the efficacy of our
approach. Data and implementation code can be downloaded at
https://github.com/GitHubLuCheng/Implementation-of-JoSEC-COLING-22.",https://github.com/GitHubLuCheng/Implementation-of-JoSEC-COLING-22,-1
0d75ae0b-cc63-49be-a1c7-23ba36930748,How would Stance Detection Techniques Evolve after the Launch of ChatGPT?,0.784596,"Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection.",None,648
a82efd8e-c210-44f8-9335-908829c51b4c,Federated Learning: Balancing the Thin Line Between Data Intelligence and Privacy,0.0987622,"Federated learning holds great promise in learning from fragmented sensitive
data and has revolutionized how machine learning models are trained. This
article provides a systematic overview and detailed taxonomy of federated
learning. We investigate the existing security challenges in federated learning
and provide a comprehensive overview of established defense techniques for data
poisoning, inference attacks, and model poisoning attacks. The work also
presents an overview of current training challenges for federated learning,
focusing on handling non-i.i.d. data, high dimensionality issues, and
heterogeneous architecture, and discusses several solutions for the associated
challenges. Finally, we discuss the remaining challenges in managing federated
learning training and suggest focused research directions to address the open
questions. Potential candidate areas for federated learning, including IoT
ecosystem, healthcare applications, are discussed with a particular focus on
banking and financial domains.",None,-1
c73e516e-cb87-45b7-8c81-c258feba2e44,PointInst3D: Segmenting 3D Instances by Points,0.691061,"The current state-of-the-art methods in 3D instance segmentation typically
involve a clustering step, despite the tendency towards heuristics, greedy
algorithms, and a lack of robustness to the changes in data statistics. In
contrast, we propose a fully-convolutional 3D point cloud instance segmentation
method that works in a per-point prediction fashion. In doing so it avoids the
challenges that clustering-based methods face: introducing dependencies among
different tasks of the model. We find the key to its success is assigning a
suitable target to each sampled point. Instead of the commonly used static or
distance-based assignment strategies, we propose to use an Optimal Transport
approach to optimally assign target masks to the sampled points according to
the dynamic matching costs. Our approach achieves promising results on both
ScanNet and S3DIS benchmarks. The proposed approach removes intertask
dependencies and thus represents a simpler and more flexible 3D instance
segmentation framework than other competing methods, while achieving improved
segmentation accuracy.",None,-1
73428662-21b4-47a9-8231-3bb9efd6c27b,Hyper-parameter tuning of physics-informed neural networks: Application to Helmholtz problems,0.564891,"We consider physics-informed neural networks (PINNs) [Raissi et al.,
J.~Comput. Phys. 278 (2019) 686-707] for forward physical problems. In order to
find optimal PINNs configuration, we introduce a hyper-parameter optimization
(HPO) procedure via Gaussian processes-based Bayesian optimization. We apply
the HPO to Helmholtz equation for bounded domains and conduct a thorough study,
focusing on: (i) performance, (ii) the collocation points density $r$ and (iii)
the frequency $\kappa$, confirming the applicability and necessity of the
method. Numerical experiments are performed in two and three dimensions,
including comparison to finite element methods.",https://github.com/lululxvi/deepxde/,-1
64a662f3-7e5d-430c-a486-512e24d0d5c7,KD-MVS: Knowledge Distillation Based Self-supervised Learning for Multi-view Stereo,0.46226,"Supervised multi-view stereo (MVS) methods have achieved remarkable progress
in terms of reconstruction quality, but suffer from the challenge of collecting
large-scale ground-truth depth. In this paper, we propose a novel
self-supervised training pipeline for MVS based on knowledge distillation,
termed KD-MVS, which mainly consists of self-supervised teacher training and
distillation-based student training. Specifically, the teacher model is trained
in a self-supervised fashion using both photometric and featuremetric
consistency. Then we distill the knowledge of the teacher model to the student
model through probabilistic knowledge transferring. With the supervision of
validated knowledge, the student model is able to outperform its teacher by a
large margin. Extensive experiments performed on multiple datasets show our
method can even outperform supervised methods.",https://github.com/megvii-research/KD-MVS,-1
529692b5-da99-4fb8-b25a-94782d034dfa,An Application of Pseudo-Log-Likelihoods to Natural Language Scoring,0.145375,"Language models built using semi-supervised machine learning on large corpora
of natural language have very quickly enveloped the fields of natural language
generation and understanding. In this paper we apply a zero-shot approach
independently developed by a number of researchers now gaining recognition as a
significant alternative to fine-tuning for evaluation on common sense tasks. A
language model with relatively few parameters and training steps compared to a
more recent language model (T5) can outperform it on a recent large data set
(TimeDial), while displaying robustness in its performance across a similar
class of language tasks. Surprisingly, this result is achieved by using a
hyperparameter-free zero-shot method with the smaller model, compared to
fine-tuning to the larger model. We argue that robustness of the smaller model
ought to be understood in terms of compositionality, in a sense that we draw
from recent literature on a class of similar models. We identify a practical
cost for our method and model: high GPU-time for natural language evaluation.
The zero-shot measurement technique that produces remarkable stability, both
for ALBERT and other BERT variants, is an application of pseudo-log-likelihoods
to masked language models for the relative measurement of probability for
substitution alternatives in forced choice language tasks such as the Winograd
Schema Challenge, Winogrande, and others. One contribution of this paper is to
bring together a number of similar, but independent strands of research. We
produce some absolute state-of-the-art results for common sense reasoning in
binary choice tasks, performing better than any published result in the
literature, including fine-tuned efforts. We show a remarkable consistency of
the model's performance under adversarial settings, which we argue is best
explained by the model's compositionality of representations.",https://anonymous.4open.science/r/NotSoFineTuning-4620/,-1
9d63739b-15a7-42ad-9d12-08b68f51eb1e,Bounding Counterfactuals under Selection Bias,0.586299,"Causal analysis may be affected by selection bias, which is defined as the
systematic exclusion of data from a certain subpopulation. Previous work in
this area focused on the derivation of identifiability conditions. We propose
instead a first algorithm to address both identifiable and unidentifiable
queries. We prove that, in spite of the missingness induced by the selection
bias, the likelihood of the available data is unimodal. This enables us to use
the causal expectation-maximisation scheme to obtain the values of causal
queries in the identifiable case, and to compute bounds otherwise. Experiments
demonstrate the approach to be practically viable. Theoretical convergence
characterisations are provided.",https://github.com/IDSIA-papers/2022-PGM-selection,-1
8c92f3fe-0adf-489b-b4b4-9a43a20d5cc9,Variational Reasoning over Incomplete Knowledge Graphs for Conversational Recommendation,0.629163,"Conversational recommender systems (CRSs) often utilize external knowledge
graphs (KGs) to introduce rich semantic information and recommend relevant
items through natural language dialogues. However, original KGs employed in
existing CRSs are often incomplete and sparse, which limits the reasoning
capability in recommendation. Moreover, only few of existing studies exploit
the dialogue context to dynamically refine knowledge from KGs for better
recommendation. To address the above issues, we propose the Variational
Reasoning over Incomplete KGs Conversational Recommender (VRICR). Our key idea
is to incorporate the large dialogue corpus naturally accompanied with CRSs to
enhance the incomplete KGs; and perform dynamic knowledge reasoning conditioned
on the dialogue context. Specifically, we denote the dialogue-specific
subgraphs of KGs as latent variables with categorical priors for adaptive
knowledge graphs refactor. We propose a variational Bayesian method to
approximate posterior distributions over dialogue-specific subgraphs, which not
only leverages the dialogue corpus for restructuring missing entity relations
but also dynamically selects knowledge based on the dialogue context. Finally,
we infuse the dialogue-specific subgraphs to decode the recommendation and
responses. We conduct experiments on two benchmark CRSs datasets. Experimental
results confirm the effectiveness of our proposed method.",https://github.com/zxd-octopus/VRICR,-1
6f65ee21-18cf-4333-b61f-d220bfc7f2de,Deep Deformable 3D Caricatures with Learned Shape Control,0.19438,"A 3D caricature is an exaggerated 3D depiction of a human face. The goal of
this paper is to model the variations of 3D caricatures in a compact parameter
space so that we can provide a useful data-driven toolkit for handling 3D
caricature deformations. To achieve the goal, we propose an MLP-based framework
for building a deformable surface model, which takes a latent code and produces
a 3D surface. In the framework, a SIREN MLP models a function that takes a 3D
position on a fixed template surface and returns a 3D displacement vector for
the input position. We create variations of 3D surfaces by learning a
hypernetwork that takes a latent code and produces the parameters of the MLP.
Once learned, our deformable model provides a nice editing space for 3D
caricatures, supporting label-based semantic editing and point-handle-based
deformation, both of which produce highly exaggerated and natural 3D caricature
shapes. We also demonstrate other applications of our deformable model, such as
automatic 3D caricature creation.",https://github.com/ycjungSubhuman/DeepDeformable3DCaricatures,-1
31fc82dc-2c7d-4093-b678-b48892e49c54,E-Graph: Minimal Solution for Rigid Rotation with Extensibility Graphs,0.274037,"Minimal solutions for relative rotation and translation estimation tasks have
been explored in different scenarios, typically relying on the so-called
co-visibility graph. However, how to build direct rotation relationships
between two frames without overlap is still an open topic, which, if solved,
could greatly improve the accuracy of visual odometry.
  In this paper, a new minimal solution is proposed to solve relative rotation
estimation between two images without overlapping areas by exploiting a new
graph structure, which we call Extensibility Graph (E-Graph). Differently from
a co-visibility graph, high-level landmarks, including vanishing directions and
plane normals, are stored in our E-Graph, which are geometrically extensible.
Based on E-Graph, the rotation estimation problem becomes simpler and more
elegant, as it can deal with pure rotational motion and requires fewer
assumptions, e.g. Manhattan/Atlanta World, planar/vertical motion. Finally, we
embed our rotation estimation strategy into a complete camera tracking and
mapping system which obtains 6-DoF camera poses and a dense 3D mesh model.
  Extensive experiments on public benchmarks demonstrate that the proposed
method achieves state-of-the-art tracking performance.",None,-1
7f9d10c7-71ba-4c98-b975-9fb711893cd9,Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography,0.416418,"Although robust PCA has been increasingly adopted to extract vessels from
X-ray coronary angiography (XCA) images, challenging problems such as
inefficient vessel-sparsity modelling, noisy and dynamic background artefacts,
and high computational cost still remain unsolved. Therefore, we propose a
novel robust PCA unrolling network with sparse feature selection for
super-resolution XCA vessel imaging. Being embedded within a patch-wise
spatiotemporal super-resolution framework that is built upon a pooling layer
and a convolutional long short-term memory network, the proposed network can
not only gradually prune complex vessel-like artefacts and noisy backgrounds in
XCA during network training but also iteratively learn and select the
high-level spatiotemporal semantic information of moving contrast agents
flowing in the XCA-imaged vessels. The experimental results show that the
proposed method significantly outperforms state-of-the-art methods, especially
in the imaging of the vessel network and its distal vessels, by restoring the
intensity and geometry profiles of heterogeneous vessels against complex and
dynamic backgrounds.",None,-1
4d441496-bd1b-4f50-a666-217c90ae5a36,Multi-hop Evidence Retrieval for Cross-document Relation Extraction,0.711505,"Relation Extraction (RE) has been extended to cross-document scenarios
because many relations are not simply described in a single document. This
inevitably brings the challenge of efficient open-space evidence retrieval to
support the inference of cross-document relations, along with the challenge of
multi-hop reasoning on top of entities and evidence scattered in an open set of
documents. To combat these challenges, we propose MR.COD (Multi-hop evidence
retrieval for Cross-document relation extraction), which is a multi-hop
evidence retrieval method based on evidence path mining and ranking. We explore
multiple variants of retrievers to show evidence retrieval is essential in
cross-document RE. We also propose a contextual dense retriever for this
setting. Experiments on CodRED show that evidence retrieval with MR.COD
effectively acquires crossdocument evidence and boosts end-to-end RE
performance in both closed and open settings.",https://github.com/luka-group/MrCoD,-1
cd9c345e-1c81-467e-9961-4a010ec831a3,An LSTM model for Twitter Sentiment Analysis,0.649342,"Sentiment analysis on social media such as Twitter provides organizations and
individuals an effective way to monitor public emotions towards them and their
competitors. As a result, sentiment analysis has become an important and
challenging task. In this work, we have collected seven publicly available and
manually annotated twitter sentiment datasets. We create a new training and
testing dataset from the collected datasets. We develop an LSTM model to
classify sentiment of a tweet and evaluate the model with the new dataset.",None,-1
9af23ada-a457-43e4-8d22-47e4eddb4352,Conformal Prediction is Robust to Dispersive Label Noise,0.0374761,"We study the robustness of conformal prediction, a powerful tool for
uncertainty quantification, to label noise. Our analysis tackles both
regression and classification problems, characterizing when and how it is
possible to construct uncertainty sets that correctly cover the unobserved
noiseless ground truth labels. We further extend our theory and formulate the
requirements for correctly controlling a general loss function, such as the
false negative proportion, with noisy labels. Our theory and experiments
suggest that conformal prediction and risk-controlling techniques with noisy
labels attain conservative risk over the clean ground truth labels except in
adversarial cases. In such cases, we can also correct for noise of bounded size
in the conformal prediction algorithm in order to ensure achieving the correct
risk of the ground truth labels without score or data regularity.",None,-1
9421cb9d-0fb4-4d4a-8334-a885001a25ce,Classifying Unstructured Clinical Notes via Automatic Weak Supervision,0.372204,"Healthcare providers usually record detailed notes of the clinical care
delivered to each patient for clinical, research, and billing purposes. Due to
the unstructured nature of these narratives, providers employ dedicated staff
to assign diagnostic codes to patients' diagnoses using the International
Classification of Diseases (ICD) coding system. This manual process is not only
time-consuming but also costly and error-prone. Prior work demonstrated
potential utility of Machine Learning (ML) methodology in automating this
process, but it has relied on large quantities of manually labeled data to
train the models. Additionally, diagnostic coding systems evolve with time,
which makes traditional supervised learning strategies unable to generalize
beyond local applications. In this work, we introduce a general
weakly-supervised text classification framework that learns from class-label
descriptions only, without the need to use any human-labeled documents. It
leverages the linguistic domain knowledge stored within pre-trained language
models and the data programming framework to assign code labels to individual
texts. We demonstrate the efficacy and flexibility of our method by comparing
it to state-of-the-art weak text classifiers across four real-world text
classification datasets, in addition to assigning ICD codes to medical notes in
the publicly available MIMIC-III database.",https://github.com/autonlab/KeyClass,-1
99d99c3e-7b40-451c-8d2f-8ce9e980012a,Exploring Visual Prompts for Adapting Large-Scale Models,0.827194,"We investigate the efficacy of visual prompting to adapt large-scale models
in vision. Following the recent approach from prompt tuning and adversarial
reprogramming, we learn a single image perturbation such that a frozen model
prompted with this perturbation performs a new task. Through comprehensive
experiments, we demonstrate that visual prompting is particularly effective for
CLIP and robust to distribution shift, achieving performance competitive with
standard linear probes. We further analyze properties of the downstream
dataset, prompt design, and output transformation in regard to adaptation
performance. The surprising effectiveness of visual prompting provides a new
perspective on adapting pre-trained models in vision. Code is available at
http://hjbahng.github.io/visual_prompting .",https://hjbahng.github.io/visual_prompting/,-1
18570b69-d97a-408c-91c7-c3bb4732bec9,QCRI's COVID-19 Disinformation Detector: A System to Fight the COVID-19 Infodemic in Social Media,0.258738,"Fighting the ongoing COVID-19 infodemic has been declared as one of the most
important focus areas by the World Health Organization since the onset of the
COVID-19 pandemic. While the information that is consumed and disseminated
consists of promoting fake cures, rumors, and conspiracy theories to spreading
xenophobia and panic, at the same time there is information (e.g., containing
advice, promoting cure) that can help different stakeholders such as
policy-makers. Social media platforms enable the infodemic and there has been
an effort to curate the content on such platforms, analyze and debunk them.
While a majority of the research efforts consider one or two aspects (e.g.,
detecting factuality) of such information, in this study we focus on a
multifaceted approach, including an
API,\url{https://app.swaggerhub.com/apis/yifan2019/Tanbih/0.8.0/} and a demo
system,\url{https://covid19.tanbih.org}, which we made freely and publicly
available. We believe that this will facilitate researchers and different
stakeholders. A screencast of the API services and demo is
available.\url{https://youtu.be/zhbcSvxEKMk}",https://github.com/GateNLP/CANTM,-1
5cc5aec8-1134-4904-bc57-99fdc1fd23b0,Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training,0.15688,"In this paper, we present a cross-modal recipe retrieval framework,
Transformer-based Network for Large Batch Training (TNLBT), which is inspired
by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).
TNLBT aims to accomplish retrieval tasks while generating images from recipe
embeddings. We apply the Hierarchical Transformer-based recipe text encoder,
the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial
network architecture to enable better cross-modal embedding learning for recipe
texts and images. In addition, we use self-supervised learning to exploit the
rich information in the recipe texts having no corresponding images. Since
contrastive learning could benefit from a larger batch size according to the
recent literature on self-supervised learning, we adopt a large batch size
during training and have validated its effectiveness. In the experiments, the
proposed framework significantly outperformed the current state-of-the-art
frameworks in both cross-modal recipe retrieval and image generation tasks on
the benchmark Recipe1M. This is the first work which confirmed the
effectiveness of large batch training on cross-modal recipe embeddings.",https://github.com/mseitzer/pytorch-fid,-1
728794c5-14b3-4957-9aa1-5cacef11c4d7,Clickbait Spoiling via Question Answering and Passage Retrieval,0.998871,"We introduce and study the task of clickbait spoiling: generating a short
text that satisfies the curiosity induced by a clickbait post. Clickbait links
to a web page and advertises its contents by arousing curiosity instead of
providing an informative summary. Our contributions are approaches to classify
the type of spoiler needed (i.e., a phrase or a passage), and to generate
appropriate spoilers. A large-scale evaluation and error analysis on a new
corpus of 5,000 manually spoiled clickbait posts -- the Webis Clickbait
Spoiling Corpus 2022 -- shows that our spoiler type classifier achieves an
accuracy of 80%, while the question answering model DeBERTa-large outperforms
all others in generating spoilers for both types.",https://github.com/webis-de/ACL-22,-1
30eb50dd-c375-4dcf-a7eb-42aded86fc46,Individual Topology Structure of Eye Movement Trajectories,0.249477,"Traditionally, extracting patterns from eye movement data relies on
statistics of different macro-events such as fixations and saccades. This
requires an additional preprocessing step to separate the eye movement
subtypes, often with a number of parameters on which the classification results
depend. Besides that, definitions of such macro events are formulated in
different ways by different researchers.
  We propose an application of a new class of features to the quantitative
analysis of personal eye movement trajectories structure. This new class of
features based on algebraic topology allows extracting patterns from different
modalities of gaze such as time series of coordinates and amplitudes, heatmaps,
and point clouds in a unified way at all scales from micro to macro. We
experimentally demonstrate the competitiveness of the new class of features
with the traditional ones and their significant synergy while being used
together for the person authentication task on the recently published eye
movement trajectories dataset.",None,63
41a9d05a-7dda-4cae-b595-c6dd0e1a32ee,Scalable Planning and Learning Framework Development for Swarm-to-Swarm Engagement Problems,0.864665,"Development of guidance, navigation and control frameworks/algorithms for
swarms attracted significant attention in recent years. That being said,
algorithms for planning swarm allocations/trajectories for engaging with enemy
swarms is largely an understudied problem. Although small-scale scenarios can
be addressed with tools from differential game theory, existing approaches fail
to scale for large-scale multi-agent pursuit evasion (PE) scenarios. In this
work, we propose a reinforcement learning (RL) based framework to decompose to
large-scale swarm engagement problems into a number of independent multi-agent
pursuit-evasion games. We simulate a variety of multi-agent PE scenarios, where
finite time capture is guaranteed under certain conditions. The calculated PE
statistics are provided as a reward signal to the high level allocation layer,
which uses an RL algorithm to allocate controlled swarm units to eliminate
enemy swarm units with maximum efficiency. We verify our approach in
large-scale swarm-to-swarm engagement simulations.",None,-1
20410385-92b5-4856-aa69-4996c421d8e5,SynSciPass: detecting appropriate uses of scientific text generation,0.167,"Approaches to machine generated text detection tend to focus on binary
classification of human versus machine written text. In the scientific domain
where publishers might use these models to examine manuscripts under
submission, misclassification has the potential to cause harm to authors.
Additionally, authors may appropriately use text generation models such as with
the use of assistive technologies like translation tools. In this setting, a
binary classification scheme might be used to flag appropriate uses of
assistive text generation technology as simply machine generated which is a
cause of concern. In our work, we simulate this scenario by presenting a
state-of-the-art detector trained on the DAGPap22 with machine translated
passages from Scielo and find that the model performs at random. Given this
finding, we develop a framework for dataset development that provides a nuanced
approach to detecting machine generated text by having labels for the type of
technology used such as for translation or paraphrase resulting in the
construction of SynSciPass. By training the same model that performed well on
DAGPap22 on SynSciPass, we show that not only is the model more robust to
domain shifts but also is able to uncover the type of technology used for
machine generated text. Despite this, we conclude that current datasets are
neither comprehensive nor realistic enough to understand how these models would
perform in the wild where manuscript submissions can come from many unknown or
novel distributions, how they would perform on scientific full-texts rather
than small passages, and what might happen when there is a mix of appropriate
and inappropriate uses of natural language generation.",https://github.com/domenicrosati/synscipass,-1
c01ebe93-e254-4fe9-ba56-bb624bd5e9ed,Transforming Gait: Video-Based Spatiotemporal Gait Analysis,0.709089,"Human pose estimation from monocular video is a rapidly advancing field that
offers great promise to human movement science and rehabilitation. This
potential is tempered by the smaller body of work ensuring the outputs are
clinically meaningful and properly calibrated. Gait analysis, typically
performed in a dedicated lab, produces precise measurements including
kinematics and step timing. Using over 7000 monocular video from an
instrumented gait analysis lab, we trained a neural network to map 3D joint
trajectories and the height of individuals onto interpretable biomechanical
outputs including gait cycle timing and sagittal plane joint kinematics and
spatiotemporal trajectories. This task specific layer produces accurate
estimates of the timing of foot contact and foot off events. After parsing the
kinematic outputs into individual gait cycles, it also enables accurate
cycle-by-cycle estimates of cadence, step time, double and single support time,
walking speed and step length.",https://github.com/open-mmlab/mmpose,-1
e6f16b2a-6402-4a28-8045-131996f6e302,Class-Incremental Learning for Action Recognition in Videos,0.871464,"We tackle catastrophic forgetting problem in the context of class-incremental
learning for video recognition, which has not been explored actively despite
the popularity of continual learning. Our framework addresses this challenging
task by introducing time-channel importance maps and exploiting the importance
maps for learning the representations of incoming examples via knowledge
distillation. We also incorporate a regularization scheme in our objective
function, which encourages individual features obtained from different time
steps in a video to be uncorrelated and eventually improves accuracy by
alleviating catastrophic forgetting. We evaluate the proposed approach on
brand-new splits of class-incremental action recognition benchmarks constructed
upon the UCF101, HMDB51, and Something-Something V2 datasets, and demonstrate
the effectiveness of our algorithm in comparison to the existing continual
learning methods that are originally designed for image data.",https://github.com/mit-han-lab/temporal-shift-module,-1
5fadd2dd-3c74-4c8f-b0df-41cde61b2b96,A Unified View of Masked Image Modeling,0.728224,"Masked image modeling has demonstrated great potential to eliminate the
label-hungry problem of training large-scale vision Transformers, achieving
impressive performance on various downstream tasks. In this work, we propose a
unified view of masked image modeling after revisiting existing methods. Under
the unified view, we introduce a simple yet effective method, termed as
MaskDistill, which reconstructs normalized semantic features from teacher
models at the masked positions, conditioning on corrupted input images.
Experimental results on image classification and semantic segmentation show
that MaskDistill achieves comparable or superior performance than
state-of-the-art methods. When using the huge vision Transformer and
pretraining 300 epochs, MaskDistill obtains 88.3% fine-tuning top-1 accuracy on
ImageNet-1k (224 size) and 58.8% semantic segmentation mIoU metric on ADE20k
(512 size). The code and pretrained models will be available at
https://aka.ms/unimim.",https://aka.ms/unimim,-1
552b2305-3b7e-4c13-b54b-2819204229c9,Investigating Lexical Replacements for Arabic-English Code-Switched Data Augmentation,0.349556,"Data sparsity is a main problem hindering the development of code-switching
(CS) NLP systems. In this paper, we investigate data augmentation techniques
for synthesizing dialectal Arabic-English CS text. We perform lexical
replacements using word-aligned parallel corpora where CS points are either
randomly chosen or learnt using a sequence-to-sequence model. We compare these
approaches against dictionary-based replacements. We assess the quality of the
generated sentences through human evaluation and evaluate the effectiveness of
data augmentation on machine translation (MT), automatic speech recognition
(ASR), and speech translation (ST) tasks. Results show that using a predictive
model results in more natural CS sentences compared to the random approach, as
reported in human judgements. In the downstream tasks, despite the random
approach generating more data, both approaches perform equally (outperforming
dictionary-based replacements). Overall, data augmentation achieves 34%
improvement in perplexity, 5.2% relative improvement on WER for ASR task,
+4.0-5.1 BLEU points on MT task, and +2.1-2.2 BLEU points on ST over a baseline
trained on available data without augmentation.",http://arzen.camel-lab.com/,-1
5e8db8d0-dc21-4a2d-8796-7e6bf15802ed,Prediction-based One-shot Dynamic Parking Pricing,0.47095,"Many U.S. metropolitan cities are notorious for their severe shortage of
parking spots. To this end, we present a proactive prediction-driven
optimization framework to dynamically adjust parking prices. We use
state-of-the-art deep learning technologies such as neural ordinary
differential equations (NODEs) to design our future parking occupancy rate
prediction model given historical occupancy rates and price information. Owing
to the continuous and bijective characteristics of NODEs, in addition, we
design a one-shot price optimization method given a pre-trained prediction
model, which requires only one iteration to find the optimal solution. In other
words, we optimize the price input to the pre-trained prediction model to
achieve targeted occupancy rates in the parking blocks. We conduct experiments
with the data collected in San Francisco and Seattle for years. Our prediction
model shows the best accuracy in comparison with various temporal or
spatio-temporal forecasting models. Our one-shot optimization method greatly
outperforms other black-box and white-box search methods in terms of the search
time and always returns the optimal price solution.",None,-1
454a3b4a-eeb3-4577-8762-4e2200001e9e,Overview of the WANLP 2022 Shared Task on Propaganda Detection in Arabic,0.531621,"Propaganda is the expression of an opinion or an action by an individual or a
group deliberately designed to influence the opinions or the actions of other
individuals or groups with reference to predetermined ends, which is achieved
by means of well-defined rhetorical and psychological devices. Propaganda
techniques are commonly used in social media to manipulate or to mislead users.
Thus, there has been a lot of recent research on automatic detection of
propaganda techniques in text as well as in memes. However, so far the focus
has been primarily on English. With the aim to bridge this language gap, we ran
a shared task on detecting propaganda techniques in Arabic tweets as part of
the WANLP 2022 workshop, which included two subtasks. Subtask~1 asks to
identify the set of propaganda techniques used in a tweet, which is a
multilabel classification problem, while Subtask~2 asks to detect the
propaganda techniques used in a tweet together with the exact span(s) of text
in which each propaganda technique appears. The task attracted 63 team
registrations, and eventually 14 and 3 teams made submissions for subtask 1 and
2, respectively. Finally, 11 teams submitted system description papers.",None,-1
a438c1da-c9f1-484a-95a6-376aeb193cad,What Do We Maximize in Self-Supervised Learning?,0.313859,"In this paper, we examine self-supervised learning methods, particularly
VICReg, to provide an information-theoretical understanding of their
construction. As a first step, we demonstrate how information-theoretic
quantities can be obtained for a deterministic network, offering a possible
alternative to prior work that relies on stochastic models. This enables us to
demonstrate how VICReg can be (re)discovered from first principles and its
assumptions about data distribution. Furthermore, we empirically demonstrate
the validity of our assumptions, confirming our novel understanding of VICReg.
Finally, we believe that the derivation and insights we obtain can be
generalized to many other SSL methods, opening new avenues for theoretical and
practical understanding of SSL and transfer learning.",None,-1
45c65db2-9e56-4876-8b80-bd290cca8a68,Grammar-Based Grounded Lexicon Learning,0.944362,"We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist
approach toward learning a compositional and grounded meaning representation of
language from grounded data, such as paired images and texts. At the core of
G2L2 is a collection of lexicon entries, which map each word to a tuple of a
syntactic type and a neuro-symbolic semantic program. For example, the word
shiny has a syntactic type of adjective; its neuro-symbolic semantic program
has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is
associated with a neural network embedding, which will be used to classify
shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries
associated with each token. It then derives the meaning of the sentence as an
executable neuro-symbolic program by composing lexical meanings based on
syntax. The recovered meaning programs can be executed on grounded inputs. To
facilitate learning in an exponentially-growing compositional space, we
introduce a joint parsing and expected execution algorithm, which does local
marginalization over derivations to reduce the training time. We evaluate G2L2
on two domains: visual reasoning and language-driven navigation. Results show
that G2L2 can generalize from small amounts of data to novel compositions of
words.",None,-1
7ffe9b46-76d9-4fc3-894d-0ee6bf02eeb2,HLDC: Hindi Legal Documents Corpus,0.758396,"Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: https://github.com/Exploration-Lab/HLDC",https://github.com/Exploration-Lab/HLDC,-1
0cee9613-8575-4fc9-8441-29b50a4abc92,Generating Coherent Narratives by Learning Dynamic and Discrete Entity States with a Contrastive Framework,0.0537756,"Despite advances in generating fluent texts, existing pretraining models tend
to attach incoherent event sequences to involved entities when generating
narratives such as stories and news. We conjecture that such issues result from
representing entities as static embeddings of superficial words, while
neglecting to model their ever-changing states, i.e., the information they
carry, as the text unfolds. Therefore, we extend the Transformer model to
dynamically conduct entity state updates and sentence realization for narrative
generation. We propose a contrastive framework to learn the state
representations in a discrete space, and insert additional attention layers
into the decoder to better exploit these states. Experiments on two narrative
datasets show that our model can generate more coherent and diverse narratives
than strong baselines with the guidance of meaningful entity states.",https://github.com/thu-coai/ERIC,-1
156644e2-07dd-44e5-918d-001561ceec97,"Event Causality Identification with Causal News Corpus -- Shared Task 3, CASE 2022",0.797048,"The Event Causality Identification Shared Task of CASE 2022 involved two
subtasks working on the Causal News Corpus. Subtask 1 required participants to
predict if a sentence contains a causal relation or not. This is a supervised
binary classification task. Subtask 2 required participants to identify the
Cause, Effect and Signal spans per causal sentence. This could be seen as a
supervised sequence labeling task. For both subtasks, participants uploaded
their predictions for a held-out test set, and ranking was done based on binary
F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes
the work of the 17 teams that submitted their results to our competition and 12
system description papers that were received. The best F1 scores achieved for
Subtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing
approaches involved pre-trained language models fine-tuned to the targeted
task. We further discuss these approaches and analyze errors across
participants' systems in this paper.",https://github.com/tanfiona/UniCausal,3459
e5dba8d3-7253-4f81-a1ce-ae63fc4b7c97,MISm: A Medical Image Segmentation Metric for Evaluation of weak labeled Data,0.0868917,"Performance measures are an important tool for assessing and comparing
different medical image segmentation algorithms. Unfortunately, the current
measures have their weaknesses when it comes to assessing certain edge cases.
These limitations arouse when images with a very small region of interest or
without a region of interest at all are assessed. As a solution for these
limitations, we propose a new medical image segmentation metric: MISm. To
evaluate MISm, the popular metrics in the medical image segmentation and MISm
were compared using images of magnet resonance tomography from several
scenarios. In order to allow application in the community and reproducibility
of experimental results, we included MISm in the publicly available evaluation
framework MISeval:
https://github.com/frankkramer-lab/miseval/tree/master/miseval",https://github.com/frankkramer-lab/miseval,-1
dd317a2a-b85a-4d90-8f0e-71a9e1121bdf,Explainable Action Advising for Multi-Agent Reinforcement Learning,0.316183,"Action advising is a knowledge transfer technique for reinforcement learning
based on the teacher-student paradigm. An expert teacher provides advice to a
student during training in order to improve the student's sample efficiency and
policy performance. Such advice is commonly given in the form of state-action
pairs. However, it makes it difficult for the student to reason with and apply
to novel states. We introduce Explainable Action Advising, in which the teacher
provides action advice as well as associated explanations indicating why the
action was chosen. This allows the student to self-reflect on what it has
learned, enabling advice generalization and leading to improved sample
efficiency and learning performance - even in environments where the teacher is
sub-optimal. We empirically show that our framework is effective in both
single-agent and multi-agent scenarios, yielding improved policy returns and
convergence rates when compared to state-of-the-art methods",https://github.com/sophieyueguo/explainable_action_advising,-1
a4ae6fd0-c91a-4954-b463-14b70abc28ea,End-to-End Visual Editing with a Generatively Pre-Trained Artist,0.113631,"We consider the targeted image editing problem: blending a region in a source
image with a driver image that specifies the desired change. Differently from
prior works, we solve this problem by learning a conditional probability
distribution of the edits, end-to-end. Training such a model requires
addressing a fundamental technical challenge: the lack of example edits for
training. To this end, we propose a self-supervised approach that simulates
edits by augmenting off-the-shelf images in a target domain. The benefits are
remarkable: implemented as a state-of-the-art auto-regressive transformer, our
approach is simple, sidesteps difficulties with previous methods based on
GAN-like priors, obtains significantly better edits, and is efficient.
Furthermore, we show that different blending effects can be learned by an
intuitive control of the augmentation process, with no other changes required
to the model architecture. We demonstrate the superiority of this approach
across several datasets in extensive quantitative and qualitative experiments,
including human studies, significantly outperforming prior work.",https://github.com/nerdyrodent/VQGAN-CLIP,-1
ca8af5e7-c866-41c6-85d2-a58a0d1385ae,"""Think Before You Speak"": Improving Multi-Action Dialog Policy by Planning Single-Action Dialogs",0.635567,"Multi-action dialog policy (MADP), which generates multiple atomic dialog
actions per turn, has been widely applied in task-oriented dialog systems to
provide expressive and efficient system responses. Existing MADP models usually
imitate action combinations from the labeled multi-action dialog samples. Due
to data limitations, they generalize poorly toward unseen dialog flows. While
interactive learning and reinforcement learning algorithms can be applied to
incorporate external data sources of real users and user simulators, they take
significant manual effort to build and suffer from instability. To address
these issues, we propose Planning Enhanced Dialog Policy (PEDP), a novel
multi-task learning framework that learns single-action dialog dynamics to
enhance multi-action prediction. Our PEDP method employs model-based planning
for conceiving what to express before deciding the current response through
simulating single-action dialogs. Experimental results on the MultiWOZ dataset
demonstrate that our fully supervised learning-based method achieves a solid
task success rate of 90.6%, improving 3% compared to the state-of-the-art
methods.",https://github.com/ShuoZhangXJTU/PEDP,-1
21817089-4764-453b-b838-8bb7af6e7cdf,"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",0.546409,"We introduce Mintaka, a complex, natural, and multilingual dataset designed
for experimenting with end-to-end question-answering models. Mintaka is
composed of 20,000 question-answer pairs collected in English, annotated with
Wikidata entities, and translated into Arabic, French, German, Hindi, Italian,
Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka
includes 8 types of complex questions, including superlative, intersection, and
multi-hop questions, which were naturally elicited from crowd workers. We run
baselines over Mintaka, the best of which achieves 38% hits@1 in English and
31% hits@1 multilingually, showing that existing models have room for
improvement. We release Mintaka at https://github.com/amazon-research/mintaka.",https://github.com/amazon-research/mintaka,-1
380b6a03-6872-4414-b011-64c7d63ff6a8,Zoom Text Detector,0.0605686,"To pursue comprehensive performance, recent text detectors improve detection
speed at the expense of accuracy. They adopt shrink-mask based text
representation strategies, which leads to a high dependency of detection
accuracy on shrink-masks. Unfortunately, three disadvantages cause unreliable
shrink-masks. Specifically, these methods try to strengthen the discrimination
of shrink-masks from the background by semantic information. However, the
feature defocusing phenomenon that coarse layers are optimized by fine-grained
objectives limits the extraction of semantic features. Meanwhile, since both
shrink-masks and the margins belong to texts, the detail loss phenomenon that
the margins are ignored hinders the distinguishment of shrink-masks from the
margins, which causes ambiguous shrink-mask edges. Moreover, false-positive
samples enjoy similar visual features with shrink-masks. They aggravate the
decline of shrink-masks recognition. To avoid the above problems, we propose a
Zoom Text Detector (ZTD) inspired by the zoom process of the camera.
Specifically, Zoom Out Module (ZOM) is introduced to provide coarse-grained
optimization objectives for coarse layers to avoid feature defocusing.
Meanwhile, Zoom In Module (ZIM) is presented to enhance the margins recognition
to prevent detail loss. Furthermore, Sequential-Visual Discriminator (SVD) is
designed to suppress false-positive samples by sequential and visual features.
Experiments verify the superior comprehensive performance of ZTD.",None,55805
1ed62e7c-7c03-480e-bd68-8f7e1f3367b2,OntoSeer -- A Recommendation System to Improve the Quality of Ontologies,0.285999,"Building an ontology is not only a time-consuming process, but it is also
confusing, especially for beginners and the inexperienced. Although ontology
developers can take the help of domain experts in building an ontology, they
are not readily available in several cases for a variety of reasons. Ontology
developers have to grapple with several questions related to the choice of
classes, properties, and the axioms that should be included. Apart from this,
there are aspects such as modularity and reusability that should be taken care
of. From among the thousands of publicly available ontologies and vocabularies
in repositories such as Linked Open Vocabularies (LOV) and BioPortal, it is
hard to know the terms (classes and properties) that can be reused in the
development of an ontology. A similar problem exists in implementing the right
set of ontology design patterns (ODPs) from among the several available.
Generally, ontology developers make use of their experience in handling these
issues, and the inexperienced ones have a hard time. In order to bridge this
gap, we propose a tool named OntoSeer, that monitors the ontology development
process and provides suggestions in real-time to improve the quality of the
ontology under development. It can provide suggestions on the naming
conventions to follow, vocabulary to reuse, ODPs to implement, and axioms to be
added to the ontology. OntoSeer has been implemented as a Prot\'eg\'e plug-in.",https://github.com/kracr/ontoseer,-1
66778364-bf52-4450-9bf9-b4ca1b725da3,Data-driven Approach to Differentiating between Depression and Dementia from Noisy Speech and Language Data,0.412422,"A significant number of studies apply acoustic and linguistic characteristics
of human speech as prominent markers of dementia and depression. However,
studies on discriminating depression from dementia are rare. Co-morbid
depression is frequent in dementia and these clinical conditions share many
overlapping symptoms, but the ability to distinguish between depression and
dementia is essential as depression is often curable. In this work, we
investigate the ability of clustering approaches in distinguishing between
depression and dementia from human speech. We introduce a novel aggregated
dataset, which combines narrative speech data from multiple conditions, i.e.,
Alzheimer's disease, mild cognitive impairment, healthy control, and
depression. We compare linear and non-linear clustering approaches and show
that non-linear clustering techniques distinguish better between distinct
disease clusters. Our interpretability analysis shows that the main
differentiating symptoms between dementia and depression are acoustic
abnormality, repetitiveness (or circularity) of speech, word finding
difficulty, coherence impairment, and differences in lexical complexity and
richness.",https://github.com/vu-minh/mlteam-lime-for-tsne,-1
18bb3ab3-d9e6-4f55-bf72-6e930ad427da,Interpretable Molecular Graph Generation via Monotonic Constraints,0.626729,"Designing molecules with specific properties is a long-lasting research
problem and is central to advancing crucial domains such as drug discovery and
material science. Recent advances in deep graph generative models treat
molecule design as graph generation problems which provide new opportunities
toward the breakthrough of this long-lasting problem. Existing models, however,
have many shortcomings, including poor interpretability and controllability
toward desired molecular properties. This paper focuses on new methodologies
for molecule generation with interpretable and controllable deep generative
models, by proposing new monotonically-regularized graph variational
autoencoders. The proposed models learn to represent the molecules with latent
variables and then learn the correspondence between them and molecule
properties parameterized by polynomial functions. To further improve the
intepretability and controllability of molecule generation towards desired
properties, we derive new objectives which further enforce monotonicity of the
relation between some latent variables and target molecule properties such as
toxicity and clogP. Extensive experimental evaluation demonstrates the
superiority of the proposed framework on accuracy, novelty, disentanglement,
and control towards desired molecular properties. The code is open-source at
https://anonymous.4open.science/r/MDVAE-FD2C.",None,-1
c04f1f8e-743d-47d5-b31c-7e43f2cf69c5,Multi-Granularity Prediction for Scene Text Recognition,0.687028,"Scene text recognition (STR) has been an active research topic in computer
vision for years. To tackle this challenging problem, numerous innovative
methods have been successively proposed and incorporating linguistic knowledge
into STR models has recently become a prominent trend. In this work, we first
draw inspiration from the recent progress in Vision Transformer (ViT) to
construct a conceptually simple yet powerful vision STR model, which is built
upon ViT and outperforms previous state-of-the-art models for scene text
recognition, including both pure vision models and language-augmented methods.
To integrate linguistic knowledge, we further propose a Multi-Granularity
Prediction strategy to inject information from the language modality into the
model in an implicit way, i.e. , subword representations (BPE and WordPiece)
widely-used in NLP are introduced into the output space, in addition to the
conventional character level representation, while no independent language
model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push
the performance envelop of STR to an even higher level. Specifically, it
achieves an average recognition accuracy of 93.35% on standard benchmarks. Code
is available at
https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR.",None,-1
7b973cde-7b92-42e5-9702-279012140b33,Momentum Decoding: Open-ended Text Generation As Graph Exploration,0.0582321,"Open-ended text generation with autoregressive language models (LMs) is one
of the core tasks in natural language processing. However, maximization-based
decoding methods (e.g., greedy/beam search) often lead to the degeneration
problem, i.e., the generated text is unnatural and contains undesirable
repetitions. Existing solutions to this problem either introduce randomness
prone to incoherence or require a look-ahead mechanism that demands extra
computational overhead. In this study, we formulate open-ended text generation
from a new perspective, i.e., we view it as an exploration process within a
directed graph. Thereby, we understand the phenomenon of degeneration as
circular loops within the directed graph. Based on our formulation, we propose
a novel decoding method -- \textit{momentum decoding} -- which encourages the
LM to \textit{greedily} explore new nodes outside the current graph. Meanwhile,
it also allows the LM to return to the existing nodes with a momentum
downgraded by a pre-defined resistance function. We extensively test our
approach on three benchmarks from different domains through automatic and human
evaluations. The results show that momentum decoding performs comparably with
the current state of the art while enjoying notably improved inference speed
and computation FLOPs. Furthermore, we conduct a detailed analysis to reveal
the merits and inner workings of our approach. Our codes and other related
resources are publicly available at
https://github.com/gmftbyGMFTBY/MomentumDecoding.",https://github.com/gmftbyGMFTBY/,-1
7c0a6b6f-346b-4336-900a-f417bef3649c,Analogical Math Word Problems Solving with Enhanced Problem-Solution Association,0.250567,"Math word problem (MWP) solving is an important task in question answering
which requires human-like reasoning ability. Analogical reasoning has long been
used in mathematical education, as it enables students to apply common
relational structures of mathematical situations to solve new problems. In this
paper, we propose to build a novel MWP solver by leveraging analogical MWPs,
which advance the solver's generalization ability across different kinds of
MWPs. The key idea, named analogy identification, is to associate the
analogical MWP pairs in a latent space, i.e., encoding an MWP close to another
analogical MWP, while moving away from the non-analogical ones. Moreover, a
solution discriminator is integrated into the MWP solver to enhance the
association between the representations of MWPs and their true solutions. The
evaluation results verify that our proposed analogical learning strategy
promotes the performance of MWP-BERT on Math23k over the state-of-the-art model
Generate2Rank, with 5 times fewer parameters in the encoder. We also find that
our model has a stronger generalization ability in solving difficult MWPs due
to the analogical learning from easy MWPs.",https://github.com/ShichaoSun/math_seq2tree,-1
e4a36043-d014-4888-83ce-a586ea124476,Uncertainty Inspired Underwater Image Enhancement,0.994254,"A main challenge faced in the deep learning-based Underwater Image
Enhancement (UIE) is that the ground truth high-quality image is unavailable.
Most of the existing methods first generate approximate reference maps and then
train an enhancement network with certainty. This kind of method fails to
handle the ambiguity of the reference map. In this paper, we resolve UIE into
distribution estimation and consensus process. We present a novel probabilistic
network to learn the enhancement distribution of degraded underwater images.
Specifically, we combine conditional variational autoencoder with adaptive
instance normalization to construct the enhancement distribution. After that,
we adopt a consensus process to predict a deterministic result based on a set
of samples from the distribution. By learning the enhancement distribution, our
method can cope with the bias introduced in the reference map labeling to some
extent. Additionally, the consensus process is useful to capture a robust and
stable result. We examined the proposed method on two widely used real-world
underwater image enhancement datasets. Experimental results demonstrate that
our approach enables sampling possible enhancement predictions. Meanwhile, the
consensus estimate yields competitive performance compared with
state-of-the-art UIE methods. Code available at
https://github.com/zhenqifu/PUIE-Net.",https://github.com/zhenqifu/PUIE-Net,-1
921d8a03-0468-409c-a87c-107023621a1a,Tensor4D : Efficient Neural 4D Decomposition for High-fidelity Dynamic Reconstruction and Rendering,1.0,"We present Tensor4D, an efficient yet effective approach to dynamic scene
modeling. The key of our solution is an efficient 4D tensor decomposition
method so that the dynamic scene can be directly represented as a 4D
spatio-temporal tensor. To tackle the accompanying memory issue, we decompose
the 4D tensor hierarchically by projecting it first into three time-aware
volumes and then nine compact feature planes. In this way, spatial information
over time can be simultaneously captured in a compact and memory-efficient
manner. When applying Tensor4D for dynamic scene reconstruction and rendering,
we further factorize the 4D fields to different scales in the sense that
structural motions and dynamic detailed changes can be learned from coarse to
fine. The effectiveness of our method is validated on both synthetic and
real-world scenes. Extensive experiments show that our method is able to
achieve high-quality dynamic reconstruction and rendering from sparse-view
camera rigs or even a monocular camera. The code and dataset will be released
at https://liuyebin.com/tensor4d/tensor4d.html.",https://github.com/DSaurus/Tensor4D,-1
835c7818-9820-4795-a32e-0f58e3c51a6e,Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues,0.447327,"Movies reflect society and also hold power to transform opinions. Social
biases and stereotypes present in movies can cause extensive damage due to
their reach. These biases are not always found to be the need of storyline but
can creep in as the author's bias. Movie production houses would prefer to
ascertain that the bias present in a script is the story's demand. Today, when
deep learning models can give human-level accuracy in multiple tasks, having an
AI solution to identify the biases present in the script at the writing stage
can help them avoid the inconvenience of stalled release, lawsuits, etc. Since
AI solutions are data intensive and there exists no domain specific data to
address the problem of biases in scripts, we introduce a new dataset of movie
scripts that are annotated for identity bias. The dataset contains dialogue
turns annotated for (i) bias labels for seven categories, viz., gender,
race/ethnicity, religion, age, occupation, LGBTQ, and other, which contains
biases like body shaming, personality bias, etc. (ii) labels for sensitivity,
stereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated
with context awareness, (iv) target groups and reason for bias labels and (v)
expert-driven group-validation process for high quality annotations. We also
report various baseline performances for bias identification and category
detection on our dataset.",https://github.com/sahoonihar/HIBD_LREC_2022,-1
db7d622f-750c-4bdb-a50b-3807b18ddaef,Semi-self-supervised Automated ICD Coding,0.045955,"Clinical Text Notes (CTNs) contain physicians' reasoning process, written in
an unstructured free text format, as they examine and interview patients. In
recent years, several studies have been published that provide evidence for the
utility of machine learning for predicting doctors' diagnoses from CTNs, a task
known as ICD coding. Data annotation is time consuming, particularly when a
degree of specialization is needed, as is the case for medical data. This paper
presents a method of augmenting a sparsely annotated dataset of Icelandic CTNs
with a machine-learned imputation in a semi-self-supervised manner. We train a
neural network on a small set of annotated CTNs and use it to extract clinical
features from a set of un-annotated CTNs. These clinical features consist of
answers to about a thousand potential questions that a physician might find the
answers to during a consultation of a patient. The features are then used to
train a classifier for the diagnosis of certain types of diseases. We report
the results of an evaluation of this data augmentation method over three tiers
of data availability to the physician. Our data augmentation method shows a
significant positive effect which is diminished when clinical features from the
examination of the patient and diagnostics are made available. We recommend our
method for augmenting scarce datasets for systems that take decisions based on
clinical features that do not include examinations or tests.",https://github.com/stofnun-arna-magnussonar/ordgreypingar_embeddings/tree/main/GloVe,-1
c381eb54-4304-491c-9053-a8d91bba6c39,"ESC-Rules: Explainable, Semantically Constrained Rule Sets",0.168671,"We describe a novel approach to explainable prediction of a continuous
variable based on learning fuzzy weighted rules. Our model trains a set of
weighted rules to maximise prediction accuracy and minimise an ontology-based
'semantic loss' function including user-specified constraints on the rules that
should be learned in order to maximise the explainability of the resulting rule
set from a user perspective. This system fuses quantitative sub-symbolic
learning with symbolic learning and constraints based on domain knowledge. We
illustrate our system on a case study in predicting the outcomes of behavioural
interventions for smoking cessation, and show that it outperforms other
interpretable approaches, achieving performance close to that of a deep
learning model, while offering transparent explainability that is an essential
requirement for decision-makers in the health domain.",https://github.com/HumanBehaviourChangeProject/semantic-prediction,-1
e4ef1efc-4a5b-4f14-baee-b1bec5f0efb5,Filler Word Detection and Classification: A Dataset and Benchmark,0.555862,"Filler words such as `uh' or `um' are sounds or words people use to signal
they are pausing to think. Finding and removing filler words from recordings is
a common and tedious task in media editing. Automatically detecting and
classifying filler words could greatly aid in this task, but few studies have
been published on this problem to date. A key reason is the absence of a
dataset with annotated filler words for model training and evaluation. In this
work, we present a novel speech dataset, PodcastFillers, with 35K annotated
filler words and 50K annotations of other sounds that commonly occur in
podcasts such as breaths, laughter, and word repetitions. We propose a pipeline
that leverages VAD and ASR to detect filler candidates and a classifier to
distinguish between filler word types. We evaluate our proposed pipeline on
PodcastFillers, compare to several baselines, and present a detailed ablation
study. In particular, we evaluate the importance of using ASR and how it
compares to a transcription-free approach resembling keyword spotting. We show
that our pipeline obtains state-of-the-art results, and that leveraging ASR
strongly outperforms a keyword spotting approach. We make PodcastFillers
publicly available, in the hope that our work serves as a benchmark for future
research.",https://github.com/podcastfillers,-1
911af1ba-acce-4541-b4d7-d6d24215dd56,TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction,0.456752,"Knowledge graph embedding methods are important for the knowledge graph
completion (or link prediction) task. One existing efficient method, PairRE,
leverages two separate vectors to model complex relations (i.e., 1-to-N,
N-to-1, and N-to-N) in knowledge graphs. However, such a method strictly
restricts entities on the hyper-ellipsoid surfaces which limits the
optimization of entity distribution, leading to suboptimal performance of
knowledge graph completion. To address this issue, we propose a novel score
function TranSHER, which leverages relation-specific translations between head
and tail entities to relax the constraint of hyper-ellipsoid restrictions. By
introducing an intuitive and simple relation-specific translation, TranSHER can
provide more direct guidance on optimization and capture more semantic
characteristics of entities with complex relations. Experimental results show
that TranSHER achieves significant performance improvements on link prediction
and generalizes well to datasets in different domains and scales. Our codes are
public available at https://github.com/yizhilll/TranSHER.",https://github.com/yizhilll/TranSHER,-1
4ecc6d8e-b6fc-4c18-869a-ab25b0f21b6e,Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation,0.144059,"Neural Machine Translation (NMT) has reached a level of maturity to be
recognized as the premier method for the translation between different
languages and aroused interest in different research areas, including software
engineering. A key step to validate the robustness of the NMT models consists
in evaluating the performance of the models on adversarial inputs, i.e., inputs
obtained from the original ones by adding small amounts of perturbation.
However, when dealing with the specific task of the code generation (i.e., the
generation of code starting from a description in natural language), it has not
yet been defined an approach to validate the robustness of the NMT models. In
this work, we address the problem by identifying a set of perturbations and
metrics tailored for the robustness assessment of such models. We present a
preliminary experimental evaluation, showing what type of perturbations affect
the model the most and deriving useful insights for future directions.",None,-1
e346ed93-81b0-4ac8-b847-889c89e10db4,Cardinality-Regularized Hawkes-Granger Model,0.89356,"We propose a new sparse Granger-causal learning framework for temporal event
data. We focus on a specific class of point processes called the Hawkes
process. We begin by pointing out that most of the existing sparse causal
learning algorithms for the Hawkes process suffer from a singularity in maximum
likelihood estimation. As a result, their sparse solutions can appear only as
numerical artifacts. In this paper, we propose a mathematically well-defined
sparse causal learning framework based on a cardinality-regularized Hawkes
process, which remedies the pathological issues of existing approaches. We
leverage the proposed algorithm for the task of instance-wise causal event
analysis, where sparsity plays a critical role. We validate the proposed
framework with two real use-cases, one from the power grid and the other from
the cloud data center management domain.",https://github.com/iancovert/Neural-GC,-1
a0d8f5d1-11a3-4542-9df4-c09097b0b5e7,Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration,0.235025,"Given two point sets, the problem of registration is to recover a
transformation that matches one set to the other. This task is challenging due
to the presence of the large number of outliers, the unknown non-rigid
deformations and the large sizes of point sets. To obtain strong robustness
against outliers, we formulate the registration problem as a partial
distribution matching (PDM) problem, where the goal is to partially match the
distributions represented by point sets in a metric space. To handle large
point sets, we propose a scalable PDM algorithm by utilizing the efficient
partial Wasserstein-1 (PW) discrepancy. Specifically, we derive the
Kantorovich-Rubinstein duality for the PW discrepancy, and show its gradient
can be explicitly computed. Based on these results, we propose a partial
Wasserstein adversarial network (PWAN), which is able to approximate the PW
discrepancy by a neural network, and minimize it by gradient descent. In
addition, it also incorporates an efficient coherence regularizer for non-rigid
transformations to avoid unrealistic deformations. We evaluate PWAN on
practical point set registration tasks, and show that the proposed PWAN is
robust, scalable and performs more favorably than the state-of-the-art methods.",None,-1
7e2c1a09-c50a-4b89-bf04-afc111a661f3,Effects of Spectral Normalization in Multi-agent Reinforcement Learning,0.321131,"A reliable critic is central to on-policy actor-critic learning. But it
becomes challenging to learn a reliable critic in a multi-agent sparse reward
scenario due to two factors: 1) The joint action space grows exponentially with
the number of agents 2) This, combined with the reward sparseness and
environment noise, leads to large sample requirements for accurate learning. We
show that regularising the critic with spectral normalization (SN) enables it
to learn more robustly, even in multi-agent on-policy sparse reward scenarios.
Our experiments show that the regularised critic is quickly able to learn from
the sparse rewarding experience in the complex SMAC and RWARE domains. These
findings highlight the importance of regularisation in the critic for stable
learning.",https://github.com/kinalmehta/epymarl,-1
8c22e898-500d-460d-a13b-aedb890b533d,What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation,0.280826,"Accurate automatic evaluation metrics for open-domain dialogs are in high
demand. Existing model-based metrics for system response evaluation are trained
on human annotated data, which is cumbersome to collect. In this work, we
propose to use information that can be automatically extracted from the next
user utterance, such as its sentiment or whether the user explicitly ends the
conversation, as a proxy to measure the quality of the previous system
response. This allows us to train on a massive set of dialogs with weak
supervision, without requiring manual system turn quality annotations.
Experiments show that our model is comparable to models trained on human
annotated data. Furthermore, our model generalizes across both spoken and
written open-domain dialog corpora collected from real and paid users.",https://github.com/exe1023/DialEvalMetrics,-1
d4d2dbce-8040-46e6-9e37-d4e2f9b1dfc1,Communication Beyond Transmitting Bits: Semantics-Guided Source and Channel Coding,0.810792,"Classical communication paradigms focus on accurately transmitting bits over
a noisy channel, and Shannon theory provides a fundamental theoretical limit on
the rate of reliable communications. In this approach, bits are treated
equally, and the communication system is oblivious to what meaning these bits
convey or how they would be used. Future communications towards intelligence
and conciseness will predictably play a dominant role, and the proliferation of
connected intelligent agents requires a radical rethinking of coded
transmission paradigm to support the new communication morphology on the
horizon. The recent concept of ""semantic communications"" offers a promising
research direction. Injecting semantic guidance into the coded transmission
design to achieve semantics-aware communications shows great potential for
further breakthrough in effectiveness and reliability. This article sheds light
on semantics-guided source and channel coding as a transmission paradigm of
semantic communications, which exploits both data semantics diversity and
wireless channel diversity together to boost the whole system performance. We
present the general system architecture and key techniques, and indicate some
open issues on this topic.",None,-1
f821bf1e-4b5a-490d-af8f-d6747770c708,Deep Sequence Models for Text Classification Tasks,0.00937728,"The exponential growth of data generated on the Internet in the current
information age is a driving force for the digital economy. Extraction of
information is the major value in an accumulated big data. Big data dependency
on statistical analysis and hand-engineered rules machine learning algorithms
are overwhelmed with vast complexities inherent in human languages. Natural
Language Processing (NLP) is equipping machines to understand these human
diverse and complicated languages. Text Classification is an NLP task which
automatically identifies patterns based on predefined or undefined labeled
sets. Common text classification application includes information retrieval,
modeling news topic, theme extraction, sentiment analysis, and spam detection.
In texts, some sequences of words depend on the previous or next word sequences
to make full meaning; this is a challenging dependency task that requires the
machine to be able to store some previous important information to impact
future meaning. Sequence models such as RNN, GRU, and LSTM is a breakthrough
for tasks with long-range dependencies. As such, we applied these models to
Binary and Multi-class classification. Results generated were excellent with
most of the models performing within the range of 80% and 94%. However, this
result is not exhaustive as we believe there is room for improvement if
machines are to compete with humans.",None,-1
a8498ce7-9d51-40c7-8fde-534e167a30b5,EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain,0.929675,"Existing summarization datasets come with two main drawbacks: (1) They tend
to focus on overly exposed domains, such as news articles or wiki-like texts,
and (2) are primarily monolingual, with few multilingual datasets. In this
work, we propose a novel dataset, called EUR-Lex-Sum, based on manually curated
document summaries of legal acts from the European Union law platform
(EUR-Lex). Documents and their respective summaries exist as cross-lingual
paragraph-aligned data in several of the 24 official European languages,
enabling access to various cross-lingual and lower-resourced summarization
setups. We obtain up to 1,500 document/summary pairs per language, including a
subset of 375 cross-lingually aligned legal acts with texts available in all 24
languages. In this work, the data acquisition process is detailed and key
characteristics of the resource are compared to existing summarization
resources. In particular, we illustrate challenging sub-problems and open
questions on the dataset that could help the facilitation of future research in
the direction of domain-specific cross-lingual summarization. Limited by the
extreme length and language diversity of samples, we further conduct
experiments with suitable extractive monolingual and cross-lingual baselines
for future work. Code for the extraction as well as access to our data and
baselines is available online at: https://github.com/achouhan93/eur-lex-sum.",https://github.com/achouhan93/eur-lex-sum,-1
8d578da3-7c67-4a69-9fb4-bc3aaa6b4013,Distillation-Resistant Watermarking for Model Protection in NLP,0.64474,"How can we protect the intellectual property of trained NLP models? Modern
NLP models are prone to stealing by querying and distilling from their publicly
exposed APIs. However, existing protection methods such as watermarking only
work for images but are not applicable to text. We propose
Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP
models from being stolen via distillation. DRW protects a model by injecting
watermarks into the victim's prediction probability corresponding to a secret
key and is able to detect such a key by probing a suspect model. We prove that
a protected model still retains the original accuracy within a certain bound.
We evaluate DRW on a diverse set of NLP tasks including text classification,
part-of-speech tagging, and named entity recognition. Experiments show that DRW
protects the original model and detects stealing suspects at 100% mean average
precision for all four tasks while the prior method fails on two.",https://github.com/XuandongZhao/DRW,-1
981d037e-b225-42b9-afa0-2e25f6bac308,Nonparametric Masked Language Modeling,0.768225,"Existing language models (LMs) predict tokens with a softmax over a finite
vocabulary, which can make it difficult to predict rare tokens or phrases. We
introduce NPM, the first nonparametric masked language model that replaces this
softmax with a nonparametric distribution over every phrase in a reference
corpus. NPM fills in the [MASK] solely from retrieving a token from a text
corpus. We show that NPM can be efficiently trained with a contrastive
objective and an in-batch approximation to full corpus retrieval. Zero-shot
evaluation on 16 tasks including classification, fact probing and question
answering demonstrates that NPM outperforms significantly larger parametric
models, either with or without a retrieve-and-generate approach. It is
particularly better at dealing with rare patterns (word senses or facts) and
predicting rare or nearly unseen words (e.g., non-Latin script). We release the
model and code at github.com/facebookresearch/NPM.",https://github.com/facebookresearch/NPM,-1
428e40b0-fe67-4c7e-81f1-ab27b3500df2,Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning,0.995101,"Conversational recommender systems (CRS) aim to proactively elicit user
preference and recommend high-quality items through natural language
conversations. Typically, a CRS consists of a recommendation module to predict
preferred items for users and a conversation module to generate appropriate
responses. To develop an effective CRS, it is essential to seamlessly integrate
the two modules. Existing works either design semantic alignment strategies, or
share knowledge resources and representations between the two modules. However,
these approaches still rely on different architectures or techniques to develop
the two modules, making it difficult for effective module integration.
  To address this problem, we propose a unified CRS model named UniCRS based on
knowledge-enhanced prompt learning. Our approach unifies the recommendation and
conversation subtasks into the prompt learning paradigm, and utilizes
knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to
fulfill both subtasks in a unified approach. In the prompt design, we include
fused knowledge representations, task-specific soft tokens, and the dialogue
context, which can provide sufficient contextual information to adapt the PLM
for the CRS task. Besides, for the recommendation subtask, we also incorporate
the generated response template as an important part of the prompt, to enhance
the information interaction between the two subtasks. Extensive experiments on
two public CRS datasets have demonstrated the effectiveness of our approach.",https://github.com/RUCAIBox/UniCRS,-1
aa675a0e-1877-4469-90bb-f05387c45f41,Learning General Inventory Management Policy for Large Supply Chain Network,0.219238,"Inventory management in warehouses directly affects profits made by
manufacturers. Particularly, large manufacturers produce a very large variety
of products that are handled by a significantly large number of retailers. In
such a case, the computational complexity of classical inventory management
algorithms is inordinately large. In recent years, learning-based approaches
have become popular for addressing such problems. However, previous studies
have not been managed systems where both the number of products and retailers
are large. This study proposes a reinforcement learning-based warehouse
inventory management algorithm that can be used for supply chain systems where
both the number of products and retailers are large. To solve the computational
problem of handling large systems, we provide a means of approximate simulation
of the system in the training phase. Our experiments on both real and
artificial data demonstrate that our algorithm with approximated simulation can
successfully handle large supply chain networks.",None,-1
419bd139-ee99-4456-bdf7-dfe5be2bec61,A Combined Approach of Process Mining and Rule-based AI for Study Planning and Monitoring in Higher Education,0.447075,"This paper presents an approach of using methods of process mining and
rule-based artificial intelligence to analyze and understand study paths of
students based on campus management system data and study program models.
Process mining techniques are used to characterize successful study paths, as
well as to detect and visualize deviations from expected plans. These insights
are combined with recommendations and requirements of the corresponding study
programs extracted from examination regulations. Here, event calculus and
answer set programming are used to provide models of the study programs which
support planning and conformance checking while providing feedback on possible
study plan violations. In its combination, process mining and rule-based
artificial intelligence are used to support study planning and monitoring by
deriving rules and recommendations for guiding students to more suitable study
paths with higher success rates. Two applications will be implemented, one for
students and one for study program designers.",None,-1
8c2d3778-a9b6-4349-a5ed-58a31147488f,Semantic-aligned Fusion Transformer for One-shot Object Detection,0.488785,"One-shot object detection aims at detecting novel objects according to merely
one given instance. With extreme data scarcity, current approaches explore
various feature fusions to obtain directly transferable meta-knowledge. Yet,
their performances are often unsatisfactory. In this paper, we attribute this
to inappropriate correlation methods that misalign query-support semantics by
overlooking spatial structures and scale variances. Upon analysis, we leverage
the attention mechanism and propose a simple but effective architecture named
Semantic-aligned Fusion Transformer (SaFT) to resolve these issues.
Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale
semantic enhancement and a horizontal fusion module (HFM) for cross-sample
feature fusion. Together, they broaden the vision for each feature point from
the support to a whole augmented feature pyramid from the query, facilitating
semantic-aligned associations. Extensive experiments on multiple benchmarks
demonstrate the superiority of our framework. Without fine-tuning on novel
classes, it brings significant performance gains to one-stage baselines,
lifting state-of-the-art results to a higher level.",None,-1
9bc845f2-54c9-4d54-9cef-5b2e3b52cfda,Self Meta Pseudo Labels: Meta Pseudo Labels Without The Teacher,0.101185,"We present Self Meta Pseudo Labels, a novel semi-supervised learning method
similar to Meta Pseudo Labels but without the teacher model. We introduce a
novel way to use a single model for both generating pseudo labels and
classification, allowing us to store only one model in memory instead of two.
Our method attains similar performance to the Meta Pseudo Labels method while
drastically reducing memory usage.",https://github.com/google-research/google-research/tree/master/meta pseudo labels,-1
19ada7c0-2acb-41e5-a9c6-63187a21492e,UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization,0.265938,"The high annotation costs and diverse demands of various summarization tasks
motivate the development of few-shot summarization. However, despite the
emergence of many summarization tasks and datasets, the current training
paradigm for few-shot summarization systems ignores potentially shareable
knowledge in heterogeneous datasets. To this end, we propose \textsc{UniSumm},
a unified few-shot summarization model pre-trained with multiple summarization
tasks and can be prefix-tuned to excel at any few-shot summarization task.
Meanwhile, to better evaluate few-shot summarizers, under the principles of
diversity and robustness, we assemble and release a new benchmark
\textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of
few-shot samples for each task, covering diverse domains. Experimental results
and analysis show that \textsc{UniSumm} outperforms strong baselines by a large
margin across all sub-tasks in \textsc{SummZoo} under both automatic and human
evaluations and achieves comparable results in human evaluation compared with a
GPT-3.5 model.",https://github.com/microsoft/UniSumm,-1
0eb06df0-b0fc-40e2-b09a-95aa997eb517,Label-Efficient Online Continual Object Detection in Streaming Video,0.117907,"Humans can watch a continuous video stream and effortlessly perform continual
acquisition and transfer of new knowledge with minimal supervision yet
retaining previously learnt experiences. In contrast, existing continual
learning (CL) methods require fully annotated labels to effectively learn from
individual frames in a video stream. Here, we examine a more realistic and
challenging problem$\unicode{x2014}$Label-Efficient Online Continual Object
Detection (LEOCOD) in streaming video. We propose a plug-and-play module,
Efficient-CLS, that can be easily inserted into and improve existing continual
learners for object detection in video streams with reduced data annotation
costs and model retraining time. We show that our method has achieved
significant improvement with minimal forgetting across all supervision levels
on two challenging CL benchmarks for streaming real-world videos. Remarkably,
with only 25% annotated video frames, our method still outperforms the base CL
learners, which are trained with 100% annotations on all video frames. The data
and source code will be publicly available at
https://github.com/showlab/Efficient-CLS.",https://github.com/showlab/Efficient-CLS,20788
19890c49-a16b-4579-aa6c-fe49633150ab,Rationale-Augmented Ensembles in Language Models,0.999987,"Recent research has shown that rationales, or step-by-step chains of thought,
can be used to improve performance in multi-step reasoning tasks. We reconsider
rationale-augmented prompting for few-shot in-context learning, where (input ->
output) prompts are expanded to (input, rationale -> output) prompts. For
rationale-augmented prompting we demonstrate how existing approaches, which
rely on manual prompt engineering, are subject to sub-optimal rationales that
may harm performance. To mitigate this brittleness, we propose a unified
framework of rationale-augmented ensembles, where we identify rationale
sampling in the output space as the key component to robustly improve
performance. This framework is general and can easily be extended to common
natural language processing tasks, even those that do not traditionally
leverage intermediate steps, such as question answering, word sense
disambiguation, and sentiment analysis. We demonstrate that rationale-augmented
ensembles achieve more accurate and interpretable results than existing
prompting approaches--including standard prompting without rationales and
rationale-based chain-of-thought prompting--while simultaneously improving
interpretability of model predictions through the associated rationales.",None,42280
1bfd1cc8-9f07-4dd4-994c-77fccc32faf4,Online Dynamic Reliability Evaluation of Wind Turbines based on Drone-assisted Monitoring,0.167217,"The offshore wind energy is increasingly becoming an attractive source of
energy due to having lower environmental impact. Effective operation and
maintenance that ensures the maximum availability of the energy generation
process using offshore facilities and minimal production cost are two key
factors to improve the competitiveness of this energy source over other
traditional sources of energy. Condition monitoring systems are widely used for
health management of offshore wind farms to have improved operation and
maintenance. Reliability of the wind farms are increasingly being evaluated to
aid in the maintenance process and thereby to improve the availability of the
farms. However, much of the reliability analysis is performed offline based on
statistical data. In this article, we propose a drone-assisted monitoring based
method for online reliability evaluation of wind turbines. A blade system of a
wind turbine is used as an illustrative example to demonstrate the proposed
approach.",None,5416
0e62bd34-f2d8-4fe8-a66a-aa6e894e2de9,SQuId: Measuring Speech Naturalness in Many Languages,0.798448,"Much of text-to-speech research relies on human evaluation, which incurs
heavy costs and slows down the development process. The problem is particularly
acute in heavily multilingual applications, where recruiting and polling judges
can take weeks. We introduce SQuId (Speech Quality Identification), a
multilingual naturalness prediction model trained on over a million ratings and
tested in 65 locales-the largest effort of this type to date. The main insight
is that training one model on many locales consistently outperforms mono-locale
baselines. We present our task, the model, and show that it outperforms a
competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then
demonstrate the effectiveness of cross-locale transfer during fine-tuning and
highlight its effect on zero-shot locales, i.e., locales for which there is no
fine-tuning data. Through a series of analyses, we highlight the role of
non-linguistic effects such as sound artifacts in cross-locale transfer.
Finally, we present the effect of our design decision, e.g., model size,
pre-training diversity, and language rebalancing with several ablation
experiments.",None,10439
94babbc1-7e89-4dff-bb5e-65776f6cb809,"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",0.555441,"Human language offers a powerful window into our thoughts -- we tell stories,
give explanations, and express our beliefs and goals through words. Abundant
evidence also suggests that language plays a developmental role in structuring
our learning. Here, we ask: how much of human-like thinking can be captured by
learning statistical patterns in language alone? We first contribute a new
challenge benchmark for comparing humans and distributional large language
models (LLMs). Our benchmark contains two problem-solving domains (planning and
explanation generation) and is designed to require generalization to new,
out-of-distribution problems expressed in language. We find that humans are far
more robust than LLMs on this benchmark. Next, we propose a hybrid
Parse-and-Solve model, which augments distributional LLMs with a structured
symbolic reasoning module. We find that this model shows more robust adaptation
to out-of-distribution planning problems, demonstrating the promise of hybrid
AI models for more human-like reasoning.",https://github.com/collinskatie/structured,-1
ee9d81ee-c89d-4a3c-8d00-a7341f1076b3,Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,0.317675,"Recent research has revealed that neural language models at scale suffer from
poor temporal generalization capability, i.e., the language model pre-trained
on static data from past years performs worse over time on emerging data.
Existing methods mainly perform continual training to mitigate such a
misalignment. While effective to some extent but is far from being addressed on
both the language modeling and downstream tasks. In this paper, we empirically
observe that temporal generalization is closely affiliated with lexical
semantic change, which is one of the essential phenomena of natural languages.
Based on this observation, we propose a simple yet effective lexical-level
masking strategy to post-train a converged language model. Experiments on two
pre-trained language models, two different classification tasks, and four
benchmark datasets demonstrate the effectiveness of our proposed method over
existing temporal adaptation methods, i.e., continual training with new data.
Our code is available at \url{https://github.com/zhaochen0110/LMLM}.",https://github.com/zhaochen0110/LMLM,-1
5469f8b4-20d6-448b-b974-12e905caff54,Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study,0.0505508,"Neural models that do not rely on pre-training have excelled in the keyphrase
generation task with large annotated datasets. Meanwhile, new approaches have
incorporated pre-trained language models (PLMs) for their data efficiency.
However, there lacks a systematic study of how the two types of approaches
compare and how different design choices can affect the performance of
PLM-based models. To fill in this knowledge gap and facilitate a more informed
use of PLMs for keyphrase extraction and keyphrase generation, we present an
in-depth empirical study. Formulating keyphrase extraction as sequence labeling
and keyphrase generation as sequence-to-sequence generation, we perform
extensive experiments in three domains. After showing that PLMs have
competitive high-resource performance and state-of-the-art low-resource
performance, we investigate important design choices including in-domain PLMs,
PLMs with different pre-training objectives, using PLMs with a parameter
budget, and different formulations for present keyphrases. Further results show
that (1) in-domain BERT-like PLMs can be used to build strong and
data-efficient keyphrase generation models; (2) with a fixed parameter budget,
prioritizing model depth over width and allocating more layers in the encoder
leads to better encoder-decoder models; and (3) introducing four in-domain
PLMs, we achieve a competitive performance in the news domain and the
state-of-the-art performance in the scientific domain.",https://github.com/uclanlp/DeepKPG,-1
c7ed0475-f315-47a3-8483-75bacc45e03e,Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions,0.63702,"Generalization across different environments with the same tasks is critical
for successful applications of visual reinforcement learning (RL) in real
scenarios. However, visual distractions -- which are common in real scenes --
from high-dimensional observations can be hurtful to the learned
representations in visual RL, thus degrading the performance of generalization.
To tackle this problem, we propose a novel approach, namely Characteristic
Reward Sequence Prediction (CRESP), to extract the task-relevant information by
learning reward sequence distributions (RSDs), as the reward signals are
task-relevant in RL and invariant to visual distractions. Specifically, to
effectively capture the task-relevant information via RSDs, CRESP introduces an
auxiliary task -- that is, predicting the characteristic functions of RSDs --
to learn task-relevant representations, because we can well approximate the
high-dimensional distributions by leveraging the corresponding characteristic
functions. Experiments demonstrate that CRESP significantly improves the
performance of generalization on unseen environments, outperforming several
state-of-the-arts on DeepMind Control tasks with different visual distractions.",None,-1
aaf77109-161c-4e16-b983-a8bca9f3445a,Sample-Efficient Reinforcement Learning of Partially Observable Markov Games,0.754062,"This paper considers the challenging tasks of Multi-Agent Reinforcement
Learning (MARL) under partial observability, where each agent only sees her own
individual observations and actions that reveal incomplete information about
the underlying state of system. This paper studies these tasks under the
general model of multiplayer general-sum Partially Observable Markov Games
(POMGs), which is significantly larger than the standard model of Imperfect
Information Extensive-Form Games (IIEFGs). We identify a rich subclass of POMGs
-- weakly revealing POMGs -- in which sample-efficient learning is tractable.
In the self-play setting, we prove that a simple algorithm combining optimism
and Maximum Likelihood Estimation (MLE) is sufficient to find approximate Nash
equilibria, correlated equilibria, as well as coarse correlated equilibria of
weakly revealing POMGs, in a polynomial number of samples when the number of
agents is small. In the setting of playing against adversarial opponents, we
show that a variant of our optimistic MLE algorithm is capable of achieving
sublinear regret when being compared against the optimal maximin policies. To
our best knowledge, this work provides the first line of sample-efficient
results for learning POMGs.",None,-1
acfc9543-b0b9-4dbc-be85-cfc7908deefc,Exploring Global Diversity and Local Context for Video Summarization,0.090327,"Video summarization aims to automatically generate a diverse and concise
summary which is useful in large-scale video processing. Most of the methods
tend to adopt self-attention mechanism across video frames, which fails to
model the diversity of video frames. To alleviate this problem, we revisit the
pairwise similarity measurement in self-attention mechanism and find that the
existing inner-product affinity leads to discriminative features rather than
diversified features. In light of this phenomenon, we propose global diverse
attention which uses the squared Euclidean distance instead to compute the
affinities. Moreover, we model the local contextual information by novel local
contextual attention to remove the redundancy in the video. By combining these
two attention mechanisms, a video SUMmarization model with Diversified
Contextual Attention scheme is developed, namely SUM-DCA. Extensive experiments
are conducted on benchmark data sets to verify the effectiveness and the
superiority of SUM-DCA in terms of F-score and rank-based evaluation without
any bells and whistles.",None,-1
2a53aa01-3ef5-452d-94eb-256ec4b6e2b2,A Hierarchical Deep Neural Network for Detecting Lines of Codes with Vulnerabilities,0.0387262,"Software vulnerabilities, caused by unintentional flaws in source codes, are
the main root cause of cyberattacks. Source code static analysis has been used
extensively to detect the unintentional defects, i.e. vulnerabilities,
introduced into the source codes by software developers. In this paper, we
propose a deep learning approach to detect vulnerabilities from their LLVM IR
representations based on the techniques that have been used in natural language
processing. The proposed approach uses a hierarchical process to first identify
source codes with vulnerabilities, and then it identifies the lines of codes
that contribute to the vulnerability within the detected source codes. This
proposed two-step approach reduces the false alarm of detecting vulnerable
lines. Our extensive experiment on real-world and synthetic codes collected in
NVD and SARD shows high accuracy (about 98\%) in detecting source code
vulnerabilities.",https://github.com/arashmahyari/PLP,-1
a3ddb00d-61db-4eed-b44e-355ebede0765,Extreme Image Transformations Affect Humans and Machines Differently,0.200596,"Some recent artificial neural networks (ANNs) claim to model aspects of
primate neural and human performance data. Their success in object recognition
is, however, dependent on exploiting low-level features for solving visual
tasks in a way that humans do not. As a result, out-of-distribution or
adversarial input is often challenging for ANNs. Humans instead learn abstract
patterns and are mostly unaffected by many extreme image distortions. We
introduce a set of novel image transforms inspired by neurophysiological
findings and evaluate humans and ANNs on an object recognition task. We show
that machines perform better than humans for certain transforms and struggle to
perform at par with humans on others that are easy for humans. We quantify the
differences in accuracy for humans and machines and find a ranking of
difficulty for our transforms for human data. We also suggest how certain
characteristics of human visual processing can be adapted to improve the
performance of ANNs for our difficult-for-machines transforms.",None,-1
6dd50c40-bcc5-4151-a1fb-e4f84cd97c03,Normalizing Flows for Human Pose Anomaly Detection,0.701968,"Video anomaly detection is an ill-posed problem because it relies on many
parameters such as appearance, pose, camera angle, background, and more. We
distill the problem to anomaly detection of human pose, thus decreasing the
risk of nuisance parameters such as appearance affecting the result. Focusing
on pose alone also has the side benefit of reducing bias against distinct
minority groups. Our model works directly on human pose graph sequences and is
exceptionally lightweight (~1K parameters), capable of running on any machine
able to run the pose estimation with negligible additional resources. We
leverage the highly compact pose representation in a normalizing flows
framework, which we extend to tackle the unique characteristics of
spatio-temporal pose data and show its advantages in this use case. The
algorithm is quite general and can handle training data of only normal examples
as well as a supervised setting that consists of labeled normal and abnormal
examples. We report state-of-the-art results on two anomaly detection
benchmarks - the unsupervised ShanghaiTech dataset and the recent supervised
UBnormal dataset.",https://github.com/orhir/STG-NF,-1
00bf32e7-6002-4f24-8bf1-42b800a62a05,SeRP: Self-Supervised Representation Learning Using Perturbed Point Clouds,0.139102,"We present SeRP, a framework for Self-Supervised Learning of 3D point clouds.
SeRP consists of encoder-decoder architecture that takes perturbed or corrupted
point clouds as inputs and aims to reconstruct the original point cloud without
corruption. The encoder learns the high-level latent representations of the
points clouds in a low-dimensional subspace and recovers the original
structure. In this work, we have used Transformers and PointNet-based
Autoencoders. The proposed framework also addresses some of the limitations of
Transformers-based Masked Autoencoders which are prone to leakage of location
information and uneven information density. We trained our models on the
complete ShapeNet dataset and evaluated them on ModelNet40 as a downstream
classification task. We have shown that the pretrained models achieved 0.5-1%
higher classification accuracies than the networks trained from scratch.
Furthermore, we also proposed VASP: Vector-Quantized Autoencoder for
Self-supervised Representation Learning for Point Clouds that employs
Vector-Quantization for discrete representation learning for Transformer-based
autoencoders.",None,-1
091e8782-f6a0-4f57-afc0-ed9d46bb44f6,NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs,0.400171,"Complex query answering (CQA) is an essential task for multi-hop and logical
reasoning on knowledge graphs (KGs). Currently, most approaches are limited to
queries among binary relational facts and pay less attention to n-ary facts
(n>=2) containing more than two entities, which are more prevalent in the real
world. Moreover, previous CQA methods can only make predictions for a few given
types of queries and cannot be flexibly extended to more complex logical
queries, which significantly limits their applications. To overcome these
challenges, in this work, we propose a novel N-ary Query Embedding (NQE) model
for CQA over hyper-relational knowledge graphs (HKGs), which include massive
n-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder and
fuzzy logic theory to satisfy all n-ary FOL queries, including existential
quantifiers, conjunction, disjunction, and negation. We also propose a parallel
processing algorithm that can train or predict arbitrary n-ary FOL queries in a
single batch, regardless of the kind of each query, with good flexibility and
extensibility. In addition, we generate a new CQA dataset WD50K-NFOL, including
diverse n-ary FOL queries over WD50K. Experimental results on WD50K-NFOL and
other standard CQA datasets show that NQE is the state-of-the-art CQA method
over HKGs with good generalization capability. Our code and dataset are
publicly available.",https://github.com/LHRLAB/NQE,2442
ac680eea-bd94-48e0-8397-5614810ecd7d,A Continuum of Generation Tasks for Investigating Length Bias and Degenerate Repetition,0.0715362,"Language models suffer from various degenerate behaviors. These differ
between tasks: machine translation (MT) exhibits length bias, while tasks like
story generation exhibit excessive repetition. Recent work has attributed the
difference to task constrainedness, but evidence for this claim has always
involved many confounding variables. To study this question directly, we
introduce a new experimental framework that allows us to smoothly vary task
constrainedness, from MT at one end to fully open-ended generation at the
other, while keeping all other aspects fixed. We find that: (1) repetition
decreases smoothly with constrainedness, explaining the difference in
repetition across tasks; (2) length bias surprisingly also decreases with
constrainedness, suggesting some other cause for the difference in length bias;
(3) across the board, these problems affect the mode, not the whole
distribution; (4) the differences cannot be attributed to a change in the
entropy of the distribution, since another method of changing the entropy,
label smoothing, does not produce the same effect.",https://github.com/darcey/transformers_without_tears/tree/mt-interpolation-paper,-1
06418f9f-4322-4111-87f8-d8ebc5e8c16d,Domain-General Crowd Counting in Unseen Scenarios,0.61098,"Domain shift across crowd data severely hinders crowd counting models to
generalize to unseen scenarios. Although domain adaptive crowd counting
approaches close this gap to a certain extent, they are still dependent on the
target domain data to adapt (e.g. finetune) their models to the specific
domain. In this paper, we aim to train a model based on a single source domain
which can generalize well on any unseen domain. This falls into the realm of
domain generalization that remains unexplored in crowd counting. We first
introduce a dynamic sub-domain division scheme which divides the source domain
into multiple sub-domains such that we can initiate a meta-learning framework
for domain generalization. The sub-domain division is dynamically refined
during the meta-learning. Next, in order to disentangle domain-invariant
information from domain-specific information in image features, we design the
domain-invariant and -specific crowd memory modules to re-encode image
features. Two types of losses, i.e. feature reconstruction and orthogonal
losses, are devised to enable this disentanglement. Extensive experiments on
several standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, show
the strong generalizability of our method.",https://github.com/ZPDu/Domain-general-Crowd-Counting-in-Unseen-Scenarios,2009
76da2cdc-027e-49ce-b344-eb065c0c48ef,MISeval: a Metric Library for Medical Image Segmentation Evaluation,0.474804,"Correct performance assessment is crucial for evaluating modern artificial
intelligence algorithms in medicine like deep-learning based medical image
segmentation models. However, there is no universal metric library in Python
for standardized and reproducible evaluation. Thus, we propose our open-source
publicly available Python package MISeval: a metric library for Medical Image
Segmentation Evaluation. The implemented metrics can be intuitively used and
easily integrated into any performance assessment pipeline. The package
utilizes modern CI/CD strategies to ensure functionality and stability. MISeval
is available from PyPI (miseval) and GitHub:
https://github.com/frankkramer-lab/miseval.",https://github.com/frankkramer-lab/miseval,-1
91db3b2d-62a0-41bf-a702-a3270c3e1a2a,Can Model Compression Improve NLP Fairness,0.656412,"Model compression techniques are receiving increasing attention; however, the
effect of compression on model fairness is still under explored. This is the
first paper to examine the effect of distillation and pruning on the toxicity
and bias of generative language models. We test Knowledge Distillation and
Pruning methods on the GPT2 model and found a consistent pattern of toxicity
and bias reduction after model distillation; this result can be potentially
interpreted by existing line of research which describes model compression as a
regularization technique; our work not only serves as a reference for safe
deployment of compressed models, but also extends the discussion of
""compression as regularization"" into the setting of neural LMs, and hints at
the possibility of using compression to develop fairer models.",https://github.com/unitaryai/detoxify,-1
5724a834-e937-47f5-a863-3d41776571af,The Royalflush System for VoxCeleb Speaker Recognition Challenge 2022,0.0834846,"In this technical report, we describe the Royalflush submissions for the
VoxCeleb Speaker Recognition Challenge 2022 (VoxSRC-22). Our submissions
contain track 1, which is for supervised speaker verification and track 3,
which is for semi-supervised speaker verification. For track 1, we develop a
powerful U-Net-based speaker embedding extractor with a symmetric architecture.
The proposed system achieves 2.06% in EER and 0.1293 in MinDCF on the
validation set. Compared with the state-of-the-art ECAPA-TDNN, it obtains a
relative improvement of 20.7% in EER and 22.70% in MinDCF. For track 3, we
employ the joint training of source domain supervision and target domain
self-supervision to get a speaker embedding extractor. The subsequent
clustering process can obtain target domain pseudo-speaker labels. We adapt the
speaker embedding extractor using all source and target domain data in a
supervised manner, where it can fully leverage both domain information.
Moreover, clustering and supervised domain adaptation can be repeated until the
performance converges on the validation set. Our final submission is a fusion
of 10 models and achieves 7.75% EER and 0.3517 MinDCF on the validation set.",None,-1
53e73f76-b806-4c39-b9a9-d7eea79314ea,SC^2-PCR: A Second Order Spatial Compatibility for Efficient and Robust Point Cloud Registration,0.923899,"In this paper, we present a second order spatial compatibility (SC^2) measure
based method for efficient and robust point cloud registration (PCR), called
SC^2-PCR. Firstly, we propose a second order spatial compatibility (SC^2)
measure to compute the similarity between correspondences. It considers the
global compatibility instead of local consistency, allowing for more
distinctive clustering between inliers and outliers at early stage. Based on
this measure, our registration pipeline employs a global spectral technique to
find some reliable seeds from the initial correspondences. Then we design a
two-stage strategy to expand each seed to a consensus set based on the SC^2
measure matrix. Finally, we feed each consensus set to a weighted SVD algorithm
to generate a candidate rigid transformation and select the best model as the
final result. Our method can guarantee to find a certain number of outlier-free
consensus sets using fewer samplings, making the model estimation more
efficient and robust. In addition, the proposed SC^2 measure is general and can
be easily plugged into deep learning based frameworks. Extensive experiments
are carried out to investigate the performance of our method. Code will be
available at \url{https://github.com/ZhiChen902/SC2-PCR}.",https://github.com/ZhiChen902/SC2-PCR,-1
5e83fa2b-4cc2-4174-939f-5bfe4e768fe3,Multi-Vector Retrieval as Sparse Alignment,0.242062,"Multi-vector retrieval models improve over single-vector dual encoders on
many information retrieval tasks. In this paper, we cast the multi-vector
retrieval problem as sparse alignment between query and document tokens. We
propose AligneR, a novel multi-vector retrieval model that learns sparsified
pairwise alignments between query and document tokens (e.g. `dog' vs. `puppy')
and per-token unary saliences reflecting their relative importance for
retrieval. We show that controlling the sparsity of pairwise token alignments
often brings significant performance gains. While most factoid questions
focusing on a specific part of a document require a smaller number of
alignments, others requiring a broader understanding of a document favor a
larger number of alignments. Unary saliences, on the other hand, decide whether
a token ever needs to be aligned with others for retrieval (e.g. `kind' from
`kind of currency is used in new zealand}'). With sparsified unary saliences,
we are able to prune a large number of query and document token vectors and
improve the efficiency of multi-vector retrieval. We learn the sparse unary
saliences with entropy-regularized linear programming, which outperforms other
methods to achieve sparsity. In a zero-shot setting, AligneR scores 51.1 points
nDCG@10, achieving a new retriever-only state-of-the-art on 13 tasks in the
BEIR benchmark. In addition, adapting pairwise alignments with a few examples
(<= 8) further improves the performance up to 15.7 points nDCG@10 for argument
retrieval tasks. The unary saliences of AligneR helps us to keep only 20% of
the document token representations with minimal performance loss. We further
show that our model often produces interpretable alignments and significantly
improves its performance when initialized from larger language models.",None,-1
242c1516-6d7a-4a96-8bd4-9234d78c95c4,The Inverse Problem for Argumentation Gradual Semantics,0.127226,"Gradual semantics with abstract argumentation provide each argument with a
score reflecting its acceptability, i.e. how ""much"" it is attacked by other
arguments. Many different gradual semantics have been proposed in the
literature, each following different principles and producing different
argument rankings. A sub-class of such semantics, the so-called weighted
semantics, takes, in addition to the graph structure, an initial set of weights
over the arguments as input, with these weights affecting the resultant
argument ranking. In this work, we consider the inverse problem over such
weighted semantics. That is, given an argumentation framework and a desired
argument ranking, we ask whether there exist initial weights such that a
particular semantics produces the given ranking. The contribution of this paper
are: (1) an algorithm to answer this problem, (2) a characterisation of the
properties that a gradual semantics must satisfy for the algorithm to operate,
and (3) an empirical evaluation of the proposed algorithm.",https://github.com/jhudsy/,4513
30e5b2d5-5a77-422f-b2b6-9cf90f71e134,Quark: Controllable Text Generation with Reinforced Unlearning,0.882168,"Large-scale language models often learn behaviors that are misaligned with
user expectations. Generated text may contain offensive or toxic language,
contain significant repetition, or be of a different sentiment than desired by
the user. We consider the task of unlearning these misalignments by fine-tuning
the language model on signals of what not to do. We introduce Quantized Reward
Konditioning (Quark), an algorithm for optimizing a reward function that
quantifies an (un)wanted property, while not straying too far from the original
model. Quark alternates between (i) collecting samples with the current
language model, (ii) sorting them into quantiles based on reward, with each
quantile identified by a reward token prepended to the language model's input,
and (iii) using a standard language modeling loss on samples from each quantile
conditioned on its reward token, while remaining nearby the original language
model via a KL-divergence penalty. By conditioning on a high-reward token at
generation time, the model generates text that exhibits less of the unwanted
property. For unlearning toxicity, negative sentiment, and repetition, our
experiments show that Quark outperforms both strong baselines and
state-of-the-art reinforcement learning methods like PPO (Schulman et al.
2017), while relying only on standard language modeling primitives.",https://github.com/GXimingLu/Quark,46374
1797b075-6670-4cdd-9b5c-5228a1509a16,Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs,0.918276,"Most existing deep neural networks (DNNs) are easily disturbed by slight
noise. However, there are few researches on physical attacks by deploying
lighting equipment. The light-based physical attacks has excellent covertness,
which brings great security risks to many vision-based applications (such as
self-driving). Therefore, we propose a light-based physical attack, called
adversarial laser spot (AdvLS), which optimizes the physical parameters of
laser spots through genetic algorithm to perform physical attacks. It realizes
robust and covert physical attack by using low-cost laser equipment. As far as
we know, AdvLS is the first light-based physical attack that perform physical
attacks in the daytime. A large number of experiments in the digital and
physical environments show that AdvLS has excellent robustness and covertness.
In addition, through in-depth analysis of the experimental data, we find that
the adversarial perturbations generated by AdvLS have superior adversarial
attack migration. The experimental results show that AdvLS impose serious
interference to advanced DNNs, we call for the attention of the proposed AdvLS.
The code of AdvLS is available at: https://github.com/ChengYinHu/AdvLS",https://github.com/ChengYinHu/AdvLS,9817
3855b7de-2d10-4790-bf26-aee9bca81c8c,Morphological Processing of Low-Resource Languages: Where We Are and What's Next,0.283779,"Automatic morphological processing can aid downstream natural language
processing applications, especially for low-resource languages, and assist
language documentation efforts for endangered languages. Having long been
multilingual, the field of computational morphology is increasingly moving
towards approaches suitable for languages with minimal or no annotated
resources. First, we survey recent developments in computational morphology
with a focus on low-resource languages. Second, we argue that the field is
ready to tackle the logical next challenge: understanding a language's
morphology from raw text alone. We perform an empirical study on a truly
unsupervised version of the paradigm completion task and show that, while
existing state-of-the-art models bridged by two newly proposed models we devise
perform reasonably, there is still much room for improvement. The stakes are
high: solving this task will increase the language coverage of morphological
resources by a number of magnitudes.",https://github.com/Adamits/tUMPC,3375
22348c43-2317-4e6e-a3a5-ae45c4e7d2b8,Learning by Distilling Context,0.639441,"Language models significantly benefit from context tokens, such as prompts or
scratchpads. They perform better when prompted with informative instructions,
and they acquire new reasoning capabilities by generating a scratch-pad before
predicting the final answers. However, they do not \textit{internalize} these
performance gains, which disappear when the context tokens are gone. Our work
proposes to apply context distillation so that a language model can improve
itself by internalizing these gains. Concretely, given a synthetic unlabeled
input for the target task, we condition the model on ``[instructions] +
[task-input]'' to predict ``[scratch-pad] + [final answer]''; then we fine-tune
the same model to predict its own ``[final answer]'' conditioned on the
``[task-input]'', without seeing the ``[instructions]'' or using the
``[scratch-pad]''.
  We show that context distillation is a general method to train language
models, and it can effectively internalize 3 types of training signals. First,
it can internalize abstract task instructions and explanations, so we can
iteratively update the model parameters with new instructions and overwrite old
ones. Second, it can internalize step-by-step reasoning for complex tasks
(e.g., 8-digit addition), and such a newly acquired capability proves to be
useful for other downstream tasks. Finally, it can internalize concrete
training examples, and it outperforms directly learning with gradient descent
by 9\% on the SPIDER Text-to-SQL dataset; furthermore, combining context
distillation operations can internalize more training examples than the context
window size allows.",None,38020
fcf526fe-41ad-4a50-ac0f-45bab206e39c,Dual-Pixel Raindrop Removal,0.167805,"Removing raindrops in images has been addressed as a significant task for
various computer vision applications. In this paper, we propose the first
method using a Dual-Pixel (DP) sensor to better address the raindrop removal.
Our key observation is that raindrops attached to a glass window yield
noticeable disparities in DP's left-half and right-half images, while almost no
disparity exists for in-focus backgrounds. Therefore, DP disparities can be
utilized for robust raindrop detection. The DP disparities also brings the
advantage that the occluded background regions by raindrops are shifted between
the left-half and the right-half images. Therefore, fusing the information from
the left-half and the right-half images can lead to more accurate background
texture recovery. Based on the above motivation, we propose a DP Raindrop
Removal Network (DPRRN) consisting of DP raindrop detection and DP fused
raindrop removal. To efficiently generate a large amount of training data, we
also propose a novel pipeline to add synthetic raindrops to real-world
background DP images. Experimental results on synthetic and real-world datasets
demonstrate that our DPRRN outperforms existing state-of-the-art methods,
especially showing better robustness to real-world situations. Our source code
and datasets are available at http://www.ok.sc.e.titech.ac.jp/res/SIR/.",http://www.ok.sc.e.titech.ac.jp/res/SIR/,-1
9245db9b-4818-45e7-824b-15ca3d9c4482,An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification,0.493819,"Non-hierarchical sparse attention Transformer-based models, such as
Longformer and Big Bird, are popular approaches to working with long documents.
There are clear benefits to these approaches compared to the original
Transformer in terms of efficiency, but Hierarchical Attention Transformer
(HAT) models are a vastly understudied alternative. We develop and release
fully pre-trained HAT models that use segment-wise followed by cross-segment
encoders and compare them with Longformer models and partially pre-trained
HATs. In several long document downstream classification tasks, our best HAT
model outperforms equally-sized Longformer models while using 10-20% less GPU
memory and processing documents 40-45% faster. In a series of ablation studies,
we find that HATs perform best with cross-segment contextualization throughout
the model than alternative configurations that implement either early or late
cross-segment contextualization. Our code is on GitHub:
https://github.com/coastalcph/hierarchical-transformers.",https://github.com/coastalcph/hierarchical-transformers,-1
aec9975b-adda-407c-9537-7f8f1376cc30,Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference,0.454463,"It has been shown that NLI models are usually biased with respect to the
word-overlap between premise and hypothesis; they take this feature as a
primary cue for predicting the entailment label. In this paper, we focus on an
overlooked aspect of the overlap bias in NLI models: the reverse word-overlap
bias. Our experimental results demonstrate that current NLI models are highly
biased towards the non-entailment label on instances with low overlap, and the
existing debiasing methods, which are reportedly successful on existing
challenge datasets, are generally ineffective in addressing this category of
bias. We investigate the reasons for the emergence of the overlap bias and the
role of minority examples in its mitigation. For the former, we find that the
word-overlap bias does not stem from pre-training, and for the latter, we
observe that in contrast to the accepted assumption, eliminating minority
examples does not affect the generalizability of debiasing methods with respect
to the overlap bias.",https://github.com/sara-rajaee/reverse_bias,-1
00399e41-564a-4889-a32b-478e66e5de59,Enhancing Local Feature Learning for 3D Point Cloud Processing using Unary-Pairwise Attention,0.242699,"We present a simple but effective attention named the unary-pairwise
attention (UPA) for modeling the relationship between 3D point clouds. Our idea
is motivated by the analysis that the standard self-attention (SA) that
operates globally tends to produce almost the same attention maps for different
query positions, revealing difficulties for learning query-independent and
query-dependent information jointly. Therefore, we reformulate the SA and
propose query-independent (Unary) and query-dependent (Pairwise) components to
facilitate the learning of both terms. In contrast to the SA, the UPA ensures
query dependence via operating locally. Extensive experiments show that the UPA
outperforms the SA consistently on various point cloud understanding tasks
including shape classification, part segmentation, and scene segmentation.
Moreover, simply equipping the popular PointNet++ method with the UPA even
outperforms or is on par with the state-of-the-art attention-based approaches.
In addition, the UPA systematically boosts the performance of both standard and
modern networks when it is integrated into them as a compositional module.",None,-1
48d8df0b-e839-42c0-93f5-f0ff123d4a2e,GriddlyJS: A Web IDE for Reinforcement Learning,0.259688,"Progress in reinforcement learning (RL) research is often driven by the
design of new, challenging environments -- a costly undertaking requiring
skills orthogonal to that of a typical machine learning researcher. The
complexity of environment development has only increased with the rise of
procedural-content generation (PCG) as the prevailing paradigm for producing
varied environments capable of testing the robustness and generalization of RL
agents. Moreover, existing environments often require complex build processes,
making reproducing results difficult. To address these issues, we introduce
GriddlyJS, a web-based Integrated Development Environment (IDE) based on the
Griddly engine. GriddlyJS allows researchers to visually design and debug
arbitrary, complex PCG grid-world environments using a convenient graphical
interface, as well as visualize, evaluate, and record the performance of
trained agent models. By connecting the RL workflow to the advanced
functionality enabled by modern web standards, GriddlyJS allows publishing
interactive agent-environment demos that reproduce experimental results
directly to the web. To demonstrate the versatility of GriddlyJS, we use it to
quickly develop a complex compositional puzzle-solving environment alongside
arbitrary human-designed environment configurations and their solutions for use
in automatic curriculum learning and offline RL. The GriddlyJS IDE is open
source and freely available at https://griddly.ai.",https://github.com/GriddlyAI/escape-rooms,-1
0fdca95a-9972-4ec3-8ded-8dd41f6baf92,MMINR: Multi-frame-to-Multi-frame Inference with Noise Resistance for Precipitation Nowcasting with Radar,0.138534,"Precipitation nowcasting based on radar echo maps is essential in
meteorological research. Recently, Convolutional RNNs based methods dominate
this field, but they cannot be solved by parallel computation resulting in
longer inference time. FCN based methods adopt a multi-frame-to-single-frame
inference (MSI) strategy to avoid this problem. They feedback into the model
again to predict the next time step to get multi-frame nowcasting results in
the prediction phase, which will lead to the accumulation of prediction errors.
In addition, precipitation noise is a crucial factor contributing to high
prediction errors because of its unpredictability. To address this problem, we
propose a novel Multi-frame-to-Multi-frame Inference (MMI) model with Noise
Resistance (NR) named MMINR. It avoids error accumulation and resists
precipitation noise\'s negative effect in parallel computation. NR contains a
Noise Dropout Module (NDM) and a Semantic Restore Module (SRM). NDM
deliberately dropout noise simple yet efficient, and SRM supplements semantic
information of features to alleviate the problem of semantic information
mistakenly lost by NDM. Experimental results demonstrate that MMINR can attain
competitive scores compared with other SOTAs. The ablation experiments show
that the proposed NDM and SRM can solve the aforementioned problems.",None,850
1471a77b-61f4-4ede-984f-9fef13f19096,A Robust Learning Methodology for Uncertainty-aware Scientific Machine Learning models,0.460016,"Robust learning is an important issue in Scientific Machine Learning (SciML).
There are several works in the literature addressing this topic. However, there
is an increasing demand for methods that can simultaneously consider all the
different uncertainty components involved in SciML model identification. Hence,
this work proposes a comprehensive methodology for uncertainty evaluation of
the SciML that also considers several possible sources of uncertainties
involved in the identification process. The uncertainties considered in the
proposed method are the absence of theory and causal models, the sensitiveness
to data corruption or imperfection, and the computational effort. Therefore, it
was possible to provide an overall strategy for the uncertainty-aware models in
the SciML field. The methodology is validated through a case study, developing
a Soft Sensor for a polymerization reactor. The results demonstrated that the
identified Soft Sensor are robust for uncertainties, corroborating with the
consistency of the proposed approach.",None,712
9aba4f07-8b57-4aa2-9d22-9aef580065a8,Uncertainty-Aware Search Framework for Multi-Objective Bayesian Optimization,0.858423,"We consider the problem of multi-objective (MO) blackbox optimization using
expensive function evaluations, where the goal is to approximate the true
Pareto set of solutions while minimizing the number of function evaluations.
For example, in hardware design optimization, we need to find the designs that
trade-off performance, energy, and area overhead using expensive simulations.
We propose a novel uncertainty-aware search framework referred to as USeMO to
efficiently select the sequence of inputs for evaluation to solve this problem.
The selection method of USeMO consists of solving a cheap MO optimization
problem via surrogate models of the true functions to identify the most
promising candidates and picking the best candidate based on a measure of
uncertainty. We also provide theoretical analysis to characterize the efficacy
of our approach. Our experiments on several synthetic and six diverse
real-world benchmark problems show that USeMO consistently outperforms the
state-of-the-art algorithms.",https://github.com/belakaria/USeMO,-1
2e3b4f2d-4768-48e9-b1df-51a634c18d3e,Poseur: Direct Human Pose Regression with Transformers,0.914351,"We propose a direct, regression-based approach to 2D human pose estimation
from single images. We formulate the problem as a sequence prediction task,
which we solve using a Transformer network. This network directly learns a
regression mapping from images to the keypoint coordinates, without resorting
to intermediate representations such as heatmaps. This approach avoids much of
the complexity associated with heatmap-based approaches. To overcome the
feature misalignment issues of previous regression-based methods, we propose an
attention mechanism that adaptively attends to the features that are most
relevant to the target keypoints, considerably improving the accuracy.
Importantly, our framework is end-to-end differentiable, and naturally learns
to exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,
two predominant pose-estimation datasets, demonstrate that our method
significantly improves upon the state-of-the-art in regression-based pose
estimation. More notably, ours is the first regression-based approach to
perform favorably compared to the best heatmap-based pose estimation methods.",https://github.com/aim-uofa/Poseur,-1
60b344f9-617a-4da1-9948-7e1337794865,Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes,0.364009,"We present a efficient multi-view inverse rendering method for large-scale
real-world indoor scenes that reconstructs global illumination and
physically-reasonable SVBRDFs. Unlike previous representations, where the
global illumination of large scenes is simplified as multiple environment maps,
we propose a compact representation called Texture-based Lighting (TBL). It
consists of 3D mesh and HDR textures, and efficiently models direct and
infinite-bounce indirect lighting of the entire large scene. Based on TBL, we
further propose a hybrid lighting representation with precomputed irradiance,
which significantly improves the efficiency and alleviates the rendering noise
in the material optimization. To physically disentangle the ambiguity between
materials, we propose a three-stage material optimization strategy based on the
priors of semantic segmentation and room segmentation. Extensive experiments
show that the proposed method outperforms the state-of-the-art quantitatively
and qualitatively, and enables physically-reasonable mixed-reality applications
such as material editing, editable novel view synthesis and relighting. The
project page is at https://lzleejean.github.io/TexIR.",https://github.com/open-mmlab/mmsegmentation,-1
d0eadf21-5318-4028-8d13-8b0c018ae07d,"Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots",0.52892,"Few-shot table-to-text generation is a task of composing fluent and faithful
sentences to convey table content using limited data. Despite many efforts
having been made towards generating impressive fluent sentences by fine-tuning
powerful pre-trained language models, the faithfulness of generated content
still needs to be improved. To this end, this paper proposes a novel approach
Attend, Memorize and Generate (called AMG), inspired by the text generation
process of humans. In particular, AMG (1) attends over the multi-granularity of
context using a novel strategy based on table slot level and traditional
token-by-token level attention to exploit both the table structure and natural
linguistic information; (2) dynamically memorizes the table slot allocation
states; and (3) generates faithful sentences according to both the context and
memory allocation states. Comprehensive experiments with human evaluation on
three domains (i.e., humans, songs, and books) of the Wiki dataset show that
our model can generate higher qualified texts when compared with several
state-of-the-art baselines, in both fluency and faithfulness.",https://github.com/wentinghome/AMG,-1
7de7502e-eb10-49f4-82dc-8d359190a5fb,Interactive Style Transfer: All is Your Palette,0.0945781,"Neural style transfer (NST) can create impressive artworks by transferring
reference style to content image. Current image-to-image NST methods are short
of fine-grained controls, which are often demanded by artistic editing. To
mitigate this limitation, we propose a drawing-like interactive style transfer
(IST) method, by which users can interactively create a harmonious-style image.
Our IST method can serve as a brush, dip style from anywhere, and then paint to
any region of the target content image. To determine the action scope, we
formulate a fluid simulation algorithm, which takes styles as pigments around
the position of brush interaction, and diffusion in style or content images
according to the similarity maps. Our IST method expands the creative dimension
of NST. By dipping and painting, even employing one style image can produce
thousands of eye-catching works. The demo video is available in supplementary
files or in http://mmcheng.net/ist.",None,-1
1c843553-a694-426c-aca1-486c04b3536f,Arabic Word-level Readability Visualization for Assisted Text Simplification,0.457826,"This demo paper presents a Google Docs add-on for automatic Arabic word-level
readability visualization. The add-on includes a lemmatization component that
is connected to a five-level readability lexicon and Arabic WordNet-based
substitution suggestions. The add-on can be used for assessing the reading
difficulty of a text and identifying difficult words as part of the task of
manual text simplification. We make our add-on and its code publicly available.",None,16583
7975afd9-75dd-4981-befa-ea7e6ff23212,Delving into Out-of-Distribution Detection with Vision-Language Representations,0.999835,"Recognizing out-of-distribution (OOD) samples is critical for machine
learning systems deployed in the open world. The vast majority of OOD detection
methods are driven by a single modality (e.g., either vision or language),
leaving the rich information in multi-modal representations untapped. Inspired
by the recent success of vision-language pre-training, this paper enriches the
landscape of OOD detection from a single-modal to a multi-modal regime.
Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective
zero-shot OOD detection method based on aligning visual features with textual
concepts. We contribute in-depth analysis and theoretical insights to
understand the effectiveness of MCM. Extensive experiments demonstrate that MCM
achieves superior performance on a wide variety of real-world tasks. MCM with
vision-language features outperforms a common baseline with pure visual
features on a hard OOD task with semantically similar classes by 13.1% (AUROC).
Code is available at https://github.com/deeplearning-wisc/MCM.",https://github.com/deeplearning-wisc/MCM,-1
810a66dd-0e09-4bfb-93ca-08d2dcb801ce,Track Targets by Dense Spatio-Temporal Position Encoding,0.693672,"In this work, we propose a novel paradigm to encode the position of targets
for target tracking in videos using transformers. The proposed paradigm, Dense
Spatio-Temporal (DST) position encoding, encodes spatio-temporal position
information in a pixel-wise dense fashion. The provided position encoding
provides location information to associate targets across frames beyond
appearance matching by comparing objects in two bounding boxes. Compared to the
typical transformer positional encoding, our proposed encoding is applied to
the 2D CNN features instead of the projected feature vectors to avoid losing
positional information. Moreover, the designed DST encoding can represent the
location of a single-frame object and the evolution of the location of the
trajectory among frames uniformly. Integrated with the DST encoding, we build a
transformer-based multi-object tracking model. The model takes a video clip as
input and conducts the target association in the clip. It can also perform
online inference by associating existing trajectories with objects from the
new-coming frames. Experiments on video multi-object tracking (MOT) and
multi-object tracking and segmentation (MOTS) datasets demonstrate the
effectiveness of the proposed DST position encoding.",https://github.com/open-mmlab/mmtracking,-1
707e05c7-3a2c-44b0-84a9-d5b13bc8f0e5,CYBORGS: Contrastively Bootstrapping Object Representations by Grounding in Segmentation,0.0234061,"Many recent approaches in contrastive learning have worked to close the gap
between pretraining on iconic images like ImageNet and pretraining on complex
scenes like COCO. This gap exists largely because commonly used random crop
augmentations obtain semantically inconsistent content in crowded scene images
of diverse objects. Previous works use preprocessing pipelines to localize
salient objects for improved cropping, but an end-to-end solution is still
elusive. In this work, we propose a framework which accomplishes this goal via
joint learning of representations and segmentation. We leverage segmentation
masks to train a model with a mask-dependent contrastive loss, and use the
partially trained model to bootstrap better masks. By iterating between these
two components, we ground the contrastive updates in segmentation information,
and simultaneously improve segmentation throughout pretraining. Experiments
show our representations transfer robustly to downstream tasks in
classification, detection and segmentation.",https://github.com/renwang435/CYBORGS,-1
6b272198-5c8d-4f2a-8adf-02ebc9685ea0,HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance,0.802133,"Marker-less monocular 3D human motion capture (MoCap) with scene interactions
is a challenging research topic relevant for extended reality, robotics and
virtual avatar generation. Due to the inherent depth ambiguity of monocular
settings, 3D motions captured with existing methods often contain severe
artefacts such as incorrect body-scene inter-penetrations, jitter and body
floating. To tackle these issues, we propose HULC, a new approach for 3D human
MoCap which is aware of the scene geometry. HULC estimates 3D poses and dense
body-environment surface contacts for improved 3D localisations, as well as the
absolute scale of the subject. Furthermore, we introduce a 3D pose trajectory
optimisation based on a novel pose manifold sampling that resolves erroneous
body-environment inter-penetrations. Although the proposed method requires less
structured inputs compared to existing scene-aware monocular MoCap algorithms,
it produces more physically-plausible poses: HULC significantly and
consistently outperforms the existing approaches in various experiments and on
different metrics. Project page: https://vcai.mpi-inf.mpg.de/projects/HULC/.",None,-1
03af8efb-b327-4be7-b4ab-db2fe6c2924f,Hyperspherical Consistency Regularization,0.868667,"Recent advances in contrastive learning have enlightened diverse applications
across various semi-supervised fields. Jointly training supervised learning and
unsupervised learning with a shared feature encoder becomes a common scheme.
Though it benefits from taking advantage of both feature-dependent information
from self-supervised learning and label-dependent information from supervised
learning, this scheme remains suffering from bias of the classifier. In this
work, we systematically explore the relationship between self-supervised
learning and supervised learning, and study how self-supervised learning helps
robust data-efficient deep learning. We propose hyperspherical consistency
regularization (HCR), a simple yet effective plug-and-play method, to
regularize the classifier using feature-dependent information and thus avoid
bias from labels. Specifically, HCR first projects logits from the classifier
and feature projections from the projection head on the respective hypersphere,
then it enforces data points on hyperspheres to have similar structures by
minimizing binary cross entropy of pairwise distances' similarity metrics.
Extensive experiments on semi-supervised and weakly-supervised learning
demonstrate the effectiveness of our method, by showing superior performance
with HCR.",None,70015
1a243fd7-8f3c-4514-8662-4916863079fc,Joint Learning-Based Stabilization of Multiple Unknown Linear Systems,0.256302,"Learning-based control of linear systems received a lot of attentions
recently. In popular settings, the true dynamical models are unknown to the
decision-maker and need to be interactively learned by applying control inputs
to the systems. Unlike the matured literature of efficient reinforcement
learning policies for adaptive control of a single system, results on joint
learning of multiple systems are not currently available. Especially, the
important problem of fast and reliable joint-stabilization remains unaddressed
and so is the focus of this work. We propose a novel joint learning-based
stabilization algorithm for quickly learning stabilizing policies for all
systems understudy, from the data of unstable state trajectories. The presented
procedure is shown to be notably effective such that it stabilizes the family
of dynamical systems in an extremely short time period.",None,-1
65be2c83-75e4-4811-abd7-afd53051b0ad,Graph Augmentation Learning,0.365285,"Graph Augmentation Learning (GAL) provides outstanding solutions for graph
learning in handling incomplete data, noise data, etc. Numerous GAL methods
have been proposed for graph-based applications such as social network analysis
and traffic flow forecasting. However, the underlying reasons for the
effectiveness of these GAL methods are still unclear. As a consequence, how to
choose optimal graph augmentation strategy for a certain application scenario
is still in black box. There is a lack of systematic, comprehensive, and
experimentally validated guideline of GAL for scholars. Therefore, in this
survey, we in-depth review GAL techniques from macro (graph), meso (subgraph),
and micro (node/edge) levels. We further detailedly illustrate how GAL enhance
the data quality and the model performance. The aggregation mechanism of
augmentation strategies and graph learning models are also discussed by
different application scenarios, i.e., data-specific, model-specific, and
hybrid scenarios. To better show the outperformance of GAL, we experimentally
validate the effectiveness and adaptability of different GAL strategies in
different downstream tasks. Finally, we share our insights on several open
issues of GAL, including heterogeneity, spatio-temporal dynamics, scalability,
and generalization.",https://github.com/yushuowiki/awesome-GAL,-1
0d1164af-4052-4c30-ba31-01bdabb053b7,Learning to Detect Mobile Objects from LiDAR Scans Without Labels,0.40574,"Current 3D object detectors for autonomous driving are almost entirely
trained on human-annotated data. Although of high quality, the generation of
such data is laborious and costly, restricting them to a few specific locations
and object types. This paper proposes an alternative approach entirely based on
unlabeled data, which can be collected cheaply and in abundance almost
everywhere on earth. Our approach leverages several simple common sense
heuristics to create an initial set of approximate seed labels. For example,
relevant traffic participants are generally not persistent across multiple
traversals of the same route, do not fly, and are never under ground. We
demonstrate that these seed labels are highly effective to bootstrap a
surprisingly accurate detector through repeated self-training without a single
human annotated label.",https://github.com/YurongYou/MODEST,46951
271b66e4-ac0d-44b6-8953-e51b14fbf9b6,Reasoning Like Program Executors,0.999728,"Reasoning over natural language is a long-standing goal for the research
community. However, studies have shown that existing language models are
inadequate in reasoning. To address the issue, we present POET, a novel
reasoning pre-training paradigm. Through pre-training language models with
programs and their execution results, POET empowers language models to harvest
the reasoning knowledge possessed by program executors via a data-driven
approach. POET is conceptually simple and can be instantiated by different
kinds of program executors. In this paper, we showcase two simple instances
POET-Math and POET-Logic, in addition to a complex instance, POET-SQL.
Experimental results on six benchmarks demonstrate that POET can significantly
boost model performance in natural language reasoning, such as numerical
reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on
reasoning-enhancement pre-training, and we hope our analysis would shed light
on the future research of reasoning like program executors.",https://github.com/microsoft/ContextualSP,-1
e4eacb7b-d272-47c4-b9db-67ed3f5ec883,EBOCA: Evidences for BiOmedical Concepts Association Ontology,0.476631,"There is a large number of online documents data sources available nowadays.
The lack of structure and the differences between formats are the main
difficulties to automatically extract information from them, which also has a
negative impact on its use and reuse. In the biomedical domain, the DISNET
platform emerged to provide researchers with a resource to obtain information
in the scope of human disease networks by means of large-scale heterogeneous
sources. Specifically in this domain, it is critical to offer not only the
information extracted from different sources, but also the evidence that
supports it. This paper proposes EBOCA, an ontology that describes (i)
biomedical domain concepts and associations between them, and (ii) evidences
supporting these associations; with the objective of providing an schema to
improve the publication and description of evidences and biomedical
associations in this domain. The ontology has been successfully evaluated to
ensure there are no errors, modelling pitfalls and that it meets the previously
defined functional requirements. Test data coming from a subset of DISNET and
automatic association extractions from texts has been transformed according to
the proposed ontology to create a Knowledge Graph that can be used in real
scenarios, and which has also been used for the evaluation of the presented
ontology.",None,-1
316cfc2e-59d3-4a88-aac1-1edad058c367,Unsupervised Opinion Summarisation in the Wasserstein Space,0.106993,"Opinion summarisation synthesises opinions expressed in a group of documents
discussing the same topic to produce a single summary. Recent work has looked
at opinion summarisation of clusters of social media posts. Such posts are
noisy and have unpredictable structure, posing additional challenges for the
construction of the summary distribution and the preservation of meaning
compared to online reviews, which has been so far the focus of opinion
summarisation. To address these challenges we present \textit{WassOS}, an
unsupervised abstractive summarization model which makes use of the Wasserstein
distance. A Variational Autoencoder is used to get the distribution of
documents/posts, and the distributions are disentangled into separate semantic
and syntactic spaces. The summary distribution is obtained using the
Wasserstein barycenter of the semantic and syntactic distributions. A latent
variable sampled from the summary distribution is fed into a GRU decoder with a
transformer layer to produce the final summary. Our experiments on multiple
datasets including Twitter clusters, Reddit threads, and reviews show that
WassOS almost always outperforms the state-of-the-art on ROUGE metrics and
consistently produces the best summaries with respect to meaning preservation
according to human evaluations.",https://github.com/Maria-Liakata-NLP-Group/,18450
452482bb-92b5-49d4-b5d9-37e8e9c1ba0f,Deep Learning based Automatic Detection of Dicentric Chromosome,0.547919,"Automatic detection of dicentric chromosomes is an essential step to estimate
radiation exposure and development of end to end emergency bio dosimetry
systems. During accidents, a large amount of data is required to be processed
for extensive testing to formulate a medical treatment plan for the masses,
which requires this process to be automated. Current approaches require human
adjustments according to the data and therefore need a human expert to
calibrate the system. This paper proposes a completely data driven framework
which requires minimum intervention of field experts and can be deployed in
emergency cases with relative ease. Our approach involves YOLOv4 to detect the
chromosomes and remove the debris in each image, followed by a classifier that
differentiates between an analysable chromosome and a non-analysable one.
Images are extracted from YOLOv4 based on the protocols described by
WHO-BIODOSNET. The analysable chromosome is classified as Monocentric or
Dicentric and an image is accepted for consideration of dose estimation based
on the analysable chromosome count. We report an accuracy in dicentric
identification of 94.33% on a 1:1 split of Dicentric and Monocentric
Chromosomes.",None,-1
a1d6f42e-8c30-41fb-90d9-868640c41ac9,Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning,0.284067,"The rapidly-developing intersection of machine learning (ML) with high-energy
physics (HEP) presents both opportunities and challenges to our community. Far
beyond applications of standard ML tools to HEP problems, genuinely new and
potentially revolutionary approaches are being developed by a generation of
talent literate in both fields. There is an urgent need to support the needs of
the interdisciplinary community driving these developments, including funding
dedicated research at the intersection of the two fields, investing in
high-performance computing at universities and tailoring allocation policies to
support this work, developing of community tools and standards, and providing
education and career paths for young researchers attracted by the intellectual
vitality of machine learning for high energy physics.",None,-1
f90b3717-3654-4bfc-910a-7363b8e98200,Cross-Modal ASR Post-Processing System for Error Correction and Utterance Rejection,0.420659,"Although modern automatic speech recognition (ASR) systems can achieve high
performance, they may produce errors that weaken readers' experience and do
harm to downstream tasks. To improve the accuracy and reliability of ASR
hypotheses, we propose a cross-modal post-processing system for speech
recognizers, which 1) fuses acoustic features and textual features from
different modalities, 2) joints a confidence estimator and an error corrector
in multi-task learning fashion and 3) unifies error correction and utterance
rejection modules. Compared with single-modal or single-task models, our
proposed system is proved to be more effective and efficient. Experiment result
shows that our post-processing system leads to more than 10% relative reduction
of character error rate (CER) for both single-speaker and multi-speaker speech
on our industrial ASR system, with about 1.7ms latency for each token, which
ensures that extra latency introduced by post-processing is acceptable in
streaming speech recognition.",None,-1
b8310d04-9e71-4ff5-ab21-2caaba1a6d62,REPAIR: REnormalizing Permuted Activations for Interpolation Repair,0.985914,"In this paper we look into the conjecture of Entezari et al. (2021) which
states that if the permutation invariance of neural networks is taken into
account, then there is likely no loss barrier to the linear interpolation
between SGD solutions. First, we observe that neuron alignment methods alone
are insufficient to establish low-barrier linear connectivity between SGD
solutions due to a phenomenon we call variance collapse: interpolated deep
networks suffer a collapse in the variance of their activations, causing poor
performance. Next, we propose REPAIR (REnormalizing Permuted Activations for
Interpolation Repair) which mitigates variance collapse by rescaling the
preactivations of such interpolated networks. We explore the interaction
between our method and the choice of normalization layer, network width, and
depth, and demonstrate that using REPAIR on top of neuron alignment methods
leads to 60%-100% relative barrier reduction across a wide variety of
architecture families and tasks. In particular, we report a 74% barrier
reduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on
CIFAR10.",https://github.com/KellerJordan/REPAIR,-1
b9502d7e-daed-4b38-9972-09d7035dfc2e,Consistency Learning via Decoding Path Augmentation for Transformers in Human Object Interaction Detection,0.643643,"Human-Object Interaction detection is a holistic visual recognition task that
entails object detection as well as interaction classification. Previous works
of HOI detection has been addressed by the various compositions of subset
predictions, e.g., Image -> HO -> I, Image -> HI -> O. Recently, transformer
based architecture for HOI has emerged, which directly predicts the HOI
triplets in an end-to-end fashion (Image -> HOI). Motivated by various
inference paths for HOI detection, we propose cross-path consistency learning
(CPC), which is a novel end-to-end learning strategy to improve HOI detection
for transformers by leveraging augmented decoding paths. CPC learning enforces
all the possible predictions from permuted inference sequences to be
consistent. This simple scheme makes the model learn consistent
representations, thereby improving generalization without increasing model
capacity. Our experiments demonstrate the effectiveness of our method, and we
achieved significant improvement on V-COCO and HICO-DET compared to the
baseline models. Our code is available at https://github.com/mlvlab/CPChoi.",https://github.com/mlvlab/CPChoi,-1
b79d594b-5a97-4896-b812-a359a4e6f701,Continual Learning by Modeling Intra-Class Variation,0.195651,"It has been observed that neural networks perform poorly when the data or
tasks are presented sequentially. Unlike humans, neural networks suffer greatly
from catastrophic forgetting, making it impossible to perform life-long
learning. To address this issue, memory-based continual learning has been
actively studied and stands out as one of the best-performing methods. We
examine memory-based continual learning and identify that large variation in
the representation space is crucial for avoiding catastrophic forgetting.
Motivated by this, we propose to diversify representations by using two types
of perturbations: model-agnostic variation (i.e., the variation is generated
without the knowledge of the learned neural network) and model-based variation
(i.e., the variation is conditioned on the learned neural network). We
demonstrate that enlarging representational variation serves as a general
principle to improve continual learning. Finally, we perform empirical studies
which demonstrate that our method, as a simple plug-and-play component, can
consistently improve a number of memory-based continual learning methods by a
large margin.",https://github.com/yulonghui/MOCA,-1
ba0d61da-2aa0-46df-ac41-1d98acbfff58,Are disentangled representations all you need to build speaker anonymization systems?,0.615406,"Speech signals contain a lot of sensitive information, such as the speaker's
identity, which raises privacy concerns when speech data get collected. Speaker
anonymization aims to transform a speech signal to remove the source speaker's
identity while leaving the spoken content unchanged. Current methods perform
the transformation by relying on content/speaker disentanglement and voice
conversion. Usually, an acoustic model from an automatic speech recognition
system extracts the content representation while an x-vector system extracts
the speaker representation. Prior work has shown that the extracted features
are not perfectly disentangled. This paper tackles how to improve features
disentanglement, and thus the converted anonymized speech. We propose enhancing
the disentanglement by removing speaker information from the acoustic model
using vector quantization. Evaluation done using the VoicePrivacy 2022 toolkit
showed that vector quantization helps conceal the original speaker identity
while maintaining utility for speech recognition.",https://colab.research.google.com/github/deep-privacy/SA-toolkit/blob/master/SA-colab.ipynb,-1
538c9be1-3eb8-4aaf-b630-1eb05185eb97,Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation,0.532949,"Generating adversarial examples for Neural Machine Translation (NMT) with
single Round-Trip Translation (RTT) has achieved promising results by releasing
the meaning-preserving restriction. However, a potential pitfall for this
approach is that we cannot decide whether the generated examples are
adversarial to the target NMT model or the auxiliary backward one, as the
reconstruction error through the RTT can be related to either. To remedy this
problem, we propose a new criterion for NMT adversarial examples based on the
Doubly Round-Trip Translation (DRTT). Specifically, apart from the
source-target-source RTT, we also consider the target-source-target one, which
is utilized to pick out the authentic adversarial examples for the target NMT
model. Additionally, to enhance the robustness of the NMT model, we introduce
the masked language models to construct bilingual adversarial pairs based on
DRTT, which are used to train the NMT model directly. Extensive experiments on
both the clean and noisy test sets (including the artificial and natural noise)
show that our approach substantially improves the robustness of NMT models.",https://github.com/lisasiyu/DRTT,-1
872fbeef-cfb5-4ce3-8691-932fb05b1492,Frequency-Aware Self-Supervised Monocular Depth Estimation,0.515564,"We present two versatile methods to generally enhance self-supervised
monocular depth estimation (MDE) models. The high generalizability of our
methods is achieved by solving the fundamental and ubiquitous problems in
photometric loss function. In particular, from the perspective of spatial
frequency, we first propose Ambiguity-Masking to suppress the incorrect
supervision under photometric loss at specific object boundaries, the cause of
which could be traced to pixel-level ambiguity. Second, we present a novel
frequency-adaptive Gaussian low-pass filter, designed to robustify the
photometric loss in high-frequency regions. We are the first to propose
blurring images to improve depth estimators with an interpretable analysis.
Both modules are lightweight, adding no parameters and no need to manually
change the network structures. Experiments show that our methods provide
performance boosts to a large number of existing models, including those who
claimed state-of-the-art, while introducing no extra inference computation at
all.",https://github.com/xingyuuchen/freq-aware-depth,-1
83650ea2-1f0c-4dc0-a3c2-2ef20db349d2,Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives,0.953495,"This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a
new member in the family of InfoNCE losses that preserves a ranked ordering of
positive samples. In contrast to the standard InfoNCE loss, which requires a
strict binary separation of the training pairs into similar and dissimilar
samples, RINCE can exploit information about a similarity ranking for learning
a corresponding embedding space. We show that the proposed loss function learns
favorable embeddings compared to the standard InfoNCE whenever at least noisy
ranking information can be obtained or when the definition of positives and
negatives is blurry. We demonstrate this for a supervised classification task
with additional superclass labels and noisy similarity scores. Furthermore, we
show that RINCE can also be applied to unsupervised training with experiments
on unsupervised representation learning from videos. In particular, the
embedding yields higher classification accuracy, retrieval rates and performs
better in out-of-distribution detection than the standard InfoNCE loss.",https://github.com/boschresearch/rince,-1
2af058b2-3999-40b6-a302-9f00c75d64ca,PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate Relation Selection and Entity Boundary Detection,0.864665,"Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation
triplets from unstructured texts under the zero-shot setting, where the
relation sets at the training and testing stages are disjoint. Previous
state-of-the-art method handles this challenging task by leveraging pretrained
language models to generate data as additional training samples, which
increases the training cost and severely constrains the model performance. To
address the above issues, we propose a novel method named PCRED for ZeroRTE
with Potential Candidate Relation Selection and Entity Boundary Detection. The
remarkable characteristic of PCRED is that it does not rely on additional data
and still achieves promising performance. The model adopts a relation-first
paradigm, recognizing unseen relations through candidate relation selection.
With this approach, the semantics of relations are naturally infused in the
context. Entities are extracted based on the context and the semantics of
relations subsequently. We evaluate our model on two ZeroRTE datasets. The
experiment results show that our method consistently outperforms previous
works. Our code will be available at https://anonymous.4open.science/r/PCRED.",https://anonymous.4open.science/r/PCRED,-1
89556dc7-bca8-448d-a0d3-bf65aa2b0b8f,Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds,0.935195,"3D single object tracking (3D SOT) in LiDAR point clouds plays a crucial role
in autonomous driving. Current approaches all follow the Siamese paradigm based
on appearance matching. However, LiDAR point clouds are usually textureless and
incomplete, which hinders effective appearance matching. Besides, previous
methods greatly overlook the critical motion clues among targets. In this work,
beyond 3D Siamese tracking, we introduce a motion-centric paradigm to handle 3D
SOT from a new perspective. Following this paradigm, we propose a matching-free
two-stage tracker M^2-Track. At the 1^st-stage, M^2-Track localizes the target
within successive frames via motion transformation. Then it refines the target
box through motion-assisted shape completion at the 2^nd-stage. Extensive
experiments confirm that M^2-Track significantly outperforms previous
state-of-the-arts on three large-scale datasets while running at 57FPS (~8%,
~17%, and ~22%) precision gains on KITTI, NuScenes, and Waymo Open Dataset
respectively). Further analysis verifies each component's effectiveness and
shows the motion-centric paradigm's promising potential when combined with
appearance matching.",https://github.com/Ghostish/Open3DSOT,-1
0cbaf748-c5b6-4106-86c1-7f76665b4241,Detecting Driver Drowsiness as an Anomaly Using LSTM Autoencoders,0.54876,"In this paper, an LSTM autoencoder-based architecture is utilized for
drowsiness detection with ResNet-34 as feature extractor. The problem is
considered as anomaly detection for a single subject; therefore, only the
normal driving representations are learned and it is expected that drowsiness
representations, yielding higher reconstruction losses, are to be distinguished
according to the knowledge of the network. In our study, the confidence levels
of normal and anomaly clips are investigated through the methodology of label
assignment such that training performance of LSTM autoencoder and
interpretation of anomalies encountered during testing are analyzed under
varying confidence rates. Our method is experimented on NTHU-DDD and
benchmarked with a state-of-the-art anomaly detection method for driver
drowsiness. Results show that the proposed model achieves detection rate of
0.8740 area under curve (AUC) and is able to provide significant improvements
on certain scenarios.",None,2076
ef5d304a-42a7-435e-83fb-5555327f12b7,Structured Local Radiance Fields for Human Avatar Modeling,0.693802,"It is extremely challenging to create an animatable clothed human avatar from
RGB videos, especially for loose clothes due to the difficulties in motion
modeling. To address this problem, we introduce a novel representation on the
basis of recent neural scene rendering techniques. The core of our
representation is a set of structured local radiance fields, which are anchored
to the pre-defined nodes sampled on a statistical human body template. These
local radiance fields not only leverage the flexibility of implicit
representation in shape and appearance modeling, but also factorize cloth
deformations into skeleton motions, node residual translations and the dynamic
detail variations inside each individual radiance field. To learn our
representation from RGB data and facilitate pose generalization, we propose to
learn the node translations and the detail variations in a conditional
generative latent space. Overall, our method enables automatic construction of
animatable human avatars for various types of clothes without the need for
scanning subject-specific templates, and can generate realistic images with
dynamic details for novel poses. Experiment show that our method outperforms
state-of-the-art methods both qualitatively and quantitatively.",https://github.com/zju3dv/EasyMocap,-1
d334d860-54c3-48a4-a2ad-3d11118c06cb,Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines,0.615549,"Strong inductive biases give humans the ability to quickly learn to perform a
variety of tasks. Although meta-learning is a method to endow neural networks
with useful inductive biases, agents trained by meta-learning may sometimes
acquire very different strategies from humans. We show that co-training these
agents on predicting representations from natural language task descriptions
and programs induced to generate such tasks guides them toward more human-like
inductive biases. Human-generated language descriptions and program induction
models that add new learned primitives both contain abstract concepts that can
compress description length. Co-training on these representations result in
more human-like behavior in downstream meta-reinforcement learning agents than
less abstract controls (synthetic language descriptions, program induction
without learned primitives), suggesting that the abstraction supported by these
representations is key.",None,-1
2c574df1-2965-4f30-b564-1604dd3aec46,HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator,0.819026,"Video prediction is an important yet challenging problem; burdened with the
tasks of generating future frames and learning environment dynamics. Recently,
autoregressive latent video models have proved to be a powerful video
prediction tool, by separating the video prediction into two sub-problems:
pre-training an image generator model, followed by learning an autoregressive
prediction model in the latent space of the image generator. However,
successfully generating high-fidelity and high-resolution videos has yet to be
seen. In this work, we investigate how to train an autoregressive latent video
prediction model capable of predicting high-fidelity future frames with minimal
modification to existing models, and produce high-resolution (256x256) videos.
Specifically, we scale up prior models by employing a high-fidelity image
generator (VQ-GAN) with a causal transformer model, and introduce additional
techniques of top-k sampling and data augmentation to further improve video
prediction quality. Despite the simplicity, the proposed method achieves
competitive performance to state-of-the-art approaches on standard video
prediction benchmarks with fewer parameters, and enables high-resolution video
prediction on complex and large-scale datasets. Videos are available at
https://sites.google.com/view/harp-videos/home.",None,-1
0887b990-bbd7-47a8-b58e-31bec1a11747,"From Perception to Programs: Regularize, Overparameterize, and Amortize",0.558913,"Toward combining inductive reasoning with perception abilities, we develop
techniques for neurosymbolic program synthesis where perceptual input is first
parsed by neural nets into a low-dimensional interpretable representation,
which is then processed by a synthesized program. We explore several techniques
for relaxing the problem and jointly learning all modules end-to-end with
gradient descent: multitask learning; amortized inference;
overparameterization; and a differentiable strategy for penalizing lengthy
programs. Collectedly this toolbox improves the stability of gradient-guided
program search, and suggests ways of learning both how to perceive input as
discrete abstractions, and how to symbolically process those abstractions as
programs.",None,1751
8effacf2-5da9-4368-8a3e-38202c993443,SaiT: Sparse Vision Transformers through Adaptive Token Pruning,0.145431,"While vision transformers have achieved impressive results, effectively and
efficiently accelerating these models can further boost performances. In this
work, we propose a dense/sparse training framework to obtain a unified model,
enabling weight sharing across various token densities. Thus one model offers a
range of accuracy and throughput tradeoffs for different applications. Besides,
we introduce adaptive token pruning to optimize the patch token sparsity based
on the input image. In addition, we investigate knowledge distillation to
enhance token selection capability in early transformer modules. Sparse
adaptive image Transformer (SaiT) offers varying levels of model acceleration
by merely changing the token sparsity on the fly. Specifically, SaiT reduces
the computation complexity (FLOPs) by 39% - 43% and increases the throughput by
67% - 91% with less than 0.5% accuracy loss for various vision transformer
models. Meanwhile, the same model also provides the zero accuracy drop option
by skipping the sparsification step. SaiT achieves better accuracy and
computation tradeoffs than state-of-the-art transformer and convolutional
models.",https://github.com/rwightman/pytorch-image-models,-1
dfd8496d-3783-4d0b-88a7-bd8e04264581,Towards Developing Safety Assurance Cases for Learning-Enabled Medical Cyber-Physical Systems,0.139441,"Machine Learning (ML) technologies have been increasingly adopted in Medical
Cyber-Physical Systems (MCPS) to enable smart healthcare. Assuring the safety
and effectiveness of learning-enabled MCPS is challenging, as such systems must
account for diverse patient profiles and physiological dynamics and handle
operational uncertainties. In this paper, we develop a safety assurance case
for ML controllers in learning-enabled MCPS, with an emphasis on establishing
confidence in the ML-based predictions. We present the safety assurance case in
detail for Artificial Pancreas Systems (APS) as a representative application of
learning-enabled MCPS, and provide a detailed analysis by implementing a deep
neural network for the prediction in APS. We check the sufficiency of the ML
data and analyze the correctness of the ML-based prediction using formal
verification. Finally, we outline open research problems based on our
experience in this paper.",https://github.com/jxx123/simglucose,-1
c7535a15-0ded-4d10-8b89-eaee44a5858b,Spiking Neural Networks for Frame-based and Event-based Single Object Localization,0.853411,"Spiking neural networks have shown much promise as an energy-efficient
alternative to artificial neural networks. However, understanding the impacts
of sensor noises and input encodings on the network activity and performance
remains difficult with common neuromorphic vision baselines like
classification. Therefore, we propose a spiking neural network approach for
single object localization trained using surrogate gradient descent, for frame-
and event-based sensors. We compare our method with similar artificial neural
networks and show that our model has competitive/better performance in
accuracy, robustness against various corruptions, and has lower energy
consumption. Moreover, we study the impact of neural coding schemes for static
images in accuracy, robustness, and energy efficiency. Our observations differ
importantly from previous studies on bio-plausible learning rules, which helps
in the design of surrogate gradient trained architectures, and offers insight
to design priorities in future neuromorphic technologies in terms of noise
characteristics and data encoding methods.",https://github.com/hendrycks/robustness,-1
edab5509-eb91-492a-bf0e-73c5752baa15,Robust $Q$-learning Algorithm for Markov Decision Processes under Wasserstein Uncertainty,0.885303,"We present a novel $Q$-learning algorithm to solve distributionally robust
Markov decision problems, where the corresponding ambiguity set of transition
probabilities for the underlying Markov decision process is a Wasserstein ball
around a (possibly estimated) reference measure. We prove convergence of the
presented algorithm and provide several examples also using real data to
illustrate both the tractability of our algorithm as well as the benefits of
considering distributional robustness when solving stochastic optimal control
problems, in particular when the estimated distributions turn out to be
misspecified in practice.",https://github.com/juliansester/Wasserstein-Q-learning,-1
a7410371-6c2b-4fc4-918e-02f51f7b8a09,Concept Graph Neural Networks for Surgical Video Understanding,0.585931,"We constantly integrate our knowledge and understanding of the world to
enhance our interpretation of what we see.
  This ability is crucial in application domains which entail reasoning about
multiple entities and concepts, such as AI-augmented surgery. In this paper, we
propose a novel way of integrating conceptual knowledge into temporal analysis
tasks via temporal concept graph networks. In the proposed networks, a global
knowledge graph is incorporated into the temporal analysis of surgical
instances, learning the meaning of concepts and relations as they apply to the
data. We demonstrate our results in surgical video data for tasks such as
verification of critical view of safety, as well as estimation of Parkland
grading scale. The results show that our method improves the recognition and
detection of complex benchmarks as well as enables other analytic applications
of interest.",https://github.com/CAMMA-public/ivtmetrics,-1
956db933-53d6-4afd-8d81-8106b360c28f,DEMETR: Diagnosing Evaluation Metrics for Translation,0.856547,"While machine translation evaluation metrics based on string overlap (e.g.,
BLEU) have their limitations, their computations are transparent: the BLEU
score assigned to a particular candidate translation can be traced back to the
presence or absence of certain words. The operations of newer learned metrics
(e.g., BLEURT, COMET), which leverage pretrained language models to achieve
higher correlations with human quality judgments than BLEU, are opaque in
comparison. In this paper, we shed light on the behavior of these learned
metrics by creating DEMETR, a diagnostic dataset with 31K English examples
(translated from 10 source languages) for evaluating the sensitivity of MT
evaluation metrics to 35 different linguistic perturbations spanning semantic,
syntactic, and morphological error categories. All perturbations were carefully
designed to form minimal pairs with the actual translation (i.e., differ in
only one aspect). We find that learned metrics perform substantially better
than string-based metrics on DEMETR. Additionally, learned metrics differ in
their sensitivity to various phenomena (e.g., BERTScore is sensitive to
untranslated words but relatively insensitive to gender manipulation, while
COMET is much more sensitive to word repetition than to aspectual changes). We
publicly release DEMETR to spur more informed future development of machine
translation evaluation metrics",https://github.com/marzenakrp/demetr,-1
59ddfbae-f7d6-4dcb-b6ea-7537f728a153,Narrowing the Gap: Improved Detector Training with Noisy Location Annotations,0.627523,"Deep learning methods require massive of annotated data for optimizing
parameters. For example, datasets attached with accurate bounding box
annotations are essential for modern object detection tasks. However, labeling
with such pixel-wise accuracy is laborious and time-consuming, and elaborate
labeling procedures are indispensable for reducing man-made noise, involving
annotation review and acceptance testing. In this paper, we focus on the impact
of noisy location annotations on the performance of object detection approaches
and aim to, on the user side, reduce the adverse effect of the noise. First,
noticeable performance degradation is experimentally observed for both
one-stage and two-stage detectors when noise is introduced to the bounding box
annotations. For instance, our synthesized noise results in performance
decrease from 38.9% AP to 33.6% AP for FCOS detector on COCO test split, and
37.8%AP to 33.7%AP for Faster R-CNN. Second, a self-correction technique based
on a Bayesian filter for prediction ensemble is proposed to better exploit the
noisy location annotations following a Teacher-Student learning paradigm.
Experiments for both synthesized and real-world scenarios consistently
demonstrate the effectiveness of our approach, e.g., our method increases the
degraded performance of the FCOS detector from 33.6% AP to 35.6% AP on COCO.",https://github.com/wangsr126/NDet,-1
1d756b38-b6b3-4f9e-8ab6-fe8a257e8aca,DiGamma: Domain-aware Genetic Algorithm for HW-Mapping Co-optimization for DNN Accelerators,0.887689,"The design of DNN accelerators includes two key parts: HW resource
configuration and mapping strategy. Intensive research has been conducted to
optimize each of them independently. Unfortunately, optimizing for both
together is extremely challenging due to the extremely large cross-coupled
search space. To address this, in this paper, we propose a HW-Mapping
co-optimization framework, an efficient encoding of the immense design space
constructed by HW and Mapping, and a domain-aware genetic algorithm, named
DiGamma, with specialized operators for improving search efficiency. We
evaluate DiGamma with seven popular DNNs models with different properties. Our
evaluations show DiGamma can achieve (geomean) 3.0x and 10.0x speedup,
comparing to the best-performing baseline optimization algorithms, in edge and
cloud settings.",https://github.com/maestro-project/digamma,16433
bbdc1db6-2678-44c7-800f-0d4a409106fb,Frustratingly Easy Label Projection for Cross-lingual Transfer,0.491652,"Translating training data into many languages has emerged as a practical
solution for improving cross-lingual transfer. For tasks that involve
span-level annotations, such as information extraction or question answering,
an additional label projection step is required to map annotated spans onto the
translated texts. Recently, a few efforts have utilized a simple
mark-then-translate method to jointly perform translation and projection by
inserting special markers around the labeled spans in the original sentence.
However, as far as we are aware, no empirical analysis has been conducted on
how this approach compares to traditional annotation projection based on word
alignment. In this paper, we present an extensive empirical study across 57
languages and three tasks (QA, NER, and Event Extraction) to evaluate the
effectiveness and limitations of both methods, filling an important gap in the
literature. Experimental results show that our optimized version of
mark-then-translate, which we call EasyProject, is easily applied to many
languages and works surprisingly well, outperforming the more complex word
alignment-based methods. We analyze several key factors that affect the
end-task performance, and show EasyProject works well because it can accurately
preserve label span boundaries after translation. We will publicly release all
our code and data.",https://github.com/edchengg/easyproject,-1
4c1c2465-eedb-44a0-b437-0f5490b6735b,Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning,0.720484,"Recent research shows synthetic data as a source of supervision helps
pretrained language models (PLM) transfer learning to new target tasks/domains.
However, this idea is less explored for spatial language. We provide two new
data resources on multiple spatial language processing tasks. The first dataset
is synthesized for transfer learning on spatial question answering (SQA) and
spatial role labeling (SpRL). Compared to previous SQA datasets, we include a
larger variety of spatial relation types and spatial expressions. Our data
generation process is easily extendable with new spatial expression lexicons.
The second one is a real-world SQA dataset with human-generated questions built
on an existing corpus with SPRL annotations. This dataset can be used to
evaluate spatial language processing models in realistic situations. We show
pretraining with automatically generated data significantly improves the SOTA
results on several SQA and SPRL benchmarks, particularly when the training data
in the target domain is small.",https://github.com/HLR/SpaRTUN,-1
85eef442-94dd-4418-8bde-1d7b52ee173d,CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities,0.473595,"An increasing number of monitoring systems have been developed in smart
cities to ensure that real-time operations of a city satisfy safety and
performance requirements. However, many existing city requirements are written
in English with missing, inaccurate, or ambiguous information. There is a high
demand for assisting city policy makers in converting human-specified
requirements to machine-understandable formal specifications for monitoring
systems. To tackle this limitation, we build CitySpec, the first intelligent
assistant system for requirement specification in smart cities. To create
CitySpec, we first collect over 1,500 real-world city requirements across
different domains from over 100 cities and extract city-specific knowledge to
generate a dataset of city vocabulary with 3,061 words. We also build a
translation model and enhance it through requirement synthesis and develop a
novel online learning framework with validation under uncertainty. The
evaluation results on real-world city requirements show that CitySpec increases
the sentence-level accuracy of requirement specification from 59.02% to 86.64%,
and has strong adaptability to a new city and a new domain (e.g., F1 score for
requirements in Seattle increases from 77.6% to 93.75% with online learning).",None,-1
411cb6d7-aea1-4d56-81cf-52ccf8c08316,Semi-supervised Object Detection via Virtual Category Learning,0.317087,"Due to the costliness of labelled data in real-world applications,
semi-supervised object detectors, underpinned by pseudo labelling, are
appealing. However, handling confusing samples is nontrivial: discarding
valuable confusing samples would compromise the model generalisation while
using them for training would exacerbate the confirmation bias issue caused by
inevitable mislabelling. To solve this problem, this paper proposes to use
confusing samples proactively without label correction. Specifically, a virtual
category (VC) is assigned to each confusing sample such that they can safely
contribute to the model optimisation even without a concrete label. It is
attributed to specifying the embedding distance between the training sample and
the virtual category as the lower bound of the inter-class distance. Moreover,
we also modify the localisation loss to allow high-quality boundaries for
location regression. Extensive experiments demonstrate that the proposed VC
learning significantly surpasses the state-of-the-art, especially with small
amounts of available labels.",https://github.com/GeoffreyChen777/VC,-1
a82b58e7-5b20-4ed0-aa02-bfb36c2ab658,CaMEL: Case Marker Extraction without Labels,0.339922,"We introduce CaMEL (Case Marker Extraction without Labels), a novel and
challenging task in computational morphology that is especially relevant for
low-resource languages. We propose a first model for CaMEL that uses a
massively multilingual corpus to extract case markers in 83 languages based
only on a noun phrase chunker and an alignment system. To evaluate CaMEL, we
automatically construct a silver standard from UniMorph. The case markers
extracted by our model can be used to detect and visualise similarities and
differences between the case systems of different languages as well as to
annotate fine-grained deep cases in languages in which they are not overtly
marked.",https://github.com/LeonieWeissweiler/CaMEL,-1
0c0ee7f5-64c8-45c4-9da4-69894eb1d3e5,Does Simultaneous Speech Translation need Simultaneous Models?,0.793159,"In simultaneous speech translation (SimulST), finding the best trade-off
between high translation quality and low latency is a challenging task. To meet
the latency constraints posed by the different application scenarios, multiple
dedicated SimulST models are usually trained and maintained, generating high
computational costs. In this paper, motivated by the increased social and
environmental impact caused by these costs, we investigate whether a single
model trained offline can serve not only the offline but also the simultaneous
task without the need for any additional training or adaptation. Experiments on
en->{de, es} indicate that, aside from facilitating the adoption of
well-established offline techniques and architectures without affecting
latency, the offline solution achieves similar or better translation quality
compared to the same model trained in simultaneous settings, as well as being
competitive with the SimulST state of the art.",https://github.com/hlt-mt/,9038
40a48daa-b978-462b-9ec2-33becf64a257,StructNeRF: Neural Radiance Fields for Indoor Scenes with Structural Hints,0.395549,"Neural Radiance Fields (NeRF) achieve photo-realistic view synthesis with
densely captured input images. However, the geometry of NeRF is extremely
under-constrained given sparse views, resulting in significant degradation of
novel view synthesis quality. Inspired by self-supervised depth estimation
methods, we propose StructNeRF, a solution to novel view synthesis for indoor
scenes with sparse inputs. StructNeRF leverages the structural hints naturally
embedded in multi-view inputs to handle the unconstrained geometry issue in
NeRF. Specifically, it tackles the texture and non-texture regions
respectively: a patch-based multi-view consistent photometric loss is proposed
to constrain the geometry of textured regions; for non-textured ones, we
explicitly restrict them to be 3D consistent planes. Through the dense
self-supervised depth constraints, our method improves both the geometry and
the view synthesis performance of NeRF without any additional training on
external data. Extensive experiments on several real-world datasets demonstrate
that StructNeRF surpasses state-of-the-art methods for indoor scenes with
sparse inputs both quantitatively and qualitatively.",https://github.com/barbararoessle/dense-depth-priors-nerf,-1
cc272ca0-c47a-4e6f-a926-91d72dc78fb1,LingYi: Medical Conversational Question Answering System based on Multi-modal Knowledge Graphs,0.741724,"The medical conversational system can relieve the burden of doctors and
improve the efficiency of healthcare, especially during the pandemic. This
paper presents a medical conversational question answering (CQA) system based
on the multi-modal knowledge graph, namely ""LingYi"", which is designed as a
pipeline framework to maintain high flexibility. Our system utilizes automated
medical procedures including medical triage, consultation, image-text drug
recommendation and record. To conduct knowledge-grounded dialogues with
patients, we first construct a Chinese Medical Multi-Modal Knowledge Graph
(CM3KG) and collect a large-scale Chinese Medical CQA (CMCQA) dataset. Compared
with the other existing medical question-answering systems, our system adopts
several state-of-the-art technologies including medical entity disambiguation
and medical dialogue generation, which is more friendly to provide medical
services to patients. In addition, we have open-sourced our codes which contain
back-end models and front-end web pages at https://github.com/WENGSYX/LingYi.
The datasets including CM3KG at https://github.com/WENGSYX/CM3KG and CMCQA at
https://github.com/WENGSYX/CMCQA are also released to further promote future
research.",https://github.com/WENGSYX/LingYi,-1
a5ea9f8c-dba5-463b-bc43-63e8a5d91ef6,Learning Across Domains and Devices: Style-Driven Source-Free Domain Adaptation in Clustered Federated Learning,0.649403,"Federated Learning (FL) has recently emerged as a possible way to tackle the
domain shift in real-world Semantic Segmentation (SS) without compromising the
private nature of the collected data. However, most of the existing works on FL
unrealistically assume labeled data in the remote clients. Here we propose a
novel task (FFREEDA) in which the clients' data is unlabeled and the server
accesses a source labeled dataset for pre-training only. To solve FFREEDA, we
propose LADD, which leverages the knowledge of the pre-trained model by
employing self-supervision with ad-hoc regularization techniques for local
training and introducing a novel federated clustered aggregation scheme based
on the clients' style. Our experiments show that our algorithm is able to
efficiently tackle the new task outperforming existing approaches. The code is
available at https://github.com/Erosinho13/LADD.",https://github.com/Erosinho13/LADD,-1
16b70efc-3d52-4eb2-b5c4-1327c5f4490c,Multiple Instance Neuroimage Transformer,0.757316,"For the first time, we propose using a multiple instance learning based
convolution-free transformer model, called Multiple Instance Neuroimage
Transformer (MINiT), for the classification of T1weighted (T1w) MRIs. We first
present several variants of transformer models adopted for neuroimages. These
models extract non-overlapping 3D blocks from the input volume and perform
multi-headed self-attention on a sequence of their linear projections. MINiT,
on the other hand, treats each of the non-overlapping 3D blocks of the input
MRI as its own instance, splitting it further into non-overlapping 3D patches,
on which multi-headed self-attention is computed. As a proof-of-concept, we
evaluate the efficacy of our model by training it to identify sex from T1w-MRIs
of two public datasets: Adolescent Brain Cognitive Development (ABCD) and the
National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA).
The learned attention maps highlight voxels contributing to identifying sex
differences in brain morphometry. The code is available at
https://github.com/singlaayush/MINIT.",None,14913
20c901c6-649b-4ea1-89e6-1e07a2efbe2d,A Reinforcement Learning Approach for Electric Vehicle Routing Problem with Vehicle-to-Grid Supply,0.346204,"The use of electric vehicles (EV) in the last mile is appealing from both
sustainability and operational cost perspectives. In addition to the inherent
cost efficiency of EVs, selling energy back to the grid during peak grid
demand, is a potential source of additional revenue to a fleet operator. To
achieve this, EVs have to be at specific locations (discharge points) during
specific points in time (peak period), even while meeting their core purpose of
delivering goods to customers. In this work, we consider the problem of EV
routing with constraints on loading capacity; time window; vehicle-to-grid
energy supply (CEVRPTW-D); which not only satisfy multiple system objectives,
but also scale efficiently to large problem sizes involving hundreds of
customers and discharge stations. We present QuikRouteFinder that uses
reinforcement learning (RL) for EV routing to overcome these challenges. Using
Solomon datasets, results from RL are compared against exact formulations based
on mixed-integer linear program (MILP) and genetic algorithm (GA)
metaheuristics. On an average, the results show that RL is 24 times faster than
MILP and GA, while being close in quality (within 20%) to the optimal.",None,1244
9e754328-045c-4d02-947f-5fc39b63209a,HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images,0.35174,"In this paper, we present the results of the HSE-NN team in the 4th
competition on Affective Behavior Analysis in-the-wild (ABAW). The novel
multi-task EfficientNet model is trained for simultaneous recognition of facial
expressions and prediction of valence and arousal on static photos. The
resulting MT-EmotiEffNet extracts visual features that are fed into simple
feed-forward neural networks in the multi-task learning challenge. We obtain
performance measure 1.3 on the validation set, which is significantly greater
when compared to either performance of baseline (0.3) or existing models that
are trained only on the s-Aff-Wild2 database. In the learning from synthetic
data challenge, the quality of the original synthetic training set is increased
by using the super-resolution techniques, such as Real-ESRGAN. Next, the
MT-EmotiEffNet is fine-tuned on the new training set. The final prediction is a
simple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our
average validation F1 score is 18% greater than the baseline convolutional
neural network.",https://github.com/HSE-asavchenko/face-emotion-recognition/blob/main/src/ABAW,-1
b2323061-eb7d-478d-8189-1c6e93b7f2db,Identifying Rhythmic Patterns for Face Forgery Detection and Categorization,0.156258,"With the emergence of GAN, face forgery technologies have been heavily
abused. Achieving accurate face forgery detection is imminent. Inspired by
remote photoplethysmography (rPPG) that PPG signal corresponds to the periodic
change of skin color caused by heartbeat in face videos, we observe that
despite the inevitable loss of PPG signal during the forgery process, there is
still a mixture of PPG signals in the forgery video with a unique rhythmic
pattern depending on its generation method. Motivated by this key observation,
we propose a framework for face forgery detection and categorization consisting
of: 1) a Spatial-Temporal Filtering Network (STFNet) for PPG signals filtering,
and 2) a Spatial-Temporal Interaction Network (STINet) for constraint and
interaction of PPG signals. Moreover, with insight into the generation of
forgery methods, we further propose intra-source and inter-source blending to
boost the performance of the framework. Overall, extensive experiments have
proved the superiority of our method.",https://github.com/deepfakes/faceswap,-1
6a810633-c255-4042-8bbb-1f56e53ab3b7,ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics,0.605246,"As machine translation (MT) metrics improve their correlation with human
judgement every year, it is crucial to understand the limitations of such
metrics at the segment level. Specifically, it is important to investigate
metric behaviour when facing accuracy errors in MT because these can have
dangerous consequences in certain contexts (e.g., legal, medical). We curate
ACES, a translation accuracy challenge set, consisting of 68 phenomena ranging
from simple perturbations at the word/character level to more complex errors
based on discourse and real-world knowledge. We use ACES to evaluate a wide
range of MT metrics including the submissions to the WMT 2022 metrics shared
task and perform several analyses leading to general recommendations for metric
developers. We recommend: a) combining metrics with different strengths, b)
developing metrics that give more weight to the source and less to
surface-level overlap with the reference and c) explicitly modelling additional
language-specific information beyond what is available via multilingual
embeddings.",https://github.com/EdinburghNLP/ACES,-1
5af61ddc-1d66-478f-b796-a73cc1f31ef2,Learning Representations for Hyper-Relational Knowledge Graphs,0.782163,"Knowledge graphs (KGs) have gained prominence for their ability to learn
representations for uni-relational facts. Recently, research has focused on
modeling hyper-relational facts, which move beyond the restriction of
uni-relational facts and allow us to represent more complex and real-world
information. However, existing approaches for learning representations on
hyper-relational KGs majorly focus on enhancing the communication from
qualifiers to base triples while overlooking the flow of information from base
triple to qualifiers. This can lead to suboptimal qualifier representations,
especially when a large amount of qualifiers are presented. It motivates us to
design a framework that utilizes multiple aggregators to learn representations
for hyper-relational facts: one from the perspective of the base triple and the
other one from the perspective of the qualifiers. Experiments demonstrate the
effectiveness of our framework for hyper-relational knowledge graph completion
across multiple datasets. Furthermore, we conduct an ablation study that
validates the importance of the various components in our framework. The code
to reproduce our results can be found at
\url{https://github.com/HarryShomer/QUAD}.",https://github.com/HarryShomer/QUAD,-1
563c4fa8-484a-4e96-9bf6-015b24f65075,Panning for gold: Lessons learned from the platform-agnostic automated detection of political content in textual data,0.332316,"The growing availability of data about online information behaviour enables
new possibilities for political communication research. However, the volume and
variety of these data makes them difficult to analyse and prompts the need for
developing automated content approaches relying on a broad range of natural
language processing techniques (e.g. machine learning- or neural network-based
ones). In this paper, we discuss how these techniques can be used to detect
political content across different platforms. Using three validation datasets,
which include a variety of political and non-political textual documents from
online platforms, we systematically compare the performance of three groups of
detection techniques relying on dictionaries, supervised machine learning, or
neural networks. We also examine the impact of different modes of data
preprocessing (e.g. stemming and stopword removal) on the low-cost
implementations of these techniques using a large set (n = 66) of detection
models. Our results show the limited impact of preprocessing on model
performance, with the best results for less noisy data being achieved by neural
network- and machine-learning-based models, in contrast to the more robust
performance of dictionary-based models on noisy data.",None,-1
846e9602-3a68-4cc0-a683-f2de3acc1bb3,Some Stylometric Remarks on Ovid's Heroides and the Epistula Sapphus,0.152375,"This article aims to contribute to two well-worn areas of debate in classical
Latin philology, relating to Ovid's Heroides. The first is the question of the
authenticity (and, to a lesser extent the correct position) of the letter
placed fifteenth by almost every editor -- the so-called Epistula Sapphus
(henceforth ES). The secondary question, although perhaps now less fervently
debated, is the authenticity of the 'Double Heroides', placed by those who
accept them as letters 16-21. I employ a variety of methods drawn from the
domain of computational stylometry to consider the poetics and the
lexico-grammatical features of these elegiac poems in the broader context of a
corpus of 'shorter' (from 20 to 546 lines) elegiac works from five authors (266
poems in all) comprising more or less all of the non-fragmentary classical
corpus. Based on a variety of techniques, every measure gives clear indication
that the poetic style of the Heroides is Ovidian, but distinctive; they can be
accurately isolated from Ovid more broadly. The Single and Double Heroides
split into two clear groups, with the ES grouped consistently with the single
letters. Furthermore, by comparing the style of the letters with the 'early'
(although there are complications in this label) works of the Amores and the
late works of the Ex Ponto, the evidence supports sequential composition --
meaning that the ES is correctly placed -- and, further, supports the growing
consensus that the double letters were composed significantly later, in exile.",https://github.com/bnagy/heroides-paper,-1
03e87339-1b9f-4a16-8078-c55a1d9cbc54,Exploiting Global and Local Hierarchies for Hierarchical Text Classification,0.306679,"Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as a structured
labels hierarchy corresponding to each text sample. It is dynamic and relevant
to text samples, which is ignored in previous methods. To exploit global and
local hierarchies,we propose Hierarchy-guided BERT with Global and Local
hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local
hierarchies.Moreover,HBGL avoids the intentional fusion of semantic and
hierarchical modules by directly modeling semantic and hierarchical information
with BERT.Compared with the state-of-the-art method HGCLR,our method achieves
significant improvement on three benchmark datasets.",http://github.com/kongds/,15937
b3cf5538-f7b1-41b0-b876-d7f75fc231cd,Vision Transformers: State of the Art and Research Challenges,0.20978,"Transformers have achieved great success in natural language processing. Due
to the powerful capability of self-attention mechanism in transformers,
researchers develop the vision transformers for a variety of computer vision
tasks, such as image recognition, object detection, image segmentation, pose
estimation, and 3D reconstruction. This paper presents a comprehensive overview
of the literature on different architecture designs and training tricks
(including self-supervised learning) for vision transformers. Our goal is to
provide a systematic review with the open research opportunities.",None,-1
607e0011-d8c0-4d36-835b-c9da7b9bf552,HYPRO: A Hybridly Normalized Probabilistic Model for Long-Horizon Prediction of Event Sequences,0.559377,"In this paper, we tackle the important yet under-investigated problem of
making long-horizon prediction of event sequences. Existing state-of-the-art
models do not perform well at this task due to their autoregressive structure.
We propose HYPRO, a hybridly normalized probabilistic model that naturally fits
this task: its first part is an autoregressive base model that learns to
propose predictions; its second part is an energy function that learns to
reweight the proposals such that more realistic predictions end up with higher
probabilities. We also propose efficient training and inference algorithms for
this model. Experiments on multiple real-world datasets demonstrate that our
proposed HYPRO model can significantly outperform previous models at making
long-horizon predictions of future events. We also conduct a range of ablation
studies to investigate the effectiveness of each component of our proposed
methods.",https://github.com/iLampard/hypro_tpp,-1
8a1ba8a4-5fae-4dc8-bfb8-27a2f0f687e3,Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes,0.487329,"We study offline reinforcement learning (RL) in partially observable Markov
decision processes. In particular, we aim to learn an optimal policy from a
dataset collected by a behavior policy which possibly depends on the latent
state. Such a dataset is confounded in the sense that the latent state
simultaneously affects the action and the observation, which is prohibitive for
existing offline RL algorithms. To this end, we propose the \underline{P}roxy
variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization
(\texttt{P3O}) algorithm, which addresses the confounding bias and the
distributional shift between the optimal and behavior policies in the context
of general function approximation. At the core of \texttt{P3O} is a coupled
sequence of pessimistic confidence regions constructed via proximal causal
inference, which is formulated as minimax estimation. Under a partial coverage
assumption on the confounded dataset, we prove that \texttt{P3O} achieves a
$n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the
dataset. To our best knowledge, \texttt{P3O} is the first provably efficient
offline RL algorithm for POMDPs with a confounded dataset.",None,-1
5f39c83c-0ef9-41b1-ada7-3ebfd14dabe5,COVID-19-related Nepali Tweets Classification in a Low Resource Setting,0.229427,"Billions of people across the globe have been using social media platforms in
their local languages to voice their opinions about the various topics related
to the COVID-19 pandemic. Several organizations, including the World Health
Organization, have developed automated social media analysis tools that
classify COVID-19-related tweets into various topics. However, these tools that
help combat the pandemic are limited to very few languages, making several
countries unable to take their benefit. While multi-lingual or low-resource
language-specific tools are being developed, they still need to expand their
coverage, such as for the Nepali language. In this paper, we identify the eight
most common COVID-19 discussion topics among the Twitter community using the
Nepali language, set up an online platform to automatically gather Nepali
tweets containing the COVID-19-related keywords, classify the tweets into the
eight topics, and visualize the results across the period in a web-based
dashboard. We compare the performance of two state-of-the-art multi-lingual
language models for Nepali tweet classification, one generic (mBERT) and the
other Nepali language family-specific model (MuRIL). Our results show that the
models' relative performance depends on the data size, with MuRIL doing better
for a larger dataset. The annotated data, models, and the web-based dashboard
are open-sourced at https://github.com/naamiinepal/covid-tweet-classification.",https://github.com/naamiinepal/covid-tweet-classification,-1
1ee5ffde-dd8c-4fb3-843b-77ee06ab924d,Supervised Machine Learning for Effective Missile Launch Based on Beyond Visual Range Air Combat Simulations,0.938358,"This work compares supervised machine learning methods using reliable data
from constructive simulations to estimate the most effective moment for
launching missiles during air combat. We employed resampling techniques to
improve the predictive model, analyzing accuracy, precision, recall, and
f1-score. Indeed, we could identify the remarkable performance of the models
based on decision trees and the significant sensitivity of other algorithms to
resampling techniques. The models with the best f1-score brought values of
0.379 and 0.465 without and with the resampling technique, respectively, which
is an increase of 22.69%. Thus, if desirable, resampling techniques can improve
the model's recall and f1-score with a slight decline in accuracy and
precision. Therefore, through data obtained through constructive simulations,
it is possible to develop decision support tools based on machine learning
models, which may improve the flight quality in BVR air combat, increasing the
effectiveness of offensive missions to hit a particular target.",https://github.com/jpadantas/effective-missile-launch,-1
68940ea7-9bab-4b92-9a1f-de93e01b2cfe,Context-Consistent Semantic Image Editing with Style-Preserved Modulation,0.222757,"Semantic image editing utilizes local semantic label maps to generate the
desired content in the edited region. A recent work borrows SPADE block to
achieve semantic image editing. However, it cannot produce pleasing results due
to style discrepancy between the edited region and surrounding pixels. We
attribute this to the fact that SPADE only uses an image-independent local
semantic layout but ignores the image-specific styles included in the known
pixels. To address this issue, we propose a style-preserved modulation (SPM)
comprising two modulations processes: The first modulation incorporates the
contextual style and semantic layout, and then generates two fused modulation
parameters. The second modulation employs the fused parameters to modulate
feature maps. By using such two modulations, SPM can inject the given semantic
layout while preserving the image-specific context style. Moreover, we design a
progressive architecture for generating the edited content in a coarse-to-fine
manner. The proposed method can obtain context-consistent results and
significantly alleviate the unpleasant boundary between the generated regions
and the known pixels.",https://github.com/WuyangLuo/SPMPGAN,-1
9d05c8db-4b2a-41a1-a75a-5499efbd9379,Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition,0.486811,"Self-supervised learning (SSL) based models have been shown to generate
powerful representations that can be used to improve the performance of
downstream speech tasks. Several state-of-the-art SSL models are available, and
each of these models optimizes a different loss which gives rise to the
possibility of their features being complementary. This paper proposes using an
ensemble of such SSL representations and models, which exploits the
complementary nature of the features extracted by the various pretrained
models. We hypothesize that this results in a richer feature representation and
shows results for the ASR downstream task. To this end, we use three SSL models
that have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and
WaveLM. We explore the ensemble of models fine-tuned for the ASR task and the
ensemble of features using the embeddings obtained from the pre-trained models
for a downstream ASR task. We get improved performance over individual models
and pre-trained features using Librispeech(100h) and WSJ dataset for the
downstream tasks.",None,-1
48e2af5f-b5c4-477f-b491-02dcb7189395,VALHALLA: Visual Hallucination for Machine Translation,0.908053,"Designing better machine translation systems by considering auxiliary inputs
such as images has attracted much attention in recent years. While existing
methods show promising performance over the conventional text-only translation
systems, they typically require paired text and image as input during
inference, which limits their applicability to real-world scenarios. In this
paper, we introduce a visual hallucination framework, called VALHALLA, which
requires only source sentences at inference time and instead uses hallucinated
visual representations for multimodal machine translation. In particular, given
a source sentence an autoregressive hallucination transformer is used to
predict a discrete visual representation from the input text, and the combined
text and hallucinated representations are utilized to obtain the target
translation. We train the hallucination transformer jointly with the
translation transformer using standard backpropagation with cross-entropy
losses while being guided by an additional loss that encourages consistency
between predictions using either ground-truth or hallucinated visual
representations. Extensive experiments on three standard translation datasets
with a diverse set of language pairs demonstrate the effectiveness of our
approach over both text-only baselines and state-of-the-art methods. Project
page: http://www.svcl.ucsd.edu/projects/valhalla.",http://www.svcl.ucsd.edu/projects/valhalla,-1
a7c73e1a-5fb8-4e47-9972-c2a34b9c78fa,From Examples to Rules: Neural Guided Rule Synthesis for Information Extraction,0.0379499,"While deep learning approaches to information extraction have had many
successes, they can be difficult to augment or maintain as needs shift.
Rule-based methods, on the other hand, can be more easily modified. However,
crafting rules requires expertise in linguistics and the domain of interest,
making it infeasible for most users. Here we attempt to combine the advantages
of these two directions while mitigating their drawbacks. We adapt recent
advances from the adjacent field of program synthesis to information
extraction, synthesizing rules from provided examples. We use a
transformer-based architecture to guide an enumerative search, and show that
this reduces the number of steps that need to be explored before a rule is
found. Further, we show that without training the synthesis algorithm on the
specific domain, our synthesized rules achieve state-of-the-art performance on
the 1-shot scenario of a task that focuses on few-shot learning for relation
classification, and competitive performance in the 5-shot scenario.",https://github.com/clulab/releases/tree/master/lrec2022-odinsynth,-1
ada75f39-dacf-45a3-87dc-218a6f57ab9d,Building Open Knowledge Graph for Metal-Organic Frameworks (MOF-KG): Challenges and Case Studies,0.439675,"Metal-Organic Frameworks (MOFs) are a class of modular, porous crystalline
materials that have great potential to revolutionize applications such as gas
storage, molecular separations, chemical sensing, catalysis, and drug delivery.
The Cambridge Structural Database (CSD) reports 10,636 synthesized MOF crystals
which in addition contains ca. 114,373 MOF-like structures. The sheer number of
synthesized (plus potentially synthesizable) MOF structures requires
researchers pursue computational techniques to screen and isolate MOF
candidates. In this demo paper, we describe our effort on leveraging knowledge
graph methods to facilitate MOF prediction, discovery, and synthesis. We
present challenges and case studies about (1) construction of a MOF knowledge
graph (MOF-KG) from structured and unstructured sources and (2) leveraging the
MOF-KG for discovery of new or missing knowledge.",None,-1
57560b93-b5bf-45e5-a85d-2714bc5cd4bb,Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval,0.105521,"Contrastive learning (CL) has become a ubiquitous approach for several
natural language processing (NLP) downstream tasks, especially for question
answering (QA). However, the major challenge, how to efficiently train the
knowledge retrieval model in an unsupervised manner, is still unresolved.
Recently the commonly used methods are composed of CL and masked language model
(MLM). Unexpectedly, MLM ignores the sentence-level training, and CL also
neglects extraction of the internal info from the query. To optimize the CL
hardly obtain internal information from the original query, we introduce a
joint training method by combining CL and Auto-MLM for self-supervised
multi-lingual knowledge retrieval. First, we acquire the fixed dimensional
sentence vector. Then, mask some words among the original sentences with random
strategy. Finally, we generate a new token representation for predicting the
masked tokens. Experimental results show that our proposed approach
consistently outperforms all the previous SOTA methods on both AliExpress $\&$
LAZADA service corpus and openly available corpora in 8 languages.",https://github.com/taolei87/askubuntu,-1
2afc570d-1c86-41dc-8733-d2516880ab29,Guided Depth Super-Resolution by Deep Anisotropic Diffusion,0.892782,"Performing super-resolution of a depth image using the guidance from an RGB
image is a problem that concerns several fields, such as robotics, medical
imaging, and remote sensing. While deep learning methods have achieved good
results in this problem, recent work highlighted the value of combining modern
methods with more formal frameworks. In this work, we propose a novel approach
which combines guided anisotropic diffusion with a deep convolutional network
and advances the state of the art for guided depth super-resolution. The edge
transferring/enhancing properties of the diffusion are boosted by the
contextual reasoning capabilities of modern networks, and a strict adjustment
step guarantees perfect adherence to the source image. We achieve unprecedented
results in three commonly used benchmarks for guided depth super-resolution.
The performance gain compared to other methods is the largest at larger scales,
such as x32 scaling. Code
(https://github.com/prs-eth/Diffusion-Super-Resolution) for the proposed method
is available to promote reproducibility of our results.",https://github.com/prs-eth/Diffusion-Super-Resolution,-1
8cf4bf7c-f801-4dc1-8f3d-4f2b513017b1,The Role of Coverage in Online Reinforcement Learning,0.649957,"Coverage conditions -- which assert that the data logging distribution
adequately covers the state space -- play a fundamental role in determining the
sample complexity of offline reinforcement learning. While such conditions
might seem irrelevant to online reinforcement learning at first glance, we
establish a new connection by showing -- somewhat surprisingly -- that the mere
existence of a data distribution with good coverage can enable sample-efficient
online RL. Concretely, we show that coverability -- that is, existence of a
data distribution that satisfies a ubiquitous coverage condition called
concentrability -- can be viewed as a structural property of the underlying
MDP, and can be exploited by standard algorithms for sample-efficient
exploration, even when the agent does not know said distribution. We complement
this result by proving that several weaker notions of coverage, despite being
sufficient for offline RL, are insufficient for online RL. We also show that
existing complexity measures for online RL, including Bellman rank and
Bellman-Eluder dimension, fail to optimally capture coverability, and propose a
new complexity measure, the sequential extrapolation coefficient, to provide a
unification.",None,-1
b7635038-bd15-4dad-8e90-a6785909b2dc,On the generalization capabilities of FSL methods through domain adaptation: a case study in endoscopic kidney stone image classification,0.129434,"Deep learning has shown great promise in diverse areas of computer vision,
such as image classification, object detection and semantic segmentation, among
many others. However, as it has been repeatedly demonstrated, deep learning
methods trained on a dataset do not generalize well to datasets from other
domains or even to similar datasets, due to data distribution shifts. In this
work, we propose the use of a meta-learning based few-shot learning approach to
alleviate these problems. In order to demonstrate its efficacy, we use two
datasets of kidney stones samples acquired with different endoscopes and
different acquisition conditions. The results show how such methods are indeed
capable of handling domain-shifts by attaining an accuracy of 74.38% and 88.52%
in the 5-way 5-shot and 5-way 20-shot settings respectively. Instead, in the
same dataset, traditional Deep Learning (DL) methods attain only an accuracy of
45%.",None,-1
6537b083-6490-4909-919a-0ebd3d10b06a,News Category Dataset,0.996538,"People rely on news to know what is happening around the world and inform
their daily lives. In today's world, when the proliferation of fake news is
rampant, having a large-scale and high-quality source of authentic news
articles with the published category information is valuable to learning
authentic news' Natural Language syntax and semantics. As part of this work, we
present a News Category Dataset that contains around 210k news headlines from
the year 2012 to 2022 obtained from HuffPost, along with useful metadata to
enable various NLP tasks. In this paper, we also produce some novel insights
from the dataset and describe various existing and potential applications of
our dataset.",None,-1
0621e379-52b2-4c8c-9503-91ba1696a152,Far Away in the Deep Space: Dense Nearest-Neighbor-Based Out-of-Distribution Detection,0.0555147,"The key to out-of-distribution detection is density estimation of the
in-distribution data or of its feature representations. This is particularly
challenging for dense anomaly detection in domains where the in-distribution
data has a complex underlying structure. Nearest-Neighbors approaches have been
shown to work well in object-centric data domains, such as industrial
inspection and image classification. In this paper, we show that
nearest-neighbor approaches also yield state-of-the-art results on dense
novelty detection in complex driving scenes when working with an appropriate
feature representation. In particular, we find that transformer-based
architectures produce representations that yield much better similarity metrics
for the task. We identify the multi-head structure of these models as one of
the reasons, and demonstrate a way to transfer some of the improvements to
CNNs. Ultimately, the approach is simple and non-invasive, i.e., it does not
affect the primary segmentation performance, refrains from training on examples
of anomalies, and achieves state-of-the-art results on RoadAnomaly,
StreetHazards, and SegmentMeIfYouCan-Anomaly.",https://github.com/silviogalesso/dense-ood-knns,155608
261a2577-9592-4a67-985d-178b206feeaa,3D Scene Inference from Transient Histograms,0.465629,"Time-resolved image sensors that capture light at pico-to-nanosecond
timescales were once limited to niche applications but are now rapidly becoming
mainstream in consumer devices. We propose low-cost and low-power imaging
modalities that capture scene information from minimal time-resolved image
sensors with as few as one pixel. The key idea is to flood illuminate large
scene patches (or the entire scene) with a pulsed light source and measure the
time-resolved reflected light by integrating over the entire illuminated area.
The one-dimensional measured temporal waveform, called \emph{transient},
encodes both distances and albedoes at all visible scene points and as such is
an aggregate proxy for the scene's 3D geometry. We explore the viability and
limitations of the transient waveforms by themselves for recovering scene
information, and also when combined with traditional RGB cameras. We show that
plane estimation can be performed from a single transient and that using only a
few more it is possible to recover a depth map of the whole scene. We also show
two proof-of-concept hardware prototypes that demonstrate the feasibility of
our approach for compact, mobile, and budget-limited applications.",None,-1
40f30f44-0408-4f54-8c05-dc9839251109,Cross-modal Learning for Image-Guided Point Cloud Shape Completion,0.487635,"In this paper we explore the recent topic of point cloud completion, guided
by an auxiliary image. We show how it is possible to effectively combine the
information from the two modalities in a localized latent space, thus avoiding
the need for complex point cloud reconstruction methods from single views used
by the state-of-the-art. We also investigate a novel weakly-supervised setting
where the auxiliary image provides a supervisory signal to the training process
by using a differentiable renderer on the completed point cloud to measure
fidelity in the image space. Experiments show significant improvements over
state-of-the-art supervised methods for both unimodal and multimodal
completion. We also show the effectiveness of the weakly-supervised approach
which outperforms a number of supervised methods and is competitive with the
latest supervised models only exploiting point cloud information.",https://github.com/diegovalsesia/XMFnet,-1
91823b49-408f-40ab-951a-9169bd5482f0,Effective and Efficient Training for Sequential Recommendation using Recency Sampling,0.675775,"Many modern sequential recommender systems use deep neural networks, which
can effectively estimate the relevance of items but require a lot of time to
train. Slow training increases expenses, hinders product development timescales
and prevents the model from being regularly updated to adapt to changing user
preferences. Training such sequential models involves appropriately sampling
past user interactions to create a realistic training objective. The existing
training objectives have limitations. For instance, next item prediction never
uses the beginning of the sequence as a learning target, thereby potentially
discarding valuable data. On the other hand, the item masking used by BERT4Rec
is only weakly related to the goal of the sequential recommendation; therefore,
it requires much more time to obtain an effective model. Hence, we propose a
novel Recency-based Sampling of Sequences training objective that addresses
both limitations. We apply our method to various recent and state-of-the-art
model architectures - such as GRU4Rec, Caser, and SASRec. We show that the
models enhanced with our method can achieve performances exceeding or very
close to stateof-the-art BERT4Rec, but with much less training time.",None,-1
3239d0aa-4acb-46fc-b0f3-54a3e9cad0bb,Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models,0.486446,"Structural probing work has found evidence for latent syntactic information
in pre-trained language models. However, much of this analysis has focused on
monolingual models, and analyses of multilingual models have employed
correlational methods that are confounded by the choice of probing tasks. In
this study, we causally probe multilingual language models (XGLM and
multilingual BERT) as well as monolingual BERT-based models across various
languages; we do this by performing counterfactual perturbations on neuron
activations and observing the effect on models' subject-verb agreement
probabilities. We observe where in the model and to what extent syntactic
agreement is encoded in each language. We find significant neuron overlap
across languages in autoregressive multilingual language models, but not masked
language models. We also find two distinct layer-wise effect patterns and two
distinct sets of neurons used for syntactic agreement, depending on whether the
subject and verb are separated by other tokens. Finally, we find that
behavioral analyses of language models are likely underestimating how sensitive
masked language models are to syntactic information.",https://github.com/aaronmueller/multilingual-lm-intervention,8208
c6a880b1-1032-4c49-86f4-96408e6ad99c,High-Resource Methodological Bias in Low-Resource Investigations,0.160063,"The central bottleneck for low-resource NLP is typically regarded to be the
quantity of accessible data, overlooking the contribution of data quality. This
is particularly seen in the development and evaluation of low-resource systems
via down sampling of high-resource language data. In this work we investigate
the validity of this approach, and we specifically focus on two well-known NLP
tasks for our empirical investigations: POS-tagging and machine translation. We
show that down sampling from a high-resource language results in datasets with
different properties than the low-resource datasets, impacting the model
performance for both POS-tagging and machine translation. Based on these
results we conclude that naive down sampling of datasets results in a biased
view of how well these systems work in a low-resource scenario.",https://github.com/flairNLP/flair/,-1
11a09566-14e2-4d15-83e2-b2d04eaeb8dd,A Human-Centric Assessment Framework for AI,0.456418,"With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner.",None,1665
46687199-1bab-42bb-bbc8-eb3ae959c351,"Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data",0.414772,"Advances in deep learning for human activity recognition have been relatively
limited due to the lack of large labelled datasets. In this study, we leverage
self-supervised learning techniques on the UK-Biobank activity tracker
dataset--the largest of its kind to date--containing more than 700,000
person-days of unlabelled wearable sensor data. Our resulting activity
recognition model consistently outperformed strong baselines across seven
benchmark datasets, with an F1 relative improvement of 2.5%-100% (median
18.4%), the largest improvements occurring in the smaller datasets. In contrast
to previous studies, our results generalise across external datasets, devices,
and environments. Our open-source model will help researchers and developers to
build customisable and generalisable activity classifiers with high
performance.",https://github.com/OxWearables/ssl-wearables,-1
f5b1f20e-deda-4c5c-9135-1528ef77c818,MMMNA-Net for Overall Survival Time Prediction of Brain Tumor Patients,0.236337,"Overall survival (OS) time is one of the most important evaluation indices
for gliomas situations. Multimodal Magnetic Resonance Imaging (MRI) scans play
an important role in the study of glioma prognosis OS time. Several deep
learning-based methods are proposed for the OS time prediction on multi-modal
MRI problems. However, these methods usually fuse multi-modal information at
the beginning or at the end of the deep learning networks and lack the fusion
of features from different scales. In addition, the fusion at the end of
networks always adapts global with global (eg. fully connected after
concatenation of global average pooling output) or local with local (eg.
bilinear pooling), which loses the information of local with global. In this
paper, we propose a novel method for multi-modal OS time prediction of brain
tumor patients, which contains an improved nonlocal features fusion module
introduced on different scales. Our method obtains a relative 8.76% improvement
over the current state-of-art method (0.6989 vs. 0.6426 on accuracy). Extensive
testing demonstrates that our method could adapt to situations with missing
modalities. The code is available at
https://github.com/TangWen920812/mmmna-net.",https://github.com/TangWen920812/mmmna-net,-1
10967893-6eb9-4ef7-8ff4-2093aaf33e2c,TINYCD: A (Not So) Deep Learning Model For Change Detection,0.838808,"In this paper, we present a lightweight and effective change detection model,
called TinyCD. This model has been designed to be faster and smaller than
current state-of-the-art change detection models due to industrial needs.
Despite being from 13 to 140 times smaller than the compared change detection
models, and exposing at least a third of the computational complexity, our
model outperforms the current state-of-the-art models by at least $1\%$ on both
F1 score and IoU on the LEVIR-CD dataset, and more than $8\%$ on the WHU-CD
dataset. To reach these results, TinyCD uses a Siamese U-Net architecture
exploiting low-level features in a globally temporal and locally spatial way.
In addition, it adopts a new strategy to mix features in the space-time domain
both to merge the embeddings obtained from the Siamese backbones, and, coupled
with an MLP block, it forms a novel space-semantic attention mechanism, the Mix
and Attention Mask Block (MAMB). Source code, models and results are available
here: https://github.com/AndreaCodegoni/Tiny_model_4_CD",https://github.com/AndreaCodegoni/Tiny_model_4_CD,82
ac6be3b9-f92e-4b66-b50d-74abba392ca6,ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering,0.75274,"With the recent advance in large pre-trained language models, researchers
have achieved record performances in NLP tasks that mostly focus on language
pattern matching. The community is experiencing the shift of the challenge from
how to model language to the imitation of complex reasoning abilities like
human beings. In this work, we investigate the application domain of finance
that involves real-world, complex numerical reasoning. We propose a new
large-scale dataset, ConvFinQA, aiming to study the chain of numerical
reasoning in conversational question answering. Our dataset poses great
challenge in modeling long-range, complex numerical reasoning paths in
real-world conversations. We conduct comprehensive experiments and analyses
with both the neural symbolic methods and the prompting-based methods, to
provide insights into the reasoning mechanisms of these two divisions. We
believe our new dataset should serve as a valuable resource to push forward the
exploration of real-world, complex reasoning tasks as the next research focus.
Our dataset and code is publicly available at
https://github.com/czyssrs/ConvFinQA.",https://github.com/czyssrs/ConvFinQA,-1
987ef3b6-98de-48ee-b940-a422e2a5fd0c,"Self-supervision and Learnable STRFs for Age, Emotion, and Country Prediction",0.0718638,"This work presents a multitask approach to the simultaneous estimation of
age, country of origin, and emotion given vocal burst audio for the 2022 ICML
Expressive Vocalizations Challenge ExVo-MultiTask track. The method of choice
utilized a combination of spectro-temporal modulation and self-supervised
features, followed by an encoder-decoder network organized in a multitask
paradigm. We evaluate the complementarity between the tasks posed by examining
independent task-specific and joint models, and explore the relative strengths
of different feature sets. We also introduce a simple score fusion mechanism to
leverage the complementarity of different feature sets for this task.
  We find that robust data preprocessing in conjunction with score fusion over
spectro-temporal receptive field and HuBERT models achieved our best
ExVo-MultiTask test score of 0.412.",https://github.com/snakers4/silero-vad,-1
0b04cf0d-bcca-4487-84c3-c020213885bb,Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining,0.782053,"Deep visuomotor policy learning, which aims to map raw visual observation to
action, achieves promising results in control tasks such as robotic
manipulation and autonomous driving. However, it requires a huge number of
online interactions with the training environment, which limits its real-world
application. Compared to the popular unsupervised feature learning for visual
recognition, feature pretraining for visuomotor control tasks is much less
explored. In this work, we aim to pretrain policy representations for driving
tasks by watching hours-long uncurated YouTube videos. Specifically, we train
an inverse dynamic model with a small amount of labeled data and use it to
predict action labels for all the YouTube video frames. A new contrastive
policy pretraining method is then developed to learn action-conditioned
features from the video frames with pseudo action labels. Experiments show that
the resulting action-conditioned features obtain substantial improvements for
the downstream reinforcement learning and imitation learning tasks,
outperforming the weights pretrained from previous unsupervised learning
methods and ImageNet pretrained weight. Code, model weights, and data are
available at: https://metadriverse.github.io/ACO.",https://metadriverse.github.io/ACO,-1
1c7cbd4c-2226-48c7-8aa7-70291928222c,AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models,0.849964,"The objective of pre-trained language models is to learn contextual
representations of textual data. Pre-trained language models have become
mainstream in natural language processing and code modeling. Using probes, a
technique to study the linguistic properties of hidden vector spaces, previous
works have shown that these pre-trained language models encode simple
linguistic properties in their hidden representations. However, none of the
previous work assessed whether these models encode the whole grammatical
structure of a programming language. In this paper, we prove the existence of a
syntactic subspace, lying in the hidden representations of pre-trained language
models, which contain the syntactic information of the programming language. We
show that this subspace can be extracted from the models' representations and
define a novel probing method, the AST-Probe, that enables recovering the whole
abstract syntax tree (AST) of an input code snippet. In our experimentations,
we show that this syntactic subspace exists in five state-of-the-art
pre-trained language models. In addition, we highlight that the middle layers
of the models are the ones that encode most of the AST information. Finally, we
estimate the optimal size of this syntactic subspace and show that its
dimension is substantially lower than those of the models' representation
spaces. This suggests that pre-trained language models use a small part of
their representation spaces to encode syntactic information of the programming
languages.",https://doi.org/10.5281/zenodo.7032076,-1
fc31e603-867b-4465-b851-f65c3ace5135,Quantitative Metrics for Evaluating Explanations of Video DeepFake Detectors,0.176735,"The proliferation of DeepFake technology is a rising challenge in today's
society, owing to more powerful and accessible generation methods. To counter
this, the research community has developed detectors of ever-increasing
accuracy. However, the ability to explain the decisions of such models to users
is lacking behind and is considered an accessory in large-scale benchmarks,
despite being a crucial requirement for the correct deployment of automated
tools for content moderation. We attribute the issue to the reliance on
qualitative comparisons and the lack of established metrics. We describe a
simple set of metrics to evaluate the visual quality and informativeness of
explanations of video DeepFake classifiers from a human-centric perspective.
With these metrics, we compare common approaches to improve explanation quality
and discuss their effect on both classification and explanation performance on
the recent DFDC and DFD datasets.",https://github.com/baldassarreFe/deepfake-detection,-1
5477b618-abc5-4232-8cb9-b3f60aa8eb4f,Detector-Free Weakly Supervised Group Activity Recognition,0.814712,"Group activity recognition is the task of understanding the activity
conducted by a group of people as a whole in a multi-person video. Existing
models for this task are often impractical in that they demand ground-truth
bounding box labels of actors even in testing or rely on off-the-shelf object
detectors. Motivated by this, we propose a novel model for group activity
recognition that depends neither on bounding box labels nor on object detector.
Our model based on Transformer localizes and encodes partial contexts of a
group activity by leveraging the attention mechanism, and represents a video
clip as a set of partial context embeddings. The embedding vectors are then
aggregated to form a single group representation that reflects the entire
context of an activity while capturing temporal evolution of each partial
context. Our method achieves outstanding performance on two benchmarks,
Volleyball and NBA datasets, surpassing not only the state of the art trained
with the same level of supervision, but also some of existing models relying on
stronger supervision.",https://github.com/JacobYuan7/DIN_GAR,-1
9196294b-f135-4004-9c96-3addb3255e96,SegTAD: Precise Temporal Action Detection via Semantic Segmentation,0.252955,"Temporal action detection (TAD) is an important yet challenging task in video
analysis. Most existing works draw inspiration from image object detection and
tend to reformulate it as a proposal generation - classification problem.
However, there are two caveats with this paradigm. First, proposals are not
equipped with annotated labels, which have to be empirically compiled, thus the
information in the annotations is not necessarily precisely employed in the
model training process. Second, there are large variations in the temporal
scale of actions, and neglecting this fact may lead to deficient representation
in the video features. To address these issues and precisely model temporal
action detection, we formulate the task of temporal action detection in a novel
perspective of semantic segmentation. Owing to the 1-dimensional property of
TAD, we are able to convert the coarse-grained detection annotations to
fine-grained semantic segmentation annotations for free. We take advantage of
them to provide precise supervision so as to mitigate the impact induced by the
imprecise proposal labels. We propose an end-to-end framework SegTAD composed
of a 1D semantic segmentation network (1D-SSN) and a proposal detection network
(PDN).",None,-1
d55b6b89-1e51-4374-8b67-b6d8b6e2152a,Code-Switching without Switching: Language Agnostic End-to-End Speech Translation,0.687652,"We propose a) a Language Agnostic end-to-end Speech Translation model (LAST),
and b) a data augmentation strategy to increase code-switching (CS)
performance. With increasing globalization, multiple languages are increasingly
used interchangeably during fluent speech. Such CS complicates traditional
speech recognition and translation, as we must recognize which language was
spoken first and then apply a language-dependent recognizer and subsequent
translation component to generate the desired target language output. Such a
pipeline introduces latency and errors. In this paper, we eliminate the need
for that, by treating speech recognition and translation as one unified
end-to-end speech translation problem. By training LAST with both input
languages, we decode speech into one target language, regardless of the input
language. LAST delivers comparable recognition and speech translation accuracy
in monolingual usage, while reducing latency and error rate considerably when
CS is observed.",None,-1
8fe7bbd7-7e78-4154-9544-62f2b60cd3bc,The Self-Optimal-Transport Feature Transform,0.71508,"The Self-Optimal-Transport (SOT) feature transform is designed to upgrade the
set of features of a data instance to facilitate downstream matching or
grouping related tasks. The transformed set encodes a rich representation of
high order relations between the instance features. Distances between
transformed features capture their direct original similarity and their third
party agreement regarding similarity to other features in the set. A particular
min-cost-max-flow fractional matching problem, whose entropy regularized
version can be approximated by an optimal transport (OT) optimization, results
in our transductive transform which is efficient, differentiable, equivariant,
parameterless and probabilistically interpretable. Empirically, the transform
is highly effective and flexible in its use, consistently improving networks it
is inserted into, in a variety of tasks and training schemes. We demonstrate
its merits through the problem of unsupervised clustering and its efficiency
and wide applicability for few-shot-classification, with state-of-the-art
results, and large-scale person re-identification.",None,-1
37b5ebcc-ce7d-4279-beaa-c166291ce38e,LCRL: Certified Policy Synthesis via Logically-Constrained Reinforcement Learning,0.213691,"LCRL is a software tool that implements model-free Reinforcement Learning
(RL) algorithms over unknown Markov Decision Processes (MDPs), synthesising
policies that satisfy a given linear temporal specification with maximal
probability. LCRL leverages partially deterministic finite-state machines known
as Limit Deterministic Buchi Automata (LDBA) to express a given linear temporal
specification. A reward function for the RL algorithm is shaped on-the-fly,
based on the structure of the LDBA. Theoretical guarantees under proper
assumptions ensure the convergence of the RL algorithm to an optimal policy
that maximises the satisfaction probability. We present case studies to
demonstrate the applicability, ease of use, scalability, and performance of
LCRL. Owing to the LDBA-guided exploration and LCRL model-free architecture, we
observe robust performance, which also scales well when compared to standard RL
approaches (whenever applicable to LTL specifications). Full instructions on
how to execute all the case studies in this paper are provided on a GitHub page
that accompanies the LCRL distribution www.github.com/grockious/lcrl.",https://github.com/grockious/lcrl,-1
dfa92a4e-5a10-405f-bbe7-928a14a2a1d1,Tabula: Efficiently Computing Nonlinear Activation Functions for Secure Neural Network Inference,0.52395,"Multiparty computation approaches to secure neural network inference
traditionally rely on garbled circuits for securely executing nonlinear
activation functions. However, garbled circuits require excessive communication
between server and client, impose significant storage overheads, and incur
large runtime penalties. To eliminate these costs, we propose an alternative to
garbled circuits: Tabula, an algorithm based on secure lookup tables. Tabula
leverages neural networks' ability to be quantized and employs a secure lookup
table approach to efficiently, securely, and accurately compute neural network
nonlinear activation functions. Compared to garbled circuits with quantized
inputs, when computing individual nonlinear functions, our experiments show
Tabula uses between $35 \times$-$70 \times$ less communication, is over
$100\times$ faster, and uses a comparable amount of storage. This leads to
significant performance gains over garbled circuits with quantized inputs
during secure inference on neural networks: Tabula reduces overall
communication by up to $9 \times$ and achieves a speedup of up to $50 \times$,
while imposing comparable storage costs.",https://github.com/tabulainference/tabula,-1
3b699b22-ad4a-4b50-b48f-18c410000dc8,Learning to Classify Open Intent via Soft Labeling and Manifold Mixup,0.332792,"Open intent classification is a practical yet challenging task in dialogue
systems. Its objective is to accurately classify samples of known intents while
at the same time detecting those of open (unknown) intents. Existing methods
usually use outlier detection algorithms combined with K-class classifier to
detect open intents, where K represents the class number of known intents.
Different from them, in this paper, we consider another way without using
outlier detection algorithms. Specifically, we directly train a (K+1)-class
classifier for open intent classification, where the (K+1)-th class represents
open intents. To address the challenge that training a (K+1)-class classifier
with training samples of only K classes, we propose a deep model based on Soft
Labeling and Manifold Mixup (SLMM). In our method, soft labeling is used to
reshape the label distribution of the known intent samples, aiming at reducing
model's overconfident on known intents. Manifold mixup is used to generate
pseudo samples for open intents, aiming at well optimizing the decision
boundary of open intents. Experiments on four benchmark datasets demonstrate
that our method outperforms previous methods and achieves state-of-the-art
performance. All the code and data of this work can be obtained at
https://github.com/zifengcheng/SLMM.",https://github.com/zifengcheng/SLMM,-1
8353c382-2611-417d-9977-59718ec56441,SAIBench: Benchmarking AI for Science,0.0204902,"Scientific research communities are embracing AI-based solutions to target
tractable scientific tasks and improve research workflows. However, the
development and evaluation of such solutions are scattered across multiple
disciplines. We formalize the problem of scientific AI benchmarking, and
propose a system called SAIBench in the hope of unifying the efforts and
enabling low-friction on-boarding of new disciplines. The system approaches
this goal with SAIL, a domain-specific language to decouple research problems,
AI models, ranking criteria, and software/hardware configuration into reusable
modules. We show that this approach is flexible and can adapt to problems, AI
models, and evaluation methods defined in different perspectives. The project
homepage is https://www.computercouncil.org/SAIBench",None,-1
e37aa94c-d109-44d3-bc89-1c983a62e930,Check and Link: Pairwise Lesion Correspondence Guides Mammogram Mass Detection,0.613655,"Detecting mass in mammogram is significant due to the high occurrence and
mortality of breast cancer. In mammogram mass detection, modeling pairwise
lesion correspondence explicitly is particularly important. However, most of
the existing methods build relatively coarse correspondence and have not
utilized correspondence supervision. In this paper, we propose a new
transformer-based framework CL-Net to learn lesion detection and pairwise
correspondence in an end-to-end manner. In CL-Net, View-Interactive Lesion
Detector is proposed to achieve dynamic interaction across candidates of cross
views, while Lesion Linker employs the correspondence supervision to guide the
interaction process more accurately. The combination of these two designs
accomplishes precise understanding of pairwise lesion correspondence for
mammograms. Experiments show that CL-Net yields state-of-the-art performance on
the public DDSM dataset and our in-house dataset. Moreover, it outperforms
previous methods by a large margin in low FPI regime.",None,-1
eaf23725-3464-42f3-bb6f-806ce87ef3b3,Point3D: tracking actions as moving points with 3D CNNs,0.448851,"Spatio-temporal action recognition has been a challenging task that involves
detecting where and when actions occur. Current state-of-the-art action
detectors are mostly anchor-based, requiring sensitive anchor designs and huge
computations due to calculating large numbers of anchor boxes. Motivated by
nascent anchor-free approaches, we propose Point3D, a flexible and
computationally efficient network with high precision for spatio-temporal
action recognition. Our Point3D consists of a Point Head for action
localization and a 3D Head for action classification. Firstly, Point Head is
used to track center points and knot key points of humans to localize the
bounding box of an action. These location features are then piped into a
time-wise attention to learn long-range dependencies across frames. The 3D Head
is later deployed for the final action classification. Our Point3D achieves
state-of-the-art performance on the JHMDB, UCF101-24, and AVA benchmarks in
terms of frame-mAP and video-mAP. Comprehensive ablation studies also
demonstrate the effectiveness of each module proposed in our Point3D.",None,-1
c2193b38-9ff9-470f-8699-e8503c36761a,Learning Attention-based Representations from Multiple Patterns for Relation Prediction in Knowledge Graphs,0.0407972,"Knowledge bases, and their representations in the form of knowledge graphs
(KGs), are naturally incomplete. Since scientific and industrial applications
have extensively adopted them, there is a high demand for solutions that
complete their information. Several recent works tackle this challenge by
learning embeddings for entities and relations, then employing them to predict
new relations among the entities. Despite their aggrandizement, most of those
methods focus only on the local neighbors of a relation to learn the
embeddings. As a result, they may fail to capture the KGs' context information
by neglecting long-term dependencies and the propagation of entities'
semantics. In this manuscript, we propose {\AE}MP (Attention-based Embeddings
from Multiple Patterns), a novel model for learning contextualized
representations by: (i) acquiring entities' context information through an
attention-enhanced message-passing scheme, which captures the entities' local
semantics while focusing on different aspects of their neighborhood; and (ii)
capturing the semantic context, by leveraging the paths and their relationships
between entities. Our empirical findings draw insights into how attention
mechanisms can improve entities' context representation and how combining
entities and semantic path contexts improves the general representation of
entities and the relation predictions. Experimental results on several large
and small knowledge graph benchmarks show that {\AE}MP either outperforms or
competes with state-of-the-art relation prediction methods.",https://github.com/MeLLL-UFF/AEMP,-1
6409863a-4b7f-47b2-8ccf-bc70e25368a2,Real-time Full-stack Traffic Scene Perception for Autonomous Driving with Roadside Cameras,0.906484,"We propose a novel and pragmatic framework for traffic scene perception with
roadside cameras. The proposed framework covers a full-stack of roadside
perception pipeline for infrastructure-assisted autonomous driving, including
object detection, object localization, object tracking, and multi-camera
information fusion. Unlike previous vision-based perception frameworks rely
upon depth offset or 3D annotation at training, we adopt a modular decoupling
design and introduce a landmark-based 3D localization method, where the
detection and localization can be well decoupled so that the model can be
easily trained based on only 2D annotations. The proposed framework applies to
either optical or thermal cameras with pinhole or fish-eye lenses. Our
framework is deployed at a two-lane roundabout located at Ellsworth Rd. and
State St., Ann Arbor, MI, USA, providing 7x24 real-time traffic flow monitoring
and high-precision vehicle trajectory extraction. The whole system runs
efficiently on a low-power edge computing device with all-component end-to-end
delay of less than 20ms.",None,-1
665b22f9-4f4d-4b8a-bd71-24aabe0fef08,A Time Series is Worth 64 Words: Long-term Forecasting with Transformers,1.0,"We propose an efficient design of Transformer-based models for multivariate
time series forecasting and self-supervised representation learning. It is
based on two key components: (i) segmentation of time series into
subseries-level patches which are served as input tokens to Transformer; (ii)
channel-independence where each channel contains a single univariate time
series that shares the same embedding and Transformer weights across all the
series. Patching design naturally has three-fold benefit: local semantic
information is retained in the embedding; computation and memory usage of the
attention maps are quadratically reduced given the same look-back window; and
the model can attend longer history. Our channel-independent patch time series
Transformer (PatchTST) can improve the long-term forecasting accuracy
significantly when compared with that of SOTA Transformer-based models. We also
apply our model to self-supervised pre-training tasks and attain excellent
fine-tuning performance, which outperforms supervised training on large
datasets. Transferring of masked pre-trained representation on one dataset to
others also produces SOTA forecasting accuracy. Code is available at:
https://github.com/yuqinie98/PatchTST.",None,-1
f7796e6f-b184-4019-a7dc-adae638362b9,Importance of Synthesizing High-quality Data for Text-to-SQL Parsing,0.376599,"Recently, there has been increasing interest in synthesizing data to improve
downstream text-to-SQL tasks. In this paper, we first examined the existing
synthesized datasets and discovered that state-of-the-art text-to-SQL
algorithms did not further improve on popular benchmarks when trained with
augmented synthetic data. We observed two shortcomings: illogical synthetic SQL
queries from independent column sampling and arbitrary table joins. To address
these issues, we propose a novel synthesis framework that incorporates key
relationships from schema, imposes strong typing, and conducts
schema-distance-weighted column sampling. We also adopt an intermediate
representation (IR) for the SQL-to-text task to further improve the quality of
the generated natural language questions. When existing powerful semantic
parsers are pre-finetuned on our high-quality synthesized data, our experiments
show that these models have significant accuracy boosts on popular benchmarks,
including new state-of-the-art performance on Spider.",https://github.com/salesforce/WikiSQL,-1
26234514-850c-4ebc-bc63-83a0b6ba787a,Hierarchical Semi-Supervised Contrastive Learning for Contamination-Resistant Anomaly Detection,0.548656,"Anomaly detection aims at identifying deviant samples from the normal data
distribution. Contrastive learning has provided a successful way to sample
representation that enables effective discrimination on anomalies. However,
when contaminated with unlabeled abnormal samples in training set under
semi-supervised settings, current contrastive-based methods generally 1) ignore
the comprehensive relation between training data, leading to suboptimal
performance, and 2) require fine-tuning, resulting in low efficiency. To
address the above two issues, in this paper, we propose a novel hierarchical
semi-supervised contrastive learning (HSCL) framework, for
contamination-resistant anomaly detection. Specifically, HSCL hierarchically
regulates three complementary relations: sample-to-sample, sample-to-prototype,
and normal-to-abnormal relations, enlarging the discrimination between normal
and abnormal samples with a comprehensive exploration of the contaminated data.
Besides, HSCL is an end-to-end learning approach that can efficiently learn
discriminative representations without fine-tuning. HSCL achieves
state-of-the-art performance in multiple scenarios, such as one-class
classification and cross-dataset detection. Extensive ablation studies further
verify the effectiveness of each considered relation. The code is available at
https://github.com/GaoangW/HSCL.",https://github.com/GaoangW/HSCL,35971
2a023d85-71f8-4711-9b88-d2602103e2a3,HLT-MT: High-resource Language-specific Training for Multilingual Neural Machine Translation,0.620594,"Multilingual neural machine translation (MNMT) trained in multiple language
pairs has attracted considerable attention due to fewer model parameters and
lower training costs by sharing knowledge among multiple languages.
Nonetheless, multilingual training is plagued by language interference
degeneration in shared parameters because of the negative interference among
different translation directions, especially on high-resource languages. In
this paper, we propose the multilingual translation model with the
high-resource language-specific training (HLT-MT) to alleviate the negative
interference, which adopts the two-stage training with the language-specific
selection mechanism. Specifically, we first train the multilingual model only
with the high-resource pairs and select the language-specific modules at the
top of the decoder to enhance the translation quality of high-resource
directions. Next, the model is further trained on all available corpora to
transfer knowledge from high-resource languages (HRLs) to low-resource
languages (LRLs). Experimental results show that HLT-MT outperforms various
strong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic
experiments validate the effectiveness of our method in mitigating the negative
interference in multilingual training.",None,97925
1d219a0e-90ef-4c5d-923d-05357845afa2,Revisiting Facial Key Point Detection: An Efficient Approach Using Deep Neural Networks,0.462075,"Facial landmark detection is a widely researched field of deep learning as
this has a wide range of applications in many fields. These key points are
distinguishing characteristic points on the face, such as the eyes center, the
eye's inner and outer corners, the mouth center, and the nose tip from which
human emotions and intent can be explained. The focus of our work has been
evaluating transfer learning models such as MobileNetV2 and NasNetMobile,
including custom CNN architectures. The objective of the research has been to
develop efficient deep learning models in terms of model size, parameters, and
inference time and to study the effect of augmentation imputation and
fine-tuning on these models. It was found that while augmentation techniques
produced lower RMSE scores than imputation techniques, they did not affect the
inference time. MobileNetV2 architecture produced the lowest RMSE and inference
time. Moreover, our results indicate that manually optimized CNN architectures
performed similarly to Auto Keras tuned architecture. However, manually
optimized architectures yielded better inference time and training curves.",None,-1
e8019c56-bb77-488b-87d3-f0b9ae5b86ef,Accurate Instance-Level CAD Model Retrieval in a Large-Scale Database,0.148365,"We present a new solution to the fine-grained retrieval of clean CAD models
from a large-scale database in order to recover detailed object shape
geometries for RGBD scans. Unlike previous work simply indexing into a
moderately small database using an object shape descriptor and accepting the
top retrieval result, we argue that in the case of a large-scale database a
more accurate model may be found within a neighborhood of the descriptor. More
importantly, we propose that the distinctiveness deficiency of shape
descriptors at the instance level can be compensated by a geometry-based
re-ranking of its neighborhood. Our approach first leverages the discriminative
power of learned representations to distinguish between different categories of
models and then uses a novel robust point set distance metric to re-rank the
CAD neighborhood, enabling fine-grained retrieval in a large shape database.
Evaluation on a real-world dataset shows that our geometry-based re-ranking is
a conceptually simple but highly effective method that can lead to a
significant improvement in retrieval accuracy compared to the state-of-the-art.",None,-1
14fb4035-eecd-4b4a-a84c-97c4b5823d46,Third Time's the Charm? Image and Video Editing with StyleGAN3,0.894966,"StyleGAN is arguably one of the most intriguing and well-studied generative
models, demonstrating impressive performance in image generation, inversion,
and manipulation. In this work, we explore the recent StyleGAN3 architecture,
compare it to its predecessor, and investigate its unique advantages, as well
as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained
on unaligned data, one can still use aligned data for training, without
hindering the ability to generate unaligned imagery. Next, our analysis of the
disentanglement of the different latent spaces of StyleGAN3 indicates that the
commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts,
underscoring the benefits of using the StyleSpace for fine-grained editing.
Considering image inversion, we observe that existing encoder-based techniques
struggle when trained on unaligned data. We therefore propose an encoding
scheme trained solely on aligned data, yet can still invert unaligned images.
Finally, we introduce a novel video inversion and editing workflow that
leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce
texture sticking and expand the field of view of the edited video.",https://yuval-alaluf.github.io/stylegan3-editing/,62088
9f027b82-59fd-470c-840c-acfd4df45c5d,Registration based Few-Shot Anomaly Detection,0.998717,"This paper considers few-shot anomaly detection (FSAD), a practical yet
under-studied setting for anomaly detection (AD), where only a limited number
of normal images are provided for each category at training. So far, existing
FSAD studies follow the one-model-per-category learning paradigm used for
standard AD, and the inter-category commonality has not been explored. Inspired
by how humans detect anomalies, i.e., comparing an image in question to normal
images, we here leverage registration, an image alignment task that is
inherently generalizable across categories, as the proxy task, to train a
category-agnostic anomaly detection model. During testing, the anomalies are
identified by comparing the registered features of the test image and its
corresponding support (normal) images. As far as we know, this is the first
FSAD method that trains a single generalizable model and requires no
re-training or parameter fine-tuning for new categories. Experimental results
have shown that the proposed method outperforms the state-of-the-art FSAD
methods by 3%-8% in AUC on the MVTec and MPDD benchmarks.",https://github.com/MediaBrain-SJTU/RegAD,-1
f0521768-5306-4bff-8e36-01b8fb79208d,Dynamic Collaborative Multi-Agent Reinforcement Learning Communication for Autonomous Drone Reforestation,0.549418,"We approach autonomous drone-based reforestation with a collaborative
multi-agent reinforcement learning (MARL) setup. Agents can communicate as part
of a dynamically changing network. We explore collaboration and communication
on the back of a high-impact problem. Forests are the main resource to control
rising CO2 conditions. Unfortunately, the global forest volume is decreasing at
an unprecedented rate. Many areas are too large and hard to traverse to plant
new trees. To efficiently cover as much area as possible, here we propose a
Graph Neural Network (GNN) based communication mechanism that enables
collaboration. Agents can share location information on areas needing
reforestation, which increases viewed area and planted tree count. We compare
our proposed communication mechanism with a multi-agent baseline without the
ability to communicate. Results show how communication enables collaboration
and increases collective performance, planting precision and the risk-taking
propensity of individual agents.",None,-1
6cf056c0-65d7-4003-9c08-974ed5a369b8,Content and Style Aware Generation of Text-line Images for Handwriting Recognition,0.790069,"Handwritten Text Recognition has achieved an impressive performance in public
benchmarks. However, due to the high inter- and intra-class variability between
handwriting styles, such recognizers need to be trained using huge volumes of
manually labeled training data. To alleviate this labor-consuming problem,
synthetic data produced with TrueType fonts has been often used in the training
loop to gain volume and augment the handwriting style variability. However,
there is a significant style bias between synthetic and real data which hinders
the improvement of recognition performance. To deal with such limitations, we
propose a generative method for handwritten text-line images, which is
conditioned on both visual appearance and textual content. Our method is able
to produce long text-line samples with diverse handwriting styles. Once
properly trained, our method can also be adapted to new target data by only
accessing unlabeled text-line images to mimic handwritten styles and produce
images with any textual content. Extensive experiments have been done on making
use of the generated samples to boost Handwritten Text Recognition performance.
Both qualitative and quantitative results demonstrate that the proposed
approach outperforms the current state of the art.",https://github.com/kaonashi-tyc/zi2zi,-1
28b26f83-ed7d-41a6-b2d9-8efe59bca704,Learning Sparse Latent Representations for Generator Model,0.0468538,"Sparsity is a desirable attribute. It can lead to more efficient and more
effective representations compared to the dense model. Meanwhile, learning
sparse latent representations has been a challenging problem in the field of
computer vision and machine learning due to its complexity. In this paper, we
present a new unsupervised learning method to enforce sparsity on the latent
space for the generator model with a gradually sparsified spike and slab
distribution as our prior. Our model consists of only one top-down generator
network that maps the latent variable to the observed data. Latent variables
can be inferred following generator posterior direction using non-persistent
gradient based method. Spike and Slab regularization in the inference step can
push non-informative latent dimensions towards zero to induce sparsity.
Extensive experiments show the model can preserve majority of the information
from original images with sparse representations while demonstrating improved
results compared to other existing methods. We observe that our model can learn
disentangled semantics and increase explainability of the latent codes while
boosting the robustness in the task of classification and denoising.",None,-1
c6f4cb98-de53-465e-8ec2-87aafb538547,Diffusion Posterior Sampling for General Noisy Inverse Problems,0.999829,"Diffusion models have been recently studied as powerful generative inverse
problem solvers, owing to their high quality reconstructions and the ease of
combining existing iterative solvers. However, most works focus on solving
simple linear inverse problems in noiseless settings, which significantly
under-represents the complexity of real-world problems. In this work, we extend
diffusion solvers to efficiently handle general noisy (non)linear inverse
problems via approximation of the posterior sampling. Interestingly, the
resulting posterior sampling scheme is a blended version of diffusion sampling
with the manifold constrained gradient without a strict measurement consistency
projection step, yielding a more desirable generative path in noisy settings
compared to the previous studies. Our method demonstrates that diffusion models
can incorporate various measurement noise statistics such as Gaussian and
Poisson, and also efficiently handle noisy nonlinear inverse problems such as
Fourier phase retrieval and non-uniform deblurring. Code available at
https://github.com/DPS2022/diffusion-posterior-sampling",https://github.com/DPS2022/diffusion-posterior-sampling,-1
825fc8e2-8b3d-44ae-9bb0-a366e67f33f2,Robust Trajectory Prediction against Adversarial Attacks,0.790037,"Trajectory prediction using deep neural networks (DNNs) is an essential
component of autonomous driving (AD) systems. However, these methods are
vulnerable to adversarial attacks, leading to serious consequences such as
collisions. In this work, we identify two key ingredients to defend trajectory
prediction models against adversarial attacks including (1) designing effective
adversarial training methods and (2) adding domain-specific data augmentation
to mitigate the performance degradation on clean data. We demonstrate that our
method is able to improve the performance by 46% on adversarial data and at the
cost of only 3% performance degradation on clean data, compared to the model
trained with clean data. Additionally, compared to existing robust methods, our
method can improve performance by 21% on adversarial examples and 9% on clean
data. Our robust model is evaluated with a planner to study its downstream
impacts. We demonstrate that our model can significantly reduce the severe
accident rates (e.g., collisions and off-road driving).",https://robustav.github.io/RobustTraj,-1
3f475743-ca52-4c3f-b85a-a99cad0050d0,TRUST XAI: Model-Agnostic Explanations for AI With a Case Study on IIoT Security,0.913296,"Despite AI's significant growth, its ""black box"" nature creates challenges in
generating adequate trust. Thus, it is seldom utilized as a standalone unit in
IoT high-risk applications, such as critical industrial infrastructures,
medical systems, and financial applications, etc. Explainable AI (XAI) has
emerged to help with this problem. However, designing appropriately fast and
accurate XAI is still challenging, especially in numerical applications. Here,
we propose a universal XAI model named Transparency Relying Upon Statistical
Theory (TRUST), which is model-agnostic, high-performing, and suitable for
numerical applications. Simply put, TRUST XAI models the statistical behavior
of the AI's outputs in an AI-based system. Factor analysis is used to transform
the input features into a new set of latent variables. We use mutual
information to rank these variables and pick only the most influential ones on
the AI's outputs and call them ""representatives"" of the classes. Then we use
multi-modal Gaussian distributions to determine the likelihood of any new
sample belonging to each class. We demonstrate the effectiveness of TRUST in a
case study on cybersecurity of the industrial Internet of things (IIoT) using
three different cybersecurity datasets. As IIoT is a prominent application that
deals with numerical data. The results show that TRUST XAI provides
explanations for new random samples with an average success rate of 98%.
Compared with LIME, a popular XAI model, TRUST is shown to be superior in the
context of performance, speed, and the method of explainability. In the end, we
also show how TRUST is explained to the user.",None,7250
fbf96439-4465-4592-828f-3143fdcfcb05,Unpacking Large Language Models with Conceptual Consistency,0.418932,"If a Large Language Model (LLM) answers ""yes"" to the question ""Are mountains
tall?"" then does it know what a mountain is? Can you rely on it responding
correctly or incorrectly to other questions about mountains? The success of
Large Language Models (LLMs) indicates they are increasingly able to answer
queries like these accurately, but that ability does not necessarily imply a
general understanding of concepts relevant to the anchor query. We propose
conceptual consistency to measure a LLM's understanding of relevant concepts.
This novel metric measures how well a model can be characterized by finding out
how consistent its responses to queries about conceptually relevant background
knowledge are. To compute it we extract background knowledge by traversing
paths between concepts in a knowledge base and then try to predict the model's
response to the anchor query from the background knowledge. We investigate the
performance of current LLMs in a commonsense reasoning setting using the CSQA
dataset and the ConceptNet knowledge base. While conceptual consistency, like
other metrics, does increase with the scale of the LLM used, we find that
popular models do not necessarily have high conceptual consistency. Our
analysis also shows significant variation in conceptual consistency across
different kinds of relations, concepts, and prompts. This serves as a step
toward building models that humans can apply a theory of mind to, and thus
interact with intuitively.",None,-1
bf7a31e8-17af-40d1-b41a-cb8a4faa83f9,Is Conditional Generative Modeling all you need for Decision-Making?,0.988665,"Recent improvements in conditional generative modeling have made it possible
to generate high-quality images from language descriptions alone. We
investigate whether these methods can directly address the problem of
sequential decision-making. We view decision-making not through the lens of
reinforcement learning (RL), but rather through conditional generative
modeling. To our surprise, we find that our formulation leads to policies that
can outperform existing offline RL approaches across standard benchmarks. By
modeling a policy as a return-conditional diffusion model, we illustrate how we
may circumvent the need for dynamic programming and subsequently eliminate many
of the complexities that come with traditional offline RL. We further
demonstrate the advantages of modeling policies as conditional diffusion models
by considering two other conditioning variables: constraints and skills.
Conditioning on a single constraint or skill during training leads to behaviors
at test-time that can satisfy several constraints together or demonstrate a
composition of skills. Our results illustrate that conditional generative
modeling is a powerful tool for decision-making.",https://github.com/jannerm/diffuser,-1
ef4076ae-3d88-433a-a47a-985e58cbdd60,RMGN: A Regional Mask Guided Network for Parser-free Virtual Try-on,0.693199,"Virtual try-on(VTON) aims at fitting target clothes to reference person
images, which is widely adopted in e-commerce.Existing VTON approaches can be
narrowly categorized into Parser-Based(PB) and Parser-Free(PF) by whether
relying on the parser information to mask the persons' clothes and synthesize
try-on images. Although abandoning parser information has improved the
applicability of PF methods, the ability of detail synthesizing has also been
sacrificed. As a result, the distraction from original cloth may persistin
synthesized images, especially in complicated postures and high resolution
applications. To address the aforementioned issue, we propose a novel PF method
named Regional Mask Guided Network(RMGN). More specifically, a regional mask is
proposed to explicitly fuse the features of target clothes and reference
persons so that the persisted distraction can be eliminated. A posture
awareness loss and a multi-level feature extractor are further proposed to
handle the complicated postures and synthesize high resolution images.
Extensive experiments demonstrate that our proposed RMGN outperforms both
state-of-the-art PB and PF methods.Ablation studies further verify the
effectiveness ofmodules in RMGN.",https://github.com/jokerlc/RMGN-VITON,-1
7f1e4a78-e4a7-4932-ab4e-b9b0f3551389,Predicting Future Occupancy Grids in Dynamic Environment with Spatio-Temporal Learning,0.685907,"Reliably predicting future occupancy of highly dynamic urban environments is
an important precursor for safe autonomous navigation. Common challenges in the
prediction include forecasting the relative position of other vehicles,
modelling the dynamics of vehicles subjected to different traffic conditions,
and vanishing surrounding objects. To tackle these challenges, we propose a
spatio-temporal prediction network pipeline that takes the past information
from the environment and semantic labels separately for generating future
occupancy predictions. Compared to the current SOTA, our approach predicts
occupancy for a longer horizon of 3 seconds and in a relatively complex
environment from the nuScenes dataset. Our experimental results demonstrate the
ability of spatio-temporal networks to understand scene dynamics without the
need for HD-Maps and explicit modeling dynamic objects. We publicly release our
occupancy grid dataset based on nuScenes to support further research.",https://github.com/ksm26/SpatioTemporal-Predictions,-1
97bcd73f-1288-4721-bf0f-d1a2cc1979f9,Continual Learning Based on OOD Detection and Task Masking,0.258442,"Existing continual learning techniques focus on either task incremental
learning (TIL) or class incremental learning (CIL) problem, but not both. CIL
and TIL differ mainly in that the task-id is provided for each test sample
during testing for TIL, but not provided for CIL. Continual learning methods
intended for one problem have limitations on the other problem. This paper
proposes a novel unified approach based on out-of-distribution (OOD) detection
and task masking, called CLOM, to solve both problems. The key novelty is that
each task is trained as an OOD detection model rather than a traditional
supervised learning model, and a task mask is trained to protect each task to
prevent forgetting. Our evaluation shows that CLOM outperforms existing
state-of-the-art baselines by large margins. The average TIL/CIL accuracy of
CLOM over six experiments is 87.6/67.9% while that of the best baselines is
only 82.4/55.0%.",https://github.com/k-gyuhak/CLOM,-1
fc5c238f-4d15-4e53-9293-0edf1d1e2f8f,"To Softmax, or not to Softmax: that is the question when applying Active Learning for Transformer Models",0.31508,"Despite achieving state-of-the-art results in nearly all Natural Language
Processing applications, fine-tuning Transformer-based language models still
requires a significant amount of labeled data to work. A well known technique
to reduce the amount of human effort in acquiring a labeled dataset is
\textit{Active Learning} (AL): an iterative process in which only the minimal
amount of samples is labeled. AL strategies require access to a quantified
confidence measure of the model predictions. A common choice is the softmax
activation function for the final layer. As the softmax function provides
misleading probabilities, this paper compares eight alternatives on seven
datasets. Our almost paradoxical finding is that most of the methods are too
good at identifying the true most uncertain samples (outliers), and that
labeling therefore exclusively outliers results in worse performance. As a
heuristic we propose to systematically ignore samples, which results in
improvements of various methods compared to the softmax function.",https://github.com/jgonsior/btw-softmax-clipping,-1
aaf8b1fa-20f3-4db2-945a-c3bbd90de337,Introducing BEREL: BERT Embeddings for Rabbinic-Encoded Language,0.615917,"We present a new pre-trained language model (PLM) for Rabbinic Hebrew, termed
Berel (BERT Embeddings for Rabbinic-Encoded Language). Whilst other PLMs exist
for processing Hebrew texts (e.g., HeBERT, AlephBert), they are all trained on
modern Hebrew texts, which diverges substantially from Rabbinic Hebrew in terms
of its lexicographical, morphological, syntactic and orthographic norms. We
demonstrate the superiority of Berel on Rabbinic texts via a challenge set of
Hebrew homographs. We release the new model and homograph challenge set for
unrestricted use.",None,-1
ee98548c-763c-4b30-b0de-4dbff1822302,SEAT: Stable and Explainable Attention,0.3239,"Currently, attention mechanism becomes a standard fixture in most
state-of-the-art natural language processing (NLP) models, not only due to
outstanding performance it could gain, but also due to plausible innate
explanation for the behaviors of neural architectures it provides, which is
notoriously difficult to analyze. However, recent studies show that attention
is unstable against randomness and perturbations during training or testing,
such as random seeds and slight perturbation of embedding vectors, which
impedes it from becoming a faithful explanation tool. Thus, a natural question
is whether we can find some substitute of the current attention which is more
stable and could keep the most important characteristics on explanation and
prediction of attention. In this paper, to resolve the problem, we provide a
first rigorous definition of such alternate namely SEAT (Stable and Explainable
Attention). Specifically, a SEAT should has the following three properties: (1)
Its prediction distribution is enforced to be close to the distribution based
on the vanilla attention; (2) Its top-k indices have large overlaps with those
of the vanilla attention; (3) It is robust w.r.t perturbations, i.e., any
slight perturbation on SEAT will not change the prediction distribution too
much, which implicitly indicates that it is stable to randomness and
perturbations. Finally, through intensive experiments on various datasets, we
compare our SEAT with other baseline methods using RNN, BiLSTM and BERT
architectures via six different evaluation metrics for model interpretation,
stability and accuracy. Results show that SEAT is more stable against different
perturbations and randomness while also keeps the explainability of attention,
which indicates it is a more faithful explanation. Moreover, compared with
vanilla attention, there is almost no utility (accuracy) degradation for SEAT.",None,-1
94625059-62c4-4285-a5c2-a5051aad85d9,Adaptation Approaches for Nearest Neighbor Language Models,0.371983,"Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced
impressive gains over purely parametric LMs, by leveraging large-scale
neighborhood retrieval over external memory datastores. However, there has been
little investigation into adapting such models for new domains. This work
attempts to fill that gap and suggests the following approaches for adapting
$k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding
neighborhood retrieval over an additional adaptation datastore, and 3) adapting
the weights (scores) of retrieved neighbors using a learned Rescorer module. We
study each adaptation strategy separately, as well as the combined performance
improvement through ablation experiments and an extensive set of evaluations
run over seven adaptation domains. Our combined adaptation approach
consistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM)
baselines that construct datastores from the adaptation data. On average, we
see perplexity improvements of 17.1% and 16% for these respective baselines,
across domains.",None,-1
6fdab619-fdc6-4a57-81d3-df329b7279e4,Otsu based Differential Evolution Method for Image Segmentation,0.0281449,"This paper proposes an OTSU based differential evolution method for satellite
image segmentation and compares it with four other methods such as Modified
Artificial Bee Colony Optimizer (MABC), Artificial Bee Colony (ABC), Genetic
Algorithm (GA), and Particle Swarm Optimization (PSO) using the objective
function proposed by Otsu for optimal multilevel thresholding. The experiments
conducted and their results illustrate that our proposed DE and OTSU algorithm
segmentation can effectively and precisely segment the input image, close to
results obtained by the other methods. In the proposed DE and OTSU algorithm,
instead of passing the fitness function variables, the entire image is passed
as an input to the DE algorithm after obtaining the threshold values for the
input number of levels in the OTSU algorithm. The image segmentation results
are obtained after learning about the image instead of learning about the
fitness variables. In comparison to other segmentation methods examined, the
proposed DE and OTSU algorithm yields promising results with minimized
computational time compared to some algorithms.",None,-1
45fb04c4-2cd7-4df1-a6aa-d70e100626b7,Unsupervised Image-to-Image Translation with Generative Prior,0.839001,"Unsupervised image-to-image translation aims to learn the translation between
two visual domains without paired data. Despite the recent progress in image
translation models, it remains challenging to build mappings between complex
domains with drastic visual discrepancies. In this work, we present a novel
framework, Generative Prior-guided UNsupervised Image-to-image Translation
(GP-UNIT), to improve the overall quality and applicability of the translation
algorithm. Our key insight is to leverage the generative prior from pre-trained
class-conditional GANs (e.g., BigGAN) to learn rich content correspondences
across various domains. We propose a novel coarse-to-fine scheme: we first
distill the generative prior to capture a robust coarse-level content
representation that can link objects at an abstract semantic level, based on
which fine-level content features are adaptively learned for more accurate
multi-level content correspondences. Extensive experiments demonstrate the
superiority of our versatile framework over state-of-the-art methods in robust,
high-quality and diversified translations, even for challenging and distant
domains.",https://github.com/williamyang1991/GP-UNIT,-1
36ae12af-e3ba-4b96-9f20-8bead6179b73,Investigating data partitioning strategies for crosslinguistic low-resource ASR evaluation,0.473378,"Many automatic speech recognition (ASR) data sets include a single
pre-defined test set consisting of one or more speakers whose speech never
appears in the training set. This ""hold-speaker(s)-out"" data partitioning
strategy, however, may not be ideal for data sets in which the number of
speakers is very small. This study investigates ten different data split
methods for five languages with minimal ASR training resources. We find that
(1) model performance varies greatly depending on which speaker is selected for
testing; (2) the average word error rate (WER) across all held-out speakers is
comparable not only to the average WER over multiple random splits but also to
any given individual random split; (3) WER is also generally comparable when
the data is split heuristically or adversarially; (4) utterance duration and
intensity are comparatively more predictive factors of variability regardless
of the data split. These results suggest that the widely used hold-speakers-out
approach to ASR data partitioning can yield results that do not reflect model
performance on unseen data or speakers. Random splits can yield more reliable
and generalizable estimates when facing data sparsity.",None,1027
565b8466-2fa9-43d2-8db8-f88ee465c100,Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Conservative Natural Policy Gradient Primal-Dual Algorithm,0.521509,"We consider the problem of constrained Markov decision process (CMDP) in
continuous state-actions spaces where the goal is to maximize the expected
cumulative reward subject to some constraints. We propose a novel Conservative
Natural Policy Gradient Primal-Dual Algorithm (C-NPG-PD) to achieve zero
constraint violation while achieving state of the art convergence results for
the objective value function. For general policy parametrization, we prove
convergence of value function to global optimal upto an approximation error due
to restricted policy class. We even improve the sample complexity of existing
constrained NPG-PD algorithm \cite{Ding2020} from $\mathcal{O}(1/\epsilon^6)$
to $\mathcal{O}(1/\epsilon^4)$. To the best of our knowledge, this is the first
work to establish zero constraint violation with Natural policy gradient style
algorithms for infinite horizon discounted CMDPs. We demonstrate the merits of
proposed algorithm via experimental evaluations.",None,-1
0ac0d690-c998-4fd6-b11f-743c0ac8fee8,Rethinking Audio-visual Synchronization for Active Speaker Detection,0.404529,"Active speaker detection (ASD) systems are important modules for analyzing
multi-talker conversations. They aim to detect which speakers or none are
talking in a visual scene at any given time. Existing research on ASD does not
agree on the definition of active speakers. We clarify the definition in this
work and require synchronization between the audio and visual speaking
activities. This clarification of definition is motivated by our extensive
experiments, through which we discover that existing ASD methods fail in
modeling the audio-visual synchronization and often classify unsynchronized
videos as active speaking. To address this problem, we propose a cross-modal
contrastive learning strategy and apply positional encoding in attention
modules for supervised ASD models to leverage the synchronization cue.
Experimental results suggest that our model can successfully detect
unsynchronized speaking as not speaking, addressing the limitation of current
models.",https://github.com/urkax/SyncTalkNet,30069
dabcc7ac-3f13-4341-84b7-9d9c119d5915,MAESTRO: Matched Speech Text Representations through Modality Matching,0.842436,"We present Maestro, a self-supervised training method to unify
representations learnt from speech and text modalities. Self-supervised
learning from speech signals aims to learn the latent structure inherent in the
signal, while self-supervised learning from text attempts to capture lexical
information. Learning aligned representations from unpaired speech and text
sequences is a challenging task. Previous work either implicitly enforced the
representations learnt from these two modalities to be aligned in the latent
space through multitasking and parameter sharing or explicitly through
conversion of modalities via speech synthesis. While the former suffers from
interference between the two modalities, the latter introduces additional
complexity. In this paper, we propose Maestro, a novel algorithm to learn
unified representations from both these modalities simultaneously that can
transfer to diverse downstream tasks such as Automated Speech Recognition (ASR)
and Speech Translation (ST). Maestro learns unified representations through
sequence alignment, duration prediction and matching embeddings in the learned
space through an aligned masked-language model loss. We establish a new
state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative
reduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)
and 21 languages to English multilingual ST on CoVoST 2 with an improvement of
2.8 BLEU averaged over 21 languages.",None,-1
240aa5eb-8f44-4268-808a-4d06b7c72a9f,Anomaly localization for copy detection patterns through print estimations,0.223598,"Copy detection patterns (CDP) are recent technologies for protecting products
from counterfeiting. However, in contrast to traditional copy fakes, deep
learning-based fakes have shown to be hardly distinguishable from originals by
traditional authentication systems. Systems based on classical supervised
learning and digital templates assume knowledge of fake CDP at training time
and cannot generalize to unseen types of fakes. Authentication based on printed
copies of originals is an alternative that yields better results even for
unseen fakes and simple authentication metrics but comes at the impractical
cost of acquisition and storage of printed copies. In this work, to overcome
these shortcomings, we design a machine learning (ML) based authentication
system that only requires digital templates and printed original CDP for
training, whereas authentication is based solely on digital templates, which
are used to estimate original printed codes. The obtained results show that the
proposed system can efficiently authenticate original and detect fake CDP by
accurately locating the anomalies in the fake CDP. The empirical evaluation of
the authentication system under investigation is performed on the original and
ML-based fakes CDP printed on two industrial printers.",https://gitlab.unige.ch/Brian.Pulfer/,-1
b7019aaf-3ac2-42a2-8f29-900559e2b5e5,Collective Relevance Labeling for Passage Retrieval,0.384779,"Deep learning for Information Retrieval (IR) requires a large amount of
high-quality query-document relevance labels, but such labels are inherently
sparse. Label smoothing redistributes some observed probability mass over
unobserved instances, often uniformly, uninformed of the true distribution. In
contrast, we propose knowledge distillation for informed labeling, without
incurring high computation overheads at evaluation time. Our contribution is
designing a simple but efficient teacher model which utilizes collective
knowledge, to outperform state-of-the-arts distilled from a more complex
teacher model. Specifically, we train up to x8 faster than the state-of-the-art
teacher, while distilling the rankings better. Our code is publicly available
at https://github.com/jihyukkim-nlp/CollectiveKD",https://github.com/jihyukkim-nlp/CollectiveKD,-1
ed84bdb8-115c-42fc-8575-e86a37187ac1,Probing Pre-Trained Language Models for Cross-Cultural Differences in Values,0.997702,"Language embeds information about social, cultural, and political values
people hold. Prior work has explored social and potentially harmful biases
encoded in Pre-Trained Language models (PTLMs). However, there has been no
systematic study investigating how values embedded in these models vary across
cultures. In this paper, we introduce probes to study which values across
cultures are embedded in these models, and whether they align with existing
theories and cross-cultural value surveys. We find that PTLMs capture
differences in values across cultures, but those only weakly align with
established value surveys. We discuss implications of using mis-aligned models
in cross-cultural settings, as well as ways of aligning PTLMs with value
surveys.",None,-1
28be8683-fdbe-44cf-b532-bcae33d8f956,Capturing Failures of Large Language Models via Human Cognitive Biases,0.946232,"Large language models generate complex, open-ended outputs: instead of
outputting a class label they write summaries, generate dialogue, or produce
working code. In order to asses the reliability of these open-ended generation
systems, we aim to identify qualitative categories of erroneous behavior,
beyond identifying individual errors. To hypothesize and test for such
qualitative errors, we draw inspiration from human cognitive biases --
systematic patterns of deviation from rational judgement. Specifically, we use
cognitive biases as motivation to (i) generate hypotheses for problems that
models may have, and (ii) develop experiments that elicit these problems. Using
code generation as a case study, we find that OpenAI's Codex errs predictably
based on how the input prompt is framed, adjusts outputs towards anchors, and
is biased towards outputs that mimic frequent training examples. We then use
our framework to elicit high-impact errors such as incorrectly deleting files.
Our results indicate that experimental methodology from cognitive science can
help characterize how machine learning systems behave.",https://github.com/ejones313/codex-cog-biases,-1
2ac01af6-efd4-4b45-bfd5-4c4753bbbd26,Learning to Fold Real Garments with One Arm: A Case Study in Cloud-Based Robotics Research,0.469098,"Autonomous fabric manipulation is a longstanding challenge in robotics, but
evaluating progress is difficult due to the cost and diversity of robot
hardware. Using Reach, a cloud robotics platform that enables low-latency
remote execution of control policies on physical robots, we present the first
systematic benchmarking of fabric manipulation algorithms on physical hardware.
We develop 4 novel learning-based algorithms that model expert actions,
keypoints, reward functions, and dynamic motions, and we compare these against
4 learning-free and inverse dynamics algorithms on the task of folding a
crumpled T-shirt with a single robot arm. The entire lifecycle of data
collection, model training, and policy evaluation is performed remotely without
physical access to the robot workcell. Results suggest a new algorithm
combining imitation learning with analytic methods achieves 84% of human-level
performance on the folding task. See
https://sites.google.com/berkeley.edu/cloudfolding for all data, code, models,
and supplemental material.",https://github.com/google-research/pyreach,167441
80d7d81b-bb0f-4734-b752-1f79a887d519,Prompt-Based Metric Learning for Few-Shot NER,0.756352,"Few-shot named entity recognition (NER) targets generalizing to unseen labels
and/or domains with few labeled examples. Existing metric learning methods
compute token-level similarities between query and support sets, but are not
able to fully incorporate label semantics into modeling. To address this issue,
we propose a simple method to largely improve metric learning for NER: 1)
multiple prompt schemas are designed to enhance label semantics; 2) we propose
a novel architecture to effectively combine multiple prompt-based
representations. Empirically, our method achieves new state-of-the-art (SOTA)
results under 16 of the 18 considered settings, substantially outperforming the
previous SOTA by an average of 8.84% and a maximum of 34.51% in relative gains
of micro F1. Our code is available at https://github.com/AChen-qaq/ProML.",https://github.com/AChen-qaq/ProML,-1
c6b71491-7ab6-4149-81d0-145941a82bec,Are AlphaZero-like Agents Robust to Adversarial Perturbations?,0.519085,"The success of AlphaZero (AZ) has demonstrated that neural-network-based Go
AIs can surpass human performance by a large margin. Given that the state space
of Go is extremely large and a human player can play the game from any legal
state, we ask whether adversarial states exist for Go AIs that may lead them to
play surprisingly wrong actions. In this paper, we first extend the concept of
adversarial examples to the game of Go: we generate perturbed states that are
``semantically'' equivalent to the original state by adding meaningless moves
to the game, and an adversarial state is a perturbed state leading to an
undoubtedly inferior action that is obvious even for Go beginners. However,
searching the adversarial state is challenging due to the large, discrete, and
non-differentiable search space. To tackle this challenge, we develop the first
adversarial attack on Go AIs that can efficiently search for adversarial states
by strategically reducing the search space. This method can also be extended to
other board games such as NoGo. Experimentally, we show that the actions taken
by both Policy-Value neural network (PV-NN) and Monte Carlo tree search (MCTS)
can be misled by adding one or two meaningless stones; for example, on 58\% of
the AlphaGo Zero self-play games, our method can make the widely used KataGo
agent with 50 simulations of MCTS plays a losing action by adding two
meaningless stones. We additionally evaluated the adversarial examples found by
our algorithm with amateur human Go players and 90\% of examples indeed lead
the Go agent to play an obviously inferior action. Our code is available at
\url{https://PaperCode.cc/GoAttack}.",https://PaperCode.cc/GoAttack,-1
0e392599-7767-48a5-aed1-97f4264db51b,UnShadowNet: Illumination Critic Guided Contrastive Learning For Shadow Removal,0.084514,"Shadows are frequently encountered natural phenomena that significantly
hinder the performance of computer vision perception systems in practical
settings, e.g., autonomous driving. A solution to this would be to eliminate
shadow regions from the images before the processing of the perception system.
Yet, training such a solution requires pairs of aligned shadowed and
non-shadowed images which are difficult to obtain. We introduce a novel weakly
supervised shadow removal framework UnShadowNet trained using contrastive
learning. It is composed of a DeShadower network responsible for the removal of
the extracted shadow under the guidance of an Illumination network which is
trained adversarially by the illumination critic and a Refinement network to
further remove artefacts. We show that UnShadowNet can be easily extended to a
fully-supervised set-up to exploit the ground-truth when available. UnShadowNet
outperforms existing state-of-the-art approaches on three publicly available
shadow datasets (ISTD, adjusted ISTD, SRD) in both the weakly and fully
supervised setups.",None,7532
4f55eb89-bbd0-4a3a-8ebd-3fd85c353d7a,Non-Parametric Stochastic Policy Gradient with Strategic Retreat for Non-Stationary Environment,0.215762,"In modern robotics, effectively computing optimal control policies under
dynamically varying environments poses substantial challenges to the
off-the-shelf parametric policy gradient methods, such as the Deep
Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic policy
gradient (TD3). In this paper, we propose a systematic methodology to
dynamically learn a sequence of optimal control policies non-parametrically,
while autonomously adapting with the constantly changing environment dynamics.
Specifically, our non-parametric kernel-based methodology embeds a policy
distribution as the features in a non-decreasing Euclidean space, therefore
allowing its search space to be defined as a very high (possible infinite)
dimensional RKHS (Reproducing Kernel Hilbert Space). Moreover, by leveraging
the similarity metric computed in RKHS, we augmented our non-parametric
learning with the technique of AdaptiveH- adaptively selecting a time-frame
window of finishing the optimal part of whole action-sequence sampled on some
preceding observed state. To validate our proposed approach, we conducted
extensive experiments with multiple classic benchmarks and one simulated
robotics benchmark equipped with dynamically changing environments. Overall,
our methodology has outperformed the well-established DDPG and TD3 methodology
by a sizeable margin in terms of learning performance.",None,-1
66f77a3e-6f26-49e0-8bd2-5a94a519fa08,Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation,0.619518,"We consider the problem of multi-agent navigation and collision avoidance
when observations are limited to the local neighborhood of each agent. We
propose InforMARL, a novel architecture for multi-agent reinforcement learning
(MARL) which uses local information intelligently to compute paths for all the
agents in a decentralized manner. Specifically, InforMARL aggregates
information about the local neighborhood of agents for both the actor and the
critic using a graph neural network and can be used in conjunction with any
standard MARL algorithm. We show that (1) in training, InforMARL has better
sample efficiency and performance than baseline approaches, despite using less
information, and (2) in testing, it scales well to environments with arbitrary
numbers of agents and obstacles. We illustrate these results using four task
environments, including one with predetermined goals for each agent, and one in
which the agents collectively try to cover all goals. Code available at
https://github.com/nsidn98/InforMARL.",None,-1
d0b4bed0-cd60-475a-8490-8bf530046b3c,LAMASSU: Streaming Language-Agnostic Multilingual Speech Recognition and Translation Using Neural Transducers,0.0407764,"Automatic speech recognition (ASR) and speech translation (ST) can both use
neural transducers as the model structure. It is thus possible to use a single
transducer model to perform both tasks. In real-world applications, such joint
ASR and ST models may need to be streaming and do not require source language
identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a
streaming language-agnostic multilingual speech recognition and translation
model using neural transducers. Based on the transducer model structure, we
propose four methods, a unified joint and prediction network for multilingual
output, a clustered multilingual encoder, target language identification for
encoder, and connectionist temporal classification regularization. Experimental
results show that LAMASSU not only drastically reduces the model size but also
reaches the performances of monolingual ASR and bilingual ST models.",None,-1
995229c9-7a36-442a-816c-ccc053b6f01b,Learning Appearance-motion Normality for Video Anomaly Detection,0.849839,"Video anomaly detection is a challenging task in the computer vision
community. Most single task-based methods do not consider the independence of
unique spatial and temporal patterns, while two-stream structures lack the
exploration of the correlations. In this paper, we propose spatial-temporal
memories augmented two-stream auto-encoder framework, which learns the
appearance normality and motion normality independently and explores the
correlations via adversarial learning. Specifically, we first design two proxy
tasks to train the two-stream structure to extract appearance and motion
features in isolation. Then, the prototypical features are recorded in the
corresponding spatial and temporal memory pools. Finally, the encoding-decoding
network performs adversarial learning with the discriminator to explore the
correlations between spatial and temporal patterns. Experimental results show
that our framework outperforms the state-of-the-art methods, achieving AUCs of
98.1% and 89.8% on UCSD Ped2 and CUHK Avenue datasets.",None,-1
fb3ba4ea-94f2-4610-bde7-67f5855a62ae,Adversarial Robustness through the Lens of Convolutional Filters,0.54567,"Deep learning models are intrinsically sensitive to distribution shifts in
the input data. In particular, small, barely perceivable perturbations to the
input data can force models to make wrong predictions with high confidence. An
common defense mechanism is regularization through adversarial training which
injects worst-case perturbations back into training to strengthen the decision
boundaries, and to reduce overfitting. In this context, we perform an
investigation of 3x3 convolution filters that form in adversarially-trained
models. Filters are extracted from 71 public models of the linf-RobustBench
CIFAR-10/100 and ImageNet1k leaderboard and compared to filters extracted from
models built on the same architectures but trained without robust
regularization. We observe that adversarially-robust models appear to form more
diverse, less sparse, and more orthogonal convolution filters than their normal
counterparts. The largest differences between robust and normal models are
found in the deepest layers, and the very first convolution layer, which
consistently and predominantly forms filters that can partially eliminate
perturbations, irrespective of the architecture. Data & Project website:
https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens",https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens,-1
1735c614-7f69-4825-a937-9f186c32a423,Natural Image Stitching Using Depth Maps,0.392618,"Natural image stitching (NIS) aims to create one natural-looking mosaic from
two overlapping images that capture the same 3D scene from different viewing
positions. Challenges inevitably arise when the scene is non-planar and the
camera baseline is wide, since parallax becomes not negligible in such cases.
In this paper, we propose a novel NIS method using depth maps, which generates
natural-looking mosaics against parallax in both overlapping and
non-overlapping regions. Firstly, we construct a robust fitting method to
filter out the outliers in feature matches and estimate the epipolar geometry
between input images. Then, we draw a triangulation of the target image and
estimate multiple local homographies, one per triangle, based on the locations
of their vertices, the rectified depth values and the epipolar geometry.
Finally, the warping image is rendered by the backward mapping of piece-wise
homographies. Panorama is then produced via average blending and image
inpainting. Experimental results demonstrate that the proposed method not only
provides accurate alignment in the overlapping regions but also virtual
naturalness in the non-overlapping region.",None,-1
8381af8b-6cb2-4673-afb2-81f80be7e964,Masked Event Modeling: Self-Supervised Pretraining for Event Cameras,0.603911,"Event cameras asynchronously capture brightness changes with low latency,
high temporal resolution, and high dynamic range. However, annotation of event
data is a costly and laborious process, which limits the use of deep learning
methods for classification and other semantic tasks with the event modality. To
reduce the dependency on labeled event data, we introduce Masked Event Modeling
(MEM), a self-supervised framework for events. Our method pretrains a neural
network on unlabeled events, which can originate from any event camera
recording. Subsequently, the pretrained model is finetuned on a downstream
task, leading to a consistent improvement of the task accuracy. For example,
our method reaches state-of-the-art classification accuracy across three
datasets, N-ImageNet, N-Cars, and N-Caltech101, increasing the top-1 accuracy
of previous work by significant margins. When tested on real-world event data,
MEM is even superior to supervised RGB-based pretraining. The models pretrained
with MEM are also label-efficient and generalize well to the dense task of
semantic image segmentation.",https://github.com/tum-vision/mem,-1
93ed2eda-c799-463c-8789-242ad44c052d,Video2IMU: Realistic IMU features and signals from videos,0.307288,"Human Activity Recognition (HAR) from wearable sensor data identifies
movements or activities in unconstrained environments. HAR is a challenging
problem as it presents great variability across subjects. Obtaining large
amounts of labelled data is not straightforward, since wearable sensor signals
are not easy to label upon simple human inspection. In our work, we propose the
use of neural networks for the generation of realistic signals and features
using human activity monocular videos. We show how these generated features and
signals can be utilized, instead of their real counterparts, to train HAR
models that can recognize activities using signals obtained with wearable
sensors. To prove the validity of our methods, we perform experiments on an
activity recognition dataset created for the improvement of industrial work
safety. We show that our model is able to realistically generate virtual sensor
signals and features usable to train a HAR classifier with comparable
performance as the one trained using real sensor data. Our results enable the
use of available, labelled video data for training HAR models to classify
signals from wearable sensors.",None,-1
11c2760a-fae9-4dfb-b835-82b3b37a8bee,4D-StOP: Panoptic Segmentation of 4D LiDAR using Spatio-temporal Object Proposal Generation and Aggregation,0.79649,"In this work, we present a new paradigm, called 4D-StOP, to tackle the task
of 4D Panoptic LiDAR Segmentation. 4D-StOP first generates spatio-temporal
proposals using voting-based center predictions, where each point in the 4D
volume votes for a corresponding center. These tracklet proposals are further
aggregated using learned geometric features. The tracklet aggregation method
effectively generates a video-level 4D scene representation over the entire
space-time volume. This is in contrast to existing end-to-end trainable
state-of-the-art approaches which use spatio-temporal embeddings that are
represented by Gaussian probability distributions. Our voting-based tracklet
generation method followed by geometric feature-based aggregation generates
significantly improved panoptic LiDAR segmentation quality when compared to
modeling the entire 4D volume using Gaussian probability distributions. 4D-StOP
achieves a new state-of-the-art when applied to the SemanticKITTI test dataset
with a score of 63.9 LSTQ, which is a large (+7%) improvement compared to
current best-performing end-to-end trainable methods. The code and pre-trained
models are available at: https://github.com/LarsKreuzberg/4D-StOP.",https://github.com/LarsKreuzberg/4D-StOP,35042
53c57875-4093-4dce-b7f2-bbabe1cf54d0,"""Dummy Grandpa, do you know anything?"": Identifying and Characterizing Ad hominem Fallacy Usage in the Wild",0.164708,"Today, participating in discussions on online forums is extremely commonplace
and these discussions have started rendering a strong influence on the overall
opinion of online users. Naturally, twisting the flow of the argument can have
a strong impact on the minds of naive users, which in the long run might have
socio-political ramifications, for example, winning an election or spreading
targeted misinformation. Thus, these platforms are potentially highly
vulnerable to malicious players who might act individually or as a cohort to
breed fallacious arguments with a motive to sway public opinion. Ad hominem
arguments are one of the most effective forms of such fallacies. Although a
simple fallacy, it is effective enough to sway public debates in offline world
and can be used as a precursor to shutting down the voice of opposition by
slander.
  In this work, we take a first step in shedding light on the usage of ad
hominem fallacies in the wild. First, we build a powerful ad hominem detector
with high accuracy (F1 more than 83%, showing a significant improvement over
prior work), even for datasets for which annotated instances constitute a very
small fraction. We then used our detector on 265k arguments collected from the
online debate forum - CreateDebate. Our crowdsourced surveys validate our
in-the-wild predictions on CreateDebate data (94% match with manual
annotation). Our analysis revealed that a surprising 31.23% of CreateDebate
content contains ad hominem fallacy, and a cohort of highly active users post
significantly more ad hominem to suppress opposing views. Then, our temporal
analysis revealed that ad hominem argument usage increased significantly since
the 2016 US Presidential election, not only for topics like Politics, but also
for Science and Law. We conclude by discussing important implications of our
work to detect and defend against ad hominem fallacies.",None,-1
e404a12c-273e-4d14-95c7-90b49d24277e,Audiogram Digitization Tool for Audiological Reports,0.227893,"A number of private and public insurers compensate workers whose hearing loss
can be directly attributed to excessive exposure to noise in the workplace. The
claim assessment process is typically lengthy and requires significant effort
from human adjudicators who must interpret hand-recorded audiograms, often sent
via fax or equivalent. In this work, we present a solution developed in
partnership with the Workplace Safety Insurance Board of Ontario to streamline
the adjudication process. In particular, we present the first audiogram
digitization algorithm capable of automatically extracting the hearing
thresholds from a scanned or faxed audiology report as a proof-of-concept. The
algorithm extracts most thresholds within 5 dB accuracy, allowing to
substantially lessen the time required to convert an audiogram into digital
format in a semi-supervised fashion, and is a first step towards the automation
of the adjudication process. The source code for the digitization algorithm and
a desktop-based implementation of our NIHL annotation portal is publicly
available on GitHub (https://github.com/GreenCUBIC/AudiogramDigitization).",https://github.com/GreenCUBIC/AudiogramDigitization,-1
26e5c346-b203-4a99-a157-8b45c6a2c1f0,End-to-End Trajectory Distribution Prediction Based on Occupancy Grid Maps,0.487497,"In this paper, we aim to forecast a future trajectory distribution of a
moving agent in the real world, given the social scene images and historical
trajectories. Yet, it is a challenging task because the ground-truth
distribution is unknown and unobservable, while only one of its samples can be
applied for supervising model learning, which is prone to bias. Most recent
works focus on predicting diverse trajectories in order to cover all modes of
the real distribution, but they may despise the precision and thus give too
much credit to unrealistic predictions. To address the issue, we learn the
distribution with symmetric cross-entropy using occupancy grid maps as an
explicit and scene-compliant approximation to the ground-truth distribution,
which can effectively penalize unlikely predictions. In specific, we present an
inverse reinforcement learning based multi-modal trajectory distribution
forecasting framework that learns to plan by an approximate value iteration
network in an end-to-end manner. Besides, based on the predicted distribution,
we generate a small set of representative trajectories through a differentiable
Transformer-based network, whose attention mechanism helps to model the
relations of trajectories. In experiments, our method achieves state-of-the-art
performance on the Stanford Drone Dataset and Intersection Drone Dataset.",https://github.com/Kguo-cs/TDOR,-1
8b589387-0658-4f6b-905c-bd3ea5333efe,Phonemic Representation and Transcription for Speech to Text Applications for Under-resourced Indigenous African Languages: The Case of Kiswahili,0.0797771,"Building automatic speech recognition (ASR) systems is a challenging task,
especially for under-resourced languages that need to construct corpora nearly
from scratch and lack sufficient training data. It has emerged that several
African indigenous languages, including Kiswahili, are technologically
under-resourced. ASR systems are crucial, particularly for the hearing-impaired
persons who can benefit from having transcripts in their native languages.
However, the absence of transcribed speech datasets has complicated efforts to
develop ASR models for these indigenous languages. This paper explores the
transcription process and the development of a Kiswahili speech corpus, which
includes both read-out texts and spontaneous speech data from native Kiswahili
speakers. The study also discusses the vowels and consonants in Kiswahili and
provides an updated Kiswahili phoneme dictionary for the ASR model that was
created using the CMU Sphinx speech recognition toolbox, an open-source speech
recognition toolkit. The ASR model was trained using an extended phonetic set
that yielded a WER and SER of 18.87% and 49.5%, respectively, an improved
performance than previous similar research for under-resourced languages.",None,-1
53f35e40-f882-4796-af57-e190c6777a41,Eye Gaze Estimation Model Analysis,0.0445418,"We explore techniques for eye gaze estimation using machine learning. Eye
gaze estimation is a common problem for various behavior analysis and
human-computer interfaces. The purpose of this work is to discuss various model
types for eye gaze estimation and present the results from predicting gaze
direction using eye landmarks in unconstrained settings. In unconstrained
real-world settings, feature-based and model-based methods are outperformed by
recent appearance-based methods due to factors like illumination changes and
other visual artifacts. We discuss a learning-based method for eye region
landmark localization trained exclusively on synthetic data. We discuss how to
use detected landmarks as input to iterative model-fitting and lightweight
learning-based gaze estimation methods and how to use the model for
person-independent and personalized gaze estimations.",https://github.com/aveenakottwani/EyeGazeEstimationModels,-1
329cb5d7-fa96-4212-83fe-62d9d6f1823a,AIREPAIR: A Repair Platform for Neural Networks,0.116221,"We present AIREPAIR, a platform for repairing neural networks. It features
the integration of existing network repair tools. Based on AIREPAIR, one can
run different repair methods on the same model, thus enabling the fair
comparison of different repair techniques. We evaluate AIREPAIR with three
state-of-the-art repair tools on popular deep-learning datasets and models. Our
evaluation confirms the utility of AIREPAIR, by comparing and analyzing the
results from different repair techniques. A demonstration is available at
https://youtu.be/UkKw5neeWhw.",https://github.com/theyoucheng/AIRepair,5264
6f349f28-f3fc-4567-87d1-3b60a2fb4646,EmbryosFormer: Deformable Transformer and Collaborative Encoding-Decoding for Embryos Stage Development Classification,0.337315,"The timing of cell divisions in early embryos during the In-Vitro
Fertilization (IVF) process is a key predictor of embryo viability. However,
observing cell divisions in Time-Lapse Monitoring (TLM) is a time-consuming
process and highly depends on experts. In this paper, we propose EmbryosFormer,
a computational model to automatically detect and classify cell divisions from
original time-lapse images. Our proposed network is designed as an
encoder-decoder deformable transformer with collaborative heads. The
transformer contracting path predicts per-image labels and is optimized by a
classification head. The transformer expanding path models the temporal
coherency between embryo images to ensure monotonic non-decreasing constraint
and is optimized by a segmentation head. Both contracting and expanding paths
are synergetically learned by a collaboration head. We have benchmarked our
proposed EmbryosFormer on two datasets: a public dataset with mouse embryos
with 8-cell stage and an in-house dataset with human embryos with 4-cell stage.
Source code: https://github.com/UARK-AICV/Embryos.",https://github.com/UARK-AICV/Embryos,-1
9a3e208b-b0d8-4ca6-8983-2a0e0c951e47,Natural Language Deduction through Search over Statement Compositions,0.592454,"In settings from fact-checking to question answering, we frequently want to
know whether a collection of evidence (premises) entails a hypothesis. Existing
methods primarily focus on the end-to-end discriminative version of this task,
but less work has treated the generative version in which a model searches over
the space of statements entailed by the premises to constructively derive the
hypothesis. We propose a system for doing this kind of deductive reasoning in
natural language by decomposing the task into separate steps coordinated by a
search procedure, producing a tree of intermediate conclusions that faithfully
reflects the system's reasoning process. Our experiments on the EntailmentBank
dataset (Dalvi et al., 2021) demonstrate that the proposed system can
successfully prove true statements while rejecting false ones. Moreover, it
produces natural language explanations with a 17% absolute higher step validity
than those produced by an end-to-end T5 model.",None,-1
58568e53-8614-4a9e-b97f-8e21224a72ae,JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus,0.818451,"Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes.",https://github.com/paracrawl/,-1
b7fb2f1a-cd4e-4d7e-b6d1-151b5b6e6050,More Interpretable Graph Similarity Computation via Maximum Common Subgraph Inference,0.675987,"Graph similarity measurement, which computes the distance/similarity between
two graphs, arises in various graph-related tasks. Recent learning-based
methods lack interpretability, as they directly transform interaction
information between two graphs into one hidden vector and then map it to
similarity. To cope with this problem, this study proposes a more interpretable
end-to-end paradigm for graph similarity learning, named Similarity Computation
via Maximum Common Subgraph Inference (INFMCS). Our critical insight into
INFMCS is the strong correlation between similarity score and Maximum Common
Subgraph (MCS). We implicitly infer MCS to obtain the normalized MCS size, with
the supervision information being only the similarity score during training. To
capture more global information, we also stack some vanilla transformer encoder
layers with graph convolution layers and propose a novel permutation-invariant
node Positional Encoding. The entire model is quite simple yet effective.
Comprehensive experiments demonstrate that INFMCS consistently outperforms
state-of-the-art baselines for graph-graph classification and regression tasks.
Ablation experiments verify the effectiveness of the proposed computation
paradigm and other components. Also, visualization and statistics of results
reveal the interpretability of INFMCS.",https://github.com/cszhangzhen/H2MN,-1
9cf784e2-aed0-4dab-8003-5e06ab0a4b2c,Evaluation of Semantic Answer Similarity Metrics,0.0751061,"There are several issues with the existing general machine translation or
natural language generation evaluation metrics, and question-answering (QA)
systems are indifferent in that context. To build robust QA systems, we need
the ability to have equivalently robust evaluation systems to verify whether
model predictions to questions are similar to ground-truth annotations. The
ability to compare similarity based on semantics as opposed to pure string
overlap is important to compare models fairly and to indicate more realistic
acceptance criteria in real-life applications. We build upon the first to our
knowledge paper that uses transformer-based model metrics to assess semantic
answer similarity and achieve higher correlations to human judgement in the
case of no lexical overlap. We propose cross-encoder augmented bi-encoder and
BERTScore models for semantic answer similarity, trained on a new dataset
consisting of name pairs of US-American public figures. As far as we are
concerned, we provide the first dataset of co-referent name string pairs along
with their similarities, which can be used for training.",https://github.com/e184633/,-1
d8e814f6-6f31-4bae-836a-9497d2428981,Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal Point Processes,0.388694,"We consider a sequential decision making problem where the agent faces the
environment characterized by the stochastic discrete events and seeks an
optimal intervention policy such that its long-term reward is maximized. This
problem exists ubiquitously in social media, finance and health informatics but
is rarely investigated by the conventional research in reinforcement learning.
To this end, we present a novel framework of the model-based reinforcement
learning where the agent's actions and observations are asynchronous stochastic
discrete events occurring in continuous-time. We model the dynamics of the
environment by Hawkes process with external intervention control term and
develop an algorithm to embed such process in the Bellman equation which guides
the direction of the value gradient. We demonstrate the superiority of our
method in both synthetic simulator and real-world problem.",https://github.com/WilliamBUG/Event-driven-rl/tree/main,-1
954af74c-2d8a-42d8-8345-5f506187fc75,Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs,0.529341,"Question answering over temporal knowledge graphs (KGs) efficiently uses
facts contained in a temporal KG, which records entity relations and when they
occur in time, to answer natural language questions (e.g., ""Who was the
president of the US before Obama?""). These questions often involve three
time-related challenges that previous work fail to adequately address: 1)
questions often do not specify exact timestamps of interest (e.g., ""Obama""
instead of 2000); 2) subtle lexical differences in time relations (e.g.,
""before"" vs ""after""); 3) off-the-shelf temporal KG embeddings that previous
work builds on ignore the temporal order of timestamps, which is crucial for
answering temporal-order related questions. In this paper, we propose a
time-sensitive question answering (TSQA) framework to tackle these problems.
TSQA features a timestamp estimation module to infer the unwritten timestamp
from the question. We also employ a time-sensitive KG encoder to inject
ordering information into the temporal KG embeddings that TSQA is based on.
With the help of techniques to reduce the search space for potential answers,
TSQA significantly outperforms the previous state of the art on a new benchmark
for question answering over temporal KGs, especially achieving a 32% (absolute)
error reduction on complex questions that require multiple steps of reasoning
over facts in the temporal KG.",https://github.com/apoorvumang/CronKGQA,-1
59366048-b341-47ce-89ec-89227f27a667,Object Localization under Single Coarse Point Supervision,0.604999,"Point-based object localization (POL), which pursues high-performance object
sensing under low-cost data annotation, has attracted increased attention.
However, the point annotation mode inevitably introduces semantic variance for
the inconsistency of annotated points. Existing POL methods heavily reply on
accurate key-point annotations which are difficult to define. In this study, we
propose a POL method using coarse point annotations, relaxing the supervision
signals from accurate key points to freely spotted points. To this end, we
propose a coarse point refinement (CPR) approach, which to our best knowledge
is the first attempt to alleviate semantic variance from the perspective of
algorithm. CPR constructs point bags, selects semantic-correlated points, and
produces semantic center points through multiple instance learning (MIL). In
this way, CPR defines a weakly supervised evolution procedure, which ensures
training high-performance object localizer under coarse point supervision.
Experimental results on COCO, DOTA and our proposed SeaPerson dataset validate
the effectiveness of the CPR approach. The dataset and code will be available
at https://github.com/ucas-vg/PointTinyBenchmark/.",https://github.com/ucas-vg/PointTinyBenchmark/,-1
a5a27e88-4619-4a8f-a44e-cdd009174b8d,Tell Your Story: Task-Oriented Dialogs for Interactive Content Creation,0.168002,"People capture photos and videos to relive and share memories of personal
significance. Recently, media montages (stories) have become a popular mode of
sharing these memories due to their intuitive and powerful storytelling
capabilities. However, creating such montages usually involves a lot of manual
searches, clicks, and selections that are time-consuming and cumbersome,
adversely affecting user experiences.
  To alleviate this, we propose task-oriented dialogs for montage creation as a
novel interactive tool to seamlessly search, compile, and edit montages from a
media collection. To the best of our knowledge, our work is the first to
leverage multi-turn conversations for such a challenging application, extending
the previous literature studying simple media retrieval tasks. We collect a new
dataset C3 (Conversational Content Creation), comprising 10k dialogs
conditioned on media montages simulated from a large media collection.
  We take a simulate-and-paraphrase approach to collect these dialogs to be
both cost and time efficient, while drawing from natural language distribution.
Our analysis and benchmarking of state-of-the-art language models showcase the
multimodal challenges present in the dataset. Lastly, we present a real-world
mobile demo application that shows the feasibility of the proposed work in
real-world applications. Our code and data will be made publicly available.",None,-1
53e4d7fc-d468-479c-9687-b0077041f80f,FNeVR: Neural Volume Rendering for Face Animation,0.563112,"Face animation, one of the hottest topics in computer vision, has achieved a
promising performance with the help of generative models. However, it remains a
critical challenge to generate identity preserving and photo-realistic images
due to the sophisticated motion deformation and complex facial detail modeling.
To address these problems, we propose a Face Neural Volume Rendering (FNeVR)
network to fully explore the potential of 2D motion warping and 3D volume
rendering in a unified framework. In FNeVR, we design a 3D Face Volume
Rendering (FVR) module to enhance the facial details for image rendering.
Specifically, we first extract 3D information with a well-designed
architecture, and then introduce an orthogonal adaptive ray-sampling module for
efficient rendering. We also design a lightweight pose editor, enabling FNeVR
to edit the facial pose in a simple yet effective way. Extensive experiments
show that our FNeVR obtains the best overall quality and performance on widely
used talking-head benchmarks.",None,-1
90a7782d-d8f0-4420-b570-e81195238020,Investigation of Data Augmentation Techniques for Disordered Speech Recognition,0.609811,"Disordered speech recognition is a highly challenging task. The underlying
neuro-motor conditions of people with speech disorders, often compounded with
co-occurring physical disabilities, lead to the difficulty in collecting large
quantities of speech required for system development. This paper investigates a
set of data augmentation techniques for disordered speech recognition,
including vocal tract length perturbation (VTLP), tempo perturbation and speed
perturbation. Both normal and disordered speech were exploited in the
augmentation process. Variability among impaired speakers in both the original
and augmented data was modeled using learning hidden unit contributions (LHUC)
based speaker adaptive training. The final speaker adapted system constructed
using the UASpeech corpus and the best augmentation approach based on speed
perturbation produced up to 2.92% absolute (9.3% relative) word error rate
(WER) reduction over the baseline system without data augmentation, and gave an
overall WER of 26.37% on the test set containing 16 dysarthric speakers.",None,-1
1c0567b3-051b-4cb8-9135-c65cc38b5808,On the Effectiveness of Parameter-Efficient Fine-Tuning,0.898387,"Fine-tuning pre-trained models has been ubiquitously proven to be effective
in a wide range of NLP tasks. However, fine-tuning the whole model is parameter
inefficient as it always yields an entirely new model for each task. Currently,
many research works propose to only fine-tune a small portion of the parameters
while keeping most of the parameters shared across different tasks. These
methods achieve surprisingly good performance and are shown to be more stable
than their corresponding fully fine-tuned counterparts. However, such kind of
methods is still not well understood. Some natural questions arise: How does
the parameter sparsity lead to promising performance? Why is the model more
stable than the fully fine-tuned models? How to choose the tunable parameters?
In this paper, we first categorize the existing methods into random approaches,
rule-based approaches, and projection-based approaches based on how they choose
which parameters to tune. Then, we show that all of the methods are actually
sparse fine-tuned models and conduct a novel theoretical analysis of them. We
indicate that the sparsity is actually imposing a regularization on the
original model by controlling the upper bound of the stability. Such stability
leads to better generalization capability which has been empirically observed
in a lot of recent research works. Despite the effectiveness of sparsity
grounded by our theory, it still remains an open problem of how to choose the
tunable parameters. To better choose the tunable parameters, we propose a novel
Second-order Approximation Method (SAM) which approximates the original problem
with an analytically solvable optimization function. The tunable parameters are
determined by directly optimizing the approximation function. The experimental
results show that our proposed SAM model outperforms many strong baseline
models and it also verifies our theoretical analysis.",https://github.com/fuzihaofzh/AnalyzeParameterEfficientFinetune,-1
c5e4046c-5948-4a8a-b5a1-fc05f86d1fa1,On the Explainability of Natural Language Processing Deep Models,0.737148,"While there has been a recent explosion of work on ExplainableAI ExAI on deep
models that operate on imagery and tabular data, textual datasets present new
challenges to the ExAI community. Such challenges can be attributed to the lack
of input structure in textual data, the use of word embeddings that add to the
opacity of the models and the difficulty of the visualization of the inner
workings of deep models when they are trained on textual data.
  Lately, methods have been developed to address the aforementioned challenges
and present satisfactory explanations on Natural Language Processing (NLP)
models. However, such methods are yet to be studied in a comprehensive
framework where common challenges are properly stated and rigorous evaluation
practices and metrics are proposed. Motivated to democratize ExAI methods in
the NLP field, we present in this work a survey that studies model-agnostic as
well as model-specific explainability methods on NLP models. Such methods can
either develop inherently interpretable NLP models or operate on pre-trained
models in a post-hoc manner. We make this distinction and we further decompose
the methods into three categories according to what they explain: (1) word
embeddings (input-level), (2) inner workings of NLP models (processing-level)
and (3) models' decisions (output-level). We also detail the different
evaluation approaches interpretability methods in the NLP field. Finally, we
present a case-study on the well-known neural machine translation in an
appendix and we propose promising future research directions for ExAI in the
NLP field.",None,-1
746e5305-687e-4693-9e1f-0aa69907d5c1,Playing Tic-Tac-Toe Games with Intelligent Single-pixel Imaging,0.0934564,"Single-pixel imaging (SPI) is a novel optical imaging technique by replacing
a two-dimensional pixelated sensor with a single-pixel detector and pattern
illuminations. SPI have been extensively used for various tasks related to
image acquisition and processing. In this work, a novel non-image-based task of
playing Tic-Tac-Toe games interactively is merged into the framework of SPI. An
optoelectronic artificial intelligent (AI) player with minimal digital
computation can detect the game states, generate optimal moves and display
output results mainly by pattern illumination and single-pixel detection.
Simulated and experimental results demonstrate the feasibility of proposed
scheme and its unbeatable performance against human players.",None,-1
d06ee1dd-a20e-4c46-9408-089a2a113c4b,Attribute-based Representations for Accurate and Interpretable Video Anomaly Detection,0.779454,"Video anomaly detection (VAD) is a challenging computer vision task with many
practical applications. As anomalies are inherently ambiguous, it is essential
for users to understand the reasoning behind a system's decision in order to
determine if the rationale is sound. In this paper, we propose a simple but
highly effective method that pushes the boundaries of VAD accuracy and
interpretability using attribute-based representations. Our method represents
every object by its velocity and pose. The anomaly scores are computed using a
density-based approach. Surprisingly, we find that this simple representation
is sufficient to achieve state-of-the-art performance in ShanghaiTech, the
largest and most complex VAD dataset. Combining our interpretable
attribute-based representations with implicit, deep representation yields
state-of-the-art performance with a $99.1\%, 93.3\%$, and $85.9\%$ AUROC on
Ped2, Avenue, and ShanghaiTech, respectively. Our method is accurate,
interpretable, and easy to implement.",https://github.com/talreiss/Accurate-Interpretable-VAD,-1
07d2622a-d219-4ea7-8952-3c65c7a97d95,EDU-level Extractive Summarization with Varying Summary Lengths,0.206524,"Extractive models usually formulate text summarization as extracting fixed
top-$k$ salient sentences from the document as a summary. Few works exploited
extracting finer-grained Elementary Discourse Unit (EDU) with little analysis
and justification for the extractive unit selection. Further, the selection
strategy of the fixed top-$k$ salient sentences fits the summarization need
poorly, as the number of salient sentences in different documents varies and
therefore a common or best $k$ does not exist in reality. To fill these gaps,
this paper first conducts the comparison analysis of oracle summaries based on
EDUs and sentences, which provides evidence from both theoretical and
experimental perspectives to justify and quantify that EDUs make summaries with
higher automatic evaluation scores than sentences. Then, considering this merit
of EDUs, this paper further proposes an EDU-level extractive model with Varying
summary Lengths and develops the corresponding learning algorithm. EDU-VL
learns to encode and predict probabilities of EDUs in the document, generate
multiple candidate summaries with varying lengths based on various $k$ values,
and encode and score candidate summaries, in an end-to-end training manner.
Finally, EDU-VL is experimented on single and multi-document benchmark datasets
and shows improved performances on ROUGE scores in comparison with
state-of-the-art extractive models, and further human evaluation suggests that
EDU-constituent summaries maintain good grammaticality and readability.",https://github.com/yuping-wu/EDU-VL,15518
b279b77e-2626-407e-a5ce-39983f4c79b9,Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing,0.790389,"Entity typing aims at predicting one or more words that describe the type(s)
of a specific mention in a sentence. Due to shortcuts from surface patterns to
annotated entity labels and biased training, existing entity typing models are
subject to the problem of spurious correlations. To comprehensively investigate
the faithfulness and reliability of entity typing methods, we first
systematically define distinct kinds of model biases that are reflected mainly
from spurious correlations. Particularly, we identify six types of existing
model biases, including mention-context bias, lexical overlapping bias, named
entity bias, pronoun bias, dependency bias, and overgeneralization bias. To
mitigate model biases, we then introduce a counterfactual data augmentation
method. By augmenting the original training set with their debiased
counterparts, models are forced to fully comprehend sentences and discover the
fundamental cues for entity typing, rather than relying on spurious
correlations for shortcuts. Experimental results on the UFET dataset show our
counterfactual data augmentation approach helps improve generalization of
different entity typing models with consistently better performance on both the
original and debiased test sets.",https://github.com/luka-group/DiagnoseET,-1
818ce70b-3ed8-446e-9d65-43d0ae9d193e,Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation,0.80541,"Fashion attribute editing is a task that aims to convert the semantic
attributes of a given fashion image while preserving the irrelevant regions.
Previous works typically employ conditional GANs where the generator explicitly
learns the target attributes and directly execute the conversion. These
approaches, however, are neither scalable nor generic as they operate only with
few limited attributes and a separate generator is required for each dataset or
attribute set. Inspired by the recent advancement of diffusion models, we
explore the classifier-guided diffusion that leverages the off-the-shelf
diffusion model pretrained on general visual semantics such as Imagenet. In
order to achieve a generic editing pipeline, we pose this as multi-attribute
image manipulation task, where the attribute ranges from item category, fabric,
pattern to collar and neckline. We empirically show that conventional methods
fail in our challenging setting, and study efficient adaptation scheme that
involves recently introduced attention-pooling technique to obtain a
multi-attribute classifier guidance. Based on this, we present a mask-free
fashion attribute editing framework that leverages the classifier logits and
the cross-attention map for manipulation. We empirically demonstrate that our
framework achieves convincing sample quality and attribute alignments.",None,-1
230b0985-574c-409a-90d4-5be47f5e91ad,Faithful Reasoning Using Large Language Models,0.969425,"Although contemporary large language models (LMs) demonstrate impressive
question-answering capabilities, their answers are typically the product of a
single call to the model. This entails an unwelcome degree of opacity and
compromises performance, especially on problems that are inherently multi-step.
To address these limitations, we show how LMs can be made to perform faithful
multi-step reasoning via a process whose causal structure mirrors the
underlying logical structure of the problem. Our approach works by chaining
together reasoning steps, where each step results from calls to two fine-tuned
LMs, one for selection and one for inference, to produce a valid reasoning
trace. Our method carries out a beam search through the space of reasoning
traces to improve reasoning quality. We demonstrate the effectiveness of our
model on multi-step logical deduction and scientific question-answering,
showing that it outperforms baselines on final answer accuracy, and generates
humanly interpretable reasoning traces whose validity can be checked by the
user.",None,13655
902fc8c4-a2e1-47f1-ab05-2bce38e378bc,A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications,0.223909,"Automatic text generation based on neural language models has achieved
performance levels that make the generated text almost indistinguishable from
those written by humans. Despite the value that text generation can have in
various applications, it can also be employed for malicious tasks. The
diffusion of such practices represent a threat to the quality of academic
publishing. To address these problems, we propose in this paper two datasets
comprised of artificially generated research content: a completely synthetic
dataset and a partial text substitution dataset. In the first case, the content
is completely generated by the GPT-2 model after a short prompt extracted from
original papers. The partial or hybrid dataset is created by replacing several
sentences of abstracts with sentences that are generated by the Arxiv-NLP
model. We evaluate the quality of the datasets comparing the generated texts to
aligned original texts using fluency metrics such as BLEU and ROUGE. The more
natural the artificial texts seem, the more difficult they are to detect and
the better is the benchmark. We also evaluate the difficulty of the task of
distinguishing original from generated text by using state-of-the-art
classification models.",https://github.com/strib/scigen.git,-1
39dbe16c-390a-4c96-b012-9261eadda4c9,Algorithmic progress in computer vision,0.595847,"We investigate algorithmic progress in image classification on ImageNet,
perhaps the most well-known test bed for computer vision. We estimate a model,
informed by work on neural scaling laws, and infer a decomposition of progress
into the scaling of compute, data, and algorithms. Using Shapley values to
attribute performance improvements, we find that algorithmic improvements have
been roughly as important as the scaling of compute for progress computer
vision. Our estimates indicate that algorithmic innovations mostly take the
form of compute-augmenting algorithmic advances (which enable researchers to
get better performance from less compute), not data-augmenting algorithmic
advances. We find that compute-augmenting algorithmic advances are made at a
pace more than twice as fast as the rate usually associated with Moore's law.
In particular, we estimate that compute-augmenting innovations halve compute
requirements every nine months (95\% confidence interval: 4 to 25 months).",None,-1
1a19abb3-9f0e-4fe3-a4f1-03b8ea456b5e,Zero-Label Prompt Selection,0.0983039,"Natural language prompts have been shown to facilitate cross-task
generalization for large language models. However, with no or limited labeled
examples, the cross-task performance is highly sensitive to the choice of
prompts, while selecting a high-performing prompt is challenging given the
scarcity of labels. To address the issue, we propose a Zero-Label Prompt
Selection (ZPS) method that selects prompts without any labeled data or
gradient update. Specifically, given the candidate human-written prompts for a
task, ZPS labels a set of unlabeled data with a prompt ensemble and uses the
pseudo-labels for prompt selection. Experiments show that ZPS improves over
prior methods by a sizeable margin in zero-label performance. We also extend
ZPS to a few-shot setting and show its advantages over strong baselines such as
prompt tuning and model tuning.",https://github.com/ChonghuaLiao/ZPS,-1
d66f855e-19f2-47d2-a6be-253ff11a58c1,MEKER: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering,0.398163,"Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG
embedding contains concise data used in NLP tasks requiring implicit
information about the real world. Furthermore, the size of KGs that may be
useful in actual NLP assignments is enormous, and creating embedding over it
has memory cost issues. We represent KG as a 3rd-order binary tensor and move
beyond the standard CP decomposition by using a data-specific generalized
version of it. The generalization of the standard CP-ALS algorithm allows
obtaining optimization gradients without a backpropagation mechanism. It
reduces the memory needed in training while providing computational benefits.
We propose a MEKER, a memory-efficient KG embedding model, which yields
SOTA-comparable performance on link prediction tasks and KG-based Question
Answering.",https://github.com/askplatypus/wikidata-simplequestions,-1
a44449ca-0912-4661-9b1c-92eaa260298b,DeepInteraction: 3D Object Detection via Modality Interaction,0.998371,"Existing top-performance 3D object detectors typically rely on the
multi-modal fusion strategy. This design is however fundamentally restricted
due to overlooking the modality-specific useful information and finally
hampering the model performance. To address this limitation, in this work we
introduce a novel modality interaction strategy where individual per-modality
representations are learned and maintained throughout for enabling their unique
characteristics to be exploited during object detection. To realize this
proposed strategy, we design a DeepInteraction architecture characterized by a
multi-modal representational interaction encoder and a multi-modal predictive
interaction decoder. Experiments on the large-scale nuScenes dataset show that
our proposed method surpasses all prior arts often by a large margin.
Crucially, our method is ranked at the first position at the highly competitive
nuScenes object detection leaderboard.",https://github.com/fudan-zvg/DeepInteraction,-1
b2720475-5a6b-4f58-8e4b-bcdefc55ec8d,One Agent To Rule Them All: Towards Multi-agent Conversational AI,0.514136,"The increasing volume of commercially available conversational agents (CAs)
on the market has resulted in users being burdened with learning and adopting
multiple agents to accomplish their tasks. Though prior work has explored
supporting a multitude of domains within the design of a single agent, the
interaction experience suffers due to the large action space of desired
capabilities. To address these problems, we introduce a new task BBAI:
Black-Box Agent Integration, focusing on combining the capabilities of multiple
black-box CAs at scale. We explore two techniques: question agent pairing and
question response pairing aimed at resolving this task. Leveraging these
techniques, we design One For All (OFA), a scalable system that provides a
unified interface to interact with multiple CAs. Additionally, we introduce
MARS: Multi-Agent Response Selection, a new encoder model for question response
pairing that jointly encodes user question and agent response pairs. We
demonstrate that OFA is able to automatically and accurately integrate an
ensemble of commercially available CAs spanning disparate domains.
Specifically, using the MARS encoder we achieve the highest accuracy on our
BBAI task, outperforming strong baselines.",https://github.com/ChrisIsKing/black-box-multi-agent-integation,-1
d4446ab2-4360-49d9-95fe-70f92f2d8590,Adversarial random forests for density estimation and generative modeling,0.245852,"We propose methods for density estimation and data synthesis using a novel
form of unsupervised random forests. Inspired by generative adversarial
networks, we implement a recursive procedure in which trees gradually learn
structural properties of the data through alternating rounds of generation and
discrimination. The method is provably consistent under minimal assumptions.
Unlike classic tree-based alternatives, our approach provides smooth
(un)conditional densities and allows for fully synthetic data generation. We
achieve comparable or superior performance to state-of-the-art probabilistic
circuits and deep learning models on various tabular data benchmarks while
executing about two orders of magnitude faster on average. An accompanying
$\texttt{R}$ package, $\texttt{arf}$, is available on $\texttt{CRAN}$.",https://github.com/bips-hb/arf_paper,-1
93618487-a14f-429d-a2fb-8608af7a20ea,Multi-Agent Deep Reinforcement Learning for Cost- and Delay-Sensitive Virtual Network Function Placement and Routing,0.6559,"This paper proposes an effective and novel multiagent deep reinforcement
learning (MADRL)-based method for solving the joint virtual network function
(VNF) placement and routing (P&R), where multiple service requests with
differentiated demands are delivered at the same time. The differentiated
demands of the service requests are reflected by their delay- and
cost-sensitive factors. We first construct a VNF P&R problem to jointly
minimize a weighted sum of service delay and resource consumption cost, which
is NP-complete. Then, the joint VNF P&R problem is decoupled into two iterative
subtasks: placement subtask and routing subtask. Each subtask consists of
multiple concurrent parallel sequential decision processes. By invoking the
deep deterministic policy gradient method and multi-agent technique, an
MADRL-P&R framework is designed to perform the two subtasks. The new joint
reward and internal rewards mechanism is proposed to match the goals and
constraints of the placement and routing subtasks. We also propose the
parameter migration-based model-retraining method to deal with changing network
topologies. Corroborated by experiments, the proposed MADRL-P&R framework is
superior to its alternatives in terms of service cost and delay, and offers
higher flexibility for personalized service demands. The parameter
migration-based model-retraining method can efficiently accelerate convergence
under moderate network topology changes.",None,-1
7bd04d70-b4cf-4078-885e-01ae466b28b7,Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss,0.626417,"In this paper, we study color image inpainting as a pure quaternion matrix
completion problem. In the literature, the theoretical guarantee for quaternion
matrix completion is not well-established. Our main aim is to propose a new
minimization problem with an objective combining nuclear norm and a quadratic
loss weighted among three channels. To fill the theoretical vacancy, we obtain
the error bound in both clean and corrupted regimes, which relies on some new
results of quaternion matrices. A general Gaussian noise is considered in
robust completion where all observations are corrupted. Motivated by the error
bound, we propose to handle unbalanced or correlated noise via a cross-channel
weight in the quadratic loss, with the main purpose of rebalancing noise level,
or removing noise correlation. Extensive experimental results on synthetic and
color image data are presented to confirm and demonstrate our theoretical
findings.",None,-1
bbdfe7a8-dd10-42f5-8f3d-aebd416f949f,Focal Length and Object Pose Estimation via Render and Compare,0.914306,"We introduce FocalPose, a neural render-and-compare method for jointly
estimating the camera-object 6D pose and camera focal length given a single RGB
input image depicting a known object. The contributions of this work are
twofold. First, we derive a focal length update rule that extends an existing
state-of-the-art render-and-compare 6D pose estimator to address the joint
estimation task. Second, we investigate several different loss functions for
jointly estimating the object pose and focal length. We find that a combination
of direct focal length regression with a reprojection loss disentangling the
contribution of translation, rotation, and focal length leads to improved
results. We show results on three challenging benchmark datasets that depict
known 3D models in uncontrolled settings. We demonstrate that our focal length
and 6D pose estimates have lower error than the existing state-of-the-art
methods.",https://ponimatkin.github.io/focalpose,-1
3239ce8b-b88e-44cf-9421-59ab9d02ea5b,Position Paper: Online Modeling for Offline Planning,0.106281,"The definition and representation of planning problems is at the heart of AI
planning research. A key part is the representation of action models. Decades
of advances improving declarative action model representations resulted in
numerous theoretical advances, and capable, working, domain-independent
planners. However, despite the maturity of the field, AI planning technology is
still rarely used outside the research community, suggesting that current
representations fail to capture real-world requirements, such as utilizing
complex mathematical functions and models learned from data. We argue that this
is because the modeling process is assumed to have taken place and completed
prior to the planning process, i.e., offline modeling for offline planning.
There are several challenges inherent to this approach, including: limited
expressiveness of declarative modeling languages; early commitment to modeling
choices and computation, that preclude using the most appropriate resolution
for each action model -- which can only be known during planning; and
difficulty in reliably using non-declarative, learned, models.
  We therefore suggest to change the AI planning process, such that is carries
out online modeling in offline planning, i.e., the use of action models that
are computed or even generated as part of the planning process, as they are
accessed. This generalizes the existing approach (offline modeling). The
proposed definition admits novel planning processes, and we suggest one
concrete implementation, demonstrating the approach. We sketch initial results
that were obtained as part of a first attempt to follow this approach by
planning with action cost estimators. We conclude by discussing open
challenges.",None,7922
acf1d438-6187-4fb3-a33f-d2b9ded560f8,Bi-directional Joint Neural Networks for Intent Classification and Slot Filling,0.6883,"Intent classification and slot filling are two critical tasks for natural
language understanding. Traditionally the two tasks proceeded independently.
However, more recently joint models for intent classification and slot filling
have achieved state-of-the-art performance, and have proved that there exists a
strong relationship between the two tasks. In this paper, we propose a
bi-directional joint model for intent classification and slot filling, which
includes a multi-stage hierarchical process via BERT and bi-directional joint
natural language understanding mechanisms, including intent2slot and
slot2intent, to obtain mutual performance enhancement between intent
classification and slot filling. The evaluations show that our model achieves
state-of-the-art results on intent classification accuracy, slot filling F1,
and significantly improves sentence-level semantic frame accuracy when applied
to publicly available benchmark datasets, ATIS (88.6%) and SNIPS (92.8%).",None,-1
8eb75bac-7b61-4090-9e00-560e6141b7c6,TriBYOL: Triplet BYOL for Self-Supervised Representation Learning,0.396943,"This paper proposes a novel self-supervised learning method for learning
better representations with small batch sizes. Many self-supervised learning
methods based on certain forms of the siamese network have emerged and received
significant attention. However, these methods need to use large batch sizes to
learn good representations and require heavy computational resources. We
present a new triplet network combined with a triple-view loss to improve the
performance of self-supervised representation learning with small batch sizes.
Experimental results show that our method can drastically outperform
state-of-the-art self-supervised learning methods on several datasets in
small-batch cases. Our method provides a feasible solution for self-supervised
learning with real-world high-resolution images that uses small batch sizes.",None,-1
a2c9e29f-ad72-421e-8765-5439d13e1072,MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer,0.998943,"Self-supervised monocular depth estimation is an attractive solution that
does not require hard-to-source depth labels for training. Convolutional neural
networks (CNNs) have recently achieved great success in this task. However,
their limited receptive field constrains existing network architectures to
reason only locally, dampening the effectiveness of the self-supervised
paradigm. In the light of the recent successes achieved by Vision Transformers
(ViTs), we propose MonoViT, a brand-new framework combining the global
reasoning enabled by ViT models with the flexibility of self-supervised
monocular depth estimation. By combining plain convolutions with Transformer
blocks, our model can reason locally and globally, yielding depth prediction at
a higher level of detail and accuracy, allowing MonoViT to achieve
state-of-the-art performance on the established KITTI dataset. Moreover,
MonoViT proves its superior generalization capacities on other datasets such as
Make3D and DrivingStereo.",https://github.com/zxcqlf/MonoViT,-1
5a981d2c-834c-456d-9673-12df77081e31,Domain Mismatch Doesn't Always Prevent Cross-Lingual Transfer Learning,0.112297,"Cross-lingual transfer learning without labeled target language data or
parallel text has been surprisingly effective in zero-shot cross-lingual
classification, question answering, unsupervised machine translation, etc.
However, some recent publications have claimed that domain mismatch prevents
cross-lingual transfer, and their results show that unsupervised bilingual
lexicon induction (UBLI) and unsupervised neural machine translation (UNMT) do
not work well when the underlying monolingual corpora come from different
domains (e.g., French text from Wikipedia but English text from UN
proceedings). In this work, we show that a simple initialization regimen can
overcome much of the effect of domain mismatch in cross-lingual transfer. We
pre-train word and contextual embeddings on the concatenated domain-mismatched
corpora, and use these as initializations for three tasks: MUSE UBLI, UN
Parallel UNMT, and the SemEval 2017 cross-lingual word similarity task. In all
cases, our results challenge the conclusions of prior work by showing that
proper initialization can recover a large portion of the losses incurred by
domain mismatch.",https://github.com/facebookresearch/XLM,-1
2c72d822-76a8-482a-b9ed-dd52deb5454a,Region Embedding with Intra and Inter-View Contrastive Learning,0.481917,"Unsupervised region representation learning aims to extract dense and
effective features from unlabeled urban data. While some efforts have been made
for solving this problem based on multiple views, existing methods are still
insufficient in extracting representations in a view and/or incorporating
representations from different views. Motivated by the success of contrastive
learning for representation learning, we propose to leverage it for multi-view
region representation learning and design a model called ReMVC (Region
Embedding with Multi-View Contrastive Learning) by following two guidelines: i)
comparing a region with others within each view for effective representation
extraction and ii) comparing a region with itself across different views for
cross-view information sharing. We design the intra-view contrastive learning
module which helps to learn distinguished region embeddings and the inter-view
contrastive learning module which serves as a soft co-regularizer to constrain
the embedding parameters and transfer knowledge across multi-views. We exploit
the learned region embeddings in two downstream tasks named land usage
clustering and region popularity prediction. Extensive experiments demonstrate
that our model achieves impressive improvements compared with seven
state-of-the-art baseline methods, and the margins are over 30% in the land
usage clustering task.",https://github.com/Liang-NTU/ReMVC,2797
b0c1c895-d59b-46a2-93bd-4538abd6f4ab,"A Distant Supervision Corpus for Extracting Biomedical Relationships Between Chemicals, Diseases and Genes",0.451174,"We introduce ChemDisGene, a new dataset for training and evaluating
multi-class multi-label document-level biomedical relation extraction models.
Our dataset contains 80k biomedical research abstracts labeled with mentions of
chemicals, diseases, and genes, portions of which human experts labeled with 18
types of biomedical relationships between these entities (intended for
evaluation), and the remainder of which (intended for training) has been
distantly labeled via the CTD database with approximately 78\% accuracy. In
comparison to similar preexisting datasets, ours is both substantially larger
and cleaner; it also includes annotations linking mentions to their entities.
We also provide three baseline deep neural network relation extraction models
trained and evaluated on our new dataset.",https://github.com/chanzuckerberg/ChemDisGene,-1
cf86b603-f0b6-4c1b-b19a-1c2208490884,Sequence Prediction Under Missing Data : An RNN Approach Without Imputation,0.0419506,"Missing data scenarios are very common in ML applications in general and
time-series/sequence applications are no exceptions. This paper pertains to a
novel Recurrent Neural Network (RNN) based solution for sequence prediction
under missing data. Our method is distinct from all existing approaches. It
tries to encode the missingness patterns in the data directly without trying to
impute data either before or during model building. Our encoding is lossless
and achieves compression. It can be employed for both sequence classification
and forecasting. We focus on forecasting here in a general context of
multi-step prediction in presence of possible exogenous inputs. In particular,
we propose novel variants of Encoder-Decoder (Seq2Seq) RNNs for this. The
encoder here adopts the above mentioned pattern encoding, while at the decoder
which has a different structure, multiple variants are feasible. We demonstrate
the utility of our proposed architecture via multiple experiments on both
single and multiple sequence (real) data-sets. We consider both scenarios where
(i)data is naturally missing and (ii)data is synthetically masked.",None,-1
c6dd35e2-f7be-47f4-b41d-7e01a36b1f3d,Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement,0.895265,"Low-Light Image Enhancement is a computer vision task which intensifies the
dark images to appropriate brightness. It can also be seen as an ill-posed
problem in image restoration domain. With the success of deep neural networks,
the convolutional neural networks surpass the traditional algorithm-based
methods and become the mainstream in the computer vision area. To advance the
performance of enhancement algorithms, we propose an image enhancement network
(HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use
a half wavelet attention block on M-Net+ to enrich the features from wavelet
domain. Furthermore, our HWMNet has competitive performance results on two
image enhancement datasets in terms of quantitative metrics and visual quality.
The source code and pretrained model are available at
https://github.com/FanChiMao/HWMNet.",https://github.com/FanChiMao/HWMNet,-1
97a96d49-0980-4b17-9de0-46161368b5b1,Data-efficient End-to-end Information Extraction for Statistical Legal Analysis,0.11551,"Legal practitioners often face a vast amount of documents. Lawyers, for
instance, search for appropriate precedents favorable to their clients, while
the number of legal precedents is ever-growing. Although legal search engines
can assist finding individual target documents and narrowing down the number of
candidates, retrieved information is often presented as unstructured text and
users have to examine each document thoroughly which could lead to information
overloading. This also makes their statistical analysis challenging. Here, we
present an end-to-end information extraction (IE) system for legal documents.
By formulating IE as a generation task, our system can be easily applied to
various tasks without domain-specific engineering effort. The experimental
results of four IE tasks on Korean precedents shows that our IE system can
achieve competent scores (-2.3 on average) compared to the rule-based baseline
with as few as 50 training examples per task and higher score (+5.4 on average)
with 200 examples. Finally, our statistical analysis on two case
categories--drunk driving and fraud--with 35k precedents reveals the resulting
structured information from our IE system faithfully reflects the macroscopic
features of Korean legal system.",https://github.com/heartexlabs/label-studio,-1
e950d196-9a30-4e04-a22a-db6f04c5c367,Risk-Driven Design of Perception Systems,0.403213,"Modern autonomous systems rely on perception modules to process complex
sensor measurements into state estimates. These estimates are then passed to a
controller, which uses them to make safety-critical decisions. It is therefore
important that we design perception systems to minimize errors that reduce the
overall safety of the system. We develop a risk-driven approach to designing
perception systems that accounts for the effect of perceptual errors on the
performance of the fully-integrated, closed-loop system. We formulate a risk
function to quantify the effect of a given perceptual error on overall safety,
and show how we can use it to design safer perception systems by including a
risk-dependent term in the loss function and generating training data in
risk-sensitive regions. We evaluate our techniques on a realistic vision-based
aircraft detect and avoid application and show that risk-driven design reduces
collision risk by 37% over a baseline system.",https://github.com/sisl/RiskDrivenPerception,-1
6591f763-8ff8-4e41-83c1-f8fda5c6a407,An Effective Iterated Two-stage Heuristic Algorithm for the Multiple Traveling Salesmen Problem,0.515148,"The multiple Traveling Salesmen Problem (mTSP) is a general extension of the
famous NP-hard Traveling Salesmen Problem (TSP), that there are m (m > 1)
salesmen to visit the cities. In this paper, we address the mTSP with both the
minsum objective and minmax objective, which aims at minimizing the total
length of the $m$ tours and the length of the longest tour among all the m
tours, respectively. We propose an iterated two-stage heuristic algorithm
called ITSHA for the mTSP. Each iteration of ITSHA consists of an
initialization stage and an improvement stage. The initialization stage aims to
generate high-quality and diverse initial solutions. The improvement stage
mainly applies the variable neighborhood search (VNS) approach based on our
proposed effective local search neighborhoods to optimize the initial solution.
Moreover, some local optima escaping approaches are employed to enhance the
search ability of the algorithm. Extensive experimental results on a wide range
of public benchmark instances show that ITSHA significantly outperforms the
state-of-the-art heuristic algorithms in solving the mTSP on both the
objectives.",None,-1
043a6fc9-02eb-4611-8519-afc9e92599a9,Phantom Sponges: Exploiting Non-Maximum Suppression to Attack Deep Object Detectors,0.594228,"Adversarial attacks against deep learning-based object detectors have been
studied extensively in the past few years. Most of the attacks proposed have
targeted the model's integrity (i.e., caused the model to make incorrect
predictions), while adversarial attacks targeting the model's availability, a
critical aspect in safety-critical domains such as autonomous driving, have not
yet been explored by the machine learning research community. In this paper, we
propose a novel attack that negatively affects the decision latency of an
end-to-end object detection pipeline. We craft a universal adversarial
perturbation (UAP) that targets a widely used technique integrated in many
object detector pipelines -- non-maximum suppression (NMS). Our experiments
demonstrate the proposed UAP's ability to increase the processing time of
individual frames by adding ""phantom"" objects that overload the NMS algorithm
while preserving the detection of the original objects which allows the attack
to go undetected for a longer period of time.",https://github.com/AvishagS422/PhantomSponges,-1
0161a5f9-d890-4a2a-a886-031babcd9735,Intelligent problem-solving as integrated hierarchical reinforcement learning,0.899674,"According to cognitive psychology and related disciplines, the development of
complex problem-solving behaviour in biological agents depends on hierarchical
cognitive mechanisms. Hierarchical reinforcement learning is a promising
computational approach that may eventually yield comparable problem-solving
behaviour in artificial agents and robots. However, to date the problem-solving
abilities of many human and non-human animals are clearly superior to those of
artificial systems. Here, we propose steps to integrate biologically inspired
hierarchical mechanisms to enable advanced problem-solving skills in artificial
agents. Therefore, we first review the literature in cognitive psychology to
highlight the importance of compositional abstraction and predictive
processing. Then we relate the gained insights with contemporary hierarchical
reinforcement learning methods. Interestingly, our results suggest that all
identified cognitive mechanisms have been implemented individually in isolated
computational architectures, raising the question of why there exists no single
unifying architecture that integrates them. As our final contribution, we
address this question by providing an integrative perspective on the
computational challenges to develop such a unifying architecture. We expect our
results to guide the development of more sophisticated cognitively inspired
hierarchical machine learning architectures.",None,13187
5c89214d-d45b-41dd-84a4-c6363d086f27,Estimating Soft Labels for Out-of-Domain Intent Detection,0.909282,"Out-of-Domain (OOD) intent detection is important for practical dialog
systems. To alleviate the issue of lacking OOD training samples, some works
propose synthesizing pseudo OOD samples and directly assigning one-hot OOD
labels to these pseudo samples. However, these one-hot labels introduce noises
to the training process because some hard pseudo OOD samples may coincide with
In-Domain (IND) intents. In this paper, we propose an adaptive soft pseudo
labeling (ASoul) method that can estimate soft labels for pseudo OOD samples
when training OOD detectors. Semantic connections between pseudo OOD samples
and IND intents are captured using an embedding graph. A co-training framework
is further introduced to produce resulting soft labels following the smoothness
assumption, i.e., close samples are likely to have similar labels. Extensive
experiments on three benchmark datasets show that ASoul consistently improves
the OOD detection performance and outperforms various competitive baselines.",None,-1
7c76c621-ffb6-411e-a029-aab359535938,"Evaluating State of the Art, Forecasting Ensembles- and Meta-learning Strategies for Model Fusion",0.402565,"Techniques of hybridisation and ensemble learning are popular model fusion
techniques for improving the predictive power of forecasting methods. With
limited research that instigates combining these two promising approaches, this
paper focuses on the utility of the Exponential-Smoothing-Recurrent Neural
Network (ES-RNN) in the pool of base models for different ensembles. We compare
against some state of the art ensembling techniques and arithmetic model
averaging as a benchmark. We experiment with the M4 forecasting data set of
100,000 time-series, and the results show that the Feature-based Forecast Model
Averaging (FFORMA), on average, is the best technique for late data fusion with
the ES-RNN. However, considering the M4's Daily subset of data, stacking was
the only successful ensemble at dealing with the case where all base model
performances are similar. Our experimental results indicate that we attain
state of the art forecasting results compared to N-BEATS as a benchmark. We
conclude that model averaging is a more robust ensemble than model selection
and stacking strategies. Further, the results show that gradient boosting is
superior for implementing ensemble learning strategies.",https://github.com/Pieter-Cawood/FFORMA-ESRNN,-1
76df7637-2162-4d12-bf51-2f80ad2ba3af,Target-Driven Structured Transformer Planner for Vision-Language Navigation,0.525488,"Vision-language navigation is the task of directing an embodied agent to
navigate in 3D scenes with natural language instructions. For the agent,
inferring the long-term navigation target from visual-linguistic clues is
crucial for reliable path planning, which, however, has rarely been studied
before in literature. In this article, we propose a Target-Driven Structured
Transformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware
navigation. Specifically, we devise an Imaginary Scene Tokenization mechanism
for explicit estimation of the long-term target (even located in unexplored
environments). In addition, we design a Structured Transformer Planner which
elegantly incorporates the explored room layout into a neural attention
architecture for structured and global planning. Experimental results
demonstrate that our TD-STP substantially improves previous best methods'
success rate by 2% and 5% on the test set of R2R and REVERIE benchmarks,
respectively. Our code is available at https://github.com/YushengZhao/TD-STP .",https://github.com/YushengZhao/TD-STP,-1
2761f0e9-739a-4fe8-8276-15bedabf0561,Mathematically Modeling the Lexicon Entropy of Emergent Language,0.0418085,"We formulate a stochastic process, FiLex, as a mathematical model of lexicon
entropy in deep learning-based emergent language systems. Defining a model
mathematically allows it to generate clear predictions which can be directly
and decisively tested. We empirically verify across four different environments
that FiLex predicts the correct correlation between hyperparameters (training
steps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax
temperature) and the emergent language's entropy in 20 out of 20
environment-hyperparameter combinations. Furthermore, our experiments reveal
that different environments show diverse relationships between their
hyperparameters and entropy which demonstrates the need for a model which can
make well-defined predictions at a precise level of granularity.",https://github.com/brendon-boldt/filex-emergent-language,1641
de571d99-87ef-42de-9199-ed0c4c6cd0e1,LightDepth: A Resource Efficient Depth Estimation Approach for Dealing with Ground Truth Sparsity via Curriculum Learning,0.102606,"Advances in neural networks enable tackling complex computer vision tasks
such as depth estimation of outdoor scenes at unprecedented accuracy. Promising
research has been done on depth estimation. However, current efforts are
computationally resource-intensive and do not consider the resource constraints
of autonomous devices, such as robots and drones. In this work, we present a
fast and battery-efficient approach for depth estimation. Our approach devises
model-agnostic curriculum-based learning for depth estimation. Our experiments
show that the accuracy of our model performs on par with the state-of-the-art
models, while its response time outperforms other models by 71%. All codes are
available online at https://github.com/fatemehkarimii/LightDepth.",https://github.com/fatemehkarimii/LightDepth,-1
9d0ba992-353f-4d66-9127-519699a21115,Semantic-Oriented Unlabeled Priming for Large-Scale Language Models,0.429592,"Due to the high costs associated with finetuning large language models,
various recent works propose to adapt them to specific tasks without any
parameter updates through in-context learning. Unfortunately, for in-context
learning there is currently no way to leverage unlabeled data, which is often
much easier to obtain in large quantities than labeled examples. In this work,
we therefore investigate ways to make use of unlabeled examples to improve the
zero-shot performance of pretrained language models without any finetuning: We
introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies
examples by retrieving semantically similar unlabeled examples, assigning
labels to them in a zero-shot fashion, and then using them for in-context
learning. We also propose bag-of-contexts priming, a new priming strategy that
is more suitable for our setting and enables the usage of more examples than
fit into the context window.",None,-1
1fab9e4d-aa8a-4142-9878-9f424a1cdafd,Transformer based Urdu Handwritten Text Optical Character Reader,0.437774,"Extracting Handwritten text is one of the most important components of
digitizing information and making it available for large scale setting.
Handwriting Optical Character Reader (OCR) is a research problem in computer
vision and natural language processing computing, and a lot of work has been
done for English, but unfortunately, very little work has been done for low
resourced languages such as Urdu. Urdu language script is very difficult
because of its cursive nature and change of shape of characters based on it's
relative position, therefore, a need arises to propose a model which can
understand complex features and generalize it for every kind of handwriting
style. In this work, we propose a transformer based Urdu Handwritten text
extraction model. As transformers have been very successful in Natural Language
Understanding task, we explore them further to understand complex Urdu
Handwriting.",None,272
2d70e041-255b-4c8f-ad59-096d10e0c735,DS-GPS : A Deep Statistical Graph Poisson Solver (for faster CFD simulations),0.0446911,"This paper proposes a novel Machine Learning-based approach to solve a
Poisson problem with mixed boundary conditions. Leveraging Graph Neural
Networks, we develop a model able to process unstructured grids with the
advantage of enforcing boundary conditions by design. By directly minimizing
the residual of the Poisson equation, the model attempts to learn the physics
of the problem without the need for exact solutions, in contrast to most
previous data-driven processes where the distance with the available solutions
is minimized.",None,-1
3540d2c8-3c42-4375-bd42-a70b9afe259d,Iterative collaborative routing among equivariant capsules for transformation-robust capsule networks,0.108642,"Transformation-robustness is an important feature for machine learning models
that perform image classification. Many methods aim to bestow this property to
models by the use of data augmentation strategies, while more formal guarantees
are obtained via the use of equivariant models. We recognise that
compositional, or part-whole structure is also an important aspect of images
that has to be considered for building transformation-robust models. Thus, we
propose a capsule network model that is, at once, equivariant and
compositionality-aware. Equivariance of our capsule network model comes from
the use of equivariant convolutions in a carefully-chosen novel architecture.
The awareness of compositionality comes from the use of our proposed novel,
iterative, graph-based routing algorithm, termed Iterative collaborative
routing (ICR). ICR, the core of our contribution, weights the predictions made
for capsules based on an iteratively averaged score of the degree-centralities
of its nearest neighbours. Experiments on transformed image classification on
FashionMNIST, CIFAR-10, and CIFAR-100 show that our model that uses ICR
outperforms convolutional and capsule baselines to achieve state-of-the-art
performance.",None,2794
e1618f89-f475-420a-8b13-eb052e7bdc1d,Region Aware Video Object Segmentation with Deep Motion Modeling,0.34186,"Current semi-supervised video object segmentation (VOS) methods usually
leverage the entire features of one frame to predict object masks and update
memory. This introduces significant redundant computations. To reduce
redundancy, we present a Region Aware Video Object Segmentation (RAVOS)
approach that predicts regions of interest (ROIs) for efficient object
segmentation and memory storage. RAVOS includes a fast object motion tracker to
predict their ROIs in the next frame. For efficient segmentation, object
features are extracted according to the ROIs, and an object decoder is designed
for object-level segmentation. For efficient memory storage, we propose motion
path memory to filter out redundant context by memorizing the features within
the motion path of objects between two frames. Besides RAVOS, we also propose a
large-scale dataset, dubbed OVOS, to benchmark the performance of VOS models
under occlusions. Evaluation on DAVIS and YouTube-VOS benchmarks and our new
OVOS dataset show that our method achieves state-of-the-art performance with
significantly faster inference time, e.g., 86.1 J&F at 42 FPS on DAVIS and 84.4
J&F at 23 FPS on YouTube-VOS.",None,-1
fec7aba1-9b4a-48e8-9f2b-518113db5751,Class-Specific Semantic Reconstruction for Open Set Recognition,0.76311,"Open set recognition enables deep neural networks (DNNs) to identify samples
of unknown classes, while maintaining high classification accuracy on samples
of known classes. Existing methods basing on auto-encoder (AE) and prototype
learning show great potential in handling this challenging task. In this study,
we propose a novel method, called Class-Specific Semantic Reconstruction
(CSSR), that integrates the power of AE and prototype learning. Specifically,
CSSR replaces prototype points with manifolds represented by class-specific
AEs. Unlike conventional prototype-based methods, CSSR models each known class
on an individual AE manifold, and measures class belongingness through AE's
reconstruction error. Class-specific AEs are plugged into the top of the DNN
backbone and reconstruct the semantic representations learned by the DNN
instead of the raw image. Through end-to-end learning, the DNN and the AEs
boost each other to learn both discriminative and representative information.
The results of experiments conducted on multiple datasets show that the
proposed method achieves outstanding performance in both close and open set
recognition and is sufficiently simple and flexible to incorporate into
existing frameworks.",None,-1
10487b90-4195-493e-adea-b8f246b23ce4,Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition,0.77845,"The research and applications of multimodal emotion recognition have become
increasingly popular recently. However, multimodal emotion recognition faces
the challenge of lack of data. To solve this problem, we propose to use
transfer learning which leverages state-of-the-art pre-trained models including
wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including
coattention-based early fusion and late fusion with the models trained on both
embeddings are explored. Also, a multi-granularity framework which extracts not
only frame-level speech embeddings but also segment-level embeddings including
phone, syllable and word-level speech embeddings is proposed to further boost
the performance. By combining our coattention-based early fusion model and late
fusion model with the multi-granularity feature extraction framework, we obtain
result that outperforms best baseline approaches by 1.3% unweighted accuracy
(UA) on the IEMOCAP dataset.",None,-1
f1afa2a0-da6f-4560-8183-9b87dc73b14f,MorDeephy: Face Morphing Detection Via Fused Classification,0.435882,"Face morphing attack detection (MAD) is one of the most challenging tasks in
the field of face recognition nowadays. In this work, we introduce a novel deep
learning strategy for a single image face morphing detection, which implies the
discrimination of morphed face images along with a sophisticated face
recognition task in a complex classification scheme. It is directed onto
learning the deep facial features, which carry information about the
authenticity of these features. Our work also introduces several additional
contributions: the public and easy-to-use face morphing detection benchmark and
the results of our wild datasets filtering strategy. Our method, which we call
MorDeephy, achieved the state of the art performance and demonstrated a
prominent ability for generalising the task of morphing detection to unseen
scenarios.",None,-1
0a5fc900-1ee0-4911-aff2-8ce6f35bd11b,Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation,0.692525,"We introduce Bi-SimCut: a simple but effective training strategy to boost
neural machine translation (NMT) performance. It consists of two procedures:
bidirectional pretraining and unidirectional finetuning. Both procedures
utilize SimCut, a simple regularization method that forces the consistency
between the output distributions of the original and the cutoff sentence pairs.
Without leveraging extra dataset via back-translation or integrating
large-scale pretrained model, Bi-SimCut achieves strong translation performance
across five translation benchmarks (data sizes range from 160K to 20.2M): BLEU
scores of 31.16 for en -> de and 38.37 for de -> en on the IWSLT14 dataset,
30.78 for en -> de and 35.15 for de -> en on the WMT14 dataset, and 27.17 for
zh -> en on the WMT17 dataset. SimCut is not a new method, but a version of
Cutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be
considered as a perturbation-based method. Given the universality and
simplicity of SimCut and Bi-SimCut, we believe they can serve as strong
baselines for future NMT research.",https://github.com/gpengzhi/Bi-SimCut,-1
9f20a40d-82b7-47c0-ad2f-3a8887ea8863,Target-Guided Open-Domain Conversation Planning,0.204206,"Prior studies addressing target-oriented conversational tasks lack a crucial
notion that has been intensively studied in the context of goal-oriented
artificial intelligence agents, namely, planning. In this study, we propose the
task of Target-Guided Open-Domain Conversation Planning (TGCP) task to evaluate
whether neural conversational agents have goal-oriented conversation planning
abilities. Using the TGCP task, we investigate the conversation planning
abilities of existing retrieval models and recent strong generative models. The
experimental results reveal the challenges facing current technology.",https://github.com/y-kishinami/TGCP,-1
1d31bc59-c98c-4f4e-a6a0-856e820d8a93,Resilient Neural Forecasting Systems,0.159601,"Industrial machine learning systems face data challenges that are often
under-explored in the academic literature. Common data challenges are data
distribution shifts, missing values and anomalies. In this paper, we discuss
data challenges and solutions in the context of a Neural Forecasting
application on labor planning.We discuss how to make this forecasting system
resilient to these data challenges. We address changes in data distribution
with a periodic retraining scheme and discuss the critical importance of model
stability in this setting. Furthermore, we show how our deep learning model
deals with missing values natively without requiring imputation. Finally, we
describe how we detect anomalies in the input data and mitigate their effect
before they impact the forecasts. This results in a fully autonomous
forecasting system that compares favorably to a hybrid system consisting of the
algorithm and human overrides.",None,-1
651956c7-cc4b-4ffc-85a5-ea264dfa4f6f,Watching the News: Towards VideoQA Models that can Read,0.517116,"Video Question Answering methods focus on commonsense reasoning and visual
cognition of objects or persons and their interactions over time. Current
VideoQA approaches ignore the textual information present in the video.
Instead, we argue that textual information is complementary to the action and
provides essential contextualisation cues to the reasoning process. To this
end, we propose a novel VideoQA task that requires reading and understanding
the text in the video. To explore this direction, we focus on news videos and
require QA systems to comprehend and answer questions about the topics
presented by combining visual and textual cues in the video. We introduce the
``NewsVideoQA'' dataset that comprises more than $8,600$ QA pairs on $3,000+$
news videos obtained from diverse news channels from around the world. We
demonstrate the limitations of current Scene Text VQA and VideoQA methods and
propose ways to incorporate scene text information into VideoQA methods.",https://github.com/facebookresearch/mmf,-1
b22d3975-0a25-4f44-a97c-15031844dacc,Representative Image Feature Extraction via Contrastive Learning Pretraining for Chest X-ray Report Generation,0.226424,"Medical report generation is a challenging task since it is time-consuming
and requires expertise from experienced radiologists. The goal of medical
report generation is to accurately capture and describe the image findings.
Previous works pretrain their visual encoding neural networks with large
datasets in different domains, which cannot learn general visual representation
in the specific medical domain. In this work, we propose a medical report
generation framework that uses a contrastive learning approach to pretrain the
visual encoder and requires no additional meta information. In addition, we
adopt lung segmentation as an augmentation method in the contrastive learning
framework. This segmentation guides the network to focus on encoding the visual
feature within the lung region. Experimental results show that the proposed
framework improves the performance and the quality of the generated medical
reports both quantitatively and qualitatively.",https://github.com/rani700/xray,-1
5ed41d31-663d-4dad-9f0d-0c1165013c03,"Sampling-based inference for large linear models, with application to linearised Laplace",0.636314,"Large-scale linear models are ubiquitous throughout machine learning, with
contemporary application as surrogate models for neural network uncertainty
quantification; that is, the linearised Laplace method. Alas, the computational
cost associated with Bayesian linear models constrains this method's
application to small networks, small output spaces and small datasets. We
address this limitation by introducing a scalable sample-based Bayesian
inference method for conjugate Gaussian multi-output linear models, together
with a matching method for hyperparameter (regularisation) selection.
Furthermore, we use a classic feature normalisation method (the g-prior) to
resolve a previously highlighted pathology of the linearised Laplace method.
Together, these contributions allow us to perform linearised neural network
inference with ResNet-18 on CIFAR100 (11M parameters, 100 outputs x 50k
datapoints), with ResNet-50 on Imagenet (50M parameters, 1000 outputs x 1.2M
datapoints) and with a U-Net on a high-resolution tomographic reconstruction
task (2M parameters, 251k output~dimensions).",None,-1
06e3f07c-2061-452e-94ab-e375c75a28c4,CADOps-Net: Jointly Learning CAD Operation Types and Steps from Boundary-Representations,0.587785,"3D reverse engineering is a long sought-after, yet not completely achieved
goal in the Computer-Aided Design (CAD) industry. The objective is to recover
the construction history of a CAD model. Starting from a Boundary
Representation (B-Rep) of a CAD model, this paper proposes a new deep neural
network, CADOps-Net, that jointly learns the CAD operation types and the
decomposition into different CAD operation steps. This joint learning allows to
divide a B-Rep into parts that were created by various types of CAD operations
at the same construction step; therefore providing relevant information for
further recovery of the design history. Furthermore, we propose the novel
CC3D-Ops dataset that includes over $37k$ CAD models annotated with CAD
operation type labels and step labels. Compared to existing datasets, the
complexity and variety of CC3D-Ops models are closer to those used for
industrial purposes. Our experiments, conducted on the proposed CC3D-Ops and
the publicly available Fusion360 datasets, demonstrate the competitive
performance of CADOps-Net with respect to state-of-the-art, and confirm the
importance of the joint learning of CAD operation types and steps.",None,-1
9bf8a5d7-a8f8-444e-9404-94e6ba06524f,Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations,0.624212,"Recently, weakly-supervised image segmentation using weak annotations like
scribbles has gained great attention, since such annotations are much easier to
obtain compared to time-consuming and label-intensive labeling at the
pixel/voxel level. However, because scribbles lack structure information of
region of interest (ROI), existing scribble-based methods suffer from poor
boundary localization. Furthermore, most current methods are designed for 2D
image segmentation, which do not fully leverage the volumetric information if
directly applied to image slices. In this paper, we propose a scribble-based
volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image
segmentation and improves boundary prediction. To achieve this, we augment a
2.5D attention UNet with a proposed label propagation module to extend semantic
information from scribbles and a combination of static and active boundary
prediction to learn ROI's boundary and regularize its shape. Extensive
experiments on three public datasets demonstrate Scribble2D5 significantly
outperforms current scribble-based methods and approaches the performance of
fully-supervised ones. Our code is available online.",https://github.com/Qybc/Scribble2D5,-1
c2ea177c-47d2-4a8e-9e75-0f0bf7da5c48,Neural Network Compression of ACAS Xu Early Prototype is Unsafe: Closed-Loop Verification through Quantized State Backreachability,0.366395,"ACAS Xu is an air-to-air collision avoidance system designed for unmanned
aircraft that issues horizontal turn advisories to avoid an intruder aircraft.
Due the use of a large lookup table in the design, a neural network compression
of the policy was proposed. Analysis of this system has spurred a significant
body of research in the formal methods community on neural network
verification. While many powerful methods have been developed, most work
focuses on open-loop properties of the networks, rather than the main point of
the system -- collision avoidance -- which requires closed-loop analysis.
  In this work, we develop a technique to verify a closed-loop approximation of
the system using state quantization and backreachability. We use favorable
assumptions for the analysis -- perfect sensor information, instant following
of advisories, ideal aircraft maneuvers and an intruder that only flies
straight. When the method fails to prove the system is safe, we refine the
quantization parameters until generating counterexamples where the original
(non-quantized) system also has collisions.",https://github.com/stanleybak/quantized_nn_backreach/releases/tag/NFM2022_submitted,-1
9cb2e46b-fc92-440d-a3d1-4ee4b3db4526,Speciesist bias in AI -- How AI applications perpetuate discrimination and unfair outcomes against animals,0.482827,"Massive efforts are made to reduce biases in both data and algorithms in
order to render AI applications fair. These efforts are propelled by various
high-profile cases where biased algorithmic decision-making caused harm to
women, people of color, minorities, etc. However, the AI fairness field still
succumbs to a blind spot, namely its insensitivity to discrimination against
animals. This paper is the first to describe the 'speciesist bias' and
investigate it in several different AI systems. Speciesist biases are learned
and solidified by AI applications when they are trained on datasets in which
speciesist patterns prevail. These patterns can be found in image recognition
systems, large language models, and recommender systems. Therefore, AI
technologies currently play a significant role in perpetuating and normalizing
violence against animals. This can only be changed when AI fairness frameworks
widen their scope and include mitigation measures for speciesist biases. This
paper addresses the AI community in this regard and stresses the influence AI
systems can have on either increasing or reducing the violence that is
inflicted on animals, and especially on farmed animals.",None,-1
abef5d5d-9f00-4569-a289-f38c90ebd205,WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection,0.451848,"Monocular 3D object detection is one of the most challenging tasks in 3D
scene understanding. Due to the ill-posed nature of monocular imagery, existing
monocular 3D detection methods highly rely on training with the manually
annotated 3D box labels on the LiDAR point clouds. This annotation process is
very laborious and expensive. To dispense with the reliance on 3D box labels,
in this paper we explore the weakly supervised monocular 3D detection.
Specifically, we first detect 2D boxes on the image. Then, we adopt the
generated 2D boxes to select corresponding RoI LiDAR points as the weak
supervision. Eventually, we adopt a network to predict 3D boxes which can
tightly align with associated RoI LiDAR points. This network is learned by
minimizing our newly-proposed 3D alignment loss between the 3D box estimates
and the corresponding RoI LiDAR points. We will illustrate the potential
challenges of the above learning problem and resolve these challenges by
introducing several effective designs into our method. Codes will be available
at https://github.com/SPengLiang/WeakM3D.",https://github.com/SPengLiang/WeakM3D,-1
5c5de268-071e-4c7b-9168-cd0158ffd7d4,Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework,0.532069,"Simultaneous machine translation (SiMT) starts translating while receiving
the streaming source inputs, and hence the source sentence is always incomplete
during translating. Different from the full-sentence MT using the conventional
seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,
which forces each target word to only align with a partial source prefix to
adapt to the incomplete source in streaming inputs. However, the source words
in the front positions are always illusoryly considered more important since
they appear in more prefixes, resulting in position bias, which makes the model
pay more attention on the front source positions in testing. In this paper, we
first analyze the phenomenon of position bias in SiMT, and develop a
Length-Aware Framework to reduce the position bias by bridging the structural
gap between SiMT and full-sentence MT. Specifically, given the streaming
inputs, we first predict the full-sentence length and then fill the future
source position with positional encoding, thereby turning the streaming inputs
into a pseudo full-sentence. The proposed framework can be integrated into most
existing SiMT methods to further improve performance. Experiments on two
representative SiMT methods, including the state-of-the-art adaptive policy,
show that our method successfully reduces the position bias and thereby
achieves better SiMT performance.",https://github.com/pytorch/fairseq/tree/master/examples/simultaneous_translation,-1
9790e377-bf8b-4b9b-acce-da883a783586,A Human Rights-Based Approach to Responsible AI,0.63646,"Research on fairness, accountability, transparency and ethics of AI-based
interventions in society has gained much-needed momentum in recent years.
However it lacks an explicit alignment with a set of normative values and
principles that guide this research and interventions. Rather, an implicit
consensus is often assumed to hold for the values we impart into our models -
something that is at odds with the pluralistic world we live in. In this paper,
we put forth the doctrine of universal human rights as a set of globally
salient and cross-culturally recognized set of values that can serve as a
grounding framework for explicit value alignment in responsible AI - and
discuss its efficacy as a framework for civil society partnership and
participation. We argue that a human rights framework orients the research in
this space away from the machines and the risks of their biases, and towards
humans and the risks to their rights, essentially helping to center the
conversation around who is harmed, what harms they face, and how those harms
may be mitigated.",None,-1
9a5051f5-2377-4e3b-8fae-46cf19420745,Blueprint Separable Residual Network for Efficient Image Super-Resolution,0.953037,"Recent advances in single image super-resolution (SISR) have achieved
extraordinary performance, but the computational cost is too heavy to apply in
edge devices. To alleviate this problem, many novel and effective solutions
have been proposed. Convolutional neural network (CNN) with the attention
mechanism has attracted increasing attention due to its efficiency and
effectiveness. However, there is still redundancy in the convolution operation.
In this paper, we propose Blueprint Separable Residual Network (BSRN)
containing two efficient designs. One is the usage of blueprint separable
convolution (BSConv), which takes place of the redundant convolution operation.
The other is to enhance the model ability by introducing more effective
attention modules. The experimental results show that BSRN achieves
state-of-the-art performance among existing efficient SR methods. Moreover, a
smaller variant of our model BSRN-S won the first place in model complexity
track of NTIRE 2022 Efficient SR Challenge. The code is available at
https://github.com/xiaom233/BSRN.",https://github.com/xiaom233/BSRN,-1
2251fc8e-9d77-4b09-afa8-07a2c8507161,Injecting Domain Knowledge in Language Models for Task-Oriented Dialogue Systems,0.904422,"Pre-trained language models (PLM) have advanced the state-of-the-art across
NLP applications, but lack domain-specific knowledge that does not naturally
occur in pre-training data. Previous studies augmented PLMs with symbolic
knowledge for different downstream NLP tasks. However, knowledge bases (KBs)
utilized in these studies are usually large-scale and static, in contrast to
small, domain-specific, and modifiable knowledge bases that are prominent in
real-world task-oriented dialogue (TOD) systems. In this paper, we showcase the
advantages of injecting domain-specific knowledge prior to fine-tuning on TOD
tasks. To this end, we utilize light-weight adapters that can be easily
integrated with PLMs and serve as a repository for facts learned from different
KBs. To measure the efficacy of proposed knowledge injection methods, we
introduce Knowledge Probing using Response Selection (KPRS) -- a probe designed
specifically for TOD models. Experiments on KPRS and the response generation
task show improvements of knowledge injection with adapters over strong
baselines.",https://github.com/amazon-research/domain-knowledge-injection,-1
374daa14-6780-4860-9181-a9a4b0d5fe11,Optimal Fine-Grained N:M sparsity for Activations and Neural Gradients,0.479383,"In deep learning, fine-grained N:M sparsity reduces the data footprint and
bandwidth of a General Matrix multiply (GEMM) by x2, and doubles throughput by
skipping computation of zero values. So far, it was only used to prune weights.
We examine how this method can be used also for activations and their gradients
(i.e., ""neural gradients""). To this end, we first establish a tensor-level
optimality criteria. Previous works aimed to minimize the mean-square-error
(MSE) of each pruned block. We show that while minimization of the MSE works
fine for pruning the activations, it catastrophically fails for the neural
gradients. Instead, we show that optimal pruning of the neural gradients
requires an unbiased minimum-variance pruning mask. We design such specialized
masks, and find that in most cases, 1:2 sparsity is sufficient for training,
and 2:4 sparsity is usually enough when this is not the case. Further, we
suggest combining several such methods together in order to potentially speed
up training even more. A reference implementation is supplied in
https://github.com/brianchmiel/Act-and-Grad-structured-sparsity.",https://github.com/brianchmiel/Act-and-Grad-structured-sparsity,-1
1a2ddaed-43a9-430c-b76a-9a628467c270,CLIPascene: Scene Sketching with Different Types and Levels of Abstraction,0.579664,"In this paper, we present a method for converting a given scene image into a
sketch using different types and multiple levels of abstraction. We distinguish
between two types of abstraction. The first considers the fidelity of the
sketch, varying its representation from a more precise portrayal of the input
to a looser depiction. The second is defined by the visual simplicity of the
sketch, moving from a detailed depiction to a sparse sketch. Using an explicit
disentanglement into two abstraction axes -- and multiple levels for each one
-- provides users additional control over selecting the desired sketch based on
their personal goals and preferences. To form a sketch at a given level of
fidelity and simplification, we train two MLP networks. The first network
learns the desired placement of strokes, while the second network learns to
gradually remove strokes from the sketch without harming its recognizability
and semantics. Our approach is able to generate sketches of complex scenes
including those with complex backgrounds (e.g., natural and urban settings) and
subjects (e.g., animals and people) while depicting gradual abstractions of the
input scene in terms of fidelity and simplicity.",None,-1
1ef821f7-1e8e-4753-9041-abb577295e56,Bio-inspired Min-Nets Improve the Performance and Robustness of Deep Networks,0.387023,"Min-Nets are inspired by end-stopped cortical cells with units that output
the minimum of two learned filters. We insert such Min-units into
state-of-the-art deep networks, such as the popular ResNet and DenseNet, and
show that the resulting Min-Nets perform better on the Cifar-10 benchmark.
Moreover, we show that Min-Nets are more robust against JPEG compression
artifacts. We argue that the minimum operation is the simplest way of
implementing an AND operation on pairs of filters and that such AND operations
introduce a bias that is appropriate given the statistics of natural images.",https://github.com/pgruening/bio_inspired_min_nets_improve_the_performance_and_robustness_of_deep_networks,7477
94bb2f18-52d6-4657-a709-afd1bfd49023,Introspective Learning : A Two-Stage Approach for Inference in Neural Networks,0.183427,"In this paper, we advocate for two stages in a neural network's decision
making process. The first is the existing feed-forward inference framework
where patterns in given data are sensed and associated with previously learned
patterns. The second stage is a slower reflection stage where we ask the
network to reflect on its feed-forward decision by considering and evaluating
all available choices. Together, we term the two stages as introspective
learning. We use gradients of trained neural networks as a measurement of this
reflection. A simple three-layered Multi Layer Perceptron is used as the second
stage that predicts based on all extracted gradient features. We perceptually
visualize the post-hoc explanations from both stages to provide a visual
grounding to introspection. For the application of recognition, we show that an
introspective network is 4% more robust and 42% less prone to calibration
errors when generalizing to noisy data. We also illustrate the value of
introspective networks in downstream tasks that require generalizability and
calibration including active learning, out-of-distribution detection, and
uncertainty estimation. Finally, we ground the proposed machine introspection
to human introspection for the application of image quality assessment.",https://github.com/olivesgatech/Introspective-Learning,-1
d127a589-afae-425b-badb-1ba5c5dc275d,EA$^2$E: Improving Consistency with Event Awareness for Document-Level Argument Extraction,0.771406,"Events are inter-related in documents. Motivated by the
one-sense-per-discourse theory, we hypothesize that a participant tends to play
consistent roles across multiple events in the same document. However recent
work on document-level event argument extraction models each individual event
in isolation and therefore causes inconsistency among extracted arguments
across events, which will further cause discrepancy for downstream applications
such as event knowledge base population, question answering, and hypothesis
generation. In this work, we formulate event argument consistency as the
constraints from event-event relations under the document-level setting. To
improve consistency we introduce the Event-Aware Argument Extraction (EA$^2$E)
model with augmented context for training and inference. Experiment results on
WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA$^2$E
compared to baseline methods.",https://github.com/ZQS1943/DOCIE,-1
73913274-54d6-49ab-a5fe-ca43247ad24f,Path-Aware Graph Attention for HD Maps in Motion Prediction,0.786419,"The success of motion prediction for autonomous driving relies on integration
of information from the HD maps. As maps are naturally graph-structured,
investigation on graph neural networks (GNNs) for encoding HD maps is
burgeoning in recent years. However, unlike many other applications where GNNs
have been straightforwardly deployed, HD maps are heterogeneous graphs where
vertices (lanes) are connected by edges (lane-lane interaction relationships)
of various nature, and most graph-based models are not designed to understand
the variety of edge types which provide crucial cues for predicting how the
agents would travel the lanes. To overcome this challenge, we propose
Path-Aware Graph Attention, a novel attention architecture that infers the
attention between two vertices by parsing the sequence of edges forming the
paths that connect them. Our analysis illustrates how the proposed attention
mechanism can facilitate learning in a didactic problem where existing graph
networks like GCN struggle. By improving map encoding, the proposed model
surpasses previous state of the art on the Argoverse Motion Forecasting
dataset, and won the first place in the 2021 Argoverse Motion Forecasting
Competition.",None,-1
1c1db1b6-76c0-4066-a852-ee50d2d49192,Adam Mickiewicz University at WMT 2022: NER-Assisted and Quality-Aware Neural Machine Translation,0.49849,"This paper presents Adam Mickiewicz University's (AMU) submissions to the
constrained track of the WMT 2022 General MT Task. We participated in the
Ukrainian $\leftrightarrow$ Czech translation directions. The systems are a
weighted ensemble of four models based on the Transformer (big) architecture.
The models use source factors to utilize the information about named entities
present in the input. Each of the models in the ensemble was trained using only
the data provided by the shared task organizers. A noisy back-translation
technique was used to augment the training corpora. One of the models in the
ensemble is a document-level model, trained on parallel and synthetic longer
sequences. During the sentence-level decoding process, the ensemble generated
the n-best list. The n-best list was merged with the n-best list generated by a
single document-level model which translated multiple sentences at a time.
Finally, existing quality estimation models and minimum Bayes risk decoding
were used to rerank the n-best list so that the best hypothesis was chosen
according to the COMET evaluation metric. According to the automatic evaluation
results, our systems rank first in both translation directions.",None,-1
98f90968-6ed8-4a41-b9e7-c7b718d1791e,Unifying Short and Long-Term Tracking with Graph Hierarchies,0.775434,"Tracking objects over long videos effectively means solving a spectrum of
problems, from short-term association for un-occluded objects to long-term
association for objects that are occluded and then reappear in the scene.
Methods tackling these two tasks are often disjoint and crafted for specific
scenarios, and top-performing approaches are often a mix of techniques, which
yields engineering-heavy solutions that lack generality. In this work, we
question the need for hybrid approaches and introduce SUSHI, a unified and
scalable multi-object tracker. Our approach processes long clips by splitting
them into a hierarchy of subclips, which enables high scalability. We leverage
graph neural networks to process all levels of the hierarchy, which makes our
model unified across temporal scales and highly general. As a result, we obtain
significant improvements over state-of-the-art on four diverse datasets. Our
code and models are available at bit.ly/sushi-mot.",None,-1
12b13e01-54c2-4068-bfd3-f75685209d18,Rendering Nighttime Image Via Cascaded Color and Brightness Compensation,0.0511761,"Image signal processing (ISP) is crucial for camera imaging, and neural
networks (NN) solutions are extensively deployed for daytime scenes. The lack
of sufficient nighttime image dataset and insights on nighttime illumination
characteristics poses a great challenge for high-quality rendering using
existing NN ISPs. To tackle it, we first built a high-resolution nighttime
RAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert
professionals. Meanwhile, to best capture the characteristics of nighttime
illumination light sources, we develop the CBUnet, a two-stage NN ISP to
cascade the compensation of color and brightness attributes. Experiments show
that our method has better visual quality compared to traditional ISP pipeline,
and is ranked at the second place in the NTIRE 2022 Night Photography Rendering
Challenge for two tracks by respective People's and Professional Photographer's
choices. The code and relevant materials are avaiable on our website:
https://njuvision.github.io/CBUnet.",None,6
45abe536-ba12-4cf8-8793-51a74c150ba3,Forming Effective Human-AI Teams: Building Machine Learning Models that Complement the Capabilities of Multiple Experts,0.603157,"Machine learning (ML) models are increasingly being used in application
domains that often involve working together with human experts. In this
context, it can be advantageous to defer certain instances to a single human
expert when they are difficult to predict for the ML model. While previous work
has focused on scenarios with one distinct human expert, in many real-world
situations several human experts with varying capabilities may be available. In
this work, we propose an approach that trains a classification model to
complement the capabilities of multiple human experts. By jointly training the
classifier together with an allocation system, the classifier learns to
accurately predict those instances that are difficult for the human experts,
while the allocation system learns to pass each instance to the most suitable
team member -- either the classifier or one of the human experts. We evaluate
our proposed approach in multiple experiments on public datasets with
""synthetic"" experts and a real-world medical dataset annotated by multiple
radiologists. Our approach outperforms prior work and is more accurate than the
best human expert or a classifier. Furthermore, it is flexibly adaptable to
teams of varying sizes and different levels of expert diversity.",https://github.com/ptrckhmmr/human-ai-teams,-1
4bf1033a-2116-4e14-8728-49d90a26e9d1,MAGVIT: Masked Generative Video Transformer,0.903484,"We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle
various video synthesis tasks with a single model. We introduce a 3D tokenizer
to quantize a video into spatial-temporal visual tokens and propose an
embedding method for masked video token modeling to facilitate multi-task
learning. We conduct extensive experiments to demonstrate the quality,
efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT
performs favorably against state-of-the-art approaches and establishes the
best-published FVD on three video generation benchmarks, including the
challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference
time by two orders of magnitude against diffusion models and by 60x against
autoregressive models. (iii) A single MAGVIT model supports ten diverse
generation tasks and generalizes across videos from different visual domains.
The source code and trained models will be released to the public at
https://magvit.cs.cmu.edu.",https://magvit.cs.cmu.edu,-1
2cedb55c-94bb-4e32-8d1f-1e77a596cc90,Social Construction of XAI: Do We Need One Definition to Rule Them All?,0.211605,"There is a growing frustration amongst researchers and developers in
Explainable AI (XAI) around the lack of consensus around what is meant by
'explainability'. Do we need one definition of explainability to rule them all?
In this paper, we argue why a singular definition of XAI is neither feasible
nor desirable at this stage of XAI's development. We view XAI through the
lenses of Social Construction of Technology (SCOT) to explicate how diverse
stakeholders (relevant social groups) have different interpretations
(interpretative flexibility) that shape the meaning of XAI. Forcing a
standardization (closure) on the pluralistic interpretations too early can
stifle innovation and lead to premature conclusions. We share how we can
leverage the pluralism to make progress in XAI without having to wait for a
definitional consensus.",None,-1
c69f70d0-72ad-43db-a297-b310370479d2,Game-theoretic Objective Space Planning,0.506061,"Generating competitive strategies and performing continuous motion planning
simultaneously in an adversarial setting is a challenging problem. In addition,
understanding the intent of other agents is crucial to deploying autonomous
systems in adversarial multi-agent environments. Existing approaches either
discretize agent action by grouping similar control inputs, sacrificing
performance in motion planning, or plan in uninterpretable latent spaces,
producing hard-to-understand agent behaviors. Furthermore, the most popular
policy optimization frameworks do not recognize the long-term effect of actions
and become myopic. This paper proposes an agent action discretization method
via abstraction that provides clear intentions of agent actions, an efficient
offline pipeline of agent population synthesis, and a planning strategy using
counterfactual regret minimization with function approximation. Finally, we
experimentally validate our findings on scaled autonomous vehicles in a
head-to-head racing setting. We demonstrate that using the proposed framework
significantly improves learning, improves the win rate against different
opponents, and the improvements can be transferred to unseen opponents in an
unseen environment.",None,-1
d408d1a1-ccdf-420a-a397-2970b083b794,CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation,0.572707,"3D human pose estimation can be handled by encoding the geometric
dependencies between the body parts and enforcing the kinematic constraints.
Recently, Transformer has been adopted to encode the long-range dependencies
between the joints in the spatial and temporal domains. While they had shown
excellence in long-range dependencies, studies have noted the need for
improving the locality of vision Transformers. In this direction, we propose a
novel pose estimation Transformer featuring rich representations of body joints
critical for capturing subtle changes across frames (i.e., inter-feature
representation). Specifically, through two novel interaction modules;
Cross-Joint Interaction and Cross-Frame Interaction, the model explicitly
encodes the local and global dependencies between the body joints. The proposed
architecture achieved state-of-the-art performance on two popular 3D human pose
estimation datasets, Human3.6 and MPI-INF-3DHP. In particular, our proposed
CrossFormer method boosts performance by 0.9% and 0.3%, compared to the closest
counterpart, PoseFormer, using the detected 2D poses and ground-truth settings
respectively.",https://github.com/xxx,-1
07d35448-f200-4aad-95c8-5159aeb1996f,Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity,0.981761,"Large Language Models (LLMs) have demonstrated impressive capabilities in
generating fluent text, as well as tendencies to reproduce undesirable social
biases. This study investigates whether LLMs reproduce the moral biases
associated with political groups in the United States, an instance of a broader
capability herein termed moral mimicry. This hypothesis is explored in the
GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral
Foundations Theory, it is shown that these LLMs are indeed moral mimics. When
prompted with a liberal or conservative political identity, the models generate
text reflecting corresponding moral biases. This study also explores the
relationship between moral mimicry and model size, and similarity between human
and LLM moral word use.",https://github.com/mbforbes/social-chemistry-101,-1
71538419-70b8-4226-884c-b42a3a0af0fa,PolyHope: Two-Level Hope Speech Detection from Tweets,0.848905,"Hope is characterized as openness of spirit toward the future, a desire,
expectation, and wish for something to happen or to be true that remarkably
affects human's state of mind, emotions, behaviors, and decisions. Hope is
usually associated with concepts of desired expectations and
possibility/probability concerning the future. Despite its importance, hope has
rarely been studied as a social media analysis task. This paper presents a hope
speech dataset that classifies each tweet first into ""Hope"" and ""Not Hope"",
then into three fine-grained hope categories: ""Generalized Hope"", ""Realistic
Hope"", and ""Unrealistic Hope"" (along with ""Not Hope""). English tweets in the
first half of 2022 were collected to build this dataset. Furthermore, we
describe our annotation process and guidelines in detail and discuss the
challenges of classifying hope and the limitations of the existing hope speech
detection corpora. In addition, we reported several baselines based on
different learning approaches, such as traditional machine learning, deep
learning, and transformers, to benchmark our dataset. We evaluated our
baselines using weighted-averaged and macro-averaged F1-scores. Observations
show that a strict process for annotator selection and detailed annotation
guidelines enhanced the dataset's quality. This strict annotation process
resulted in promising performance for simple machine learning classifiers with
only bi-grams; however, binary and multiclass hope speech detection results
reveal that contextual embedding models have higher performance in this
dataset.",None,-1
8323484f-0d22-49e9-91ae-4937bf7ff9b6,MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning,0.714053,"Transformer language models (TLMs) are critical for most NLP tasks, but they
are difficult to create for low-resource languages because of how much
pretraining data they require. In this work, we investigate two techniques for
training monolingual TLMs in a low-resource setting: greatly reducing TLM size,
and complementing the masked language modeling objective with two
linguistically rich supervised tasks (part-of-speech tagging and dependency
parsing). Results from 7 diverse languages indicate that our model, MicroBERT,
is able to produce marked improvements in downstream task evaluations relative
to a typical monolingual TLM pretraining approach. Specifically, we find that
monolingual MicroBERT models achieve gains of up to 18% for parser LAS and 11%
for NER F1 compared to a multilingual baseline, mBERT, while having less than
1% of its parameter count. We conclude reducing TLM parameter count and using
labeled data for pretraining low-resource TLMs can yield large quality benefits
and in some cases produce models that outperform multilingual approaches.",https://github.com/lgessler/microbert,-1
9dc49c8d-b0c7-473f-847a-d5b7abd11f5f,Experiencer-Specific Emotion and Appraisal Prediction,0.685414,"Emotion classification in NLP assigns emotions to texts, such as sentences or
paragraphs. With texts like ""I felt guilty when he cried"", focusing on the
sentence level disregards the standpoint of each participant in the situation:
the writer (""I"") and the other entity (""he"") could in fact have different
affective states. The emotions of different entities have been considered only
partially in emotion semantic role labeling, a task that relates semantic roles
to emotion cue words. Proposing a related task, we narrow the focus on the
experiencers of events, and assign an emotion (if any holds) to each of them.
To this end, we represent each emotion both categorically and with appraisal
variables, as a psychological access to explaining why a person develops a
particular emotion. On an event description corpus, our experiencer-aware
models of emotions and appraisals outperform the experiencer-agnostic
baselines, showing that disregarding event participants is an
oversimplification for the emotion detection task.",https://www.ims.uni-stuttgart.de/data/appraisalemotion,-1
729ce3f9-2a30-4961-a611-2335743c352a,Coverage Optimization of Camera Network for Continuous Deformable Object,0.05577,"In this paper, a deformable object is considered for cameras deployment with
the aim of visual coverage. The object contour is discretized into sampled
points as meshes, and the deformation is represented as continuous trajectories
for the sampled points. To reduce the computational complexity, some feature
points are carefully selected representing the continuous deformation process,
and the visual coverage for the deformable object is transferred to cover the
specific feature points. In particular, the vertexes of a rectangle that can
contain the entire deformation trajectory of every sampled point on the object
contour are chosen as the feature points. An improved wolf pack algorithm is
then proposed to solve the optimization problem. Finally, simulation results
are given to demonstrate the effectiveness of the proposed deployment method of
camera network.",None,-1
151faea4-77ac-45cf-a264-7d5413b209bb,Shape-Guided Diffusion with Inside-Outside Attention,0.765696,"We introduce precise object silhouette as a new form of user control in
text-to-image diffusion models, which we dub Shape-Guided Diffusion. Our
training-free method uses an Inside-Outside Attention mechanism during the
inversion and generation process to apply a shape constraint to the cross- and
self-attention maps. Our mechanism designates which spatial region is the
object (inside) vs. background (outside) then associates edits to the correct
region. We demonstrate the efficacy of our method on the shape-guided editing
task, where the model must replace an object according to a text prompt and
object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and
achieve SOTA results in shape faithfulness without a degradation in text
alignment or image realism according to both automatic metrics and annotator
ratings. Our data and code will be made available at
https://shape-guided-diffusion.github.io.",https://shape-guided-diffusion.github.io,-1
463b7a67-d36e-4136-ba50-238b0efa2dd4,Find a Way Forward: a Language-Guided Semantic Map Navigator,0.112933,"In this paper, we introduce the map-language navigation task where an agent
executes natural language instructions and moves to the target position based
only on a given 3D semantic map. To tackle the task, we design the
instruction-aware Path Proposal and Discrimination model (iPPD). Our approach
leverages map information to provide instruction-aware path proposals, i.e., it
selects all potential instruction-aligned candidate paths to reduce the
solution space. Next, to represent the map observations along a path for a
better modality alignment, a novel Path Feature Encoding scheme tailored for
semantic maps is proposed. An attention-based Language Driven Discriminator is
designed to evaluate path candidates and determine the best path as the final
result. Our method can naturally avoid error accumulation compared with
single-step greedy decision methods. Comparing to a single-step imitation
learning approach, iPPD has performance gains above 17% on navigation success
and 0.18 on path matching measurement nDTW in challenging unseen environments.",None,-1
515dbbbb-4f9b-444b-a850-9bb45a67f65f,Contrastive Training Improves Zero-Shot Classification of Semi-structured Documents,0.0173321,"We investigate semi-structured document classification in a zero-shot
setting. Classification of semi-structured documents is more challenging than
that of standard unstructured documents, as positional, layout, and style
information play a vital role in interpreting such documents. The standard
classification setting where categories are fixed during both training and
testing falls short in dynamic environments where new document categories could
potentially emerge. We focus exclusively on the zero-shot setting where
inference is done on new unseen classes. To address this task, we propose a
matching-based approach that relies on a pairwise contrastive objective for
both pretraining and fine-tuning. Our results show a significant boost in Macro
F$_1$ from the proposed pretraining step in both supervised and unsupervised
zero-shot settings.",https://github.com/jsvine/pdfplumber,-1
5f9bf4a9-b074-4f98-8617-ae9c19eb5773,Does the Market of Citations Reward Reproducible Work?,0.351938,"The field of bibliometrics, studying citations and behavior, is critical to
the discussion of reproducibility. Citations are one of the primary incentive
and reward systems for academic work, and so we desire to know if this
incentive rewards reproducible work. Yet to the best of our knowledge, only one
work has attempted to look at this combined space, concluding that
non-reproducible work is more highly cited. We show that answering this
question is more challenging than first proposed, and subtle issues can inhibit
a robust conclusion. To make inferences with more robust behavior, we propose a
hierarchical Bayesian model that incorporates the citation rate over time,
rather than the total number of citations after a fixed amount of time. In
doing so we show that, under current evidence the answer is more likely that
certain fields of study such as Medicine and Machine Learning (ML) do correlate
reproducible works with more citations, but other fields appear to have no
relationship. Further, we find that making code available and thoroughly
referencing prior works appear to also positively correlate with increased
citations. Our code and data can be found at
https://github.com/EdwardRaff/ReproducibleCitations .",None,-1
211b27bb-a6f4-4866-aac3-5535894b6251,WR-ONE2SET: Towards Well-Calibrated Keyphrase Generation,0.0943829,"Keyphrase generation aims to automatically generate short phrases summarizing
an input document. The recently emerged ONE2SET paradigm (Ye et al., 2021)
generates keyphrases as a set and has achieved competitive performance.
Nevertheless, we observe serious calibration errors outputted by ONE2SET,
especially in the over-estimation of $\varnothing$ token (means ""no
corresponding keyphrase""). In this paper, we deeply analyze this limitation and
identify two main reasons behind: 1) the parallel generation has to introduce
excessive $\varnothing$ as padding tokens into training instances; and 2) the
training mechanism assigning target to each slot is unstable and further
aggravates the $\varnothing$ token over-estimation. To make the model
well-calibrated, we propose WR-ONE2SET which extends ONE2SET with an adaptive
instance-level cost Weighting strategy and a target Re-assignment mechanism.
The former dynamically penalizes the over-estimated slots for different
instances thus smoothing the uneven training distribution. The latter refines
the original inappropriate assignment and reduces the supervisory signals of
over-estimated slots. Experimental results on commonly-used datasets
demonstrate the effectiveness and generality of our proposed paradigm.",https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py,13445
50d819b7-3e23-4a2c-bde8-772525a655a7,Unsupervised Image Representation Learning with Deep Latent Particles,0.0742353,"We propose a new representation of visual data that disentangles object
position from appearance. Our method, termed Deep Latent Particles (DLP),
decomposes the visual input into low-dimensional latent ``particles'', where
each particle is described by its spatial location and features of its
surrounding region. To drive learning of such representations, we follow a
VAE-based approach and introduce a prior for particle positions based on a
spatial-softmax architecture, and a modification of the evidence lower bound
loss inspired by the Chamfer distance between particles. We demonstrate that
our DLP representations are useful for downstream tasks such as unsupervised
keypoint (KP) detection, image manipulation, and video prediction for scenes
composed of multiple dynamic objects. In addition, we show that our
probabilistic interpretation of the problem naturally provides uncertainty
estimates for particle locations, which can be used for model selection, among
other tasks. Videos and code are available:
https://taldatech.github.io/deep-latent-particles-web/",https://github.com/taldatech/deep-latent-particles-pytorch,-1
3510782f-afd9-4c78-926e-cfba6024ba99,CoLI-Machine Learning Approaches for Code-mixed Language Identification at the Word Level in Kannada-English Texts,0.931666,"The task of automatically identifying a language used in a given text is
called Language Identification (LI). India is a multilingual country and many
Indians especially youths are comfortable with Hindi and English, in addition
to their local languages. Hence, they often use more than one language to post
their comments on social media. Texts containing more than one language are
called ""code-mixed texts"" and are a good source of input for LI. Languages in
these texts may be mixed at sentence level, word level or even at sub-word
level. LI at word level is a sequence labeling problem where each and every
word in a sentence is tagged with one of the languages in the predefined set of
languages. In order to address word level LI in code-mixed Kannada-English
(Kn-En) texts, this work presents i) the construction of code-mixed Kn-En
dataset called CoLI-Kenglish dataset, ii) code-mixed Kn-En embedding and iii)
learning models using Machine Learning (ML), Deep Learning (DL) and Transfer
Learning (TL) approaches. Code-mixed Kn-En texts are extracted from Kannada
YouTube video comments to construct CoLI-Kenglish dataset and code-mixed Kn-En
embedding. The words in CoLI-Kenglish dataset are grouped into six major
categories, namely, ""Kannada"", ""English"", ""Mixed-language"", ""Name"", ""Location""
and ""Other"". The learning models, namely, CoLI-vectors and CoLI-ngrams based on
ML, CoLI-BiLSTM based on DL and CoLI-ULMFiT based on TL approaches are built
and evaluated using CoLI-Kenglish dataset. The performances of the learning
models illustrated, the superiority of CoLI-ngrams model, compared to other
models with a macro average F1-score of 0.64. However, the results of all the
learning models were quite competitive with each other.",None,-1
1e9fb3b4-588d-42c9-ad14-a982708862e0,Investigating the Impact of Cross-lingual Acoustic-Phonetic Similarities on Multilingual Speech Recognition,0.188759,"Multilingual automatic speech recognition (ASR) systems mostly benefit low
resource languages but suffer degradation in performance across several
languages relative to their monolingual counterparts. Limited studies have
focused on understanding the languages behaviour in the multilingual speech
recognition setups. In this paper, a novel data-driven approach is proposed to
investigate the cross-lingual acoustic-phonetic similarities. This technique
measures the similarities between posterior distributions from various
monolingual acoustic models against a target speech signal. Deep neural
networks are trained as mapping networks to transform the distributions from
different acoustic models into a directly comparable form. The analysis
observes that the languages closeness can not be truly estimated by the volume
of overlapping phonemes set. Entropy analysis of the proposed mapping networks
exhibits that a language with lesser overlap can be more amenable to
cross-lingual transfer, and hence more beneficial in the multilingual setup.
Finally, the proposed posterior transformation approach is leveraged to fuse
monolingual models for a target language. A relative improvement of ~8% over
monolingual counterpart is achieved.",None,-1
45e8930b-e5da-48db-b840-414f5ba14fb8,"Graph Neural Networks Meet Wireless Communications: Motivation, Applications, and Future Directions",0.855809,"As an efficient graph analytical tool, graph neural networks (GNNs) have
special properties that are particularly fit for the characteristics and
requirements of wireless communications, exhibiting good potential for the
advancement of next-generation wireless communications. This article aims to
provide a comprehensive overview of the interplay between GNNs and wireless
communications, including GNNs for wireless communications (GNN4Com) and
wireless communications for GNNs (Com4GNN). In particular, we discuss GNN4Com
based on how graphical models are constructed and introduce Com4GNN with
corresponding incentives. We also highlight potential research directions to
promote future research endeavors for GNNs in wireless communications.",None,-1
6b57ee63-d156-4314-b93c-8fa7e23127b1,SPA-VAE: Similar-Parts-Assignment for Unsupervised 3D Point Cloud Generation,0.112796,"This paper addresses the problem of unsupervised parts-aware point cloud
generation with learned parts-based self-similarity. Our SPA-VAE infers a set
of latent canonical candidate shapes for any given object, along with a set of
rigid body transformations for each such candidate shape to one or more
locations within the assembled object. In this way, noisy samples on the
surface of, say, each leg of a table, are effectively combined to estimate a
single leg prototype. When parts-based self-similarity exists in the raw data,
sharing data among parts in this way confers numerous advantages: modeling
accuracy, appropriately self-similar generative outputs, precise in-filling of
occlusions, and model parsimony. SPA-VAE is trained end-to-end using a
variational Bayesian approach which uses the Gumbel-softmax trick for the
shared part assignments, along with various novel losses to provide appropriate
inductive biases. Quantitative and qualitative analyses on ShapeNet demonstrate
the advantage of SPA-VAE.",None,-1
9af7e8d3-1947-4c75-ae08-f40785ae381b,Coalescing Global and Local Information for Procedural Text Understanding,0.367772,"Procedural text understanding is a challenging language reasoning task that
requires models to track entity states across the development of a narrative. A
complete procedural understanding solution should combine three core aspects:
local and global views of the inputs, and global view of outputs. Prior methods
considered a subset of these aspects, resulting in either low precision or low
recall. In this paper, we propose Coalescing Global and Local Information
(CGLI), a new model that builds entity- and timestep-aware input
representations (local input) considering the whole context (global input), and
we jointly model the entity states with a structured prediction objective
(global output). Thus, CGLI simultaneously optimizes for both precision and
recall. We extend CGLI with additional output layers and integrate it into a
story reasoning framework. Extensive experiments on a popular procedural text
understanding dataset show that our model achieves state-of-the-art results;
experiments on a story reasoning benchmark show the positive impact of our
model on downstream reasoning.",https://github.com/Mayer123/CGLI,-1
ba44d0e7-f628-48ae-b0f0-e7b060e14ca5,Overcoming Language Priors in Visual Question Answering via Distinguishing Superficially Similar Instances,0.200449,"Despite the great progress of Visual Question Answering (VQA), current VQA
models heavily rely on the superficial correlation between the question type
and its corresponding frequent answers (i.e., language priors) to make
predictions, without really understanding the input. In this work, we define
the training instances with the same question type but different answers as
\textit{superficially similar instances}, and attribute the language priors to
the confusion of VQA model on such instances. To solve this problem, we propose
a novel training framework that explicitly encourages the VQA model to
distinguish between the superficially similar instances. Specifically, for each
training instance, we first construct a set that contains its superficially
similar counterparts. Then we exploit the proposed distinguishing module to
increase the distance between the instance and its counterparts in the answer
space. In this way, the VQA model is forced to further focus on the other parts
of the input beyond the question type, which helps to overcome the language
priors. Experimental results show that our method achieves the state-of-the-art
performance on VQA-CP v2. Codes are available at
\href{https://github.com/wyk-nku/Distinguishing-VQA.git}{Distinguishing-VQA}.",None,-1
74ba1968-2011-4e69-a550-3f0f82bc324b,Informative Language Representation Learning for Massively Multilingual Neural Machine Translation,0.131527,"In a multilingual neural machine translation model that fully shares
parameters across all languages, an artificial language token is usually used
to guide translation into the desired target language. However, recent studies
show that prepending language tokens sometimes fails to navigate the
multilingual neural machine translation models into right translation
directions, especially on zero-shot translation. To mitigate this issue, we
propose two methods, language embedding embodiment and language-aware
multi-head attention, to learn informative language representations to channel
translation into right directions. The former embodies language embeddings into
different critical switching points along the information flow from the source
to the target, aiming at amplifying translation direction guiding signals. The
latter exploits a matrix, instead of a vector, to represent a language in the
continuous space. The matrix is chunked into multiple heads so as to learn
language representations in multiple subspaces. Experiment results on two
datasets for massively multilingual neural machine translation demonstrate that
language-aware multi-head attention benefits both supervised and zero-shot
translation and significantly alleviates the off-target translation issue.
Further linguistic typology prediction experiments show that matrix-based
language representations learned by our methods are capable of capturing rich
linguistic typology features.",https://github.com/cordercorder/nmt-multi,-1
dfdb9f1e-96da-4ce1-957c-acc6364e2307,TrimBERT: Tailoring BERT for Trade-offs,0.127162,"Models based on BERT have been extremely successful in solving a variety of
natural language processing (NLP) tasks. Unfortunately, many of these large
models require a great deal of computational resources and/or time for
pre-training and fine-tuning which limits wider adoptability. While
self-attention layers have been well-studied, a strong justification for
inclusion of the intermediate layers which follow them remains missing in the
literature. In this work, we show that reducing the number of intermediate
layers in BERT-Base results in minimal fine-tuning accuracy loss of downstream
tasks while significantly decreasing model size and training time. We further
mitigate two key bottlenecks, by replacing all softmax operations in the
self-attention layers with a computationally simpler alternative and removing
half of all layernorm operations. This further decreases the training time
while maintaining a high level of fine-tuning accuracy.",https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT,-1
4fec8d23-c2a8-4cf2-918e-43bb06f824e3,DALL-E 2 Fails to Reliably Capture Common Syntactic Processes,0.457871,"Machine intelligence is increasingly being linked to claims about sentience,
language processing, and an ability to comprehend and transform natural
language into a range of stimuli. We systematically analyze the ability of
DALL-E 2 to capture 8 grammatical phenomena pertaining to compositionality that
are widely discussed in linguistics and pervasive in human language: binding
principles and coreference, passives, word order, coordination, comparatives,
negation, ellipsis, and structural ambiguity. Whereas young children routinely
master these phenomena, learning systematic mappings between syntax and
semantics, DALL-E 2 is unable to reliably infer meanings that are consistent
with the syntax. These results challenge recent claims concerning the capacity
of such systems to understand of human language. We make available the full set
of test materials as a benchmark for future testing.",None,-1
533a90b8-b823-4cef-beba-66303dc4f953,Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes,0.727531,"We present a novel approach to efficiently compute tight non-convex
enclosures of the image through neural networks with ReLU, sigmoid, or
hyperbolic tangent activation functions. In particular, we abstract the
input-output relation of each neuron by a polynomial approximation, which is
evaluated in a set-based manner using polynomial zonotopes. While our approach
can also can be beneficial for open-loop neural network verification, our main
application is reachability analysis of neural network controlled systems,
where polynomial zonotopes are able to capture the non-convexity caused by the
neural network as well as the system dynamics. This results in a superior
performance compared to other methods, as we demonstrate on various benchmarks.",None,-1
7e9ac318-0098-4784-85f4-89dbdd907dfb,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,0.525516,"While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method.",https://github.com/Jxu-Thu/DITTO,-1
842f60b7-fd92-4b7f-8303-0c493879dcb3,CEPHA29: Automatic Cephalometric Landmark Detection Challenge 2023,0.523052,"Quantitative cephalometric analysis is the most widely used clinical and
research tool in modern orthodontics. Accurate localization of cephalometric
landmarks enables the quantification and classification of anatomical
abnormalities, however, the traditional manual way of marking these landmarks
is a very tedious job. Endeavours have constantly been made to develop
automated cephalometric landmark detection systems but they are inadequate for
orthodontic applications. The fundamental reason for this is that the amount of
publicly available datasets as well as the images provided for training in
these datasets are insufficient for an AI model to perform well. To facilitate
the development of robust AI solutions for morphometric analysis, we organise
the CEPHA29 Automatic Cephalometric Landmark Detection Challenge in conjunction
with IEEE International Symposium on Biomedical Imaging (ISBI 2023). In this
context, we provide the largest known publicly available dataset, consisting of
1000 cephalometric X-ray images. We hope that our challenge will not only
derive forward research and innovation in automatic cephalometric landmark
identification but will also signal the beginning of a new era in the
discipline.",https://github.com/manwaarkhd/CEPHA29,-1
f5468bf9-fa8e-4ad7-bb49-6c5db388fe73,Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,0.795867,"End-to-end speech-to-speech translation (S2ST) without relying on
intermediate text representations is a rapidly emerging frontier of research.
Recent works have demonstrated that the performance of such direct S2ST systems
is approaching that of conventional cascade S2ST when trained on comparable
datasets. However, in practice, the performance of direct S2ST is bounded by
the availability of paired S2ST training data. In this work, we explore
multiple approaches for leveraging much more widely available unsupervised and
weakly-supervised speech and text data to improve the performance of direct
S2ST based on Translatotron 2. With our most effective approaches, the average
translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is
improved by +13.6 BLEU (or +113% relatively), as compared to the previous
state-of-the-art trained without additional data. The improvements on
low-resource language are even more significant (+398% relatively on average).
Our comparative studies suggest future research directions for S2ST and speech
representation learning.",None,-1
a450c1e9-5fde-49cb-b9ed-204e6bdacb4e,ESCM$^2$: Entire Space Counterfactual Multi-Task Model for Post-Click Conversion Rate Estimation,0.97627,"Accurate estimation of post-click conversion rate is critical for building
recommender systems, which has long been confronted with sample selection bias
and data sparsity issues. Methods in the Entire Space Multi-task Model (ESMM)
family leverage the sequential pattern of user actions, i.e.
$impression\rightarrow click \rightarrow conversion$ to address data sparsity
issue. However, they still fail to ensure the unbiasedness of CVR estimates. In
this paper, we theoretically demonstrate that ESMM suffers from the following
two problems: (1) Inherent Estimation Bias (IEB), where the estimated CVR of
ESMM is inherently higher than the ground truth; (2) Potential Independence
Priority (PIP) for CTCVR estimation, where there is a risk that the ESMM
overlooks the causality from click to conversion. To this end, we devise a
principled approach named Entire Space Counterfactual Multi-task Modelling
(ESCM$^2$), which employs a counterfactual risk miminizer as a regularizer in
ESMM to address both IEB and PIP issues simultaneously. Extensive experiments
on offline datasets and online environments demonstrate that our proposed
ESCM$^2$ can largely mitigate the inherent IEB and PIP issues and achieve
better performance than baseline models.",https://github.com/PaddlePaddle/PaddleRec/tree/master/models/multitask,-1
f6a74232-4984-48fb-807e-6dd045fea6d8,ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs,0.672696,"As Abstract Meaning Representation (AMR) implicitly involves compound
semantic annotations, we hypothesize auxiliary tasks which are semantically or
formally related can better enhance AMR parsing. We find that 1) Semantic role
labeling (SRL) and dependency parsing (DP), would bring more performance gain
than other tasks e.g. MT and summarization in the text-to-AMR transition even
with much less data. 2) To make a better fit for AMR, data from auxiliary tasks
should be properly ""AMRized"" to PseudoAMR before training. Knowledge from
shallow level parsing tasks can be better transferred to AMR Parsing with
structure transform. 3) Intermediate-task learning is a better paradigm to
introduce auxiliary tasks to AMR parsing, compared to multitask learning. From
an empirical perspective, we propose a principled method to involve auxiliary
tasks to boost AMR parsing. Extensive experiments show that our method achieves
new state-of-the-art performance on different benchmarks especially in
topology-related scores.",https://github.com/PKUnlp-icler/ATP,-1
1b44cba2-4f51-459d-93ef-11113339be5b,Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically Ambiguous Settings for Low Resource Languages,0.286689,"We leverage pre-trained language models to solve the task of complex NER for
two low-resource languages: Chinese and Spanish. We use the technique of Whole
Word Masking(WWM) to boost the performance of masked language modeling
objective on large and unsupervised corpora. We experiment with multiple neural
network architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on
top of a fine-tuned BERT layer. All our models outperform the baseline by a
significant margin and our best performing model obtains a competitive position
on the evaluation leaderboard for the blind test set.",https://github.com/AmitPandey-Research/Complex_NER,1943
3cfb316c-e08f-417b-b283-15100b4bca0c,Heart rate estimation in intense exercise videos,0.606504,"Estimating heart rate from video allows non-contact health monitoring with
applications in patient care, human interaction, and sports. Existing work can
robustly measure heart rate under some degree of motion by face tracking.
However, this is not always possible in unconstrained settings, as the face
might be occluded or even outside the camera. Here, we present IntensePhysio: a
challenging video heart rate estimation dataset with realistic face occlusions,
severe subject motion, and ample heart rate variation. To ensure heart rate
variation in a realistic setting we record each subject for around 1-2 hours.
The subject is exercising (at a moderate to high intensity) on a cycling
ergometer with an attached video camera and is given no instructions regarding
positioning or movement. We have 11 subjects, and approximately 20 total hours
of video. We show that the existing remote photo-plethysmography methods have
difficulty in estimating heart rate in this setting. In addition, we present
IBIS-CNN, a new baseline using spatio-temporal superpixels, which improves on
existing models by eliminating the need for a visible face/face tracking. We
will make the code and data publically available soon.",https://github.com/ynapolean/IBIS-CNN,-1
d5f69485-a9d0-4310-872c-68aefd2d9489,How to Understand Masked Autoencoders,0.517066,"""Masked Autoencoders (MAE) Are Scalable Vision Learners"" revolutionizes the
self-supervised learning method in that it not only achieves the
state-of-the-art for image pre-training, but is also a milestone that bridges
the gap between visual and linguistic masked autoencoding (BERT-style)
pre-trainings. However, to our knowledge, to date there are no theoretical
perspectives to explain the powerful expressivity of MAE. In this paper, we,
for the first time, propose a unified theoretical framework that provides a
mathematical understanding for MAE. Specifically, we explain the patch-based
attention approaches of MAE using an integral kernel under a non-overlapping
domain decomposition setting. To help the research community to further
comprehend the main reasons of the great success of MAE, based on our
framework, we pose five questions and answer them with mathematical rigor using
insights from operator theory.",https://github.com/facebookresearch/mae,-1
ab4dee82-44c0-4d72-91b8-fea649d99587,Clinical Dialogue Transcription Error Correction using Seq2Seq Models,0.193411,"Good communication is critical to good healthcare. Clinical dialogue is a
conversation between health practitioners and their patients, with the explicit
goal of obtaining and sharing medical information. This information contributes
to medical decision-making regarding the patient and plays a crucial role in
their healthcare journey. The reliance on note taking and manual scribing
processes are extremely inefficient and leads to manual transcription errors
when digitizing notes. Automatic Speech Recognition (ASR) plays a significant
role in speech-to-text applications, and can be directly used as a text
generator in conversational applications. However, recording clinical dialogue
presents a number of general and domain-specific challenges. In this paper, we
present a seq2seq learning approach for ASR transcription error correction of
clinical dialogues. We introduce a new Gastrointestinal Clinical Dialogue (GCD)
Dataset which was gathered by healthcare professionals from a NHS Inflammatory
Bowel Disease clinic and use this in a comparative study with four commercial
ASR systems. Using self-supervision strategies, we fine-tune a seq2seq model on
a mask-filling task using a domain-specific PubMed dataset which we have shared
publicly for future research. The BART model fine-tuned for mask-filling was
able to correct transcription errors and achieve lower word error rates for
three out of four commercial ASR outputs.",None,-1
66bccbaf-f861-4b9d-b4a2-937a824d629a,Invertible Mask Network for Face Privacy-Preserving,0.0665303,"Face privacy-preserving is one of the hotspots that arises dramatic interests
of research. However, the existing face privacy-preserving methods aim at
causing the missing of semantic information of face and cannot preserve the
reusability of original facial information. To achieve the naturalness of the
processed face and the recoverability of the original protected face, this
paper proposes face privacy-preserving method based on Invertible ""Mask""
Network (IMN). In IMN, we introduce a Mask-net to generate ""Mask"" face firstly.
Then, put the ""Mask"" face onto the protected face and generate the masked face,
in which the masked face is indistinguishable from ""Mask"" face. Finally, ""Mask""
face can be put off from the masked face and obtain the recovered face to the
authorized users, in which the recovered face is visually indistinguishable
from the protected face. The experimental results show that the proposed method
can not only effectively protect the privacy of the protected face, but also
almost perfectly recover the protected face from the masked face.",None,-1
8147a44f-041d-4114-abf4-237d946c586b,Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning,0.958067,"In recent years, reinforcement learning (RL) has shown outstanding
performance for locomotion control of highly articulated robotic systems. Such
approaches typically involve tedious reward function tuning to achieve the
desired motion style. Imitation learning approaches such as adversarial motion
priors aim to reduce this problem by encouraging a pre-defined motion style. In
this work, we present an approach to augment the concept of adversarial motion
prior-based RL to allow for multiple, discretely switchable styles. We show
that multiple styles and skills can be learned simultaneously without notable
performance differences, even in combination with motion data-free skills. Our
approach is validated in several real-world experiments with a wheeled-legged
quadruped robot showing skills learned from existing RL controllers and
trajectory optimization, such as ducking and walking, and novel skills such as
switching between a quadrupedal and humanoid configuration. For the latter
skill, the robot is required to stand up, navigate on two wheels, and sit down.
Instead of tuning the sit-down motion, we verify that a reverse playback of the
stand-up movement helps the robot discover feasible sit-down behaviors and
avoids tedious reward function tuning.",None,-1
aaaa9d7e-655d-4326-9ed1-ac678155bddc,MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing,0.52258,"Snow removal causes challenges due to its characteristic of complex
degradations. To this end, targeted treatment of multi-scale snow degradations
is critical for the network to learn effective snow removal. In order to handle
the diverse scenes, we propose a multi-scale projection transformer
(MSP-Former), which understands and covers a variety of snow degradation
features in a multi-path manner, and integrates comprehensive scene context
information for clean reconstruction via self-attention operation. For the
local details of various snow degradations, the local capture module is
introduced in parallel to assist in the rebuilding of a clean image. Such
design achieves the SOTA performance on three desnowing benchmark datasets
while costing the low parameters and computational complexity, providing a
guarantee of practicality.",None,-1
a4fa9dfd-e590-47d2-8f8e-8da4ee5281e1,CommunityLM: Probing Partisan Worldviews from Language Models,0.321067,"As political attitudes have diverged ideologically in the United States,
political speech has diverged lingusitically. The ever-widening polarization
between the US political parties is accelerated by an erosion of mutual
understanding between them. We aim to make these communities more
comprehensible to each other with a framework that probes community-specific
responses to the same survey questions using community language models
CommunityLM. In our framework we identify committed partisan members for each
community on Twitter and fine-tune LMs on the tweets authored by them. We then
assess the worldviews of the two groups using prompt-based probing of their
corresponding LMs, with prompts that elicit opinions about public figures and
groups surveyed by the American National Election Studies (ANES) 2020
Exploratory Testing Survey. We compare the responses generated by the LMs to
the ANES survey results, and find a level of alignment that greatly exceeds
several baseline methods. Our work aims to show that we can use community LMs
to query the worldview of any group of people given a sufficiently large sample
of their social media discussions or media diet.",https://github.com/hjian42/CommunityLM,-1
58584ef8-c796-4bb1-825e-9eb5ffeab3ce,Long-tailed Instance Segmentation using Gumbel Optimized Loss,0.720271,"Major advancements have been made in the field of object detection and
segmentation recently. However, when it comes to rare categories, the
state-of-the-art methods fail to detect them, resulting in a significant
performance gap between rare and frequent categories. In this paper, we
identify that Sigmoid or Softmax functions used in deep detectors are a major
reason for low performance and are sub-optimal for long-tailed detection and
segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for
long-tailed detection and segmentation. It aligns with the Gumbel distribution
of rare classes in imbalanced datasets, considering the fact that most classes
in long-tailed detection have low expected probability. The proposed GOL
significantly outperforms the best state-of-the-art method by 1.1% on AP , and
boosts the overall segmentation by 9.0% and detection by 8.0%, particularly
improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS
dataset. Code available at: https://github.com/kostas1515/GOL",https://github.com/kostas1515/GOL,-1
26f6fb75-ca54-475d-8e1c-1024cd6a49f1,Split-U-Net: Preventing Data Leakage in Split Learning for Collaborative Multi-Modal Brain Tumor Segmentation,0.206661,"Split learning (SL) has been proposed to train deep learning models in a
decentralized manner. For decentralized healthcare applications with vertical
data partitioning, SL can be beneficial as it allows institutes with
complementary features or images for a shared set of patients to jointly
develop more robust and generalizable models. In this work, we propose
""Split-U-Net"" and successfully apply SL for collaborative biomedical image
segmentation. Nonetheless, SL requires the exchanging of intermediate
activation maps and gradients to allow training models across different feature
spaces, which might leak data and raise privacy concerns. Therefore, we also
quantify the amount of data leakage in common SL scenarios for biomedical image
segmentation and provide ways to counteract such leakage by applying
appropriate defense strategies.",None,21315
267362c8-250e-4c64-ae74-7592b8bb2034,Register Variation Remains Stable Across 60 Languages,0.225407,"This paper measures the stability of cross-linguistic register variation. A
register is a variety of a language that is associated with extra-linguistic
context. The relationship between a register and its context is functional: the
linguistic features that make up a register are motivated by the needs and
constraints of the communicative situation. This view hypothesizes that
register should be universal, so that we expect a stable relationship between
the extra-linguistic context that defines a register and the sets of linguistic
features which the register contains. In this paper, the universality and
robustness of register variation is tested by comparing variation within vs.
between register-specific corpora in 60 languages using corpora produced in
comparable communicative situations: tweets and Wikipedia articles. Our
findings confirm the prediction that register variation is, in fact, universal.",None,-1
6e1a1a27-8b3a-45a1-917d-256fd12b90c5,"Revisiting Crowd Counting: State-of-the-art, Trends, and Future Perspectives",0.918591,"Crowd counting is an effective tool for situational awareness in public
places. Automated crowd counting using images and videos is an interesting yet
challenging problem that has gained significant attention in computer vision.
Over the past few years, various deep learning methods have been developed to
achieve state-of-the-art performance. The methods evolved over time vary in
many aspects such as model architecture, input pipeline, learning paradigm,
computational complexity, and accuracy gains etc. In this paper, we present a
systematic and comprehensive review of the most significant contributions in
the area of crowd counting. Although few surveys exist on the topic, our survey
is most up-to date and different in several aspects. First, it provides a more
meaningful categorization of the most significant contributions by model
architectures, learning methods (i.e., loss functions), and evaluation methods
(i.e., evaluation metrics). We chose prominent and distinct works and excluded
similar works. We also sort the well-known crowd counting models by their
performance over benchmark datasets. We believe that this survey can be a good
resource for novice researchers to understand the progressive developments and
contributions over time and the current state-of-the-art.",None,-1
208c7497-7f46-495a-91e6-416e536f9eb4,Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment,0.939559,"Most research on question answering focuses on the pre-deployment stage;
i.e., building an accurate model for deployment. In this paper, we ask the
question: Can we improve QA systems further \emph{post-}deployment based on
user interactions? We focus on two kinds of improvements: 1) improving the QA
system's performance itself, and 2) providing the model with the ability to
explain the correctness or incorrectness of an answer. We collect a
retrieval-based QA dataset, FeedbackQA, which contains interactive feedback
from users. We collect this dataset by deploying a base QA system to
crowdworkers who then engage with the system and provide feedback on the
quality of its answers. The feedback contains both structured ratings and
unstructured natural language explanations. We train a neural model with this
feedback data that can generate explanations and re-score answer candidates. We
show that feedback data not only improves the accuracy of the deployed QA
system but also other stronger non-deployed systems. The generated explanations
also help users make informed decisions about the correctness of answers.
Project page: https://mcgill-nlp.github.io/feedbackqa/",None,-1
7203aa2b-4bed-4800-9ecb-e3084e93fba5,Arabic Fake News Detection Based on Deep Contextualized Embedding Models,0.928891,"Social media is becoming a source of news for many people due to its ease and
freedom of use. As a result, fake news has been spreading quickly and easily
regardless of its credibility, especially in the last decade. Fake news
publishers take advantage of critical situations such as the Covid-19 pandemic
and the American presidential elections to affect societies negatively. Fake
news can seriously impact society in many fields including politics, finance,
sports, etc. Many studies have been conducted to help detect fake news in
English, but research conducted on fake news detection in the Arabic language
is scarce. Our contribution is twofold: first, we have constructed a large and
diverse Arabic fake news dataset. Second, we have developed and evaluated
transformer-based classifiers to identify fake news while utilizing eight
state-of-the-art Arabic contextualized embedding models. The majority of these
models had not been previously used for Arabic fake news detection. We conduct
a thorough analysis of the state-of-the-art Arabic contextualized embedding
models as well as comparison with similar fake news detection systems.
Experimental results confirm that these state-of-the-art models are robust,
with accuracy exceeding 98%.",None,-1
aea1167f-300e-4d04-ba6d-7f0961952c65,Multi-Faceted Distillation of Base-Novel Commonality for Few-shot Object Detection,0.393716,"Most of existing methods for few-shot object detection follow the fine-tuning
paradigm, which potentially assumes that the class-agnostic generalizable
knowledge can be learned and transferred implicitly from base classes with
abundant samples to novel classes with limited samples via such a two-stage
training strategy. However, it is not necessarily true since the object
detector can hardly distinguish between class-agnostic knowledge and
class-specific knowledge automatically without explicit modeling. In this work
we propose to learn three types of class-agnostic commonalities between base
and novel classes explicitly: recognition-related semantic commonalities,
localization-related semantic commonalities and distribution commonalities. We
design a unified distillation framework based on a memory bank, which is able
to perform distillation of all three types of commonalities jointly and
efficiently. Extensive experiments demonstrate that our method can be readily
integrated into most of existing fine-tuning based methods and consistently
improve the performance by a large margin.",https://github.com/WuShuang1998/MFDC,-1
f1bc0a43-3c32-4a01-a4c3-3437dce10cb5,SS-VAERR: Self-Supervised Apparent Emotional Reaction Recognition from Video,0.135303,"This work focuses on the apparent emotional reaction recognition (AERR) from
the video-only input, conducted in a self-supervised fashion. The network is
first pre-trained on different self-supervised pretext tasks and later
fine-tuned on the downstream target task. Self-supervised learning facilitates
the use of pre-trained architectures and larger datasets that might be deemed
unfit for the target task and yet might be useful to learn informative
representations and hence provide useful initializations for further
fine-tuning on smaller more suitable data. Our presented contribution is
two-fold: (1) an analysis of different state-of-the-art (SOTA) pretext tasks
for the video-only apparent emotional reaction recognition architecture, and
(2) an analysis of various combinations of the regression and classification
losses that are likely to improve the performance further. Together these two
contributions result in the current state-of-the-art performance for the
video-only spontaneous apparent emotional reaction recognition with continuous
annotations.",None,-1
0c81eba3-d9ac-4f27-97a3-b40031c9b65a,Semi-Supervised Learning of Optical Flow by Flow Supervisor,0.552894,"A training pipeline for optical flow CNNs consists of a pretraining stage on
a synthetic dataset followed by a fine tuning stage on a target dataset.
However, obtaining ground truth flows from a target video requires a tremendous
effort. This paper proposes a practical fine tuning method to adapt a
pretrained model to a target dataset without ground truth flows, which has not
been explored extensively. Specifically, we propose a flow supervisor for
self-supervision, which consists of parameter separation and a student output
connection. This design is aimed at stable convergence and better accuracy over
conventional self-supervision methods which are unstable on the fine tuning
task. Experimental results show the effectiveness of our method compared to
different self-supervision methods for semi-supervised learning. In addition,
we achieve meaningful improvements over state-of-the-art optical flow models on
Sintel and KITTI benchmarks by exploiting additional unlabeled datasets. Code
is available at https://github.com/iwbn/flow-supervisor.",https://github.com/iwbn/flow-supervisor,-1
3e8f57b9-41f0-4b67-baaa-4f0732805d6a,Czech Dataset for Cross-lingual Subjectivity Classification,0.431071,"In this paper, we introduce a new Czech subjectivity dataset of 10k manually
annotated subjective and objective sentences from movie reviews and
descriptions. Our prime motivation is to provide a reliable dataset that can be
used with the existing English dataset as a benchmark to test the ability of
pre-trained multilingual models to transfer knowledge between Czech and English
and vice versa. Two annotators annotated the dataset reaching 0.83 of the
Cohen's \k{appa} inter-annotator agreement. To the best of our knowledge, this
is the first subjectivity dataset for the Czech language. We also created an
additional dataset that consists of 200k automatically labeled sentences. Both
datasets are freely available for research purposes. Furthermore, we fine-tune
five pre-trained BERT-like models to set a monolingual baseline for the new
dataset and we achieve 93.56% of accuracy. We fine-tune models on the existing
English dataset for which we obtained results that are on par with the current
state-of-the-art results. Finally, we perform zero-shot cross-lingual
subjectivity classification between Czech and English to verify the usability
of our dataset as the cross-lingual benchmark. We compare and discuss the
cross-lingual and monolingual results and the ability of multilingual models to
transfer knowledge between languages.",https://github.com/pauli31/czech-subjectivity-dataset,3078
2c8ef081-b37f-4d14-afc2-3fc91557e1e3,Knowledge-Grounded Conversational Data Augmentation with Generative Conversational Networks,0.190301,"While rich, open-domain textual data are generally available and may include
interesting phenomena (humor, sarcasm, empathy, etc.) most are designed for
language processing tasks, and are usually in a non-conversational format. In
this work, we take a step towards automatically generating conversational data
using Generative Conversational Networks, aiming to benefit from the breadth of
available language and knowledge data, and train open domain social
conversational agents. We evaluate our approach on conversations with and
without knowledge on the Topical Chat dataset using automatic metrics and human
evaluators. Our results show that for conversations without knowledge
grounding, GCN can generalize from the seed data, producing novel conversations
that are less relevant but more engaging and for knowledge-grounded
conversations, it can produce more knowledge-focused, fluent, and engaging
conversations. Specifically, we show that for open-domain conversations with
10\% of seed data, our approach performs close to the baseline that uses 100%
of the data, while for knowledge-grounded conversations, it achieves the same
using only 1% of the data, on human ratings of engagingness, fluency, and
relevance.",None,21260
4b9719cf-801e-4942-9516-6a68ac9260e0,Inkorrect: Online Handwriting Spelling Correction,0.506327,"We introduce Inkorrect, a data- and label-efficient approach for online
handwriting (Digital Ink) spelling correction - DISC. Unlike previous work, the
proposed method does not require multiple samples from the same writer, or
access to character level segmentation. We show that existing automatic
evaluation metrics do not fully capture and are not correlated with the human
perception of the quality of the spelling correction, and propose new ones that
correlate with human perception. We additionally surface an interesting
phenomenon: a trade-off between the similarity and recognizability of the
spell-corrected inks. We further create a family of models corresponding to
different points on the Pareto frontier between those two axes. We show that
Inkorrect's Pareto frontier dominates the points that correspond to prior work.",None,-1
b64d216c-9a8b-424d-9d25-e614b2ad319a,JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking,0.952795,"Autonomous robotic systems operating in human environments must understand
their surroundings to make accurate and safe decisions. In crowded human scenes
with close-up human-robot interaction and robot navigation, a deep
understanding requires reasoning about human motion and body dynamics over time
with human body pose estimation and tracking. However, existing datasets either
do not provide pose annotations or include scene types unrelated to robotic
applications. Many datasets also lack the diversity of poses and occlusions
found in crowded human scenes. To address this limitation we introduce
JRDB-Pose, a large-scale dataset and benchmark for multi-person pose estimation
and tracking using videos captured from a social navigation robot. The dataset
contains challenge scenes with crowded indoor and outdoor locations and a
diverse range of scales and occlusion types. JRDB-Pose provides human pose
annotations with per-keypoint occlusion labels and track IDs consistent across
the scene. A public evaluation server is made available for fair evaluation on
a held-out test set. JRDB-Pose is available at https://jrdb.erc.monash.edu/ .",None,-1
2139fdf3-6c0b-4e3d-ad47-7c05c0360a06,Mutation Models: Learning to Generate Levels by Imitating Evolution,0.603739,"Search-based procedural content generation (PCG) is a well-known method for
level generation in games. Its key advantage is that it is generic and able to
satisfy functional constraints. However, due to the heavy computational costs
to run these algorithms online, search-based PCG is rarely utilized for
real-time generation. In this paper, we introduce mutation models, a new type
of iterative level generator based on machine learning. We train a model to
imitate the evolutionary process and use the trained model to generate levels.
This trained model is able to modify noisy levels sequentially to create better
levels without the need for a fitness function during inference. We evaluate
our trained models on a 2D maze generation task. We compare several different
versions of the method: training the models either at the end of evolution
(normal evolution) or every 100 generations (assisted evolution) and using the
model as a mutation function during evolution. Using the assisted evolution
process, the final trained models are able to generate mazes with a success
rate of 99% and high diversity of 86%. The trained model is many times faster
than the evolutionary process it was trained on. This work opens the door to a
new way of learning level generators guided by an evolutionary process, meaning
automatic creation of generators with specifiable constraints and objectives
that are fast enough for runtime deployment in games.",None,22404
e15c7613-46e4-4a96-81ed-f522ffeb291f,QUAK: A Synthetic Quality Estimation Dataset for Korean-English Neural Machine Translation,0.0963536,"With the recent advance in neural machine translation demonstrating its
importance, research on quality estimation (QE) has been steadily progressing.
QE aims to automatically predict the quality of machine translation (MT) output
without reference sentences. Despite its high utility in the real world, there
remain several limitations concerning manual QE data creation: inevitably
incurred non-trivial costs due to the need for translation experts, and issues
with data scaling and language expansion. To tackle these limitations, we
present QUAK, a Korean-English synthetic QE dataset generated in a fully
automatic manner. This consists of three sub-QUAK datasets QUAK-M, QUAK-P, and
QUAK-H, produced through three strategies that are relatively free from
language constraints. Since each strategy requires no human effort, which
facilitates scalability, we scale our data up to 1.58M for QUAK-P, H and 6.58M
for QUAK-M. As an experiment, we quantitatively analyze word-level QE results
in various ways while performing statistical analysis. Moreover, we show that
datasets scaled in an efficient way also contribute to performance improvements
by observing meaningful performance gains in QUAK-M, P when adding data up to
1.58M.",None,-1
a38a5781-3be8-47c9-b2bb-f97ca900011f,Semantically Proportional Patchmix for Few-Shot Learning,0.0229563,"Few-shot learning aims to classify unseen classes with only a limited number
of labeled data. Recent works have demonstrated that training models with a
simple transfer learning strategy can achieve competitive results in few-shot
classification. Although excelling at distinguishing training data, these
models are not well generalized to unseen data, probably due to insufficient
feature representations on evaluation. To tackle this issue, we propose
Semantically Proportional Patchmix (SePPMix), in which patches are cut and
pasted among training images and the ground truth labels are mixed
proportionally to the semantic information of the patches. In this way, we can
improve the generalization ability of the model by regional dropout effect
without introducing severe label noise. To learn more robust representations of
data, we further take rotate transformation on the mixed images and predict
rotations as a rule-based regularizer. Extensive experiments on prevalent
few-shot benchmarks have shown the effectiveness of our proposed method.",None,-1
5e9e0887-9439-4180-abd8-30fd9f3d4813,"Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI",0.307079,"Users' physical safety is an increasing concern as the market for intelligent
systems continues to grow, where unconstrained systems may recommend users
dangerous actions that can lead to serious injury. Covertly unsafe text is an
area of particular interest, as such text may arise from everyday scenarios and
are challenging to detect as harmful. We propose FARM, a novel framework
leveraging external knowledge for trustworthy rationale generation in the
context of safety. In particular, FARM foveates on missing knowledge to qualify
the information required to reason in specific scenarios and retrieves this
information with attribution to trustworthy sources. This knowledge is used to
both classify the safety of the original text and generate human-interpretable
rationales, shedding light on the risk of systems to specific user groups and
helping both stakeholders manage the risks of their systems and policymakers to
provide concrete safeguards for consumer safety. Our experiments show that FARM
obtains state-of-the-art results on the SafeText dataset, showing absolute
improvement in safety classification accuracy by 5.9%.",https://github.com/alexmeigz/FARM,-1
5e8bf410-aed1-49a2-8358-cdc6c9d4e23c,A Token-level Contrastive Framework for Sign Language Translation,0.170854,"Sign Language Translation (SLT) is a promising technology to bridge the
communication gap between the deaf and the hearing people. Recently,
researchers have adopted Neural Machine Translation (NMT) methods, which
usually require large-scale corpus for training, to achieve SLT. However, the
publicly available SLT corpus is very limited, which causes the collapse of the
token representations and the inaccuracy of the generated tokens. To alleviate
this issue, we propose ConSLT, a novel token-level \textbf{Con}trastive
learning framework for \textbf{S}ign \textbf{L}anguage \textbf{T}ranslation ,
which learns effective token representations by incorporating token-level
contrastive learning into the SLT decoding process. Concretely, ConSLT treats
each token and its counterpart generated by different dropout masks as positive
pairs during decoding, and then randomly samples $K$ tokens in the vocabulary
that are not in the current sentence to construct negative examples. We conduct
comprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both
end-to-end and cascaded settings. The experimental results demonstrate that
ConSLT can achieve better translation quality than the strong baselines.",https://github.com/biaofuxmu/ConSLT,-1
605546bd-fea3-4ed1-a808-385514f33533,Deep Surrogate of Modular Multi Pump using Active Learning,0.0570714,"Due to the high cost and reliability of sensors, the designers of a pump
reduce the needed number of sensors for the estimation of the feasible
operating point as much as possible. The major challenge to obtain a good
estimation is the low amount of data available. Using this amount of data, the
performance of the estimation method is not enough to satisfy the client
requests. To solve this problem of scarcity of data, getting high quality data
is important to obtain a good estimation. Based on these considerations, we
develop an active learning framework for estimating the operating point of a
Modular Multi Pump used in energy field. In particular we focus on the
estimation of the surge distance. We apply Active learning to estimate the
surge distance with minimal dataset. Results report that active learning is a
valuable technique also for real application.",None,-1
2ab0ab90-8cd0-4dc5-9c86-bb5a6325fc50,Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation,0.501909,"Single-Image Super-Resolution can support robotic tasks in environments where
a reliable visual stream is required to monitor the mission, handle
teleoperation or study relevant visual details. In this work, we propose an
efficient Generative Adversarial Network model for real-time Super-Resolution,
called EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We
adopt a tailored architecture of the original SRGAN and model quantization to
boost the execution on CPU and Edge TPU devices, achieving up to 200 fps
inference. We further optimize our model by distilling its knowledge to a
smaller version of the network and obtain remarkable improvements compared to
the standard training approach. Our experiments show that our fast and
lightweight model preserves considerably satisfying image quality compared to
heavier state-of-the-art models. Finally, we conduct experiments on image
transmission with bandwidth degradation to highlight the advantages of the
proposed system for mobile robotic applications.",https://github.com/PIC4SeR/EdgeSRGAN,-1
69810f2b-eebf-480a-a7ac-d300559c06e1,A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels,0.047551,"Label noise is common in large real-world datasets, and its presence harms
the training process of deep neural networks. Although several works have
focused on the training strategies to address this problem, there are few
studies that evaluate the impact of data augmentation as a design choice for
training deep neural networks. In this work, we analyse the model robustness
when using different data augmentations and their improvement on the training
with the presence of noisy labels. We evaluate state-of-the-art and classical
data augmentation strategies with different levels of synthetic noise for the
datasets MNist, CIFAR-10, CIFAR-100, and the real-world dataset Clothing1M. We
evaluate the methods using the accuracy metric. Results show that the
appropriate selection of data augmentation can drastically improve the model
robustness to label noise, increasing up to 177.84% of relative best test
accuracy compared to the baseline with no augmentation, and an increase of up
to 6% in absolute value with the state-of-the-art DivideMix training strategy.",None,-1
06e9469c-ee4a-4e3c-aa15-8cf37e2ef69e,RustSEG -- Automated segmentation of corrosion using deep learning,0.298259,"The inspection of infrastructure for corrosion remains a task that is
typically performed manually by qualified engineers or inspectors. This task of
inspection is laborious, slow, and often requires complex access. Recently,
deep learning based algorithms have revealed promise and performance in the
automatic detection of corrosion. However, to date, research regarding the
segmentation of images for automated corrosion detection has been limited, due
to the lack of availability of per-pixel labelled data sets which are required
for model training. Herein, a novel deep learning approach (termed RustSEG) is
presented, that can accurately segment images for automated corrosion
detection, without the requirement of per-pixel labelled data sets for
training. The RustSEG method will first, using deep learning techniques,
determine if corrosion is present in an image (i.e. a classification task), and
then if corrosion is present, the model will examine what pixels in the
original image contributed to that classification decision. Finally, the method
can refine its predictions into a pixel-level segmentation mask. In ideal
cases, the method is able to generate precise masks of corrosion in images,
demonstrating that the automated segmentation of corrosion without per-pixel
training data is possible, addressing a significant hurdle in automated
infrastructure inspection.",None,-1
8a8fe9c7-f578-4e99-8b03-07f2fb9ef799,Cross-lingual Word Embeddings in Hyperbolic Space,0.212957,"Cross-lingual word embeddings can be applied to several natural language
processing applications across multiple languages. Unlike prior works that use
word embeddings based on the Euclidean space, this short paper presents a
simple and effective cross-lingual Word2Vec model that adapts to the Poincar\'e
ball model of hyperbolic space to learn unsupervised cross-lingual word
representations from a German-English parallel corpus. It has been shown that
hyperbolic embeddings can capture and preserve hierarchical relationships. We
evaluate the model on both hypernymy and analogy tasks. The proposed model
achieves comparable performance with the vanilla Word2Vec model on the
cross-lingual analogy task, the hypernymy task shows that the cross-lingual
Poincar\'e Word2Vec model can capture latent hierarchical structure from free
text across languages, which are absent from the Euclidean-based Word2Vec
representations. Our results show that by preserving the latent hierarchical
information, hyperbolic spaces can offer better representations for
cross-lingual embeddings.",None,-1
ee601675-0aa5-44aa-9eb6-bf838d67a8ce,TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization,0.591278,"Robustness evaluation against adversarial examples has become increasingly
important to unveil the trustworthiness of the prevailing deep models in
natural language processing (NLP). However, in contrast to the computer vision
domain where the first-order projected gradient descent (PGD) is used as the
benchmark approach to generate adversarial examples for robustness evaluation,
there lacks a principled first-order gradient-based robustness evaluation
framework in NLP. The emerging optimization challenges lie in 1) the discrete
nature of textual inputs together with the strong coupling between the
perturbation location and the actual content, and 2) the additional constraint
that the perturbed text should be fluent and achieve a low perplexity under a
language model. These challenges make the development of PGD-like NLP attacks
difficult. To bridge the gap, we propose TextGrad, a new attack generator using
gradient-driven optimization, supporting high-accuracy and high-quality
assessment of adversarial robustness in NLP. Specifically, we address the
aforementioned challenges in a unified optimization framework. And we develop
an effective convex relaxation method to co-optimize the continuously-relaxed
site selection and perturbation variables and leverage an effective sampling
method to establish an accurate mapping from the continuous optimization
variables to the discrete textual perturbations. Moreover, as a first-order
attack generation method, TextGrad can be baked into adversarial training to
further improve the robustness of NLP models. Extensive experiments are
provided to demonstrate the effectiveness of TextGrad not only in attack
generation for robustness evaluation but also in adversarial defense.",https://github.com/UCSB-NLP-Chang/TextGrad,-1
1e897a38-2269-4a51-b869-4751dc753328,Solutions for Fine-grained and Long-tailed Snake Species Recognition in SnakeCLEF 2022,0.576998,"Automatic snake species recognition is important because it has vast
potential to help lower deaths and disabilities caused by snakebites. We
introduce our solution in SnakeCLEF 2022 for fine-grained snake species
recognition on a heavy long-tailed class distribution. First, a network
architecture is designed to extract and fuse features from multiple modalities,
i.e. photograph from visual modality and geographic locality information from
language modality. Then, logit adjustment based methods are studied to relieve
the impact caused by the severe class imbalance. Next, a combination of
supervised and self-supervised learning method is proposed to make full use of
the dataset, including both labeled training data and unlabeled testing data.
Finally, post processing strategies, such as multi-scale and multi-crop
test-time-augmentation, location filtering and model ensemble, are employed for
better performance. With an ensemble of several different models, a private
score 82.65%, ranking the 3rd, is achieved on the final leaderboard.",None,13984
753871d4-6984-46ae-a7e4-e50b9fd2ee73,Unsupervised Domain Adaptive Salient Object Detection Through Uncertainty-Aware Pseudo-Label Learning,0.707335,"Recent advances in deep learning significantly boost the performance of
salient object detection (SOD) at the expense of labeling larger-scale
per-pixel annotations. To relieve the burden of labor-intensive labeling, deep
unsupervised SOD methods have been proposed to exploit noisy labels generated
by handcrafted saliency methods. However, it is still difficult to learn
accurate saliency details from rough noisy labels. In this paper, we propose to
learn saliency from synthetic but clean labels, which naturally has higher
pixel-labeling quality without the effort of manual annotations. Specifically,
we first construct a novel synthetic SOD dataset by a simple copy-paste
strategy. Considering the large appearance differences between the synthetic
and real-world scenarios, directly training with synthetic data will lead to
performance degradation on real-world scenarios. To mitigate this problem, we
propose a novel unsupervised domain adaptive SOD method to adapt between these
two domains by uncertainty-aware self-training. Experimental results show that
our proposed method outperforms the existing state-of-the-art deep unsupervised
SOD methods on several benchmark datasets, and is even comparable to
fully-supervised ones.",None,-1
2587fe7f-947e-4c4c-a0bb-a75809e70324,Leveraging Graph-based Cross-modal Information Fusion for Neural Sign Language Translation,0.335657,"Sign Language (SL), as the mother tongue of the deaf community, is a special
visual language that most hearing people cannot understand. In recent years,
neural Sign Language Translation (SLT), as a possible way for bridging
communication gap between the deaf and the hearing people, has attracted
widespread academic attention. We found that the current mainstream end-to-end
neural SLT models, which tries to learning language knowledge in a weakly
supervised manner, could not mine enough semantic information under the
condition of low data resources. Therefore, we propose to introduce additional
word-level semantic knowledge of sign language linguistics to assist in
improving current end-to-end neural SLT models. Concretely, we propose a novel
neural SLT model with multi-modal feature fusion based on the dynamic graph, in
which the cross-modal information, i.e. text and video, is first assembled as a
dynamic graph according to their correlation, and then the graph is processed
by a multi-modal graph encoder to generate the multi-modal embeddings for
further usage in the subsequent neural translation models. To the best of our
knowledge, we are the first to introduce graph neural networks, for fusing
multi-modal information, into neural sign language translation models.
Moreover, we conducted experiments on a publicly available popular SLT dataset
RWTH-PHOENIX-Weather-2014T. and the quantitative experiments show that our
method can improve the model.",None,70015
5c9b6ccf-c6cc-4a46-b3b8-56f7d77ac997,SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models,0.58979,"Vision-language models such as CLIP are pretrained on large volumes of
internet sourced image and text pairs, and have been shown to sometimes exhibit
impressive zero- and low-shot image classification performance. However, due to
their size, fine-tuning these models on new datasets can be prohibitively
expensive, both in terms of the supervision and compute required. To combat
this, a series of light-weight adaptation methods have been proposed to
efficiently adapt such models when limited supervision is available. In this
work, we show that while effective on internet-style datasets, even those
remedies under-deliver on classification tasks with images that differ
significantly from those commonly found online. To address this issue, we
present a new approach called SVL-Adapter that combines the complementary
strengths of both vision-language pretraining and self-supervised
representation learning. We report an average classification accuracy
improvement of 10% in the low-shot setting when compared to existing methods,
on a set of challenging visual classification tasks. Further, we present a
fully automatic way of selecting an important blending hyperparameter for our
model that does not require any held-out labeled validation data. Code for our
project is available here: https://github.com/omipan/svl_adapter.",https://github.com/omipan/svl_adapter,-1
882fe9eb-9dea-45d4-b46a-95d8ea169ff1,Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization,0.912676,"The problems of unfaithful summaries have been widely discussed under the
context of abstractive summarization. Though extractive summarization is less
prone to the common unfaithfulness issues of abstractive summaries, does that
mean extractive is equal to faithful? Turns out that the answer is no. In this
work, we define a typology with five types of broad unfaithfulness problems
(including and beyond not-entailment) that can appear in extractive summaries,
including incorrect coreference, incomplete coreference, incorrect discourse,
incomplete discourse, as well as other misleading information. We ask humans to
label these problems out of 1600 English summaries produced by 16 diverse
extractive systems. We find that 30% of the summaries have at least one of the
five issues. To automatically detect these problems, we find that 5 existing
faithfulness evaluation metrics for summarization have poor correlations with
human judgment. To remedy this, we propose a new metric, ExtEval, that is
designed for detecting unfaithful extractive summaries and is shown to have the
best performance. We hope our work can increase the awareness of unfaithfulness
problems in extractive summarization and help future work to evaluate and
resolve these issues. Our data and code are publicly available at
https://github.com/ZhangShiyue/extractive_is_not_faithful",https://github.com/ZhangShiyue/extractive_is_not_faithful,-1
3fdc6efd-84b7-4f62-bdff-a77694a18188,Training Language Models with Memory Augmentation,0.935083,"Recent work has improved language models (LMs) remarkably by equipping them
with a non-parametric memory component. However, most existing approaches only
introduce mem-ories at testing time or represent them using a separately
trained encoder, resulting in suboptimal training of the language model. In
this work, we present TRIME, a novel yet simple training approach designed for
training LMs with memory augmentation. Our approach uses a training objective
that directly takes in-batch examples as accessible memory. We also present new
methods for memory construction and data batching, which are used for adapting
to different sets of memories--local, long-term, and external memory--at
testing time. We evaluate TRIME on multiple language modeling and machine
translation benchmarks and show that it is able to achieve significant
improvements across all the settings. Concretely, TRIME reduces the perplexity
from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory
set from the training corpus. Compared to standard LM training, TRIME adds
negligible computational overhead and is compatible with different neural
architectures, making it a versatile solution for training memory-augmented
LMs.",https://github.com/princeton-nlp/TRIME,49836
22e38f93-0b6c-4837-9411-f5ce9d894105,Explaining (Sarcastic) Utterances to Enhance Affect Understanding in Multimodal Dialogues,0.228969,"Conversations emerge as the primary media for exchanging ideas and
conceptions. From the listener's perspective, identifying various affective
qualities, such as sarcasm, humour, and emotions, is paramount for
comprehending the true connotation of the emitted utterance. However, one of
the major hurdles faced in learning these affect dimensions is the presence of
figurative language, viz. irony, metaphor, or sarcasm. We hypothesize that any
detection system constituting the exhaustive and explicit presentation of the
emitted utterance would improve the overall comprehension of the dialogue. To
this end, we explore the task of Sarcasm Explanation in Dialogues, which aims
to unfold the hidden irony behind sarcastic utterances. We propose MOSES, a
deep neural network, which takes a multimodal (sarcastic) dialogue instance as
an input and generates a natural language sentence as its explanation.
Subsequently, we leverage the generated explanation for various natural
language understanding tasks in a conversational dialogue setup, such as
sarcasm detection, humour identification, and emotion recognition. Our
evaluation shows that MOSES outperforms the state-of-the-art system for SED by
an average of ~2% on different evaluation metrics, such as ROUGE, BLEU, and
METEOR. Further, we observe that leveraging the generated explanation advances
three downstream tasks for affect classification - an average improvement of
~14% F1-score in the sarcasm detection task and ~2% in the humour
identification and emotion recognition task. We also perform extensive analyses
to assess the quality of the results.",https://github.com/LCS2-IIITD/MOSES.git,-1
5432686a-e34d-4f7d-8f7a-9e07c57f01a2,Towards Textual Out-of-Domain Detection without In-Domain Labels,0.872743,"In many real-world settings, machine learning models need to identify user
inputs that are out-of-domain (OOD) so as to avoid performing wrong actions.
This work focuses on a challenging case of OOD detection, where no labels for
in-domain data are accessible (e.g., no intent labels for the intent
classification task). To this end, we first evaluate different language model
based approaches that predict likelihood for a sequence of tokens. Furthermore,
we propose a novel representation learning based method by combining
unsupervised clustering and contrastive learning so that better data
representations for OOD detection can be learned. Through extensive
experiments, we demonstrate that this method can significantly outperform
likelihood-based methods and can be even competitive to the state-of-the-art
supervised approaches with label information.",None,-1
50e40109-6d30-440f-afed-9c1f2fc52575,PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices,0.871218,"Neural radiance-density field methods have become increasingly popular for
the task of novel-view rendering. Their recent extension to hash-based
positional encoding ensures fast training and inference with visually pleasing
results. However, density-based methods struggle with recovering accurate
surface geometry. Hybrid methods alleviate this issue by optimizing the density
based on an underlying SDF. However, current SDF methods are overly smooth and
miss fine geometric details. In this work, we combine the strengths of these
two lines of work in a novel hash-based implicit surface representation. We
propose improvements to the two areas by replacing the voxel hash encoding with
a permutohedral lattice which optimizes faster, especially for higher
dimensions. We additionally propose a regularization scheme which is crucial
for recovering high-frequency geometric detail. We evaluate our method on
multiple datasets and show that we can recover geometric detail at the level of
pores and wrinkles while using only RGB images for supervision. Furthermore,
using sphere tracing we can render novel views at 30 fps on an RTX 3090. Code
is publicly available at: https://radualexandru.github.io/permuto_sdf",https://radualexandru.github.io/permuto_sdf,-1
f438d363-bb24-42da-b68f-3255e80120f2,Online Coreference Resolution for Dialogue Processing: Improving Mention-Linking on Real-Time Conversations,0.26113,"This paper suggests a direction of coreference resolution for online decoding
on actively generated input such as dialogue, where the model accepts an
utterance and its past context, then finds mentions in the current utterance as
well as their referents, upon each dialogue turn. A baseline and four
incremental-updated models adapted from the mention-linking paradigm are
proposed for this new setting, which address different aspects including the
singletons, speaker-grounded encoding and cross-turn mention contextualization.
Our approach is assessed on three datasets: Friends, OntoNotes, and BOLT.
Results show that each aspect brings out steady improvement, and our best
models outperform the baseline by over 10%, presenting an effective system for
this setting. Further analysis highlights the task characteristics, such as the
significance of addressing the mention recall.",https://github.com/emorynlp/elit,-1
8e3f86f2-6abe-4a90-9fc1-fb6304f74b38,Imitating Past Successes can be Very Suboptimal,0.287571,"Prior work has proposed a simple strategy for reinforcement learning (RL):
label experience with the outcomes achieved in that experience, and then
imitate the relabeled experience. These outcome-conditioned imitation learning
methods are appealing because of their simplicity, strong performance, and
close ties with supervised learning. However, it remains unclear how these
methods relate to the standard RL objective, reward maximization. In this
paper, we formally relate outcome-conditioned imitation learning to reward
maximization, drawing a precise relationship between the learned policy and
Q-values and explaining the close connections between these methods and prior
EM-based policy search methods. This analysis shows that existing
outcome-conditioned imitation learning methods do not necessarily improve the
policy, but a simple modification results in a method that does guarantee
policy improvement, under some assumptions.",https://github.com/ben-eysenbach/normalized-ocbc/blob/main/experiments.ipynb,-1
dd6d5197-b3ae-4c3d-b46c-0fb78bd83592,PalGAN: Image Colorization with Palette Generative Adversarial Networks,0.595831,"Multimodal ambiguity and color bleeding remain challenging in colorization.
To tackle these problems, we propose a new GAN-based colorization approach
PalGAN, integrated with palette estimation and chromatic attention. To
circumvent the multimodality issue, we present a new colorization formulation
that estimates a probabilistic palette from the input gray image first, then
conducts color assignment conditioned on the palette through a generative
model. Further, we handle color bleeding with chromatic attention. It studies
color affinities by considering both semantic and intensity correlation. In
extensive experiments, PalGAN outperforms state-of-the-arts in quantitative
evaluation and visual comparison, delivering notable diverse, contrastive, and
edge-preserving appearances. With the palette design, our method enables color
transfer between images even with irrelevant contexts.",https://github.com/shepnerd/PalGAN,-1
0ff54a01-807c-4441-a22c-7897dddae26c,Pre-trained Token-replaced Detection Model as Few-shot Learner,0.365409,"Pre-trained masked language models have demonstrated remarkable ability as
few-shot learners. In this paper, as an alternative, we propose a novel
approach to few-shot learning with pre-trained token-replaced detection models
like ELECTRA. In this approach, we reformulate a classification or a regression
task as a token-replaced detection problem. Specifically, we first define a
template and label description words for each task and put them into the input
to form a natural language prompt. Then, we employ the pre-trained
token-replaced detection model to predict which label description word is the
most original (i.e., least replaced) among all label description words in the
prompt. A systematic evaluation on 16 datasets demonstrates that our approach
outperforms few-shot learners with pre-trained masked language models in both
one-sentence and two-sentence learning tasks.",https://github.com/cjfarmer/TRD_FSL,5176
eccad1e9-23ad-4f20-aaa6-4942edf942f1,TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,0.719008,"Recently, the structural reading comprehension (SRC) task on web pages has
attracted increasing research interests. Although previous SRC work has
leveraged extra information such as HTML tags or XPaths, the informative
topology of web pages is not effectively exploited. In this work, we propose a
Topological Information Enhanced model (TIE), which transforms the token-level
task into a tag-level task by introducing a two-stage process (i.e. node
locating and answer refining). Based on that, TIE integrates Graph Attention
Network (GAT) and Pre-trained Language Model (PLM) to leverage the topological
information of both logical structures and spatial structures. Experimental
results demonstrate that our model outperforms strong baselines and achieves
state-of-the-art performances on the web-based SRC benchmark WebSRC at the time
of writing. The code of TIE will be publicly available at
https://github.com/X-LANCE/TIE.",https://github.com/X-LANCE/TIE,-1
5d88efd0-9305-4d26-9f7f-c3629a63be6a,PAL: Persona-Augmented Emotional Support Conversation Generation,0.951647,"Due to the lack of human resources for mental health support, there is an
increasing demand for employing conversational agents for support. Recent work
has demonstrated the effectiveness of dialogue models in providing emotional
support. As previous studies have demonstrated that seekers' persona is an
important factor for effective support, we investigate whether there are
benefits to modeling such information in dialogue models for support. In this
paper, our empirical analysis verifies that persona has an important impact on
emotional support. Therefore, we propose a framework for dynamically inferring
and modeling seekers' persona. We first train a model for inferring the
seeker's persona from the conversation history. Accordingly, we propose PAL, a
model that leverages persona information and, in conjunction with our
strategy-based controllable generation method, provides personalized emotional
support. Automatic and manual evaluations demonstrate that PAL achieves
state-of-the-art results, outperforming the baselines on the studied benchmark.
Our code and data are publicly available at https://github.com/chengjl19/PAL.",https://github.com/chengjl19/PAL,-1
234eee63-bccf-4f73-9bb0-a0f77d320042,MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning,0.849612,"The deep learning community has witnessed an exponentially growing interest
in self-supervised learning (SSL). However, it still remains unexplored how to
build a framework for learning useful representations of raw music waveforms in
a self-supervised manner. In this work, we design Music2Vec, a framework
exploring different SSL algorithmic components and tricks for music audio
recordings. Our model achieves comparable results to the state-of-the-art
(SOTA) music SSL model Jukebox, despite being significantly smaller with less
than 2% of parameters of the latter. The model will be released on
Huggingface(Please refer to: https://huggingface.co/m-a-p/music2vec-v1)",https://huggingface.co/m-a-p/music2vec-v1,-1
ebb4ccb3-fcda-46a6-b985-6e5aed85ba05,Statistical guarantees for sparse deep learning,0.53075,"Neural networks are becoming increasingly popular in applications, but our
mathematical understanding of their potential and limitations is still limited.
In this paper, we further this understanding by developing statistical
guarantees for sparse deep learning. In contrast to previous work, we consider
different types of sparsity, such as few active connections, few active nodes,
and other norm-based types of sparsity. Moreover, our theories cover important
aspects that previous theories have neglected, such as multiple outputs,
regularization, and l2-loss. The guarantees have a mild dependence on network
widths and depths, which means that they support the application of sparse but
wide and deep networks from a statistical perspective. Some of the concepts and
tools that we use in our derivations are uncommon in deep learning and, hence,
might be of additional interest.",None,-1
ac790832-c724-4c01-942b-40cab6caf482,Few-shot Single-view 3D Reconstruction with Memory Prior Contrastive Network,0.594711,"3D reconstruction of novel categories based on few-shot learning is appealing
in real-world applications and attracts increasing research interests. Previous
approaches mainly focus on how to design shape prior models for different
categories. Their performance on unseen categories is not very competitive. In
this paper, we present a Memory Prior Contrastive Network (MPCN) that can store
shape prior knowledge in a few-shot learning based 3D reconstruction framework.
With the shape memory, a multi-head attention module is proposed to capture
different parts of a candidate shape prior and fuse these parts together to
guide 3D reconstruction of novel categories. Besides, we introduce a 3D-aware
contrastive learning method, which can not only complement the retrieval
accuracy of memory network, but also better organize image features for
downstream tasks. Compared with previous few-shot 3D reconstruction methods,
MPCN can handle the inter-class variability without category annotations.
Experimental results on a benchmark synthetic dataset and the Pascal3D+
real-world dataset show that our model outperforms the current state-of-the-art
methods significantly.",None,-1
a677f0f9-825c-446d-8146-ac3ce7e5dc45,Single Image Deraining via Rain-Steaks Aware Deep Convolutional Neural Network,0.163482,"It is challenging to remove rain-steaks from a single rainy image because the
rain steaks are spatially varying in the rainy image. This problem is studied
in this paper by combining conventional image processing techniques and deep
learning based techniques. An improved weighted guided image filter (iWGIF) is
proposed to extract high frequency information from a rainy image. The high
frequency information mainly includes rain steaks and noise, and it can guide
the rain steaks aware deep convolutional neural network (RSADCNN) to pay more
attention to rain steaks. The efficiency and explain-ability of RSADNN are
improved. Experiments show that the proposed algorithm significantly
outperforms state-of-the-art methods on both synthetic and real-world images in
terms of both qualitative and quantitative measures. It is useful for
autonomous navigation in raining conditions.",None,-1
83b6fd79-60e9-4f75-b797-4b2bad4c96c0,Unsupervised Face Morphing Attack Detection via Self-paced Anomaly Detection,0.688597,"The supervised-learning-based morphing attack detection (MAD) solutions
achieve outstanding success in dealing with attacks from known morphing
techniques and known data sources. However, given variations in the morphing
attacks, the performance of supervised MAD solutions drops significantly due to
the insufficient diversity and quantity of the existing MAD datasets. To
address this concern, we propose a completely unsupervised MAD solution via
self-paced anomaly detection (SPL-MAD) by leveraging the existing large-scale
face recognition (FR) datasets and the unsupervised nature of convolutional
autoencoders. Using general FR datasets that might contain unintentionally and
unlabeled manipulated samples to train an autoencoder can lead to a diverse
reconstruction behavior of attack and bona fide samples. We analyze this
behavior empirically to provide a solid theoretical ground for designing our
unsupervised MAD solution. This also results in proposing to integrate our
adapted modified self-paced learning paradigm to enhance the reconstruction
error separability between the bona fide and attack samples in a completely
unsupervised manner. Our experimental results on a diverse set of MAD
evaluation datasets show that the proposed unsupervised SPL-MAD solution
outperforms the overall performance of a wide range of supervised MAD solutions
and provides higher generalizability on unknown attacks.",https://github.com/meilfang/SPL-MAD,-1
c5cd5a66-e654-4e72-adef-0b51aa2ebae7,Panchromatic and Multispectral Image Fusion via Alternating Reverse Filtering Network,0.343064,"Panchromatic (PAN) and multi-spectral (MS) image fusion, named
Pan-sharpening, refers to super-resolve the low-resolution (LR) multi-spectral
(MS) images in the spatial domain to generate the expected high-resolution (HR)
MS images, conditioning on the corresponding high-resolution PAN images. In
this paper, we present a simple yet effective \textit{alternating reverse
filtering network} for pan-sharpening. Inspired by the classical reverse
filtering that reverses images to the status before filtering, we formulate
pan-sharpening as an alternately iterative reverse filtering process, which
fuses LR MS and HR MS in an interpretable manner. Different from existing
model-driven methods that require well-designed priors and degradation
assumptions, the reverse filtering process avoids the dependency on pre-defined
exact priors. To guarantee the stability and convergence of the iterative
process via contraction mapping on a metric space, we develop the learnable
multi-scale Gaussian kernel module, instead of using specific filters. We
demonstrate the theoretical feasibility of such formulations. Extensive
experiments on diverse scenes to thoroughly verify the performance of our
method, significantly outperforming the state of the arts.",None,-1
d809306e-b87d-422b-80f5-18cd69500f25,Accelerating Machine Learning via the Weber-Fechner Law,0.528608,"The Weber-Fechner Law observes that human perception scales as the logarithm
of the stimulus. We argue that learning algorithms for human concepts could
benefit from the Weber-Fechner Law. Specifically, we impose Weber-Fechner on
simple neural networks, with or without convolution, via the logarithmic power
series of their sorted output. Our experiments show surprising performance and
accuracy on the MNIST data set within a few training iterations and limited
computational resources, suggesting that Weber-Fechner can accelerate machine
learning of human concepts.",None,-1
1521fe70-22fb-49a7-b0ed-3dc010220047,Anticipative Feature Fusion Transformer for Multi-Modal Action Anticipation,0.86545,"Although human action anticipation is a task which is inherently multi-modal,
state-of-the-art methods on well known action anticipation datasets leverage
this data by applying ensemble methods and averaging scores of unimodal
anticipation networks. In this work we introduce transformer based modality
fusion techniques, which unify multi-modal data at an early stage. Our
Anticipative Feature Fusion Transformer (AFFT) proves to be superior to popular
score fusion approaches and presents state-of-the-art results outperforming
previous methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily
extensible and allows for adding new modalities without architectural changes.
Consequently, we extracted audio features on EpicKitchens-100 which we add to
the set of commonly used features in the community.",https://github.com/zeyun-zhong/AFFT,21427
5768fdb7-3d61-43a6-9d1f-efd757828f79,Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words,0.465183,"The standard BERT adopts subword-based tokenization, which may break a word
into two or more wordpieces (e.g., converting ""lossless"" to ""loss"" and ""less"").
This will bring inconvenience in following situations: (1) what is the best way
to obtain the contextual vector of a word that is divided into multiple
wordpieces? (2) how to predict a word via cloze test without knowing the number
of wordpieces in advance? In this work, we explore the possibility of
developing BERT-style pretrained model over a vocabulary of words instead of
wordpieces. We call such word-level BERT model as WordBERT. We train models
with different vocabulary sizes, initialization configurations and languages.
Results show that, compared to standard wordpiece-based BERT, WordBERT makes
significant improvements on cloze test and machine reading comprehension. On
many other natural language understanding tasks, including POS tagging,
chunking and NER, WordBERT consistently performs better than BERT. Model
analysis indicates that the major advantage of WordBERT over BERT lies in the
understanding for low-frequency words and rare words. Furthermore, since the
pipeline is language-independent, we train WordBERT for Chinese language and
obtain significant gains on five natural language understanding datasets.
Lastly, the analyse on inference speed illustrates WordBERT has comparable time
cost to BERT in natural language understanding tasks.",None,-1
9820dafa-863f-42be-bd1b-1d34ea945234,Hierarchical Instance Mixing across Domains in Aerial Segmentation,0.648687,"We investigate the task of unsupervised domain adaptation in aerial semantic
segmentation and discover that the current state-of-the-art algorithms designed
for autonomous driving based on domain mixing do not translate well to the
aerial setting. This is due to two factors: (i) a large disparity in the
extension of the semantic categories, which causes a domain imbalance in the
mixed image, and (ii) a weaker structural consistency in aerial scenes than in
driving scenes since the same scene might be viewed from different perspectives
and there is no well-defined and repeatable structure of the semantic elements
in the images. Our solution to these problems is composed of: (i) a new mixing
strategy for aerial segmentation across domains called Hierarchical Instance
Mixing (HIMix), which extracts a set of connected components from each semantic
mask and mixes them according to a semantic hierarchy and, (ii) a twin-head
architecture in which two separate segmentation heads are fed with variations
of the same images in a contrastive fashion to produce finer segmentation maps.
We conduct extensive experiments on the LoveDA benchmark, where our solution
outperforms the current state-of-the-art.",None,-1
1b00acd4-8fba-4a3b-b8a5-358807110f33,C-Pack of IPAs: A C90 Program Benchmark of Introductory Programming Assignments,0.594346,"Due to the vast number of students enrolled in Massive Open Online Courses
(MOOCs), there has been an increasing number of automated program repair
techniques focused on introductory programming assignments (IPAs). Such
techniques take advantage of previous correct student implementations in order
to provide automated, comprehensive, and personalized feedback to students.
  This paper presents C-Pack-IPAs, a publicly available benchmark of students'
programs submitted for 25 different IPAs. C-Pack-IPAs contains semantically
correct, semantically incorrect, and syntactically incorrect programs plus a
test suite for each IPA. Hence, C-Pack-IPAs can be used to help evaluate the
development of novel semantic, as well as syntactic, automated program repair
frameworks, focused on providing feedback to novice programmers.",https://github.com/pmorvalho/C-Pack-IPAs,2685
8e2cd029-e951-4165-a3e9-5ab929f0c0ff,Intelligent analysis of EEG signals to assess consumer decisions: A Study on Neuromarketing,0.583814,"Neuromarketing is an emerging field that combines neuroscience and marketing
to understand the factors that influence consumer decisions better. The study
proposes a method to understand consumers' positive and negative reactions to
advertisements (ads) and products by analysing electroencephalogram (EEG)
signals. These signals are recorded using a low-cost single electrode headset
from volunteers belonging to the ages 18-22. A detailed subject dependent (SD)
and subject independent (SI) analysis was performed employing machine learning
methods like Naive Bayes (NB), Support Vector Machine (SVM), k-nearest
neighbour and Decision Tree and the proposed deep learning (DL) model. SVM and
NB yielded an accuracy (Acc.) of 0.63 for the SD analysis. In SI analysis, SVM
performed better for the advertisement, product and gender-based analysis.
Furthermore, the performance of the DL model was on par with that of SVM,
especially, in product and ads-based analysis.",None,-1
020c6e12-1493-4ea9-98e2-f2f0fb308ff1,Learning Texture Transformer Network for Light Field Super-Resolution,0.17142,"Hand-held light field cameras suffer from low spatial resolution due to the
inherent spatio-angular tradeoff. In this paper, we propose a method to improve
the spatial resolution of light field images with the aid of the Texture
Transformer Network (TTSR). The proposed method consists of three modules: the
first module produces an all-in focus high-resolution perspective image which
serves as a reference image for the second module, i.e. TTSR, which in turn
produces a high-resolution light field. The last module refines the spatial
resolution by imposing a light field prior. The results demonstrate around 4 dB
to 6 dB PSNR gain over a bicubically resized light field image",None,-1
809d689d-d039-4082-a5a8-11df3116e6b5,An End-to-End Transformer Model for Crowd Localization,0.619025,"Crowd localization, predicting head positions, is a more practical and
high-level task than simply counting. Existing methods employ pseudo-bounding
boxes or pre-designed localization maps, relying on complex post-processing to
obtain the head positions. In this paper, we propose an elegant, end-to-end
Crowd Localization Transformer named CLTR that solves the task in the
regression-based paradigm. The proposed method views the crowd localization as
a direct set prediction problem, taking extracted features and trainable
embeddings as input of the transformer-decoder. To reduce the ambiguous points
and generate more reasonable matching results, we introduce a KMO-based
Hungarian matcher, which adopts the nearby context as the auxiliary matching
cost. Extensive experiments conducted on five datasets in various data settings
show the effectiveness of our method. In particular, the proposed method
achieves the best localization performance on the NWPU-Crowd, UCF-QNRF, and
ShanghaiTech Part A datasets.",https://dk-liang.github.io/CLTR/,-1
33cffb09-a51a-4892-94a3-3e2a2e45f169,Multi-task Active Learning for Pre-trained Transformer-based Models,0.338477,"Multi-task learning, in which several tasks are jointly learned by a single
model, allows NLP models to share information from multiple annotations and may
facilitate better predictions when the tasks are inter-related. This technique,
however, requires annotating the same text with multiple annotation schemes
which may be costly and laborious. Active learning (AL) has been demonstrated
to optimize annotation processes by iteratively selecting unlabeled examples
whose annotation is most valuable for the NLP model. Yet, multi-task active
learning (MT-AL) has not been applied to state-of-the-art pre-trained
Transformer-based NLP models. This paper aims to close this gap. We explore
various multi-task selection criteria in three realistic multi-task scenarios,
reflecting different relations between the participating tasks, and demonstrate
the effectiveness of multi-task compared to single-task selection. Our results
suggest that MT-AL can be effectively used in order to minimize annotation
efforts for multi-task NLP models.",https://github.com/rotmanguy/MTAL,7661
6d94c2c9-72ef-4f45-8567-5b59291f7218,Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection,0.15529,"Over the past few years there has been major progress in the field of
synthetic data generation using simulation based techniques. These methods use
high-end graphics engines and physics-based ray-tracing rendering in order to
represent the world in 3D and create highly realistic images. Datagen has
specialized in the generation of high-quality 3D humans, realistic 3D
environments and generation of realistic human motion. This technology has been
developed into a data generation platform which we used for these experiments.
This work demonstrates the use of synthetic photo-realistic in-cabin data to
train a Driver Monitoring System that uses a lightweight neural network to
detect whether the driver's hands are on the wheel. We demonstrate that when
only a small amount of real data is available, synthetic data can be a simple
way to boost performance. Moreover, we adopt the data-centric approach and show
how performing error analysis and generating the missing edge-cases in our
platform boosts performance. This showcases the ability of human-centric
synthetic data to generalize well to the real world, and help train algorithms
in computer vision settings where data from the target domain is scarce or hard
to collect.",None,-1
02896e49-bf8f-4c29-8ac0-d33ce82455d7,Towards Robust k-Nearest-Neighbor Machine Translation,0.713316,"k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research
direction of NMT in recent years. Its main idea is to retrieve useful key-value
pairs from an additional datastore to modify translations without updating the
NMT model. However, the underlying retrieved noisy pairs will dramatically
deteriorate the model performance. In this paper, we conduct a preliminary
study and find that this problem results from not fully exploiting the
prediction of the NMT model. To alleviate the impact of noise, we propose a
confidence-enhanced kNN-MT model with robust training. Concretely, we introduce
the NMT confidence to refine the modeling of two important components of
kNN-MT: kNN distribution and the interpolation weight. Meanwhile we inject two
types of perturbations into the retrieved pairs for robust training.
Experimental results on four benchmark datasets demonstrate that our model not
only achieves significant improvements over current kNN-MT models, but also
exhibits better robustness. Our code is available at
https://github.com/DeepLearnXMU/Robust-knn-mt.",https://github.com/DeepLearnXMU/Robust-knn-mt,4230
321934a5-e0eb-4d1b-aa3a-bec74dedc488,Low Light Video Enhancement by Learning on Static Videos with Cross-Frame Attention,0.151477,"The design of deep learning methods for low light video enhancement remains a
challenging problem owing to the difficulty in capturing low light and ground
truth video pairs. This is particularly hard in the context of dynamic scenes
or moving cameras where a long exposure ground truth cannot be captured. We
approach this problem by training a model on static videos such that the model
can generalize to dynamic videos. Existing methods adopting this approach
operate frame by frame and do not exploit the relationships among neighbouring
frames. We overcome this limitation through a selfcross dilated attention
module that can effectively learn to use information from neighbouring frames
even when dynamics between the frames are different during training and test
times. We validate our approach through experiments on multiple datasets and
show that our method outperforms other state-of-the-art video enhancement
algorithms when trained only on static videos.",None,-1
63c3b7bf-e733-476a-9d4d-fb16fbd78a39,On resolving conflicts between arguments,0.0806153,"Argument systems are based on the idea that one can construct arguments for
propositions; i.e., structured reasons justifying the belief in a proposition.
Using defeasible rules, arguments need not be valid in all circumstances,
therefore, it might be possible to construct an argument for a proposition as
well as its negation. When arguments support conflicting propositions, one of
the arguments must be defeated, which raises the question of \emph{which
(sub-)arguments can be subject to defeat}?
  In legal argumentation, meta-rules determine the valid arguments by
considering the last defeasible rule of each argument involved in a conflict.
Since it is easier to evaluate arguments using their last rules, \emph{can a
conflict be resolved by considering only the last defeasible rules of the
arguments involved}?
  We propose a new argument system where, instead of deriving a defeat relation
between arguments, \emph{undercutting-arguments} for the defeat of defeasible
rules are constructed. This system allows us, (\textit{i}) to resolve conflicts
(a generalization of rebutting arguments) using only the last rules of the
arguments for inconsistencies, (\textit{ii}) to determine a set of valid
(undefeated) arguments in linear time using an algorithm based on a JTMS,
(\textit{iii}) to establish a relation with Default Logic, and (\textit{iv}) to
prove closure properties such as \emph{cumulativity}. We also propose an
extension of the argument system that enables \emph{reasoning by cases}.",None,-1
7ad63ac5-5e2b-4244-8f48-7c9dac4e5401,Equilibrium Aggregation: Encoding Sets via Optimization,0.211275,"Processing sets or other unordered, potentially variable-sized inputs in
neural networks is usually handled by aggregating a number of input tensors
into a single representation. While a number of aggregation methods already
exist from simple sum pooling to multi-head attention, they are limited in
their representational power both from theoretical and empirical perspectives.
On the search of a principally more powerful aggregation strategy, we propose
an optimization-based method called Equilibrium Aggregation. We show that many
existing aggregation methods can be recovered as special cases of Equilibrium
Aggregation and that it is provably more efficient in some important cases.
Equilibrium Aggregation can be used as a drop-in replacement in many existing
architectures and applications. We validate its efficiency on three different
tasks: median estimation, class counting, and molecular property prediction. In
all experiments, Equilibrium Aggregation achieves higher performance than the
other aggregation techniques we test.",http://github.com/google/jax,-1
2704a77f-cd75-418b-9534-849ac59abdc1,Selective Residual M-Net for Real Image Denoising,0.36953,"Image restoration is a low-level vision task which is to restore degraded
images to noise-free images. With the success of deep neural networks, the
convolutional neural networks surpass the traditional restoration methods and
become the mainstream in the computer vision area. To advance the performanceof
denoising algorithms, we propose a blind real image denoising network (SRMNet)
by employing a hierarchical architecture improved from U-Net. Specifically, we
use a selective kernel with residual block on the hierarchical structure called
M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet
has competitive performance results on two synthetic and two real-world noisy
datasets in terms of quantitative metrics and visual quality. The source code
and pretrained model are available at
https://github.com/TentativeGitHub/SRMNet.",https://github.com/TentativeGitHub/SRMNet,1572
58275832-143a-403a-b0ee-1a72b7787e51,High-resolution Face Swapping via Latent Semantics Disentanglement,0.865615,"We present a novel high-resolution face swapping method using the inherent
prior knowledge of a pre-trained GAN model. Although previous research can
leverage generative priors to produce high-resolution results, their quality
can suffer from the entangled semantics of the latent space. We explicitly
disentangle the latent semantics by utilizing the progressive nature of the
generator, deriving structure attributes from the shallow layers and appearance
attributes from the deeper ones. Identity and pose information within the
structure attributes are further separated by introducing a landmark-driven
structure transfer latent direction. The disentangled latent code produces rich
generative features that incorporate feature blending to produce a plausible
swapping result. We further extend our method to video face swapping by
enforcing two spatio-temporal constraints on the latent space and the image
space. Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art image/video face swapping methods in terms of hallucination
quality and consistency. Code can be found at:
https://github.com/cnnlstm/FSLSD_HiRes.",https://github.com/cnnlstm/FSLSD_HiRes,-1
243b718b-0fdc-42bd-add9-328aad023147,LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling,0.449597,"Compared with traditional task-irrelevant downsampling methods, task-oriented
neural networks have shown improved performance in point cloud downsampling
range. Recently, Transformer family of networks has shown a more powerful
learning capacity in visual tasks. However, Transformer-based architectures
potentially consume too many resources which are usually worthless for low
overhead task networks in downsampling range. This paper proposes a novel
light-weight Transformer network (LighTN) for task-oriented point cloud
downsampling, as an end-to-end and plug-and-play solution. In LighTN, a
single-head self-correlation module is presented to extract refined global
contextual features, where three projection matrices are simultaneously
eliminated to save resource overhead, and the output of symmetric matrix
satisfies the permutation invariant. Then, we design a novel downsampling loss
function to guide LighTN focuses on critical point cloud regions with more
uniform distribution and prominent points coverage. Furthermore, We introduce a
feed-forward network scaling mechanism to enhance the learnable capacity of
LighTN according to the expand-reduce strategy. The result of extensive
experiments on classification and registration tasks demonstrates LighTN can
achieve state-of-the-art performance with limited resource overhead.",None,14870
dec4c018-751e-4eaf-8645-b47a0008514a,Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition,0.195201,"We propose novel AI-empowered chat bots for learning as conversation where a
user does not read a passage but gains information and knowledge through
conversation with a teacher bot. Our information-acquisition-oriented dialogue
system employs a novel adaptation of reinforced self-play so that the system
can be transferred to various domains without in-domain dialogue data, and can
carry out conversations both informative and attentive to users. Our extensive
subjective and objective evaluations on three large public data corpora
demonstrate the effectiveness of our system to deliver knowledge-intensive and
attentive conversations and help end users substantially gain knowledge without
reading passages. Our code and datasets are publicly available for follow-up
research.",https://github.com/IBM/reinforced-dialog-system-for-learning,-1
f19db7db-0d06-45dc-a428-aefee2bb6362,AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages,0.512352,"In recent years, multilingual pre-trained language models have gained
prominence due to their remarkable performance on numerous downstream Natural
Language Processing tasks (NLP). However, pre-training these large multilingual
language models requires a lot of training data, which is not available for
African Languages. Active learning is a semi-supervised learning algorithm, in
which a model consistently and dynamically learns to identify the most
beneficial samples to train itself on, in order to achieve better optimization
and performance on downstream tasks. Furthermore, active learning effectively
and practically addresses real-world data scarcity. Despite all its benefits,
active learning, in the context of NLP and especially multilingual language
models pretraining, has received little consideration. In this paper, we
present AfroLM, a multilingual language model pretrained from scratch on 23
African languages (the largest effort to date) using our novel self-active
learning framework. Pretrained on a dataset significantly (14x) smaller than
existing baselines, AfroLM outperforms many multilingual pretrained language
models (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text
classification, and sentiment analysis). Additional out-of-domain sentiment
analysis experiments show that \textbf{AfroLM} is able to generalize well
across various domains. We release the code source, and our datasets used in
our framework at https://github.com/bonaventuredossou/MLM_AL.",https://github.com/bonaventuredossou/MLM_AL,-1
6ed53b07-d63c-443d-9e87-78c79d98f223,Towards Efficient Neural Scene Graphs by Learning Consistency Fields,0.166185,"Neural Radiance Fields (NeRF) achieves photo-realistic image rendering from
novel views, and the Neural Scene Graphs (NSG) \cite{ost2021neural} extends it
to dynamic scenes (video) with multiple objects. Nevertheless, computationally
heavy ray marching for every image frame becomes a huge burden. In this paper,
taking advantage of significant redundancy across adjacent frames in videos, we
propose a feature-reusing framework. From the first try of naively reusing the
NSG features, however, we learn that it is crucial to disentangle
object-intrinsic properties consistent across frames from transient ones. Our
proposed method, \textit{Consistency-Field-based NSG (CF-NSG)}, reformulates
neural radiance fields to additionally consider \textit{consistency fields}.
With disentangled representations, CF-NSG takes full advantage of the
feature-reusing scheme and performs an extended degree of scene manipulation in
a more controllable manner. We empirically verify that CF-NSG greatly improves
the inference efficiency by using 85\% less queries than NSG without notable
degradation in rendering quality. Code will be available at:
https://github.com/ldynx/CF-NSG",https://github.com/ldynx/CF-NSG,-1
a11f4862-9147-4895-be74-4014d69c4b71,Fairness Increases Adversarial Vulnerability,0.346236,"The remarkable performance of deep learning models and their applications in
consequential domains (e.g., facial recognition) introduces important
challenges at the intersection of equity and security. Fairness and robustness
are two desired notions often required in learning models. Fairness ensures
that models do not disproportionately harm (or benefit) some groups over
others, while robustness measures the models' resilience against small input
perturbations.
  This paper shows the existence of a dichotomy between fairness and
robustness, and analyzes when achieving fairness decreases the model robustness
to adversarial samples. The reported analysis sheds light on the factors
causing such contrasting behavior, suggesting that distance to the decision
boundary across groups as a key explainer for this behavior. Extensive
experiments on non-linear models and different architectures validate the
theoretical findings in multiple vision domains. Finally, the paper proposes a
simple, yet effective, solution to construct models achieving good tradeoffs
between fairness and robustness.",None,-1
61c6d1cf-cec0-46fc-8b8a-e2031a8c2865,BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information,0.12473,"Time is an important aspect of documents and is used in a range of NLP and IR
tasks. In this work, we investigate methods for incorporating temporal
information during pre-training to further improve the performance on
time-related tasks. Compared with common pre-trained language models like BERT
which utilize synchronic document collections (e.g., BookCorpus and Wikipedia)
as the training corpora, we use long-span temporal news article collection for
building word representations. We introduce BiTimeBERT, a novel language
representation model trained on a temporal collection of news articles via two
new pre-training tasks, which harnesses two distinct temporal signals to
construct time-aware language representations. The experimental results show
that BiTimeBERT consistently outperforms BERT and other existing pre-trained
models with substantial gains on different downstream NLP tasks and
applications for which time is of importance (e.g., the accuracy improvement
over BERT is 155\% on the event time estimation task).",https://github.com/WangJiexin/BiTimeBERT,-1
f104536c-87cf-4bb1-8057-6c26bbdc6364,Online Segmentation of LiDAR Sequences: Dataset and Algorithm,0.537489,"Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles.
However, most semantic datasets and algorithms used for LiDAR sequence
segmentation operate on $360^\circ$ frames, causing an acquisition latency
incompatible with real-time applications. To address this issue, we first
introduce HelixNet, a $10$ billion point dataset with fine-grained labels,
timestamps, and sensor rotation information necessary to accurately assess the
real-time readiness of segmentation algorithms. Second, we propose Helix4D, a
compact and efficient spatio-temporal transformer architecture specifically
designed for rotating LiDAR sequences. Helix4D operates on acquisition slices
corresponding to a fraction of a full sensor rotation, significantly reducing
the total latency. Helix4D reaches accuracy on par with the best segmentation
algorithms on HelixNet and SemanticKITTI with a reduction of over $5\times$ in
terms of latency and $50\times$ in model size. The code and data are available
at: https://romainloiseau.fr/helixnet",https://romainloiseau.fr/helixnet,-1
66d1d767-a167-4df2-8485-8905be2da3c0,Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting,0.802616,"In this paper, we propose a novel end-to-end user-defined keyword spotting
method that utilizes linguistically corresponding patterns between speech and
text sequences. Unlike previous approaches requiring speech keyword enrollment,
our method compares input queries with an enrolled text keyword sequence. To
place the audio and text representations within a common latent space, we adopt
an attention-based cross-modal matching approach that is trained in an
end-to-end manner with monotonic matching loss and keyword classification loss.
We also utilize a de-noising loss for the acoustic embedding network to improve
robustness in noisy environments. Additionally, we introduce the LibriPhrase
dataset, a new short-phrase dataset based on LibriSpeech for efficiently
training keyword spotting models. Our proposed method achieves competitive
results on various evaluation sets compared to other single-modal and
cross-modal baselines.",https://github.com/gusrud1103/LibriPhrase.git,-1
3a4e42ce-0a03-4420-9288-45566cf95fa6,HIT at SemEval-2022 Task 2: Pre-trained Language Model for Idioms Detection,0.0379321,"The same multi-word expressions may have different meanings in different
sentences. They can be mainly divided into two categories, which are literal
meaning and idiomatic meaning. Non-contextual-based methods perform poorly on
this problem, and we need contextual embedding to understand the idiomatic
meaning of multi-word expressions correctly. We use a pre-trained language
model, which can provide a context-aware sentence embedding, to detect whether
multi-word expression in the sentence is idiomatic usage.",None,-1
378836c0-03bf-4151-99b9-66604f8bc5f1,Stochastic analysis of the Elo rating algorithm in round-robin tournaments,0.367307,"The Elo algorithm, renowned for its simplicity, is widely used for rating in
sports tournaments and other applications. However, despite its widespread use,
a detailed understanding of the convergence characteristics of the Elo
algorithm is still lacking. Aiming to fill this gap, this paper presents a
comprehensive (stochastic) analysis of the Elo algorithm, considering
round-robin tournaments. Specifically, analytical expressions are derived
describing the evolution of the skills and performance metrics. Then, taking
into account the relationship between the behavior of the algorithm and the
step-size value, which is a hyperparameter that can be controlled, design
guidelines and discussions about the performance of the algorithm are provided.
Experimental results are shown confirming the accuracy of the analysis and
illustrating the applicability of the theoretical findings using real-world
data obtained from SuperLega, the Italian volleyball league.",https://github.com/dangpzanco/elo-rating,2617
f61b012b-310e-4abd-aa3c-c9ea545fa957,Exemplar Free Class Agnostic Counting,0.819298,"We tackle the task of Class Agnostic Counting, which aims to count objects in
a novel object category at test time without any access to labeled training
data for that category. All previous class agnostic counting methods cannot
work in a fully automated setting, and require computationally expensive test
time adaptation. To address these challenges, we propose a visual counter which
operates in a fully automated setting and does not require any test time
adaptation. Our proposed approach first identifies exemplars from repeating
objects in an image, and then counts the repeating objects. We propose a novel
region proposal network for identifying the exemplars. After identifying the
exemplars, we obtain the corresponding count by using a density estimation
based Visual Counter. We evaluate our proposed approach on FSC-147 dataset, and
show that it achieves superior performance compared to the existing approaches.",None,-1
2a1962d7-e51c-477b-a8e4-c7e188acd20a,Multi-Target Search in Euclidean Space with Ray Shooting (Full Version),0.077497,"The Euclidean shortest path problem (ESPP) is a well studied problem with
many practical applications. Recently a new efficient online approach to this
problem, RayScan, has been developed, based on ray shooting and polygon
scanning. In this paper we show how we can improve RayScan by carefully
reasoning about polygon scans. We also look into how RayScan could be applied
in the single-source multi-target scenario, where logic during scanning is used
to reduce the number of rays shots required. This improvement also helps in the
single target case. We compare the improved RayScan+ against the
state-of-the-art ESPP algorithm, illustrating the situations where it is
better.",https://bitbucket.org/ryanhech/rayscan/,-1
47a18233-6bc6-4640-aa18-a9c9a6f5fd96,Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction,0.518598,"Dimensionality reduction is crucial both for visualization and preprocessing
high dimensional data for machine learning. We introduce a novel method based
on a hierarchy built on 1-nearest neighbor graphs in the original space which
is used to preserve the grouping properties of the data distribution on
multiple levels. The core of the proposal is an optimization-free projection
that is competitive with the latest versions of t-SNE and UMAP in performance
and visualization quality while being an order of magnitude faster in run-time.
Furthermore, its interpretable mechanics, the ability to project new data, and
the natural separation of data clusters in visualizations make it a general
purpose unsupervised dimension reduction technique. In the paper, we argue
about the soundness of the proposed method and evaluate it on a diverse
collection of datasets with sizes varying from 1K to 11M samples and dimensions
from 28 to 16K. We perform comparisons with other state-of-the-art methods on
multiple metrics and target dimensions highlighting its efficiency and
performance. Code is available at https://github.com/koulakis/h-nne",https://github.com/koulakis/h-nne,-1
01cafb4a-650d-4a69-a622-5e0f554d1053,Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness,0.56065,"With Artificial intelligence (AI) to aid or automate decision-making
advancing rapidly, a particular concern is its fairness. In order to create
reliable, safe and trustworthy systems through human-centred artificial
intelligence (HCAI) design, recent efforts have produced user interfaces (UIs)
for AI experts to investigate the fairness of AI models. In this work, we
provide a design space exploration that supports not only data scientists but
also domain experts to investigate AI fairness. Using loan applications as an
example, we held a series of workshops with loan officers and data scientists
to elicit their requirements. We instantiated these requirements into FairHIL,
a UI to support human-in-the-loop fairness investigations, and describe how
this UI could be generalized to other use cases. We evaluated FairHIL through a
think-aloud user study. Our work contributes better designs to investigate an
AI model's fairness-and move closer towards responsible AI.",None,758
1eb661cd-3934-4f0b-9d39-820f01bf2aa8,A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering,0.458414,"Outside-knowledge visual question answering (OK-VQA) requires the agent to
comprehend the image, make use of relevant knowledge from the entire web, and
digest all the information to answer the question. Most previous works address
the problem by first fusing the image and question in the multi-modal space,
which is inflexible for further fusion with a vast amount of external
knowledge. In this paper, we call for a paradigm shift for the OK-VQA task,
which transforms the image into plain text, so that we can enable knowledge
passage retrieval, and generative question-answering in the natural language
space. This paradigm takes advantage of the sheer volume of gigantic knowledge
bases and the richness of pre-trained language models. A
Transform-Retrieve-Generate framework (TRiG) framework is proposed, which can
be plug-and-played with alternative image-to-text models and textual knowledge
bases. Experimental results show that our TRiG framework outperforms all
state-of-the-art supervised methods by at least 11.1% absolute margin.",https://github.com/JaidedAI/EasyOCR,-1
fd25600f-8c7f-44ff-a678-b3bcef6a6238,Learning to Imitate Object Interactions from Internet Videos,0.693131,"We study the problem of imitating object interactions from Internet videos.
This requires understanding the hand-object interactions in 4D, spatially in 3D
and over time, which is challenging due to mutual hand-object occlusions. In
this paper we make two main contributions: (1) a novel reconstruction technique
RHOV (Reconstructing Hands and Objects from Videos), which reconstructs 4D
trajectories of both the hand and the object using 2D image cues and temporal
smoothness constraints; (2) a system for imitating object interactions in a
physics simulator with reinforcement learning. We apply our reconstruction
technique to 100 challenging Internet videos. We further show that we can
successfully imitate a range of different object interactions in a physics
simulator. Our object-centric approach is not limited to human-like
end-effectors and can learn to imitate object interactions using different
embodiments, like a robotic arm with a parallel jaw gripper.",None,-1
03c00ae7-6ab7-4b44-b88c-a1e66500980a,Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-SQL Parsers,0.103145,"Text-to-SQL parsers typically struggle with databases unseen during the train
time. Adapting parsers to new databases is a challenging problem due to the
lack of natural language queries in the new schemas. We present ReFill, a
framework for synthesizing high-quality and textually diverse parallel datasets
for adapting a Text-to-SQL parser to a target schema. ReFill learns to
retrieve-and-edit text queries from the existing schemas and transfers them to
the target schema. We show that retrieving diverse existing text, masking their
schema-specific tokens, and refilling with tokens relevant to the target
schema, leads to significantly more diverse text queries than achievable by
standard SQL-to-Text generation methods. Through experiments spanning multiple
databases, we demonstrate that fine-tuning parsers on datasets synthesized
using ReFill consistently outperforms the prior data-augmentation methods.",https://github.com/awasthiabhijeet/refill,-1
39bf0807-55f6-4fe4-b30f-1fa3604f6a50,"Theories of ""Gender"" in NLP Bias Research",0.859482,"The rise of concern around Natural Language Processing (NLP) technologies
containing and perpetuating social biases has led to a rich and rapidly growing
area of research. Gender bias is one of the central biases being analyzed, but
to date there is no comprehensive analysis of how ""gender"" is theorized in the
field. We survey nearly 200 articles concerning gender bias in NLP to discover
how the field conceptualizes gender both explicitly (e.g. through definitions
of terms) and implicitly (e.g. through how gender is operationalized in
practice). In order to get a better idea of emerging trajectories of thought,
we split these articles into two sections by time.
  We find that the majority of the articles do not make their theorization of
gender explicit, even if they clearly define ""bias."" Almost none use a model of
gender that is intersectional or inclusive of nonbinary genders; and many
conflate sex characteristics, social gender, and linguistic gender in ways that
disregard the existence and experience of trans, nonbinary, and intersex
people. There is an increase between the two time-sections in statements
acknowledging that gender is a complicated reality, however, very few articles
manage to put this acknowledgment into practice. In addition to analyzing these
findings, we provide specific recommendations to facilitate interdisciplinary
work, and to incorporate theory and methodology from Gender Studies. Our hope
is that this will produce more inclusive gender bias research in NLP.",None,-1
01d83098-3e11-4d3e-bc67-499b07b8c4f5,PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales,0.64001,"Neural language models (LMs) have achieved impressive results on various
language-based reasoning tasks by utilizing latent knowledge encoded in their
own pretrained parameters. To make this reasoning process more explicit, recent
works retrieve a rationalizing LM's internal knowledge by training or prompting
it to generate free-text rationales, which can be used to guide task
predictions made by either the same LM or a separate reasoning LM. However,
rationalizing LMs require expensive rationale annotation and/or computation,
without any assurance that their generated rationales improve LM task
performance or faithfully reflect LM decision-making. In this paper, we propose
PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns
to faithfully reason over rationales via counterfactual regularization. First,
PINTO maps out a suitable reasoning process for the task input by prompting a
frozen rationalizing LM to generate a free-text rationale. Second, PINTO's
reasoning LM is fine-tuned to solve the task using the generated rationale as
context, while regularized to output less confident predictions when the
rationale is perturbed. Across four datasets, we show that PINTO significantly
improves the generalization ability of the reasoning LM, yielding higher
performance on both in-distribution and out-of-distribution test sets. Also, we
find that PINTO's rationales are more faithful to its task predictions than
those generated by competitive baselines.",https://github.com/wangpf3/pinto-faithful-language-reasoning,-1
9fff456f-6ef7-4fe4-b504-a48a40d0b381,Reducing the Vision and Language Bias for Temporal Sentence Grounding,0.869615,"Temporal sentence grounding (TSG) is an important yet challenging task in
multimedia information retrieval. Although previous TSG methods have achieved
decent performance, they tend to capture the selection biases of frequently
appeared video-query pairs in the dataset rather than present robust multimodal
reasoning abilities, especially for the rarely appeared pairs. In this paper,
we study the above issue of selection biases and accordingly propose a
Debiasing-TSG (D-TSG) model to filter and remove the negative biases in both
vision and language modalities for enhancing the model generalization ability.
Specifically, we propose to alleviate the issue from two perspectives: 1)
Feature distillation. We built a multi-modal debiasing branch to firstly
capture the vision and language biases, and then apply a bias identification
module to explicitly recognize the true negative biases and remove them from
the benign multi-modal representations. 2) Contrastive sample generation. We
construct two types of negative samples to enforce the model to accurately
learn the aligned multi-modal semantics and make complete semantic reasoning.
We apply the proposed model to both commonly and rarely appeared TSG cases, and
demonstrate its effectiveness by achieving the state-of-the-art performance on
three benchmark datasets (ActivityNet Caption, TACoS, and Charades-STA).",None,-1
3e460cea-a957-43d3-a93c-f576d0fed5e3,Smart Multi-tenant Federated Learning,0.242747,"Federated learning (FL) is an emerging distributed machine learning method
that empowers in-situ model training on decentralized edge devices. However,
multiple simultaneous training activities could overload resource-constrained
devices. In this work, we propose a smart multi-tenant FL system, MuFL, to
effectively coordinate and execute simultaneous training activities. We first
formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and
introduce a vanilla multi-tenant FL system that trains activities sequentially
to form baselines. Then, we propose two approaches to optimize multi-tenant FL:
1) activity consolidation merges training activities into one activity with a
multi-task architecture; 2) after training it for rounds, activity splitting
divides it into groups by employing affinities among activities such that
activities within a group have better synergy. Extensive experiments
demonstrate that MuFL outperforms other methods while consuming 40% less
energy. We hope this work will inspire the community to further study and
optimize multi-tenant FL.",https://github.com/tstandley/taskgrouping,20521
19fc0655-f653-4eb5-9f80-9dbefbad4b39,Improving Chinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity,0.788199,"Chinese spelling check (CSC) is a fundamental NLP task that detects and
corrects spelling errors in Chinese texts. As most of these spelling errors are
caused by phonetic similarity, effectively modeling the pronunciation of
Chinese characters is a key factor for CSC. In this paper, we consider
introducing an auxiliary task of Chinese pronunciation prediction (CPP) to
improve CSC, and, for the first time, systematically discuss the adaptivity and
granularity of this auxiliary task. We propose SCOPE which builds on top of a
shared encoder two parallel decoders, one for the primary CSC task and the
other for a fine-grained auxiliary CPP task, with a novel adaptive weighting
scheme to balance the two tasks. In addition, we design a delicate iterative
correction strategy for further improvements during inference. Empirical
evaluation shows that SCOPE achieves new state-of-the-art on three CSC
benchmarks, demonstrating the effectiveness and superiority of the auxiliary
CPP task. Comprehensive ablation studies further verify the positive effects of
adaptivity and granularity of the task. Code and data used in this paper are
publicly available at https://github.com/jiahaozhenbang/SCOPE.",https://github.com/jiahaozhenbang/SCOPE,-1
46f066a7-d068-4038-b92a-51d8efce0a8d,Collaborating Domain-shared and Target-specific Feature Clustering for Cross-domain 3D Action Recognition,0.550491,"In this work, we consider the problem of cross-domain 3D action recognition
in the open-set setting, which has been rarely explored before. Specifically,
there is a source domain and a target domain that contain the skeleton
sequences with different styles and categories, and our purpose is to cluster
the target data by utilizing the labeled source data and unlabeled target data.
For such a challenging task, this paper presents a novel approach dubbed CoDT
to collaboratively cluster the domain-shared features and target-specific
features. CoDT consists of two parallel branches. One branch aims to learn
domain-shared features with supervised learning in the source domain, while the
other is to learn target-specific features using contrastive learning in the
target domain. To cluster the features, we propose an online clustering
algorithm that enables simultaneous promotion of robust pseudo label generation
and feature clustering. Furthermore, to leverage the complementarity of
domain-shared features and target-specific features, we propose a novel
collaborative clustering strategy to enforce pair-wise relationship consistency
between the two branches. We conduct extensive experiments on multiple
cross-domain 3D action recognition datasets, and the results demonstrate the
effectiveness of our method.",None,-1
60ac230a-2767-4210-a796-604e95158f02,Multi-modal Contrastive Representation Learning for Entity Alignment,0.90175,"Multi-modal entity alignment aims to identify equivalent entities between two
different multi-modal knowledge graphs, which consist of structural triples and
images associated with entities. Most previous works focus on how to utilize
and encode information from different modalities, while it is not trivial to
leverage multi-modal knowledge in entity alignment because of the modality
heterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive
Learning based Entity Alignment model, to obtain effective joint
representations for multi-modal entity alignment. Different from previous
works, MCLEA considers task-oriented modality and models the inter-modal
relationships for each entity representation. In particular, MCLEA firstly
learns multiple individual representations from multiple modalities, and then
performs contrastive learning to jointly model intra-modal and inter-modal
interactions. Extensive experimental results show that MCLEA outperforms
state-of-the-art baselines on public datasets under both supervised and
unsupervised settings.",https://github.com/lzxlin/MCLEA,-1
294d0db9-38d6-46a9-815a-0109cfa7dded,Geometric Graph Representation Learning via Maximizing Rate Reduction,0.487427,"Learning discriminative node representations benefits various downstream
tasks in graph analysis such as community detection and node classification.
Existing graph representation learning methods (e.g., based on random walk and
contrastive learning) are limited to maximizing the local similarity of
connected nodes. Such pair-wise learning schemes could fail to capture the
global distribution of representations, since it has no explicit constraints on
the global geometric properties of representation space. To this end, we
propose Geometric Graph Representation Learning (G2R) to learn node
representations in an unsupervised manner via maximizing rate reduction. In
this way, G2R maps nodes in distinct groups (implicitly stored in the adjacency
matrix) into different subspaces, while each subspace is compact and different
subspaces are dispersedly distributed. G2R adopts a graph neural network as the
encoder and maximizes the rate reduction with the adjacency matrix.
Furthermore, we theoretically and empirically demonstrate that rate reduction
maximization is equivalent to maximizing the principal angles between different
subspaces. Experiments on real-world datasets show that G2R outperforms various
baselines on node classification and community detection tasks.",None,12156
0a10d4d3-f7e2-4b49-ad42-2b74e0415e3f,FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing,0.987216,"We present a benchmark suite of four datasets for evaluating the fairness of
pre-trained language models and the techniques used to fine-tune them for
downstream tasks. Our benchmarks cover four jurisdictions (European Council,
USA, Switzerland, and China), five languages (English, German, French, Italian
and Chinese) and fairness across five attributes (gender, age, region,
language, and legal area). In our experiments, we evaluate pre-trained language
models using several group-robust fine-tuning techniques and show that
performance group disparities are vibrant in many cases, while none of these
techniques guarantee fairness, nor consistently mitigate group disparities.
Furthermore, we provide a quantitative and qualitative analysis of our results,
highlighting open challenges in the development of robustness methods in legal
NLP.",https://github.com/coastalcph/fairlex,-1
1c3e242d-60a6-4c7d-9a4c-e1486bcc731d,POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events,0.126722,"Knowledge about outcomes is critical for complex event understanding but is
hard to acquire. We show that by pre-identifying a participant in a complex
event, crowd workers are able to (1) infer the collective impact of salient
events that make up the situation, (2) annotate the volitional engagement of
participants in causing the situation, and (3) ground the outcome of the
situation in state changes of the participants. By creating a multi-step
interface and a careful quality control strategy, we collect a high quality
annotated dataset of 8K short newswire narratives and ROCStories with high
inter-annotator agreement (0.74-0.96 weighted Fleiss Kappa). Our dataset, POQue
(Participant Outcome Questions), enables the exploration and development of
models that address multiple aspects of semantic understanding. Experimentally,
we show that current language models lag behind human performance in subtle
ways through our task formulations that target abstract and specific
comprehension of a complex event, its outcome, and a participant's influence
over the event culmination.",https://github.com/saiumbc/POQue,-1
76daa354-a863-477e-afb7-52e9384a33a2,"Surround-view Fisheye BEV-Perception for Valet Parking: Dataset, Baseline and Distortion-insensitive Multi-task Framework",0.354069,"Surround-view fisheye perception under valet parking scenes is fundamental
and crucial in autonomous driving. Environmental conditions in parking lots
perform differently from the common public datasets, such as imperfect light
and opacity, which substantially impacts on perception performance. Most
existing networks based on public datasets may generalize suboptimal results on
these valet parking scenes, also affected by the fisheye distortion. In this
article, we introduce a new large-scale fisheye dataset called Fisheye Parking
Dataset(FPD) to promote the research in dealing with diverse real-world
surround-view parking cases. Notably, our compiled FPD exhibits excellent
characteristics for different surround-view perception tasks. In addition, we
also propose our real-time distortion-insensitive multi-task framework Fisheye
Perception Network (FPNet), which improves the surround-view fisheye BEV
perception by enhancing the fisheye distortion operation and multi-task
lightweight designs. Extensive experiments validate the effectiveness of our
approach and the dataset's exceptional generalizability.",None,-1
71ca7f3f-906b-442f-9024-e853b76db4a4,"Automated Clinical Coding: What, Why, and Where We Are?",0.95329,"Clinical coding is the task of transforming medical information in a
patient's health records into structured codes so that they can be used for
statistical analysis. This is a cognitive and time-consuming task that follows
a standard process in order to achieve a high level of consistency. Clinical
coding could potentially be supported by an automated system to improve the
efficiency and accuracy of the process. We introduce the idea of automated
clinical coding and summarise its challenges from the perspective of Artificial
Intelligence (AI) and Natural Language Processing (NLP), based on the
literature, our project experience over the past two and half years (late 2019
- early 2022), and discussions with clinical coding experts in Scotland and the
UK. Our research reveals the gaps between the current deep learning-based
approach applied to clinical coding and the need for explainability and
consistency in real-world practice. Knowledge-based methods that represent and
reason the standard, explainable process of a task may need to be incorporated
into deep learning-based methods for clinical coding. Automated clinical coding
is a promising task for AI, despite the technical and organisational
challenges. Coders are needed to be involved in the development process. There
is much to achieve to develop and deploy an AI-based automated system to
support coding in the next five years and beyond.",None,-1
1b1ad592-7ffc-4d5e-abee-88caad768d64,Demystifying Prompts in Language Models via Perplexity Estimation,0.953758,"Language models can be prompted to perform a wide variety of zero- and
few-shot learning problems. However, performance varies significantly with the
choice of prompt, and we do not yet understand why this happens or how to pick
the best prompts. In this work, we analyze the factors that contribute to this
variance and establish a new empirical hypothesis: the performance of a prompt
is coupled with the extent to which the model is familiar with the language it
contains. Over a wide range of tasks, we show that the lower the perplexity of
the prompt is, the better the prompt is able to perform the task. As a result,
we devise a method for creating prompts: (1) automatically extend a small seed
set of manually written prompts by paraphrasing using GPT3 and backtranslation
and (2) choose the lowest perplexity prompts to get significant gains in
performance.",https://github.com/bigscience-workshop/promptsource,104111
95eab57b-69b2-40fa-a222-c75f4f51ef37,Instance Shadow Detection with A Single-Stage Detector,0.52337,"This paper formulates a new problem, instance shadow detection, which aims to
detect shadow instance and the associated object instance that cast each shadow
in the input image. To approach this task, we first compile a new dataset with
the masks for shadow instances, object instances, and shadow-object
associations. We then design an evaluation metric for quantitative evaluation
of the performance of instance shadow detection. Further, we design a
single-stage detector to perform instance shadow detection in an end-to-end
manner, where the bidirectional relation learning module and the deformable
maskIoU head are proposed in the detector to directly learn the relation
between shadow instances and object instances and to improve the accuracy of
the predicted masks. Finally, we quantitatively and qualitatively evaluate our
method on the benchmark dataset of instance shadow detection and show the
applicability of our method on light direction estimation and photo editing.",https://github.com/stevewongv/SSIS,-1
60f3e8fe-090c-4578-8f25-60d6989ff01d,Learnable Optimal Sequential Grouping for Video Scene Detection,0.160383,"Video scene detection is the task of dividing videos into temporal semantic
chapters. This is an important preliminary step before attempting to analyze
heterogeneous video content. Recently, Optimal Sequential Grouping (OSG) was
proposed as a powerful unsupervised solution to solve a formulation of the
video scene detection problem. In this work, we extend the capabilities of OSG
to the learning regime. By giving the capability to both learn from examples
and leverage a robust optimization formulation, we can boost performance and
enhance the versatility of the technology. We present a comprehensive analysis
of incorporating OSG into deep learning neural networks under various
configurations. These configurations include learning an embedding in a
straight-forward manner, a tailored loss designed to guide the solution of OSG,
and an integrated model where the learning is performed through the OSG
pipeline. With thorough evaluation and analysis, we assess the benefits and
behavior of the various configurations, and show that our learnable OSG
approach exhibits desirable behavior and enhanced performance compared to the
state of the art.",None,-1
160d8797-dce0-4e04-bbf0-480616ebb474,SERE: Exploring Feature Self-relation for Self-supervised Transformer,0.189226,"Learning representations with self-supervision for convolutional networks
(CNN) has been validated to be effective for vision tasks. As an alternative to
CNN, vision transformers (ViT) have strong representation ability with spatial
self-attention and channel-level feedforward networks. Recent works reveal that
self-supervised learning helps unleash the great potential of ViT. Still, most
works follow self-supervised strategies designed for CNN, e.g., instance-level
discrimination of samples, but they ignore the properties of ViT. We observe
that relational modeling on spatial and channel dimensions distinguishes ViT
from other networks. To enforce this property, we explore the feature
SElf-RElation (SERE) for training self-supervised ViT. Specifically, instead of
conducting self-supervised learning solely on feature embeddings from multiple
views, we utilize the feature self-relations, i.e., spatial/channel
self-relations, for self-supervised learning. Self-relation based learning
further enhances the relation modeling ability of ViT, resulting in stronger
representations that stably improve performance on multiple downstream tasks.
Our source code is publicly available at: https://github.com/MCG-NKU/SERE.",https://github.com/MCG-NKU/SERE,-1
7434a2ef-2189-4290-b910-baedc3024f2d,A Distributional Lens for Multi-Aspect Controllable Text Generation,0.738625,"Multi-aspect controllable text generation is a more challenging and practical
task than single-aspect control. Existing methods achieve complex multi-aspect
control by fusing multiple controllers learned from single-aspect, but suffer
from attribute degeneration caused by the mutual interference of these
controllers. To address this, we provide observations on attribute fusion from
a distributional perspective and propose to directly search for the
intersection areas of multiple attribute distributions as their combination for
generation. Our method first estimates the attribute space with an autoencoder
structure. Afterward, we iteratively approach the intersections by jointly
minimizing distances to points representing different attributes. Finally, we
map them to attribute-relevant sentences with a prefix-tuning-based decoder.
Experiments on the three-aspect control task, including sentiment, topic, and
detoxification aspects, reveal that our method outperforms several strong
baselines on attribute relevance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory support for the effectiveness
of our approach.",https://github.com/HappyGu0524/MultiControl,-1
bbcd76ea-75c2-491f-8c99-0cc6a214d661,Proper Reuse of Image Classification Features Improves Object Detection,0.391092,"A common practice in transfer learning is to initialize the downstream model
weights by pre-training on a data-abundant upstream task. In object detection
specifically, the feature backbone is typically initialized with Imagenet
classifier weights and fine-tuned on the object detection task. Recent works
show this is not strictly necessary under longer training regimes and provide
recipes for training the backbone from scratch. We investigate the opposite
direction of this end-to-end training trend: we show that an extreme form of
knowledge preservation -- freezing the classifier-initialized backbone --
consistently improves many different detection models, and leads to
considerable resource savings. We hypothesize and corroborate experimentally
that the remaining detector components capacity and structure is a crucial
factor in leveraging the frozen backbone. Immediate applications of our
findings include performance improvements on hard cases like detection of
long-tail object classes and computational and memory resource savings that
contribute to making the field more accessible to researchers with access to
fewer computational resources.",https://github.com/tensorflow/models/blob/master/official/projects/backbone_reuse/README.md,-1
8a44c4ee-8c74-4f75-acbf-dc75e9382121,Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection,0.44463,"Personalized response selection systems are generally grounded on persona.
However, there exists a co-relation between persona and empathy, which is not
explored well in these systems. Also, faithfulness to the conversation context
plunges when a contradictory or an off-topic response is selected. This paper
attempts to address these issues by proposing a suite of fusion strategies that
capture the interaction between persona, emotion, and entailment information of
the utterances. Ablation studies on the Persona-Chat dataset show that
incorporating emotion and entailment improves the accuracy of response
selection. We combine our fusion strategies and concept-flow encoding to train
a BERT-based model which outperforms the previous methods by margins larger
than 2.3 % on original personas and 1.9 % on revised personas in terms of
hits@1 (top-1 accuracy), achieving a new state-of-the-art performance on the
Persona-Chat dataset.",https://github.com/allenai/allennlp-models,-1
7ea20d2e-f92b-4728-9ba7-f7f5dec9d94a,Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words,0.902884,"Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,
QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in
which word similarities estimated by cosine over BERT embeddings are
understated and trace this effect to training data frequency. We find that
relative to human judgements, cosine similarity underestimates the similarity
of frequent words with other instances of the same word or other words across
contexts, even after controlling for polysemy and other factors. We conjecture
that this underestimation of similarity for high frequency words is due to
differences in the representational geometry of high and low frequency words
and provide a formal argument for the two-dimensional case.",https://github.com/katezhou/cosine_and_frequency,-1
9a0b02de-56c2-4daf-af7b-775a8de3ff92,Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation,0.310956,"Large Language Models (LLMs) have in recent years demonstrated impressive
prowess in natural language generation. A common practice to improve generation
diversity is to sample multiple outputs from the model. However, there lacks a
simple and robust way of selecting the best output from these stochastic
samples. As a case study framed in the context of question generation, we
propose two prompt-based approaches to selecting high-quality questions from a
set of LLM-generated candidates. Our method works under the constraints of 1) a
black-box (non-modifiable) question generation model and 2) lack of access to
human-annotated references -- both of which are realistic limitations for
real-world deployment of LLMs. With automatic as well as human evaluations, we
empirically demonstrate that our approach can effectively select questions of
higher qualities than greedy generation.",https://github.com/kingoflolz/mesh-transformer-jax,-1
58a83dbf-912c-4c2f-9c71-774fe3684777,Deep neural networks for fine-grained surveillance of overdose mortality,0.42305,"Surveillance of drug overdose deaths relies on death certificates for
identification of the substances that caused death. Drugs and drug classes can
be identified through the International Classification of Diseases, 10th
Revision (ICD-10) codes present on death certificates. However, ICD-10 codes do
not always provide high levels of specificity in drug identification. To
achieve more fine-grained identification of substances on a death certificate,
the free-text cause of death section, completed by the medical certifier, must
be analyzed. Current methods for analyzing free-text death certificates rely
solely on look-up tables for identifying specific substances, which must be
frequently updated and maintained. To improve identification of drugs on death
certificates, a deep learning named-entity recognition model was developed,
which achieved an F1-score of 99.13%. This model can identify new drug
misspellings and novel substances that are not present on current surveillance
look-up tables, enhancing the surveillance of drug overdose deaths.",https://github.com/pjward5656/DC_flair,-1
eb7f806e-8168-4901-869e-8c606912a686,Disentangled Latent Transformer for Interpretable Monocular Height Estimation,0.450925,"Monocular height estimation (MHE) from remote sensing imagery has high
potential in generating 3D city models efficiently for a quick response to
natural disasters. Most existing works pursue higher performance. However,
there is little research exploring the interpretability of MHE networks. In
this paper, we target at exploring how deep neural networks predict height from
a single monocular image. Towards a comprehensive understanding of MHE
networks, we propose to interpret them from multiple levels: 1) Neurons:
unit-level dissection. Exploring the semantic and height selectivity of the
learned internal deep representations; 2) Instances: object-level
interpretation. Studying the effects of different semantic classes, scales, and
spatial contexts on height estimation; 3) Attribution: pixel-level analysis.
Understanding which input pixels are important for the height estimation. Based
on the multi-level interpretation, a disentangled latent Transformer network is
proposed towards a more compact, reliable, and explainable deep model for
monocular height estimation. Furthermore, a novel unsupervised semantic
segmentation task based on height estimation is first introduced in this work.
Additionally, we also construct a new dataset for joint semantic segmentation
and height estimation. Our work provides novel insights for both understanding
and designing MHE models.",https://github.com/ShadowXZT/DLT-Height-Estimation.pytorch,-1
1f3943b1-a1f2-48ed-a8dd-66034000d1a9,Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field,0.625551,"Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases
that correctly describe the categories of a given entity mention in a sentence.
Most recent works infer each entity type independently, ignoring the
correlations between types, e.g., when an entity is inferred as a president, it
should also be a politician and a leader. To this end, we use an undirected
graphical model called pairwise conditional random field (PCRF) to formulate
the UFET problem, in which the type variables are not only unarily influenced
by the input but also pairwisely relate to all the other type variables. We use
various modern backbones for entity typing to compute unary potentials, and
derive pairwise potentials from type phrase representations that both capture
prior semantic information and facilitate accelerated inference. We use
mean-field variational inference for efficient type inference on very large
type sets and unfold it as a neural network module to enable end-to-end
training. Experiments on UFET show that the Neural-PCRF consistently
outperforms its backbones with little cost and results in a competitive
performance against cross-encoder based SOTA while being thousands of times
faster. We also find Neural- PCRF effective on a widely used fine-grained
entity typing dataset with a smaller type set. We pack Neural-PCRF as a network
module that can be plugged onto multi-label type classifiers with ease and
release it in https://github.com/modelscope/adaseq/tree/master/examples/NPCRF.",https://github.com/modelscope/adaseq/tree/master/examples/NPCRF,-1
9be62ad6-72b8-4f90-b1ce-e5171258f7c6,Robust Contrastive Learning against Noisy Views,0.566712,"Contrastive learning relies on an assumption that positive pairs contain
related views, e.g., patches of an image or co-occurring multimodal signals of
a video, that share certain underlying information about an instance. But what
if this assumption is violated? The literature suggests that contrastive
learning produces suboptimal representations in the presence of noisy views,
e.g., false positive pairs with no apparent shared information. In this work,
we propose a new contrastive loss function that is robust against noisy views.
We provide rigorous theoretical justifications by showing connections to robust
symmetric losses for noisy binary classification and by establishing a new
contrastive bound for mutual information maximization based on the Wasserstein
distance measure. The proposed loss is completely modality-agnostic and a
simple drop-in replacement for the InfoNCE loss, which makes it easy to apply
to existing contrastive frameworks. We show that our approach provides
consistent improvements over the state-of-the-art on image, video, and graph
contrastive learning benchmarks that exhibit a variety of real-world noise
patterns.",https://github.com/chingyaoc/RINCE,-1
91016dbd-841f-4c59-bef3-8722ded91e26,Trustworthy Autonomous Systems (TAS): Engaging TAS experts in curriculum design,0.624536,"Recent advances in artificial intelligence, specifically machine learning,
contributed positively to enhancing the autonomous systems industry, along with
introducing social, technical, legal and ethical challenges to make them
trustworthy. Although Trustworthy Autonomous Systems (TAS) is an established
and growing research direction that has been discussed in multiple disciplines,
e.g., Artificial Intelligence, Human-Computer Interaction, Law, and Psychology.
The impact of TAS on education curricula and required skills for future TAS
engineers has rarely been discussed in the literature. This study brings
together the collective insights from a number of TAS leading experts to
highlight significant challenges for curriculum design and potential TAS
required skills posed by the rapid emergence of TAS. Our analysis is of
interest not only to the TAS education community but also to other researchers,
as it offers ways to guide future research toward operationalising TAS
education.",None,-1
9b9187d7-78a9-41c4-bca3-f1a4a242f425,The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding,0.42494,"In a real-world dialogue system, generated text must be truthful and
informative while remaining fluent and adhering to a prescribed style.
Satisfying these constraints simultaneously is difficult for the two
predominant paradigms in language generation: neural language modeling and
rule-based generation. We describe a hybrid architecture for dialogue response
generation that combines the strengths of both paradigms. The first component
of this architecture is a rule-based content selection model defined using a
new formal framework called dataflow transduction, which uses declarative rules
to transduce a dialogue agent's actions and their results (represented as
dataflow graphs) into context-free grammars representing the space of
contextually acceptable responses. The second component is a constrained
decoding procedure that uses these grammars to constrain the output of a neural
language model, which selects fluent utterances. Our experiments show that this
system outperforms both rule-based and learned approaches in human evaluations
of fluency, relevance, and truthfulness.",https://github.com/microsoft/dataflow2text,-1
6f3dd30f-dec8-476b-ba89-dacaa8c3f582,FrOoDo: Framework for Out-of-Distribution Detection,0.0913083,"FrOoDo is an easy-to-use and flexible framework for Out-of-Distribution
detection tasks in digital pathology. It can be used with PyTorch
classification and segmentation models, and its modular design allows for easy
extension. The goal is to automate the task of OoD Evaluation such that
research can focus on the main goal of either designing new models, new methods
or evaluating a new dataset. The code can be found at
https://github.com/MECLabTUDA/FrOoDo.",None,-1
63331e21-886b-4069-b856-8f58a7dcf472,Deep Learning in Business Analytics: A Clash of Expectations and Reality,0.976897,"Our fast-paced digital economy shaped by global competition requires
increased data-driven decision-making based on artificial intelligence (AI) and
machine learning (ML). The benefits of deep learning (DL) are manifold, but it
comes with limitations that have - so far - interfered with widespread industry
adoption. This paper explains why DL - despite its popularity - has
difficulties speeding up its adoption within business analytics. It is shown -
by a mixture of content analysis and empirical study - that the adoption of
deep learning is not only affected by computational complexity, lacking big
data architecture, lack of transparency (black-box), and skill shortage, but
also by the fact that DL does not outperform traditional ML models in the case
of structured datasets with fixed-length feature vectors. Deep learning should
be regarded as a powerful addition to the existing body of ML models instead of
a one size fits all solution.",None,-1
a01bb471-a050-444b-b3cc-6ddc2c38c86b,SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition,0.722004,"Few-shot Named Entity Recognition (NER) aims to identify named entities with
very little annotated data. Previous methods solve this problem based on
token-wise classification, which ignores the information of entity boundaries,
and inevitably the performance is affected by the massive non-entity tokens. To
this end, we propose a seminal span-based prototypical network (SpanProto) that
tackles few-shot NER via a two-stage approach, including span extraction and
mention classification. In the span extraction stage, we transform the
sequential tags into a global boundary matrix, enabling the model to focus on
the explicit boundary information. For mention classification, we leverage
prototypical learning to capture the semantic representations for each labeled
span and make the model better adapt to novel-class entities. To further
improve the model performance, we split out the false positives generated by
the span extractor but not labeled in the current episode set, and then present
a margin-based loss to separate them from each prototype region. Experiments
over multiple benchmarks demonstrate that our model outperforms strong
baselines by a large margin.",https://github.com/alibaba/EasyNLP,-1
4d194239-ba3f-473e-b64d-ff010572a74a,Meet-in-the-middle: Multi-scale upsampling and matching for cross-resolution face recognition,0.318799,"In this paper, we aim to address the large domain gap between high-resolution
face images, e.g., from professional portrait photography, and low-quality
surveillance images, e.g., from security cameras. Establishing an identity
match between disparate sources like this is a classical surveillance face
identification scenario, which continues to be a challenging problem for modern
face recognition techniques. To that end, we propose a method that combines
face super-resolution, resolution matching, and multi-scale template
accumulation to reliably recognize faces from long-range surveillance footage,
including from low quality sources. The proposed approach does not require
training or fine-tuning on the target dataset of real surveillance images.
Extensive experiments show that our proposed method is able to outperform even
existing methods fine-tuned to the SCFace dataset.",None,-1
1e35ddc0-b1a0-4ccd-8c14-424b38591b34,InFIP: An Explainable DNN Intellectual Property Protection Method based on Intrinsic Features,0.461726,"Intellectual property (IP) protection for Deep Neural Networks (DNNs) has
raised serious concerns in recent years. Most existing works embed watermarks
in the DNN model for IP protection, which need to modify the model and lack of
interpretability. In this paper, for the first time, we propose an
interpretable intellectual property protection method for DNN based on
explainable artificial intelligence. Compared with existing works, the proposed
method does not modify the DNN model, and the decision of the ownership
verification is interpretable. We extract the intrinsic features of the DNN
model by using Deep Taylor Decomposition. Since the intrinsic feature is
composed of unique interpretation of the model's decision, the intrinsic
feature can be regarded as fingerprint of the model. If the fingerprint of a
suspected model is the same as the original model, the suspected model is
considered as a pirated model. Experimental results demonstrate that the
fingerprints can be successfully used to verify the ownership of the model and
the test accuracy of the model is not affected. Furthermore, the proposed
method is robust to fine-tuning attack, pruning attack, watermark overwriting
attack, and adaptive attack.",None,11673
714f8f51-3b62-4ac3-91f7-7b84b9c0b484,Towards Federated Long-Tailed Learning,0.466454,"Data privacy and class imbalance are the norm rather than the exception in
many machine learning tasks. Recent attempts have been launched to, on one
side, address the problem of learning from pervasive private data, and on the
other side, learn from long-tailed data. However, both assumptions might hold
in practical applications, while an effective method to simultaneously
alleviate both issues is yet under development. In this paper, we focus on
learning with long-tailed (LT) data distributions under the context of the
popular privacy-preserved federated learning (FL) framework. We characterize
three scenarios with different local or global long-tailed data distributions
in the FL framework, and highlight the corresponding challenges. The
preliminary results under different scenarios reveal that substantial future
work are of high necessity to better resolve the characterized federated
long-tailed learning tasks.",None,-1
34b708d4-083c-4156-b7d4-462d87e19b43,Are Vision Transformers Robust to Spurious Correlations?,0.460456,"Deep neural networks may be susceptible to learning spurious correlations
that hold on average but not in atypical test samples. As with the recent
emergence of vision transformer (ViT) models, it remains underexplored how
spurious correlations are manifested in such architectures. In this paper, we
systematically investigate the robustness of vision transformers to spurious
correlations on three challenging benchmark datasets and compare their
performance with popular CNNs. Our study reveals that when pre-trained on a
sufficiently large dataset, ViT models are more robust to spurious correlations
than CNNs. Key to their success is the ability to generalize better from the
examples where spurious correlations do not hold. Further, we perform extensive
ablations and experiments to understand the role of the self-attention
mechanism in providing robustness under spuriously correlated environments. We
hope that our work will inspire future research on further understanding the
robustness of ViT models.",https://github.com/deeplearning-wisc/vit-spurious-robustness,-1
47e52a54-02c1-48af-93dd-bb05be0a3bdd,Rethinking and Refining the Distinct Metric,0.864665,"Distinct-$n$ score\cite{Li2016} is a widely used automatic metric for
evaluating diversity in language generation tasks. However, we observed that
the original approach for calculating distinct scores has evident biases that
tend to assign higher penalties to longer sequences. We refine the calculation
of distinct scores by scaling the number of distinct tokens based on their
expectations. We provide both empirical and theoretical evidence to show that
our method effectively removes the biases existing in the original distinct
score. Our experiments show that our proposed metric,
\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human
judgment in evaluating response diversity. To foster future research, we
provide an example implementation at
\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.",https://github.com/lsy641/Expectation-Adjusted-Distinct,-1
91a3a04a-ae6c-429a-8187-720ccc99bb1a,Unified Speech-Text Pre-training for Speech Translation and Recognition,0.867796,"We describe a method to jointly pre-train speech and text in an
encoder-decoder modeling framework for speech translation and recognition. The
proposed method incorporates four self-supervised and supervised subtasks for
cross modality learning. A self-supervised speech subtask leverages unlabelled
speech data, and a (self-)supervised text to text subtask makes use of abundant
text training data. Two auxiliary supervised speech tasks are included to unify
speech and text modeling space. Our contribution lies in integrating linguistic
information from the text corpus into the speech pre-training. Detailed
analysis reveals learning interference among subtasks. Two pre-training
configurations for speech translation and recognition, respectively, are
presented to alleviate subtask interference. Our experiments show the proposed
method can effectively fuse speech and text information into one model. It
achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the
MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the
Librispeech speech recognition task.",https://github.com/pytorch/fairseq/tree/main/examples/speech_text_joint_to_text,-1
ca9055af-150e-47f3-8279-98dfb99c3c97,Hierarchical Collaborative Hyper-parameter Tuning,0.0636376,"Hyper-parameter Tuning is among the most critical stages in building machine
learning solutions. This paper demonstrates how multi-agent systems can be
utilized to develop a distributed technique for determining near-optimal values
for any arbitrary set of hyper-parameters in a machine learning model. The
proposed method employs a distributedly formed hierarchical agent-based
architecture for the cooperative searching procedure of tuning hyper-parameter
values. The presented generic model is used to develop a guided randomized
agent-based tuning technique, and its behavior is investigated in both machine
learning and global function optimization applications. According the empirical
results, the proposed model outperformed both of its underlying randomized
tuning strategies in terms of classification error and function evaluations,
notably in higher number of dimensions.",None,-1
5d12e717-c269-4205-8758-1eb705a7827a,Surgical Fine-Tuning Improves Adaptation to Distribution Shifts,0.906729,"A common approach to transfer learning under distribution shift is to
fine-tune the last few layers of a pre-trained model, preserving learned
features while also adapting to the new task. This paper shows that in such
settings, selectively fine-tuning a subset of layers (which we term surgical
fine-tuning) matches or outperforms commonly used fine-tuning approaches.
Moreover, the type of distribution shift influences which subset is more
effective to tune: for example, for image corruptions, fine-tuning only the
first few layers works best. We validate our findings systematically across
seven real-world data tasks spanning three types of distribution shifts.
Theoretically, we prove that for two-layer neural networks in an idealized
setting, first-layer tuning can outperform fine-tuning all layers. Intuitively,
fine-tuning more parameters on a small target dataset can cause information
learned during pre-training to be forgotten, and the relevant information
depends on the type of shift.",None,-1
8016080a-7ef1-43ce-b0de-b593ad9954fa,Unified Semantic Typing with Meaningful Label Inference,0.864046,"Semantic typing aims at classifying tokens or spans of interest in a textual
context into semantic categories such as relations, entity types, and event
types. The inferred labels of semantic categories meaningfully interpret how
machines understand components of text. In this paper, we present UniST, a
unified framework for semantic typing that captures label semantics by
projecting both inputs and labels into a joint semantic embedding space. To
formulate different lexical and relational semantic typing tasks as a unified
task, we incorporate task descriptions to be jointly encoded with the input,
allowing UniST to be adapted to different tasks without introducing
task-specific model components. UniST optimizes a margin ranking loss such that
the semantic relatedness of the input and labels is reflected from their
embedding similarity. Our experiments demonstrate that UniST achieves strong
performance across three semantic typing tasks: entity typing, relation
classification and event typing. Meanwhile, UniST effectively transfers
semantic knowledge of labels and substantially improves generalizability on
inferring rarely seen and unseen types. In addition, multiple semantic typing
tasks can be jointly trained within the unified framework, leading to a single
compact multi-tasking model that performs comparably to dedicated single-task
models, while offering even better transferability.",https://github.com/luka-group/UniST,-1
2bc7e5d6-ca2d-452b-812b-8dfa35f33326,Multiscale Analysis for Improving Texture Classification,0.562578,"Information from an image occurs over multiple and distinct spatial scales.
Image pyramid multiresolution representations are a useful data structure for
image analysis and manipulation over a spectrum of spatial scales. This paper
employs the Gaussian-Laplacian pyramid to treat different spatial frequency
bands of a texture separately. First, we generate three images corresponding to
three levels of the Gaussian-Laplacian pyramid for an input image to capture
intrinsic details. Then we aggregate features extracted from gray and color
texture images using bio-inspired texture descriptors, information-theoretic
measures, gray-level co-occurrence matrix features, and Haralick statistical
features into a single feature vector. Such an aggregation aims at producing
features that characterize textures to their maximum extent, unlike employing
each descriptor separately, which may lose some relevant textural information
and reduce the classification performance. The experimental results on texture
and histopathologic image datasets have shown the advantages of the proposed
method compared to state-of-the-art approaches. Such findings emphasize the
importance of multiscale image analysis and corroborate that the descriptors
mentioned above are complementary.",None,-1
36b0d1b1-bdac-469e-849e-ac416483fd21,Egocentric Prediction of Action Target in 3D,0.54661,"We are interested in anticipating as early as possible the target location of
a person's object manipulation action in a 3D workspace from egocentric vision.
It is important in fields like human-robot collaboration, but has not yet
received enough attention from vision and learning communities. To stimulate
more research on this challenging egocentric vision task, we propose a large
multimodality dataset of more than 1 million frames of RGB-D and IMU streams,
and provide evaluation metrics based on our high-quality 2D and 3D labels from
semi-automatic annotation. Meanwhile, we design baseline methods using
recurrent neural networks and conduct various ablation studies to validate
their effectiveness. Our results demonstrate that this new task is worthy of
further study by researchers in robotics, vision, and learning communities.",None,30070
2029f0a8-ffd8-4c76-984d-ac2117633857,Lempel-Ziv Networks,0.369687,"Sequence processing has long been a central area of machine learning
research. Recurrent neural nets have been successful in processing sequences
for a number of tasks; however, they are known to be both ineffective and
computationally expensive when applied to very long sequences.
Compression-based methods have demonstrated more robustness when processing
such sequences -- in particular, an approach pairing the Lempel-Ziv Jaccard
Distance (LZJD) with the k-Nearest Neighbor algorithm has shown promise on long
sequence problems (up to $T=200,000,000$ steps) involving malware
classification. Unfortunately, use of LZJD is limited to discrete domains. To
extend the benefits of LZJD to a continuous domain, we investigate the
effectiveness of a deep-learning analog of the algorithm, the Lempel-Ziv
Network. While we achieve successful proof of concept, we are unable to improve
meaningfully on the performance of a standard LSTM across a variety of datasets
and sequence processing tasks. In addition to presenting this negative result,
our work highlights the problem of sub-par baseline tuning in newer research
areas.",None,10329
9bf13d1d-3fcd-45a1-a794-d537b3065dab,CXTrack: Improving 3D Point Cloud Tracking with Contextual Information,0.532579,"3D single object tracking plays an essential role in many applications, such
as autonomous driving. It remains a challenging problem due to the large
appearance variation and the sparsity of points caused by occlusion and limited
sensor capabilities. Therefore, contextual information across two consecutive
frames is crucial for effective object tracking. However, points containing
such useful information are often overlooked and cropped out in existing
methods, leading to insufficient use of important contextual knowledge. To
address this issue, we propose CXTrack, a novel transformer-based network for
3D object tracking, which exploits ConteXtual information to improve the
tracking results. Specifically, we design a target-centric transformer network
that directly takes point features from two consecutive frames and the previous
bounding box as input to explore contextual information and implicitly
propagate target cues. To achieve accurate localization for objects of all
sizes, we propose a transformer-based localization head with a novel center
embedding module to distinguish the target from distractors. Extensive
experiments on three large-scale datasets, KITTI, nuScenes and Waymo Open
Dataset, show that CXTrack achieves state-of-the-art tracking performance while
running at 34 FPS.",None,-1
73c2f340-73df-4110-8fe7-8de2b83ece83,Human Activity Recognition from Wi-Fi CSI Data Using Principal Component-Based Wavelet CNN,0.833996,"Human Activity Recognition (HAR) is an emerging technology with several
applications in surveillance, security, and healthcare sectors. Noninvasive HAR
systems based on Wi-Fi Channel State Information (CSI) signals can be developed
leveraging the quick growth of ubiquitous Wi-Fi technologies, and the
correlation between CSI dynamics and body motions. In this paper, we propose
Principal Component-based Wavelet Convolutional Neural Network (or PCWCNN) -- a
novel approach that offers robustness and efficiency for practical real-time
applications. Our proposed method incorporates two efficient preprocessing
algorithms -- the Principal Component Analysis (PCA) and the Discrete Wavelet
Transform (DWT). We employ an adaptive activity segmentation algorithm that is
accurate and computationally light. Additionally, we used the Wavelet CNN for
classification, which is a deep convolutional network analogous to the
well-studied ResNet and DenseNet networks. We empirically show that our
proposed PCWCNN model performs very well on a real dataset, outperforming
existing approaches.",None,-1
52c7a82b-c293-485e-9baf-9e252d194c52,Chasing Streams with Existential Rules,0.0473641,"We study reasoning with existential rules to perform query answering over
streams of data. On static databases, this problem has been widely studied, but
its extension to rapidly changing data has not yet been considered. To bridge
this gap, we extend LARS, a well-known framework for rule-based stream
reasoning, to support existential rules. For that, we show how to translate
LARS with existentials into a semantics-preserving set of existential rules. As
query answering with such rules is undecidable in general, we describe how to
leverage the temporal nature of streams and present suitable notions of
acyclicity that ensure decidability.",https://github.com/karmaresearch/elars,-1
7d1d5f1e-14d5-4d9f-9e7e-cfe42911e94f,The Quest for a Common Model of the Intelligent Decision Maker,0.585173,"The premise of the Multi-disciplinary Conference on Reinforcement Learning
and Decision Making is that multiple disciplines share an interest in
goal-directed decision making over time. The idea of this paper is to sharpen
and deepen this premise by proposing a perspective on the decision maker that
is substantive and widely held across psychology, artificial intelligence,
economics, control theory, and neuroscience, which I call the ""common model of
the intelligent agent"". The common model does not include anything specific to
any organism, world, or application domain. The common model does include
aspects of the decision maker's interaction with its world (there must be input
and output, and a goal) and internal components of the decision maker (for
perception, decision-making, internal evaluation, and a world model). I
identify these aspects and components, note that they are given different names
in different disciplines but refer essentially to the same ideas, and discuss
the challenges and benefits of devising a neutral terminology that can be used
across disciplines. It is time to recognize and build on the convergence of
multiple diverse disciplines on a substantive common model of the intelligent
agent.",None,-1
11c42af2-1ea3-4c28-821e-f62f7740ce15,Rethinking Performance Gains in Image Dehazing Networks,0.863311,"Image dehazing is an active topic in low-level vision, and many image
dehazing networks have been proposed with the rapid development of deep
learning. Although these networks' pipelines work fine, the key mechanism to
improving image dehazing performance remains unclear. For this reason, we do
not target to propose a dehazing network with fancy modules; rather, we make
minimal modifications to popular U-Net to obtain a compact dehazing network.
Specifically, we swap out the convolutional blocks in U-Net for residual blocks
with the gating mechanism, fuse the feature maps of main paths and skip
connections using the selective kernel, and call the resulting U-Net variant
gUNet. As a result, with a significantly reduced overhead, gUNet is superior to
state-of-the-art methods on multiple image dehazing datasets. Finally, we
verify these key designs to the performance gain of image dehazing networks
through extensive ablation studies.",https://github.com/IDKiro/gUNet,6873
5a9e06f5-279d-48d1-94ec-39395113051d,LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation,0.606678,"In this study, we propose a simple and effective preprocessing method for
subword segmentation based on a data compression algorithm. Compression-based
subword segmentation has recently attracted significant attention as a
preprocessing method for training data in Neural Machine Translation. Among
them, BPE/BPE-dropout is one of the fastest and most effective method compared
to conventional approaches. However, compression-based approach has a drawback
in that generating multiple segmentations is difficult due to the determinism.
To overcome this difficulty, we focus on a probabilistic string algorithm,
called locally-consistent parsing (LCP), that has been applied to achieve
optimum compression. Employing the probabilistic mechanism of LCP, we propose
LCP-dropout for multiple subword segmentation that improves BPE/BPE-dropout,
and show that it outperforms various baselines in learning from especially
small training data.",https://github.com/moses-smt/mosesdecoder,-1
5c255d55-3cd4-4638-a951-2159d9a64ee9,CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery,0.993599,"We introduce Contrastive Intrinsic Control (CIC), an algorithm for
unsupervised skill discovery that maximizes the mutual information between
state-transitions and latent skill vectors. CIC utilizes contrastive learning
between state-transitions and skills to learn behavior embeddings and maximizes
the entropy of these embeddings as an intrinsic reward to encourage behavioral
diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning
Benchmark, which consists of a long reward-free pre-training phase followed by
a short adaptation phase to downstream tasks with extrinsic rewards. CIC
substantially improves over prior methods in terms of adaptation efficiency,
outperforming prior unsupervised skill discovery methods by 1.79x and the next
leading overall exploration algorithm by 1.18x.",None,-1
1ab33d5c-ce9c-4767-a48d-81511c21a4b4,Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation,0.67135,"After the great success of Vision Transformer variants (ViTs) in computer
vision, it has also demonstrated great potential in domain adaptive semantic
segmentation. Unfortunately, straightforwardly applying local ViTs in domain
adaptive semantic segmentation does not bring in expected improvement. We find
that the pitfall of local ViTs is due to the severe high-frequency components
generated during both the pseudo-label construction and features alignment for
target domains. These high-frequency components make the training of local ViTs
very unsmooth and hurt their transferability. In this paper, we introduce a
low-pass filtering mechanism, momentum network, to smooth the learning dynamics
of target domain features and pseudo labels. Furthermore, we propose a dynamic
of discrepancy measurement to align the distributions in the source and target
domains via dynamic weights to evaluate the importance of the samples. After
tackling the above issues, extensive experiments on sim2real benchmarks show
that the proposed method outperforms the state-of-the-art methods. Our codes
are available at https://github.com/alpc91/TransDA",https://github.com/alpc91/TransDA,24953
74cdc112-1d34-4400-970d-fdd01e3c1a8f,Font Shape-to-Impression Translation,0.42676,"Different fonts have different impressions, such as elegant, scary, and cool.
This paper tackles part-based shape-impression analysis based on the
Transformer architecture, which is able to handle the correlation among local
parts by its self-attention mechanism. This ability will reveal how
combinations of local parts realize a specific impression of a font. The
versatility of Transformer allows us to realize two very different approaches
for the analysis, i.e., multi-label classification and translation. A
quantitative evaluation shows that our Transformer-based approaches estimate
the font impressions from a set of local parts more accurately than other
approaches. A qualitative evaluation then indicates the important local parts
for a specific impression.",None,-1
a361401f-bdb2-49d0-bf1f-3cde4a337905,Understanding Adversarial Robustness of Vision Transformers via Cauchy Problem,0.372323,"Recent research on the robustness of deep learning has shown that Vision
Transformers (ViTs) surpass the Convolutional Neural Networks (CNNs) under some
perturbations, e.g., natural corruption, adversarial attacks, etc. Some papers
argue that the superior robustness of ViT comes from the segmentation of its
input images; others say that the Multi-head Self-Attention (MSA) is the key to
preserving the robustness. In this paper, we aim to introduce a principled and
unified theoretical framework to investigate such an argument on ViT's
robustness. We first theoretically prove that, unlike Transformers in Natural
Language Processing, ViTs are Lipschitz continuous. Then we theoretically
analyze the adversarial robustness of ViTs from the perspective of the Cauchy
Problem, via which we can quantify how the robustness propagates through
layers. We demonstrate that the first and last layers are the critical factors
to affect the robustness of ViTs. Furthermore, based on our theory, we
empirically show that unlike the claims from existing research, MSA only
contributes to the adversarial robustness of ViTs under weak adversarial
attacks, e.g., FGSM, and surprisingly, MSA actually comprises the model's
adversarial robustness under stronger attacks, e.g., PGD attacks.",None,-1
41cd846f-bea9-4538-a8ff-bed3dd9cbf1b,Improving the Adversarial Robustness of NLP Models by Information Bottleneck,0.679301,"Existing studies have demonstrated that adversarial examples can be directly
attributed to the presence of non-robust features, which are highly predictive,
but can be easily manipulated by adversaries to fool NLP models. In this study,
we explore the feasibility of capturing task-specific robust features, while
eliminating the non-robust ones by using the information bottleneck theory.
Through extensive experiments, we show that the models trained with our
information bottleneck-based method are able to achieve a significant
improvement in robust accuracy, exceeding performances of all the previously
reported defense methods while suffering almost no performance drop in clean
accuracy on SST-2, AGNEWS and IMDB datasets.",https://github.com/zhangcen456/IB,-1
aab6914e-1140-4083-9064-d6ec3d187ad0,Super forecasting the technological singularity risks from artificial intelligence,0.631679,"The article forecasts emerging cyber-risks from the integration of AI in
cybersecurity.",None,-1
2b630827-c68b-4a33-801a-67330c500b37,Conformance Checking with Uncertainty via SMT (Extended Version),0.513689,"Logs of real-life processes often feature uncertainty pertaining the recorded
timestamps, data values, and/or events. We consider the problem of checking
conformance of uncertain logs against data-aware reference processes.
Specifically, we show how to solve it via SMT encodings, lifting previous work
on data-aware SMT-based conformance checking to this more sophisticated
setting. Our approach is modular, in that it homogeneously accommodates for
different types of uncertainty. Moreover, using appropriate cost functions,
different conformance checking tasks can be addressed. We show the correctness
of our approach and witness feasibility through a proof-of-concept
implementation.",https://github.com/bytekid/cocomot,8569
56cc1f05-10ff-4d14-9417-8914e27121ff,Multi-Dimensional Model Compression of Vision Transformer,0.33667,"Vision transformers (ViT) have recently attracted considerable attentions,
but the huge computational cost remains an issue for practical deployment.
Previous ViT pruning methods tend to prune the model along one dimension
solely, which may suffer from excessive reduction and lead to sub-optimal model
quality. In contrast, we advocate a multi-dimensional ViT compression paradigm,
and propose to harness the redundancy reduction from attention head, neuron and
sequence dimensions jointly. We firstly propose a statistical dependence based
pruning criterion that is generalizable to different dimensions for identifying
deleterious components. Moreover, we cast the multi-dimensional compression as
an optimization, learning the optimal pruning policy across the three
dimensions that maximizes the compressed model's accuracy under a computational
budget. The problem is solved by our adapted Gaussian process search with
expected improvement. Experimental results show that our method effectively
reduces the computational cost of various ViT models. For example, our method
reduces 40\% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models,
outperforming previous state-of-the-arts.",None,-1
345da955-b6fb-4af2-8d63-de6b3ad10005,Bingham Policy Parameterization for 3D Rotations in Reinforcement Learning,0.108674,"We propose a new policy parameterization for representing 3D rotations during
reinforcement learning. Today in the continuous control reinforcement learning
literature, many stochastic policy parameterizations are Gaussian. We argue
that universally applying a Gaussian policy parameterization is not always
desirable for all environments. One such case in particular where this is true
are tasks that involve predicting a 3D rotation output, either in isolation, or
coupled with translation as part of a full 6D pose output. Our proposed Bingham
Policy Parameterization (BPP) models the Bingham distribution and allows for
better rotation (quaternion) prediction over a Gaussian policy parameterization
in a range of reinforcement learning tasks. We evaluate BPP on the rotation
Wahba problem task, as well as a set of vision-based next-best pose robot
manipulation tasks from RLBench. We hope that this paper encourages more
research into developing other policy parameterization that are more suited for
particular environments, rather than always assuming Gaussian.",https://github.com/DLR-RM/stable-baselines3,-1
ec243ca2-86e5-4ec3-81cd-aa8d728515e0,Accelerating Shapley Explanation via Contributive Cooperator Selection,0.619467,"Even though Shapley value provides an effective explanation for a DNN model
prediction, the computation relies on the enumeration of all possible input
feature coalitions, which leads to the exponentially growing complexity. To
address this problem, we propose a novel method SHEAR to significantly
accelerate the Shapley explanation for DNN models, where only a few coalitions
of input features are involved in the computation. The selection of the feature
coalitions follows our proposed Shapley chain rule to minimize the absolute
error from the ground-truth Shapley values, such that the computation can be
both efficient and accurate. To demonstrate the effectiveness, we
comprehensively evaluate SHEAR across multiple metrics including the absolute
error from the ground-truth Shapley value, the faithfulness of the
explanations, and running speed. The experimental results indicate SHEAR
consistently outperforms state-of-the-art baseline methods across different
evaluation metrics, which demonstrates its potentials in real-world
applications where the computational resource is limited.",https://github.com/guanchuwang/SHEAR,-1
7bdb9a0e-397b-4dc7-a6ec-005d56635413,Concordance based Survival Cobra with regression type weak learners,0.351477,"In this paper, we predict conditional survival functions through a combined
regression strategy. We take weak learners as different random survival trees.
We propose to maximize concordance in the right-censored set up to find the
optimal parameters. We explore two approaches, a usual survival cobra and a
novel weighted predictor based on the concordance index. Our proposed
formulations use two different norms, say, Max-norm and Frobenius norm, to find
a proximity set of predictions from query points in the test dataset. We
illustrate our algorithms through three different real-life dataset
implementations.",None,-1
53700508-a61e-43a9-9e9a-13388410176b,"Re2G: Retrieve, Rerank, Generate",0.85879,"As demonstrated by GPT-3 and T5, transformers grow in capability as parameter
spaces become larger and larger. However, for tasks that require a large amount
of knowledge, non-parametric memory allows models to grow dramatically with a
sub-linear increase in computational cost and GPU memory requirements. Recent
models such as RAG and REALM have introduced retrieval into conditional
generation. These models incorporate neural initial retrieval from a corpus of
passages. We build on this line of research, proposing Re2G, which combines
both neural initial retrieval and reranking into a BART-based
sequence-to-sequence generation. Our reranking approach also permits merging
retrieval results from sources with incomparable scores, enabling an ensemble
of BM25 and neural initial retrieval. To train our system end-to-end, we
introduce a novel variation of knowledge distillation to train the initial
retrieval, reranker, and generation using only ground truth on the target
sequence output. We find large gains in four diverse tasks: zero-shot slot
filling, question answering, fact-checking, and dialog, with relative gains of
9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make
our code available as open source at
https://github.com/IBM/kgi-slot-filling/tree/re2g.",https://github.com/IBM/kgi-slot-filling,-1
2607cd80-2cb9-48d6-bb7e-ad62c952a297,Block-coordinate Frank-Wolfe algorithm and convergence analysis for semi-relaxed optimal transport problem,0.329122,"The optimal transport (OT) problem has been used widely for machine learning.
It is necessary for computation of an OT problem to solve linear programming
with tight mass-conservation constraints. These constraints prevent its
application to large-scale problems. To address this issue, loosening such
constraints enables us to propose the relaxed-OT method using a faster
algorithm. This approach has demonstrated its effectiveness for applications.
However, it remains slow. As a superior alternative, we propose a fast
block-coordinate Frank-Wolfe (BCFW) algorithm for a convex semi-relaxed OT.
Specifically, we prove their upper bounds of the worst convergence iterations,
and equivalence between the linearization duality gap and the Lagrangian
duality gap. Additionally, we develop two fast variants of the proposed BCFW.
Numerical experiments have demonstrated that our proposed algorithms are
effective for color transfer and surpass state-of-the-art algorithms. This
report presents a short version of arXiv:2103.05857.",https://github.com/hiroyuki-kasai/srot,-1
2e35c5b6-a44f-4482-ad4f-8b823b2e1bb3,SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers,0.446176,"This paper aims to improve the performance of text-to-SQL parsing by
exploring the intrinsic uncertainties in the neural network based approaches
(called SUN). From the data uncertainty perspective, it is indisputable that a
single SQL can be learned from multiple semantically-equivalent
questions.Different from previous methods that are limited to one-to-one
mapping, we propose a data uncertainty constraint to explore the underlying
complementary semantic information among multiple semantically-equivalent
questions (many-to-one) and learn the robust feature representations with
reduced spurious associations. In this way, we can reduce the sensitivity of
the learned representations and improve the robustness of the parser. From the
model uncertainty perspective, there is often structural information
(dependence) among the weights of neural networks. To improve the
generalizability and stability of neural text-to-SQL parsers, we propose a
model uncertainty constraint to refine the query representations by enforcing
the output representations of different perturbed encoding networks to be
consistent with each other. Extensive experiments on five benchmark datasets
demonstrate that our method significantly outperforms strong competitors and
achieves new state-of-the-art results. For reproducibility, we release our code
and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/sunsql.",https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/sunsql,-1
0823e884-293b-440e-8b5f-de60dec11608,Exploration of Machine Learning Classification Models Used for Behavioral Biometrics Authentication,0.707976,"Mobile devices have been manufactured and enhanced at growing rates in the
past decades. While this growth has significantly evolved the capability of
these devices, their security has been falling behind. This contrast in
development between capability and security of mobile devices is a significant
problem with the sensitive information of the public at risk. Continuing the
previous work in this field, this study identifies key Machine Learning
algorithms currently being used for behavioral biometric mobile authentication
schemes and aims to provide a comprehensive review of these algorithms when
used with touch dynamics and phone movement. Throughout this paper the
benefits, limitations, and recommendations for future work will be discussed.",None,664
a293726f-ea27-4775-a8fc-c44be8592ec4,Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards,0.982963,"Neural image-to-text radiology report generation systems offer the potential
to improve radiology reporting by reducing the repetitive process of report
drafting and identifying possible medical errors. These systems have achieved
promising performance as measured by widely used NLG metrics such as BLEU and
CIDEr. However, the current systems face important limitations. First, they
present an increased complexity in architecture that offers only marginal
improvements on NLG metrics. Secondly, these systems that achieve high
performance on these metrics are not always factually complete or consistent
due to both inadequate training and evaluation. Recent studies have shown the
systems can be substantially improved by using new methods encouraging 1) the
generation of domain entities consistent with the reference and 2) describing
these entities in inferentially consistent ways. So far, these methods rely on
weakly-supervised approaches (rule-based) and named entity recognition systems
that are not specific to the chest X-ray domain. To overcome this limitation,
we propose a new method, the RadGraph reward, to further improve the factual
completeness and correctness of generated radiology reports. More precisely, we
leverage the RadGraph dataset containing annotated chest X-ray reports with
entities and relations between entities. On two open radiology report datasets,
our system substantially improves the scores up to 14.2% and 25.3% on metrics
evaluating the factual correctness and completeness of reports.",https://github.com/jbdel/vilmedic,-1
1856cc7d-e5ff-4ccf-9c44-086752037a99,MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors,0.928169,"In this paper, we propose MOTRv2, a simple yet effective pipeline to
bootstrap end-to-end multi-object tracking with a pretrained object detector.
Existing end-to-end methods, MOTR and TrackFormer are inferior to their
tracking-by-detection counterparts mainly due to their poor detection
performance. We aim to improve MOTR by elegantly incorporating an extra object
detector. We first adopt the anchor formulation of queries and then use an
extra object detector to generate proposals as anchors, providing detection
prior to MOTR. The simple modification greatly eases the conflict between joint
learning detection and association tasks in MOTR. MOTRv2 keeps the query
propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the
1st place (73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in
Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on
the BDD100K dataset. We hope this simple and effective pipeline can provide
some new insights to the end-to-end MOT community. Code is available at
\url{https://github.com/megvii-research/MOTRv2}.",https://github.com/megvii-research/MOTRv2,2415
9dfa3dff-882f-4a41-af22-79fb0cda62f7,Biological Robots: Perspectives on an Emerging Interdisciplinary Field,0.70404,"Advances in science and engineering often reveal the limitations of classical
approaches initially used to understand, predict, and control phenomena. With
progress, conceptual categories must often be re-evaluated to better track
recently discovered invariants across disciplines. It is essential to refine
frameworks and resolve conflicting boundaries between disciplines such that
they better facilitate, not restrict, experimental approaches and capabilities.
In this essay, we discuss issues at the intersection of developmental biology,
computer science, and robotics. In the context of biological robots, we explore
changes across concepts and previously distinct fields that are driven by
recent advances in materials, information, and life sciences. Herein, each
author provides their own perspective on the subject, framed by their own
disciplinary training. We argue that as with computation, certain aspects of
developmental biology and robotics are not tied to specific materials; rather,
the consilience of these fields can help to shed light on issues of multi-scale
control, self-assembly, and relationships between form and function. We hope
new fields can emerge as boundaries arising from technological limitations are
overcome, furthering practical applications from regenerative medicine to
useful synthetic living machines.",None,-1
7bd05783-875e-4fd6-8097-1268dab49cf0,QuantFace: Towards Lightweight Face Recognition by Synthetic Data Low-bit Quantization,0.874404,"Deep learning-based face recognition models follow the common trend in deep
neural networks by utilizing full-precision floating-point networks with high
computational costs. Deploying such networks in use-cases constrained by
computational requirements is often infeasible due to the large memory required
by the full-precision model. Previous compact face recognition approaches
proposed to design special compact architectures and train them from scratch
using real training data, which may not be available in a real-world scenario
due to privacy concerns. We present in this work the QuantFace solution based
on low-bit precision format model quantization. QuantFace reduces the required
computational cost of the existing face recognition models without the need for
designing a particular architecture or accessing real training data. QuantFace
introduces privacy-friendly synthetic face data to the quantization process to
mitigate potential privacy concerns and issues related to the accessibility to
real training data. Through extensive evaluation experiments on seven
benchmarks and four network architectures, we demonstrate that QuantFace can
successfully reduce the model size up to 5x while maintaining, to a large
degree, the verification performance of the full-precision model without
accessing real training datasets.",None,-1
a4832d8a-7404-4fe5-889c-c290f7653c11,Prioritizing emergency evacuations under compounding levels of uncertainty,0.164286,"Well-executed emergency evacuations can save lives and reduce suffering.
However, decision makers struggle to determine optimal evacuation policies
given the chaos, uncertainty, and value judgments inherent in emergency
evacuations. We propose and analyze a decision support tool for pre-crisis
training exercises for teams preparing for civilian evacuations and explore the
tool in the case of the 2021 U.S.-led evacuation from Afghanistan. We use
different classes of Markov decision processes (MDPs) to capture compounding
levels of uncertainty in (1) the priority category of who appears next at the
gate for evacuation, (2) the distribution of priority categories at the
population level, and (3) individuals' claimed priority category. We compare
the number of people evacuated by priority status under eight heuristic
policies. The optimized MDP policy achieves the best performance compared to
all heuristic baselines. We also show that accounting for the compounding
levels of model uncertainty incurs added complexity without improvement in
policy performance. Useful heuristics can be extracted from the optimized
policies to inform human decision makers. We open-source all tools to encourage
robust dialogue about the trade-offs, limitations, and potential of integrating
algorithms into high-stakes humanitarian decision-making.",https://github.com/sisl/EvacuationPOMDP.jl,-1
fe96519a-b853-4ebd-aab6-ae17764156f5,EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL,0.458687,"Reinforcement learning (RL) in long horizon and sparse reward tasks is
notoriously difficult and requires a lot of training steps. A standard solution
to speed up the process is to leverage additional reward signals, shaping it to
better guide the learning process. In the context of language-conditioned RL,
the abstraction and generalisation properties of the language input provide
opportunities for more efficient ways of shaping the reward. In this paper, we
leverage this idea and propose an automated reward shaping method where the
agent extracts auxiliary objectives from the general language goal. These
auxiliary objectives use a question generation (QG) and question answering (QA)
system: they consist of questions leading the agent to try to reconstruct
partial information about the global goal using its own trajectory. When it
succeeds, it receives an intrinsic reward proportional to its confidence in its
answer. This incentivizes the agent to generate trajectories which
unambiguously explain various aspects of the general language goal. Our
experimental study shows that this approach, which does not require engineer
intervention to design the auxiliary objectives, improves sample efficiency by
effectively directing exploration.",https://github.com/flowersteam/EAGER,-1
580034de-5650-48dc-a2e2-f6880a2e723b,Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction,0.758732,"Recently, prompt-tuning has attracted growing interests in event argument
extraction (EAE). However, the existing prompt-tuning methods have not achieved
satisfactory performance due to the lack of consideration of entity
information. In this paper, we propose a bi-directional iterative prompt-tuning
method for EAE, where the EAE task is treated as a cloze-style task to take
full advantage of entity information and pre-trained language models (PLMs).
Furthermore, our method explores event argument interactions by introducing the
argument roles of contextual entities into prompt construction. Since template
and verbalizer are two crucial components in a cloze-style prompt, we propose
to utilize the role label semantic knowledge to construct a semantic verbalizer
and design three kinds of templates for the EAE task. Experiments on the ACE
2005 English dataset with standard and low-resource settings show that the
proposed method significantly outperforms the peer state-of-the-art methods.
Our code is available at https://github.com/HustMinsLab/BIP.",https://github.com/HustMinsLab/BIP,-1
a36fd127-21e5-4bd6-ab5a-ee586bcaab51,LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning,0.817618,"Recently, deep learning models have made great progress in MWP solving on
answer accuracy. However, they are uninterpretable since they mainly rely on
shallow heuristics to achieve high performance without understanding and
reasoning the grounded math logic. To address this issue and make a step
towards interpretable MWP solving, we first construct a high-quality MWP
dataset named InterMWP which consists of 11,495 MWPs and annotates
interpretable logical formulas based on algebraic knowledge as the grounded
linguistic logic of each solution equation. Different from existing MWP
datasets, our InterMWP benchmark asks for a solver to not only output the
solution expressions but also predict the corresponding logical formulas. We
further propose a novel approach with logical prompt and interpretation
generation, called LogicSolver. For each MWP, our LogicSolver first retrieves
some highly-correlated algebraic knowledge and then passes them to the backbone
model as prompts to improve the semantic representations of MWPs. With these
improved semantic representations, our LogicSolver generates corresponding
solution expressions and interpretable knowledge formulas in accord with the
generated solution expressions, simultaneously. Experimental results show that
our LogicSolver has stronger logical formula-based interpretability than
baselines while achieving higher answer accuracy with the help of logical
prompts, simultaneously. The source code and dataset is available at
https://github.com/yangzhch6/InterMWP.",https://github.com/yangzhch6/InterMWP,-1
18c56192-4725-40ff-871a-d4315e442014,Active Self-Training for Weakly Supervised 3D Scene Semantic Segmentation,0.353525,"Since the preparation of labeled data for training semantic segmentation
networks of point clouds is a time-consuming process, weakly supervised
approaches have been introduced to learn from only a small fraction of data.
These methods are typically based on learning with contrastive losses while
automatically deriving per-point pseudo-labels from a sparse set of
user-annotated labels. In this paper, our key observation is that the selection
of what samples to annotate is as important as how these samples are used for
training. Thus, we introduce a method for weakly supervised segmentation of 3D
scenes that combines self-training with active learning. The active learning
selects points for annotation that likely result in performance improvements to
the trained model, while the self-training makes efficient use of the
user-provided labels for learning the model. We demonstrate that our approach
leads to an effective method that provides improvements in scene segmentation
over previous works and baselines, while requiring only a small number of user
annotations.",None,6439
434bf510-0361-43cc-be3a-e61a696386c4,Towards Stroke Patients' Upper-limb Automatic Motor Assessment Using Smartwatches,0.853605,"Assessing the physical condition in rehabilitation scenarios is a challenging
problem, since it involves Human Activity Recognition (HAR) and kinematic
analysis methods. In addition, the difficulties increase in unconstrained
rehabilitation scenarios, which are much closer to the real use cases. In
particular, our aim is to design an upper-limb assessment pipeline for stroke
patients using smartwatches. We focus on the HAR task, as it is the first part
of the assessing pipeline. Our main target is to automatically detect and
recognize four key movements inspired by the Fugl-Meyer assessment scale, which
are performed in both constrained and unconstrained scenarios. In addition to
the application protocol and dataset, we propose two detection and
classification baseline methods. We believe that the proposed framework,
dataset and baseline results will serve to foster this research field.",None,-1
976333ef-b10d-4332-bdc0-f07944ac1796,OPD: Single-view 3D Openable Part Detection,0.470841,"We address the task of predicting what parts of an object can open and how
they move when they do so. The input is a single image of an object, and as
output we detect what parts of the object can open, and the motion parameters
describing the articulation of each openable part. To tackle this task, we
create two datasets of 3D objects: OPDSynth based on existing synthetic
objects, and OPDReal based on RGBD reconstructions of real objects. We then
design OPDRCNN, a neural architecture that detects openable parts and predicts
their motion parameters. Our experiments show that this is a challenging task
especially when considering generalization across object categories, and the
limited amount of information in a single image. Our architecture outperforms
baselines and prior work especially for RGB image inputs. Short video summary
at https://www.youtube.com/watch?v=P85iCaD0rfc",https://github.com/facebookresearch/detectron2,-1
844b2036-e401-4b74-a418-542ad79dfb3b,Interpretability for Language Learners Using Example-Based Grammatical Error Correction,0.986321,"Grammatical Error Correction (GEC) should not focus only on high accuracy of
corrections but also on interpretability for language learning. However,
existing neural-based GEC models mainly aim at improving accuracy, and their
interpretability has not been explored. A promising approach for improving
interpretability is an example-based method, which uses similar retrieved
examples to generate corrections. In addition, examples are beneficial in
language learning, helping learners understand the basis of grammatically
incorrect/correct texts and improve their confidence in writing. Therefore, we
hypothesize that incorporating an example-based method into GEC can improve
interpretability as well as support language learners. In this study, we
introduce an Example-Based GEC (EB-GEC) that presents examples to language
learners as a basis for a correction result. The examples consist of pairs of
correct and incorrect sentences similar to a given input and its predicted
correction. Experiments demonstrate that the examples presented by EB-GEC help
language learners decide to accept or refuse suggestions from the GEC output.
Furthermore, the experiments also show that retrieved examples improve the
accuracy of corrections.",https://github.com/kanekomasahiro/eb-gec,-1
6876bbce-5274-4fad-94fc-18378b1ecf3c,Smooth Robust Tensor Completion for Background/Foreground Separation with Missing Pixels: Novel Algorithm with Convergence Guarantee,0.619791,"The objective of this study is to address the problem of
background/foreground separation with missing pixels by combining the video
acquisition, video recovery, background/foreground separation into a single
framework. To achieve this, a smooth robust tensor completion (SRTC) model is
proposed to recover the data and decompose it into the static background and
smooth foreground, respectively. Specifically, the static background is modeled
by the low-rank tucker decomposition and the smooth foreground (moving objects)
is modeled by the spatiotemporal continuity, which is enforced by the total
variation regularization. An efficient algorithm based on tensor proximal
alternating minimization (tenPAM) is implemented to solve the proposed model
with global convergence guarantee under very mild conditions. Extensive
experiments on real data demonstrate that the proposed method significantly
outperforms the state-of-the-art approaches for background/foreground
separation with missing pixels.",https://github.com/ZihengLi6321/MCOS,-1
3ce8c5b1-f0ae-4609-93a5-d366129b5829,Differentiable Inference of Temporal Logic Formulas,0.610211,"We demonstrate the first Recurrent Neural Network architecture for learning
Signal Temporal Logic formulas, and present the first systematic comparison of
formula inference methods. Legacy systems embed much expert knowledge which is
not explicitly formalized. There is great interest in learning formal
specifications that characterize the ideal behavior of such systems -- that is,
formulas in temporal logic that are satisfied by the system's output signals.
Such specifications can be used to better understand the system's behavior and
improve design of its next iteration. Previous inference methods either assumed
certain formula templates, or did a heuristic enumeration of all possible
templates. This work proposes a neural network architecture that infers the
formula structure via gradient descent, eliminating the need for imposing any
specific templates. It combines learning of formula structure and parameters in
one optimization. Through systematic comparison, we demonstrate that this
method achieves similar or better mis-classification rates (MCR) than
enumerative and lattice methods. We also observe that different formulas can
achieve similar MCR, empirically demonstrating the under-determinism of the
problem of temporal logic inference.",https://github.com/nicaless/fernn,51
7b78cd5f-35ff-4a26-b720-38f3f3643403,Blockchain-Based Decentralized Knowledge Marketplace Using Active Inference,0.198481,"A knowledge market can be described as a type of market where there is a
consistent supply of data to satisfy the demand for information and is
responsible for the mapping of potential problem solvers with the entities
which need these solutions. It is possible to define them as value-exchange
systems in which the dynamic features of the creation and exchange of
intellectual assets serve as the fundamental drivers of the frequency, nature,
and outcomes of interactions among various stakeholders. Furthermore, the
provision of financial backing for research is an essential component in the
process of developing a knowledge market that is capable of enduring over time,
and it is also an essential driver of the progression of scientific
investigation. This paper underlines flaws associated with the conventional
knowledge-based market, including but not limited to excessive financing
concentration, ineffective information exchange, a lack of security, mapping of
entities, etc. The authors present a decentralized framework for the knowledge
marketplace incorporating technologies such as blockchain, active inference,
zero-knowledge proof, etc. The proposed decentralized framework provides not
only an efficient mapping mechanism to map entities in the marketplace but also
a more secure and controlled way to share knowledge and services among various
stakeholders.",None,-1
e5305708-2bf7-4e12-91e7-8c1f91ce251b,Coupled Iterative Refinement for 6D Multi-Object Pose Estimation,0.961303,"We address the task of 6D multi-object pose: given a set of known 3D objects
and an RGB or RGB-D input image, we detect and estimate the 6D pose of each
object. We propose a new approach to 6D object pose estimation which consists
of an end-to-end differentiable architecture that makes use of geometric
knowledge. Our approach iteratively refines both pose and correspondence in a
tightly coupled manner, allowing us to dynamically remove outliers to improve
accuracy. We use a novel differentiable layer to perform pose refinement by
solving an optimization problem we refer to as Bidirectional Depth-Augmented
Perspective-N-Point (BD-PnP). Our method achieves state-of-the-art accuracy on
standard 6D Object Pose benchmarks. Code is available at
https://github.com/princeton-vl/Coupled-Iterative-Refinement.",None,-1
93b70b2f-2ba6-4370-92ef-36a5399ee992,Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding,0.397495,"Unsupervised pre-training on millions of digital-born or scanned documents
has shown promising advances in visual document understanding~(VDU). While
various vision-language pre-training objectives are studied in existing
solutions, the document textline, as an intrinsic granularity in VDU, has
seldom been explored so far. A document textline usually contains words that
are spatially and semantically correlated, which can be easily obtained from
OCR engines. In this paper, we propose Wukong-Reader, trained with new
pre-training objectives to leverage the structural knowledge nested in document
textlines. We introduce textline-region contrastive learning to achieve
fine-grained alignment between the visual regions and texts of document
textlines. Furthermore, masked region modeling and textline-grid matching are
also designed to enhance the visual and layout representations of textlines.
Experiments show that our Wukong-Reader has superior performance on various VDU
tasks such as information extraction. The fine-grained alignment over textlines
also empowers Wukong-Reader with promising localization ability.",None,-1
cf858836-ee75-473d-8fcd-5679f13dfd41,DePlot: One-shot visual language reasoning by plot-to-table translation,0.531973,"Visual language such as charts and plots is ubiquitous in the human world.
Comprehending plots and charts requires strong reasoning skills. Prior
state-of-the-art (SOTA) models require at least tens of thousands of training
examples and their reasoning capabilities are still much limited, especially on
complex human-written queries. This paper presents the first one-shot solution
to visual language reasoning. We decompose the challenge of visual language
reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over
the translated text. The key in this method is a modality conversion module,
named as DePlot, which translates the image of a plot or chart to a linearized
table. The output of DePlot can then be directly used to prompt a pretrained
large language model (LLM), exploiting the few-shot reasoning capabilities of
LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing
unified task formats and metrics, and train DePlot end-to-end on this task.
DePlot can then be used off-the-shelf together with LLMs in a plug-and-play
fashion. Compared with a SOTA model finetuned on more than >28k data points,
DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over
finetuned SOTA on human-written queries from the task of chart QA.",github.com/google-research/google-research/tree/master/deplot,-1
5499eba4-1afa-4e8b-bc7a-03608029b72a,Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on Synthetic Images,0.0588285,"Most recent style-transfer techniques based on generative architectures are
able to obtain synthetic multimedia contents, or commonly called deepfakes,
with almost no artifacts. Researchers already demonstrated that synthetic
images contain patterns that can determine not only if it is a deepfake but
also the generative architecture employed to create the image data itself.
These traces can be exploited to study problems that have never been addressed
in the context of deepfakes. To this aim, in this paper a first approach to
investigate the image ballistics on deepfake images subject to style-transfer
manipulations is proposed. Specifically, this paper describes a study on
detecting how many times a digital image has been processed by a generative
architecture for style transfer. Moreover, in order to address and study
accurately forensic ballistics on deepfake images, some mathematical properties
of style-transfer operations were investigated.",https://github.com/deepfakes/faceswap,-1
759f3a87-2151-49db-bed0-8193a28636f0,TSAM: A Two-Stream Attention Model for Causal Emotion Entailment,0.582973,"Causal Emotion Entailment (CEE) aims to discover the potential causes behind
an emotion in a conversational utterance. Previous works formalize CEE as
independent utterance pair classification problems, with emotion and speaker
information neglected. From a new perspective, this paper considers CEE in a
joint framework. We classify multiple utterances synchronously to capture the
correlations between utterances in a global view and propose a Two-Stream
Attention Model (TSAM) to effectively model the speaker's emotional influences
in the conversational history. Specifically, the TSAM comprises three modules:
Emotion Attention Network (EAN), Speaker Attention Network (SAN), and
interaction module. The EAN and SAN incorporate emotion and speaker information
in parallel, and the subsequent interaction module effectively interchanges
relevant information between the EAN and SAN via a mutual BiAffine
transformation. Extensive experimental results demonstrate that our model
achieves new State-Of-The-Art (SOTA) performance and outperforms baselines
remarkably.",https://github.com/huggingface/transformers,-1
2095a4cf-fc43-49cd-be98-2f45472c0aca,DraftRec: Personalized Draft Recommendation for Winning in Multi-Player Online Battle Arena Games,0.882195,"This paper presents a personalized character recommendation system for
Multiplayer Online Battle Arena (MOBA) games which are considered as one of the
most popular online video game genres around the world. When playing MOBA
games, players go through a draft stage, where they alternately select a
virtual character to play. When drafting, players select characters by not only
considering their character preferences, but also the synergy and competence of
their team's character combination. However, the complexity of drafting induces
difficulties for beginners to choose the appropriate characters based on the
characters of their team while considering their own champion preferences. To
alleviate this problem, we propose DraftRec, a novel hierarchical model which
recommends characters by considering each player's champion preferences and the
interaction between the players. DraftRec consists of two networks: the player
network and the match network. The player network captures the individual
player's champion preference, and the match network integrates the complex
relationship between the players and their respective champions. We train and
evaluate our model from a manually collected 280,000 matches of League of
Legends and a publicly available 50,000 matches of Dota2. Empirically, our
method achieved state-of-the-art performance in character recommendation and
match outcome prediction task. Furthermore, a comprehensive user survey
confirms that DraftRec provides convincing and satisfying recommendations. Our
code and dataset are available at https://github.com/dojeon-ai/DraftRec.",https://github.com/dojeon-ai/DraftRec,12146
4f67c92f-0dc7-4d6c-8f22-d41729985569,Online Continual Learning for Embedded Devices,0.956598,"Real-time on-device continual learning is needed for new applications such as
home robots, user personalization on smartphones, and augmented/virtual reality
headsets. However, this setting poses unique challenges: embedded devices have
limited memory and compute capacity and conventional machine learning models
suffer from catastrophic forgetting when updated on non-stationary data
streams. While several online continual learning models have been developed,
their effectiveness for embedded applications has not been rigorously studied.
In this paper, we first identify criteria that online continual learners must
meet to effectively perform real-time, on-device learning. We then study the
efficacy of several online continual learning methods when used with mobile
neural networks. We measure their performance, memory usage, compute
requirements, and ability to generalize to out-of-domain inputs.",https://github.com/tyler-hayes/Embedded-CL,-1
3eaa0c02-47ae-4d90-9aa7-e234393b6817,Identifying Electrocardiogram Abnormalities Using a Handcrafted-Rule-Enhanced Neural Network,0.132401,"A large number of people suffer from life-threatening cardiac abnormalities,
and electrocardiogram (ECG) analysis is beneficial to determining whether an
individual is at risk of such abnormalities. Automatic ECG classification
methods, especially the deep learning based ones, have been proposed to detect
cardiac abnormalities using ECG records, showing good potential to improve
clinical diagnosis and help early prevention of cardiovascular diseases.
However, the predictions of the known neural networks still do not
satisfactorily meet the needs of clinicians, and this phenomenon suggests that
some information used in clinical diagnosis may not be well captured and
utilized by these methods. In this paper, we introduce some rules into
convolutional neural networks, which help present clinical knowledge to deep
learning based ECG analysis, in order to improve automated ECG diagnosis
performance. Specifically, we propose a Handcrafted-Rule-enhanced Neural
Network (called HRNN) for ECG classification with standard 12-lead ECG input,
which consists of a rule inference module and a deep learning module.
Experiments on two large-scale public ECG datasets show that our new approach
considerably outperforms existing state-of-the-art methods. Further, our
proposed approach not only can improve the diagnosis performance, but also can
assist in detecting mislabelled ECG samples. Our codes are available at
https://github.com/alwaysbyx/ecg_processing.",https://github.com/alwaysbyx/ecg processing/,10744
f465dcef-b998-4dc9-8908-3270d4d618b5,SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval,0.673324,"Sampling proper negatives from a large document pool is vital to effectively
train a dense retrieval model. However, existing negative sampling strategies
suffer from the uninformative or false negative problem. In this work, we
empirically show that according to the measured relevance scores, the negatives
ranked around the positives are generally more informative and less likely to
be false negatives. Intuitively, these negatives are not too hard (\emph{may be
false negatives}) or too easy (\emph{uninformative}). They are the ambiguous
negatives and need more attention during training. Thus, we propose a simple
ambiguous negatives sampling method, SimANS, which incorporates a new sampling
probability distribution to sample more ambiguous negatives. Extensive
experiments on four public and one industry datasets show the effectiveness of
our approach. We made the code and models publicly available in
\url{https://github.com/microsoft/SimXNS}.",https://github.com/microsoft/SimXNS,-1
7b7b7e67-1576-44c7-9fe3-26096ce6af76,Less Is More: Linear Layers on CLIP Features as Powerful VizWiz Model,0.0188489,"Current architectures for multi-modality tasks such as visual question
answering suffer from their high complexity. As a result, these architectures
are difficult to train and require high computational resources. To address
these problems we present a CLIP-based architecture that does not require any
fine-tuning of the feature extractors. A simple linear classifier is used on
the concatenated features of the image and text encoder. During training an
auxiliary loss is added which operates on the answer types. The resulting
classification is then used as an attention gate on the answer class selection.
On the VizWiz 2022 Visual Question Answering Challenge we achieve 60.15 %
accuracy on Task 1: Predict Answer to a Visual Question and AP score of 83.78 %
on Task 2: Predict Answerability of a Visual Question.",None,-1
aaf0df49-fcf7-44da-b6dc-ce7f09f60d2c,"SeasoNet: A Seasonal Scene Classification, segmentation and Retrieval dataset for satellite Imagery over Germany",0.417159,"This work presents SeasoNet, a new large-scale multi-label land cover and
land use scene understanding dataset. It includes $1\,759\,830$ images from
Sentinel-2 tiles, with 12 spectral bands and patch sizes of up to $ 120 \
\mathrm{px} \times 120 \ \mathrm{px}$. Each image is annotated with large scale
pixel level labels from the German land cover model LBM-DE2018 with land cover
classes based on the CORINE Land Cover database (CLC) 2018 and a five times
smaller minimum mapping unit (MMU) than the original CLC maps. We provide pixel
synchronous examples from all four seasons, plus an additional snowy set. These
properties make SeasoNet the currently most versatile and biggest remote
sensing scene understanding dataset with possible applications ranging from
scene classification over land cover mapping to content-based cross season
image retrieval and self-supervised feature learning. We provide baseline
results by evaluating state-of-the-art deep networks on the new dataset in
scene classification and semantic segmentation scenarios.",None,-1
e8fd3bbe-6901-4911-9497-13c7353a197a,Modernizing Open-Set Speech Language Identification,0.10955,"While most modern speech Language Identification methods are closed-set, we
want to see if they can be modified and adapted for the open-set problem. When
switching to the open-set problem, the solution gains the ability to reject an
audio input when it fails to match any of our known language options. We tackle
the open-set task by adapting two modern-day state-of-the-art approaches to
closed-set language identification: the first using a CRNN with attention and
the second using a TDNN. In addition to enhancing our input feature embeddings
using MFCCs, log spectral features, and pitch, we will be attempting two
approaches to out-of-set language detection: one using thresholds, and the
other essentially performing a verification task. We will compare both the
performance of the TDNN and the CRNN, as well as our detection approaches.",None,-1
4cdbd97c-94a7-420d-8490-aba0ae48b771,A Machine With Human-Like Memory Systems,0.0386783,"Inspired by the cognitive science theory, we explicitly model an agent with
both semantic and episodic memory systems, and show that it is better than
having just one of the two memory systems. In order to show this, we have
designed and released our own challenging environment, ""the Room"", compatible
with OpenAI Gym, where an agent has to properly learn how to encode, store, and
retrieve memories to maximize its rewards. The Room environment allows for a
hybrid intelligence setup where machines and humans can collaborate. We show
that two agents collaborating with each other results in better performance
than one agent acting alone. We have open-sourced our code and models at
https://github.com/tae898/explicit-memory.",https://github.com/tae898/explicit-memory,-1
83265bf0-692b-4a09-b2fa-0355d1bd9fd5,Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,0.820799,"Language models increasingly rely on massive web dumps for diverse text data.
However, these sources are rife with undesirable content. As such, resources
like Wikipedia, books, and newswire often serve as anchors for automatically
selecting web text most suitable for language modeling, a process typically
referred to as quality filtering. Using a new dataset of U.S. high school
newspaper articles -- written by students from across the country -- we
investigate whose language is preferred by the quality filter used for GPT-3.
We find that newspapers from larger schools, located in wealthier, educated,
and urban ZIP codes are more likely to be classified as high quality. We then
demonstrate that the filter's measurement of quality is unaligned with other
sensible metrics, such as factuality or literary acclaim. We argue that
privileging any corpus as high quality entails a language ideology, and more
care is needed to construct training corpora for language models, with better
transparency and justification for the inclusion or exclusion of various texts.",https://github.com/kernelmachine/quality-filter,-1
dfa87f09-30da-4ea1-a834-c9de0b8e8bb5,Acquiring and Modelling Abstract Commonsense Knowledge via Conceptualization,0.766864,"Conceptualization, or viewing entities and situations as instances of
abstract concepts in mind and making inferences based on that, is a vital
component in human intelligence for commonsense reasoning. Although recent
artificial intelligence has made progress in acquiring and modelling
commonsense, attributed to large neural language models and commonsense
knowledge graphs (CKGs), conceptualization is yet to thoroughly be introduced,
making current approaches ineffective to cover knowledge about countless
diverse entities and situations in the real world. To address the problem, we
thoroughly study the possible role of conceptualization in commonsense
reasoning, and formulate a framework to replicate human conceptual induction
from acquiring abstract knowledge about abstract concepts. Aided by the
taxonomy Probase, we develop tools for contextualized conceptualization on
ATOMIC, a large-scale human annotated CKG. We annotate a dataset for the
validity of conceptualizations for ATOMIC on both event and triple level,
develop a series of heuristic rules based on linguistic features, and train a
set of neural models, so as to generate and verify abstract knowledge. Based on
these components, a pipeline to acquire abstract knowledge is built. A large
abstract CKG upon ATOMIC is then induced, ready to be instantiated to infer
about unseen entities or situations. Furthermore, experiments find directly
augmenting data with abstract triples to be helpful in commonsense modelling.",https://github.com/HKUST-KnowComp/atomic-conceptualization,-1
1bbd2a76-db49-4dac-8fa9-1af1eb26d62f,What Makes Good Contrastive Learning on Small-Scale Wearable-based Tasks?,0.418547,"Self-supervised learning establishes a new paradigm of learning
representations with much fewer or even no label annotations. Recently there
has been remarkable progress on large-scale contrastive learning models which
require substantial computing resources, yet such models are not practically
optimal for small-scale tasks. To fill the gap, we aim to study contrastive
learning on the wearable-based activity recognition task. Specifically, we
conduct an in-depth study of contrastive learning from both algorithmic-level
and task-level perspectives. For algorithmic-level analysis, we decompose
contrastive models into several key components and conduct rigorous
experimental evaluations to better understand the efficacy and rationale behind
contrastive learning. More importantly, for task-level analysis, we show that
the wearable-based signals bring unique challenges and opportunities to
existing contrastive models, which cannot be readily solved by existing
algorithms. Our thorough empirical studies suggest important practices and shed
light on future research challenges. In the meantime, this paper presents an
open-source PyTorch library \texttt{CL-HAR}, which can serve as a practical
tool for researchers. The library is highly modularized and easy to use, which
opens up avenues for exploring novel contrastive models quickly in the future.",https://github.com/Tian0426/CL-HAR,36321
4b82c0a7-ad7d-4b10-b3c2-1298df8d285e,Social Diversity Reduces the Complexity and Cost of Fostering Fairness,0.512968,"Institutions and investors are constantly faced with the challenge of
appropriately distributing endowments. No budget is limitless and optimising
overall spending without sacrificing positive outcomes has been approached and
resolved using several heuristics. To date, prior works have failed to consider
how to encourage fairness in a population where social diversity is ubiquitous,
and in which investors can only partially observe the population. Herein, by
incorporating social diversity in the Ultimatum game through heterogeneous
graphs, we investigate the effects of several interference mechanisms which
assume incomplete information and flexible standards of fairness. We quantify
the role of diversity and show how it reduces the need for information
gathering, allowing us to relax a strict, costly interference process.
Furthermore, we find that the influence of certain individuals, expressed by
different network centrality measures, can be exploited to further reduce
spending if minimal fairness requirements are lowered. Our results indicate
that diversity changes and opens up novel mechanisms available to institutions
wishing to promote fairness. Overall, our analysis provides novel insights to
guide institutional policies in socially diverse complex systems.",None,-1
851907d2-cb1c-402d-b3a7-25ae3227fdcc,Structure Extraction in Task-Oriented Dialogues with Slot Clustering,0.28447,"Extracting structure information from dialogue data can help us better
understand user and system behaviors. In task-oriented dialogues, dialogue
structure has often been considered as transition graphs among dialogue states.
However, annotating dialogue states manually is expensive and time-consuming.
In this paper, we propose a simple yet effective approach for structure
extraction in task-oriented dialogues. We first detect and cluster possible
slot tokens with a pre-trained model to approximate dialogue ontology for a
target domain. Then we track the status of each identified token group and
derive a state transition structure. Empirical results show that our approach
outperforms unsupervised baseline models by far in dialogue structure
extraction. In addition, we show that data augmentation based on extracted
structures enriches the surface formats of training data and can achieve a
significant performance boost in dialogue response generation.",https://github.com/salesforce/dialog-flow-extraction,-1
622a17a5-edc7-4b3b-9f79-3eb3de7e35ca,Counterfactual reasoning: Do language models need world knowledge for causal understanding?,0.0283709,"Current pre-trained language models have enabled remarkable improvements in
downstream tasks, but it remains difficult to distinguish effects of
statistical correlation from more systematic logical reasoning grounded on
understanding of the real world. In this paper we tease these factors apart by
leveraging counterfactual conditionals, which force language models to predict
unusual consequences based on hypothetical propositions. We introduce a set of
tests drawn from psycholinguistic experiments, as well as larger-scale
controlled datasets, to probe counterfactual predictions from a variety of
popular pre-trained language models. We find that models are consistently able
to override real-world knowledge in counterfactual scenarios, and that this
effect is more robust in case of stronger baseline world knowledge -- however,
we also find that for most models this effect appears largely to be driven by
simple lexical cues. When we mitigate effects of both world knowledge and
lexical cues to test knowledge of linguistic nuances of counterfactuals, we
find that only GPT-3 shows sensitivity to these nuances, though this
sensitivity is also non-trivially impacted by lexical associative factors.",https://github.com/goldengua/Counterfactual_Inference_LM,-1
1c1f8232-13fb-4146-96cb-e17a1c4e4100,Ontologically Faithful Generation of Non-Player Character Dialogues,0.441543,"We introduce a language generation task grounded in a popular video game
environment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration)
requires models to produce trees of dialogue between video game characters that
accurately reflect quest and entity specifications stated in natural language.
KNUDGE is constructed from side quest dialogues drawn directly from game data
of Obsidian Entertainment's The Outer Worlds, leading to real-world
complexities in generation: (1) dialogues are branching trees as opposed to
linear chains of utterances; (2) utterances must remain faithful to the game
lore -- character personas, backstories, and entity relationships; and (3) a
dialogue must accurately reveal new quest details to the human player. We
report results for a set of neural generation models using supervised and
in-context learning techniques; we find competent performance but room for
future work addressing the challenges of creating realistic, game-quality
dialogues.",https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py,-1
2f9276ae-e5e6-4a24-bc80-0324558e81df,Learning Equivariant Segmentation with Instance-Unique Querying,0.979609,"Prevalent state-of-the-art instance segmentation methods fall into a
query-based scheme, in which instance masks are derived by querying the image
feature using a set of instance-aware embeddings. In this work, we devise a new
training framework that boosts query-based models through discriminative query
embedding learning. It explores two essential properties, namely dataset-level
uniqueness and transformation equivariance, of the relation between queries and
instances. First, our algorithm uses the queries to retrieve the corresponding
instances from the whole training dataset, instead of only searching within
individual scenes. As querying instances across scenes is more challenging, the
segmenters are forced to learn more discriminative queries for effective
instance separation. Second, our algorithm encourages both image (instance)
representations and queries to be equivariant against geometric
transformations, leading to more robust, instance-query matching. On top of
four famous, query-based models ($i.e.,$ CondInst, SOLOv2, SOTR, and
Mask2Former), our training algorithm provides significant performance gains
($e.g.,$ +1.6 - 3.2 AP) on COCO dataset. In addition, our algorithm promotes
the performance of SOLOv2 by 2.7 AP, on LVISv1 dataset.",https://github.com/JamesLiang819/Instance_Unique_Querying,1818
de9c1f03-5319-4c8e-b063-fc1d6de3680a,Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation,0.507519,"Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in
Mandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest
challenges is the task of polyphone disambiguation. Most of the previous
polyphone disambiguation models are trained on manually annotated datasets, and
publicly available datasets for polyphone disambiguation are scarce. In this
paper we propose a simple back-translation-style data augmentation method for
mandarin Chinese polyphone disambiguation, utilizing a large amount of
unlabeled text data. Inspired by the back-translation technique proposed in the
field of machine translation, we build a Grapheme-to-Phoneme (G2P) model to
predict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme
(P2G) model to predict pronunciation into text. Meanwhile, a window-based
matching strategy and a multi-model scoring strategy are proposed to judge the
correctness of the pseudo-label. We design a data balance strategy to improve
the accuracy of some typical polyphonic characters in the training set with
imbalanced distribution or data scarcity. The experimental result shows the
effectiveness of the proposed back-translation-style data augmentation method.",None,-1
538fd6c0-fb55-4845-97bd-0152a126e264,MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation,0.657132,"Responding with multi-modal content has been recognized as an essential
capability for an intelligent conversational agent. In this paper, we introduce
the MMDialog dataset to better facilitate multi-modal conversation. MMDialog is
composed of a curated set of 1.08 million real-world dialogues with 1.53
million unique images across 4,184 topics. MMDialog has two main and unique
advantages. First, it is the largest multi-modal conversation dataset by the
number of dialogues by 88x. Second, it contains massive topics to generalize
the open-domain. To build engaging dialogue system with this dataset, we
propose and normalize two response producing tasks based on retrieval and
generative scenarios. In addition, we build two baselines for above tasks with
state-of-the-art techniques and report their experimental performance. We also
propose a novel evaluation metric MM-Relevance to measure the multi-modal
responses. Our dataset and scripts are available in
https://github.com/victorsungo/MMDialog.",https://github.com/victorsungo/MMDialog,-1
88331ce1-62f7-41e6-ad99-da7759f87d83,SpA-Former: Transformer image shadow detection and removal via spatial attention,0.307232,"In this paper, we propose an end-to-end SpA-Former to recover a shadow-free
image from a single shaded image. Unlike traditional methods that require two
steps for shadow detection and then shadow removal, the SpA-Former unifies
these steps into one, which is a one-stage network capable of directly learning
the mapping function between shadows and no shadows, it does not require a
separate shadow detection. Thus, SpA-former is adaptable to real image
de-shadowing for shadows projected on different semantic regions. SpA-Former
consists of transformer layer and a series of joint Fourier transform residual
blocks and two-wheel joint spatial attention. The network in this paper is able
to handle the task while achieving a very fast processing efficiency.
  Our code is relased on
https://github.com/zhangbaijin/SpA-Former-shadow-removal",https://github.com/zhangbaijin/SpA-Former-shadow-removal,-1
1b912c64-2fd8-43d5-85ae-c7a35c299517,CoHS-CQG: Context and History Selection for Conversational Question Generation,0.348421,"Conversational question generation (CQG) serves as a vital task for machines
to assist humans, such as interactive reading comprehension, through
conversations. Compared to traditional single-turn question generation (SQG),
CQG is more challenging in the sense that the generated question is required
not only to be meaningful, but also to align with the occurred conversation
history. While previous studies mainly focus on how to model the flow and
alignment of the conversation, there has been no thorough study to date on
which parts of the context and history are necessary for the model. We argue
that shortening the context and history is crucial as it can help the model to
optimise more on the conversational alignment property. To this end, we propose
CoHS-CQG, a two-stage CQG framework, which adopts a CoHS module to shorten the
context and history of the input. In particular, CoHS selects contiguous
sentences and history turns according to their relevance scores by a top-p
strategy. Our model achieves state-of-the-art performances on CoQA in both the
answer-aware and answer-unaware settings.",https://github.com/dxlong2000/CoHS-CQG,11730
6eea8f2b-82d8-45cf-9001-f123e2cc129c,Cognitive Semantic Communication Systems Driven by Knowledge Graph,0.985599,"Semantic communication is envisioned as a promising technique to break
through the Shannon limit. However, the existing semantic communication
frameworks do not involve inference and error correction, which limits the
achievable performance. In this paper, in order to tackle this issue, a
cognitive semantic communication framework is proposed by exploiting knowledge
graph. Moreover, a simple, general and interpretable solution for semantic
information detection is developed by exploiting triples as semantic symbols.
It also allows the receiver to correct errors occurring at the symbolic level.
Furthermore, the pre-trained model is fine-tuned to recover semantic
information, which overcomes the drawback that a fixed bit length coding is
used to encode sentences of different lengths. Simulation results on the public
WebNLG corpus show that our proposed system is superior to other benchmark
systems in terms of the data compression rate and the reliability of
communication.",None,-1
ac5d9c97-f78f-4310-8464-0436d9e2f11f,Representation Learning with Diffusion Models,0.163101,"Diffusion models (DMs) have achieved state-of-the-art results for image
synthesis tasks as well as density estimation. Applied in the latent space of a
powerful pretrained autoencoder (LDM), their immense computational requirements
can be significantly reduced without sacrificing sampling quality. However, DMs
and LDMs lack a semantically meaningful representation space as the diffusion
process gradually destroys information in the latent variables. We introduce a
framework for learning such representations with diffusion models (LRDM). To
that end, a LDM is conditioned on the representation extracted from the clean
image by a separate encoder. In particular, the DM and the representation
encoder are trained jointly in order to learn rich representations specific to
the generative denoising process. By introducing a tractable representation
prior, we can efficiently sample from the representation distribution for
unconditional image synthesis without training of any additional model. We
demonstrate that i) competitive image generation results can be achieved with
image-parameterized LDMs, ii) LRDMs are capable of learning semantically
meaningful representations, allowing for faithful image reconstructions and
semantic interpolations. Our implementation is available at
https://github.com/jeremiastraub/diffusion.",https://github.com/jeremiastraub/diusion,-1
36d2aeb0-87bd-4345-bd51-8556f0f1cf86,Dense Learning based Semi-Supervised Object Detection,0.864572,"Semi-supervised object detection (SSOD) aims to facilitate the training and
deployment of object detectors with the help of a large amount of unlabeled
data. Though various self-training based and consistency-regularization based
SSOD methods have been proposed, most of them are anchor-based detectors,
ignoring the fact that in many real-world applications anchor-free detectors
are more demanded. In this paper, we intend to bridge this gap and propose a
DenSe Learning (DSL) based anchor-free SSOD algorithm. Specifically, we achieve
this goal by introducing several novel techniques, including an Adaptive
Filtering strategy for assigning multi-level and accurate dense pixel-wise
pseudo-labels, an Aggregated Teacher for producing stable and precise
pseudo-labels, and an uncertainty-consistency-regularization term among scales
and shuffled patches for improving the generalization capability of the
detector. Extensive experiments are conducted on MS-COCO and PASCAL-VOC, and
the results show that our proposed DSL method records new state-of-the-art SSOD
performance, surpassing existing methods by a large margin. Codes can be found
at \textcolor{blue}{https://github.com/chenbinghui1/DSL}.",https://github.com/chenbinghui1/DSL,-1
4b5b8327-8b9e-4fcf-b1aa-6a6b46437d93,CFA: Coupled-hypersphere-based Feature Adaptation for Target-Oriented Anomaly Localization,0.999463,"For a long time, anomaly localization has been widely used in industries.
Previous studies focused on approximating the distribution of normal features
without adaptation to a target dataset. However, since anomaly localization
should precisely discriminate normal and abnormal features, the absence of
adaptation may make the normality of abnormal features overestimated. Thus, we
propose Coupled-hypersphere-based Feature Adaptation (CFA) which accomplishes
sophisticated anomaly localization using features adapted to the target
dataset. CFA consists of (1) a learnable patch descriptor that learns and
embeds target-oriented features and (2) scalable memory bank independent of the
size of the target dataset. And, CFA adopts transfer learning to increase the
normal feature density so that abnormal features can be clearly distinguished
by applying patch descriptor and memory bank to a pre-trained CNN. The proposed
method outperforms the previous methods quantitatively and qualitatively. For
example, it provides an AUROC score of 99.5% in anomaly detection and 98.5% in
anomaly localization of MVTec AD benchmark. In addition, this paper points out
the negative effects of biased features of pre-trained CNNs and emphasizes the
importance of the adaptation to the target dataset. The code is publicly
available at https://github.com/sungwool/CFA_for_anomaly_localization.",https://github.com/sungwool/CFA_for_anomaly_localization,-1
079fd354-4f75-422c-af06-c29419da7ce7,nerf2nerf: Pairwise Registration of Neural Radiance Fields,0.60803,"We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io",None,9133
94a66700-df7d-4113-9609-0a245cbb3a2d,Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling,0.509038,"Recent advances in federated learning have demonstrated its promising
capability to learn on decentralized datasets. However, a considerable amount
of work has raised concerns due to the potential risks of adversaries
participating in the framework to poison the global model for an adversarial
purpose. This paper investigates the feasibility of model poisoning for
backdoor attacks through rare word embeddings of NLP models. In text
classification, less than 1% of adversary clients suffices to manipulate the
model output without any drop in the performance on clean sentences. For a less
complex dataset, a mere 0.1% of adversary clients is enough to poison the
global model effectively. We also propose a technique specialized in the
federated learning scheme called Gradient Ensemble, which enhances the backdoor
performance in all our experimental settings.",None,-1
e3e51a7a-268f-47c4-968d-a185ee573b04,CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022,0.970615,"In this paper, we describe our submission to the Simultaneous Speech
Translation at IWSLT 2022. We explore strategies to utilize an offline model in
a simultaneous setting without the need to modify the original model. In our
experiments, we show that our onlinization algorithm is almost on par with the
offline setting while being $3\times$ faster than offline in terms of latency
on the test set. We also show that the onlinized offline model outperforms the
best IWSLT2021 simultaneous system in medium and high latency regimes and is
almost on par in the low latency regime. We make our system publicly available.",https://hub.docker.com/repository/docker/polape7/cuni-kit-simultaneous,-1
acddf5d2-5905-43f8-a36d-0ac24d4755a6,A Double-Graph Based Framework for Frame Semantic Parsing,0.739206,"Frame semantic parsing is a fundamental NLP task, which consists of three
subtasks: frame identification, argument identification and role
classification. Most previous studies tend to neglect relations between
different subtasks and arguments and pay little attention to ontological frame
knowledge defined in FrameNet. In this paper, we propose a Knowledge-guided
Incremental semantic parser with Double-graph (KID). We first introduce Frame
Knowledge Graph (FKG), a heterogeneous graph containing both frames and FEs
(Frame Elements) built on the frame knowledge so that we can derive
knowledge-enhanced representations for frames and FEs. Besides, we propose
Frame Semantic Graph (FSG) to represent frame semantic structures extracted
from the text with graph structures. In this way, we can transform frame
semantic parsing into an incremental graph construction problem to strengthen
interactions between subtasks and relations between arguments. Our experiments
show that KID outperforms the previous state-of-the-art method by up to 1.7
F1-score on two FrameNet datasets. Our code is availavle at
https://github.com/PKUnlp-icler/KID.",https://github.com/PKUnlp-icler/KID,-1
58263ba6-4961-4b42-b91d-513b156978bb,A Machine Learning Approach for DeepFake Detection,0.0227,"With the spread of DeepFake techniques, this technology has become quite
accessible and good enough that there is concern about its malicious use. Faced
with this problem, detecting forged faces is of utmost importance to ensure
security and avoid socio-political problems, both on a global and private
scale. This paper presents a solution for the detection of DeepFakes using
convolution neural networks and a dataset developed for this purpose -
Celeb-DF. The results show that, with an overall accuracy of 95% in the
classification of these images, the proposed model is close to what exists in
the state of the art with the possibility of adjustment for better results in
the manipulation techniques that arise in the future.",None,-1
f49bc2c5-4285-48e1-84f0-c95eb2696fb2,"When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues",0.602522,"Indirect speech such as sarcasm achieves a constellation of discourse goals
in human communication. While the indirectness of figurative language warrants
speakers to achieve certain pragmatic goals, it is challenging for AI agents to
comprehend such idiosyncrasies of human communication. Though sarcasm
identification has been a well-explored topic in dialogue analysis, for
conversational systems to truly grasp a conversation's innate meaning and
generate appropriate responses, simply detecting sarcasm is not enough; it is
vital to explain its underlying sarcastic connotation to capture its true
essence. In this work, we study the discourse structure of sarcastic
conversations and propose a novel task - Sarcasm Explanation in Dialogue (SED).
Set in a multimodal and code-mixed setting, the task aims to generate natural
language explanations of satirical conversations. To this end, we curate WITS,
a new dataset to support our task. We propose MAF (Modality Aware Fusion), a
multimodal context-aware attention and global information fusion module to
capture multimodality and use it to benchmark WITS. The proposed attention
module surpasses the traditional multimodal fusion baselines and reports the
best performance on almost all metrics. Lastly, we carry out detailed analyses
both quantitatively and qualitatively.",https://github.com/LCS2-IIITD/MAF.git,-1
60a4be15-1a4e-4afd-9819-8edc7955b4ae,Self-Normalized Density Map (SNDM) for Counting Microbiological Objects,0.643304,"The statistical properties of the density map (DM) approach to counting
microbiological objects on images are studied in detail. The DM is given by
U$^2$-Net. Two statistical methods for deep neural networks are utilized: the
bootstrap and the Monte Carlo (MC) dropout. The detailed analysis of the
uncertainties for the DM predictions leads to a deeper understanding of the DM
model's deficiencies. Based on our investigation, we propose a
self-normalization module in the network. The improved network model, called
\textit{Self-Normalized Density Map} (SNDM), can correct its output density map
by itself to accurately predict the total number of objects in the image. The
SNDM architecture outperforms the original model. Moreover, both statistical
frameworks -- bootstrap and MC dropout -- have consistent statistical results
for SNDM, which were not observed in the original model. The SNDM efficiency is
comparable with the detector-base models, such as Faster and Cascade R-CNN
detectors.",None,-1
c82fbaa1-9799-4ace-924d-0797a56e25f1,The State of Sparse Training in Deep Reinforcement Learning,0.597717,"The use of sparse neural networks has seen rapid growth in recent years,
particularly in computer vision. Their appeal stems largely from the reduced
number of parameters required to train and store, as well as in an increase in
learning efficiency. Somewhat surprisingly, there have been very few efforts
exploring their use in Deep Reinforcement Learning (DRL). In this work we
perform a systematic investigation into applying a number of existing sparse
training techniques on a variety of DRL agents and environments. Our results
corroborate the findings from sparse training in the computer vision domain -
sparse networks perform better than dense networks for the same parameter count
- in the DRL domain. We provide detailed analyses on how the various components
in DRL are affected by the use of sparse networks and conclude by suggesting
promising avenues for improving the effectiveness of sparse training methods,
as well as for advancing their use in DRL.",None,-1
b1d3e427-a460-42ac-a157-e2275e313a57,An Intelligent Self-driving Truck System For Highway Transportation,0.658314,"Recently, there have been many advances in autonomous driving society,
attracting a lot of attention from academia and industry. However, existing
works mainly focus on cars, extra development is still required for
self-driving truck algorithms and models. In this paper, we introduce an
intelligent self-driving truck system. Our presented system consists of three
main components, 1) a realistic traffic simulation module for generating
realistic traffic flow in testing scenarios, 2) a high-fidelity truck model
which is designed and evaluated for mimicking real truck response in real-world
deployment, 3) an intelligent planning module with learning-based decision
making algorithm and multi-mode trajectory planner, taking into account the
truck's constraints, road slope changes, and the surrounding traffic flow. We
provide quantitative evaluations for each component individually to demonstrate
the fidelity and performance of each part. We also deploy our proposed system
on a real truck and conduct real world experiments which shows our system's
capacity of mitigating sim-to-real gap. Our code is available at
https://github.com/InceptioResearch/IITS",https://github.com/InceptioResearch/IITS,-1
408cd507-cc62-4fdb-9358-d50b02cf47da,Asynchronous Optimisation for Event-based Visual Odometry,0.794295,"Event cameras open up new possibilities for robotic perception due to their
low latency and high dynamic range. On the other hand, developing effective
event-based vision algorithms that fully exploit the beneficial properties of
event cameras remains work in progress. In this paper, we focus on event-based
visual odometry (VO). While existing event-driven VO pipelines have adopted
continuous-time representations to asynchronously process event data, they
either assume a known map, restrict the camera to planar trajectories, or
integrate other sensors into the system. Towards map-free event-only monocular
VO in SE(3), we propose an asynchronous structure-from-motion optimisation
back-end. Our formulation is underpinned by a principled joint optimisation
problem involving non-parametric Gaussian Process motion modelling and
incremental maximum a posteriori inference. A high-performance incremental
computation engine is employed to reason about the camera trajectory with every
incoming event. We demonstrate the robustness of our asynchronous back-end in
comparison to frame-based methods which depend on accurate temporal
accumulation of measurements.",None,-1
17cbebb8-70cd-466e-bfdf-1edb58ac47d0,A Transfer Learning Based Model for Text Readability Assessment in German,0.618049,"Text readability assessment has a wide range of applications for different
target people, from language learners to people with disabilities. The fast
pace of textual content production on the web makes it impossible to measure
text complexity without the benefit of machine learning and natural language
processing techniques. Although various research addressed the readability
assessment of English text in recent years, there is still room for improvement
of the models for other languages. In this paper, we proposed a new model for
text complexity assessment for German text based on transfer learning. Our
results show that the model outperforms more classical solutions based on
linguistic features extraction from input text. The best model is based on the
BERT pre-trained language model achieved the Root Mean Square Error (RMSE) of
0.483.",None,12355
2d3da736-18b4-4970-b336-4c6033de5400,Facilitating Global Team Meetings Between Language-Based Subgroups: When and How Can Machine Translation Help?,0.060631,"Global teams frequently consist of language-based subgroups who put together
complementary information to achieve common goals. Previous research outlines a
two-step work communication flow in these teams. There are team meetings using
a required common language (i.e., English); in preparation for those meetings,
people have subgroup conversations in their native languages. Work
communication at team meetings is often less effective than in subgroup
conversations. In the current study, we investigate the idea of leveraging
machine translation (MT) to facilitate global team meetings. We hypothesize
that exchanging subgroup conversation logs before a team meeting offers
contextual information that benefits teamwork at the meeting. MT can translate
these logs, which enables comprehension at a low cost. To test our hypothesis,
we conducted a between-subjects experiment where twenty quartets of
participants performed a personnel selection task. Each quartet included two
English native speakers (NS) and two non-native speakers (NNS) whose native
language was Mandarin. All participants began the task with subgroup
conversations in their native languages, then proceeded to team meetings in
English. We manipulated the exchange of subgroup conversation logs prior to
team meetings: with MT-mediated exchanges versus without. Analysis of
participants' subjective experience, task performance, and depth of discussions
as reflected through their conversational moves jointly indicates that team
meeting quality improved when there were MT-mediated exchanges of subgroup
conversation logs as opposed to no exchanges. We conclude with reflections on
when and how MT could be applied to enhance global teamwork across a language
barrier.",https://github.com/awslabs/sockeye,4903
d197e36a-4e40-465c-b96b-7828a28f77e3,Are Synonym Substitution Attacks Really Synonym Substitution Attacks?,0.357494,"In this paper, we explore the following question: Are synonym substitution
attacks really synonym substitution attacks (SSAs)? We approach this question
by examining how SSAs replace words in the original sentence and show that
there are still unresolved obstacles that make current SSAs generate invalid
adversarial samples. We reveal that four widely used word substitution methods
generate a large fraction of invalid substitution words that are ungrammatical
or do not preserve the original sentence's semantics. Next, we show that the
semantic and grammatical constraints used in SSAs for detecting invalid word
replacements are highly insufficient in detecting invalid adversarial samples.",https://textattack.readthedocs.io/en/latest/3recipes/models.html,-1
2ea9549b-2633-4d71-8421-6c064200ebbb,A Model for Multi-View Residual Covariances based on Perspective Deformation,0.388786,"In this work, we derive a model for the covariance of the visual residuals in
multi-view SfM, odometry and SLAM setups. The core of our approach is the
formulation of the residual covariances as a combination of geometric and
photometric noise sources. And our key novel contribution is the derivation of
a term modelling how local 2D patches suffer from perspective deformation when
imaging 3D surfaces around a point. Together, these add up to an efficient and
general formulation which not only improves the accuracy of both feature-based
and direct methods, but can also be used to estimate more accurate measures of
the state entropy and hence better founded point visibility thresholds. We
validate our model with synthetic and real data and integrate it into
photometric and feature-based Bundle Adjustment, improving their accuracy with
a negligible overhead.",None,-1
66c0eee0-1755-464d-a84d-f231dc08f257,Dataless Knowledge Fusion by Merging Weights of Language Models,0.991309,"Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.",https://github.com/bloomberg/dataless-model-merging,-1
b6c276ac-f669-4b34-82be-eb577508773b,Beyond the Return: Off-policy Function Estimation under User-specified Error-measuring Distributions,0.749518,"Off-policy evaluation often refers to two related tasks: estimating the
expected return of a policy and estimating its value function (or other
functions of interest, such as density ratios). While recent works on
marginalized importance sampling (MIS) show that the former can enjoy provable
guarantees under realizable function approximation, the latter is only known to
be feasible under much stronger assumptions such as prohibitively expressive
discriminators. In this work, we provide guarantees for off-policy function
estimation under only realizability, by imposing proper regularization on the
MIS objectives. Compared to commonly used regularization in MIS, our
regularizer is much more flexible and can account for an arbitrary
user-specified distribution, under which the learned function will be close to
the groundtruth. We provide exact characterization of the optimal dual solution
that needs to be realized by the discriminator class, which determines the
data-coverage assumption in the case of value-function learning. As another
surprising observation, the regularizer can be altered to relax the
data-coverage requirement, and completely eliminate it in the ideal case with
strong side information.",None,-1
035ae3f8-488f-49df-aa2a-18f2925865c6,Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages,0.11789,"Translation Quality Estimation (QE) is the task of predicting the quality of
machine translation (MT) output without any reference. This task has gained
increasing attention as an important component in the practical applications of
MT. In this paper, we first propose XLMRScore, which is a cross-lingual
counterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric
can be used as a simple unsupervised QE method, nevertheless facing two issues:
firstly, the untranslated tokens leading to unexpectedly high translation
scores, and secondly, the issue of mismatching errors between source and
hypothesis tokens when applying the greedy matching in XLMRScore. To mitigate
these issues, we suggest replacing untranslated words with the unknown token
and the cross-lingual alignment of the pre-trained model to represent aligned
words closer to each other, respectively. We evaluate the proposed method on
four low-resource language pairs of the WMT21 QE shared task, as well as a new
English$\rightarrow$Persian (En-Fa) test dataset introduced in this paper.
Experiments show that our method could get comparable results with the
supervised baseline for two zero-shot scenarios, i.e., with less than 0.01
difference in Pearson correlation, while outperforming unsupervised rivals in
all the low-resource language pairs for above 8%, on average.",https://github.com/fatemeh-azadi/Unsupervised-QE,1410
3a91b249-cf1e-4bce-9be4-f8abd1c18d88,Bilingual Synchronization: Restoring Translational Relationships with Editing Operations,0.362788,"Machine Translation (MT) is usually viewed as a one-shot process that
generates the target language equivalent of some source text from scratch. We
consider here a more general setting which assumes an initial target sequence,
that must be transformed into a valid translation of the source, thereby
restoring parallelism between source and target. For this bilingual
synchronization task, we consider several architectures (both autoregressive
and non-autoregressive) and training regimes, and experiment with multiple
practical settings such as simulated interactive MT, translating with
Translation Memory (TM) and TM cleaning. Our results suggest that one single
generic edit-based system, once fine-tuned, can compare with, or even
outperform, dedicated systems specifically trained for these tasks.",None,7732
7515fd8b-cc7e-43b5-aeb8-35f012a9296a,AGQA 2.0: An Updated Benchmark for Compositional Spatio-Temporal Reasoning,0.562196,"Prior benchmarks have analyzed models' answers to questions about videos in
order to measure visual compositional reasoning. Action Genome Question
Answering (AGQA) is one such benchmark. AGQA provides a training/test split
with balanced answer distributions to reduce the effect of linguistic biases.
However, some biases remain in several AGQA categories. We introduce AGQA 2.0,
a version of this benchmark with several improvements, most namely a stricter
balancing procedure. We then report results on the updated benchmark for all
experiments.",None,-1
43a09723-4e75-43e7-90ed-dd91a6b81a4f,Continual Learning Approaches for Anomaly Detection,0.116383,"Anomaly Detection is a relevant problem that arises in numerous real-world
applications, especially when dealing with images. However, there has been
little research for this task in the Continual Learning setting. In this work,
we introduce a novel approach called SCALE (SCALing is Enough) to perform
Compressed Replay in a framework for Anomaly Detection in Continual Learning
setting. The proposed technique scales and compresses the original images using
a Super Resolution model which, to the best of our knowledge, is studied for
the first time in the Continual Learning setting. SCALE can achieve a high
level of compression while maintaining a high level of image reconstruction
quality. In conjunction with other Anomaly Detection approaches, it can achieve
optimal results. To validate the proposed approach, we use a real-world dataset
of images with pixel-based anomalies, with the scope to provide a reliable
benchmark for Anomaly Detection in the context of Continual Learning, serving
as a foundation for further advancements in the field.",https://github.com/dallepezze/adcl_scale,-1
7a1236d4-fb85-4c93-85a0-bc7f363fbee7,Relation-Aware Language-Graph Transformer for Question Answering,0.197294,"Question Answering (QA) is a task that entails reasoning over natural
language contexts, and many relevant works augment language models (LMs) with
graph neural networks (GNNs) to encode the Knowledge Graph (KG) information.
However, most existing GNN-based modules for QA do not take advantage of rich
relational information of KGs and depend on limited information interaction
between the LM and the KG. To address these issues, we propose Question
Answering Transformer (QAT), which is designed to jointly reason over language
and graphs with respect to entity relations in a unified manner. Specifically,
QAT constructs Meta-Path tokens, which learn relation-centric embeddings based
on diverse structural and semantic relations. Then, our Relation-Aware
Self-Attention module comprehensively integrates different modalities via the
Cross-Modal Relative Position Bias, which guides information exchange between
relevant entites of different modalities. We validate the effectiveness of QAT
on commonsense question answering datasets like CommonsenseQA and OpenBookQA,
and on a medical question answering dataset, MedQA-USMLE. On all the datasets,
our method achieves state-of-the-art performance. Our code is available at
http://github.com/mlvlab/QAT.",None,-1
08831fb6-4171-40aa-b754-0130931a4cc0,WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation,0.513385,"Existing few-shot image generation approaches typically employ fusion-based
strategies, either on the image or the feature level, to produce new images.
However, previous approaches struggle to synthesize high-frequency signals with
fine details, deteriorating the synthesis quality. To address this, we propose
WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we
disentangle encoded features into multiple frequency components and perform
low-frequency skip connections to preserve outline and structural information.
Then we alleviate the generator's struggles of synthesizing fine details by
employing high-frequency skip connections, thus providing informative frequency
information to the generator. Moreover, we utilize a frequency L1-loss on the
generated and real images to further impede frequency information loss.
Extensive experiments demonstrate the effectiveness and advancement of our
method on three datasets. Noticeably, we achieve new state-of-the-art with FID
42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822
respectively on Flower, Animal Faces, and VGGFace. GitHub:
https://github.com/kobeshegu/ECCV2022_WaveGAN",https://github.com/kobeshegu/ECCV2022_WaveGAN,2755
5622bd10-fe98-485d-a72e-f98ded330e2e,Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation,0.365894,"Speech segmentation, which splits long speech into short segments, is
essential for speech translation (ST). Popular VAD tools like WebRTC VAD have
generally relied on pause-based segmentation. Unfortunately, pauses in speech
do not necessarily match sentence boundaries, and sentences can be connected by
a very short pause that is difficult to detect by VAD. In this study, we
propose a speech segmentation method using a binary classification model
trained using a segmented bilingual speech corpus. We also propose a hybrid
method that combines VAD and the above speech segmentation method. Experimental
results revealed that the proposed method is more suitable for cascade and
end-to-end ST systems than conventional segmentation methods. The hybrid
approach further improved the translation performance.",https://github.com/wiseman/py-webrtcvad,-1
6052c2a5-278a-4905-a778-181cba76e4cc,GCS-Q: Quantum Graph Coalition Structure Generation,0.684159,"The problem of generating an optimal coalition structure for a given
coalition game of rational agents is to find a partition that maximizes their
social welfare and is known to be NP-hard. This paper proposes GCS-Q, a novel
quantum-supported solution for Induced Subgraph Games (ISGs) in coalition
structure generation. GCS-Q starts by considering the grand coalition as
initial coalition structure and proceeds by iteratively splitting the
coalitions into two nonempty subsets to obtain a coalition structure with a
higher coalition value. In particular, given an $n$-agent ISG, the GCS-Q solves
the optimal split problem $\mathcal{O} (n)$ times using a quantum annealing
device, exploring $\mathcal{O}(2^n)$ partitions at each step. We show that
GCS-Q outperforms the currently best classical solvers with its runtime in the
order of $n^2$ and an expected worst-case approximation ratio of $93\%$ on
standard benchmark datasets.",None,-1
660f7e16-2cdd-4e29-a7e8-08831cdb4c0e,Towards Uniform Point Distribution in Feature-preserving Point Cloud Filtering,0.315988,"As a popular representation of 3D data, point cloud may contain noise and
need to be filtered before use. Existing point cloud filtering methods either
cannot preserve sharp features or result in uneven point distribution in the
filtered output. To address this problem, this paper introduces a point cloud
filtering method that considers both point distribution and feature
preservation during filtering. The key idea is to incorporate a repulsion term
with a data term in energy minimization. The repulsion term is responsible for
the point distribution, while the data term is to approximate the noisy
surfaces while preserving the geometric features. This method is capable of
handling models with fine-scale features and sharp features. Extensive
experiments show that our method yields better results with a more uniform
point distribution ($5.8\times10^{-5}$ Chamfer Distance on average) in seconds.",None,-1
9a768e34-75b3-43e0-b2f4-000df01a1c26,Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions,0.915768,"Training a high-dimensional simulated agent with an under-specified reward
function often leads the agent to learn physically infeasible strategies that
are ineffective when deployed in the real world. To mitigate these unnatural
behaviors, reinforcement learning practitioners often utilize complex reward
functions that encourage physically plausible behaviors. However, a tedious
labor-intensive tuning process is often required to create hand-designed
rewards which might not easily generalize across platforms and tasks. We
propose substituting complex reward functions with ""style rewards"" learned from
a dataset of motion capture demonstrations. A learned style reward can be
combined with an arbitrary task reward to train policies that perform tasks
using naturalistic strategies. These natural strategies can also facilitate
transfer to the real world. We build upon Adversarial Motion Priors -- an
approach from the computer graphics domain that encodes a style reward from a
dataset of reference motions -- to demonstrate that an adversarial approach to
training policies can produce behaviors that transfer to a real quadrupedal
robot without requiring complex reward functions. We also demonstrate that an
effective style reward can be learned from a few seconds of motion capture data
gathered from a German Shepherd and leads to energy-efficient locomotion
strategies with natural gait transitions.",https://bit.ly/3hpvbD6,-1
733512a1-5372-4bc4-890b-8008db74f4db,FAN-Trans: Online Knowledge Distillation for Facial Action Unit Detection,0.705288,"Due to its importance in facial behaviour analysis, facial action unit (AU)
detection has attracted increasing attention from the research community.
Leveraging the online knowledge distillation framework, we propose the
``FANTrans"" method for AU detection. Our model consists of a hybrid network of
convolution and transformer blocks to learn per-AU features and to model AU
co-occurrences. The model uses a pre-trained face alignment network as the
feature extractor. After further transformation by a small learnable add-on
convolutional subnet, the per-AU features are fed into transformer blocks to
enhance their representation. As multiple AUs often appear together, we propose
a learnable attention drop mechanism in the transformer block to learn the
correlation between the features for different AUs. We also design a classifier
that predicts AU presence by considering all AUs' features, to explicitly
capture label dependencies. Finally, we make the attempt of adapting online
knowledge distillation in the training stage for this task, further improving
the model's performance. Experiments on the BP4D and DISFA datasets
demonstrating the effectiveness of proposed method.",https://github.com/rwightman/pytorch-image-models,-1
03806996-7c42-4c49-b578-84cea4662282,Exploration via Elliptical Episodic Bonuses,0.770185,"In recent years, a number of reinforcement learning (RL) methods have been
proposed to explore complex environments which differ across episodes. In this
work, we show that the effectiveness of these methods critically relies on a
count-based episodic term in their exploration bonus. As a result, despite
their success in relatively simple, noise-free settings, these methods fall
short in more realistic scenarios where the state space is vast and prone to
noise. To address this limitation, we introduce Exploration via Elliptical
Episodic Bonuses (E3B), a new method which extends count-based episodic bonuses
to continuous state spaces and encourages an agent to explore states that are
diverse under a learned embedding within each episode. The embedding is learned
using an inverse dynamics model in order to capture controllable aspects of the
environment. Our method sets a new state-of-the-art across 16 challenging tasks
from the MiniHack suite, without requiring task-specific inductive biases. E3B
also matches existing methods on sparse reward, pixel-based VizDoom
environments, and outperforms existing methods in reward-free exploration on
Habitat, demonstrating that it can scale to high-dimensional pixel-based
observations and realistic environments.",https://github.com/facebookresearch/e3b,-1
9d29d27e-054e-4ef5-95d0-81a7f689a12a,Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control,0.739434,"Abstractive summarization systems leveraging pre-training language models
have achieved superior results on benchmark datasets. However, such models have
been shown to be more prone to hallucinate facts that are unfaithful to the
input context. In this paper, we propose a method to remedy entity-level
extrinsic hallucinations with Entity Coverage Control (ECC). We first compute
entity coverage precision and prepend the corresponding control code for each
training example, which implicitly guides the model to recognize faithfulness
contents in the training phase. We further extend our method via intermediate
fine-tuning on large but noisy data extracted from Wikipedia to unlock
zero-shot summarization. We show that the proposed method leads to more
faithful and salient abstractive summarization in supervised fine-tuning and
zero-shot settings according to our experimental results on three benchmark
datasets XSum, Pubmed, and SAMSum of very different domains and styles.",None,-1
a2e7baed-db4a-4af4-aa08-e21a7f12e233,Can domain adaptation make object recognition work for everyone?,0.295872,"Despite the rapid progress in deep visual recognition, modern computer vision
datasets significantly overrepresent the developed world and models trained on
such datasets underperform on images from unseen geographies. We investigate
the effectiveness of unsupervised domain adaptation (UDA) of such models across
geographies at closing this performance gap. To do so, we first curate two
shifts from existing datasets to study the Geographical DA problem, and
discover new challenges beyond data distribution shift: context shift, wherein
object surroundings may change significantly across geographies, and
subpopulation shift, wherein the intra-category distributions may shift. We
demonstrate the inefficacy of standard DA methods at Geographical DA,
highlighting the need for specialized geographical adaptation solutions to
address the challenge of making object recognition work for everyone.",None,-1
900e2723-856a-467e-8057-13af8bd3de23,A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis,0.408954,"Recently, some span-based methods have achieved encouraging performances for
joint aspect-sentiment analysis, which first extract aspects (aspect
extraction) by detecting aspect boundaries and then classify the span-level
sentiments (sentiment classification). However, most existing approaches either
sequentially extract task-specific features, leading to insufficient feature
interactions, or they encode aspect features and sentiment features in a
parallel manner, implying that feature representation in each task is largely
independent of each other except for input sharing. Both of them ignore the
internal correlations between the aspect extraction and sentiment
classification. To solve this problem, we novelly propose a hierarchical
interactive network (HI-ASA) to model two-way interactions between two tasks
appropriately, where the hierarchical interactions involve two steps:
shallow-level interaction and deep-level interaction. First, we utilize
cross-stitch mechanism to combine the different task-specific features
selectively as the input to ensure proper two-way interactions. Second, the
mutual information technique is applied to mutually constrain learning between
two tasks in the output layer, thus the aspect input and the sentiment input
are capable of encoding features of the other task via backpropagation.
Extensive experiments on three real-world datasets demonstrate HI-ASA's
superiority over baselines.",https://github.com/cwei01/HI-ASA,-1
261da98e-e62b-4e1f-b74a-57b44e57d99d,Learning Perception-Aware Agile Flight in Cluttered Environments,0.77599,"Recently, neural control policies have outperformed existing model-based
planning-and-control methods for autonomously navigating quadrotors through
cluttered environments in minimum time. However, they are not perception aware,
a crucial requirement in vision-based navigation due to the camera's limited
field of view and the underactuated nature of a quadrotor. We propose a
learning-based system that achieves perception-aware, agile flight in cluttered
environments. Our method combines imitation learning with reinforcement
learning (RL) by leveraging a privileged learning-by-cheating framework. Using
RL, we first train a perception-aware teacher policy with full-state
information to fly in minimum time through cluttered environments. Then, we use
imitation learning to distill its knowledge into a vision-based student policy
that only perceives the environment via a camera. Our approach tightly couples
perception and control, showing a significant advantage in computation speed
(10 times faster) and success rate. We demonstrate the closed-loop control
performance using hardware-in-the-loop simulation.",None,-1
090bcd5c-b994-4bac-b99f-30d604441bf9,Understanding DDPM Latent Codes Through Optimal Transport,0.990803,"Diffusion models have recently outperformed alternative approaches to model
the distribution of natural images, such as GANs. Such diffusion models allow
for deterministic sampling via the probability flow ODE, giving rise to a
latent space and an encoder map. While having important practical applications,
such as estimation of the likelihood, the theoretical properties of this map
are not yet fully understood. In the present work, we partially address this
question for the popular case of the VP SDE (DDPM) approach. We show that,
perhaps surprisingly, the DDPM encoder map coincides with the optimal transport
map for common distributions; we support this claim theoretically and by
extensive numerical experiments.",https://github.com/openai/guided-diffusion,-1
8b7b36cd-9faa-478b-9ead-1d21c45bd038,Interpretable Research Replication Prediction via Variational Contextual Consistency Sentence Masking,0.134553,"Research Replication Prediction (RRP) is the task of predicting whether a
published research result can be replicated or not. Building an interpretable
neural text classifier for RRP promotes the understanding of why a research
paper is predicted as replicable or non-replicable and therefore makes its
real-world application more reliable and trustworthy. However, the prior works
on model interpretation mainly focused on improving the model interpretability
at the word/phrase level, which are insufficient especially for long research
papers in RRP. Furthermore, the existing methods cannot utilize a large size of
unlabeled dataset to further improve the model interpretability. To address
these limitations, we aim to build an interpretable neural model which can
provide sentence-level explanations and apply weakly supervised approach to
further leverage the large corpus of unlabeled datasets to boost the
interpretability in addition to improving prediction performance as existing
works have done. In this work, we propose the Variational Contextual
Consistency Sentence Masking (VCCSM) method to automatically extract key
sentences based on the context in the classifier, using both labeled and
unlabeled datasets. Results of our experiments on RRP along with European
Convention of Human Rights (ECHR) datasets demonstrate that VCCSM is able to
improve the model interpretability for the long document classification tasks
using the area over the perturbation curve and post-hoc accuracy as evaluation
metrics.",None,-1
255c57d5-f2b1-4f22-8463-d7778bf9d4a2,Assembly Planning from Observations under Physical Constraints,0.0723811,"This paper addresses the problem of copying an unknown assembly of primitives
with known shape and appearance using information extracted from a single
photograph by an off-the-shelf procedure for object detection and pose
estimation. The proposed algorithm uses a simple combination of physical
stability constraints, convex optimization and Monte Carlo tree search to plan
assemblies as sequences of pick-and-place operations represented by STRIPS
operators. It is efficient and, most importantly, robust to the errors in
object detection and pose estimation unavoidable in any real robotic system.
The proposed approach is demonstrated with thorough experiments on a UR5
manipulator.",None,147997
8754d3b8-80d7-4ab0-8bf1-d4594480ef3a,A Learning-Based Trajectory Planning of Multiple UAVs for AoI Minimization in IoT Networks,0.476605,"Many emerging Internet of Things (IoT) applications rely on information
collected by sensor nodes where the freshness of information is an important
criterion. \textit{Age of Information} (AoI) is a metric that quantifies
information timeliness, i.e., the freshness of the received information or
status update. This work considers a setup of deployed sensors in an IoT
network, where multiple unmanned aerial vehicles (UAVs) serve as mobile relay
nodes between the sensors and the base station. We formulate an optimization
problem to jointly plan the UAVs' trajectory, while minimizing the AoI of the
received messages. This ensures that the received information at the base
station is as fresh as possible. The complex optimization problem is
efficiently solved using a deep reinforcement learning (DRL) algorithm. In
particular, we propose a deep Q-network, which works as a function
approximation to estimate the state-action value function. The proposed scheme
is quick to converge and results in a lower AoI than the random walk scheme.
Our proposed algorithm reduces the average age by approximately $25\%$ and
requires down to $50\%$ less energy when compared to the baseline scheme.",None,-1
8026e806-5af2-4320-ac10-56f54a83d755,Towards PAC Multi-Object Detection and Tracking,0.167169,"Accurately detecting and tracking multi-objects is important for
safety-critical applications such as autonomous navigation. However, it remains
challenging to provide guarantees on the performance of state-of-the-art
techniques based on deep learning. We consider a strategy known as conformal
prediction, which predicts sets of labels instead of a single label; in the
classification and regression settings, these algorithms can guarantee that the
true label lies within the prediction set with high probability. Building on
these ideas, we propose multi-object detection and tracking algorithms that
come with probably approximately correct (PAC) guarantees. They do so by
constructing both a prediction set around each object detection as well as
around the set of edge transitions; given an object, the detection prediction
set contains its true bounding box with high probability, and the edge
prediction set contains its true transition across frames with high
probability. We empirically demonstrate that our method can detect and track
objects with PAC guarantees on the COCO and MOT-17 datasets.",None,-1
00d0c032-215a-491a-b3a5-a1a0976cafae,Predictive Object-Centric Process Monitoring,0.282477,"The automation and digitalization of business processes has resulted in large
amounts of data captured in information systems, which can aid businesses in
understanding their processes better, improve workflows, or provide operational
support. By making predictions about ongoing processes, bottlenecks can be
identified and resources reallocated, as well as insights gained into the state
of a process instance (case). Traditionally, data is extracted from systems in
the form of an event log with a single identifying case notion, such as an
order id for an Order to Cash (O2C) process. However, real processes often have
multiple object types, for example, order, item, and package, so a format that
forces the use of a single case notion does not reflect the underlying
relations in the data. The Object-Centric Event Log (OCEL) format was
introduced to correctly capture this information. The state-of-the-art
predictive methods have been tailored to only traditional event logs. This
thesis shows that a prediction method utilizing Generative Adversarial Networks
(GAN), Long Short-Term Memory (LSTM) architectures, and Sequence to Sequence
models (Seq2seq), can be augmented with the rich data contained in OCEL.
Objects in OCEL can have attributes that are useful in predicting the next
event and timestamp, such as a priority class attribute for an object type
package indicating slower or faster processing. In the metrics of sequence
similarity of predicted remaining events and mean absolute error (MAE) of the
timestamp, the approach in this thesis matches or exceeds previous research,
depending on whether selected object attributes are useful features for the
model. Additionally, this thesis provides a web interface to predict the next
sequence of activities from user input.",None,-1
14242df4-5dc1-47db-aa71-c73d10b4cf8f,Disentangling Architecture and Training for Optical Flow,0.844879,"How important are training details and datasets to recent optical flow models
like RAFT? And do they generalize? To explore these questions, rather than
develop a new model, we revisit three prominent models, PWC-Net, IRR-PWC and
RAFT, with a common set of modern training techniques and datasets, and observe
significant performance gains, demonstrating the importance and generality of
these training details. Our newly trained PWC-Net and IRR-PWC models show
surprisingly large improvements, up to 30% versus original published results on
Sintel and KITTI 2015 benchmarks. They outperform the more recent Flow1D on
KITTI 2015 while being 3x faster during inference. Our newly trained RAFT
achieves an Fl-all score of 4.31% on KITTI 2015, more accurate than all
published optical flow methods at the time of writing. Our results demonstrate
the benefits of separating the contributions of models, training techniques and
datasets when analyzing performance gains of optical flow methods. Our source
code will be publicly available.",https://autoflow-google.github.io,-1
0ab62f02-e6d7-444b-be5e-112c14aba5f2,Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations,0.160778,"Visual Entailment with natural language explanations aims to infer the
relationship between a text-image pair and generate a sentence to explain the
decision-making process. Previous methods rely mainly on a pre-trained
vision-language model to perform the relation inference and a language model to
generate the corresponding explanation. However, the pre-trained
vision-language models mainly build token-level alignment between text and
image yet ignore the high-level semantic alignment between the phrases (chunks)
and visual contents, which is critical for vision-language reasoning. Moreover,
the explanation generator based only on the encoded joint representation does
not explicitly consider the critical decision-making points of relation
inference. Thus the generated explanations are less faithful to visual-language
reasoning. To mitigate these problems, we propose a unified Chunk-aware
Alignment and Lexical Constraint based method, dubbed as CALeC. It contains a
Chunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical
Constraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence
structure inherent in language and various image regions to build chunk-aware
semantic alignment. Relation inferrer uses an attention-based reasoning network
to incorporate the token-level and chunk-level vision-language representations.
LeCG utilizes lexical constraints to expressly incorporate the words or chunks
focused by the relation inferrer into explanation generation, improving the
faithfulness and informativeness of the explanations. We conduct extensive
experiments on three datasets, and experimental results indicate that CALeC
significantly outperforms other competitor models on inference accuracy and
quality of generated explanations.",https://github.com/HITsz-TMG/ExplainableVisualEntailment,13445
98f9ae2e-94f6-4951-a221-ade1c957a69d,"Textual Stylistic Variation: Choices, Genres and Individuals",0.202921,"This chapter argues for more informed target metrics for the statistical
processing of stylistic variation in text collections. Much as operationalised
relevance proved a useful goal to strive for in information retrieval, research
in textual stylistics, whether application oriented or philologically inclined,
needs goals formulated in terms of pertinence, relevance, and utility - notions
that agree with reader experience of text. Differences readers are aware of are
mostly based on utility - not on textual characteristics per se. Mostly,
readers report stylistic differences in terms of genres. Genres, while vague
and undefined, are well-established and talked about: very early on, readers
learn to distinguish genres. This chapter discusses variation given by genre,
and contrasts it to variation occasioned by individual choice.",None,-1
38a2262f-f36d-423b-a9a0-1a41c561793c,Bias-Eliminated Semantic Refinement for Any-Shot Learning,0.166073,"When training samples are scarce, the semantic embedding technique, ie,
describing class labels with attributes, provides a condition to generate
visual features for unseen objects by transferring the knowledge from seen
objects. However, semantic descriptions are usually obtained in an external
paradigm, such as manual annotation, resulting in weak consistency between
descriptions and visual features. In this paper, we refine the coarse-grained
semantic description for any-shot learning tasks, ie, zero-shot learning (ZSL),
generalized zero-shot learning (GZSL), and few-shot learning (FSL). A new
model, namely, the semantic refinement Wasserstein generative adversarial
network (SRWGAN) model, is designed with the proposed multihead representation
and hierarchical alignment techniques. Unlike conventional methods, semantic
refinement is performed with the aim of identifying a bias-eliminated condition
for disjoint-class feature generation and is applicable in both inductive and
transductive settings. We extensively evaluate model performance on six
benchmark datasets and observe state-of-the-art results for any-shot learning;
eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset
and 82.2% harmonic accuracy for the Oxford Flowers (FLO) dataset in the
standard GZSL setting. Various visualizations are also provided to show the
bias-eliminated generation of SRWGAN. Our code is available.",https://github.com/LiangjunFeng/SRWGAN,8266
fc6d0adf-12dd-482d-81d7-9f95e09dc2af,TRoVE: Transforming Road Scene Datasets into Photorealistic Virtual Environments,0.289401,"High-quality structured data with rich annotations are critical components in
intelligent vehicle systems dealing with road scenes. However, data curation
and annotation require intensive investments and yield low-diversity scenarios.
The recently growing interest in synthetic data raises questions about the
scope of improvement in such systems and the amount of manual work still
required to produce high volumes and variations of simulated data. This work
proposes a synthetic data generation pipeline that utilizes existing datasets,
like nuScenes, to address the difficulties and domain-gaps present in simulated
datasets. We show that using annotations and visual cues from existing
datasets, we can facilitate automated multi-modal data generation, mimicking
real scene properties with high-fidelity, along with mechanisms to diversify
samples in a physically meaningful way. We demonstrate improvements in mIoU
metrics by presenting qualitative and quantitative experiments with real and
synthetic data for semantic segmentation on the Cityscapes and KITTI-STEP
datasets. All relevant code and data is released on github
(https://github.com/shubham1810/trove_toolkit).",https://github.com/shubham1810/trove,-1
6694ff34-b902-400c-a365-9936c1c4b927,Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning,0.430551,"The next step for intelligent dialog agents is to escape their role as silent
bystanders and become proactive. Well-defined proactive behavior may improve
human-machine cooperation, as the agent takes a more active role during
interaction and takes off responsibility from the user. However, proactivity is
a double-edged sword because poorly executed pre-emptive actions may have a
devastating effect not only on the task outcome but also on the relationship
with the user. For designing adequate proactive dialog strategies, we propose a
novel approach including both social as well as task-relevant features in the
dialog. Here, the primary goal is to optimize proactive behavior so that it is
task-oriented - this implies high task success and efficiency - while also
being socially effective by fostering user trust. Including both aspects in the
reward function for training a proactive dialog agent using reinforcement
learning showed the benefit of our approach for more successful human-machine
cooperation.",None,-1
ab3e534e-d20d-4c5c-b263-e8c20d8559cb,Neutral Utterances are Also Causes: Enhancing Conversational Causal Emotion Entailment with Social Commonsense Knowledge,0.503415,"Conversational Causal Emotion Entailment aims to detect causal utterances for
a non-neutral targeted utterance from a conversation. In this work, we build
conversations as graphs to overcome implicit contextual modelling of the
original entailment style. Following the previous work, we further introduce
the emotion information into graphs. Emotion information can markedly promote
the detection of causal utterances whose emotion is the same as the targeted
utterance. However, it is still hard to detect causal utterances with different
emotions, especially neutral ones. The reason is that models are limited in
reasoning causal clues and passing them between utterances. To alleviate this
problem, we introduce social commonsense knowledge (CSK) and propose a
Knowledge Enhanced Conversation graph (KEC). KEC propagates the CSK between two
utterances. As not all CSK is emotionally suitable for utterances, we therefore
propose a sentiment-realized knowledge selecting strategy to filter CSK. To
process KEC, we further construct the Knowledge Enhanced Directed Acyclic Graph
networks. Experimental results show that our method outperforms baselines and
infers more causes with different emotions from the targeted utterance.",https://github.com/LeqsNaN/KEC,-1
438dc6dc-1d65-4704-94ba-7aa00fcea128,Explaining Translationese: why are Neural Classifiers Better and what do they Learn?,0.451188,"Recent work has shown that neural feature- and representation-learning, e.g.
BERT, achieves superior performance over traditional manual feature engineering
based approaches, with e.g. SVMs, in translationese classification tasks.
Previous research did not show $(i)$ whether the difference is because of the
features, the classifiers or both, and $(ii)$ what the neural classifiers
actually learn. To address $(i)$, we carefully design experiments that swap
features between BERT- and SVM-based classifiers. We show that an SVM fed with
BERT representations performs at the level of the best BERT classifiers, while
BERT learning and using handcrafted features performs at the level of an SVM
using handcrafted features. This shows that the performance differences are due
to the features. To address $(ii)$ we use integrated gradients and find that
$(a)$ there is indication that information captured by hand-crafted features is
only a subset of what BERT learns, and $(b)$ part of BERT's top performance
results are due to BERT learning topic differences and spurious correlations
with translationese.",None,-1
6eb7a3a1-5c0a-4aae-ae24-a65700bb2cc7,L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors,0.0400932,"In this research, we propose a tiny image segmentation model, L^3U-net, that
works on low-resource edge devices in real-time. We introduce a data folding
technique that reduces inference latency by leveraging the parallel
convolutional layer processing capability of the CNN accelerators. We also
deploy the proposed model to such a device, MAX78000, and the results show that
L^3U-net achieves more than 90% accuracy over two different segmentation
datasets with 10 fps.",None,-1
7187240a-904b-442b-a6a8-10ba8a6b4328,GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields,0.832094,"We propose GazeNeRF, a 3D-aware method for the task of gaze redirection.
Existing gaze redirection methods operate on 2D images and struggle to generate
3D consistent results. Instead, we build on the intuition that the face region
and eyeballs are separate 3D structures that move in a coordinated yet
independent fashion. Our method leverages recent advancements in conditional
image-based neural radiance fields and proposes a two-stream architecture that
predicts volumetric features for the face and eye regions separately. Rigidly
transforming the eye features via a 3D rotation matrix provides fine-grained
control over the desired gaze angle. The final, redirected image is then
attained via differentiable volume compositing. Our experiments show that this
architecture outperforms naively conditioned NeRF baselines as well as previous
state-of-the-art 2D gaze redirection methods in terms of redirection accuracy
and identity preservation.",https://github.com/zllrunning/face-parsing.PyTorch,-1
6685a24a-885d-45dd-8295-c49b2d920b0d,LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval,0.623964,"Retrieval models based on dense representations in semantic space have become
an indispensable branch for first-stage retrieval. These retrievers benefit
from surging advances in representation learning towards compressive global
sequence-level embeddings. However, they are prone to overlook local salient
phrases and entity mentions in texts, which usually play pivot roles in
first-stage retrieval. To mitigate this weakness, we propose to make a dense
retriever align a well-performing lexicon-aware representation model. The
alignment is achieved by weakened knowledge distillations to enlighten the
retriever via two aspects -- 1) a lexicon-augmented contrastive objective to
challenge the dense encoder and 2) a pair-wise rank-consistent regularization
to make dense model's behavior incline to the other. We evaluate our model on
three public benchmarks, which shows that with a comparable lexicon-aware
retriever as the teacher, our proposed dense one can bring consistent and
significant improvements, and even outdo its teacher. In addition, we found our
improvement on the dense retriever is complementary to the standard ranker
distillation, which can further lift state-of-the-art performance.",None,-1
bf427bf7-5fd2-4854-834c-4acb4c104a19,Is a Modular Architecture Enough?,0.805171,"Inspired from human cognition, machine learning systems are gradually
revealing advantages of sparser and more modular architectures. Recent work
demonstrates that not only do some modular architectures generalize well, but
they also lead to better out-of-distribution generalization, scaling
properties, learning speed, and interpretability. A key intuition behind the
success of such systems is that the data generating system for most real-world
settings is considered to consist of sparsely interacting parts, and endowing
models with similar inductive biases will be helpful. However, the field has
been lacking in a rigorous quantitative assessment of such systems because
these real-world data distributions are complex and unknown. In this work, we
provide a thorough assessment of common modular architectures, through the lens
of simple and known modular data distributions. We highlight the benefits of
modularity and sparsity and reveal insights on the challenges faced while
optimizing modular systems. In doing so, we propose evaluation metrics that
highlight the benefits of modularity, the regimes in which these benefits are
substantial, as well as the sub-optimality of current end-to-end learned
modular systems as opposed to their claimed potential.",https://github.com/sarthmit/Mod_Arch,-1
4d466675-b656-46af-8617-e539e72c392c,3D Reconstruction of Sculptures from Single Images via Unsupervised Domain Adaptation on Implicit Models,0.128982,"Acquiring the virtual equivalent of exhibits, such as sculptures, in virtual
reality (VR) museums, can be labour-intensive and sometimes infeasible. Deep
learning based 3D reconstruction approaches allow us to recover 3D shapes from
2D observations, among which single-view-based approaches can reduce the need
for human intervention and specialised equipment in acquiring 3D sculptures for
VR museums. However, there exist two challenges when attempting to use the
well-researched human reconstruction methods: limited data availability and
domain shift. Considering sculptures are usually related to humans, we propose
our unsupervised 3D domain adaptation method for adapting a single-view 3D
implicit reconstruction model from the source (real-world humans) to the target
(sculptures) domain. We have compared the generated shapes with other methods
and conducted ablation studies as well as a user study to demonstrate the
effectiveness of our adaptation method. We also deploy our results in a VR
application.",https://github.com/mrzzy2021/SculptureRecon,-1
5f6911af-a8e7-4738-a9ef-56cd60855423,Estimating and Explaining Model Performance When Both Covariates and Labels Shift,0.579429,"Deployed machine learning (ML) models often encounter new user data that
differs from their training data. Therefore, estimating how well a given model
might perform on the new data is an important step toward reliable ML
applications. This is very challenging, however, as the data distribution can
change in flexible ways, and we may not have any labels on the new data, which
is often the case in monitoring settings. In this paper, we propose a new
distribution shift model, Sparse Joint Shift (SJS), which considers the joint
shift of both labels and a few features. This unifies and generalizes several
existing shift models including label shift and sparse covariate shift, where
only marginal feature or label distribution shifts are considered. We describe
mathematical conditions under which SJS is identifiable. We further propose
SEES, an algorithmic framework to characterize the distribution shift under SJS
and to estimate a model's performance on new data without any labels. We
conduct extensive experiments on several real-world datasets with various ML
models. Across different datasets and distribution shifts, SEES achieves
significant (up to an order of magnitude) shift estimation error improvements
over existing approaches.",None,-1
0caa322b-86eb-4ace-b5f2-d72151dcdd41,SAT: Self-adaptive training for fashion compatibility prediction,0.405306,"This paper presents a self-adaptive training (SAT) model for fashion
compatibility prediction. It focuses on the learning of some hard items, such
as those that share similar color, texture, and pattern features but are
considered incompatible due to the aesthetics or temporal shifts. Specifically,
we first design a method to define hard outfits and a difficulty score (DS) is
defined and assigned to each outfit based on the difficulty in recommending an
item for it. Then, we propose a self-adaptive triplet loss (SATL), where the DS
of the outfit is considered. Finally, we propose a very simple conditional
similarity network combining the proposed SATL to achieve the learning of hard
items in the fashion compatibility prediction. Experiments on the publicly
available Polyvore Outfits and Polyvore Outfits-D datasets demonstrate our
SAT's effectiveness in fashion compatibility prediction. Besides, our SATL can
be easily extended to other conditional similarity networks to improve their
performance.",None,7956
b3a3b2e7-c6c3-4ca8-98a7-e7b1789d71a0,Domain-aware Self-supervised Pre-training for Label-Efficient Meme Analysis,0.352656,"Existing self-supervised learning strategies are constrained to either a
limited set of objectives or generic downstream tasks that predominantly target
uni-modal applications. This has isolated progress for imperative multi-modal
applications that are diverse in terms of complexity and domain-affinity, such
as meme analysis. Here, we introduce two self-supervised pre-training methods,
namely Ext-PIE-Net and MM-SimCLR that (i) employ off-the-shelf multi-modal
hate-speech data during pre-training and (ii) perform self-supervised learning
by incorporating multiple specialized pretext tasks, effectively catering to
the required complex multi-modal representation learning for meme analysis. We
experiment with different self-supervision strategies, including potential
variants that could help learn rich cross-modality representations and evaluate
using popular linear probing on the Hateful Memes task. The proposed solutions
strongly compete with the fully supervised baseline via label-efficient
training while distinctly outperforming them on all three tasks of the Memotion
challenge with 0.18%, 23.64%, and 0.93% performance gain, respectively.
Further, we demonstrate the generalizability of the proposed solutions by
reporting competitive performance on the HarMeme task. Finally, we empirically
establish the quality of the learned representations by analyzing task-specific
learning, using fewer labeled training samples, and arguing that the complexity
of the self-supervision strategy and downstream task at hand are correlated.
Our efforts highlight the requirement of better multi-modal self-supervision
methods involving specialized pretext tasks for efficient fine-tuning and
generalizable performance.",None,-1
2d464100-412f-4358-b351-dd8586c8174f,Natural Language Inference Prompts for Zero-shot Emotion Classification in Text across Corpora,0.316883,"Within textual emotion classification, the set of relevant labels depends on
the domain and application scenario and might not be known at the time of model
development. This conflicts with the classical paradigm of supervised learning
in which the labels need to be predefined. A solution to obtain a model with a
flexible set of labels is to use the paradigm of zero-shot learning as a
natural language inference task, which in addition adds the advantage of not
needing any labeled training data. This raises the question how to prompt a
natural language inference model for zero-shot learning emotion classification.
Options for prompt formulations include the emotion name anger alone or the
statement ""This text expresses anger"". With this paper, we analyze how
sensitive a natural language inference-based zero-shot-learning classifier is
to such changes to the prompt under consideration of the corpus: How carefully
does the prompt need to be selected? We perform experiments on an established
set of emotion datasets presenting different language registers according to
different sources (tweets, events, blogs) with three natural language inference
models and show that indeed the choice of a particular prompt formulation needs
to fit to the corpus. We show that this challenge can be tackled with
combinations of multiple prompts. Such ensemble is more robust across corpora
than individual prompts and shows nearly the same performance as the individual
best prompt for a particular corpus.",https://github.com/fmplaza/zsl_nli_emotion_prompts,-1
14d1856b-efb0-4fc5-96ad-fa912b72d4f3,Improving Subgraph Representation Learning via Multi-View Augmentation,0.0694903,"Subgraph representation learning based on Graph Neural Network (GNN) has
exhibited broad applications in scientific advancements, such as predictions of
molecular structure-property relationships and collective cellular function. In
particular, graph augmentation techniques have shown promising results in
improving graph-based and node-based classification tasks. Still, they have
rarely been explored in the existing GNN-based subgraph representation learning
studies. In this study, we develop a novel multi-view augmentation mechanism to
improve subgraph representation learning models and thus the accuracy of
downstream prediction tasks. Our augmentation technique creates multiple
variants of subgraphs and embeds these variants into the original graph to
achieve highly improved training efficiency, scalability, and accuracy.
Benchmark experiments on several real-world biological and physiological
datasets demonstrate the superiority of our proposed multi-view augmentation
techniques in subgraph representation learning.",None,-1
a06efee5-8914-4b89-b5e4-bc56c9f21427,COLD: A Benchmark for Chinese Offensive Language Detection,0.998362,"Offensive language detection is increasingly crucial for maintaining a
civilized social media platform and deploying pre-trained language models.
However, this task in Chinese is still under exploration due to the scarcity of
reliable datasets. To this end, we propose a benchmark --COLD for Chinese
offensive language analysis, including a Chinese Offensive Language Dataset
--COLDATASET and a baseline detector --COLDETECTOR which is trained on the
dataset. We show that the COLD benchmark contributes to Chinese offensive
language detection which is challenging for existing resources. We then deploy
the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained
language models. We first analyze the offensiveness of existing generative
models and show that these models inevitably expose varying degrees of
offensive issues. Furthermore, we investigate the factors that influence the
offensive generations, and we find that anti-bias contents and keywords
referring to certain groups or revealing negative attitudes trigger offensive
outputs easier.",https://github.com/thu-coai/COLDataset,-1
a4f83f9f-cda6-418f-8fc5-e6580fb67b8a,Coreference Resolution through a seq2seq Transition-Based System,0.508337,"Most recent coreference resolution systems use search algorithms over
possible spans to identify mentions and resolve coreference. We instead present
a coreference resolution system that uses a text-to-text (seq2seq) paradigm to
predict mentions and links jointly. We implement the coreference system as a
transition system and use multilingual T5 as an underlying language model. We
obtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score
for English (a 2.3 higher F1-score than previous work (Dobrovolskii, 2021))
using only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than
previous work) and 74.3 F1-score for Chinese (+5.3). In addition we use the
SemEval-2010 data sets for experiments in the zero-shot setting, a few-shot
setting, and supervised setting using all available training data. We get
substantially higher zero-shot F1-scores for 3 out of 4 languages than previous
approaches and significantly exceed previous supervised state-of-the-art
results for all five tested languages.",https://github.com/google-research/coref_mt5,-1
19f9a8f2-418d-416c-bf70-53ac808867d5,Deep Surrogate Assisted Generation of Environments,0.795734,"Recent progress in reinforcement learning (RL) has started producing
generally capable agents that can solve a distribution of complex environments.
These agents are typically tested on fixed, human-authored environments. On the
other hand, quality diversity (QD) optimization has been proven to be an
effective component of environment generation algorithms, which can generate
collections of high-quality environments that are diverse in the resulting
agent behaviors. However, these algorithms require potentially expensive
simulations of agents on newly generated environments. We propose Deep
Surrogate Assisted Generation of Environments (DSAGE), a sample-efficient QD
environment generation algorithm that maintains a deep surrogate model for
predicting agent behaviors in new environments. Results in two benchmark
domains show that DSAGE significantly outperforms existing QD environment
generation algorithms in discovering collections of environments that elicit
diverse behaviors of a state-of-the-art RL agent and a planning agent. Our
source code and videos are available at https://dsagepaper.github.io/.",https://dsagepaper.github.io/,3297
6056f1ef-6287-4fdb-b4f2-3ab0560f5c0c,Red-Teaming the Stable Diffusion Safety Filter,0.785509,"Stable Diffusion is a recent open-source image generation model comparable to
proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with
a safety filter that aims to prevent generating explicit images. Unfortunately,
the filter is obfuscated and poorly documented. This makes it hard for users to
prevent misuse in their applications, and to understand the filter's
limitations and improve it. We first show that it is easy to generate
disturbing content that bypasses the safety filter. We then reverse-engineer
the filter and find that while it aims to prevent sexual content, it ignores
violence, gore, and other similarly disturbing content. Based on our analysis,
we argue safety measures in future model releases should strive to be fully
open and properly documented to stimulate security contributions from the
community.",https://github.com/huggingface/diffusers/blob/84b9df5/src/diffusers/pipelines/stable_diffusion/safety_checker.py,-1
a6c0fc23-71be-4657-abaa-756b02a453e1,Formal Algorithms for Transformers,0.703051,"This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.",None,-1
1a60ad92-bd33-4223-a52e-efe30818cbae,When to Use What: An In-Depth Comparative Empirical Analysis of OpenIE Systems for Downstream Applications,0.411692,"Open Information Extraction (OpenIE) has been used in the pipelines of
various NLP tasks. Unfortunately, there is no clear consensus on which models
to use in which tasks. Muddying things further is the lack of comparisons that
take differing training sets into account. In this paper, we present an
application-focused empirical survey of neural OpenIE models, training sets,
and benchmarks in an effort to help users choose the most suitable OpenIE
systems for their applications. We find that the different assumptions made by
different models and datasets have a statistically significant effect on
performance, making it important to choose the most appropriate model for one's
applications. We demonstrate the applicability of our recommendations on a
downstream Complex QA application.",None,-1
434805f6-8b77-48d9-b3f2-73ea37a3ec99,Thunder: Thumbnail based Fast Lightweight Image Denoising Network,0.109099,"To achieve promising results on removing noise from real-world images, most
of existing denoising networks are formulated with complex network structure,
making them impractical for deployment. Some attempts focused on reducing the
number of filters and feature channels but suffered from large performance
loss, and a more practical and lightweight denoising network with fast
inference speed is of high demand.
  To this end, a \textbf{Thu}mb\textbf{n}ail based \textbf{D}\textbf{e}noising
Netwo\textbf{r}k dubbed Thunder, is proposed and implemented as a lightweight
structure for fast restoration without comprising the denoising capabilities.
Specifically, the Thunder model contains two newly-established modules:
  (1) a wavelet-based Thumbnail Subspace Encoder (TSE) which can leverage
sub-bands correlation to provide an approximate thumbnail based on the
low-frequent feature; (2) a Subspace Projection based Refine Module (SPR) which
can restore the details for thumbnail progressively based on the subspace
projection approach.
  Extensive experiments have been carried out on two real-world denoising
benchmarks, demonstrating that the proposed Thunder outperforms the existing
lightweight models and achieves competitive performance on PSNR and SSIM when
compared with the complex designs.",None,-1
12772d86-c5bb-4164-8003-ca75ed66337b,Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings,0.668811,"Previous knowledge graph embedding approaches usually map entities to
representations and utilize score functions to predict the target entities, yet
they typically struggle to reason rare or emerging unseen entities. In this
paper, we propose kNN-KGE, a new knowledge graph embedding approach with
pre-trained language models, by linearly interpolating its entity distribution
with k-nearest neighbors. We compute the nearest neighbors based on the
distance in the entity embedding space from the knowledge store. Our approach
can allow rare or emerging entities to be memorized explicitly rather than
implicitly in model parameters. Experimental results demonstrate that our
approach can improve inductive and transductive link prediction results and
yield better performance for low-resource settings with only a few triples,
which might be easier to reason via explicit memory. Code is available at
https://github.com/zjunlp/KNN-KG.",https://github.com/zjunlp/KNN-KG,-1
fa661bd4-3759-4992-bfb3-5b79524d75af,EgoEnv: Human-centric environment representations from egocentric video,0.162704,"First-person video highlights a camera-wearer's activities in the context of
their persistent environment. However, current video understanding approaches
reason over visual features from short video clips that are detached from the
underlying physical space and capture only what is immediately visible. To
facilitate human-centric environment understanding, we present an approach that
links egocentric video and the environment by learning representations that are
predictive of the camera-wearer's (potentially unseen) local surroundings. We
train such models using videos from agents in simulated 3D environments where
the environment is fully observable, and test them on human-captured real-world
videos from unseen environments. On two human-centric video tasks, we show that
models equipped with our environment-aware features consistently outperform
their counterparts with traditional clip features. Moreover, despite being
trained exclusively on simulated videos, our approach successfully handles
real-world videos from HouseTours and Ego4D, and achieves state-of-the-art
results on the Ego4D NLQ challenge. Project page:
https://vision.cs.utexas.edu/projects/ego-env/",https://vision.cs.utexas.edu/projects/ego-env/,-1
9ac0ceee-ef36-4fba-9ec0-dea4a13c102d,Object Detection in Aerial Images: What Improves the Accuracy?,0.0215421,"Object detection is a challenging and popular computer vision problem. The
problem is even more challenging in aerial images due to significant variation
in scale and viewpoint in a diverse set of object categories. Recently, deep
learning-based object detection approaches have been actively explored for the
problem of object detection in aerial images. In this work, we investigate the
impact of Faster R-CNN for aerial object detection and explore numerous
strategies to improve its performance for aerial images. We conduct extensive
experiments on the challenging iSAID dataset. The resulting adapted Faster
R-CNN obtains a significant mAP gain of 4.96% over its vanilla baseline
counterpart on the iSAID validation set, demonstrating the impact of different
strategies investigated in this work.",None,-1
0c3e981d-8794-4b76-a805-9cf0786fcb1b,GODEL: Large-Scale Pre-Training for Goal-Directed Dialog,0.999988,"We introduce GODEL (Grounded Open Dialogue Language Model), a large
pre-trained language model for dialog. In contrast with earlier models such as
DialoGPT, GODEL leverages a new phase of grounded pre-training designed to
better support adapting GODEL to a wide range of downstream dialog tasks that
require information external to the current conversation (e.g., a database or
document) to produce good responses. Experiments against an array of benchmarks
that encompass task-oriented dialog, conversational QA, and grounded
open-domain dialog show that GODEL outperforms state-of-the-art pre-trained
dialog models in few-shot fine-tuning setups, in terms of both human and
automatic evaluation. A novel feature of our evaluation methodology is the
introduction of a notion of utility that assesses the usefulness of responses
(extrinsic evaluation) in addition to their communicative features (intrinsic
evaluation). We show that extrinsic evaluation offers improved inter-annotator
agreement and correlation with automated metrics. Code and data processing
scripts are publicly available.",https://github.com/kingoolz/,-1
d4260d10-4756-4fda-a75c-c72817f80000,DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech,0.965861,"The majority of current Text-to-Speech (TTS) datasets, which are collections
of individual utterances, contain few conversational aspects. In this paper, we
introduce DailyTalk, a high-quality conversational speech dataset designed for
conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the
open-domain dialogue dataset DailyDialog inheriting its annotated attributes.
On top of our dataset, we extend prior work as our baseline, where a
non-autoregressive TTS is conditioned on historical information in a dialogue.
From the baseline experiment with both general and our novel metrics, we show
that DailyTalk can be used as a general TTS dataset, and more than that, our
baseline can represent contextual information from DailyTalk. The DailyTalk
dataset and baseline code are freely available for academic use with CC-BY-SA
4.0 license.",https://github.com/keonlee9420/DailyTalk,-1
47c2dc4d-eb82-4b06-93d1-9c7bce5fc3df,Efficient Hybrid Network: Inducting Scattering Features,0.0546577,"Recent work showed that hybrid networks, which combine predefined and learnt
filters within a single architecture, are more amenable to theoretical analysis
and less prone to overfitting in data-limited scenarios. However, their
performance has yet to prove competitive against the conventional counterparts
when sufficient amounts of training data are available. In an attempt to
address this core limitation of current hybrid networks, we introduce an
Efficient Hybrid Network (E-HybridNet). We show that it is the first scattering
based approach that consistently outperforms its conventional counterparts on a
diverse range of datasets. It is achieved with a novel inductive architecture
that embeds scattering features into the network flow using Hybrid Fusion
Blocks. We also demonstrate that the proposed design inherits the key property
of prior hybrid networks -- an effective generalisation in data-limited
scenarios. Our approach successfully combines the best of the two worlds:
flexibility and power of learnt features and stability and predictability of
scattering representations.",https://github.com/dminskiy/EHybridNet-icpr2022,-1
8b2070b4-4eb1-43cb-aa01-b6af6a5bec75,Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production,0.606095,"Mixture of Experts (MoE) models with conditional execution of sparsely
activated layers have enabled training models with a much larger number of
parameters. As a result, these models have achieved significantly better
quality on various natural language processing tasks including machine
translation. However, it remains challenging to deploy such models in real-life
scenarios due to the large memory requirements and inefficient inference. In
this work, we introduce a highly efficient inference framework with several
optimization approaches to accelerate the computation of sparse models and cut
down the memory consumption significantly. While we achieve up to 26x speed-up
in terms of throughput, we also reduce the model size almost to one eighth of
the original 32-bit float model by quantizing expert weights into 4-bit
integers. As a result, we are able to deploy 136x larger models with 27% less
cost and significantly better quality compared to the existing solutions. This
enables a paradigm shift in deploying large scale multilingual MoE transformers
models replacing the traditional practice of distilling teacher models into
dozens of smaller models per language or task.",https://github.com/NVIDIA/FasterTransformer,-1
1f643a27-dd81-49f1-a5da-602f9eb4bd66,PSFormer: Point Transformer for 3D Salient Object Detection,0.0202627,"We propose PSFormer, an effective point transformer model for 3D salient
object detection. PSFormer is an encoder-decoder network that takes full
advantage of transformers to model the contextual information in both
multi-scale point- and scene-wise manners. In the encoder, we develop a Point
Context Transformer (PCT) module to capture region contextual features at the
point level; PCT contains two different transformers to excavate the
relationship among points. In the decoder, we develop a Scene Context
Transformer (SCT) module to learn context representations at the scene level;
SCT contains both Upsampling-and-Transformer blocks and Multi-context
Aggregation units to integrate the global semantic and multi-level features
from the encoder into the global scene context. Experiments show clear
improvements of PSFormer over its competitors and validate that PSFormer is
more robust to challenging cases such as small objects, multiple objects, and
objects with complex structures.",None,-1
72fbe0e5-ae26-4e82-9bfd-5e47f070f9c6,Towards Better Chinese-centric Neural Machine Translation for Low-resource Languages,0.818251,"The last decade has witnessed enormous improvements in science and
technology, stimulating the growing demand for economic and cultural exchanges
in various countries. Building a neural machine translation (NMT) system has
become an urgent trend, especially in the low-resource setting. However, recent
work tends to study NMT systems for low-resource languages centered on English,
while few works focus on low-resource NMT systems centered on other languages
such as Chinese. To achieve this, the low-resource multilingual translation
challenge of the 2021 iFLYTEK AI Developer Competition provides the
Chinese-centric multilingual low-resource NMT tasks, where participants are
required to build NMT systems based on the provided low-resource samples. In
this paper, we present the winner competition system that leverages monolingual
word embeddings data enhancement, bilingual curriculum learning, and
contrastive re-ranking. In addition, a new Incomplete-Trust (In-trust) loss
function is proposed to replace the traditional cross-entropy loss when
training. The experimental results demonstrate that the implementation of these
ideas leads better performance than other state-of-the-art methods. All the
experimental codes are released at:
https://github.com/WENGSYX/Low-resource-text-translation.",https://github.com/WENGSYX/Low-resource-text-translation,-1
a5a0b082-becc-42e1-a5d6-588bb62f38ea,PrivHAR: Recognizing Human Actions From Privacy-preserving Lens,0.656398,"The accelerated use of digital cameras prompts an increasing concern about
privacy and security, particularly in applications such as action recognition.
In this paper, we propose an optimizing framework to provide robust visual
privacy protection along the human action recognition pipeline. Our framework
parameterizes the camera lens to successfully degrade the quality of the videos
to inhibit privacy attributes and protect against adversarial attacks while
maintaining relevant features for activity recognition. We validate our
approach with extensive simulations and hardware experiments.",None,-1
4d25e921-ccbc-4f83-af8e-75e8f1960a95,Learning Discriminative Representations and Decision Boundaries for Open Intent Detection,0.328646,"Open intent detection is a significant problem in natural language
understanding, which aims to identify the unseen open intent while ensuring
known intent identification performance. However, current methods face two
major challenges. Firstly, they struggle to learn friendly representations to
detect the open intent with prior knowledge of only known intents. Secondly,
there is a lack of an effective approach to obtaining specific and compact
decision boundaries for known intents. To address these issues, this paper
presents an original framework called DA-ADB, which successively learns
distance-aware intent representations and adaptive decision boundaries for open
intent detection. Specifically, we first leverage distance information to
enhance the distinguishing capability of the intent representations. Then, we
design a novel loss function to obtain appropriate decision boundaries by
balancing both empirical and open space risks. Extensive experiments
demonstrate the effectiveness of the proposed distance-aware and boundary
learning strategies. Compared to state-of-the-art methods, our framework
achieves substantial improvements on three benchmark datasets. Furthermore, it
yields robust performance with varying proportions of labeled data and known
categories.",https://github.com/thuiar/TEXTOIR,-1
ae5bb618-35ea-4357-8f14-0b15350c278b,VPAIR -- Aerial Visual Place Recognition and Localization in Large-scale Outdoor Environments,0.624076,"Visual Place Recognition and Visual Localization are essential components in
navigation and mapping for autonomous vehicles especially in GNSS-denied
navigation scenarios. Recent work has focused on ground or close to ground
applications such as self-driving cars or indoor-scenarios and low-altitude
drone flights. However, applications such as Urban Air Mobility require
operations in large-scale outdoor environments at medium to high altitudes. We
present a new dataset named VPAIR. The dataset was recorded on board a light
aircraft flying at an altitude of more than 300 meters above ground capturing
images with a downwardfacing camera. Each image is paired with a high
resolution reference render including dense depth information and 6-DoF
reference poses. The dataset covers a more than one hundred kilometers long
trajectory over various types of challenging landscapes, e.g. urban, farmland
and forests. Experiments on this dataset illustrate the challenges introduced
by the change in perspective to a bird's eye view such as in-plane rotations.",https://github.com/AerVisLoc/vpair,-1
332ba513-6202-4372-9be8-2866a14101aa,Deep Hyperspectral-Depth Reconstruction Using Single Color-Dot Projection,0.337256,"Depth reconstruction and hyperspectral reflectance reconstruction are two
active research topics in computer vision and image processing. Conventionally,
these two topics have been studied separately using independent imaging setups
and there is no existing method which can acquire depth and spectral
reflectance simultaneously in one shot without using special hardware. In this
paper, we propose a novel single-shot hyperspectral-depth reconstruction method
using an off-the-shelf RGB camera and projector. Our method is based on a
single color-dot projection, which simultaneously acts as structured light for
depth reconstruction and spatially-varying color illuminations for
hyperspectral reflectance reconstruction. To jointly reconstruct the depth and
the hyperspectral reflectance from a single color-dot image, we propose a novel
end-to-end network architecture that effectively incorporates a geometric
color-dot pattern loss and a photometric hyperspectral reflectance loss.
Through the experiments, we demonstrate that our hyperspectral-depth
reconstruction method outperforms the combination of an existing
state-of-the-art single-shot hyperspectral reflectance reconstruction method
and depth reconstruction method.",http://www.ok.sc.e.titech.ac.jp/res/DHD/,15690
1e3e426f-1a9d-45a3-b2df-a8ee3f8d62a0,FLAG: Flow-based 3D Avatar Generation from Sparse Observations,0.894228,"To represent people in mixed reality applications for collaboration and
communication, we need to generate realistic and faithful avatar poses.
However, the signal streams that can be applied for this task from head-mounted
devices (HMDs) are typically limited to head pose and hand pose estimates.
While these signals are valuable, they are an incomplete representation of the
human body, making it challenging to generate a faithful full-body avatar. We
address this challenge by developing a flow-based generative model of the 3D
human body from sparse observations, wherein we learn not only a conditional
distribution of 3D human pose, but also a probabilistic mapping from
observations to the latent space from which we can generate a plausible pose
along with uncertainty estimates for the joints. We show that our approach is
not only a strong predictive model, but can also act as an efficient pose prior
in different optimization settings where a good initial latent code plays a
major role.",https://microsoft.github.io/flag,-1
f2382f5e-b26b-421b-b428-14d70e7d6516,"Imitating, Fast and Slow: Robust learning from demonstrations via decision-time planning",0.0606629,"The goal of imitation learning is to mimic expert behavior from
demonstrations, without access to an explicit reward signal. A popular class of
approach infers the (unknown) reward function via inverse reinforcement
learning (IRL) followed by maximizing this reward function via reinforcement
learning (RL). The policies learned via these approaches are however very
brittle in practice and deteriorate quickly even with small test-time
perturbations due to compounding errors. We propose Imitation with Planning at
Test-time (IMPLANT), a new meta-algorithm for imitation learning that utilizes
decision-time planning to correct for compounding errors of any base imitation
policy. In contrast to existing approaches, we retain both the imitation policy
and the rewards model at decision-time, thereby benefiting from the learning
signal of the two components. Empirically, we demonstrate that IMPLANT
significantly outperforms benchmark imitation learning approaches on standard
control environments and excels at zero-shot generalization when subject to
challenging perturbations in test-time dynamics.",None,-1
347b557d-c910-4521-bff6-f1deea05f31b,C2F-TCN: A Framework for Semi and Fully Supervised Temporal Action Segmentation,0.634983,"Temporal action segmentation tags action labels for every frame in an input
untrimmed video containing multiple actions in a sequence. For the task of
temporal action segmentation, we propose an encoder-decoder-style architecture
named C2F-TCN featuring a ""coarse-to-fine"" ensemble of decoder outputs. The
C2F-TCN framework is enhanced with a novel model agnostic temporal feature
augmentation strategy formed by the computationally inexpensive strategy of the
stochastic max-pooling of segments. It produces more accurate and
well-calibrated supervised results on three benchmark action segmentation
datasets. We show that the architecture is flexible for both supervised and
representation learning. In line with this, we present a novel unsupervised way
to learn frame-wise representation from C2F-TCN. Our unsupervised learning
approach hinges on the clustering capabilities of the input features and the
formation of multi-resolution features from the decoder's implicit structure.
Further, we provide the first semi-supervised temporal action segmentation
results by merging representation learning with conventional supervised
learning. Our semi-supervised learning scheme, called
``Iterative-Contrastive-Classify (ICC)'', progressively improves in performance
with more labeled data. The ICC semi-supervised learning in C2F-TCN, with 40%
labeled videos, performs similar to fully supervised counterparts.",None,-1
662941e9-6ff5-4cc6-952d-35bb994220ae,HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python,0.098964,"Large collections of time series data are commonly organized into structures
with different levels of aggregation; examples include product and geographical
groupings. It is often important to ensure that the forecasts are coherent so
that the predicted values at disaggregate levels add up to the aggregate
forecast. The growing interest of the Machine Learning community in
hierarchical forecasting systems indicates that we are in a propitious moment
to ensure that scientific endeavors are grounded on sound baselines. For this
reason, we put forward the HierarchicalForecast library, which contains
preprocessed publicly available datasets, evaluation metrics, and a compiled
set of statistical baseline models. Our Python-based reference framework aims
to bridge the gap between statistical and econometric modeling, and Machine
Learning forecasting research. Code and documentation are available in
https://github.com/Nixtla/hierarchicalforecast.",None,-1
23ccb92d-7fba-4140-9b13-b9cd999b4d5f,Design and Development of Rule-based open-domain Question-Answering System on SQuAD v2.0 Dataset,0.16923,"Human mind is the palace of curious questions that seek answers.
Computational resolution of this challenge is possible through Natural Language
Processing techniques. Statistical techniques like machine learning and deep
learning require a lot of data to train and despite that they fail to tap into
the nuances of language. Such systems usually perform best on close-domain
datasets. We have proposed development of a rule-based open-domain
question-answering system which is capable of answering questions of any domain
from a corresponding context passage. We have used 1000 questions from SQuAD
2.0 dataset for testing the developed system and it gives satisfactory results.
In this paper, we have described the structure of the developed system and have
analyzed the performance.",None,1659
446d426b-c428-43ae-ba56-419bb416a933,Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR Image Retrieval,0.686632,"Spaceborne synthetic aperture radar (SAR) can provide accurate images of the
ocean surface roughness day-or-night in nearly all weather conditions, being an
unique asset for many geophysical applications. Considering the huge amount of
data daily acquired by satellites, automated techniques for physical features
extraction are needed. Even if supervised deep learning methods attain
state-of-the-art results, they require great amount of labeled data, which are
difficult and excessively expensive to acquire for ocean SAR imagery. To this
end, we use the subaperture decomposition (SD) algorithm to enhance the
unsupervised learning retrieval on the ocean surface, empowering ocean
researchers to search into large ocean databases. We empirically prove that SD
improve the retrieval precision with over 20% for an unsupervised transformer
auto-encoder network. Moreover, we show that SD brings important performance
boost when Doppler centroid images are used as input data, leading the way to
new unsupervised physics guided retrieval algorithms.",None,-1
3bc98672-f6e1-4502-92ef-50d001b8c2d7,Consent as a Foundation for Responsible Autonomy,0.692958,"This paper focuses on a dynamic aspect of responsible autonomy, namely, to
make intelligent agents be responsible at run time. That is, it considers
settings where decision making by agents impinges upon the outcomes perceived
by other agents. For an agent to act responsibly, it must accommodate the
desires and other attitudes of its users and, through other agents, of their
users.
  The contribution of this paper is twofold. First, it provides a conceptual
analysis of consent, its benefits and misuses, and how understanding consent
can help achieve responsible autonomy. Second, it outlines challenges for AI
(in particular, for agents and multiagent systems) that merit investigation to
form as a basis for modeling consent in multiagent systems and applying consent
to achieve responsible autonomy.",None,31895
f7c364e4-4ef1-4af8-a646-19f6a0d2a20f,Holistic Transformer: A Joint Neural Network for Trajectory Prediction and Decision-Making of Autonomous Vehicles,0.453538,"Trajectory prediction and behavioral decision-making are two important tasks
for autonomous vehicles that require good understanding of the environmental
context; behavioral decisions are better made by referring to the outputs of
trajectory predictions. However, most current solutions perform these two tasks
separately. Therefore, a joint neural network that combines multiple cues is
proposed and named as the holistic transformer to predict trajectories and make
behavioral decisions simultaneously. To better explore the intrinsic
relationships between cues, the network uses existing knowledge and adopts
three kinds of attention mechanisms: the sparse multi-head type for reducing
noise impact, feature selection sparse type for optimally using partial prior
knowledge, and multi-head with sigmoid activation type for optimally using
posteriori knowledge. Compared with other trajectory prediction models, the
proposed model has better comprehensive performance and good interpretability.
Perceptual noise robustness experiments demonstrate that the proposed model has
good noise robustness. Thus, simultaneous trajectory prediction and behavioral
decision-making combining multiple cues can reduce computational costs and
enhance semantic relationships between scenes and agents.",None,-1
8811ce7d-69da-4bd6-b62f-4f1506ffa36d,Can Visual Context Improve Automatic Speech Recognition for an Embodied Agent?,0.282516,"The usage of automatic speech recognition (ASR) systems are becoming
omnipresent ranging from personal assistant to chatbots, home, and industrial
automation systems, etc. Modern robots are also equipped with ASR capabilities
for interacting with humans as speech is the most natural interaction modality.
However, ASR in robots faces additional challenges as compared to a personal
assistant. Being an embodied agent, a robot must recognize the physical
entities around it and therefore reliably recognize the speech containing the
description of such entities. However, current ASR systems are often unable to
do so due to limitations in ASR training, such as generic datasets and
open-vocabulary modeling. Also, adverse conditions during inference, such as
noise, accented, and far-field speech makes the transcription inaccurate. In
this work, we present a method to incorporate a robot's visual information into
an ASR system and improve the recognition of a spoken utterance containing a
visible entity. Specifically, we propose a new decoder biasing technique to
incorporate the visual context while ensuring the ASR output does not degrade
for incorrect context. We achieve a 59% relative reduction in WER from an
unmodified ASR system.",https://github.com/mozilla/TTS,-1
43fe2fef-b1a3-464d-84e1-1044b49d1194,Online Motion Style Transfer for Interactive Character Control,0.0905783,"Motion style transfer is highly desired for motion generation systems for
gaming. Compared to its offline counterpart, the research on online motion
style transfer under interactive control is limited. In this work, we propose
an end-to-end neural network that can generate motions with different styles
and transfer motion styles in real-time under user control. Our approach
eliminates the use of handcrafted phase features, and could be easily trained
and directly deployed in game systems. In the experiment part, we evaluate our
approach from three aspects that are essential for industrial game design:
accuracy, flexibility, and variety, and our model performs a satisfying result.",None,-1
fbfa6ab6-569e-4db2-ac01-50f6bdcd4a00,RepMix: Representation Mixing for Robust Attribution of Synthesized Images,0.779171,"Rapid advances in Generative Adversarial Networks (GANs) raise new challenges
for image attribution; detecting whether an image is synthetic and, if so,
determining which GAN architecture created it. Uniquely, we present a solution
to this task capable of 1) matching images invariant to their semantic content;
2) robust to benign transformations (changes in quality, resolution, shape,
etc.) commonly encountered as images are re-shared online. In order to
formalize our research, a challenging benchmark, Attribution88, is collected
for robust and practical image attribution. We then propose RepMix, our GAN
fingerprinting technique based on representation mixing and a novel loss. We
validate its capability of tracing the provenance of GAN-generated images
invariant to the semantic content of the image and also robust to
perturbations. We show our approach improves significantly from existing GAN
fingerprinting works on both semantic generalization and robustness. Data and
code are available at https://github.com/TuBui/image_attribution.",https://github.com/TuBui/image-attribution,-1
7ab5c308-be92-4aae-817f-d5259c7ac439,Differentiable Logics for Neural Network Training and Verification,0.115972,"The rising popularity of neural networks (NNs) in recent years and their
increasing prevalence in real-world applications have drawn attention to the
importance of their verification. While verification is known to be
computationally difficult theoretically, many techniques have been proposed for
solving it in practice. It has been observed in the literature that by default
neural networks rarely satisfy logical constraints that we want to verify. A
good course of action is to train the given NN to satisfy said constraint prior
to verifying them. This idea is sometimes referred to as continuous
verification, referring to the loop between training and verification. Usually
training with constraints is implemented by specifying a translation for a
given formal logic language into loss functions. These loss functions are then
used to train neural networks. Because for training purposes these functions
need to be differentiable, these translations are called differentiable logics
(DL). This raises several research questions. What kind of differentiable
logics are possible? What difference does a specific choice of DL make in the
context of continuous verification? What are the desirable criteria for a DL
viewed from the point of view of the resulting loss function? In this extended
abstract we will discuss and answer these questions.",None,-1
4122cc56-2b20-406f-9128-8b1d8643ab0c,Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems,0.80854,"Do all instances need inference through the big models for a correct
prediction? Perhaps not; some instances are easy and can be answered correctly
by even small capacity models. This provides opportunities for improving the
computational efficiency of systems. In this work, we present an explorative
study on 'model cascading', a simple technique that utilizes a collection of
models of varying capacities to accurately yet efficiently output predictions.
Through comprehensive experiments in multiple task settings that differ in the
number of models available for cascading (K value), we show that cascading
improves both the computational efficiency and the prediction accuracy. For
instance, in K=3 setting, cascading saves up to 88.93% computation cost and
consistently achieves superior prediction accuracy with an improvement of up to
2.18%. We also study the impact of introducing additional models in the cascade
and show that it further increases the efficiency improvements. Finally, we
hope that our work will facilitate development of efficient NLP systems making
their widespread adoption in real-world applications possible.",None,-1
71712c49-4d99-4779-9ec5-1811812d34d6,Tapping the Potential of Coherence and Syntactic Features in Neural Models for Automatic Essay Scoring,0.104578,"In the prompt-specific holistic score prediction task for Automatic Essay
Scoring, the general approaches include pre-trained neural model, coherence
model, and hybrid model that incorporate syntactic features with neural model.
In this paper, we propose a novel approach to extract and represent essay
coherence features with prompt-learning NSP that shows to match the
state-of-the-art AES coherence model, and achieves the best performance for
long essays. We apply syntactic feature dense embedding to augment BERT-based
model and achieve the best performance for hybrid methodology for AES. In
addition, we explore various ideas to combine coherence, syntactic information
and semantic embeddings, which no previous study has done before. Our combined
model also performs better than the SOTA available for combined model, even
though it does not outperform our syntactic enhanced neural model. We further
offer analyses that can be useful for future study.",None,-1
9a641430-28cd-44b4-914c-ebf9f89dafb6,Adapting the Exploration Rate for Value-of-Information-Based Reinforcement Learning,0.413733,"In this paper, we consider the problem of adjusting the exploration rate when
using value-of-information-based exploration. We do this by converting the
value-of-information optimization into a problem of finding equilibria of a
flow for a changing exploration rate. We then develop an efficient
path-following scheme for converging to these equilibria and hence uncovering
optimal action-selection policies. Under this scheme, the exploration rate is
automatically adapted according to the agent's experiences. Global convergence
is theoretically assured.
  We first evaluate our exploration-rate adaptation on the Nintendo GameBoy
games Centipede and Millipede. We demonstrate aspects of the search process,
like that it yields a hierarchy of state abstractions. We also show that our
approach returns better policies in fewer episodes than conventional search
strategies relying on heuristic, annealing-based exploration-rate adjustments.
We then illustrate that these trends hold for deep, value-of-information-based
agents that learn to play ten simple games and over forty more complicated
games for the Nintendo GameBoy system. Performance either near or well above
the level of human play is observed.",None,-1
37499101-bd78-451c-9b3b-ca60d45f9d30,Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions,0.752118,"Role-oriented dialogue summarization is to generate summaries for different
roles in the dialogue, e.g., merchants and consumers. Existing methods handle
this task by summarizing each role's content separately and thus are prone to
ignore the information from other roles. However, we believe that other roles'
content could benefit the quality of summaries, such as the omitted information
mentioned by other roles. Therefore, we propose a novel role interaction
enhanced method for role-oriented dialogue summarization. It adopts cross
attention and decoder self-attention interactions to interactively acquire
other roles' critical information. The cross attention interaction aims to
select other roles' critical dialogue utterances, while the decoder
self-attention interaction aims to obtain key information from other roles'
summaries. Experimental results have shown that our proposed method
significantly outperforms strong baselines on two public role-oriented dialogue
summarization datasets. Extensive analyses have demonstrated that other roles'
content could help generate summaries with more complete semantics and correct
topic structures.",https://github.com/xiaolinAndy/RODS,15943
4d3a412c-c40e-4646-b20e-730aa2741e7c,Unleashing the Power of Transformer for Graphs,0.0873068,"Despite recent successes in natural language processing and computer vision,
Transformer suffers from the scalability problem when dealing with graphs. The
computational complexity is unacceptable for large-scale graphs, e.g.,
knowledge graphs. One solution is to consider only the near neighbors, which,
however, will lose the key merit of Transformer to attend to the elements at
any distance. In this paper, we propose a new Transformer architecture, named
dual-encoding Transformer (DET). DET has a structural encoder to aggregate
information from connected neighbors and a semantic encoder to focus on
semantically useful distant nodes. In comparison with resorting to multi-hop
neighbors, DET seeks the desired distant neighbors via self-supervised
training. We further find these two encoders can be incorporated to boost each
others' performance. Our experiments demonstrate DET has achieved superior
performance compared to the respective state-of-the-art methods in dealing with
molecules, networks and knowledge graphs with various sizes.",None,-1
7a62e1a5-7712-43c9-8ff4-3126b437d4ab,Ontology-enhanced Prompt-tuning for Few-shot Learning,0.794797,"Few-shot Learning (FSL) is aimed to make predictions based on a limited
number of samples. Structured data such as knowledge graphs and ontology
libraries has been leveraged to benefit the few-shot setting in various tasks.
However, the priors adopted by the existing methods suffer from challenging
knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder
the performance for few-shot learning. In this study, we explore knowledge
injection for FSL with pre-trained language models and propose
ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the
ontology transformation based on the external knowledge graph to address the
knowledge missing issue, which fulfills and converts structure knowledge to
text. We further introduce span-sensitive knowledge injection via a visible
matrix to select informative knowledge to handle the knowledge noise issue. To
bridge the gap between knowledge and text, we propose a collective training
algorithm to optimize representations jointly. We evaluate our proposed
OntoPrompt in three tasks, including relation extraction, event extraction, and
knowledge graph completion, with eight datasets. Experimental results
demonstrate that our approach can obtain better few-shot performance than
baselines.",None,-1
1183dfe0-d3c2-4984-b70b-808bc2dc920e,What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis,0.669387,"Market sentiment analysis on social media content requires knowledge of both
financial markets and social media jargon, which makes it a challenging task
for human raters. The resulting lack of high-quality labeled data stands in the
way of conventional supervised learning methods. Instead, we approach this
problem using semi-supervised learning with a large language model (LLM). Our
pipeline generates weak financial sentiment labels for Reddit posts with an LLM
and then uses that data to train a small model that can be served in
production. We find that prompting the LLM to produce Chain-of-Thought
summaries and forcing it through several reasoning paths helps generate more
stable and accurate labels, while using a regression loss further improves
distillation quality. With only a handful of prompts, the final model performs
on par with existing supervised models. Though production applications of our
model are limited by ethical considerations, the model's competitive
performance points to the great potential of using LLMs for tasks that
otherwise require skill-intensive annotation.",None,-1
0b1cc74b-6861-498c-ab17-3d83e6c7471e,Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts,0.951533,"Recent progress in language model pre-training has led to important
improvements in Named Entity Recognition (NER). Nonetheless, this progress has
been mainly tested in well-formatted documents such as news, Wikipedia, or
scientific articles. In social media the landscape is different, in which it
adds another layer of complexity due to its noisy and dynamic nature. In this
paper, we focus on NER in Twitter, one of the largest social media platforms,
and construct a new NER dataset, TweetNER7, which contains seven entity types
annotated over 11,382 tweets from September 2019 to August 2021. The dataset
was constructed by carefully distributing the tweets over time and taking
representative trends as a basis. Along with the dataset, we provide a set of
language model baselines and perform an analysis on the language model
performance on the task, especially analyzing the impact of different time
periods. In particular, we focus on three important temporal aspects in our
analysis: short-term degradation of NER models over time, strategies to
fine-tune a language model over different periods, and self-labeling as an
alternative to lack of recently-labeled data. TweetNER7 is released publicly
(https://huggingface.co/datasets/tner/tweetner7) along with the models
fine-tuned on it.",https://github.com/asahi417/tner/tree/master/examples/tweetner7_paper,-1
56d692cf-ba7a-41e0-8ee5-0c00097113ac,Synthetic Point Cloud Generation for Class Segmentation Applications,0.0544524,"Maintenance of industrial facilities is a growing hazard due to the
cumbersome process needed to identify infrastructure degradation. Digital Twins
have the potential to improve maintenance by monitoring the continuous digital
representation of infrastructure. However, the time needed to map the existing
geometry makes their use prohibitive. We previously developed class
segmentation algorithms to automate digital twinning, however a vast amount of
annotated point clouds is needed. Currently, synthetic data generation for
automated segmentation is non-existent. We used Helios++ to automatically
segment point clouds from 3D models. Our research has the potential to pave the
ground for efficient industrial class segmentation.",None,-1
b5d12435-c327-4fe7-a780-b495f8a8a2df,HSGNet: Object Re-identification with Hierarchical Similarity Graph Network,0.562526,"Object re-identification method is made up of backbone network, feature
aggregation, and loss function. However, most backbone networks lack a special
mechanism to handle rich scale variations and mine discriminative feature
representations. In this paper, we firstly design a hierarchical similarity
graph module (HSGM) to reduce the conflict of backbone and re-identification
networks. The designed HSGM builds a rich hierarchical graph to mine the
mapping relationships between global-local and local-local. Secondly, we divide
the feature map along with the spatial and channel directions in each
hierarchical graph. The HSGM applies the spatial features and channel features
extracted from different locations as nodes, respectively, and utilizes the
similarity scores between nodes to construct spatial and channel similarity
graphs. During the learning process of HSGM, we utilize a learnable parameter
to re-optimize the importance of each position, as well as evaluate the
correlation between different nodes. Thirdly, we develop a novel hierarchical
similarity graph network (HSGNet) by embedding the HSGM in the backbone
network. Furthermore, HSGM can be easily embedded into backbone networks of any
depth to improve object re-identification ability. Finally, extensive
experiments on three large-scale object datasets demonstrate that the proposed
HSGNet is superior to state-of-the-art object re-identification approaches.",None,-1
1c46fa75-7ed7-4134-9a07-2f9b0381af88,Toxicity Detection for Indic Multilingual Social Media Content,0.0808696,"Toxic content is one of the most critical issues for social media platforms
today. India alone had 518 million social media users in 2020. In order to
provide a good experience to content creators and their audience, it is crucial
to flag toxic comments and the users who post that. But the big challenge is
identifying toxicity in low resource Indic languages because of the presence of
multiple representations of the same text. Moreover, the posts/comments on
social media do not adhere to a particular format, grammar or sentence
structure; this makes the task of abuse detection even more challenging for
multilingual social media platforms. This paper describes the system proposed
by team 'Moj Masti' using the data provided by ShareChat/Moj in \emph{IIIT-D
Multilingual Abusive Comment Identification} challenge. We focus on how we can
leverage multilingual transformer based pre-trained and fine-tuned models to
approach code-mixed/code-switched classification tasks. Our best performing
system was an ensemble of XLM-RoBERTa and MuRIL which achieved a Mean F-1 score
of 0.9 on the test data/leaderboard. We also observed an increase in the
performance by adding transliterated data. Furthermore, using weak metadata,
ensembling and some post-processing techniques boosted the performance of our
system, thereby placing us 1st on the leaderboard.",https://github.com/isi-nlp/uroman,4373
4ce68478-e79e-4daf-8c24-acdba2a44d3a,A Systematic Evaluation of Response Selection for Open Domain Dialogue,0.787324,"Recent progress on neural approaches for language processing has triggered a
resurgence of interest on building intelligent open-domain chatbots. However,
even the state-of-the-art neural chatbots cannot produce satisfying responses
for every turn in a dialog. A practical solution is to generate multiple
response candidates for the same context, and then perform response
ranking/selection to determine which candidate is the best. Previous work in
response selection typically trains response rankers using synthetic data that
is formed from existing dialogs by using a ground truth response as the single
appropriate response and constructing inappropriate responses via random
selection or using adversarial methods. In this work, we curated a dataset
where responses from multiple response generators produced for the same dialog
context are manually annotated as appropriate (positive) and inappropriate
(negative). We argue that such training data better matches the actual use case
examples, enabling the models to learn to rank responses effectively. With this
new dataset, we conduct a systematic evaluation of state-of-the-art methods for
response selection, and demonstrate that both strategies of using multiple
positive candidates and using manually verified hard negative candidates can
bring in significant performance improvement in comparison to using the
adversarial training data, e.g., increase of 3% and 13% in Recall@1 score,
respectively.",https://github.com/golsun/DialogRPT/,-1
8b9a9aa2-dcc1-4747-8fbd-7fc50dcb626b,Zero-Shot Cost Models for Out-of-the-box Learned Cost Prediction,0.49958,"In this paper, we introduce zero-shot cost models which enable learned cost
estimation that generalizes to unseen databases. In contrast to
state-of-the-art workload-driven approaches which require to execute a large
set of training queries on every new database, zero-shot cost models thus allow
to instantiate a learned cost model out-of-the-box without expensive training
data collection. To enable such zero-shot cost models, we suggest a new
learning paradigm based on pre-trained cost models. As core contributions to
support the transfer of such a pre-trained cost model to unseen databases, we
introduce a new model architecture and representation technique for encoding
query workloads as input to those models. As we will show in our evaluation,
zero-shot cost estimation can provide more accurate cost estimates than
state-of-the-art models for a wide range of (real-world) databases without
requiring any query executions on unseen databases. Furthermore, we show that
zero-shot cost models can be used in a few-shot mode that further improves
their quality by retraining them just with a small number of additional
training queries on the unseen database.",None,-1
621159f8-77d5-4859-8cc6-a948a553213f,imitation: Clean Imitation Learning Implementations,0.396492,"imitation provides open-source implementations of imitation and reward
learning algorithms in PyTorch. We include three inverse reinforcement learning
(IRL) algorithms, three imitation learning algorithms and a preference
comparison algorithm. The implementations have been benchmarked against
previous results, and automated tests cover 98% of the code. Moreover, the
algorithms are implemented in a modular fashion, making it simple to develop
novel algorithms in the framework. Our source code, including documentation and
examples, is available at https://github.com/HumanCompatibleAI/imitation",https://github.com/HumanCompatibleAI/imitation,-1
5a374ec8-f171-4cd0-b5b6-0bdf76ac02dd,UniCLIP: Unified Framework for Contrastive Language-Image Pre-training,0.600511,"Pre-training vision-language models with contrastive objectives has shown
promising results that are both scalable to large uncurated datasets and
transferable to many downstream applications. Some following works have
targeted to improve data efficiency by adding self-supervision terms, but
inter-domain (image-text) contrastive loss and intra-domain (image-image)
contrastive loss are defined on individual spaces in those works, so many
feasible combinations of supervision are overlooked. To overcome this issue, we
propose UniCLIP, a Unified framework for Contrastive Language-Image
Pre-training. UniCLIP integrates the contrastive loss of both inter-domain
pairs and intra-domain pairs into a single universal space. The discrepancies
that occur when integrating contrastive loss between different domains are
resolved by the three key components of UniCLIP: (1) augmentation-aware feature
embedding, (2) MP-NCE loss, and (3) domain dependent similarity measure.
UniCLIP outperforms previous vision-language pre-training methods on various
single- and multi-modality downstream tasks. In our experiments, we show that
each component that comprises UniCLIP contributes well to the final
performance.",https://github.com/Sense-GVT/DeCLIP,-1
d99e72aa-09a7-4c18-a128-392c3c8a178f,Memory-free Online Change-point Detection: A Novel Neural Network Approach,0.747131,"Change-point detection (CPD), which detects abrupt changes in the data
distribution, is recognized as one of the most significant tasks in time series
analysis. Despite the extensive literature on offline CPD, unsupervised online
CPD still suffers from major challenges, including scalability, hyperparameter
tuning, and learning constraints. To mitigate some of these challenges, in this
paper, we propose a novel deep learning approach for unsupervised online CPD
from multi-dimensional time series, named Adaptive LSTM-Autoencoder
Change-Point Detection (ALACPD). ALACPD exploits an LSTM-autoencoder-based
neural network to perform unsupervised online CPD. It continuously adapts to
the incoming samples without keeping the previously received input, thus being
memory-free. We perform an extensive evaluation on several real-world time
series CPD benchmarks. We show that ALACPD, on average, ranks first among
state-of-the-art CPD algorithms in terms of quality of the time series
segmentation, and it is on par with the best performer in terms of the accuracy
of the estimated change-points. The implementation of ALACPD is available
online on Github\footnote{\url{https://github.com/zahraatashgahi/ALACPD}}.",https://github.com/zahraatashgahi/ALACPD,-1
c855fed3-0038-42ea-affd-e0028d0fe859,Investigating Bias with a Synthetic Data Generator: Empirical Evidence and Philosophical Interpretation,0.168872,"Machine learning applications are becoming increasingly pervasive in our
society. Since these decision-making systems rely on data-driven learning, risk
is that they will systematically spread the bias embedded in data. In this
paper, we propose to analyze biases by introducing a framework for generating
synthetic data with specific types of bias and their combinations. We delve
into the nature of these biases discussing their relationship to moral and
justice frameworks. Finally, we exploit our proposed synthetic data generator
to perform experiments on different scenarios, with various bias combinations.
We thus analyze the impact of biases on performance and fairness metrics both
in non-mitigated and mitigated machine learning models.",https://github.com/rcrupiISP/ISParity,-1
311e3dea-db3d-4e4c-82cd-a5c9caad02d1,Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns,0.862106,"There is a growing interest in the combined use of NLP and machine learning
methods to predict gaze patterns during naturalistic reading. While promising
results have been obtained through the use of transformer-based language
models, little work has been undertaken to relate the performance of such
models to general text characteristics. In this paper we report on experiments
with two eye-tracking corpora of naturalistic reading and two language models
(BERT and GPT-2). In all experiments, we test effects of a broad spectrum of
features for predicting human reading behavior that fall into five categories
(syntactic complexity, lexical richness, register-based multiword combinations,
readability and psycholinguistic word properties). Our experiments show that
both the features included and the architecture of the transformer-based
language models play a role in predicting multiple eye-tracking measures during
naturalistic reading. We also report the results of experiments aimed at
determining the relative importance of features from different groups using
SP-LIME.",None,-1
682ee121-c5be-45fe-8479-803afe80f3ea,Improved Evaluation and Generation of Grid Layouts using Distance Preservation Quality and Linear Assignment Sorting,0.576825,"Images sorted by similarity enables more images to be viewed simultaneously,
and can be very useful for stock photo agencies or e-commerce applications.
Visually sorted grid layouts attempt to arrange images so that their proximity
on the grid corresponds as closely as possible to their similarity. Various
metrics exist for evaluating such arrangements, but there is low experimental
evidence on correlation between human perceived quality and metric value. We
propose Distance Preservation Quality (DPQ) as a new metric to evaluate the
quality of an arrangement. Extensive user testing revealed stronger correlation
of DPQ with user-perceived quality and performance in image retrieval tasks
compared to other metrics. In addition, we introduce Fast Linear Assignment
Sorting (FLAS) as a new algorithm for creating visually sorted grid layouts.
FLAS achieves very good sorting qualities while improving run time and
computational resources.",https://github.com/Visual-Computing/LAS_FLAS,-1
e70aeb20-95a0-4b6e-83d5-d6441b2466af,Thin-Plate Spline Motion Model for Image Animation,0.99312,"Image animation brings life to the static object in the source image
according to the driving video. Recent works attempt to perform motion transfer
on arbitrary objects through unsupervised methods without using a priori
knowledge. However, it remains a significant challenge for current unsupervised
methods when there is a large pose gap between the objects in the source and
driving images. In this paper, a new end-to-end unsupervised motion transfer
framework is proposed to overcome such issue. Firstly, we propose thin-plate
spline motion estimation to produce a more flexible optical flow, which warps
the feature maps of the source image to the feature domain of the driving
image. Secondly, in order to restore the missing regions more realistically, we
leverage multi-resolution occlusion masks to achieve more effective feature
fusion. Finally, additional auxiliary loss functions are designed to ensure
that there is a clear division of labor in the network modules, encouraging the
network to generate high-quality images. Our method can animate a variety of
objects, including talking faces, human bodies, and pixel animations.
Experiments demonstrate that our method performs better on most benchmarks than
the state of the art with visible improvements in pose-related metrics.",https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model,-1
13e51601-9728-4202-9f09-9c0a6ae99399,Meta-Learning a Cross-lingual Manifold for Semantic Parsing,0.710179,"Localizing a semantic parser to support new languages requires effective
cross-lingual generalization. Recent work has found success with
machine-translation or zero-shot methods although these approaches can struggle
to model how native speakers ask questions. We consider how to effectively
leverage minimal annotated examples in new languages for few-shot cross-lingual
semantic parsing. We introduce a first-order meta-learning algorithm to train a
semantic parser with maximal sample efficiency during cross-lingual transfer.
Our algorithm uses high-resource languages to train the parser and
simultaneously optimizes for cross-lingual generalization for lower-resource
languages. Results across six languages on ATIS demonstrate that our
combination of generalization steps yields accurate semantic parsers sampling
$\le$10% of source training data in each new language. Our approach also trains
a competitive model on Spider using English with generalization to Chinese
similarly sampling $\le$10% of training data.",https://github.com/tomsherborne/xgr,-1
26d55242-afc7-4054-a6b0-e1f9f79f569b,Monotonic Differentiable Sorting Networks,0.94116,"Differentiable sorting algorithms allow training with sorting and ranking
supervision, where only the ordering or ranking of samples is known. Various
methods have been proposed to address this challenge, ranging from optimal
transport-based differentiable Sinkhorn sorting algorithms to making classic
sorting networks differentiable. One problem of current differentiable sorting
methods is that they are non-monotonic. To address this issue, we propose a
novel relaxation of conditional swap operations that guarantees monotonicity in
differentiable sorting networks. We introduce a family of sigmoid functions and
prove that they produce differentiable sorting networks that are monotonic.
Monotonicity ensures that the gradients always have the correct sign, which is
an advantage in gradient-based optimization. We demonstrate that monotonic
differentiable sorting networks improve upon previous differentiable sorting
methods.",https://github.com/Felix-Petersen/diffsort,13631
be655f91-a741-4b0d-9783-50312af3dc52,Twitter-Demographer: A Flow-based Tool to Enrich Twitter Data,0.28938,"Twitter data have become essential to Natural Language Processing (NLP) and
social science research, driving various scientific discoveries in recent
years. However, the textual data alone are often not enough to conduct studies:
especially social scientists need more variables to perform their analysis and
control for various factors. How we augment this information, such as users'
location, age, or tweet sentiment, has ramifications for anonymity and
reproducibility, and requires dedicated effort. This paper describes
Twitter-Demographer, a simple, flow-based tool to enrich Twitter data with
additional information about tweets and users. Twitter-Demographer is aimed at
NLP practitioners and (computational) social scientists who want to enrich
their datasets with aggregated information, facilitating reproducibility, and
providing algorithmic privacy-by-design measures for pseudo-anonymity. We
discuss our design choices, inspired by the flow-based programming paradigm, to
use black-box components that can easily be chained together and extended. We
also analyze the ethical issues related to the use of this tool, and the
built-in measures to facilitate pseudo-anonymity.",https://github.com/MilaNLProc/twitter-demographer,-1
2e639c47-e224-496b-949e-1c140dc1efb3,News Summarization and Evaluation in the Era of GPT-3,0.999999,"The recent success of prompting large language models like GPT-3 has led to a
paradigm shift in NLP research. In this paper, we study its impact on text
summarization, focusing on the classic benchmark domain of news summarization.
First, we investigate how GPT-3 compares against fine-tuned models trained on
large summarization datasets. We show that not only do humans overwhelmingly
prefer GPT-3 summaries, prompted using only a task description, but these also
do not suffer from common dataset-specific issues such as poor factuality.
Next, we study what this means for evaluation, particularly the role of gold
standard test sets. Our experiments show that both reference-based and
reference-free automatic metrics cannot reliably evaluate GPT-3 summaries.
Finally, we evaluate models on a setting beyond generic summarization,
specifically keyword-based summarization, and show how dominant fine-tuning
approaches compare to prompting.
  To support further research, we release: (a) a corpus of 10K generated
summaries from fine-tuned and prompt-based models across 4 standard
summarization benchmarks, (b) 1K human preference judgments comparing different
systems for generic- and keyword-based summarization.",None,5490
27345a4f-463e-4ac7-92c8-54ada880a9b4,MPANet: Multi-Patch Attention For Infrared Small Target object Detection,0.625713,"Infrared small target detection (ISTD) has attracted widespread attention and
been applied in various fields. Due to the small size of infrared targets and
the noise interference from complex backgrounds, the performance of ISTD using
convolutional neural networks (CNNs) is restricted. Moreover, the constriant
that long-distance dependent features can not be encoded by the vanilla CNNs
also impairs the robustness of capturing targets' shapes and locations in
complex scenarios. To this end, a multi-patch attention network (MPANet) based
on the axial-attention encoder and the multi-scale patch branch (MSPB)
structure is proposed. Specially, an axial-attention-improved encoder
architecture is designed to highlight the effective features of small targets
and suppress background noises. Furthermore, the developed MSPB structure fuses
the coarse-grained and fine-grained features from different semantic scales.
Extensive experiments on the SIRST dataset show the superiority performance and
effectiveness of the proposed MPANet compared to the state-of-the-art methods.",None,15602
10e346c8-ff5c-42c0-aeb4-4b94d9eabf2d,SKIPP'D: a SKy Images and Photovoltaic Power Generation Dataset for Short-term Solar Forecasting,0.62498,"Large-scale integration of photovoltaics (PV) into electricity grids is
challenged by the intermittent nature of solar power. Sky-image-based solar
forecasting using deep learning has been recognized as a promising approach to
predicting the short-term fluctuations. However, there are few publicly
available standardized benchmark datasets for image-based solar forecasting,
which limits the comparison of different forecasting models and the exploration
of forecasting methods. To fill these gaps, we introduce SKIPP'D -- a SKy
Images and Photovoltaic Power Generation Dataset. The dataset contains three
years (2017-2019) of quality-controlled down-sampled sky images and PV power
generation data that is ready-to-use for short-term solar forecasting using
deep learning. In addition, to support the flexibility in research, we provide
the high resolution, high frequency sky images and PV power generation data as
well as the concurrent sky video footage. We also include a code base
containing data processing scripts and baseline model implementations for
researchers to reproduce our previous work and accelerate their research in
solar forecasting.",https://github.com/yuhao-nie/Stanford-solar-forecasting-dataset,-1
c0b2366b-2602-4b3c-90ce-6d5a0f4f08fd,Lightweight Monocular Depth Estimation through Guided Decoding,0.565896,"We present a lightweight encoder-decoder architecture for monocular depth
estimation, specifically designed for embedded platforms. Our main contribution
is the Guided Upsampling Block (GUB) for building the decoder of our model.
Motivated by the concept of guided image filtering, GUB relies on the image to
guide the decoder on upsampling the feature representation and the depth map
reconstruction, achieving high resolution results with fine-grained details.
Based on multiple GUBs, our model outperforms the related methods on the NYU
Depth V2 dataset in terms of accuracy while delivering up to 35.1 fps on the
NVIDIA Jetson Nano and up to 144.5 fps on the NVIDIA Xavier NX. Similarly, on
the KITTI dataset, inference is possible with up to 23.7 fps on the Jetson Nano
and 102.9 fps on the Xavier NX. Our code and models are made publicly
available.",https://github.com/mic-rud/GuidedDecoding,-1
74f9b1e2-a0d2-416f-b4fd-42cede3ab303,Extending Temporal Data Augmentation for Video Action Recognition,0.264164,"Pixel space augmentation has grown in popularity in many Deep Learning areas,
due to its effectiveness, simplicity, and low computational cost. Data
augmentation for videos, however, still remains an under-explored research
topic, as most works have been treating inputs as stacks of static images
rather than temporally linked series of data. Recently, it has been shown that
involving the time dimension when designing augmentations can be superior to
its spatial-only variants for video action recognition. In this paper, we
propose several novel enhancements to these techniques to strengthen the
relationship between the spatial and temporal domains and achieve a deeper
level of perturbations. The video action recognition results of our techniques
outperform their respective variants in Top-1 and Top-5 settings on the UCF-101
and the HMDB-51 datasets.",None,-1
e7cc1d9f-523e-47bd-a4f6-83b92fe3de25,LPAttack: A Feasible Annotation Scheme for Capturing Logic Pattern of Attacks in Arguments,0.115664,"In argumentative discourse, persuasion is often achieved by refuting or
attacking others arguments. Attacking is not always straightforward and often
comprise complex rhetorical moves such that arguers might agree with a logic of
an argument while attacking another logic. Moreover, arguer might neither deny
nor agree with any logics of an argument, instead ignore them and attack the
main stance of the argument by providing new logics and presupposing that the
new logics have more value or importance than the logics present in the
attacked argument. However, no existing studies in the computational
argumentation capture such complex rhetorical moves in attacks or the
presuppositions or value judgements in them. In order to address this gap, we
introduce LPAttack, a novel annotation scheme that captures the common modes
and complex rhetorical moves in attacks along with the implicit presuppositions
and value judgements in them. Our annotation study shows moderate
inter-annotator agreement, indicating that human annotation for the proposed
scheme is feasible. We publicly release our annotated corpus and the annotation
guidelines.",None,-1
f8c3359e-7e3c-42ee-9d70-108c343eef58,Optimizing Data Collection for Machine Learning,0.864665,"Modern deep learning systems require huge data sets to achieve impressive
performance, but there is little guidance on how much or what kind of data to
collect. Over-collecting data incurs unnecessary present costs, while
under-collecting may incur future costs and delay workflows. We propose a new
paradigm for modeling the data collection workflow as a formal optimal data
collection problem that allows designers to specify performance targets,
collection costs, a time horizon, and penalties for failing to meet the
targets. Additionally, this formulation generalizes to tasks requiring multiple
data sources, such as labeled and unlabeled data used in semi-supervised
learning. To solve our problem, we develop Learn-Optimize-Collect (LOC), which
minimizes expected future collection costs. Finally, we numerically compare our
framework to the conventional baseline of estimating data requirements by
extrapolating from neural scaling laws. We significantly reduce the risks of
failing to meet desired performance targets on several classification,
segmentation, and detection tasks, while maintaining low total collection
costs.",None,-1
b7663622-ee2d-4045-a04d-aa207f88d823,Unsupervised Domain Adaptation for COVID-19 Information Service with Contrastive Adversarial Domain Mixup,0.377368,"In the real-world application of COVID-19 misinformation detection, a
fundamental challenge is the lack of the labeled COVID data to enable
supervised end-to-end training of the models, especially at the early stage of
the pandemic. To address this challenge, we propose an unsupervised domain
adaptation framework using contrastive learning and adversarial domain mixup to
transfer the knowledge from an existing source data domain to the target
COVID-19 data domain. In particular, to bridge the gap between the source
domain and the target domain, our method reduces a radial basis function (RBF)
based discrepancy between these two domains. Moreover, we leverage the power of
domain adversarial examples to establish an intermediate domain mixup, where
the latent representations of the input text from both domains could be mixed
during the training process. Extensive experiments on multiple real-world
datasets suggest that our method can effectively adapt misinformation detection
systems to the unseen COVID-19 target domain with significant improvements
compared to the state-of-the-art baselines.",None,-1
385fb687-7c7e-426b-97b2-2f3e7c707e59,TALM: Tool Augmented Language Models,0.99359,"Transformer based language models (LMs) demonstrate increasing performance
with scale across a wide variety of tasks. Scale alone however cannot enable
models to solve tasks that require access to ephemeral, changing, or private
data that was unavailable at training time. Many useful tasks may also benefit
from LMs being able to access APIs that read or modify state. In this work, we
present Tool Augmented Language Models (TALM), combining a text-only approach
to augment language models with non-differentiable tools, and an iterative
""self-play"" technique to bootstrap performance starting from few tool
demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA
task and a reasoning oriented math task with simple tools. At a given model
scale, TALM significantly outperforms non-augmented LMs. We further demonstrate
that TALM successfully performs out-of-distribution inferences on both QA and
math tasks, where non-augmented LMs fail. Our results suggest that Tool
Augmented Language Models are a promising direction to enrich LMs'
capabilities, with less dependence on scale.",None,-1
5cdb7db5-8850-4d26-91b5-d54edb289edb,Constrained Optimization with Dynamic Bound-scaling for Effective NLPBackdoor Defense,0.822469,"We develop a novel optimization method for NLPbackdoor inversion. We leverage
a dynamically reducing temperature coefficient in the softmax function to
provide changing loss landscapes to the optimizer such that the process
gradually focuses on the ground truth trigger, which is denoted as a one-hot
value in a convex hull. Our method also features a temperature rollback
mechanism to step away from local optimals, exploiting the observation that
local optimals can be easily deter-mined in NLP trigger inversion (while not in
general optimization). We evaluate the technique on over 1600 models (with
roughly half of them having injected backdoors) on 3 prevailing NLP tasks, with
4 different backdoor attacks and 7 architectures. Our results show that the
technique is able to effectively and efficiently detect and remove backdoors,
outperforming 4 baseline methods.",None,-1
3847c8e5-eb76-47fa-9d82-76268737de53,Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging,0.956756,"Training vision or language models on large datasets can take days, if not
weeks. We show that averaging the weights of the k latest checkpoints, each
collected at the end of an epoch, can speed up the training progression in
terms of loss and accuracy by dozens of epochs, corresponding to time savings
up to ~68 and ~30 GPU hours when training a ResNet50 on ImageNet and
RoBERTa-Base model on WikiText-103, respectively. We also provide the code and
model checkpoint trajectory to reproduce the results and facilitate research on
reusing historical weights for faster convergence.",https://github.com/jeankaddour/lawa,632
16f023e3-a6a2-4470-9eb1-6080b95ab37c,GIFS: Neural Implicit Function for General Shape Representation,0.844034,"Recent development of neural implicit function has shown tremendous success
on high-quality 3D shape reconstruction. However, most works divide the space
into inside and outside of the shape, which limits their representing power to
single-layer and watertight shapes. This limitation leads to tedious data
processing (converting non-watertight raw data to watertight) as well as the
incapability of representing general object shapes in the real world. In this
work, we propose a novel method to represent general shapes including
non-watertight shapes and shapes with multi-layer surfaces. We introduce
General Implicit Function for 3D Shape (GIFS), which models the relationships
between every two points instead of the relationships between points and
surfaces. Instead of dividing 3D space into predefined inside-outside regions,
GIFS encodes whether two points are separated by any surface. Experiments on
ShapeNet show that GIFS outperforms previous state-of-the-art methods in terms
of reconstruction quality, rendering efficiency, and visual fidelity. Project
page is available at https://jianglongye.com/gifs .",https://jianglongye.com/gifs,25908
eaa2d5df-eb79-4805-a8e0-e977899a48d7,Disentangling visual and written concepts in CLIP,0.513664,"The CLIP network measures the similarity between natural text and images; in
this work, we investigate the entanglement of the representation of word images
and natural images in its image encoder. First, we find that the image encoder
has an ability to match word images with natural images of scenes described by
those words. This is consistent with previous research that suggests that the
meaning and the spelling of a word might be entangled deep within the network.
On the other hand, we also find that CLIP has a strong ability to match
nonsense words, suggesting that processing of letters is separated from
processing of their meaning. To explicitly determine whether the spelling
capability of CLIP is separable, we devise a procedure for identifying
representation subspaces that selectively isolate or eliminate spelling
capabilities. We benchmark our methods against a range of retrieval tasks, and
we also test them by measuring the appearance of text in CLIP-guided generated
images. We find that our methods are able to cleanly separate spelling
capabilities of CLIP from the visual processing of natural images.",None,-1
52a685e0-a344-44d5-803d-a3325470fe81,Task-Balanced Distillation for Object Detection,0.518518,"Mainstream object detectors are commonly constituted of two sub-tasks,
including classification and regression tasks, implemented by two parallel
heads. This classic design paradigm inevitably leads to inconsistent spatial
distributions between classification score and localization quality (IOU).
Therefore, this paper alleviates this misalignment in the view of knowledge
distillation. First, we observe that the massive teacher achieves a higher
proportion of harmonious predictions than the lightweight student. Based on
this intriguing observation, a novel Harmony Score (HS) is devised to estimate
the alignment of classification and regression qualities. HS models the
relationship between two sub-tasks and is seen as prior knowledge to promote
harmonious predictions for the student. Second, this spatial misalignment will
result in inharmonious region selection when distilling features. To alleviate
this problem, a novel Task-decoupled Feature Distillation (TFD) is proposed by
flexibly balancing the contributions of classification and regression tasks.
Eventually, HD and TFD constitute the proposed method, named Task-Balanced
Distillation (TBD). Extensive experiments demonstrate the considerable
potential and generalization of the proposed method. Specifically, when
equipped with TBD, RetinaNet with ResNet-50 achieves 41.0 mAP under the COCO
benchmark, outperforming the recent FGD and FRS.",None,-1
3ef7e241-f643-4f84-afcb-5e785fc8969c,Don't Say What You Don't Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search,0.684194,"Abstractive summarization systems today produce fluent and relevant output,
but often ""hallucinate"" statements not supported by the source text. We analyze
the connection between hallucinations and training data, and find evidence that
models hallucinate because they train on target summaries that are unsupported
by the source. Based on our findings, we present PINOCCHIO, a new decoding
method that improves the consistency of a transformer-based abstractive
summarizer by constraining beam search to avoid hallucinations. Given the model
states and outputs at a given step, PINOCCHIO detects likely model
hallucinations based on various measures of attribution to the source text.
PINOCCHIO backtracks to find more consistent output, and can opt to produce no
summary at all when no consistent generation can be found. In experiments, we
find that PINOCCHIO improves the consistency of generation (in terms of F1) by
an average of~67% on two abstractive summarization datasets.",https://github.com/allenai/pinocchio,14304
9e5c98f5-8b8b-4db2-a15f-4ae9bfc73f2d,Semi-analytical Industrial Cooling System Model for Reinforcement Learning,0.604499,"We present a hybrid industrial cooling system model that embeds analytical
solutions within a multi-physics simulation. This model is designed for
reinforcement learning (RL) applications and balances simplicity with
simulation fidelity and interpretability. The model's fidelity is evaluated
against real world data from a large scale cooling system. This is followed by
a case study illustrating how the model can be used for RL research. For this,
we develop an industrial task suite that allows specifying different problem
settings and levels of complexity, and use it to evaluate the performance of
different RL algorithms.",https://github.com/deepmind/acme/tree/master/acme/agents/tf/dmpo,-1
573250c3-b569-4110-8a8b-0c6a50b3f150,Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge Graph Completion,0.834493,"Knowledge Graph Completion (KGC) has been recently extended to multiple
knowledge graph (KG) structures, initiating new research directions, e.g.
static KGC, temporal KGC and few-shot KGC. Previous works often design KGC
models closely coupled with specific graph structures, which inevitably results
in two drawbacks: 1) structure-specific KGC models are mutually incompatible;
2) existing KGC methods are not adaptable to emerging KGs. In this paper, we
propose KG-S2S, a Seq2Seq generative framework that could tackle different
verbalizable graph structures by unifying the representation of KG facts into
""flat"" text, regardless of their original form. To remedy the KG structure
information loss from the ""flat"" text, we further improve the input
representations of entities and relations, and the inference algorithm in
KG-S2S. Experiments on five benchmarks show that KG-S2S outperforms many
competitive baselines, setting new state-of-the-art performance. Finally, we
analyze KG-S2S's ability on the different relations and the Non-entity
Generations.",https://github.com/chenchens190009/KG-S2S,-1
66871836-01c8-4e27-b3b4-023a2ce249f2,DCVQE: A Hierarchical Transformer for Video Quality Assessment,0.0690705,"The explosion of user-generated videos stimulates a great demand for
no-reference video quality assessment (NR-VQA). Inspired by our observation on
the actions of human annotation, we put forward a Divide and Conquer Video
Quality Estimator (DCVQE) for NR-VQA. Starting from extracting the frame-level
quality embeddings (QE), our proposal splits the whole sequence into a number
of clips and applies Transformers to learn the clip-level QE and update the
frame-level QE simultaneously; another Transformer is introduced to combine the
clip-level QE to generate the video-level QE. We call this hierarchical
combination of Transformers as a Divide and Conquer Transformer (DCTr) layer.
An accurate video quality feature extraction can be achieved by repeating the
process of this DCTr layer several times. Taking the order relationship among
the annotated data into account, we also propose a novel correlation loss term
for model training. Experiments on various datasets confirm the effectiveness
and robustness of our DCVQE model.",None,-1
04eeac76-33f7-4b7e-834a-d14aef58b9f0,Efficient Feedback and Partial Credit Grading for Proof Blocks Problems,0.121118,"Proof Blocks is a software tool that allows students to practice writing
mathematical proofs by dragging and dropping lines instead of writing proofs
from scratch. Proof Blocks offers the capability of assigning partial credit
and providing solution quality feedback to students. This is done by computing
the edit distance from a student's submission to some predefined set of
solutions. In this work, we propose an algorithm for the edit distance problem
that significantly outperforms the baseline procedure of exhaustively
enumerating over the entire search space. Our algorithm relies on a reduction
to the minimum vertex cover problem. We benchmark our algorithm on thousands of
student submissions from multiple courses, showing that the baseline algorithm
is intractable, and that our proposed algorithm is critical to enable classroom
deployment. Our new algorithm has also been used for problems in many other
domains where the solution space can be modeled as a DAG, including but not
limited to Parsons Problems for writing code, helping students understand
packet ordering in networking protocols, and helping students sketch solution
steps for physics problems. Integrated into multiple learning management
systems, the algorithm serves thousands of students each year.",None,-1
e77faae5-6d64-4a50-af77-4f472198c963,FabKG: A Knowledge graph of Manufacturing Science domain utilizing structured and unconventional unstructured knowledge source,0.146252,"As the demands for large-scale information processing have grown, knowledge
graph-based approaches have gained prominence for representing general and
domain knowledge. The development of such general representations is essential,
particularly in domains such as manufacturing which intelligent processes and
adaptive education can enhance. Despite the continuous accumulation of text in
these domains, the lack of structured data has created information extraction
and knowledge transfer barriers. In this paper, we report on work towards
developing robust knowledge graphs based upon entity and relation data for both
commercial and educational uses. To create the FabKG (Manufacturing knowledge
graph), we have utilized textbook index words, research paper keywords, FabNER
(manufacturing NER), to extract a sub knowledge base contained within Wikidata.
Moreover, we propose a novel crowdsourcing method for KG creation by leveraging
student notes, which contain invaluable information but are not captured as
meaningful information, excluding their use in personal preparation for
learning and written exams. We have created a knowledge graph containing 65000+
triples using all data sources. We have also shown the use case of
domain-specific question answering and expression/formula-based question
answering for educational purposes.",None,-1
fdb63e12-88d6-4dd2-a60a-999331ed04ad,AeDet: Azimuth-invariant Multi-view 3D Object Detection,0.509829,"Recent LSS-based multi-view 3D object detection has made tremendous progress,
by processing the features in Brid-Eye-View (BEV) via the convolutional
detector. However, the typical convolution ignores the radial symmetry of the
BEV features and increases the difficulty of the detector optimization. To
preserve the inherent property of the BEV features and ease the optimization,
we propose an azimuth-equivariant convolution (AeConv) and an
azimuth-equivariant anchor. The sampling grid of AeConv is always in the radial
direction, thus it can learn azimuth-invariant BEV features. The proposed
anchor enables the detection head to learn predicting azimuth-irrelevant
targets. In addition, we introduce a camera-decoupled virtual depth to unify
the depth prediction for the images with different camera intrinsic parameters.
The resultant detector is dubbed Azimuth-equivariant Detector (AeDet).
Extensive experiments are conducted on nuScenes, and AeDet achieves a 62.0%
NDS, surpassing the recent multi-view 3D object detectors such as PETRv2 and
BEVDepth by a large margin. Project page: https://fcjian.github.io/aedet.",https://fcjian.github.io/aedet/,-1
399977b9-5468-4abb-883c-e339fe144729,Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models,0.998493,"Image restoration under adverse weather conditions has been of significant
interest for various computer vision applications. Recent successful methods
rely on the current progress in deep neural network architectural designs
(e.g., with vision transformers). Motivated by the recent progress achieved
with state-of-the-art conditional generative models, we present a novel
patch-based image restoration algorithm based on denoising diffusion
probabilistic models. Our patch-based diffusion modeling approach enables
size-agnostic image restoration by using a guided denoising process with
smoothed noise estimates across overlapping patches during inference. We
empirically evaluate our model on benchmark datasets for image desnowing,
combined deraining and dehazing, and raindrop removal. We demonstrate our
approach to achieve state-of-the-art performances on both weather-specific and
multi-weather image restoration, and experimentally show strong generalization
to real-world test images.",https://github.com/IGITUGraz/WeatherDiffusion,6767
9a94c941-02a5-4634-b42a-1b07171538f5,Penalized Proximal Policy Optimization for Safe Reinforcement Learning,0.774289,"Safe reinforcement learning aims to learn the optimal policy while satisfying
safety constraints, which is essential in real-world applications. However,
current algorithms still struggle for efficient policy updates with hard
constraint satisfaction. In this paper, we propose Penalized Proximal Policy
Optimization (P3O), which solves the cumbersome constrained policy iteration
via a single minimization of an equivalent unconstrained problem. Specifically,
P3O utilizes a simple-yet-effective penalty function to eliminate cost
constraints and removes the trust-region constraint by the clipped surrogate
objective. We theoretically prove the exactness of the proposed method with a
finite penalty factor and provide a worst-case analysis for approximate error
when evaluated on sample trajectories. Moreover, we extend P3O to more
challenging multi-constraint and multi-agent scenarios which are less studied
in previous work. Extensive experiments show that P3O outperforms
state-of-the-art algorithms with respect to both reward improvement and
constraint satisfaction on a set of constrained locomotive tasks.",https://github.com/openai/safety-starter-agents,-1
39dd764f-54f4-4b45-8c4a-e78289b5304b,A Graph Isomorphism Network with Weighted Multiple Aggregators for Speech Emotion Recognition,0.447011,"Speech emotion recognition (SER) is an essential part of human-computer
interaction. In this paper, we propose an SER network based on a Graph
Isomorphism Network with Weighted Multiple Aggregators (WMA-GIN), which can
effectively handle the problem of information confusion when neighbour nodes'
features are aggregated together in GIN structure. Moreover, a Full-Adjacent
(FA) layer is adopted for alleviating the over-squashing problem, which is
existed in all Graph Neural Network (GNN) structures, including GIN.
Furthermore, a multi-phase attention mechanism and multi-loss training strategy
are employed to avoid missing the useful emotional information in the stacked
WMA-GIN layers. We evaluated the performance of our proposed WMA-GIN on the
popular IEMOCAP dataset. The experimental results show that WMA-GIN outperforms
other GNN-based methods and is comparable to some advanced non-graph-based
methods by achieving 72.48% of weighted accuracy (WA) and 67.72% of unweighted
accuracy (UA).",None,-1
1194c593-a224-483d-b869-8e01c571bc44,"Reflectance-Guided, Contrast-Accumulated Histogram Equalization",0.0717411,"Existing image enhancement methods fall short of expectations because with
them it is difficult to improve global and local image contrast simultaneously.
To address this problem, we propose a histogram equalization-based method that
adapts to the data-dependent requirements of brightness enhancement and
improves the visibility of details without losing the global contrast. This
method incorporates the spatial information provided by image context in
density estimation for discriminative histogram equalization. To minimize the
adverse effect of non-uniform illumination, we propose defining spatial
information on the basis of image reflectance estimated with edge preserving
smoothing. Our method works particularly well for determining how the
background brightness should be adaptively adjusted and for revealing useful
image details hidden in the dark.",None,-1
a90b9025-f71e-46d5-8536-0feecb036809,The Isabelle ENIGMA,0.765013,"We significantly improve the performance of the E automated theorem prover on
the Isabelle Sledgehammer problems by combining learning and theorem proving in
several ways. In particular, we develop targeted versions of the ENIGMA
guidance for the Isabelle problems, targeted versions of neural premise
selection, and targeted strategies for E. The methods are trained in several
iterations over hundreds of thousands untyped and typed first-order problems
extracted from Isabelle. Our final best single-strategy ENIGMA and premise
selection system improves the best previous version of E by 25.3% in 15
seconds, outperforming also all other previous ATP and SMT systems.",https://github.com/ai4reason/ATP_Proofs,-1
17d62115-ef09-4da5-b71d-8163f582be64,Cross-Image Relational Knowledge Distillation for Semantic Segmentation,0.99806,"Current Knowledge Distillation (KD) methods for semantic segmentation often
guide the student to mimic the teacher's structured information generated from
individual data samples. However, they ignore the global semantic relations
among pixels across various images that are valuable for KD. This paper
proposes a novel Cross-Image Relational KD (CIRKD), which focuses on
transferring structured pixel-to-pixel and pixel-to-region relations among the
whole images. The motivation is that a good teacher network could construct a
well-structured feature space in terms of global pixel dependencies. CIRKD
makes the student mimic better structured semantic relations from the teacher,
thus improving the segmentation performance. Experimental results over
Cityscapes, CamVid and Pascal VOC datasets demonstrate the effectiveness of our
proposed approach against state-of-the-art distillation methods. The code is
available at https://github.com/winycg/CIRKD.",https://github.com/winycg/CIRKD,-1
35d5110e-116b-4c87-b7fe-2161e2e564ef,Non-Deterministic Face Mask Removal Based On 3D Priors,0.131635,"This paper presents a novel image inpainting framework for face mask removal.
Although current methods have demonstrated their impressive ability in
recovering damaged face images, they suffer from two main problems: the
dependence on manually labeled missing regions and the deterministic result
corresponding to each input. The proposed approach tackles these problems by
integrating a multi-task 3D face reconstruction module with a face inpainting
module. Given a masked face image, the former predicts a 3DMM-based
reconstructed face together with a binary occlusion map, providing dense
geometrical and textural priors that greatly facilitate the inpainting task of
the latter. By gradually controlling the 3D shape parameters, our method
generates high-quality dynamic inpainting results with different expressions
and mouth movements. Qualitative and quantitative experiments verify the
effectiveness of the proposed method.",https://github.com/face3d0725/face_de_mask,11135
1b0c5cc8-2532-48ed-9932-626406c98771,NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors,0.567973,"Though Neural Radiance Field (NeRF) demonstrates compelling novel view
synthesis results, it is still unintuitive to edit a pre-trained NeRF because
the neural network's parameters and the scene geometry/appearance are often not
explicitly associated. In this paper, we introduce the first framework that
enables users to remove unwanted objects or retouch undesired regions in a 3D
scene represented by a pre-trained NeRF without any category-specific data and
training. The user first draws a free-form mask to specify a region containing
unwanted objects over a rendered view from the pre-trained NeRF. Our framework
first transfers the user-provided mask to other rendered views and estimates
guiding color and depth images within these transferred masked regions. Next,
we formulate an optimization problem that jointly inpaints the image content in
all masked regions across multiple views by updating the NeRF model's
parameters. We demonstrate our framework on diverse scenes and show it obtained
visual plausible and structurally consistent results across multiple views
using shorter time and less user manual efforts.",None,-1
5bb86b4a-0546-4c21-bfab-7514559ab9c3,Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest,0.437841,"Medical Relation Extraction (MRE) task aims to extract relations between
entities in medical texts. Traditional relation extraction methods achieve
impressive success by exploring the syntactic information, e.g., dependency
tree. However, the quality of the 1-best dependency tree for medical texts
produced by an out-of-domain parser is relatively limited so that the
performance of medical relation extraction method may degenerate. To this end,
we propose a method to jointly model semantic and syntactic information from
medical texts based on causal explanation theory. We generate dependency
forests consisting of the semantic-embedded 1-best dependency tree. Then, a
task-specific causal explainer is adopted to prune the dependency forests,
which are further fed into a designed graph convolutional network to learn the
corresponding representation for downstream task. Empirically, the various
comparisons on benchmark medical datasets demonstrate the effectiveness of our
model.",None,-1
3a3120fc-4127-487b-9d08-de8db351ae86,Self-Supervised Pre-training of Vision Transformers for Dense Prediction Tasks,0.0609561,"We present a new self-supervised pre-training of Vision Transformers for
dense prediction tasks. It is based on a contrastive loss across views that
compares pixel-level representations to global image representations. This
strategy produces better local features suitable for dense prediction tasks as
opposed to contrastive pre-training based on global image representation only.
Furthermore, our approach does not suffer from a reduced batch size since the
number of negative examples needed in the contrastive loss is in the order of
the number of local features. We demonstrate the effectiveness of our
pre-training strategy on two dense prediction tasks: semantic segmentation and
monocular depth estimation.",None,638
36a8e3a8-1566-4277-bb61-ab5b274081d4,Fast AdvProp,0.207468,"Adversarial Propagation (AdvProp) is an effective way to improve recognition
models, leveraging adversarial examples. Nonetheless, AdvProp suffers from the
extremely slow training speed, mainly because: a) extra forward and backward
passes are required for generating adversarial examples; b) both original
samples and their adversarial counterparts are used for training (i.e.,
2$\times$ data). In this paper, we introduce Fast AdvProp, which aggressively
revamps AdvProp's costly training components, rendering the method nearly as
cheap as the vanilla training. Specifically, our modifications in Fast AdvProp
are guided by the hypothesis that disentangled learning with adversarial
examples is the key for performance improvements, while other training recipes
(e.g., paired clean and adversarial training samples, multi-step adversarial
attackers) could be largely simplified.
  Our empirical results show that, compared to the vanilla training baseline,
Fast AdvProp is able to further model performance on a spectrum of visual
benchmarks, without incurring extra training cost. Additionally, our ablations
find Fast AdvProp scales better if larger models are used, is compatible with
existing data augmentation methods (i.e., Mixup and CutMix), and can be easily
adapted to other recognition tasks like object detection. The code is available
here: https://github.com/meijieru/fast_advprop.",https://github.com/meijieru/fast_advprop,-1
24e83865-46db-4f1a-ae27-c01157a0a241,Uncertainty-guided Source-free Domain Adaptation,0.991725,"Source-free domain adaptation (SFDA) aims to adapt a classifier to an
unlabelled target data set by only using a pre-trained source model. However,
the absence of the source data and the domain shift makes the predictions on
the target data unreliable. We propose quantifying the uncertainty in the
source model predictions and utilizing it to guide the target adaptation. For
this, we construct a probabilistic source model by incorporating priors on the
network parameters inducing a distribution over the model predictions.
Uncertainties are estimated by employing a Laplace approximation and
incorporated to identify target data points that do not lie in the source
manifold and to down-weight them when maximizing the mutual information on the
target data. Unlike recent works, our probabilistic treatment is
computationally lightweight, decouples source training and target adaptation,
and requires no specialized source training or changes of the model
architecture. We show the advantages of uncertainty-guided SFDA over
traditional SFDA in the closed-set and open-set settings and provide empirical
evidence that our approach is more robust to strong domain shifts even without
tuning.",https://github.com/roysubhankar/uncertainty-sfda,-1
16ee8f1f-2d2d-4cb5-9cf0-bf083dbe2a71,Confidence Score for Source-Free Unsupervised Domain Adaptation,0.956563,"Source-free unsupervised domain adaptation (SFUDA) aims to obtain high
performance in the unlabeled target domain using the pre-trained source model,
not the source data. Existing SFUDA methods assign the same importance to all
target samples, which is vulnerable to incorrect pseudo-labels. To
differentiate between sample importance, in this study, we propose a novel
sample-wise confidence score, the Joint Model-Data Structure (JMDS) score for
SFUDA. Unlike existing confidence scores that use only one of the source or
target domain knowledge, the JMDS score uses both knowledge. We then propose a
Confidence score Weighting Adaptation using the JMDS (CoWA-JMDS) framework for
SFUDA. CoWA-JMDS consists of the JMDS scores as sample weights and weight Mixup
that is our proposed variant of Mixup. Weight Mixup promotes the model make
more use of the target domain knowledge. The experimental results show that the
JMDS score outperforms the existing confidence scores. Moreover, CoWA-JMDS
achieves state-of-the-art performance on various SFUDA scenarios: closed, open,
and partial-set scenarios.",https://github.com/Jhyun17/CoWA-JMDS,-1
3df81323-b600-4f2e-ae68-06dc4a8f21aa,Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data,0.0655516,"A number of studies have investigated the training of neural networks with
synthetic data for applications in the real world. The aim of this study is to
quantify how much real world data can be saved when using a mixed dataset of
synthetic and real world data. By modeling the relationship between the number
of training examples and detection performance by a simple power law, we find
that the need for real world data can be reduced by up to 70% without
sacrificing detection performance. The training of object detection networks is
especially enhanced by enriching the mixed dataset with classes
underrepresented in the real world dataset. The results indicate that mixed
datasets with real world data ratios between 5% and 20% reduce the need for
real world data the most without reducing the detection performance.",https://github.com/NVIDIA/pix2pixHD,12
4c1e58a5-ecc8-4ab0-b323-7cf41a20c230,Not always about you: Prioritizing community needs when developing endangered language technology,0.757213,"Languages are classified as low-resource when they lack the quantity of data
necessary for training statistical and machine learning tools and models.
Causes of resource scarcity vary but can include poor access to technology for
developing these resources, a relatively small population of speakers, or a
lack of urgency for collecting such resources in bilingual populations where
the second language is high-resource. As a result, the languages described as
low-resource in the literature are as different as Finnish on the one hand,
with millions of speakers using it in every imaginable domain, and Seneca, with
only a small-handful of fluent speakers using the language primarily in a
restricted domain. While issues stemming from the lack of resources necessary
to train models unite this disparate group of languages, many other issues cut
across the divide between widely-spoken low resource languages and endangered
languages. In this position paper, we discuss the unique technological,
cultural, practical, and ethical challenges that researchers and indigenous
speech community members face when working together to develop language
technology to support endangered language documentation and revitalization. We
report the perspectives of language teachers, Master Speakers and elders from
indigenous communities, as well as the point of view of academics. We describe
an ongoing fruitful collaboration and make recommendations for future
partnerships between academic researchers and language community stakeholders.",None,-1
a008c6f8-8c81-465d-b146-454f2045a707,ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing,0.570739,"Sketch-and-extrude is a common and intuitive modeling process in computer
aided design. This paper studies the problem of learning the shape given in the
form of point clouds by inverse sketch-and-extrude. We present ExtrudeNet, an
unsupervised end-to-end network for discovering sketch and extrude from point
clouds. Behind ExtrudeNet are two new technical components: 1) an effective
representation for sketch and extrude, which can model extrusion with freeform
sketches and conventional cylinder and box primitives as well; and 2) a
numerical method for computing the signed distance field which is used in the
network learning. This is the first attempt that uses machine learning to
reverse engineer the sketch-and-extrude modeling process of a shape in an
unsupervised fashion. ExtrudeNet not only outputs a compact, editable and
interpretable representation of the shape that can be seamlessly integrated
into modern CAD software, but also aligns with the standard CAD modeling
process facilitating various editing applications, which distinguishes our work
from existing shape parsing research. Code is released at
https://github.com/kimren227/ExtrudeNet.",https://github.com/kimren227/ExtrudeNet,-1
5bd025cf-ffcd-42ac-b07a-9aea9f9d6100,Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval,0.301088,"State-of-the-art neural (re)rankers are notoriously data-hungry which --
given the lack of large-scale training data in languages other than English --
makes them rarely used in multilingual and cross-lingual retrieval settings.
Current approaches therefore commonly transfer rankers trained on English data
to other languages and cross-lingual setups by means of multilingual encoders:
they fine-tune all parameters of pretrained massively multilingual Transformers
(MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy
them in the target language(s). In this work, we show that two
parameter-efficient approaches to cross-lingual transfer, namely Sparse
Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more
effective zero-shot transfer to multilingual and cross-lingual retrieval tasks.
We first train language adapters (or SFTMs) via Masked Language Modelling and
then train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping
all other parameters fixed. At inference, this modular design allows us to
compose the ranker by applying the (re)ranking adapter (or SFTM) trained with
source language data together with the language adapter (or SFTM) of a target
language. We carry out a large scale evaluation on the CLEF-2003 and HC4
benchmarks and additionally, as another contribution, extend the former with
queries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed
parameter-efficient methods outperform standard zero-shot transfer with full
MMT fine-tuning, while being more modular and reducing training times. The
gains are particularly pronounced for low-resource languages, where our
approaches also substantially outperform the competitive machine
translation-based rankers.",https://github.com/rlitschk/ModularCLIR,-1
1d7028ab-2685-40de-8789-a41a44baebb1,CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation,0.411054,"The full power of human language-based communication cannot be realized
without negation. All human languages have some form of negation. Despite this,
negation remains a challenging phenomenon for current natural language
understanding systems. To facilitate the future development of models that can
process negation effectively, we present CONDAQA, the first English reading
comprehension dataset which requires reasoning about the implications of
negated statements in paragraphs. We collect paragraphs with diverse negation
cues, then have crowdworkers ask questions about the implications of the
negated statement in the passage. We also have workers make three kinds of
edits to the passage -- paraphrasing the negated statement, changing the scope
of the negation, and reversing the negation -- resulting in clusters of
question-answer pairs that are difficult for models to answer with spurious
shortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique
negation cues and is challenging for current state-of-the-art models. The best
performing model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our
consistency metric, well below human performance which is 81%. We release our
dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to
facilitate the development of future NLP methods that work on negated language.",https://github.com/AbhilashaRavichander/CondaQA,-1
e4c21488-d246-44a2-8c7b-57f926b4dee3,Preserving Fine-Grain Feature Information in Classification via Entropic Regularization,0.151416,"Labeling a classification dataset implies to define classes and associated
coarse labels, that may approximate a smoother and more complicated ground
truth. For example, natural images may contain multiple objects, only one of
which is labeled in many vision datasets, or classes may result from the
discretization of a regression problem. Using cross-entropy to train
classification models on such coarse labels is likely to roughly cut through
the feature space, potentially disregarding the most meaningful such features,
in particular losing information on the underlying fine-grain task. In this
paper we are interested in the problem of solving fine-grain classification or
regression, using a model trained on coarse-grain labels only. We show that
standard cross-entropy can lead to overfitting to coarse-related features. We
introduce an entropy-based regularization to promote more diversity in the
feature space of trained models, and empirically demonstrate the efficacy of
this methodology to reach better performance on the fine-grain problems. Our
results are supported through theoretical developments and empirical
validation.",https://anonymous.4open.science/r/FIERCE-repo-CFE2/README.md,-1
8533aa23-8fbe-4d7e-af2d-d18413edb588,Quantum computing overview: discrete vs. continuous variable models,0.0728847,"In this Near Intermediate-Scale Quantum era, there are two types of near-term
quantum devices available on cloud: superconducting quantum processing units
(QPUs) based on the discrete variable model and linear optics (photonics) QPUs
based on the continuous variable (CV) model. Quantum computation in the
discrete variable model is performed in a finite dimensional quantum state
space and the CV model in an infinite dimensional space. In implementing
quantum algorithms, the CV model offers more quantum gates that are not
available in the discrete variable model. CV-based photonic quantum computers
provide additional flexibility of controlling the length of the output vectors
of quantum circuits, using different methods of measurement and the notion of
cutoff dimension.",None,-1
08f4f576-36c5-44a3-96ee-ff51ea9a4f70,ST-MoE: Designing Stable and Transferable Sparse Expert Models,0.835356,"Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3).",https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py,-1
250246ac-4b69-4606-b297-5f4f3f4f9815,Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge,0.864665,"Cognitively plausible visual dialogue models should keep a mental scoreboard
of shared established facts in the dialogue context. We propose a theory-based
evaluation method for investigating to what degree models pretrained on the
VisDial dataset incrementally build representations that appropriately do
scorekeeping. Our conclusion is that the ability to make the distinction
between shared and privately known statements along the dialogue is moderately
present in the analysed models, but not always incrementally consistent, which
may partially be due to the limited need for grounding interactions in the
original task.",https://github.com/vmurahari3/visdial-diversity,-1
1359ecd2-1a7d-47ad-98d2-6809bc5ab8c7,Low-complexity CNNs for Acoustic Scene Classification,0.416972,"This paper presents a low-complexity framework for acoustic scene
classification (ASC). Most of the frameworks designed for ASC use convolutional
neural networks (CNNs) due to their learning ability and improved performance
compared to hand-engineered features. However, CNNs are resource hungry due to
their large size and high computational complexity. Therefore, CNNs are
difficult to deploy on resource constrained devices. This paper addresses the
problem of reducing the computational complexity and memory requirement in
CNNs. We propose a low-complexity CNN architecture, and apply pruning and
quantization to further reduce the parameters and memory. We then propose an
ensemble framework that combines various low-complexity CNNs to improve the
overall performance. An experimental evaluation of the proposed framework is
performed on the publicly available DCASE 2022 Task 1 that focuses on ASC. The
proposed ensemble framework has approximately 60K parameters, requires 19M
multiply-accumulate operations and improves the performance by approximately
2-4 percentage points compared to the DCASE 2022 Task 1 baseline network.",None,17789
64863d57-f718-4f2c-b479-ab101a47abce,A Comparison Between Tsetlin Machines and Deep Neural Networks in the Context of Recommendation Systems,0.105515,"Recommendation Systems (RSs) are ubiquitous in modern society and are one of
the largest points of interaction between humans and AI. Modern RSs are often
implemented using deep learning models, which are infamously difficult to
interpret. This problem is particularly exasperated in the context of
recommendation scenarios, as it erodes the user's trust in the RS. In contrast,
the newly introduced Tsetlin Machines (TM) possess some valuable properties due
to their inherent interpretability. TMs are still fairly young as a technology.
As no RS has been developed for TMs before, it has become necessary to perform
some preliminary research regarding the practicality of such a system. In this
paper, we develop the first RS based on TMs to evaluate its practicality in
this application domain. This paper compares the viability of TMs with other
machine learning models prevalent in the field of RS. We train and investigate
the performance of the TM compared with a vanilla feed-forward deep learning
model. These comparisons are based on model performance,
interpretability/explainability, and scalability. Further, we provide some
benchmark performance comparisons to similar machine learning solutions
relevant to RSs.",https://github.com/cair/,-1
09083b19-34bf-4db9-8f21-b21134e71225,Fine-Grained Object Classification via Self-Supervised Pose Alignment,0.823612,"Semantic patterns of fine-grained objects are determined by subtle appearance
difference of local parts, which thus inspires a number of part-based methods.
However, due to uncontrollable object poses in images, distinctive details
carried by local regions can be spatially distributed or even self-occluded,
leading to a large variation on object representation. For discounting pose
variations, this paper proposes to learn a novel graph based object
representation to reveal a global configuration of local parts for
self-supervised pose alignment across classes, which is employed as an
auxiliary feature regularization on a deep representation learning
network.Moreover, a coarse-to-fine supervision together with the proposed
pose-insensitive constraint on shallow-to-deep sub-networks encourages
discriminative features in a curriculum learning manner. We evaluate our method
on three popular fine-grained object classification benchmarks, consistently
achieving the state-of-the-art performance. Source codes are available at
https://github.com/yangxh11/P2P-Net.",https://github.com/yangxh11/P2P-Net,15319
ee6e9efc-acfd-4202-abf2-4e1595b79fd8,Neighbor Correspondence Matching for Flow-based Video Frame Synthesis,0.263295,"Video frame synthesis, which consists of interpolation and extrapolation, is
an essential video processing technique that can be applied to various
scenarios. However, most existing methods cannot handle small objects or large
motion well, especially in high-resolution videos such as 4K videos. To
eliminate such limitations, we introduce a neighbor correspondence matching
(NCM) algorithm for flow-based frame synthesis. Since the current frame is not
available in video frame synthesis, NCM is performed in a
current-frame-agnostic fashion to establish multi-scale correspondences in the
spatial-temporal neighborhoods of each pixel. Based on the powerful motion
representation capability of NCM, we further propose to estimate intermediate
flows for frame synthesis in a heterogeneous coarse-to-fine scheme.
Specifically, the coarse-scale module is designed to leverage neighbor
correspondences to capture large motion, while the fine-scale module is more
computationally efficient to speed up the estimation process. Both modules are
trained progressively to eliminate the resolution gap between training dataset
and real-world videos. Experimental results show that NCM achieves
state-of-the-art performance on several benchmarks. In addition, NCM can be
applied to various practical scenarios such as video compression to achieve
better performance.",None,-1
8bfcc3b1-9625-4745-adc7-9fb98f982d0b,Using Linguistic Typology to Enrich Multilingual Lexicons: the Case of Lexical Gaps in Kinship,0.536722,"This paper describes a method to enrich lexical resources with content
relating to linguistic diversity, based on knowledge from the field of lexical
typology. We capture the phenomenon of diversity through the notions of lexical
gap and language-specific word and use a systematic method to infer gaps
semi-automatically on a large scale. As a first result obtained for the domain
of kinship terminology, known to be very diverse throughout the world, we
publish a lexico-semantic resource consisting of 198 domain concepts, 1,911
words, and 37,370 gaps covering 699 languages. We see potential in the use of
resources such as ours for the improvement of a variety of cross-lingual NLP
tasks, which we demonstrate through a downstream application for the evaluation
of machine translation systems.",https://github.com/kbatsuren/KinDiv,-1
31c41951-56a0-4b40-aa4d-ab87f3b0210c,Audio-driven Neural Gesture Reenactment with Video Motion Graphs,0.575627,"Human speech is often accompanied by body gestures including arm and hand
gestures. We present a method that reenacts a high-quality video with gestures
matching a target speech audio. The key idea of our method is to split and
re-assemble clips from a reference video through a novel video motion graph
encoding valid transitions between clips. To seamlessly connect different clips
in the reenactment, we propose a pose-aware video blending network which
synthesizes video frames around the stitched frames between two clips.
Moreover, we developed an audio-based gesture searching algorithm to find the
optimal order of the reenacted frames. Our system generates reenactments that
are consistent with both the audio rhythms and the speech content. We evaluate
our synthesized video quality quantitatively, qualitatively, and with user
studies, demonstrating that our method produces videos of much higher quality
and consistency with the target audio compared to previous work and baselines.",https://yzhou359.github.io/video_reenact,-1
26d58482-d8de-4960-9cc4-60195c8613d8,Multi-Game Decision Transformers,1.0,"A longstanding goal of the field of AI is a method for learning a highly
capable, generalist agent from diverse experience. In the subfields of vision
and language, this was largely achieved by scaling up transformer-based models
and training them on large, diverse datasets. Motivated by this progress, we
investigate whether the same strategy can be used to produce generalist
reinforcement learning agents. Specifically, we show that a single
transformer-based model - with a single set of weights - trained purely offline
can play a suite of up to 46 Atari games simultaneously at close-to-human
performance. When trained and evaluated appropriately, we find that the same
trends observed in language and vision hold, including scaling of performance
with model size and rapid adaptation to new games via fine-tuning. We compare
several approaches in this multi-game setting, such as online and offline RL
methods and behavioral cloning, and find that our Multi-Game Decision
Transformer models offer the best scalability and performance. We release the
pre-trained models and code to encourage further research in this direction.",https://github.com/deepmind,34890
9f855844-d019-4f07-ba7d-637e8f072053,Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning,0.94417,"We present Bit Diffusion: a simple and generic approach for generating
discrete data with continuous state and continuous time diffusion models. The
main idea behind our approach is to first represent the discrete data as binary
bits, and then train a continuous diffusion model to model these bits as real
numbers which we call analog bits. To generate samples, the model first
generates the analog bits, which are then thresholded to obtain the bits that
represent the discrete variables. We further propose two simple techniques,
namely Self-Conditioning and Asymmetric Time Intervals, which lead to a
significant improvement in sample quality. Despite its simplicity, the proposed
approach can achieve strong performance in both discrete image generation and
image captioning tasks. For discrete image generation, we significantly improve
previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens)
and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the
best autoregressive model in both sample quality (measured by FID) and
efficiency. For image captioning on MS-COCO dataset, our approach achieves
competitive results compared to autoregressive models.",https://github.com/google-research/pix2seq,-1
bacc71c0-59a8-42a3-b127-c0139c41c53b,Knowledge Removal in Sampling-based Bayesian Inference,0.903963,"The right to be forgotten has been legislated in many countries, but its
enforcement in the AI industry would cause unbearable costs. When single data
deletion requests come, companies may need to delete the whole models learned
with massive resources. Existing works propose methods to remove knowledge
learned from data for explicitly parameterized models, which however are not
appliable to the sampling-based Bayesian inference, i.e., Markov chain Monte
Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we
propose the first machine unlearning algorithm for MCMC. We first convert the
MCMC unlearning problem into an explicit optimization problem. Based on this
problem conversion, an {\it MCMC influence function} is designed to provably
characterize the learned knowledge from data, which then delivers the MCMC
unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not
compromise the generalizability of the MCMC models. Experiments on Gaussian
mixture models and Bayesian neural networks confirm the effectiveness of the
proposed algorithm. The code is available at
\url{https://github.com/fshp971/mcmc-unlearning}.",https://github.com/fshp971/mcmc-unlearning,-1
2134958b-5e15-4b7d-b6e3-ddbfb812ffa4,Unsupervised Domain Adaptation with Implicit Pseudo Supervision for Semantic Segmentation,0.226125,"Pseudo-labelling is a popular technique in unsuper-vised domain adaptation
for semantic segmentation. However, pseudo labels are noisy and inevitably have
confirmation bias due to the discrepancy between source and target domains and
training process. In this paper, we train the model by the pseudo labels which
are implicitly produced by itself to learn new complementary knowledge about
target domain. Specifically, we propose a tri-learning architecture, where
every two branches produce the pseudo labels to train the third one. And we
align the pseudo labels based on the similarity of the probability
distributions for each two branches. To further implicitly utilize the pseudo
labels, we maximize the distances of features for different classes and
minimize the distances for the same classes by triplet loss. Extensive
experiments on GTA5 to Cityscapes and SYNTHIA to Cityscapes tasks show that the
proposed method has considerable improvements.",None,-1
918918b9-8d82-4c0f-952d-1cd8fcf250fb,Independent Components of Word Embeddings Represent Semantic Features,0.285958,"Independent Component Analysis (ICA) is an algorithm originally developed for
finding separate sources in a mixed signal, such as a recording of multiple
people in the same room speaking at the same time. It has also been used to
find linguistic features in distributional representations. In this paper, we
used ICA to analyze words embeddings. We have found that ICA can be used to
find semantic features of the words and these features can easily be combined
to search for words that satisfy the combination. We show that only some of the
independent components represent such features, but those that do are stable
with regard to random initialization of the algorithm.",None,-1
94235dc2-2379-43d7-b83e-8a1fa2fc7ad6,Fine-Grained Scene Graph Generation with Data Transfer,0.914471,"Scene graph generation (SGG) is designed to extract (subject, predicate,
object) triplets in images. Recent works have made a steady progress on SGG,
and provide useful tools for high-level vision and language understanding.
However, due to the data distribution problems including long-tail distribution
and semantic ambiguity, the predictions of current SGG models tend to collapse
to several frequent but uninformative predicates (e.g., on, at), which limits
practical application of these models in downstream tasks. To deal with the
problems above, we propose a novel Internal and External Data Transfer
(IETrans) method, which can be applied in a plug-and-play fashion and expanded
to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the
data distribution problem by automatically creating an enhanced dataset that
provides more sufficient and coherent annotations for all predicates. By
training on the enhanced dataset, a Neural Motif model doubles the macro
performance while maintaining competitive micro performance. The code and data
are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch.",https://github.com/waxnkw/IETrans-SGG.pytorch,-1
a508f267-bf7a-4128-8c07-06428fa5e215,Computer vision application for improved product traceability in the granite manufacturing industry,0.0593926,"The traceability of granite blocks consists in identifying each block with a
finite number of color bands which represent a numerical code. This code has to
be read several times throughout the manufacturing process, but its accuracy is
subject to human errors, leading to cause faults in the traceability system. A
computer vision system is presented to address this problem through color
detection and the decryption of the associated code. The system developed makes
use of color space transformations, and several thresholds for the isolation of
the colors. Computer vision methods are implemented, along with contour
detection procedures for color identification. Lastly, the analysis of
geometrical features is used to decrypt the color code captured. The proposed
algorithm is trained on a set of 109 pictures taken in different environmental
conditions and validated on a set of 21 images. The outcome shows promising
results with an accuracy rate of 75.00% in the validation process. Therefore,
the application presented can help employees reduce the number of mistakes on
product tracking.",None,-1
44567ba3-5fa7-4586-9373-842da7b9ac3f,OptG: Optimizing Gradient-driven Criteria in Network Sparsity,0.145053,"Network sparsity receives popularity mostly due to its capability to reduce
the network complexity. Extensive studies excavate gradient-driven sparsity.
Typically, these methods are constructed upon premise of weight independence,
which however, is contrary to the fact that weights are mutually influenced.
Thus, their performance remains to be improved. In this paper, we propose to
optimize gradient-driven sparsity (OptG) by solving this independence paradox.
Our motive comes from the recent advances in supermask training which shows
that high-performing sparse subnetworks can be located by simply updating mask
values without modifying any weight. We prove that supermask training is to
accumulate the criteria of gradient-driven sparsity for both removed and
preserved weights, and it can partly solve the independence paradox.
Consequently, OptG integrates supermask training into gradient-driven sparsity,
and a novel supermask optimizer is further proposed to comprehensively mitigate
the independence paradox. Experiments show that OptG can well surpass many
existing state-of-the-art competitors, especially at ultra-high sparsity
levels. Our code is available at \url{https://github.com/zyxxmu/OptG}.",https://github.com/zyxxmu/OptG,-1
0beab9c1-7bad-45d4-a488-087a43de0f02,STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding,0.225042,"In this technical report, we introduce our solution to human-centric
spatio-temporal video grounding task. We propose a concise and effective
framework named STVGFormer, which models spatiotemporal visual-linguistic
dependencies with a static branch and a dynamic branch. The static branch
performs cross-modal understanding in a single frame and learns to localize the
target object spatially according to intra-frame visual cues like object
appearances. The dynamic branch performs cross-modal understanding across
multiple frames. It learns to predict the starting and ending time of the
target moment according to dynamic visual cues like motions. Both the static
and dynamic branches are designed as cross-modal transformers. We further
design a novel static-dynamic interaction block to enable the static and
dynamic branches to transfer useful and complementary information from each
other, which is shown to be effective to improve the prediction on hard cases.
Our proposed method achieved 39.6% vIoU and won the first place in the HC-STVG
track of the 4th Person in Context Challenge.",None,20700
28f121f1-8369-42c5-a500-aa0453dd0791,Automatic Creativity Measurement in Scratch Programs Across Modalities,0.27607,"Promoting creativity is considered an important goal of education, but
creativity is notoriously hard to measure.In this paper, we make the journey
fromdefining a formal measure of creativity that is efficientlycomputable to
applying the measure in a practical domain. The measure is general and relies
on coretheoretical concepts in creativity theory, namely fluency, flexibility,
and originality, integratingwith prior cognitive science literature. We adapted
the general measure for projects in the popular visual programming language
Scratch.We designed a machine learning model for predicting the creativity of
Scratch projects, trained and evaluated on human expert creativity assessments
in an extensive user study. Our results show that opinions about creativity in
Scratch varied widely across experts. The automatic creativity assessment
aligned with the assessment of the human experts more than the experts agreed
with each other. This is a first step in providing computational models for
measuring creativity that can be applied to educational technologies, and to
scale up the benefit of creativity education in schools.",None,-1
0f28f945-a95e-4e3f-b15e-bbaf7b3a3ee4,Prompt Consistency for Zero-Shot Task Generalization,0.569058,"One of the most impressive results of recent NLP history is the ability of
pre-trained language models to solve new tasks in a zero-shot setting. To
achieve this, NLP tasks are framed as natural language prompts, generating a
response indicating the predicted output. Nonetheless, the performance in such
settings often lags far behind its supervised counterpart, suggesting a large
space for potential improvement. In this paper, we explore methods to utilize
unlabeled data to improve zero-shot performance. Specifically, we take
advantage of the fact that multiple prompts can be used to specify a single
task, and propose to regularize prompt consistency, encouraging consistent
predictions over this diverse set of prompts. Our method makes it possible to
fine-tune the model either with extra unlabeled training data, or directly on
test input at inference time in an unsupervised manner. In experiments, our
approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,
2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points
in terms of accuracy. The gains are often attained with a small number of
unlabeled examples.",https://github.com/violet-zct/swarm-distillation-zero-shot,33484
d1a30245-3de5-4fe6-8bac-6233882131c7,Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning,0.798809,"Novel category discovery aims at adapting models trained on known categories
to novel categories. Previous works only focus on the scenario where known and
novel categories are of the same granularity. In this paper, we investigate a
new practical scenario called Fine-grained Category Discovery under
Coarse-grained supervision (FCDC). FCDC aims at discovering fine-grained
categories with only coarse-grained labeled data, which can adapt models to
categories of different granularity from known ones and reduce significant
labeling cost. It is also a challenging task since supervised training on
coarse-grained categories tends to focus on inter-class distance (distance
between coarse-grained classes) but ignore intra-class distance (distance
between fine-grained sub-classes) which is essential for separating
fine-grained categories. Considering most current methods cannot transfer
knowledge from coarse-grained level to fine-grained level, we propose a
hierarchical weighted self-contrastive network by building a novel weighted
self-contrastive module and combining it with supervised learning in a
hierarchical manner. Extensive experiments on public datasets show both
effectiveness and efficiency of our model over compared methods. Code and data
are available at https://github.com/Lackel/Hierarchical_Weighted_SCL.",https://github.com/Lackel/Hierarchical_Weighted_SCL,-1
2b68985d-934a-4ea6-97cd-fbdacbf0222f,Variable Functioning and Its Application to Large Scale Steel Frame Design Optimization,0.117122,"To solve complex real-world problems, heuristics and concept-based approaches
can be used in order to incorporate information into the problem. In this
study, a concept-based approach called variable functioning Fx is introduced to
reduce the optimization variables and narrow down the search space. In this
method, the relationships among one or more subset of variables are defined
with functions using information prior to optimization; thus, instead of
modifying the variables in the search process, the function variables are
optimized. By using problem structure analysis technique and engineering expert
knowledge, the $Fx$ method is used to enhance the steel frame design
optimization process as a complex real-world problem. The proposed approach is
coupled with particle swarm optimization and differential evolution algorithms
and used for three case studies. The algorithms are applied to optimize the
case studies by considering the relationships among column cross-section areas.
The results show that $Fx$ can significantly improve both the convergence rate
and the final design of a frame structure, even if it is only used for seeding.",None,204281
9e336d9a-91f7-4f73-84cb-493f66022311,A description of Turkish Discourse Bank 1.2 and an examination of common dependencies in Turkish discourse,0.345296,"We describe Turkish Discourse Bank 1.2, the latest version of a discourse
corpus annotated for explicitly or implicitly conveyed discourse relations,
their constitutive units, and senses in the Penn Discourse Treebank style. We
present an evaluation of the recently added tokens and examine three commonly
occurring dependency patterns that hold among the constitutive units of a pair
of adjacent discourse relations, namely, shared arguments, full embedding and
partial containment of a discourse relation. We present three major findings:
(a) implicitly conveyed relations occur more often than explicitly conveyed
relations in the data; (b) it is much more common for two adjacent implicit
discourse relations to share an argument than for two adjacent explicit
relations to do so; (c) both full embedding and partial containment of
discourse relations are pervasive in the corpus, which can be partly due to
subordinator connectives whose preposed subordinate clause tends to be selected
together with the matrix clause rather than being selected alone. Finally, we
briefly discuss the implications of our findings for Turkish discourse parsing.",https://github.com/disrpt/sharedtask2019/tree/master/data/tur.pdtb.tdb,-1
c12c0820-690b-41e2-ac7c-4b7a257f2793,GeoFill: Reference-Based Image Inpainting with Better Geometric Understanding,0.119934,"Reference-guided image inpainting restores image pixels by leveraging the
content from another single reference image. The primary challenge is how to
precisely place the pixels from the reference image into the hole region.
Therefore, understanding the 3D geometry that relates pixels between two views
is a crucial step towards building a better model. Given the complexity of
handling various types of reference images, we focus on the scenario where the
images are captured by freely moving the same camera around. Compared to the
previous work, we propose a principled approach that does not make heuristic
assumptions about the planarity of the scene. We leverage a monocular depth
estimate and predict relative pose between cameras, then align the reference
image to the target by a differentiable 3D reprojection and a joint
optimization of relative pose and depth map scale and offset. Our approach
achieves state-of-the-art performance on both RealEstate10K and
MannequinChallenge dataset with large baselines, complex geometry and extreme
camera motions. We experimentally verify our approach is also better at
handling large holes.",None,-1
dea930b6-4bb9-47bd-aac9-b4b3cc964204,Neuro-Symbolic Verification of Deep Neural Networks,0.435713,"Formal verification has emerged as a powerful approach to ensure the safety
and reliability of deep neural networks. However, current verification tools
are limited to only a handful of properties that can be expressed as
first-order constraints over the inputs and output of a network. While
adversarial robustness and fairness fall under this category, many real-world
properties (e.g., ""an autonomous vehicle has to stop in front of a stop sign"")
remain outside the scope of existing verification technology. To mitigate this
severe practical restriction, we introduce a novel framework for verifying
neural networks, named neuro-symbolic verification. The key idea is to use
neural networks as part of the otherwise logical specification, enabling the
verification of a wide variety of complex, real-world properties, including the
one above. Moreover, we demonstrate how neuro-symbolic verification can be
implemented on top of existing verification infrastructure for neural networks,
making our framework easily accessible to researchers and practitioners alike.",https://github.com/LebronX/Neuro-Symbolic-Verification,-1
8a4b6c7d-fa36-45ab-a17c-f86bb8a9f4fc,Crosslingual Generalization through Multitask Finetuning,0.772832,"Multitask prompted finetuning (MTF) has been shown to help large language
models generalize to new tasks in a zero-shot setting, but so far explorations
of MTF have focused on English data and models. We apply MTF to the pretrained
multilingual BLOOM and mT5 model families to produce finetuned variants called
BLOOMZ and mT0. We find finetuning large multilingual language models on
English tasks with English prompts allows for task generalization to
non-English languages that appear only in the pretraining corpus. Finetuning on
multilingual tasks with English prompts further improves performance on English
and non-English tasks leading to various state-of-the-art zero-shot results. We
also investigate finetuning on multilingual tasks with prompts that have been
machine-translated from English to match the language of each dataset. We find
training on these machine-translated prompts leads to better performance on
human-written prompts in the respective languages. Surprisingly, we find models
are capable of zero-shot generalization to tasks in languages they have never
intentionally seen. We conjecture that the models are learning higher-level
capabilities that are both task- and language-agnostic. In addition, we
introduce xP3, a composite of supervised datasets in 46 languages with English
and machine-translated prompts. Our code, datasets and models are freely
available at https://github.com/bigscience-workshop/xmtf.",None,-1
6cb09006-fea7-4399-b48b-5d690617467a,"""What makes a question inquisitive?"" A Study on Type-Controlled Inquisitive Question Generation",0.46581,"We propose a type-controlled framework for inquisitive question generation.
We annotate an inquisitive question dataset with question types, train question
type classifiers, and finetune models for type-controlled question generation.
Empirical results demonstrate that we can generate a variety of questions that
adhere to specific types while drawing from the source texts. We also
investigate strategies for selecting a single question from a generated set,
considering both an informative vs.~inquisitive question classifier and a
pairwise ranker trained from a small set of expert annotations. Question
selection using the pairwise ranker yields strong results in automatic and
manual evaluation. Our human evaluation assesses multiple aspects of the
generated questions, finding that the ranker chooses questions with the best
syntax (4.59), semantics (4.37), and inquisitiveness (3.92) on a scale of 1-5,
even rivaling the performance of human-written questions.",https://github.com/EducationalTestingService/inquisitive-questions,-1
455c4b82-cc29-41e3-8660-e010dfd5a9fa,Unifying Event Detection and Captioning as Sequence Generation via Pre-Training,0.339094,"Dense video captioning aims to generate corresponding text descriptions for a
series of events in the untrimmed video, which can be divided into two
sub-tasks, event detection and event captioning. Unlike previous works that
tackle the two sub-tasks separately, recent works have focused on enhancing the
inter-task association between the two sub-tasks. However, designing inter-task
interactions for event detection and captioning is not trivial due to the large
differences in their task specific solutions. Besides, previous event detection
methods normally ignore temporal dependencies between events, leading to event
redundancy or inconsistency problems. To tackle above the two defects, in this
paper, we define event detection as a sequence generation task and propose a
unified pre-training and fine-tuning framework to naturally enhance the
inter-task association between event detection and captioning. Since the model
predicts each event with previous events as context, the inter-dependency
between events is fully exploited and thus our model can detect more diverse
and consistent events in the video. Experiments on the ActivityNet dataset show
that our model outperforms the state-of-the-art methods, and can be further
boosted when pre-trained on extra large-scale video-text data. Code is
available at \url{https://github.com/QiQAng/UEDVC}.",https://github.com/QiQAng/UEDVC,-1
cfe09e2f-4b83-4586-8d07-f0ce1d750d32,Space-based gravitational wave signal detection and extraction with deep neural network,0.730964,"Space-based gravitational wave (GW) detectors will be able to observe signals
from sources that are otherwise nearly impossible from current ground-based
detection. Consequently, the well established signal detection method, matched
filtering, will require a complex template bank, leading to a computational
cost that is too expensive in practice. Here, we develop a high-accuracy GW
signal detection and extraction method for all space-based GW sources. As a
proof of concept, we show that a science-driven and uniform multi-stage
self-attention-based deep neural network can identify synthetic signals that
are submerged in Gaussian noise. Our method exhibits a detection rate exceeding
99% in identifying signals from various sources, with the signal-to-noise ratio
at 50, at a false alarm rate of 1%. while obtaining at least 95% similarity
compared with target signals. We further demonstrate the interpretability and
strong generalization behavior for several extended scenarios.",https://github.com/AI-HPC-Research-Team/space_signal_detection_1,-1
6fa3378d-bca5-430a-bc43-4fb6a37a3c46,FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric,0.183343,"Syntax is a fundamental component of language, yet few metrics have been
employed to capture syntactic similarity or coherence at the utterance- and
document-level. The existing standard document-level syntactic similarity
metric is computationally expensive and performs inconsistently when faced with
syntactically dissimilar documents. To address these challenges, we present
FastKASSIM, a metric for utterance- and document-level syntactic similarity
which pairs and averages the most similar constituency parse trees between a
pair of documents based on tree kernels. FastKASSIM is more robust to syntactic
dissimilarities and runs up to to 5.32 times faster than its predecessor over
documents in the r/ChangeMyView corpus. FastKASSIM's improvements allow us to
examine hypotheses in two settings with large documents. We find that
syntactically similar arguments on r/ChangeMyView tend to be more persuasive,
and that syntax is predictive of authorship attribution in the Australian High
Court Judgment corpus.",https://github.com/jasonyux/FastKASSIM,-1
331f1edc-cf8a-4b69-8b33-5318d5d1d7ca,Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,0.905425,"This work presents a detailed linguistic analysis into why larger
Transformer-based pre-trained language models with more parameters and lower
perplexity nonetheless yield surprisal estimates that are less predictive of
human reading times. First, regression analyses show a strictly monotonic,
positive log-linear relationship between perplexity and fit to reading times
for the more recently released five GPT-Neo variants and eight OPT variants on
two separate datasets, replicating earlier results limited to just GPT-2 (Oh et
al., 2022). Subsequently, analysis of residual errors reveals a systematic
deviation of the larger variants, such as underpredicting reading times of
named entities and making compensatory overpredictions for reading times of
function words such as modals and conjunctions. These results suggest that the
propensity of larger Transformer-based models to 'memorize' sequences during
training makes their surprisal estimates diverge from humanlike expectations,
which warrants caution in using pre-trained language models to study human
language processing.",https://github.com/byungdoh/llm_surprisal,176
097ccbb8-d8e6-4fa0-b857-79fa7cda91f1,Where did you tweet from? Inferring the origin locations of tweets based on contextual information,0.617902,"Public conversations on Twitter comprise many pertinent topics including
disasters, protests, politics, propaganda, sports, climate change,
epidemics/pandemic outbreaks, etc., that can have both regional and global
aspects. Spatial discourse analysis rely on geographical data. However, today
less than 1% of tweets are geotagged; in both cases--point location or bounding
place information. A major issue with tweets is that Twitter users can be at
location A and exchange conversations specific to location B, which we call the
Location A/B problem. The problem is considered solved if location entities can
be classified as either origin locations (Location As) or non-origin locations
(Location Bs). In this work, we propose a simple yet effective framework--the
True Origin Model--to address the problem that uses machine-level natural
language understanding to identify tweets that conceivably contain their origin
location information. The model achieves promising accuracy at country (80%),
state (67%), city (58%), county (56%) and district (64%) levels with support
from a Location Extraction Model as basic as the CoNLL-2003-based RoBERTa. We
employ a tweet contexualizer (locBERT) which is one of the core components of
the proposed model, to investigate multiple tweets' distributions for
understanding Twitter users' tweeting behavior in terms of mentioning origin
and non-origin locations. We also highlight a major concern with the currently
regarded gold standard test set (ground truth) methodology, introduce a new
data set, and identify further research avenues for advancing the area.",None,-1
31a7ea13-e4df-4de9-b407-547d88066586,Attention Option-Critic,0.448617,"Temporal abstraction in reinforcement learning is the ability of an agent to
learn and use high-level behaviors, called options. The option-critic
architecture provides a gradient-based end-to-end learning method to construct
options. We propose an attention-based extension to this framework, which
enables the agent to learn to focus different options on different aspects of
the observation space. We show that this leads to behaviorally diverse options
which are also capable of state abstraction, and prevents the degeneracy
problems of option domination and frequent option switching that occur in
option-critic, while achieving a similar sample complexity. We also demonstrate
the more efficient, interpretable, and reusable nature of the learned options
in comparison with option-critic, through different transfer learning tasks.
Experimental results in a relatively simple four-rooms environment and the more
complex ALE (Arcade Learning Environment) showcase the efficacy of our
approach.",None,33465
57859df6-455c-4b9c-a049-68e9fc3fe8cf,VRKitchen2.0-IndoorKit: A Tutorial for Augmented Indoor Scene Building in Omniverse,0.206621,"With the recent progress of simulations by 3D modeling software and game
engines, many researchers have focused on Embodied AI tasks in the virtual
environment. However, the research community lacks a platform that can easily
serve both indoor scene synthesis and model benchmarking with various
algorithms. Meanwhile, computer graphics-related tasks need a toolkit for
implementing advanced synthesizing techniques. To facilitate the study of
indoor scene building methods and their potential robotics applications, we
introduce INDOORKIT: a built-in toolkit for NVIDIA OMNIVERSE that provides
flexible pipelines for indoor scene building, scene randomizing, and animation
controls. Besides, combining Python coding in the animation software INDOORKIT
assists researchers in creating real-time training and controlling avatars and
robotics. The source code for this toolkit is available at
https://github.com/realvcla/VRKitchen2.0-Tutorial, and the tutorial along with
the toolkit is available at https://vrkitchen20-tutorial.readthedocs.io/en/",https://github.com/realvcla/VRKitchen2.0-Tutorial,-1
497e8fa4-9824-4e3c-b1d7-21ae4c071412,MEAT: Maneuver Extraction from Agent Trajectories,0.0861104,"Advances in learning-based trajectory prediction are enabled by large-scale
datasets. However, in-depth analysis of such datasets is limited. Moreover, the
evaluation of prediction models is limited to metrics averaged over all samples
in the dataset. We propose an automated methodology that allows to extract
maneuvers (e.g., left turn, lane change) from agent trajectories in such
datasets. The methodology considers information about the agent dynamics and
information about the lane segments the agent traveled along. Although it is
possible to use the resulting maneuvers for training classification networks,
we exemplary use them for extensive trajectory dataset analysis and
maneuver-specific evaluation of multiple state-of-the-art trajectory prediction
models. Additionally, an analysis of the datasets and an evaluation of the
prediction models based on the agent dynamics is provided.",None,-1
a39695d8-a892-4cb5-9ede-76b48ba8cfb6,DiffPose: Toward More Reliable 3D Pose Estimation,0.99734,"Monocular 3D human pose estimation is quite challenging due to the inherent
ambiguity and occlusion, which often lead to high uncertainty and
indeterminacy. On the other hand, diffusion models have recently emerged as an
effective tool for generating high-quality images from noise. Inspired by their
capability, we explore a novel pose estimation framework (DiffPose) that
formulates 3D pose estimation as a reverse diffusion process. We incorporate
novel designs into our DiffPose to facilitate the diffusion process for 3D pose
estimation: a pose-specific initialization of pose uncertainty distributions, a
Gaussian Mixture Model-based forward diffusion process, and a
context-conditioned reverse diffusion process. Our proposed DiffPose
significantly outperforms existing methods on the widely used pose estimation
benchmarks Human3.6M and MPI-INF-3DHP. Project page:
https://gongjia0208.github.io/Diffpose/.",https://gongjia0208.github.io/Diffpose/,-1
7cbdd6d6-8f48-4823-b41d-18f8482b26c8,Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control,0.60274,"Hierarchical Imitation Learning (HIL) has been proposed to recover
highly-complex behaviors in long-horizon tasks from expert demonstrations by
modeling the task hierarchy with the option framework. Existing methods either
overlook the causal relationship between the subtask and its corresponding
policy or cannot learn the policy in an end-to-end fashion, which leads to
suboptimality. In this work, we develop a novel HIL algorithm based on
Adversarial Inverse Reinforcement Learning and adapt it with the
Expectation-Maximization algorithm in order to directly recover a hierarchical
policy from the unannotated demonstrations. Further, we introduce a directed
information term to the objective function to enhance the causality and propose
a Variational Autoencoder framework for learning with our objectives in an
end-to-end fashion. Theoretical justifications and evaluations on challenging
robotic control tasks are provided to show the superiority of our algorithm.
The codes are available at https://github.com/LucasCJYSDL/HierAIRL.",https://github.com/LucasCJYSDL/HierAIRL,-1
1cbb5a13-1688-49a6-bf22-7f9ee8e1d674,Towards Enabling Dynamic Convolution Neural Network Inference for Edge Intelligence,0.234058,"Deep learning applications have achieved great success in numerous real-world
applications. Deep learning models, especially Convolution Neural Networks
(CNN) are often prototyped using FPGA because it offers high power efficiency
and reconfigurability. The deployment of CNNs on FPGAs follows a design cycle
that requires saving of model parameters in the on-chip memory during
High-level synthesis (HLS). Recent advances in edge intelligence require CNN
inference on edge network to increase throughput and reduce latency. To provide
flexibility, dynamic parameter allocation to different mobile devices is
required to implement either a predefined or defined on-the-fly CNN
architecture. In this study, we present novel methodologies for dynamically
streaming the model parameters at run-time to implement a traditional CNN
architecture. We further propose a library-based approach to design scalable
and dynamic distributed CNN inference on the fly leveraging
partial-reconfiguration techniques, which is particularly suitable for
resource-constrained edge devices. The proposed techniques are implemented on
the Xilinx PYNQ-Z2 board to prove the concept by utilizing the LeNet-5 CNN
model. The results show that the proposed methodologies are effective, with
classification accuracy rates of 92%, 86%, and 94% respectively",None,-1
9ebf677f-a53f-4308-8f88-28a72d30d405,Studying Bias in GANs through the Lens of Race,0.978034,"In this work, we study how the performance and evaluation of generative image
models are impacted by the racial composition of their training datasets. By
examining and controlling the racial distributions in various training
datasets, we are able to observe the impacts of different training
distributions on generated image quality and the racial distributions of the
generated images. Our results show that the racial compositions of generated
images successfully preserve that of the training data. However, we observe
that truncation, a technique used to generate higher quality images during
inference, exacerbates racial imbalances in the data. Lastly, when examining
the relationship between image quality and race, we find that the highest
perceived visual quality images of a given race come from a distribution where
that race is well-represented, and that annotators consistently prefer
generated images of white people over those of Black people.",None,-1
f7714a45-667f-48c5-b683-55e30cb8f7b3,Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation,0.898629,"Optimizing behaviors for dexterous manipulation has been a longstanding
challenge in robotics, with a variety of methods from model-based control to
model-free reinforcement learning having been previously explored in
literature. Perhaps one of the most powerful techniques to learn complex
manipulation strategies is imitation learning. However, collecting and learning
from demonstrations in dexterous manipulation is quite challenging. The
complex, high-dimensional action-space involved with multi-finger control often
leads to poor sample efficiency of learning-based methods. In this work, we
propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning
framework for dexterous manipulation. DIME only requires a single RGB camera to
observe a human operator and teleoperate our robotic hand. Once demonstrations
are collected, DIME employs standard imitation learning methods to train
dexterous manipulation policies. On both simulation and real robot benchmarks
we demonstrate that DIME can be used to solve complex, in-hand manipulation
tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro
hand. Our framework along with pre-collected demonstrations is publicly
available at https://nyu-robot-learning.github.io/dime.",https://nyu-robot-learning.github.io/dime,-1
30f34c4e-7fec-4d70-a300-0585c9b1abdb,CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization,0.792252,"The quest for seeking health information has swamped the web with consumers'
health-related questions. Generally, consumers use overly descriptive and
peripheral information to express their medical condition or other healthcare
needs, contributing to the challenges of natural language understanding. One
way to address this challenge is to summarize the questions and distill the key
information of the original question. To address this issue, we introduce a new
dataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health
questions and corresponding summaries. The dataset is derived from the
community question-answering forum and therefore provides a valuable resource
for understanding consumer health-related posts on social media. We benchmark
the dataset on multiple state-of-the-art summarization models to show the
effectiveness of the dataset.",https://github.com/shwetanlp/Yahoo-CHQ-Summ,14142
513f756b-3407-4e69-9765-97c28b280c7d,1Cademy @ Causal News Corpus 2022: Enhance Causal Span Detection via Beam-Search-based Position Selector,0.622464,"In this paper, we present our approach and empirical observations for
Cause-Effect Signal Span Detection -- Subtask 2 of Shared task
3~\cite{tan-etal-2022-event} at CASE 2022. The shared task aims to extract the
cause, effect, and signal spans from a given causal sentence. We model the task
as a reading comprehension (RC) problem and apply a token-level RC-based span
prediction paradigm to the task as the baseline. We explore different training
objectives to fine-tune the model, as well as data augmentation (DA) tricks
based on the language model (LM) for performance improvement. Additionally, we
propose an efficient beam-search post-processing strategy to due with the
drawbacks of span detection to obtain a further performance gain. Our approach
achieves an average $F_1$ score of 54.15 and ranks \textbf{$1^{st}$} in the
CASE competition. Our code is available at
\url{https://github.com/Gzhang-umich/1CademyTeamOfCASE}.",https://github.com/Gzhang-umich/1CademyTeamOfCASE,-1
d90b3ae3-acbe-4aaf-b661-ab97604ef184,Learning Smooth Neural Functions via Lipschitz Regularization,0.879769,"Neural implicit fields have recently emerged as a useful representation for
3D shapes. These fields are commonly represented as neural networks which map
latent descriptors and 3D coordinates to implicit function values. The latent
descriptor of a neural field acts as a deformation handle for the 3D shape it
represents. Thus, smoothness with respect to this descriptor is paramount for
performing shape-editing operations. In this work, we introduce a novel
regularization designed to encourage smooth latent spaces in neural fields by
penalizing the upper bound on the field's Lipschitz constant. Compared with
prior Lipschitz regularized networks, ours is computationally fast, can be
implemented in four lines of code, and requires minimal hyperparameter tuning
for geometric applications. We demonstrate the effectiveness of our approach on
shape interpolation and extrapolation as well as partial shape reconstruction
from 3D point clouds, showing both qualitative and quantitative improvements
over existing state-of-the-art and non-regularized baselines.",http://github.com/google/jax,-1
d6dd95e8-f469-48c5-8c65-4fb6bd7b8f94,Convolutional and Residual Networks Provably Contain Lottery Tickets,0.926726,"The Lottery Ticket Hypothesis continues to have a profound practical impact
on the quest for small scale deep neural networks that solve modern deep
learning tasks at competitive performance. These lottery tickets are identified
by pruning large randomly initialized neural networks with architectures that
are as diverse as their applications. Yet, theoretical insights that attest
their existence have been mostly focused on deep fully-connected feed forward
networks with ReLU activation functions. We prove that also modern
architectures consisting of convolutional and residual layers that can be
equipped with almost arbitrary activation functions can contain lottery tickets
with high probability.",None,-1
126fe5e5-ec9f-44e0-aacb-889381273bac,Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance,0.854222,"Summarizing legal decisions requires the expertise of law practitioners,
which is both time- and cost-intensive. This paper presents techniques for
extractive summarization of legal decisions in a low-resource setting using
limited expert annotated data. We test a set of models that locate relevant
content using a sequential model and tackle redundancy by leveraging maximal
marginal relevance to compose summaries. We also demonstrate an implicit
approach to help train our proposed models generate more informative summaries.
Our multi-task learning model variant leverages rhetorical role identification
as an auxiliary task to further improve the summarizer. We perform extensive
experiments on datasets containing legal decisions from the US Board of
Veterans' Appeals and conduct quantitative and expert-ranked evaluations of our
models. Our results show that the proposed approaches can achieve ROUGE scores
vis-\`a-vis expert extracted summaries that match those achieved by
inter-annotator comparison.",https://github.com/LLTLab/VetClaims-JSON,-1
2bc4c2b2-7ec1-4acb-a815-232556d7853f,Image Super-Resolution With Deep Variational Autoencoders,0.49849,"Image super-resolution (SR) techniques are used to generate a high-resolution
image from a low-resolution image. Until now, deep generative models such as
autoregressive models and Generative Adversarial Networks (GANs) have proven to
be effective at modelling high-resolution images. VAE-based models have often
been criticised for their feeble generative performance, but with new
advancements such as VDVAE, there is now strong evidence that deep VAEs have
the potential to outperform current state-of-the-art models for high-resolution
image generation. In this paper, we introduce VDVAE-SR, a new model that aims
to exploit the most recent deep VAE methodologies to improve upon the results
of similar models. VDVAE-SR tackles image super-resolution using transfer
learning on pretrained VDVAEs. The presented model is competitive with other
state-of-the-art models, having comparable results on image quality metrics.",None,-1
a329d252-6012-436c-beaf-f059cb1e8aa4,A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and Pre-training,0.340155,"Multi-modal pre-training and knowledge discovery are two important research
topics in multi-modal machine learning. Nevertheless, none of existing works
make attempts to link knowledge discovery with knowledge guided multi-modal
pre-training. In this paper, we propose to unify them into a continuous
learning framework for mutual improvement. Taking the open-domain uni-modal
datasets of images and texts as input, we maintain a knowledge graph as the
foundation to support these two tasks. For knowledge discovery, a pre-trained
model is used to identify cross-modal links on the graph. For model
pre-training, the knowledge graph is used as the external knowledge to guide
the model updating. These two steps are iteratively performed in our framework
for continuous learning. The experimental results on MS-COCO and Flickr30K with
respect to both knowledge discovery and the pre-trained model validate the
effectiveness of our framework.",None,-1
21eb4914-0076-4149-9ac7-f45815f4f997,Counterfactual Data Augmentation improves Factuality of Abstractive Summarization,0.294218,"Abstractive summarization systems based on pretrained language models often
generate coherent but factually inconsistent sentences. In this paper, we
present a counterfactual data augmentation approach where we augment data with
perturbed summaries that increase the training data diversity. Specifically, we
present three augmentation approaches based on replacing (i) entities from
other and the same category and (ii) nouns with their corresponding WordNet
hypernyms. We show that augmenting the training data with our approach improves
the factual correctness of summaries without significantly affecting the ROUGE
score. We show that in two commonly used summarization datasets (CNN/Dailymail
and XSum), we improve the factual correctness by about 2.5 points on average",None,61938
8e03f137-8f36-4118-92d7-74fc508c301b,Explanations as Programs in Probabilistic Logic Programming,0.208357,"The generation of comprehensible explanations is an essential feature of
modern artificial intelligence systems. In this work, we consider probabilistic
logic programming, an extension of logic programming which can be useful to
model domains with relational structure and uncertainty. Essentially, a program
specifies a probability distribution over possible worlds (i.e., sets of
facts). The notion of explanation is typically associated with that of a world,
so that one often looks for the most probable world as well as for the worlds
where the query is true. Unfortunately, such explanations exhibit no causal
structure. In particular, the chain of inferences required for a specific
prediction (represented by a query) is not shown. In this paper, we propose a
novel approach where explanations are represented as programs that are
generated from a given query by a number of unfolding-like transformations.
Here, the chain of inferences that proves a given query is made explicit.
Furthermore, the generated explanations are minimal (i.e., contain no
irrelevant information) and can be parameterized w.r.t. a specification of
visible predicates, so that the user may hide uninteresting details from
explanations.",None,-1
47abc17a-954e-4a33-8348-cd622383a6f0,PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition,0.929234,"The widely studied task of Natural Language Inference (NLI) requires a system
to recognize whether one piece of text is textually entailed by another, i.e.
whether the entirety of its meaning can be inferred from the other. In current
NLI datasets and models, textual entailment relations are typically defined on
the sentence- or paragraph-level. However, even a simple sentence often
contains multiple propositions, i.e. distinct units of meaning conveyed by the
sentence. As these propositions can carry different truth values in the context
of a given premise, we argue for the need to recognize the textual entailment
relation of each proposition in a sentence individually.
  We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert
human raters. Our dataset structure resembles the tasks of (1) segmenting
sentences within a document to the set of propositions, and (2) classifying the
entailment relation of each proposition with respect to a different yet
topically-aligned document, i.e. documents describing the same event or entity.
We establish strong baselines for the segmentation and entailment tasks.
Through case studies on summary hallucination detection and document-level NLI,
we demonstrate that our conceptual framework is potentially useful for
understanding and explaining the compositionality of NLI labels.",https://github.com/google-research-datasets/propsegment,-1
31f5c7a0-26c1-493e-b055-d6f8a7cdcd2a,SciNLI: A Corpus for Natural Language Inference on Scientific Text,0.745651,"Existing Natural Language Inference (NLI) datasets, while being instrumental
in the advancement of Natural Language Understanding (NLU) research, are not
related to scientific text. In this paper, we introduce SciNLI, a large dataset
for NLI that captures the formality in scientific text and contains 107,412
sentence pairs extracted from scholarly papers on NLP and computational
linguistics. Given that the text used in scientific literature differs vastly
from the text used in everyday language both in terms of vocabulary and
sentence structure, our dataset is well suited to serve as a benchmark for the
evaluation of scientific NLU models. Our experiments show that SciNLI is harder
to classify than the existing NLI datasets. Our best performing model with
XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23%
showing that there is substantial room for improvement.",https://github.com/msadat3/SciNLI,6491
9379c13d-a345-4ce5-9450-7d0016c62406,CenterFormer: Center-based Transformer for 3D Object Detection,0.850695,"Query-based transformer has shown great potential in constructing long-range
attention in many image-domain tasks, but has rarely been considered in
LiDAR-based 3D object detection due to the overwhelming size of the point cloud
data. In this paper, we propose CenterFormer, a center-based transformer
network for 3D object detection. CenterFormer first uses a center heatmap to
select center candidates on top of a standard voxel-based point cloud encoder.
It then uses the feature of the center candidate as the query embedding in the
transformer. To further aggregate features from multiple frames, we design an
approach to fuse features through cross-attention. Lastly, regression heads are
added to predict the bounding box on the output center feature representation.
Our design reduces the convergence difficulty and computational complexity of
the transformer structure. The results show significant improvements over the
strong baseline of anchor-free object detection networks. CenterFormer achieves
state-of-the-art performance for a single model on the Waymo Open Dataset, with
73.7% mAPH on the validation set and 75.6% mAPH on the test set, significantly
outperforming all previously published CNN and transformer-based methods. Our
code is publicly available at https://github.com/TuSimple/centerformer",https://github.com/TuSimple/centerformer,-1
fcdc90ee-8ead-4be3-80a3-9af1c51ff65d,Darwinian Model Upgrades: Model Evolving with Selective Compatibility,0.150483,"The traditional model upgrading paradigm for retrieval requires recomputing
all gallery embeddings before deploying the new model (dubbed as
""backfilling""), which is quite expensive and time-consuming considering
billions of instances in industrial applications. BCT presents the first step
towards backward-compatible model upgrades to get rid of backfilling. It is
workable but leaves the new model in a dilemma between new feature
discriminativeness and new-to-old compatibility due to the undifferentiated
compatibility constraints. In this work, we propose Darwinian Model Upgrades
(DMU), which disentangle the inheritance and variation in the model evolving
with selective backward compatibility and forward adaptation, respectively. The
old-to-new heritable knowledge is measured by old feature discriminativeness,
and the gallery features, especially those of poor quality, are evolved in a
lightweight manner to become more adaptive in the new latent space. We
demonstrate the superiority of DMU through comprehensive experiments on
large-scale landmark retrieval and face recognition benchmarks. DMU effectively
alleviates the new-to-new degradation and improves new-to-old compatibility,
rendering a more proper model upgrading paradigm in large-scale retrieval
systems.",None,-1
807f3311-42b0-439d-a522-4f09bb1a851c,Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation,0.241894,"We introduce Action-GPT, a plug-and-play framework for incorporating Large
Language Models (LLMs) into text-based action generation models. Action phrases
in current motion capture datasets contain minimal and to-the-point
information. By carefully crafting prompts for LLMs, we generate richer and
fine-grained descriptions of the action. We show that utilizing these detailed
descriptions instead of the original action phrases leads to better alignment
of text and motion spaces. We introduce a generic approach compatible with
stochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion
models. In addition, the approach enables multiple text descriptions to be
utilized. Our experiments show (i) noticeable qualitative and quantitative
improvement in the quality of synthesized motions, (ii) benefits of utilizing
multiple LLM-generated descriptions, (iii) suitability of the prompt function,
and (iv) zero-shot generation capabilities of the proposed approach. Project
page: https://actiongpt.github.io",https://actiongpt.github.io,-1
345873d9-9d45-46b3-a538-048f5a4a867b,Deep Speech Based End-to-End Automated Speech Recognition (ASR) for Indian-English Accents,0.345501,"Automated Speech Recognition (ASR) is an interdisciplinary application of
computer science and linguistics that enable us to derive the transcription
from the uttered speech waveform. It finds several applications in Military
like High-performance fighter aircraft, helicopters, air-traffic controller.
Other than military speech recognition is used in healthcare, persons with
disabilities and many more. ASR has been an active research area. Several
models and algorithms for speech to text (STT) have been proposed. One of the
most recent is Mozilla Deep Speech, it is based on the Deep Speech research
paper by Baidu. Deep Speech is a state-of-art speech recognition system is
developed using end-to-end deep learning, it is trained using well-optimized
Recurrent Neural Network (RNN) training system utilizing multiple Graphical
Processing Units (GPUs). This training is mostly done using American-English
accent datasets, which results in poor generalizability to other English
accents. India is a land of vast diversity. This can even be seen in the
speech, there are several English accents which vary from state to state. In
this work, we have used transfer learning approach using most recent Deep
Speech model i.e., deepspeech-0.9.3 to develop an end-to-end speech recognition
system for Indian-English accents. This work utilizes fine-tuning and data
argumentation to further optimize and improve the Deep Speech ASR system. Indic
TTS data of Indian-English accents is used for transfer learning and
fine-tuning the pre-trained Deep Speech model. A general comparison is made
among the untrained model, our trained model and other available speech
recognition services for Indian-English Accents.",None,15
1ca17405-7a85-427d-a54b-0f9a6916624a,Smart Speech Segmentation using Acousto-Linguistic Features with look-ahead,0.519278,"Segmentation for continuous Automatic Speech Recognition (ASR) has
traditionally used silence timeouts or voice activity detectors (VADs), which
are both limited to acoustic features. This segmentation is often overly
aggressive, given that people naturally pause to think as they speak.
Consequently, segmentation happens mid-sentence, hindering both punctuation and
downstream tasks like machine translation for which high-quality segmentation
is critical. Model-based segmentation methods that leverage acoustic features
are powerful, but without an understanding of the language itself, these
approaches are limited. We present a hybrid approach that leverages both
acoustic and language information to improve segmentation. Furthermore, we show
that including one word as a look-ahead boosts segmentation quality. On
average, our models improve segmentation-F0.5 score by 9.8% over baseline. We
show that this approach works for multiple languages. For the downstream task
of machine translation, it improves the translation BLEU score by an average of
1.05 points.",None,753
21f04163-605b-47ec-b5b6-c13cb9bce571,Enriching Unsupervised User Embedding via Medical Concepts,0.056089,"Clinical notes in Electronic Health Records (EHR) present rich documented
information of patients to inference phenotype for disease diagnosis and study
patient characteristics for cohort selection. Unsupervised user embedding aims
to encode patients into fixed-length vectors without human supervisions.
Medical concepts extracted from the clinical notes contain rich connections
between patients and their clinical categories. However, existing unsupervised
approaches of user embeddings from clinical notes do not explicitly incorporate
medical concepts. In this study, we propose a concept-aware unsupervised user
embedding that jointly leverages text documents and medical concepts from two
clinical corpora, MIMIC-III and Diabetes. We evaluate user embeddings on both
extrinsic and intrinsic tasks, including phenotype classification, in-hospital
mortality prediction, patient retrieval, and patient relatedness. Experiments
on the two clinical corpora show our approach exceeds unsupervised baselines,
and incorporating medical concepts can significantly improve the baseline
performance.",None,-1
bd76d05a-010d-4ef9-8a99-e96e95ca87f2,The Internet of Senses: Building on Semantic Communications and Edge Intelligence,0.853393,"The Internet of Senses (IoS) holds the promise of flawless telepresence-style
communication for all human `receptors' and therefore blurs the difference of
virtual and real environments. We commence by highlighting the compelling use
cases empowered by the IoS and also the key network requirements. We then
elaborate on how the emerging semantic communications and Artificial
Intelligence (AI)/Machine Learning (ML) paradigms along with 6G technologies
may satisfy the requirements of IoS use cases. On one hand, semantic
communications can be applied for extracting meaningful and significant
information and hence efficiently exploit the resources and for harnessing a
priori information at the receiver to satisfy IoS requirements. On the other
hand, AI/ML facilitates frugal network resource management by making use of the
enormous amount of data generated in IoS edge nodes and devices, as well as by
optimizing the IoS performance via intelligent agents. However, the intelligent
agents deployed at the edge are not completely aware of each others' decisions
and the environments of each other, hence they operate in a partially rather
than fully observable environment. Therefore, we present a case study of
Partially Observable Markov Decision Processes (POMDP) for improving the User
Equipment (UE) throughput and energy consumption, as they are imperative for
IoS use cases, using Reinforcement Learning for astutely activating and
deactivating the component carriers in carrier aggregation. Finally, we outline
the challenges and open issues of IoS implementations and employing semantic
communications, edge intelligence as well as learning under partial
observability in the IoS context.",None,-1
5160076c-211b-4d1d-bfad-57e09dadc557,Clinical Prompt Learning with Frozen Language Models,0.344357,"Prompt learning is a new paradigm in the Natural Language Processing (NLP)
field which has shown impressive performance on a number of natural language
tasks with common benchmarking text datasets in full, few-shot, and zero-shot
train-evaluation setups. Recently, it has even been observed that large but
frozen pre-trained language models (PLMs) with prompt learning outperform
smaller but fine-tuned models. However, as with many recent NLP trends, the
performance of even the largest PLMs such as GPT-3 do not perform well on
specialized domains (e.g. medical text), and the common practice to achieve
State of the Art (SoTA) results still consists of pre-training and fine-tuning
the PLMs on downstream tasks. The reliance on fine-tuning large PLMs is
problematic in clinical settings where data is often held in non-GPU
environments, and more resource efficient methods of training specialized
domain models is crucial. We investigated the viability of prompt learning on
clinically meaningful decision tasks and directly compared with more
traditional fine-tuning methods. Results are partially in line with the prompt
learning literature, with prompt learning able to match or improve on
traditional fine-tuning with substantially fewer trainable parameters and
requiring less training data. We argue that prompt learning therefore provides
lower computational resource costs applicable to clinical settings, that can
serve as an alternative to fine-tuning ever increasing in size PLMs.
Complementary code to reproduce experiments presented in this work can be found
at: https://github.com/NtaylorOX/Public_Clinical_Prompt.",None,-1
f5773c30-f0e7-4543-87f3-8506af84eec8,A Novel Underwater Image Enhancement and Improved Underwater Biological Detection Pipeline,0.323108,"For aquaculture resource evaluation and ecological environment monitoring,
automatic detection and identification of marine organisms is critical.
However, due to the low quality of underwater images and the characteristics of
underwater biological, a lack of abundant features may impede traditional
hand-designed feature extraction approaches or CNN-based object detection
algorithms, particularly in complex underwater environment. Therefore, the goal
of this paper is to perform object detection in the underwater environment.
This paper proposed a novel method for capturing feature information, which
adds the convolutional block attention module (CBAM) to the YOLOv5 backbone.
The interference of underwater creature characteristics on object
characteristics is decreased, and the output of the backbone network to object
information is enhanced. In addition, the self-adaptive global histogram
stretching algorithm (SAGHS) is designed to eliminate the degradation problems
such as low contrast and color loss caused by underwater environmental
information to better restore image quality. Extensive experiments and
comprehensive evaluation on the URPC2021 benchmark dataset demonstrate the
effectiveness and adaptivity of our methods. Beyond that, this paper conducts
an exhaustive analysis of the role of training data on performance.",None,-1
8e849333-d3e7-4d2e-9d4d-2ab0c2ffc423,On amortizing convex conjugates for optimal transport,0.967344,"This paper focuses on computing the convex conjugate operation that arises
when solving Euclidean Wasserstein-2 optimal transport problems. This
conjugation, which is also referred to as the Legendre-Fenchel conjugate or
c-transform,is considered difficult to compute and in practice,Wasserstein-2
methods are limited by not being able to exactly conjugate the dual potentials
in continuous space. To overcome this, the computation of the conjugate can be
approximated with amortized optimization, which learns a model to predict the
conjugate. I show that combining amortized approximations to the conjugate with
a solver for fine-tuning significantly improves the quality of transport maps
learned for the Wasserstein-2 benchmark by Korotin et al. (2021a) and is able
to model many 2-dimensional couplings and flows considered in the literature.
All of the baselines, methods, and solvers in this paper are available at
http://github.com/facebookresearch/w2ot.",http://github.com/facebookresearch/w2ot,-1
32d8be1a-db0f-4fe1-b1b7-6fa5e15adb3f,SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection,0.940169,"Convolutional neural networks (CNNs) are good at extracting contexture
features within certain receptive fields, while transformers can model the
global long-range dependency features. By absorbing the advantage of
transformer and the merit of CNN, Swin Transformer shows strong feature
representation ability. Based on it, we propose a cross-modality fusion model
SwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin
Transformer to extract the hierarchical features, boosted by attention
mechanism to bridge the gap between two modalities, and guided by edge
information to sharp the contour of salient object. To be specific, two-stream
Swin Transformer encoder first extracts multi-modality features, and then
spatial alignment and channel re-calibration module is presented to optimize
intra-level cross-modality features. To clarify the fuzzy boundary, edge-guided
decoder achieves inter-level cross-modality fusion under the guidance of edge
features. The proposed model outperforms the state-of-the-art models on RGB-D
and RGB-T datasets, showing that it provides more insight into the
cross-modality complementarity task.",https://github.com/liuzywen/SwinNet,5263
b283b368-65c7-4859-9eea-2961194c4ab9,IMPaSh: A Novel Domain-shift Resistant Representation for Colorectal Cancer Tissue Classification,0.362213,"The appearance of histopathology images depends on tissue type, staining and
digitization procedure. These vary from source to source and are the potential
causes for domain-shift problems. Owing to this problem, despite the great
success of deep learning models in computational pathology, a model trained on
a specific domain may still perform sub-optimally when we apply them to another
domain. To overcome this, we propose a new augmentation called PatchShuffling
and a novel self-supervised contrastive learning framework named IMPaSh for
pre-training deep learning models. Using these, we obtained a ResNet50 encoder
that can extract image representation resistant to domain-shift. We compared
our derived representation against those acquired based on other
domain-generalization techniques by using them for the cross-domain
classification of colorectal tissue images. We show that the proposed method
outperforms other traditional histology domain-adaptation and state-of-the-art
self-supervised learning methods. Code is available at:
https://github.com/trinhvg/IMPash .",https://github.com/trinhvg/IMPash,-1
92c97088-fb82-4cb2-9937-fcdf4dcd8904,SwinUNet3D -- A Hierarchical Architecture for Deep Traffic Prediction using Shifted Window Transformers,0.358019,"Traffic forecasting is an important element of mobility management, an
important key that drives the logistics industry. Over the years, lots of work
have been done in Traffic forecasting using time series as well as
spatiotemporal dynamic forecasting. In this paper, we explore the use of vision
transformer in a UNet setting. We completely remove all convolution-based
building blocks in UNet, while using 3D shifted window transformer in both
encoder and decoder branches. In addition, we experiment with the use of
feature mixing just before patch encoding to control the inter-relationship of
the feature while avoiding contraction of the depth dimension of our
spatiotemporal input. The proposed network is tested on the data provided by
Traffic Map Movie Forecasting Challenge 2021(Traffic4cast2021), held in the
competition track of Neural Information Processing Systems (NeurIPS).
Traffic4cast2021 task is to predict an hour (6 frames) of traffic conditions
(volume and average speed)from one hour of given traffic state (12 frames
averaged in 5 minutes time span). Source code is available online at
https://github.com/bojesomo/Traffic4Cast2021-SwinUNet3D.",https://github.com/bojesomo/Trafc4Cast2021-SwinUNet3D,-1
8b6b5d44-2ba4-42e2-bf45-49bec01498ed,CorefDiffs: Co-referential and Differential Knowledge Flow in Document Grounded Conversations,0.46327,"Knowledge-grounded dialog systems need to incorporate smooth transitions
among knowledge selected for generating responses, to ensure that dialog flows
naturally. For document-grounded dialog systems, the inter- and intra-document
knowledge relations can be used to model such conversational flows. We develop
a novel Multi-Document Co-Referential Graph (Coref-MDG) to effectively capture
the inter-document relationships based on commonsense and similarity and the
intra-document co-referential structures of knowledge segments within the
grounding documents. We propose CorefDiffs, a Co-referential and Differential
flow management method, to linearize the static Coref-MDG into conversational
sequence logic. CorefDiffs performs knowledge selection by accounting for
contextual graph structures and the knowledge difference sequences. CorefDiffs
significantly outperforms the state-of-the-art by 9.5\%, 7.4\%, and 8.2\% on
three public benchmarks. This demonstrates that the effective modeling of
co-reference and knowledge difference for dialog flows are critical for
transitions in document-grounded conversation",https://github.com/cathyxl/coref-diffs,-1
ab16e516-fcdd-4f11-816d-33484202c5d8,Enriching Relation Extraction with OpenIE,0.0893831,"Relation extraction (RE) is a sub-discipline of information extraction (IE)
which focuses on the prediction of a relational predicate from a
natural-language input unit (such as a sentence, a clause, or even a short
paragraph consisting of multiple sentences and/or clauses). Together with
named-entity recognition (NER) and disambiguation (NED), RE forms the basis for
many advanced IE tasks such as knowledge-base (KB) population and verification.
In this work, we explore how recent approaches for open information extraction
(OpenIE) may help to improve the task of RE by encoding structured information
about the sentences' principal units, such as subjects, objects, verbal
phrases, and adverbials, into various forms of vectorized (and hence
unstructured) representations of the sentences. Our main conjecture is that the
decomposition of long and possibly convoluted sentences into multiple smaller
clauses via OpenIE even helps to fine-tune context-sensitive language models
such as BERT (and its plethora of variants) for RE. Our experiments over two
annotated corpora, KnowledgeNet and FewRel, demonstrate the improved accuracy
of our enriched models compared to existing RE approaches. Our best results
reach 92% and 71% of F1 score for KnowledgeNet and FewRel, respectively,
proving the effectiveness of our approach on competitive benchmarks.",https://github.com/dair-iitd/OpenIE-standalone,-1
055781b6-a489-4a94-9581-e811e475915f,Learnable Graph Convolutional Network and Feature Fusion for Multi-view Learning,0.983776,"In practical applications, multi-view data depicting objectives from assorted
perspectives can facilitate the accuracy increase of learning algorithms.
However, given multi-view data, there is limited work for learning
discriminative node relationships and graph information simultaneously via
graph convolutional network that has drawn the attention from considerable
researchers in recent years. Most of existing methods only consider the
weighted sum of adjacency matrices, yet a joint neural network of both feature
and graph fusion is still under-explored. To cope with these issues, this paper
proposes a joint deep learning framework called Learnable Graph Convolutional
Network and Feature Fusion (LGCN-FF), consisting of two stages: feature fusion
network and learnable graph convolutional network. The former aims to learn an
underlying feature representation from heterogeneous views, while the latter
explores a more discriminative graph fusion via learnable weights and a
parametric activation function dubbed Differentiable Shrinkage Activation (DSA)
function. The proposed LGCN-FF is validated to be superior to various
state-of-the-art methods in multi-view semi-supervised classification.",None,-1
d21e4321-9eae-4e06-b727-3342c9d43911,Lymphoma segmentation from 3D PET-CT images using a deep evidential network,0.304672,"An automatic evidential segmentation method based on Dempster-Shafer theory
and deep learning is proposed to segment lymphomas from three-dimensional
Positron Emission Tomography (PET) and Computed Tomography (CT) images. The
architecture is composed of a deep feature-extraction module and an evidential
layer. The feature extraction module uses an encoder-decoder framework to
extract semantic feature vectors from 3D inputs. The evidential layer then uses
prototypes in the feature space to compute a belief function at each voxel
quantifying the uncertainty about the presence or absence of a lymphoma at this
location. Two evidential layers are compared, based on different ways of using
distances to prototypes for computing mass functions. The whole model is
trained end-to-end by minimizing the Dice loss function. The proposed
combination of deep feature extraction and evidential segmentation is shown to
outperform the baseline UNet model as well as three other state-of-the-art
models on a dataset of 173 patients.",https://github.com/iWeisskohl,-1
3658edac-0c1b-4d82-8577-da8405c42d45,Bi-Noising Diffusion: Towards Conditional Diffusion Models with Generative Restoration Priors,0.0671218,"Conditional diffusion probabilistic models can model the distribution of
natural images and can generate diverse and realistic samples based on given
conditions. However, oftentimes their results can be unrealistic with
observable color shifts and textures. We believe that this issue results from
the divergence between the probabilistic distribution learned by the model and
the distribution of natural images. The delicate conditions gradually enlarge
the divergence during each sampling timestep. To address this issue, we
introduce a new method that brings the predicted samples to the training data
manifold using a pretrained unconditional diffusion model. The unconditional
model acts as a regularizer and reduces the divergence introduced by the
conditional model at each sampling step. We perform comprehensive experiments
to demonstrate the effectiveness of our approach on super-resolution,
colorization, turbulence removal, and image-deraining tasks. The improvements
obtained by our method suggest that the priors can be incorporated as a general
plugin for improving conditional diffusion models.",None,-1
b8fc29c6-194c-41a6-bff7-0f16b0bbe488,Fine-tuning Image Transformers using Learnable Memory,0.413635,"In this paper we propose augmenting Vision Transformer models with learnable
memory tokens. Our approach allows the model to adapt to new tasks, using few
parameters, while optionally preserving its capabilities on previously learned
tasks. At each layer we introduce a set of learnable embedding vectors that
provide contextual information useful for specific datasets. We call these
""memory tokens"". We show that augmenting a model with just a handful of such
tokens per layer significantly improves accuracy when compared to conventional
head-only fine-tuning, and performs only slightly below the significantly more
expensive full fine-tuning. We then propose an attention-masking approach that
enables extension to new downstream tasks, with a computation reuse. In this
setup in addition to being parameters efficient, models can execute both old
and new tasks as a part of single inference at a small incremental cost.",None,-1
3c3dbb71-5a78-4779-b955-e2ef1723bf64,Understanding Natural Language in Context,0.0380673,"Recent years have seen an increasing number of applications that have a
natural language interface, either in the form of chatbots or via personal
assistants such as Alexa (Amazon), Google Assistant, Siri (Apple), and Cortana
(Microsoft). To use these applications, a basic dialog between the robot and
the human is required.
  While this kind of dialog exists today mainly within ""static"" robots that do
not make any movement in the household space, the challenge of reasoning about
the information conveyed by the environment increases significantly when
dealing with robots that can move and manipulate objects in our home
environment.
  In this paper, we focus on cognitive robots, which have some knowledge-based
models of the world and operate by reasoning and planning with this model.
Thus, when the robot and the human communicate, there is already some formalism
they can use - the robot's knowledge representation formalism.
  Our goal in this research is to translate natural language utterances into
this robot's formalism, allowing much more complicated household tasks to be
completed. We do so by combining off-the-shelf SOTA language models, planning
tools, and the robot's knowledge-base for better communication. In addition, we
analyze different directive types and illustrate the contribution of the
world's context to the translation process.",None,-1
0fecc4c7-09af-4b1f-b843-829ec93e1401,Near-Optimal Multi-Agent Learning for Safe Coverage Control,0.836703,"In multi-agent coverage control problems, agents navigate their environment
to reach locations that maximize the coverage of some density. In practice, the
density is rarely known $\textit{a priori}$, further complicating the original
NP-hard problem. Moreover, in many applications, agents cannot visit arbitrary
locations due to $\textit{a priori}$ unknown safety constraints. In this paper,
we aim to efficiently learn the density to approximately solve the coverage
problem while preserving the agents' safety. We first propose a conditionally
linear submodular coverage function that facilitates theoretical analysis.
Utilizing this structure, we develop MacOpt, a novel algorithm that efficiently
trades off the exploration-exploitation dilemma due to partial observability,
and show that it achieves sublinear regret. Next, we extend results on
single-agent safe exploration to our multi-agent setting and propose SafeMac
for safe coverage and exploration. We analyze SafeMac and give first of its
kind results: near optimal coverage in finite time while provably guaranteeing
safety. We extensively evaluate our algorithms on synthetic and real problems,
including a bio-diversity monitoring task under safety constraints, where
SafeMac outperforms competing methods.",https://github.com/manish-pra/SafeMaC,-1
6bd1b311-1e4b-4f79-a397-c8044b80c490,HEATGait: Hop-Extracted Adjacency Technique in Graph Convolution based Gait Recognition,0.152832,"Biometric authentication using gait has become a promising field due to its
unobtrusive nature. Recent approaches in model-based gait recognition
techniques utilize spatio-temporal graphs for the elegant extraction of gait
features. However, existing methods often rely on multi-scale operators for
extracting long-range relationships among joints resulting in biased weighting.
In this paper, we present HEATGait, a gait recognition system that improves the
existing multi-scale graph convolution by efficient hop-extraction technique to
alleviate the issue. Combined with preprocessing and augmentation techniques,
we propose a powerful feature extractor that utilizes ResGCN to achieve
state-of-the-art performance in model-based gait recognition on the CASIA-B
gait dataset.",None,-1
b30a9518-4aa8-49fd-86c2-e73e921ef016,SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation,0.995159,"Visual anomaly detection is commonly used in industrial quality inspection.
In this paper, we present a new dataset as well as a new self-supervised
learning method for ImageNet pre-training to improve anomaly detection and
segmentation in 1-class and 2-class 5/10/high-shot training setups. We release
the Visual Anomaly (VisA) Dataset consisting of 10,821 high-resolution color
images (9,621 normal and 1,200 anomalous samples) covering 12 objects in 3
domains, making it the largest industrial anomaly detection dataset to date.
Both image and pixel-level labels are provided. We also propose a new
self-supervised framework - SPot-the-difference (SPD) - which can regularize
contrastive self-supervised pre-training, such as SimSiam, MoCo and SimCLR, to
be more suitable for anomaly detection tasks. Our experiments on VisA and
MVTec-AD dataset show that SPD consistently improves these contrastive
pre-training baselines and even the supervised pre-training. For example, SPD
improves Area Under the Precision-Recall curve (AU-PR) for anomaly segmentation
by 5.9% and 6.8% over SimSiam and supervised pre-training respectively in the
2-class high-shot regime. We open-source the project at
http://github.com/amazon-research/spot-diff .",http://github.com/amazon-research/spot-diff,-1
96541ecf-0c47-4164-81a9-ba08846eabea,A new band selection approach based on information theory and support vector machine for hyperspectral images reduction and classification,0.278905,"The high dimensionality of hyperspectral images consisting of several bands
often imposes a big computational challenge for image processing. Therefore,
spectral band selection is an essential step for removing the irrelevant, noisy
and redundant bands. Consequently increasing the classification accuracy.
However, identification of useful bands from hundreds or even thousands of
related bands is a nontrivial task. This paper aims at identifying a small set
of highly discriminative bands, for improving computational speed and
prediction accuracy. Hence, we proposed a new strategy based on joint mutual
information to measure the statistical dependence and correlation between the
selected bands and evaluate the relative utility of each one to classification.
The proposed filter approach is compared to an effective reproduced filters
based on mutual information. Simulations results on the hyperpectral image HSI
AVIRIS 92AV3C using the SVM classifier have shown that the effective proposed
algorithm outperforms the reproduced filters strategy performance.
  Keywords-Hyperspectral images, Classification, band Selection, Joint Mutual
Information, dimensionality reduction ,correlation, SVM.",None,-1
686383b9-5438-41e2-9606-8ac5ef6242de,Unsupervised Scene Sketch to Photo Synthesis,0.798104,"Sketches make an intuitive and powerful visual expression as they are fast
executed freehand drawings. We present a method for synthesizing realistic
photos from scene sketches. Without the need for sketch and photo pairs, our
framework directly learns from readily available large-scale photo datasets in
an unsupervised manner. To this end, we introduce a standardization module that
provides pseudo sketch-photo pairs during training by converting photos and
sketches to a standardized domain, i.e. the edge map. The reduced domain gap
between sketch and photo also allows us to disentangle them into two
components: holistic scene structures and low-level visual styles such as color
and texture. Taking this advantage, we synthesize a photo-realistic image by
combining the structure of a sketch and the visual style of a reference photo.
Extensive experimental results on perceptual similarity metrics and human
perceptual studies show the proposed method could generate realistic photos
with high fidelity from scene sketches and outperform state-of-the-art photo
synthesis baselines. We also demonstrate that our framework facilitates a
controllable manipulation of photo synthesis by editing strokes of
corresponding sketches, delivering more fine-grained details than previous
approaches that rely on region-level editing.",None,-1
7f8f9713-3983-42f7-ab16-1a5a0cdd4701,Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition,0.920629,"Capturing the dependencies between joints is critical in skeleton-based
action recognition task. Transformer shows great potential to model the
correlation of important joints. However, the existing Transformer-based
methods cannot capture the correlation of different joints between frames,
which the correlation is very useful since different body parts (such as the
arms and legs in ""long jump"") between adjacent frames move together. Focus on
this problem, A novel spatio-temporal tuples Transformer (STTFormer) method is
proposed. The skeleton sequence is divided into several parts, and several
consecutive frames contained in each part are encoded. And then a
spatio-temporal tuples self-attention module is proposed to capture the
relationship of different joints in consecutive frames. In addition, a feature
aggregation module is introduced between non-adjacent frames to enhance the
ability to distinguish similar actions. Compared with the state-of-the-art
methods, our method achieves better performance on two large-scale datasets.",https://github.com/heleiqiu/STTFormer,-1
4e77dfe1-2e4f-4fe6-ba23-df78a3e54a93,Generating Compressed Combinatory Proof Structures -- An Approach to Automated First-Order Theorem Proving,0.312119,"Representing a proof tree by a combinator term that reduces to the tree lets
subtle forms of duplication within the tree materialize as duplicated subterms
of the combinator term. In a DAG representation of the combinator term these
straightforwardly factor into shared subgraphs. To search for proofs,
combinator terms can be enumerated, like clausal tableaux, interwoven with
unification of formulas that are associated with nodes of the enumerated
structures. To restrict the search space, the enumeration can be based on proof
schemas defined as parameterized combinator terms. We introduce here this
""combinator term as proof structure"" approach to automated first-order proving,
present an implementation and first experimental results. The approach builds
on a term view of proof structures rooted in condensed detachment and the
connection method. It realizes features known from the connection structure
calculus, which has not been implemented so far.",None,-1
44175900-888c-451d-9254-fc46a6ffae20,Ranking-Enhanced Unsupervised Sentence Representation Learning,0.708646,"Unsupervised sentence representation learning has progressed through
contrastive learning and data augmentation methods such as dropout masking.
Despite this progress, sentence encoders are still limited to using only an
input sentence when predicting its semantic vector. In this work, we show that
the semantic meaning of a sentence is also determined by nearest-neighbor
sentences that are similar to the input sentence. Based on this finding, we
propose a novel unsupervised sentence encoder, RankEncoder. RankEncoder
predicts the semantic vector of an input sentence by leveraging its
relationship with other sentences in an external corpus, as well as the input
sentence itself. We evaluate RankEncoder on semantic textual benchmark
datasets. From the experimental results, we verify that 1) RankEncoder achieves
80.07% Spearman's correlation, a 1.1% absolute improvement compared to the
previous state-of-the-art performance, 2) RankEncoder is universally applicable
to existing unsupervised sentence embedding methods, and 3) RankEncoder is
specifically effective for predicting the similarity scores of similar sentence
pairs.",https://github.com/yeonsw/RankEncoder.git,-1
69640d9f-25c8-49ab-a01b-c88e912e8191,Gaussian Multi-head Attention for Simultaneous Machine Translation,0.607292,"Simultaneous machine translation (SiMT) outputs translation while receiving
the streaming source inputs, and hence needs a policy to determine where to
start translating. The alignment between target and source words often implies
the most informative source word for each target word, and hence provides the
unified control over translation quality and latency, but unfortunately the
existing SiMT methods do not explicitly model the alignment to perform the
control. In this paper, we propose Gaussian Multi-head Attention (GMA) to
develop a new SiMT policy by modeling alignment and translation in a unified
manner. For SiMT policy, GMA models the aligned source position of each target
word, and accordingly waits until its aligned position to start translating. To
integrate the learning of alignment into the translation model, a Gaussian
distribution centered on predicted aligned position is introduced as an
alignment-related prior, which cooperates with translation-related soft
attention to determine the final attention. Experiments on En-Vi and De-En
tasks show that our method outperforms strong baselines on the trade-off
between translation and latency.",https://github.com/ictnlp/GMA,-1
8c7e095f-1db7-470a-bbb6-29fafe3b44e1,How do we get there? Evaluating transformer neural networks as cognitive models for English past tense inflection,0.00201993,"There is an ongoing debate on whether neural networks can grasp the
quasi-regularities in languages like humans. In a typical quasi-regularity
task, English past tense inflections, the neural network model has long been
criticized that it learns only to generalize the most frequent pattern, but not
the regular pattern, thus can not learn the abstract categories of regular and
irregular and is dissimilar to human performance. In this work, we train a set
of transformer models with different settings to examine their behavior on this
task. The models achieved high accuracy on unseen regular verbs and some
accuracy on unseen irregular verbs. The models' performance on the regulars is
heavily affected by type frequency and ratio but not token frequency and ratio,
and vice versa for the irregulars. The different behaviors on the regulars and
irregulars suggest that the models have some degree of symbolic learning on the
regularity of the verbs. In addition, the models are weakly correlated with
human behavior on nonce verbs. Although the transformer model exhibits some
level of learning on the abstract category of verb regularity, its performance
does not fit human data well, suggesting that it might not be a good cognitive
model.",https://github.com/xiaomeng-ma/English-Past-Tense,-1
736846ad-a90c-42b6-a7a2-c4cab53e9bad,Sparse Conditional Hidden Markov Model for Weakly Supervised Named Entity Recognition,0.650643,"Weakly supervised named entity recognition methods train label models to
aggregate the token annotations of multiple noisy labeling functions (LFs)
without seeing any manually annotated labels. To work well, the label model
needs to contextually identify and emphasize well-performed LFs while
down-weighting the under-performers. However, evaluating the LFs is challenging
due to the lack of ground truths. To address this issue, we propose the sparse
conditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire
emission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating
its diagonal elements, which are considered as the reliability scores of the
LFs. The sparse scores are then expanded to the full-fledged emission matrix
with pre-defined expansion functions. We also augment the emission with
weighted XOR scores, which track the probabilities of an LF observing incorrect
entities. Sparse-CHMM is optimized through unsupervised learning with a
three-stage training pipeline that reduces the training difficulty and prevents
the model from falling into local optima. Compared with the baselines in the
Wrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on
five comprehensive datasets. Experiments show that each component of
Sparse-CHMM is effective, and the estimated LF reliabilities strongly correlate
with true LF F1 scores.",https://github.com/Yinghao-Li/Sparse-CHMM,-1
976381f0-85c6-441c-9a7c-6f066b9f5aea,Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models,0.213508,"Retriever-reader models achieve competitive performance across many different
NLP tasks such as open question answering and dialogue conversations. In this
work, we notice these models easily overfit the top-rank retrieval passages and
standard training fails to reason over the entire retrieval passages. We
introduce a learnable passage mask mechanism which desensitizes the impact from
the top-rank retrieval passages and prevents the model from overfitting.
Controlling the gradient variance with fewer mask candidates and selecting the
mask candidates with one-shot bi-level optimization, our learnable
regularization strategy enforces the answer generation to focus on the entire
retrieval passages. Experiments on different tasks across open question
answering, dialogue conversation, and fact verification show that our method
consistently outperforms its baselines. Extensive experiments and ablation
studies demonstrate that our method can be general, effective, and beneficial
for many NLP tasks.",https://github.com/facebookresearch/FiD,-1
044fbe34-6ef0-459d-a252-9bc8b2201f99,The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs,0.487016,"Despite widespread use of LLMs as conversational agents, evaluations of
performance fail to capture a crucial aspect of communication: interpreting
language in context -- incorporating its pragmatics. Humans interpret language
using beliefs and prior knowledge about the world. For example, we intuitively
understand the response ""I wore gloves"" to the question ""Did you leave
fingerprints?"" as meaning ""No"". To investigate whether LLMs have the ability to
make this type of inference, known as an implicature, we design a simple task
and evaluate four categories of widely used state-of-the-art models. We find
that, despite only evaluating on utterances that require a binary inference
(yes or no), models in three of these categories perform close to random.
However, LLMs instruction-tuned at the example-level perform significantly
better. These results suggest that certain fine-tuning strategies are far
better at inducing pragmatic understanding in models. We present our findings
as the starting point for further research into evaluating how LLMs interpret
language in context and to drive the development of more pragmatic and useful
models of human discourse.",https://github.com/LauraRuis/do-pigs-fly,-1
f72d7229-a27e-4984-8ed3-a8e1ed33c21e,$$-GNF : A Novel Sensitivity Analysis Approach Under Unobserved Confounders,0.0775631,"We propose a new sensitivity analysis model that combines copulas and
normalizing flows for causal inference under unobserved confounding. We refer
to the new model as $\rho$-GNF ($\rho$-Graphical Normalizing Flow), where
$\rho{\in}[-1,+1]$ is a bounded sensitivity parameter representing the backdoor
non-causal association due to unobserved confounding modeled using the most
well studied and widely popular Gaussian copula. Specifically, $\rho$-GNF
enables us to estimate and analyse the frontdoor causal effect or average
causal effect (ACE) as a function of $\rho$. We call this the $\rho_{curve}$.
The $\rho_{curve}$ enables us to specify the confounding strength required to
nullify the ACE. We call this the $\rho_{value}$. Further, the $\rho_{curve}$
also enables us to provide bounds for the ACE given an interval of $\rho$
values. We illustrate the benefits of $\rho$-GNF with experiments on simulated
and real-world data in terms of our empirical ACE bounds being narrower than
other popular ACE bounds.",https://github.com/username/rhoGNF,-1
46517c33-f805-4d6b-8a30-4621fd159916,Restoring Vision in Hazy Weather with Hierarchical Contrastive Learning,0.67158,"Image restoration under hazy weather condition, which is called single image
dehazing, has been of significant interest for various computer vision
applications. In recent years, deep learning-based methods have achieved
success. However, existing image dehazing methods typically neglect the
hierarchy of features in the neural network and fail to exploit their
relationships fully. To this end, we propose an effective image dehazing method
named Hierarchical Contrastive Dehazing (HCD), which is based on feature fusion
and contrastive learning strategies. HCD consists of a hierarchical dehazing
network (HDN) and a novel hierarchical contrastive loss (HCL). Specifically,
the core design in the HDN is a hierarchical interaction module, which utilizes
multi-scale activation to revise the feature responses hierarchically. To
cooperate with the training of HDN, we propose HCL which performs contrastive
learning on hierarchically paired exemplars, facilitating haze removal.
Extensive experiments on public datasets, RESIDE, HazeRD, and DENSE-HAZE,
demonstrate that HCD quantitatively outperforms the state-of-the-art methods in
terms of PSNR, SSIM and achieves better visual quality.",None,-1
20720690-aac9-4e7f-b157-21a1e9887c5d,Extending Logic Explained Networks to Text Classification,0.266963,"Recently, Logic Explained Networks (LENs) have been proposed as
explainable-by-design neural models providing logic explanations for their
predictions. However, these models have only been applied to vision and tabular
data, and they mostly favour the generation of global explanations, while local
ones tend to be noisy and verbose. For these reasons, we propose LENp,
improving local explanations by perturbing input words, and we test it on text
classification. Our results show that (i) LENp provides better local
explanations than LIME in terms of sensitivity and faithfulness, and (ii) logic
explanations are more useful and user-friendly than feature scoring provided by
LIME as attested by a human survey.",None,-1
677ffa08-c755-4e01-81a4-d23084e4a054,Task-Aware Asynchronous Multi-Task Model with Class Incremental Contrastive Learning for Surgical Scene Understanding,0.217537,"Purpose: Surgery scene understanding with tool-tissue interaction recognition
and automatic report generation can play an important role in intra-operative
guidance, decision-making and postoperative analysis in robotic surgery.
However, domain shifts between different surgeries with inter and intra-patient
variation and novel instruments' appearance degrade the performance of model
prediction. Moreover, it requires output from multiple models, which can be
computationally expensive and affect real-time performance.
  Methodology: A multi-task learning (MTL) model is proposed for surgical
report generation and tool-tissue interaction prediction that deals with domain
shift problems. The model forms of shared feature extractor, mesh-transformer
branch for captioning and graph attention branch for tool-tissue interaction
prediction. The shared feature extractor employs class incremental contrastive
learning (CICL) to tackle intensity shift and novel class appearance in the
target domain. We design Laplacian of Gaussian (LoG) based curriculum learning
into both shared and task-specific branches to enhance model learning. We
incorporate a task-aware asynchronous MTL optimization technique to fine-tune
the shared weights and converge both tasks optimally.
  Results: The proposed MTL model trained using task-aware optimization and
fine-tuning techniques reported a balanced performance (BLEU score of 0.4049
for scene captioning and accuracy of 0.3508 for interaction detection) for both
tasks on the target domain and performed on-par with single-task models in
domain adaptation.
  Conclusion: The proposed multi-task model was able to adapt to domain shifts,
incorporate novel instruments in the target domain, and perform tool-tissue
interaction detection and report generation on par with single-task models.",https://github.com/lalithjets/Domain-adaptation-in-MTL,-1
631567b8-5db0-4346-a3f6-ae35e15a35c6,Retrieval of surgical phase transitions using reinforcement learning,0.696355,"In minimally invasive surgery, surgical workflow segmentation from video
analysis is a well studied topic. The conventional approach defines it as a
multi-class classification problem, where individual video frames are
attributed a surgical phase label.
  We introduce a novel reinforcement learning formulation for offline phase
transition retrieval. Instead of attempting to classify every video frame, we
identify the timestamp of each phase transition. By construction, our model
does not produce spurious and noisy phase transitions, but contiguous phase
blocks. We investigate two different configurations of this model. The first
does not require processing all frames in a video (only <60% and <20% of frames
in 2 different applications), while producing results slightly under the
state-of-the-art accuracy. The second configuration processes all video frames,
and outperforms the state-of-the art at a comparable computational cost.
  We compare our method against the recent top-performing frame-based
approaches TeCNO and Trans-SVNet on the public dataset Cholec80 and also on an
in-house dataset of laparoscopic sacrocolpopexy. We perform both a frame-based
(accuracy, precision, recall and F1-score) and an event-based (event ratio)
evaluation of our algorithms.",None,-1
7edcc794-affb-46f4-b050-74399c4623f6,FinBERT-MRC: financial named entity recognition using BERT under the machine reading comprehension paradigm,0.872557,"Financial named entity recognition (FinNER) from literature is a challenging
task in the field of financial text information extraction, which aims to
extract a large amount of financial knowledge from unstructured texts. It is
widely accepted to use sequence tagging frameworks to implement FinNER tasks.
However, such sequence tagging models cannot fully take advantage of the
semantic information in the texts. Instead, we formulate the FinNER task as a
machine reading comprehension (MRC) problem and propose a new model termed
FinBERT-MRC. This formulation introduces significant prior information by
utilizing well-designed queries, and extracts start index and end index of
target entities without decoding modules such as conditional random fields
(CRF). We conduct experiments on a publicly available Chinese financial dataset
ChFinAnn and a real-word bussiness dataset AdminPunish. FinBERT-MRC model
achieves average F1 scores of 92.78% and 96.80% on the two datasets,
respectively, with average F1 gains +3.94% and +0.89% over some sequence
tagging models including BiLSTM-CRF, BERT-Tagger, and BERT-CRF. The source code
is available at https://github.com/zyz0000/FinBERT-MRC.",https://github.com/zyz0000/FinBERT-MRC,-1
4711b739-d681-48be-bedc-b0f074025b5e,Non-Markovian Reward Modelling from Trajectory Labels via Interpretable Multiple Instance Learning,0.519171,"We generalise the problem of reward modelling (RM) for reinforcement learning
(RL) to handle non-Markovian rewards. Existing work assumes that human
evaluators observe each step in a trajectory independently when providing
feedback on agent behaviour. In this work, we remove this assumption, extending
RM to capture temporal dependencies in human assessment of trajectories. We
show how RM can be approached as a multiple instance learning (MIL) problem,
where trajectories are treated as bags with return labels, and steps within the
trajectories are instances with unseen reward labels. We go on to develop new
MIL models that are able to capture the time dependencies in labelled
trajectories. We demonstrate on a range of RL tasks that our novel MIL models
can reconstruct reward functions to a high level of accuracy, and can be used
to train high-performing agent policies.",https://github.com/JAEarly/MIL-for-Non-Markovian-Reward-Modelling,-1
08ae0cce-e7f8-4989-a519-4be4d05c41aa,Scalable Multi-view Clustering with Graph Filtering,0.431922,"With the explosive growth of multi-source data, multi-view clustering has
attracted great attention in recent years. Most existing multi-view methods
operate in raw feature space and heavily depend on the quality of original
feature representation. Moreover, they are often designed for feature data and
ignore the rich topology structure information. Accordingly, in this paper, we
propose a generic framework to cluster both attribute and graph data with
heterogeneous features. It is capable of exploring the interplay between
feature and structure. Specifically, we first adopt graph filtering technique
to eliminate high-frequency noise to achieve a clustering-friendly smooth
representation. To handle the scalability challenge, we develop a novel
sampling strategy to improve the quality of anchors. Extensive experiments on
attribute and graph benchmarks demonstrate the superiority of our approach with
respect to state-of-the-art approaches.",https://github.com/EricliuLiang/SMC,-1
62eead94-c98a-42b3-8921-18a26f7b8037,ImplantFormer: Vision Transformer based Implant Position Regression Using Dental CBCT Data,0.596685,"Implant prosthesis is the most appropriate treatment for dentition defect or
dentition loss, which usually involves a surgical guide design process to
decide the implant position. However, such design heavily relies on the
subjective experiences of dentists. In this paper, a transformer-based Implant
Position Regression Network, ImplantFormer, is proposed to automatically
predict the implant position based on the oral CBCT data. We creatively propose
to predict the implant position using the 2D axial view of the tooth crown area
and fit a centerline of the implant to obtain the actual implant position at
the tooth root. Convolutional stem and decoder are designed to coarsely extract
image features before the operation of patch embedding and integrate
multi-level feature maps for robust prediction, respectively. As both
long-range relationship and local features are involved, our approach can
better represent global information and achieves better location performance.
Extensive experiments on a dental implant dataset through five-fold
cross-validation demonstrated that the proposed ImplantFormer achieves superior
performance than existing methods.",None,-1
2eb7835b-6fff-45c9-a695-30f32030cd5b,NICO++: Towards Better Benchmarking for Domain Generalization,0.941617,"Despite the remarkable performance that modern deep neural networks have
achieved on independent and identically distributed (I.I.D.) data, they can
crash under distribution shifts. Most current evaluation methods for domain
generalization (DG) adopt the leave-one-out strategy as a compromise on the
limited number of domains. We propose a large-scale benchmark with extensive
labeled domains named NICO++ along with more rational evaluation methods for
comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose
two metrics to quantify covariate shift and concept shift, respectively. Two
novel generalization bounds from the perspective of data construction are
proposed to prove that limited concept shift and significant covariate shift
favor the evaluation capability for generalization. Through extensive
experiments, NICO++ shows its superior evaluation capability compared with
current DG datasets and its contribution in alleviating unfairness caused by
the leak of oracle knowledge in model selection.",https://github.com/xxgege/NICO-plus,51525
5356df50-d714-4199-8fbb-22a352d42005,Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models,0.739619,"Self-supervised learning (SSL) is seen as a very promising approach with high
performance for several speech downstream tasks. Since the parameters of SSL
models are generally so large that training and inference require a lot of
memory and computational cost, it is desirable to produce compact SSL models
without a significant performance degradation by applying compression methods
such as knowledge distillation (KD). Although the KD approach is able to shrink
the depth and/or width of SSL model structures, there has been little research
on how varying the depth and width impacts the internal representation of the
small-footprint model. This paper provides an empirical study that addresses
the question. We investigate the performance on SUPERB while varying the
structure and KD methods so as to keep the number of parameters constant; this
allows us to analyze the contribution of the representation introduced by
varying the model architecture. Experiments demonstrate that a certain depth is
essential for solving content-oriented tasks (e.g. automatic speech
recognition) accurately, whereas a certain width is necessary for achieving
high performance on several speaker-oriented tasks (e.g. speaker
identification). Based on these observations, we identify, for SUPERB, a more
compressed model with better performance than previous studies.",https://github.com/s3prl/s3prl,-1
3a80b5af-2a81-4653-8d90-68e312fba8e2,Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences,0.816864,"We propose Probabilistic Warp Consistency, a weakly-supervised learning
objective for semantic matching. Our approach directly supervises the dense
matching scores predicted by the network, encoded as a conditional probability
distribution. We first construct an image triplet by applying a known warp to
one of the images in a pair depicting different instances of the same object
class. Our probabilistic learning objectives are then derived using the
constraints arising from the resulting image triplet. We further account for
occlusion and background clutter present in real image pairs by extending our
probabilistic output space with a learnable unmatched state. To supervise it,
we design an objective between image pairs depicting different object classes.
We validate our method by applying it to four recent semantic matching
architectures. Our weakly-supervised approach sets a new state-of-the-art on
four challenging semantic matching benchmarks. Lastly, we demonstrate that our
objective also brings substantial improvements in the strongly-supervised
regime, when combined with keypoint annotations.",https://github.com/PruneTruong/DenseMatching,-1
1d16f599-f576-43ca-a036-e612dc2234d8,"Computational Inference in Cognitive Science: Operational, Societal and Ethical Considerations",0.388046,"Emerging research frontiers and computational advances have gradually
transformed cognitive science into a multidisciplinary and data-driven field.
As a result, there is a proliferation of cognitive theories investigated and
interpreted from different academic lens and in different levels of
abstraction. We formulate this applied aspect of this challenge as the
computational cognitive inference, and describe the major routes of
computational approaches. To balance the potential optimism alongside the speed
and scale of the data-driven era of cognitive science, we propose to inspect
this trend in more empirical terms by identifying the operational challenges,
societal impacts and ethical guidelines in conducting research and interpreting
results from the computational inference in cognitive science.",None,-1
5f1d231f-f7ae-457b-b7fb-24bd691e6ef7,BlobGAN: Spatially Disentangled Scene Representations,0.800611,"We propose an unsupervised, mid-level representation for a generative model
of scenes. The representation is mid-level in that it is neither per-pixel nor
per-image; rather, scenes are modeled as a collection of spatial, depth-ordered
""blobs"" of features. Blobs are differentiably placed onto a feature grid that
is decoded into an image by a generative adversarial network. Due to the
spatial uniformity of blobs and the locality inherent to convolution, our
network learns to associate different blobs with different entities in a scene
and to arrange these blobs to capture scene layout. We demonstrate this
emergent behavior by showing that, despite training without any supervision,
our method enables applications such as easy manipulation of objects within a
scene (e.g., moving, removing, and restyling furniture), creation of feasible
scenes given constraints (e.g., plausible rooms with drawers at a particular
location), and parsing of real-world images into constituent parts. On a
challenging multi-category dataset of indoor scenes, BlobGAN outperforms
StyleGAN2 in image quality as measured by FID. See our project page for video
results and interactive demo: https://www.dave.ml/blobgan",None,-1
ba1468fc-ed8c-42c3-9604-b4543eaa29c7,Domain Adaptation for Time-Series Classification to Mitigate Covariate Shift,0.592017,"The performance of a machine learning model degrades when it is applied to
data from a similar but different domain than the data it has initially been
trained on. To mitigate this domain shift problem, domain adaptation (DA)
techniques search for an optimal transformation that converts the (current)
input data from a source domain to a target domain to learn a domain-invariant
representation that reduces domain discrepancy. This paper proposes a novel
supervised DA based on two steps. First, we search for an optimal
class-dependent transformation from the source to the target domain from a few
samples. We consider optimal transport methods such as the earth mover's
distance, Sinkhorn transport and correlation alignment. Second, we use
embedding similarity techniques to select the corresponding transformation at
inference. We use correlation metrics and higher-order moment matching
techniques. We conduct an extensive evaluation on time-series datasets with
domain shift including simulated and various online handwriting datasets to
demonstrate the performance.",None,-1
245f5764-5dff-4f8e-bb00-18d6584561cc,IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages,0.672191,"The rapid growth of machine translation (MT) systems has necessitated
comprehensive studies to meta-evaluate evaluation metrics being used, which
enables a better selection of metrics that best reflect MT quality.
Unfortunately, most of the research focuses on high-resource languages, mainly
English, the observations for which may not always apply to other languages.
Indian languages, having over a billion speakers, are linguistically different
from English, and to date, there has not been a systematic study of evaluating
MT systems from English into Indian languages. In this paper, we fill this gap
by creating an MQM dataset consisting of 7000 fine-grained annotations,
spanning 5 Indian languages and 7 MT systems, and use it to establish
correlations between annotator scores and scores obtained using existing
automatic metrics. Our results show that pre-trained metrics, such as COMET,
have the highest correlations with annotator scores. Additionally, we find that
the metrics do not adequately capture fluency-based errors in Indian languages,
and there is a need to develop metrics focused on Indian languages. We hope
that our dataset and analysis will help promote further research in this area.",https://github.com/AI4Bharat/IndicMT-Eval,-1
ed8dccbc-37a7-45f4-904e-56fefccfa8fc,Domain Generalization Strategy to Train Classifiers Robust to Spatial-Temporal Shift,0.637755,"Deep learning-based weather prediction models have advanced significantly in
recent years. However, data-driven models based on deep learning are difficult
to apply to real-world applications because they are vulnerable to
spatial-temporal shifts. A weather prediction task is especially susceptible to
spatial-temporal shifts when the model is overfitted to locality and
seasonality. In this paper, we propose a training strategy to make the weather
prediction model robust to spatial-temporal shifts. We first analyze the effect
of hyperparameters and augmentations of the existing training strategy on the
spatial-temporal shift robustness of the model. Next, we propose an optimal
combination of hyperparameters and augmentation based on the analysis results
and a test-time augmentation. We performed all experiments on the W4C22
Transfer dataset and achieved the 1st performance.",https://github.com/seominseok0429/W4C22-Simple-Baseline-for-Weather-Forecasting-Using-Spatiotemporal-Context-Aggregation-Network,-1
577b284c-1b3a-49f2-8ab6-6495146eb4d6,Self-supervised Contrastive Learning for Audio-Visual Action Recognition,0.181817,"The underlying correlation between audio and visual modalities can be
utilized to learn supervised information for unlabeled videos. In this paper,
we propose an end-to-end self-supervised framework named Audio-Visual
Contrastive Learning (AVCL), to learn discriminative audio-visual
representations for action recognition. Specifically, we design an attention
based multi-modal fusion module (AMFM) to fuse audio and visual modalities. To
align heterogeneous audio-visual modalities, we construct a novel
co-correlation guided representation alignment module (CGRA). To learn
supervised information from unlabeled videos, we propose a novel
self-supervised contrastive learning module (SelfCL). Furthermore, we build a
new audio-visual action recognition dataset named Kinetics-Sounds100.
Experimental results on Kinetics-Sounds32 and Kinetics-Sounds100 datasets
demonstrate the superiority of our AVCL over the state-of-the-art methods on
large-scale action recognition benchmark.",None,-1
be6801a7-98eb-4448-8bc1-eef5ca0027b7,Distance-Aware Occlusion Detection with Focused Attention,0.218159,"For humans, understanding the relationships between objects using visual
signals is intuitive. For artificial intelligence, however, this task remains
challenging. Researchers have made significant progress studying semantic
relationship detection, such as human-object interaction detection and visual
relationship detection. We take the study of visual relationships a step
further from semantic to geometric. In specific, we predict relative occlusion
and relative distance relationships. However, detecting these relationships
from a single image is challenging. Enforcing focused attention to
task-specific regions plays a critical role in successfully detecting these
relationships. In this work, (1) we propose a novel three-decoder architecture
as the infrastructure for focused attention; 2) we use the generalized
intersection box prediction task to effectively guide our model to focus on
occlusion-specific regions; 3) our model achieves a new state-of-the-art
performance on distance-aware relationship detection. Specifically, our model
increases the distance F1-score from 33.8% to 38.6% and boosts the occlusion
F1-score from 34.4% to 41.2%. Our code is publicly available.",https://github.com/Yang-Li-2000/Distance-Aware-Occlusion-Detection-with-Focused-Attention.git,-1
491dc554-bf9a-4d2a-99b9-a8c22d4d4e2e,Improving Chess Commentaries by Combining Language Models with Symbolic Reasoning Engines,0.494572,"Despite many recent advancements in language modeling, state-of-the-art
language models lack grounding in the real world and struggle with tasks
involving complex reasoning. Meanwhile, advances in the symbolic reasoning
capabilities of AI have led to systems that outperform humans in games like
chess and Go (Silver et al., 2018). Chess commentary provides an interesting
domain for bridging these two fields of research, as it requires reasoning over
a complex board state and providing analyses in natural language. In this work
we demonstrate how to combine symbolic reasoning engines with controllable
language models to generate chess commentaries. We conduct experiments to
demonstrate that our approach generates commentaries that are preferred by
human judges over previous baselines.",None,-1
4bbe4736-9fde-4716-9b98-6d113afe54c3,Aggregate effects of advertising decisions: a complex systems look at search engine advertising via an experimental study,0.449051,"Purpose: We model group advertising decisions, which are the collective
decisions of every single advertiser within the set of advertisers who are
competing in the same auction or vertical industry, and examine resulting
market outcomes, via a proposed simulation framework named EXP-SEA
(Experimental Platform for Search Engine Advertising) supporting experimental
studies of collective behaviors in the context of search engine advertising.
Design: We implement the EXP-SEA to validate the proposed simulation framework,
also conduct three experimental studies on the aggregate impact of electronic
word-of-mouth, the competition level, and strategic bidding behaviors. EXP-SEA
supports heterogeneous participants, various auction mechanisms, and also
ranking and pricing algorithms. Findings: Findings from our three experiments
show that (a) both the market profit and advertising indexes such as number of
impressions and number of clicks are larger when the eWOM effect presents,
meaning social media certainly has some effect on search engine advertising
outcomes, (b) the competition level has a monotonic increasing effect on the
market performance, thus search engines have an incentive to encourage both the
eWOM among search users and competition among advertisers, and (c) given the
market-level effect of the percentage of advertisers employing a dynamic greedy
bidding strategy, there is a cut-off point for strategic bidding behaviors.
Originality: This is one of the first research works to explore collective
group decisions and resulting phenomena in the complex context of search engine
advertising via developing and validating a simulation framework that supports
assessments of various advertising strategies and estimations of the impact of
mechanisms on the search market.",None,-1
2ce84109-36ac-4bee-b88e-bd89f3b5d3c0,Why we do need Explainable AI for Healthcare,0.501396,"The recent spike in certified Artificial Intelligence (AI) tools for
healthcare has renewed the debate around adoption of this technology. One
thread of such debate concerns Explainable AI and its promise to render AI
devices more transparent and trustworthy. A few voices active in the medical AI
space have expressed concerns on the reliability of Explainable AI techniques,
questioning their use and inclusion in guidelines and standards. Revisiting
such criticisms, this article offers a balanced and comprehensive perspective
on the utility of Explainable AI, focusing on the specificity of clinical
applications of AI and placing them in the context of healthcare interventions.
Against its detractors and despite valid concerns, we argue that the
Explainable AI research program is still central to human-machine interaction
and ultimately our main tool against loss of control, a danger that cannot be
prevented by rigorous clinical validation alone.",None,-1
03ff2004-4640-4941-8e52-adf375b85fba,Chart Question Answering: State of the Art and Future Directions,0.516864,"Information visualizations such as bar charts and line charts are very common
for analyzing data and discovering critical insights. Often people analyze
charts to answer questions that they have in mind. Answering such questions can
be challenging as they often require a significant amount of perceptual and
cognitive effort. Chart Question Answering (CQA) systems typically take a chart
and a natural language question as input and automatically generate the answer
to facilitate visual data analysis. Over the last few years, there has been a
growing body of literature on the task of CQA. In this survey, we
systematically review the current state-of-the-art research focusing on the
problem of chart question answering. We provide a taxonomy by identifying
several important dimensions of the problem domain including possible inputs
and outputs of the task and discuss the advantages and limitations of proposed
solutions. We then summarize various evaluation techniques used in the surveyed
papers. Finally, we outline the open challenges and future research
opportunities related to chart question answering.",None,-1
6f3aaf55-10b2-4e26-8e89-ece62b1ae5a2,"PVSeRF: Joint Pixel-, Voxel- and Surface-Aligned Radiance Field for Single-Image Novel View Synthesis",0.140032,"We present PVSeRF, a learning framework that reconstructs neural radiance
fields from single-view RGB images, for novel view synthesis. Previous
solutions, such as pixelNeRF, rely only on pixel-aligned features and suffer
from feature ambiguity issues. As a result, they struggle with the
disentanglement of geometry and appearance, leading to implausible geometries
and blurry results. To address this challenge, we propose to incorporate
explicit geometry reasoning and combine it with pixel-aligned features for
radiance field prediction. Specifically, in addition to pixel-aligned features,
we further constrain the radiance field learning to be conditioned on i)
voxel-aligned features learned from a coarse volumetric grid and ii) fine
surface-aligned features extracted from a regressed point cloud. We show that
the introduction of such geometry-aware features helps to achieve a better
disentanglement between appearance and geometry, i.e. recovering more accurate
geometries and synthesizing higher quality images of novel views. Extensive
experiments against state-of-the-art methods on ShapeNet benchmarks demonstrate
the superiority of our approach for single-image novel view synthesis.",https://github.com/autonomousvision/differentiable,-1
928943c4-ff30-4bca-b522-73f7a26a89ca,SPIN: An Empirical Evaluation on Sharing Parameters of Isotropic Networks,0.0666767,"Recent isotropic networks, such as ConvMixer and vision transformers, have
found significant success across visual recognition tasks, matching or
outperforming non-isotropic convolutional neural networks (CNNs). Isotropic
architectures are particularly well-suited to cross-layer weight sharing, an
effective neural network compression technique. In this paper, we perform an
empirical evaluation on methods for sharing parameters in isotropic networks
(SPIN). We present a framework to formalize major weight sharing design
decisions and perform a comprehensive empirical evaluation of this design
space. Guided by our experimental results, we propose a weight sharing strategy
to generate a family of models with better overall efficiency, in terms of
FLOPs and parameters versus accuracy, compared to traditional scaling methods
alone, for example compressing ConvMixer by 1.9x while improving accuracy on
ImageNet. Finally, we perform a qualitative study to further understand the
behavior of weight sharing in isotropic architectures. The code is available at
https://github.com/apple/ml-spin.",https://github.com/apple/ml-spin,-1
311d07c0-c14d-4b41-a70a-4eff9a28d8ec,Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations,0.656546,"Although large language models can be prompted for both zero- and few-shot
learning, performance drops significantly when no demonstrations are available.
In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap
by constructing pseudo-demonstrations for a given test input using a raw text
corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the
nearest neighbors to the test input from the corpus and pairing them with
random task labels, and (2) applying a set of techniques to reduce the amount
of direct copying the model does from the resulting demonstrations. Evaluation
on nine classification datasets shows that Z-ICL outperforms previous zero-shot
methods by a significant margin, and is on par with in-context learning with
labeled training data in the few-shot setting. Overall, Z-ICL provides a
significantly higher estimate of the zero-shot performance levels of a model,
and supports future efforts to develop better pseudo-demonstrations that
further improve zero-shot results.",https://github.com/alrope123/z-icl,-1
a444ff8d-be3d-4e0e-939f-261cc4fe1cb9,Uplifting Message Passing Neural Network with Graph Original Information,0.0408806,"Message passing neural networks (MPNNs) learn the representation of
graph-structured data based on graph original information, including node
features and graph structures, and have shown astonishing improvement in node
classification tasks. However, the expressive power of MPNNs is upper bounded
by the first-order Weisfeiler-Leman test and its accuracy still has room for
improvement. This work studies how to improve MPNNs' expressiveness and
generalizability by fully exploiting graph original information both
theoretically and empirically. It further proposes a new GNN model called INGNN
(INformation-enhanced Graph Neural Network) that leverages the insights to
improve node classification performance. Extensive experiments on both
synthetic and real datasets demonstrate the superiority (average rank 1.78) of
our INGNN compared with state-of-the-art methods.",None,-1
1b5d568a-67bc-4223-b8af-f688a4843460,CONDA: Continual Unsupervised Domain Adaptation Learning in Visual Perception for Self-Driving Cars,0.350859,"Although unsupervised domain adaptation methods have achieved remarkable
performance in semantic scene segmentation in visual perception for
self-driving cars, these approaches remain impractical in real-world use cases.
In practice, the segmentation models may encounter new data that have not been
seen yet. Also, the previous data training of segmentation models may be
inaccessible due to privacy problems. Therefore, to address these problems, in
this work, we propose a Continual Unsupervised Domain Adaptation (CONDA)
approach that allows the model to continuously learn and adapt with respect to
the presence of the new data. Moreover, our proposed approach is designed
without the requirement of accessing previous training data. To avoid the
catastrophic forgetting problem and maintain the performance of the
segmentation models, we present a novel Bijective Maximum Likelihood loss to
impose the constraint of predicted segmentation distribution shifts. The
experimental results on the benchmark of continual unsupervised domain
adaptation have shown the advanced performance of the proposed CONDA method.",None,4806
634493cc-41b9-473a-8a09-6bf9828dee00,Encryption and encoding of facial images into quick response and high capacity color 2d code for biometric passport security system,0.427058,"In this thesis, a multimodal biometric, secure encrypted data and encrypted
biometric encoded into the QR code-based biometric-passport authentication
method is proposed for national security applications. Firstly, using the
Extended Profile - Local Binary Patterns (EP-LBP), a Canny edge detector, and
the Scale Invariant Feature Transform (SIFT) algorithm with Image File
Information (IMFINFO) process, the facial mark size recognition is initially
achieved. Secondly, by using the Active Shape Model (ASM) into Active
Appearance Model (AAM) to follow the hand and infusion the hand geometry
characteristics for verification and identification, hand geometry recognition
is achieved. Thirdly, the encrypted biometric passport information that is
publicly accessible is encoded into the QR code and inserted into the
electronic passport to improve protection. Further, Personal information and
biometric data are encrypted by applying the Advanced Encryption Standard (AES)
and the Secure Hash Algorithm (SHA) 256 algorithm. It will enhance the
biometric passport security system.",None,-1
f71a861d-f652-4f33-b3db-aa66a5a5a408,Generalizing to the Future: Mitigating Entity Bias in Fake News Detection,0.722496,"The wide dissemination of fake news is increasingly threatening both
individuals and society. Fake news detection aims to train a model on the past
news and detect fake news of the future. Though great efforts have been made,
existing fake news detection methods overlooked the unintended entity bias in
the real-world data, which seriously influences models' generalization ability
to future data. For example, 97\% of news pieces in 2010-2017 containing the
entity `Donald Trump' are real in our data, but the percentage falls down to
merely 33\% in 2018. This would lead the model trained on the former set to
hardly generalize to the latter, as it tends to predict news pieces about
`Donald Trump' as real for lower training loss. In this paper, we propose an
entity debiasing framework (\textbf{ENDEF}) which generalizes fake news
detection models to the future data by mitigating entity bias from a
cause-effect perspective. Based on the causal graph among entities, news
contents, and news veracity, we separately model the contribution of each cause
(entities and contents) during training. In the inference stage, we remove the
direct effect of the entities to mitigate entity bias. Extensive offline
experiments on the English and Chinese datasets demonstrate that the proposed
framework can largely improve the performance of base fake news detectors, and
online tests verify its superiority in practice. To the best of our knowledge,
this is the first work to explicitly improve the generalization ability of fake
news detection models to the future data. The code has been released at
https://github.com/ICTMCG/ENDEF-SIGIR2022.",https://github.com/ICTMCG/ENDEF-SIGIR2022,-1
8cf3bcbd-bee4-4cd2-aaf7-a719c22a1215,Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation,0.945983,"In this paper we present Mask DINO, a unified object detection and
segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising
Anchor Boxes) by adding a mask prediction branch which supports all image
segmentation tasks (instance, panoptic, and semantic). It makes use of the
query embeddings from DINO to dot-product a high-resolution pixel embedding map
to predict a set of binary masks. Some key components in DINO are extended for
segmentation through a shared architecture and training process. Mask DINO is
simple, efficient, and scalable, and it can benefit from joint large-scale
detection and segmentation datasets. Our experiments show that Mask DINO
significantly outperforms all existing specialized segmentation methods, both
on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably,
Mask DINO establishes the best results to date on instance segmentation (54.5
AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation
(60.8 mIoU on ADE20K) among models under one billion parameters. Code is
available at \url{https://github.com/IDEACVR/MaskDINO}.",https://github.com/IDEA-Research/MaskDINO,-1
1424aa0f-e7d1-4a37-9db8-c5aa9ead0577,A Few-shot Approach to Resume Information Extraction via Prompts,0.116009,"Prompt learning's fine-tune performance on text classification tasks has
attracted the NLP community. This paper applies it to resume information
extraction, improving existing methods for this task. We created manual
templates and verbalizers tailored to resume texts and compared the performance
of Masked Language Model (MLM) and Seq2Seq PLMs. Also, we enhanced the
verbalizer design for Knowledgeable Prompt-tuning, contributing to prompt
template design across NLP tasks. We present the Manual Knowledgeable
Verbalizer (MKV), a rule for constructing verbalizers for specific
applications. Our tests show that MKV rules yield more effective, robust
templates and verbalizers than existing methods. Our MKV approach resolved
sample imbalance, surpassing current automatic prompt methods. This study
underscores the value of tailored prompt learning for resume extraction,
stressing the importance of custom-designed templates and verbalizers.",https://github.com/thunlp/OpenPrompt,-1
6753d187-9c26-4d2c-bd1b-0d0890dedd4c,Biometric Signature Verification Using Recurrent Neural Networks,0.737732,"Architectures based on Recurrent Neural Networks (RNNs) have been
successfully applied to many different tasks such as speech or handwriting
recognition with state-of-the-art results. The main contribution of this work
is to analyse the feasibility of RNNs for on-line signature verification in
real practical scenarios. We have considered a system based on Long Short-Term
Memory (LSTM) with a Siamese architecture whose goal is to learn a similarity
metric from pairs of signatures. For the experimental work, the BiosecurID
database comprised of 400 users and 4 separated acquisition sessions are
considered. Our proposed LSTM RNN system has outperformed the results of recent
published works on the BiosecurID benchmark in figures ranging from 17.76% to
28.00% relative verification performance improvement for skilled forgeries.",None,-1
27c3276e-936d-4cc3-892c-7d9c87d540a3,Multi-objective hyperparameter optimization with performance uncertainty,0.104688,"The performance of any Machine Learning (ML) algorithm is impacted by the
choice of its hyperparameters. As training and evaluating a ML algorithm is
usually expensive, the hyperparameter optimization (HPO) method needs to be
computationally efficient to be useful in practice. Most of the existing
approaches on multi-objective HPO use evolutionary strategies and
metamodel-based optimization. However, few methods have been developed to
account for uncertainty in the performance measurements. This paper presents
results on multi-objective hyperparameter optimization with uncertainty on the
evaluation of ML algorithms. We combine the sampling strategy of
Tree-structured Parzen Estimators (TPE) with the metamodel obtained after
training a Gaussian Process Regression (GPR) with heterogeneous noise.
Experimental results on three analytical test functions and three ML problems
show the improvement over multi-objective TPE and GPR, achieved with respect to
the hypervolume indicator.",None,-1
7da45cc3-5885-4eb6-adf2-afd7fab74cfe,Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication,0.444694,"Social media has become extremely influential when it comes to policy making
in modern societies, especially in the western world, where platforms such as
Twitter allow users to follow politicians, thus making citizens more involved
in political discussion. In the same vein, politicians use Twitter to express
their opinions, debate among others on current topics and promote their
political agendas aiming to influence voter behaviour. In this paper, we
attempt to analyse tweets of politicians from three European countries and
explore the virality of their tweets. Previous studies have shown that tweets
conveying negative sentiment are likely to be retweeted more frequently. By
utilising state-of-the-art pre-trained language models, we performed sentiment
analysis on hundreds of thousands of tweets collected from members of
parliament in Greece, Spain and the United Kingdom, including devolved
administrations. We achieved this by systematically exploring and analysing the
differences between influential and less popular tweets. Our analysis indicates
that politicians' negatively charged tweets spread more widely, especially in
more recent times, and highlights interesting differences between political
parties as well as between politicians and the general population.",https://github.com/cardiffnlp/politics-and-virality-twitter,-1
6a75c110-8cee-4eb6-9d1a-349aab090046,End-to-end system for object detection from sub-sampled radar data,0.0208478,"Robust and accurate sensing is of critical importance for advancing
autonomous automotive systems. The need to acquire situational awareness in
complex urban conditions using sensors such as radar has motivated research on
power and latency-efficient signal acquisition methods. In this paper, we
present an end-to-end signal processing pipeline, capable of operating in
extreme weather conditions, that relies on sub-sampled radar data to perform
object detection in vehicular settings. The results of the object detection are
further utilized to sub-sample forthcoming radar data, which stands in contrast
to prior work where the sub-sampling relies on image information. We show
robust detection based on radar data reconstructed using 20% of samples under
extreme weather conditions such as snow or fog, and on low-illuminated nights.
Additionally, we generate 20% sampled radar data in a fine-tuning set and show
1.1% gain in AP50 across scenes and 3% AP50 gain in motorway condition.",https://github.com/Madhusakth/RADIATE-,-1
0d0af82f-3cf7-42eb-83c2-a1fc957b2d3a,Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task,0.617908,"The adoption of pre-trained language models in task-oriented dialogue systems
has resulted in significant enhancements of their text generation abilities.
However, these architectures are slow to use because of the large number of
trainable parameters and can sometimes fail to generate diverse responses. To
address these limitations, we propose two models with auxiliary tasks for
response selection - (1) distinguishing distractors from ground truth responses
and (2) distinguishing synthetic responses from ground truth labels. They
achieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined
scores of 107.5 and 108.3 and outperform a baseline with three times more
parameters. We publish reproducible code and checkpoints and discuss the
effects of applying auxiliary tasks to T5-based architectures.",https://github.com/,-1
a2f026f0-dc5b-4da1-9af4-b809267c2067,EISeg: An Efficient Interactive Segmentation Tool based on PaddlePaddle,0.34178,"In recent years, the rapid development of deep learning has brought great
advancements to image and video segmentation methods based on neural networks.
However, to unleash the full potential of such models, large numbers of
high-quality annotated images are necessary for model training. Currently, many
widely used open-source image segmentation software relies heavily on manual
annotation which is tedious and time-consuming. In this work, we introduce
EISeg, an Efficient Interactive SEGmentation annotation tool that can
drastically improve image segmentation annotation efficiency, generating highly
accurate segmentation masks with only a few clicks. We also provide various
domain-specific models for remote sensing, medical imaging, industrial quality
inspections, human segmentation, and temporal aware models for video
segmentation. The source code for our algorithm and user interface are
available at: https://github.com/PaddlePaddle/PaddleSeg.",https://github.com/PaddlePaddle/PaddleSeg,-1
2c140b7e-5b71-47e5-926a-29d2b88226cf,Towards Understanding Mixture of Experts in Deep Learning,0.856862,"The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by
a router, has achieved great success in deep learning. However, the
understanding of such architecture remains elusive. In this paper, we formally
study how the MoE layer improves the performance of neural network learning and
why the mixture model will not collapse into a single model. Our empirical
results suggest that the cluster structure of the underlying problem and the
non-linearity of the expert are pivotal to the success of MoE. To further
understand this, we consider a challenging classification problem with
intrinsic cluster structures, which is hard to learn using a single expert. Yet
with the MoE layer, by choosing the experts as two-layer nonlinear
convolutional neural networks (CNNs), we show that the problem can be learned
successfully. Furthermore, our theory shows that the router can learn the
cluster-center features, which helps divide the input complex problem into
simpler linear classification sub-problems that individual experts can conquer.
To our knowledge, this is the first result towards formally understanding the
mechanism of the MoE layer for deep learning.",https://github.com/TheophileBlard/french-sentiment-analysis-with-bert,-1
fcdab991-8c0f-42c1-802c-19b6e7cc448b,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,0.133828,"Given its effectiveness on knowledge-intensive natural language processing
tasks, dense retrieval models have become increasingly popular. Specifically,
the de-facto architecture for open-domain question answering uses two
isomorphic encoders that are initialized from the same pretrained model but
separately parameterized for questions and passages. This bi-encoder
architecture is parameter-inefficient in that there is no parameter sharing
between encoders. Further, recent studies show that such dense retrievers
underperform BM25 in various settings. We thus propose a new architecture,
Task-aware Specialization for dense Retrieval (TASER), which enables parameter
sharing by interleaving shared and specialized blocks in a single encoder. Our
experiments on five question answering datasets show that TASER can achieve
superior accuracy, surpassing BM25, while using about 60% of the parameters as
bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also
empirically more robust than bi-encoder dense retrievers. Our code is available
at https://github.com/microsoft/taser.",https://github.com/microsoft/taser,-1
49edd048-7af2-4b82-a80d-c40230de6c52,DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation,0.802446,"This paper proposes a simple baseline framework for video-based 2D/3D human
pose estimation that can achieve 10 times efficiency improvement over existing
works without any performance degradation, named DeciWatch. Unlike current
solutions that estimate each frame in a video, DeciWatch introduces a simple
yet effective sample-denoise-recover framework that only watches sparsely
sampled frames, taking advantage of the continuity of human motions and the
lightweight pose representation. Specifically, DeciWatch uniformly samples less
than 10% video frames for detailed estimation, denoises the estimated 2D/3D
poses with an efficient Transformer architecture, and then accurately recovers
the rest of the frames using another Transformer-based network. Comprehensive
experimental results on three video-based human pose estimation and body mesh
recovery tasks with four datasets validate the efficiency and effectiveness of
DeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.",https://github.com/cure-lab/DeciWatch,-1
ec8f8433-73b4-4d80-859c-800b5e8d0606,BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric,0.770012,"End-to-End speech-to-speech translation (S2ST) is generally evaluated with
text-based metrics. This means that generated speech has to be automatically
transcribed, making the evaluation dependent on the availability and quality of
automatic speech recognition (ASR) systems. In this paper, we propose a
text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the
dependency on ASR systems. BLASER leverages a multilingual multimodal encoder
to directly encode the speech segments for source input, translation output and
reference into a shared embedding space and computes a score of the translation
quality that can be used as a proxy to human evaluation. To evaluate our
approach, we construct training and evaluation sets from more than 40k human
annotations covering seven language directions. The best results of BLASER are
achieved by training with supervision from human rating scores. We show that
when evaluated at the sentence level, BLASER correlates significantly better
with human judgment compared to ASR-dependent metrics including ASR-SENTBLEU in
all translation directions and ASR-COMET in five of them. Our analysis shows
combining speech and text as inputs to BLASER does not increase the correlation
with human scores, but best correlations are achieved when using speech, which
motivates the goal of our research. Moreover, we show that using ASR for
references is detrimental for text-based metrics.",https://github.com/facebookresearch/stopes,-1
ed477016-e408-4877-b190-c5ab4dffda82,Occluded Person Re-Identification via Relational Adaptive Feature Correction Learning,0.828262,"Occluded person re-identification (Re-ID) in images captured by multiple
cameras is challenging because the target person is occluded by pedestrians or
objects, especially in crowded scenes. In addition to the processes performed
during holistic person Re-ID, occluded person Re-ID involves the removal of
obstacles and the detection of partially visible body parts. Most existing
methods utilize the off-the-shelf pose or parsing networks as pseudo labels,
which are prone to error. To address these issues, we propose a novel Occlusion
Correction Network (OCNet) that corrects features through relational-weight
learning and obtains diverse and representative features without using external
networks. In addition, we present a simple concept of a center feature in order
to provide an intuitive solution to pedestrian occlusion scenarios.
Furthermore, we suggest the idea of Separation Loss (SL) for focusing on
different parts between global features and part features. We conduct extensive
experiments on five challenging benchmark datasets for occluded and holistic
Re-ID tasks to demonstrate that our method achieves superior performance to
state-of-the-art methods especially on occluded scene.",None,-1
72eb4458-2994-406d-8cf5-cc4d896301bf,Robust Multi-Object Tracking by Marginal Inference,0.372523,"Multi-object tracking in videos requires to solve a fundamental problem of
one-to-one assignment between objects in adjacent frames. Most methods address
the problem by first discarding impossible pairs whose feature distances are
larger than a threshold, followed by linking objects using Hungarian algorithm
to minimize the overall distance. However, we find that the distribution of the
distances computed from Re-ID features may vary significantly for different
videos. So there isn't a single optimal threshold which allows us to safely
discard impossible pairs. To address the problem, we present an efficient
approach to compute a marginal probability for each pair of objects in real
time. The marginal probability can be regarded as a normalized distance which
is significantly more stable than the original feature distance. As a result,
we can use a single threshold for all videos. The approach is general and can
be applied to the existing trackers to obtain about one point improvement in
terms of IDF1 metric. It achieves competitive results on MOT17 and MOT20
benchmarks. In addition, the computed probability is more interpretable which
facilitates subsequent post-processing operations.",None,-1
421c4ee6-9221-4eed-9bbb-eeed03b422d6,Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation,0.421889,"With rising male infertility, sperm head morphology classification becomes
critical for accurate and timely clinical diagnosis. Recent deep learning (DL)
morphology analysis methods achieve promising benchmark results, but leave
performance and robustness on the table by relying on limited and possibly
noisy class labels. To address this, we introduce a new DL training framework
that leverages anatomical and image priors from human sperm microscopy crops to
extract useful features without additional labeling cost. Our core idea is to
distill sperm head information with reliably-generated pseudo-masks and
unsupervised spatial prediction tasks. The predicted foreground masks from this
distillation step are then leveraged to regularize and reduce image and label
noise in the tuning stage. We evaluate our new approach on two public sperm
datasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy
and 96.5% HuSHeM accuracy).",None,-1
42e0faf8-51de-4d44-b62b-fcb7056cc82c,COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning,0.930748,"Graph contrastive learning (GCL) improves graph representation learning,
leading to SOTA on various downstream tasks. The graph augmentation step is a
vital but scarcely studied step of GCL. In this paper, we show that the node
embedding obtained via the graph augmentations is highly biased, somewhat
limiting contrastive models from learning discriminative features for
downstream tasks. Thus, instead of investigating graph augmentation in the
input space, we alternatively propose to perform augmentations on the hidden
features (feature augmentation). Inspired by so-called matrix sketching, we
propose COSTA, a novel COvariance-preServing feaTure space Augmentation
framework for GCL, which generates augmented features by maintaining a ""good
sketch"" of original features. To highlight the superiority of feature
augmentation with COSTA, we investigate a single-view setting (in addition to
multi-view one) which conserves memory and computations. We show that the
feature augmentation with COSTA achieves comparable/better results than graph
augmentation based models.",None,-1
d543a4a4-f1b9-4c05-b78b-066141d8a475,GRU-TV: Time- and velocity-aware GRU for patient representation on multivariate clinical time-series data,0.0996537,"Electronic health records (EHRs) are usually highly dimensional,
heterogeneous, and multimodal. Besides, the random recording of clinical
variables results in high missing rates and uneven time intervals between
adjacent records in the multivariate clinical time-series data extracted from
EHRs. Current works using clinical time-series data for patient representation
regard the patients' physiological status as a discrete process described by
sporadically collected records. However, changes in the patient's physiological
condition are continuous and dynamic processes. The perception of time and
velocity of change is crucial for patient representation learning. In this
study, we propose a time- and velocity-aware gated recurrent unit model
(GRU-TV) for patient representation learning of clinical multivariate
time-series data in a time-continuous manner. The neural ordinary differential
equations (ODEs) and velocity perception mechanism are applied to perceive the
time interval between adjacent records and changing rate of the patient's
physiological status, respectively. Our experiments on two real clinical EHR
datasets (PhysioNet2012, MIMIC-III) establish that GRU-TV is a robust model on
computer-aided diagnosis (CAD) tasks, especially on sequences with
high-variance time intervals.",None,-1
71e1fa56-9c51-4eaa-bcc5-304f77601b89,NFLAT: Non-Flat-Lattice Transformer for Chinese Named Entity Recognition,0.864484,"Recently, Flat-LAttice Transformer (FLAT) has achieved great success in
Chinese Named Entity Recognition (NER). FLAT performs lexical enhancement by
constructing flat lattices, which mitigates the difficulties posed by blurred
word boundaries and the lack of word semantics. In FLAT, the positions of
starting and ending characters are used to connect a matching word. However,
this method is likely to match more words when dealing with long texts,
resulting in long input sequences. Therefore, it significantly increases the
memory and computational costs of the self-attention module. To deal with this
issue, we advocate a novel lexical enhancement method, InterFormer, that
effectively reduces the amount of computational and memory costs by
constructing non-flat lattices. Furthermore, with InterFormer as the backbone,
we implement NFLAT for Chinese NER. NFLAT decouples lexicon fusion and context
feature encoding. Compared with FLAT, it reduces unnecessary attention
calculations in ""word-character"" and ""word-word"". This reduces the memory usage
by about 50% and can use more extensive lexicons or higher batches for network
training. The experimental results obtained on several well-known benchmarks
demonstrate the superiority of the proposed method over the state-of-the-art
hybrid (character-word) models.",None,-1
315881df-1b30-459c-aee2-fbf96c5e2c18,Protecting Celebrities from DeepFake with Identity Consistency Transformer,0.985963,"In this work we propose Identity Consistency Transformer, a novel face
forgery detection method that focuses on high-level semantics, specifically
identity information, and detecting a suspect face by finding identity
inconsistency in inner and outer face regions. The Identity Consistency
Transformer incorporates a consistency loss for identity consistency
determination. We show that Identity Consistency Transformer exhibits superior
generalization ability not only across different datasets but also across
various types of image degradation forms found in real-world applications
including deepfake videos. The Identity Consistency Transformer can be easily
enhanced with additional identity information when such information is
available, and for this reason it is especially well-suited for detecting face
forgeries involving celebrities. Code will be released at
\url{https://github.com/LightDXY/ICT_DeepFake}",https://github.com/LightDXY/ICT_DeepFake,-1
292adf72-95f7-4920-8789-000fa10246e4,Consistent Representation Learning for Continual Relation Extraction,0.954679,"Continual relation extraction (CRE) aims to continuously train a model on
data with new relations while avoiding forgetting old ones. Some previous work
has proved that storing a few typical samples of old relations and replaying
them when learning new relations can effectively avoid forgetting. However,
these memory-based methods tend to overfit the memory samples and perform
poorly on imbalanced datasets. To solve these challenges, a consistent
representation learning method is proposed, which maintains the stability of
the relation embedding by adopting contrastive learning and knowledge
distillation when replaying memory. Specifically, supervised contrastive
learning based on a memory bank is first used to train each new task so that
the model can effectively learn the relation representation. Then, contrastive
replay is conducted of the samples in memory and makes the model retain the
knowledge of historical relations through memory knowledge distillation to
prevent the catastrophic forgetting of the old task. The proposed method can
better learn consistent representations to alleviate forgetting effectively.
Extensive experiments on FewRel and TACRED datasets show that our method
significantly outperforms state-of-the-art baselines and yield strong
robustness on the imbalanced dataset.",https://github.com/thuiar/CRL,-1
aba73d1d-5047-4e28-b60b-f12095cd6b89,RV4JaCa -- Runtime Verification for Multi-Agent Systems,0.423207,"This paper presents a Runtime Verification (RV) approach for Multi-Agent
Systems (MAS) using the JaCaMo framework. Our objective is to bring a layer of
security to the MAS. This layer is capable of controlling events during the
execution of the system without needing a specific implementation in the
behaviour of each agent to recognise the events. MAS have been used in the
context of hybrid intelligence. This use requires communication between
software agents and human beings. In some cases, communication takes place via
natural language dialogues. However, this kind of communication brings us to a
concern related to controlling the flow of dialogue so that agents can prevent
any change in the topic of discussion that could impair their reasoning. We
demonstrate the implementation of a monitor that aims to control this dialogue
flow in a MAS that communicates with the user through natural language to aid
decision-making in hospital bed allocation.",https://github.com/DeboraEngelmann/RV4JaCa,-1
06b57b08-805d-4457-9e64-fe9cff4b4f08,Deep Model-Based Super-Resolution with Non-uniform Blur,0.450162,"We propose a state-of-the-art method for super-resolution with non-uniform
blur. Single-image super-resolution methods seek to restore a high-resolution
image from blurred, subsampled, and noisy measurements. Despite their
impressive performance, existing techniques usually assume a uniform blur
kernel. Hence, these techniques do not generalize well to the more general case
of non-uniform blur. Instead, in this paper, we address the more realistic and
computationally challenging case of spatially-varying blur. To this end, we
first propose a fast deep plug-and-play algorithm, based on linearized ADMM
splitting techniques, which can solve the super-resolution problem with
spatially-varying blur. Second, we unfold our iterative algorithm into a single
network and train it end-to-end. In this way, we overcome the intricacy of
manually tuning the parameters involved in the optimization scheme. Our
algorithm presents remarkable performance and generalizes well after a single
training to a large family of spatially-varying blur kernels, noise levels and
scale factors.",https://github.com/claroche-r/DMBSR,471
130d9e53-3a92-4ccc-becc-b1ece3c15a43,Interventional Contrastive Learning with Meta Semantic Regularizer,0.292086,"Contrastive learning (CL)-based self-supervised learning models learn visual
representations in a pairwise manner. Although the prevailing CL model has
achieved great progress, in this paper, we uncover an ever-overlooked
phenomenon: When the CL model is trained with full images, the performance
tested in full images is better than that in foreground areas; when the CL
model is trained with foreground areas, the performance tested in full images
is worse than that in foreground areas. This observation reveals that
backgrounds in images may interfere with the model learning semantic
information and their influence has not been fully eliminated. To tackle this
issue, we build a Structural Causal Model (SCM) to model the background as a
confounder. We propose a backdoor adjustment-based regularization method,
namely Interventional Contrastive Learning with Meta Semantic Regularizer
(ICL-MSR), to perform causal intervention towards the proposed SCM. ICL-MSR can
be incorporated into any existing CL methods to alleviate background
distractions from representation learning. Theoretically, we prove that ICL-MSR
achieves a tighter error bound. Empirically, our experiments on multiple
benchmark datasets demonstrate that ICL-MSR is able to improve the performances
of different state-of-the-art CL methods.",None,-1
07489216-6e1e-4f37-b658-a955f688a527,MagicPony: Learning Articulated 3D Animals in the Wild,0.975782,"We consider the problem of predicting the 3D shape, articulation, viewpoint,
texture, and lighting of an articulated animal like a horse given a single test
image as input. We present a new method, dubbed MagicPony, that learns this
predictor purely from in-the-wild single-view images of the object category,
with minimal assumptions about the topology of deformation. At its core is an
implicit-explicit representation of articulated shape and appearance, combining
the strengths of neural fields and meshes. In order to help the model
understand an object's shape and pose, we distil the knowledge captured by an
off-the-shelf self-supervised vision transformer and fuse it into the 3D model.
To overcome local optima in viewpoint estimation, we further introduce a new
viewpoint sampling scheme that comes at no additional training cost. MagicPony
outperforms prior work on this challenging task and demonstrates excellent
generalisation in reconstructing art, despite the fact that it is only trained
on real images.",https://3dmagicpony.github.io/,96096
d6c8b6ad-ba33-4035-8fb9-6fa32c691911,Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation,0.561553,"In reinforcement learning for visual navigation, it is common to develop a
model for each new task, and train that model from scratch with task-specific
interactions in 3D environments. However, this process is expensive; massive
amounts of interactions are needed for the model to generalize well. Moreover,
this process is repeated whenever there is a change in the task type or the
goal modality. We present a unified approach to visual navigation using a novel
modular transfer learning model. Our model can effectively leverage its
experience from one source task and apply it to multiple target tasks (e.g.,
ObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch,
audio, label). Furthermore, our model enables zero-shot experience learning,
whereby it can solve the target tasks without receiving any task-specific
interactive training. Our experiments on multiple photorealistic datasets and
challenging tasks show that our approach learns faster, generalizes better, and
outperforms SoTA models by a significant margin.",https://vision.cs.utexas.edu/projects/zsel/,-1
e29de5ac-a743-472c-b846-ef1a02db6c5a,Detecting Methane Plumes using PRISMA: Deep Learning Model and Data Augmentation,0.764123,"The new generation of hyperspectral imagers, such as PRISMA, has improved
significantly our detection capability of methane (CH4) plumes from space at
high spatial resolution (30m). We present here a complete framework to identify
CH4 plumes using images from the PRISMA satellite mission and a deep learning
model able to detect plumes over large areas. To compensate for the relative
scarcity of PRISMA images, we trained our model by transposing high resolution
plumes from Sentinel-2 to PRISMA. Our methodology thus avoids computationally
expensive synthetic plume generation from Large Eddy Simulations by generating
a broad and realistic training database, and paves the way for large-scale
detection of methane plumes using future hyperspectral sensors (EnMAP, EMIT,
CarbonMapper).",None,-1
18a851fa-b1ad-495e-94d7-7693086e6da6,HCL: Improving Graph Representation with Hierarchical Contrastive Learning,0.0623505,"Contrastive learning has emerged as a powerful tool for graph representation
learning. However, most contrastive learning methods learn features of graphs
with fixed coarse-grained scale, which might underestimate either local or
global information. To capture more hierarchical and richer representation, we
propose a novel Hierarchical Contrastive Learning (HCL) framework that
explicitly learns graph representation in a hierarchical manner. Specifically,
HCL includes two key components: a novel adaptive Learning to Pool (L2Pool)
method to construct more reasonable multi-scale graph topology for more
comprehensive contrastive objective, a novel multi-channel pseudo-siamese
network to further enable more expressive learning of mutual information within
each scale. Comprehensive experimental results show HCL achieves competitive
performance on 12 datasets involving node classification, node clustering and
graph classification. In addition, the visualization of learned representation
reveals that HCL successfully captures meaningful characteristics of graphs.",None,-1
1210a834-c75c-4b6d-a6c5-0419c9b87300,Zero-shot Aspect-level Sentiment Classification via Explicit Utilization of Aspect-to-Document Sentiment Composition,0.0812469,"As aspect-level sentiment labels are expensive and labor-intensive to
acquire, zero-shot aspect-level sentiment classification is proposed to learn
classifiers applicable to new domains without using any annotated aspect-level
data. In contrast, document-level sentiment data with ratings are more easily
accessible. In this work, we achieve zero-shot aspect-level sentiment
classification by only using document-level reviews. Our key intuition is that
the sentiment representation of a document is composed of the sentiment
representations of all the aspects of that document. Based on this, we propose
the AF-DSC method to explicitly model such sentiment composition in reviews.
AF-DSC first learns sentiment representations for all potential aspects and
then aggregates aspect-level sentiments into a document-level one to perform
document-level sentiment classification. In this way, we obtain the
aspect-level sentiment classifier as the by-product of the document-level
sentiment classifier. Experimental results on aspect-level sentiment
classification benchmarks demonstrate the effectiveness of explicit utilization
of sentiment composition in document-level sentiment classification. Our model
with only 30k training data outperforms previous work utilizing millions of
data.",https://github.com/explosion/,-1
dc346381-a0f4-4f69-9971-2ea8c83ad00e,Controllable User Dialogue Act Augmentation for Dialogue State Tracking,0.561887,"Prior work has demonstrated that data augmentation is useful for improving
dialogue state tracking. However, there are many types of user utterances,
while the prior method only considered the simplest one for augmentation,
raising the concern about poor generalization capability. In order to better
cover diverse dialogue acts and control the generation quality, this paper
proposes controllable user dialogue act augmentation (CUDA-DST) to augment user
utterances with diverse behaviors. With the augmented data, different state
trackers gain improvement and show better robustness, achieving the
state-of-the-art performance on MultiWOZ 2.1",https://github.com/MiuLab/CUDA-DST,-1
910a0c8e-b3f0-4292-80a4-fa5cb3e494a1,Overview of the HASOC Subtrack at FIRE 2022: Offensive Language Identification in Marathi,0.897191,"The widespread of offensive content online has become a reason for great
concern in recent years, motivating researchers to develop robust systems
capable of identifying such content automatically. With the goal of carrying
out a fair evaluation of these systems, several international competitions have
been organized, providing the community with important benchmark data and
evaluation methods for various languages. Organized since 2019, the HASOC (Hate
Speech and Offensive Content Identification) shared task is one of these
initiatives. In its fourth iteration, HASOC 2022 included three subtracks for
English, Hindi, and Marathi. In this paper, we report the results of the HASOC
2022 Marathi subtrack which provided participants with a dataset containing
data from Twitter manually annotated using the popular OLID taxonomy. The
Marathi track featured three additional subtracks, each corresponding to one
level of the taxonomy: Task A - offensive content identification (offensive vs.
non-offensive); Task B - categorization of offensive types (targeted vs.
untargeted), and Task C - offensive target identification (individual vs. group
vs. others). Overall, 59 runs were submitted by 10 teams. The best systems
obtained an F1 of 0.9745 for Subtrack 3A, an F1 of 0.9207 for Subtrack 3B, and
F1 of 0.9607 for Subtrack 3C. The best performing algorithms were a mixture of
traditional and deep learning approaches.",None,-1
23f79907-1120-405c-aaca-33219d3ed608,On Monocular Depth Estimation and Uncertainty Quantification using Classification Approaches for Regression,0.071735,"Monocular depth is important in many tasks, such as 3D reconstruction and
autonomous driving. Deep learning based models achieve state-of-the-art
performance in this field. A set of novel approaches for estimating monocular
depth consists of transforming the regression task into a classification one.
However, there is a lack of detailed descriptions and comparisons for
Classification Approaches for Regression (CAR) in the community and no in-depth
exploration of their potential for uncertainty estimation. To this end, this
paper will introduce a taxonomy and summary of CAR approaches, a new
uncertainty estimation solution for CAR, and a set of experiments on depth
accuracy and uncertainty quantification for CAR-based models on KITTI dataset.
The experiments reflect the differences in the portability of various CAR
methods on two backbones. Meanwhile, the newly proposed method for uncertainty
estimation can outperform the ensembling method with only one forward
propagation.",None,-1
4201edf7-730f-4a92-9bfe-a954d817a8fb,Adversarial Learning to Reason in an Arbitrary Logic,0.165288,"Existing approaches to learning to prove theorems focus on particular logics
and datasets. In this work, we propose Monte-Carlo simulations guided by
reinforcement learning that can work in an arbitrarily specified logic, without
any human knowledge or set of problems. Since the algorithm does not need any
training dataset, it is able to learn to work with any logical foundation, even
when there is no body of proofs or even conjectures available. We practically
demonstrate the feasibility of the approach in multiple logical systems. The
approach is stronger than training on randomly generated data but weaker than
the approaches trained on tailored axiom and conjecture sets. It however allows
us to apply machine learning to automated theorem proving for many logics,
where no such attempts have been tried to date, such as intuitionistic logic or
linear logic.",None,-1
a6056969-4c84-421f-a344-f5c3147b5f7a,Differentiable Self-Adaptive Learning Rate,0.214954,"Learning rate adaptation is a popular topic in machine learning. Gradient
Descent trains neural nerwork with a fixed learning rate. Learning rate
adaptation is proposed to accelerate the training process through adjusting the
step size in the training session. Famous works include Momentum, Adam and
Hypergradient. Hypergradient is the most special one. Hypergradient achieved
adaptation by calculating the derivative of learning rate with respect to cost
function and utilizing gradient descent for learning rate. However,
Hypergradient is still not perfect. In practice, Hypergradient fail to decrease
training loss after learning rate adaptation with a large probability. Apart
from that, evidence has been found that Hypergradient are not suitable for
dealing with large datesets in the form of minibatch training. Most
unfortunately, Hypergradient always fails to get a good accuracy on the
validation dataset although it could reduce training loss to a very tiny value.
To solve Hypergradient's problems, we propose a novel adaptation algorithm,
where learning rate is parameter specific and internal structured. We conduct
extensive experiments on multiple network models and datasets compared with
various benchmark optimizers. It is shown that our algorithm can achieve faster
and higher qualified convergence than those state-of-art optimizers.",None,-1
4d3de473-3dd9-45e5-9a3a-61fcdff45607,Evaluating Distributional Distortion in Neural Language Modeling,0.249122,"A fundamental characteristic of natural language is the high rate at which
speakers produce novel expressions. Because of this novelty, a heavy-tail of
rare events accounts for a significant amount of the total probability mass of
distributions in language (Baayen, 2001). Standard language modeling metrics
such as perplexity quantify the performance of language models (LM) in
aggregate. As a result, we have relatively little understanding of whether
neural LMs accurately estimate the probability of sequences in this heavy-tail
of rare events. To address this gap, we develop a controlled evaluation scheme
which uses generative models trained on natural data as artificial languages
from which we can exactly compute sequence probabilities. Training LMs on
generations from these artificial languages, we compare the sequence-level
probability estimates given by LMs to the true probabilities in the target
language. Our experiments reveal that LSTM and Transformer language models (i)
systematically underestimate the probability of sequences drawn from the target
language, and (ii) do so more severely for less-probable sequences.
Investigating where this probability mass went, (iii) we find that LMs tend to
overestimate the probability of ill formed (perturbed) sequences. In addition,
we find that this underestimation behaviour (iv) is weakened, but not
eliminated by greater amounts of training data, and (v) is exacerbated for
target distributions with lower entropy.",None,-1
9f7f6770-13dd-4c44-bb88-fdf7ad25cd1c,Machine Learning Methods in Solving the Boolean Satisfiability Problem,0.849792,"This paper reviews the recent literature on solving the Boolean
satisfiability problem (SAT), an archetypal NP-complete problem, with the help
of machine learning techniques. Despite the great success of modern SAT solvers
to solve large industrial instances, the design of handcrafted heuristics is
time-consuming and empirical. Under the circumstances, the flexible and
expressive machine learning methods provide a proper alternative to solve this
long-standing problem. We examine the evolving ML-SAT solvers from naive
classifiers with handcrafted features to the emerging end-to-end SAT solvers
such as NeuroSAT, as well as recent progress on combinations of existing CDCL
and local search solvers with machine learning methods. Overall, solving SAT
with machine learning is a promising yet challenging research topic. We
conclude the limitations of current works and suggest possible future
directions.",None,-1
5357fa98-19d9-4a00-bed9-1467ede17b84,K-level Reasoning for Zero-Shot Coordination in Hanabi,0.344294,"The standard problem setting in cooperative multi-agent settings is self-play
(SP), where the goal is to train a team of agents that works well together.
However, optimal SP policies commonly contain arbitrary conventions
(""handshakes"") and are not compatible with other, independently trained agents
or humans. This latter desiderata was recently formalized by Hu et al. 2020 as
the zero-shot coordination (ZSC) setting and partially addressed with their
Other-Play (OP) algorithm, which showed improved ZSC and human-AI performance
in the card game Hanabi. OP assumes access to the symmetries of the environment
and prevents agents from breaking these in a mutually incompatible way during
training. However, as the authors point out, discovering symmetries for a given
environment is a computationally hard problem. Instead, we show that through a
simple adaption of k-level reasoning (KLR) Costa Gomes et al. 2006,
synchronously training all levels, we can obtain competitive ZSC and ad-hoc
teamplay performance in Hanabi, including when paired with a human-like proxy
bot. We also introduce a new method, synchronous-k-level reasoning with a best
response (SyKLRBR), which further improves performance on our synchronous KLR
by co-training a best response.",None,-1
1399643b-ebd0-4466-875d-ae6c1f58c9ad,That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory,0.335971,"The evolution of language follows the rule of gradual change. Grammar,
vocabulary, and lexical semantic shifts take place over time, resulting in a
diachronic linguistic gap. As such, a considerable amount of texts are written
in languages of different eras, which creates obstacles for natural language
processing tasks, such as word segmentation and machine translation. Although
the Chinese language has a long history, previous Chinese natural language
processing research has primarily focused on tasks within a specific era.
Therefore, we propose a cross-era learning framework for Chinese word
segmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to
incorporate era-specific linguistic knowledge. Experiments on four corpora from
different eras show that the performance of each corpus significantly improves.
Further analyses also demonstrate that the SM can effectively integrate the
knowledge of the eras into the neural network.",http://github.com/jiayan/Jiayan/,1697
74648a8a-0102-4bc7-9232-afed4f3a1e35,Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer,0.876453,"Videos are created to express emotion, exchange information, and share
experiences. Video synthesis has intrigued researchers for a long time. Despite
the rapid progress driven by advances in visual synthesis, most existing
studies focus on improving the frames' quality and the transitions between
them, while little progress has been made in generating longer videos. In this
paper, we present a method that builds on 3D-VQGAN and transformers to generate
videos with thousands of frames. Our evaluation shows that our model trained on
16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,
and Taichi-HD datasets can generate diverse, coherent, and high-quality long
videos. We also showcase conditional extensions of our approach for generating
meaningful long videos by incorporating temporal information with text and
audio. Videos and code can be found at
https://songweige.github.io/projects/tats/index.html.",https://songweige.github.io/projects/tats,-1
83bd75ec-1b8e-4bab-90d6-9e7367aace0f,Dataset Distillation via Factorization,0.942004,"In this paper, we study \xw{dataset distillation (DD)}, from a novel
perspective and introduce a \emph{dataset factorization} approach, termed
\emph{HaBa}, which is a plug-and-play strategy portable to any existing DD
baseline. Unlike conventional DD approaches that aim to produce distilled and
representative samples, \emph{HaBa} explores decomposing a dataset into two
components: data \emph{Ha}llucination networks and \emph{Ba}ses, where the
latter is fed into the former to reconstruct image samples. The flexible
combinations between bases and hallucination networks, therefore, equip the
distilled data with exponential informativeness gain, which largely increase
the representation capability of distilled datasets. To furthermore increase
the data efficiency of compression results, we further introduce a pair of
adversarial contrastive constraints on the resultant hallucination networks and
bases, which increase the diversity of generated images and inject more
discriminant information into the factorization. Extensive comparisons and
experiments demonstrate that our method can yield significant improvement on
downstream classification tasks compared with previous state of the arts, while
reducing the total number of compressed parameters by up to 65\%. Moreover,
distilled datasets by our approach also achieve \textasciitilde10\% higher
accuracy than baseline methods in cross-architecture generalization. Our code
is available \href{https://github.com/Huage001/DatasetFactorization}{here}.",None,-1
0c7da365-cf5f-4753-8c3c-7c710d5c073c,On Improving Cross-dataset Generalization of Deepfake Detectors,0.478737,"Facial manipulation by deep fake has caused major security risks and raised
severe societal concerns. As a countermeasure, a number of deep fake detection
methods have been proposed recently. Most of them model deep fake detection as
a binary classification problem using a backbone convolutional neural network
(CNN) architecture pretrained for the task. These CNN-based methods have
demonstrated very high efficacy in deep fake detection with the Area under the
Curve (AUC) as high as 0.99. However, the performance of these methods degrades
significantly when evaluated across datasets. In this paper, we formulate deep
fake detection as a hybrid combination of supervised and reinforcement learning
(RL) to improve its cross-dataset generalization performance. The proposed
method chooses the top-k augmentations for each test sample by an RL agent in
an image-specific manner. The classification scores, obtained using CNN, of all
the augmentations of each test image are averaged together for final real or
fake classification. Through extensive experimental validation, we demonstrate
the superiority of our method over existing published research in cross-dataset
generalization of deep fake detectors, thus obtaining state-of-the-art
performance.",None,3040
f98949db-9b76-4140-a0e7-0f432c941afb,Evaluating the Text-to-SQL Capabilities of Large Language Models,0.999249,"We perform an empirical evaluation of Text-to-SQL capabilities of the Codex
language model. We find that, without any finetuning, Codex is a strong
baseline on the Spider benchmark; we also analyze the failure modes of Codex in
this setting. Furthermore, we demonstrate on the GeoQuery and Scholar
benchmarks that a small number of in-domain examples provided in the prompt
enables Codex to perform better than state-of-the-art models finetuned on such
few-shot examples.",https://github.com/nitarshan/codex-text2sql,-1
2d2b45cd-bc65-41b6-862e-8598036ccf7e,Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for the Diagnosis of Eosinophilic Esophagitis,0.271492,"Eosinophilic esophagitis (EoE) is a chronic allergic inflammatory condition
of the esophagus associated with elevated esophageal eosinophils. Second only
to gastroesophageal reflux disease, EoE is one of the leading causes of chronic
refractory dysphagia in adults and children. EoE diagnosis requires enumerating
the density of esophageal eosinophils in esophageal biopsies, a somewhat
subjective task that is time-consuming, thus reducing the ability to process
the complex tissue structure. Previous artificial intelligence (AI) approaches
that aimed to improve histology-based diagnosis focused on recapitulating
identification and quantification of the area of maximal eosinophil density.
However, this metric does not account for the distribution of eosinophils or
other histological features, over the whole slide image. Here, we developed an
artificial intelligence platform that infers local and spatial biomarkers based
on semantic segmentation of intact eosinophils and basal zone distributions.
Besides the maximal density of eosinophils (referred to as Peak Eosinophil
Count [PEC]) and a maximal basal zone fraction, we identify two additional
metrics that reflect the distribution of eosinophils and basal zone fractions.
This approach enables a decision support system that predicts EoE activity and
classifies the histological severity of EoE patients. We utilized a cohort that
includes 1066 biopsy slides from 400 subjects to validate the system's
performance and achieved a histological severity classification accuracy of
86.70%, sensitivity of 84.50%, and specificity of 90.09%. Our approach
highlights the importance of systematically analyzing the distribution of
biopsy features over the entire slide and paves the way towards a personalized
decision support system that will assist not only in counting cells but can
also potentially improve diagnosis and provide treatment prediction.",None,73438
55607afb-72a8-4151-b5a8-c02c2cfe3734,What AI can do for horse-racing ?,0.273233,"Since the 1980s, machine learning has been widely used for horse-racing
predictions, gradually expanding to where algorithms are now playing a huge
role in the betting market. Machine learning has changed the horse-racing
betting market over the last ten years, but main changes are still to come. The
paradigm shift of neural networks (deep learning) may not only improve our
ability to simply predict the outcome of a race, but it will also certainly
shake our entire way of thinking about horse-racing - and maybe more generally
about horses. Since 2012, deep learning provided more and more state-of-the-art
results in computer vision and now statistical learning or game theory. We
describe how the convergence of the three machine learning fields (computer
vision, statistical learning, and game theory) will be game-changers in the
next decade in our ability to predict and understand horse-racing. We consider
that horse-racing is a real world laboratory where we can work on the
animal-human interaction and build a non-anthropocentric Artificial
Intelligence. We believe that this will lead us to understand the horses better
and the interactions between animals and humans in general.",None,-1
701c41cb-c7e1-475a-b38d-35626fc1848d,Balancing Stability and Plasticity through Advanced Null Space in Continual Learning,0.483086,"Continual learning is a learning paradigm that learns tasks sequentially with
resources constraints, in which the key challenge is stability-plasticity
dilemma, i.e., it is uneasy to simultaneously have the stability to prevent
catastrophic forgetting of old tasks and the plasticity to learn new tasks
well. In this paper, we propose a new continual learning approach, Advanced
Null Space (AdNS), to balance the stability and plasticity without storing any
old data of previous tasks. Specifically, to obtain better stability, AdNS
makes use of low-rank approximation to obtain a novel null space and projects
the gradient onto the null space to prevent the interference on the past tasks.
To control the generation of the null space, we introduce a non-uniform
constraint strength to further reduce forgetting. Furthermore, we present a
simple but effective method, intra-task distillation, to improve the
performance of the current task. Finally, we theoretically find that null space
plays a key role in plasticity and stability, respectively. Experimental
results show that the proposed method can achieve better performance compared
to state-of-the-art continual learning approaches.",None,-1
ac6922c3-6ee1-498c-9e11-b2b1ad20ae02,Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes,0.281679,"Analogy-making gives rise to reasoning, abstraction, flexible categorization
and counterfactual inference -- abilities lacking in even the best AI systems
today. Much research has suggested that analogies are key to non-brittle
systems that can adapt to new domains. Despite their importance, analogies
received little attention in the NLP community, with most research focusing on
simple word analogies. Work that tackled more complex analogies relied heavily
on manually constructed, hard-to-scale input representations. In this work, we
explore a more realistic, challenging setup: our input is a pair of natural
language procedural texts, describing a situation or a process (e.g., how the
heart works/how a pump works). Our goal is to automatically extract entities
and their relations from the text and find a mapping between the different
domains based on relational similarity (e.g., blood is mapped to water). We
develop an interpretable, scalable algorithm and demonstrate that it identifies
the correct mappings 87% of the time for procedural texts and 94% for stories
from cognitive-psychology literature. We show it can extract analogies from a
large dataset of procedural texts, achieving 79% precision (analogy prevalence
in data: 3%). Lastly, we demonstrate that our algorithm is robust to
paraphrasing the input texts.",https://github.com/orensul/analogies_mining,-1
1b086712-279f-4ec0-9bea-3e8b60fafa91,Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint,0.805876,"Active learning is a promising alternative to alleviate the issue of high
annotation cost in the computer vision tasks by consciously selecting more
informative samples to label. Active learning for object detection is more
challenging and existing efforts on it are relatively rare. In this paper, we
propose a novel hybrid approach to address this problem, where the
instance-level uncertainty and diversity are jointly considered in a bottom-up
manner. To balance the computational complexity, the proposed approach is
designed as a two-stage procedure. At the first stage, an Entropy-based
Non-Maximum Suppression (ENMS) is presented to estimate the uncertainty of
every image, which performs NMS according to the entropy in the feature space
to remove predictions with redundant information gains. At the second stage, a
diverse prototype (DivProto) strategy is explored to ensure the diversity
across images by progressively converting it into the intra-class and
inter-class diversities of the entropy-based class-specific prototypes.
Extensive experiments are conducted on MS COCO and Pascal VOC, and the proposed
approach achieves state of the art results and significantly outperforms the
other counterparts, highlighting its superiority.",https://github.com/facebookresearch/maskrcnn-benchmark,347
dab2a3c8-61bb-4c34-a58b-6213a9867ec1,Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation and Instance Generation,0.655283,"We study the problem of few-shot Fine-grained Entity Typing (FET), where only
a few annotated entity mentions with contexts are given for each entity type.
Recently, prompt-based tuning has demonstrated superior performance to standard
fine-tuning in few-shot scenarios by formulating the entity type classification
task as a ''fill-in-the-blank'' problem. This allows effective utilization of
the strong language modeling capability of Pre-trained Language Models (PLMs).
Despite the success of current prompt-based tuning approaches, two major
challenges remain: (1) the verbalizer in prompts is either manually designed or
constructed from external knowledge bases, without considering the target
corpus and label hierarchy information, and (2) current approaches mainly
utilize the representation power of PLMs, but have not explored their
generation power acquired through extensive general-domain pre-training. In
this work, we propose a novel framework for few-shot FET consisting of two
modules: (1) an entity type label interpretation module automatically learns to
relate type labels to the vocabulary by jointly leveraging few-shot instances
and the label hierarchy, and (2) a type-based contextualized instance generator
produces new instances based on given instances to enlarge the training set for
better generalization. On three benchmark datasets, our model outperforms
existing methods by significant margins. Code can be found at
https://github.com/teapot123/Fine-Grained-Entity-Typing.",https://github.com/teapot123/Fine-Grained-Entity-Typing,-1
7f835733-faf1-4f6d-b3a3-20009a360816,Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words,0.205463,"Prompt-based fine-tuning for pre-trained models has proven effective for many
natural language processing tasks under few-shot settings in general domain.
However, tuning with prompt in biomedical domain has not been investigated
thoroughly. Biomedical words are often rare in general domain, but quite
ubiquitous in biomedical contexts, which dramatically deteriorates the
performance of pre-trained models on downstream biomedical applications even
after fine-tuning, especially in low-resource scenarios. We propose a simple
yet effective approach to helping models learn rare biomedical words during
tuning with prompt. Experimental results show that our method can achieve up to
6% improvement in biomedical natural language inference task without any extra
parameters or training steps using few-shot vanilla prompt settings.",https://github.com/s65b40/prompt_n_paraphrase,-1
efece088-5135-491f-a5e9-bb63bac97c19,Solving The Long-Tailed Problem via Intra- and Inter-Category Balance,0.0607673,"Benchmark datasets for visual recognition assume that data is uniformly
distributed, while real-world datasets obey long-tailed distribution. Current
approaches handle the long-tailed problem to transform the long-tailed dataset
to uniform distribution by re-sampling or re-weighting strategies. These
approaches emphasize the tail classes but ignore the hard examples in head
classes, which result in performance degradation. In this paper, we propose a
novel gradient harmonized mechanism with category-wise adaptive precision to
decouple the difficulty and sample size imbalance in the long-tailed problem,
which are correspondingly solved via intra- and inter-category balance
strategies. Specifically, intra-category balance focuses on the hard examples
in each category to optimize the decision boundary, while inter-category
balance aims to correct the shift of decision boundary by taking each category
as a unit. Extensive experiments demonstrate that the proposed method
consistently outperforms other approaches on all the datasets.",None,-1
36aa5920-4ee6-4c24-9408-17d3dcbb267b,SGPT: GPT Sentence Embeddings for Semantic Search,0.999996,"Decoder transformers have continued increasing in scale reaching hundreds of
billions of parameters. Due to their scale the same decoder sets
state-of-the-art results on various language tasks via prompting or
fine-tuning. Yet, these large foundation models remain unusable for the related
fields of semantic search and sentence embeddings. This prevents possibly new
state-of-the-art results and forces organizations to train and maintain
separate models. To this end, we propose SGPT to use decoders for sentence
embeddings and semantic search via prompting or fine-tuning. At 5.8 billion
parameters SGPT improves on the previously best sentence embeddings by a margin
of 7% and outperforms a concurrent method with 175 billion parameters as
measured on the BEIR search benchmark. Code, models and result files are freely
available at https://github.com/Muennighoff/sgpt.",https://github.com/Muennighoff/sgpt,4781
50e71ecc-c859-4c7c-bc94-0efe208fe148,Revealing interactions between HVDC cross-area flows and frequency stability with explainable AI,0.607329,"The energy transition introduces more volatile energy sources into the power
grids. In this context, power transfer between different synchronous areas
through High Voltage Direct Current (HVDC) links becomes increasingly
important. Such links can balance volatile generation by enabling long-distance
transport or by leveraging their fast control behavior. Here, we investigate
the interaction of power imbalances - represented through the power grid
frequency - and power flows on HVDC links between synchronous areas in Europe.
We use explainable machine learning to identify key dependencies and
disentangle the interaction of critical features. Our results show that
market-based HVDC flows introduce deterministic frequency deviations, which
however can be mitigated through strict ramping limits. Moreover, varying HVDC
operation modes strongly affect the interaction with the grid. In particular,
we show that load-frequency control via HVDC links can both have control-like
or disturbance-like impacts on frequency stability.",None,-1
35b44299-39d3-4ecc-8b1d-8cb57c7fbcd4,Maknuune: A Large Open Palestinian Arabic Lexicon,0.264859,"We present Maknuune, a large open lexicon for the Palestinian Arabic dialect.
Maknuune has over 36K entries from 17K lemmas, and 3.7K roots. All entries
include diacritized Arabic orthography, phonological transcription and English
glosses. Some entries are enriched with additional information such as broken
plurals and templatic feminine forms, associated phrases and collocations,
Standard Arabic glosses, and examples or notes on grammar, usage, or location
of collected entry.",None,16583
a2d90ec4-dae0-4690-980c-80c2d597b454,Multi-modal Multi-label Facial Action Unit Detection with Transformer,0.904693,"Facial Action Coding System is an important approach of facial expression
analysis.This paper describes our submission to the third Affective Behavior
Analysis (ABAW) 2022 competition. We proposed a transfomer based model to
detect facial action unit (FAU) in video. To be specific, we firstly trained a
multi-modal model to extract both audio and visual feature. After that, we
proposed a action units correlation module to learn relationships between each
action unit labels and refine action unit detection result. Experimental
results on validation dataset shows that our method achieves better performance
than baseline model, which verifies that the effectiveness of proposed network.",None,-1
f3ea8a8b-59d8-454d-ae04-9cd8680817af,BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning,0.700646,"Current pre-trained language models rely on large datasets for achieving
state-of-the-art performance. However, past research has shown that not all
examples in a dataset are equally important during training. In fact, it is
sometimes possible to prune a considerable fraction of the training set while
maintaining the test performance. Established on standard vision benchmarks,
two gradient-based scoring metrics for finding important examples are GraNd and
its estimated version, EL2N. In this work, we employ these two metrics for the
first time in NLP. We demonstrate that these metrics need to be computed after
at least one epoch of fine-tuning and they are not reliable in early steps.
Furthermore, we show that by pruning a small portion of the examples with the
highest GraNd/EL2N scores, we can not only preserve the test accuracy, but also
surpass it. This paper details adjustments and implementation choices which
enable GraNd and EL2N to be applied to NLP.",None,-1
4ce27c3e-3f91-4ad9-ad2d-98c15e44f623,Generating natural images with direct Patch Distributions Matching,0.777994,"Many traditional computer vision algorithms generate realistic images by
requiring that each patch in the generated image be similar to a patch in a
training image and vice versa. Recently, this classical approach has been
replaced by adversarial training with a patch discriminator. The adversarial
approach avoids the computational burden of finding nearest neighbors of
patches but often requires very long training times and may fail to match the
distribution of patches. In this paper we leverage the recently developed
Sliced Wasserstein Distance and develop an algorithm that explicitly and
efficiently minimizes the distance between patch distributions in two images.
Our method is conceptually simple, requires no training and can be implemented
in a few lines of codes. On a number of image generation tasks we show that our
results are often superior to single-image-GANs, require no training, and can
generate high quality images in a few seconds. Our implementation is available
at https://github.com/ariel415el/GPDM",https://github.com/ariel415el/GPDM,-1
3604ce65-f2a7-4b74-9b66-abbc24682729,FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference,0.632054,"Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the
architecture used for FiD was chosen by making minimal modifications to a
standard T5 model, which our analysis shows to be highly suboptimal for a
retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to
the encoder, while the majority of inference time results from memory bandwidth
constraints in the decoder. We propose two simple changes to the FiD
architecture to alleviate memory bandwidth constraints, and speed up inference
by 7x. This allows us to use a much larger decoder at modest cost. We denote
FiD with the above modifications as FiDO, and show that it strongly improves
performance over existing FiD models for a wide range of inference budgets. For
example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves
better performance than FiD-Large.",None,20836
547b018c-263c-43b1-ae1e-a450ccdf3678,Surgical Skill Assessment via Video Semantic Aggregation,0.827443,"Automated video-based assessment of surgical skills is a promising task in
assisting young surgical trainees, especially in poor-resource areas. Existing
works often resort to a CNN-LSTM joint framework that models long-term
relationships by LSTMs on spatially pooled short-term CNN features. However,
this practice would inevitably neglect the difference among semantic concepts
such as tools, tissues, and background in the spatial dimension, impeding the
subsequent temporal relationship modeling. In this paper, we propose a novel
skill assessment framework, Video Semantic Aggregation (ViSA), which discovers
different semantic parts and aggregates them across spatiotemporal dimensions.
The explicit discovery of semantic parts provides an explanatory visualization
that helps understand the neural network's decisions. It also enables us to
further incorporate auxiliary information such as the kinematic data to improve
representation learning and performance. The experiments on two datasets show
the competitiveness of ViSA compared to state-of-the-art methods. Source code
is available at: bit.ly/MICCAI2022ViSA.",None,-1
b1ad5f2a-57e2-4382-955d-4ba318ef7b0b,Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users,0.678684,"Machine-learned models for author profiling in social media often rely on
data acquired via self-reporting-based psychometric tests (questionnaires)
filled out by social media users. This is an expensive but accurate data
collection strategy. Another, less costly alternative, which leads to
potentially more noisy and biased data, is to rely on labels inferred from
publicly available information in the profiles of the users, for instance
self-reported diagnoses or test results. In this paper, we explore a third
strategy, namely to directly use a corpus of items from validated psychometric
tests as training data. Items from psychometric tests often consist of
sentences from an I-perspective (e.g., ""I make friends easily.""). Such corpora
of test items constitute 'small data', but their availability for many concepts
is a rich resource. We investigate this approach for personality profiling, and
evaluate BERT classifiers fine-tuned on such psychometric test items for the
big five personality traits (openness, conscientiousness, extraversion,
agreeableness, neuroticism) and analyze various augmentation strategies
regarding their potential to address the challenges coming with such a small
corpus. Our evaluation on a publicly available Twitter corpus shows a
comparable performance to in-domain training for 4/5 personality traits with
T5-based data augmentation.",None,-1
18ebf9d7-e340-4938-adb5-aba6bcec09ad,Tools and Practices for Responsible AI Engineering,0.612201,"Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs.",https://github.com/mit-ll-responsible-ai/,4156
2694bee1-f257-476a-8b71-47516e0ea4ad,A dynamic programming algorithm for span-based nested named-entity recognition in O(n^2),0.624836,"Span-based nested named-entity recognition (NER) has a cubic-time complexity
using a variant of the CYK algorithm. We show that by adding a supplementary
structural constraint on the search space, nested NER has a quadratic-time
complexity, that is the same asymptotic complexity than the non-nested case.
The proposed algorithm covers a large part of three standard English benchmarks
and delivers comparable experimental results.",None,-1
285f469e-e11b-4e66-875d-d93b34eceee9,Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches,0.971271,"Deep learning has substantially boosted the performance of Monocular Depth
Estimation (MDE), a critical component in fully vision-based autonomous driving
(AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack
against learning-based MDE. In particular, we use an optimization-based method
to systematically generate stealthy physical-object-oriented adversarial
patches to attack depth estimation. We balance the stealth and effectiveness of
our attack with object-oriented adversarial design, sensitive region
localization, and natural style camouflage. Using real-world driving scenarios,
we evaluate our attack on concurrent MDE models and a representative downstream
task for AD (i.e., 3D object detection). Experimental results show that our
method can generate stealthy, effective, and robust adversarial patches for
different target objects and models and achieves more than 6 meters mean depth
estimation error and 93% attack success rate (ASR) in object detection with a
patch of 1/9 of the vehicle's rear area. Field tests on three different driving
routes with a real vehicle indicate that we cause over 6 meters mean depth
estimation error and reduce the object detection rate from 90.70% to 5.16% in
continuous video frames.",None,15242
64869df1-2e5f-4b96-bdb2-fa38dcf5dc10,Considerations for meaningful sign language machine translation based on glosses,0.678136,"Automatic sign language processing is gaining popularity in Natural Language
Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in
particular, sign language translation based on glosses is a prominent approach.
In this paper, we review recent works on neural gloss translation. We find that
limitations of glosses in general and limitations of specific datasets are not
discussed in a transparent manner and that there is no common standard for
evaluation.
  To address these issues, we put forward concrete recommendations for future
research on gloss translation. Our suggestions advocate awareness of the
inherent limitations of gloss-based approaches, realistic datasets, stronger
baselines and convincing evaluation.",None,-1
a76906de-e45e-40c7-b258-c6a28f74cf17,Action Recognition for American Sign Language,0.20595,"In this research, we present our findings to recognize American Sign Language
from series of hand gestures. While most researches in literature focus only on
static handshapes, our work target dynamic hand gestures. Since dynamic signs
dataset are very few, we collect an initial dataset of 150 videos for 10 signs
and an extension of 225 videos for 15 signs. We apply transfer learning models
in combination with deep neural networks and background subtraction for videos
in different temporal settings. Our primarily results show that we can get an
accuracy of $0.86$ and $0.71$ using DenseNet201, LSTM with video sequence of 12
frames accordingly.",None,-1
5b627bf5-c005-4749-9786-a686851167fe,Multi-Modal and Multi-Factor Branching Time Active Inference,0.0379965,"Active inference is a state-of-the-art framework for modelling the brain that
explains a wide range of mechanisms such as habit formation, dopaminergic
discharge and curiosity. Recently, two versions of branching time active
inference (BTAI) based on Monte-Carlo tree search have been developed to handle
the exponential (space and time) complexity class that occurs when computing
the prior over all possible policies up to the time horizon. However, those two
versions of BTAI still suffer from an exponential complexity class w.r.t the
number of observed and latent variables being modelled. In the present paper,
we resolve this limitation by first allowing the modelling of several
observations, each of them having its own likelihood mapping. Similarly, we
allow each latent state to have its own transition mapping. The inference
algorithm then exploits the factorisation of the likelihood and transition
mappings to accelerate the computation of the posterior. Those two
optimisations were tested on the dSprites environment in which the metadata of
the dSprites dataset was used as input to the model instead of the dSprites
images. On this task, $BTAI_{VMP}$ (Champion et al., 2022b,a) was able to solve
96.9\% of the task in 5.1 seconds, and $BTAI_{BF}$ (Champion et al., 2021a) was
able to solve 98.6\% of the task in 17.5 seconds. Our new approach
($BTAI_{3MF}$) outperformed both of its predecessors by solving the task
completly (100\%) in only 2.559 seconds. Finally, $BTAI_{3MF}$ has been
implemented in a flexible and easy to use (python) package, and we developed a
graphical user interface to enable the inspection of the model's beliefs,
planning process and behaviour.",https://github.com/ChampiB/Experiments_AI_TS,-1
f2b348df-d8b7-421e-a368-ba4622cdc01b,Overlap-guided Gaussian Mixture Models for Point Cloud Registration,0.436437,"Probabilistic 3D point cloud registration methods have shown competitive
performance in overcoming noise, outliers, and density variations. However,
registering point cloud pairs in the case of partial overlap is still a
challenge. This paper proposes a novel overlap-guided probabilistic
registration approach that computes the optimal transformation from matched
Gaussian Mixture Model (GMM) parameters. We reformulate the registration
problem as the problem of aligning two Gaussian mixtures such that a
statistical discrepancy measure between the two corresponding mixtures is
minimized. We introduce a Transformer-based detection module to detect
overlapping regions, and represent the input point clouds using GMMs by guiding
their alignment through overlap scores computed by this detection module.
Experiments show that our method achieves superior registration accuracy and
efficiency than state-of-the-art methods when handling point clouds with
partial overlap and different densities on synthetic and real-world datasets.
https://github.com/gfmei/ogmm",https://github.com/gfmei/ogmm,-1
840eb647-f85f-488b-952e-267ad9dba40a,Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation,0.994078,"The performance of nighttime semantic segmentation is restricted by the poor
illumination and a lack of pixel-wise annotation, which severely limit its
application in autonomous driving. Existing works, e.g., using the twilight as
the intermediate target domain to perform the adaptation from daytime to
nighttime, may fail to cope with the inherent difference between datasets
caused by the camera equipment and the urban style. Faced with these two types
of domain shifts, i.e., the illumination and the inherent difference of the
datasets, we propose a novel domain adaptation framework via cross-domain
correlation distillation, called CCDistill. The invariance of illumination or
inherent difference between two images is fully explored so as to make up for
the lack of labels for nighttime images. Specifically, we extract the content
and style knowledge contained in features, calculate the degree of inherent or
illumination difference between two images. The domain adaptation is achieved
using the invariance of the same kind of difference. Extensive experiments on
Dark Zurich and ACDC demonstrate that CCDistill achieves the state-of-the-art
performance for nighttime semantic segmentation. Notably, our method is a
one-stage domain adaptation network which can avoid affecting the inference
time. Our implementation is available at https://github.com/ghuan99/CCDistill.",https://github.com/ghuan99/CCDistill,-1
a40d3ce3-ad42-4846-923c-4c4ba4a281d1,TEMPERA: Test-Time Prompting via Reinforcement Learning,0.726416,"Careful prompt design is critical to the use of large language models in
zero-shot or few-shot learning. As a consequence, there is a growing interest
in automated methods to design optimal prompts. In this work, we propose
Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to
prior prompt generation methods, TEMPERA can efficiently leverage prior
knowledge, is adaptive to different queries and provides an interpretable
prompt for every query. To achieve this, we design a novel action space that
allows flexible editing of the initial prompts covering a wide set of
commonly-used components like instructions, few-shot exemplars, and
verbalizers. The proposed method achieves significant gains compared with
recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a
variety of tasks including sentiment analysis, topic classification, natural
language inference, and reading comprehension. Our method achieves 5.33x on
average improvement in sample efficiency when compared to the traditional
fine-tuning methods.",https://github.com/tianjunz/TEMPERA,-1
502f1f30-ed03-4005-849b-dc03d8d9caca,Provable Benefits of Representational Transfer in Reinforcement Learning,0.441008,"We study the problem of representational transfer in RL, where an agent first
pretrains in a number of source tasks to discover a shared representation,
which is subsequently used to learn a good policy in a \emph{target task}. We
propose a new notion of task relatedness between source and target tasks, and
develop a novel approach for representational transfer under this assumption.
Concretely, we show that given generative access to source tasks, we can
discover a representation, using which subsequent linear RL techniques quickly
converge to a near-optimal policy in the target task.
  The sample complexity is close to knowing the ground truth features in the
target task, and comparable to prior representation learning results in the
source tasks. We complement our positive results with lower bounds without
generative access, and validate our findings with empirical evaluation on rich
observation MDPs that require deep exploration. In our experiments, we observe
a speed up in learning in the target by pre-training, and also validate the
need for generative access in source tasks.",None,-1
97a33ed9-51f6-469f-981c-93e8b09e7016,VideoDex: Learning Dexterity from Internet Videos,0.842396,"To build general robotic agents that can operate in many environments, it is
often imperative for the robot to collect experience in the real world.
However, this is often not feasible due to safety, time, and hardware
restrictions. We thus propose leveraging the next best thing as real-world
experience: internet videos of humans using their hands. Visual priors, such as
visual features, are often learned from videos, but we believe that more
information from videos can be utilized as a stronger prior. We build a
learning algorithm, VideoDex, that leverages visual, action, and physical
priors from human video datasets to guide robot behavior. These actions and
physical priors in the neural network dictate the typical human behavior for a
particular robot task. We test our approach on a robot arm and dexterous
hand-based system and show strong results on various manipulation tasks,
outperforming various state-of-the-art methods. Videos at
https://video-dex.github.io",https://github.com/AGI-Labs/robot_baselines,-1
41e69fac-fba3-4486-aa54-50e39be6acec,DICE: Data-Efficient Clinical Event Extraction with Generative Models,0.784135,"Event extraction for the clinical domain is an under-explored research area.
The lack of training data along with the high volume of domain-specific
terminologies with vague entity boundaries makes the task especially
challenging. In this paper, we introduce DICE, a robust and data-efficient
generative model for clinical event extraction. DICE frames event extraction as
a conditional generation problem and introduces a contrastive learning
objective to accurately decide the boundaries of biomedical mentions. DICE also
trains an auxiliary mention identification task jointly with event extraction
tasks to better identify entity mention boundaries, and further introduces
special markers to incorporate identified entity mentions as trigger and
argument candidates for their respective tasks. To benchmark clinical event
extraction, we compose MACCROBAT-EE, the first clinical event extraction
dataset with argument annotation, based on an existing clinical information
extraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art
performances of DICE for clinical and news domain event extraction, especially
under low data settings.",None,-1
8cf53e97-7fe9-4d9c-95da-9f058fad8fad,Probabilistic Representations for Video Contrastive Learning,0.716789,"This paper presents Probabilistic Video Contrastive Learning, a
self-supervised representation learning method that bridges contrastive
learning with probabilistic representation. We hypothesize that the clips
composing the video have different distributions in short-term duration, but
can represent the complicated and sophisticated video distribution through
combination in a common embedding space. Thus, the proposed method represents
video clips as normal distributions and combines them into a Mixture of
Gaussians to model the whole video distribution. By sampling embeddings from
the whole video distribution, we can circumvent the careful sampling strategy
or transformations to generate augmented views of the clips, unlike previous
deterministic methods that have mainly focused on such sample generation
strategies for contrastive learning. We further propose a stochastic
contrastive loss to learn proper video distributions and handle the inherent
uncertainty from the nature of the raw video. Experimental results verify that
our probabilistic embedding stands as a state-of-the-art video representation
learning for action recognition and video retrieval on the most popular
benchmarks, including UCF101 and HMDB51.",None,-1
9747e168-e1a9-4773-86b9-9019b47eb266,A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism,0.151805,"Image captioning is a fast-growing research field of computer vision and
natural language processing that involves creating text explanations for
images. This study aims to develop a system that uses a pre-trained
convolutional neural network (CNN) to extract features from an image,
integrates the features with an attention mechanism, and creates captions using
a recurrent neural network (RNN). To encode an image into a feature vector as
graphical attributes, we employed multiple pre-trained convolutional neural
networks. Following that, a language model known as GRU is chosen as the
decoder to construct the descriptive sentence. In order to increase
performance, we merge the Bahdanau attention model with GRU to allow learning
to be focused on a specific portion of the image. On the MSCOCO dataset, the
experimental results achieve competitive performance against state-of-the-art
approaches.",None,-1
96af97ab-785b-4c0c-a8c5-f4f55446800a,Generalized Strategic Classification and the Case of Aligned Incentives,0.609014,"Strategic classification studies learning in settings where self-interested
users can strategically modify their features to obtain favorable predictive
outcomes. A key working assumption, however, is that ""favorable"" always means
""positive""; this may be appropriate in some applications (e.g., loan approval),
but reduces to a fairly narrow view of what user interests can be. In this work
we argue for a broader perspective on what accounts for strategic user
behavior, and propose and study a flexible model of generalized strategic
classification. Our generalized model subsumes most current models but includes
other novel settings; among these, we identify and target one intriguing
sub-class of problems in which the interests of users and the system are
aligned. This setting reveals a surprising fact: that standard max-margin
losses are ill-suited for strategic inputs. Returning to our fully generalized
model, we propose a novel max-margin framework for strategic learning that is
practical and effective, and which we analyze theoretically. We conclude with a
set of experiments that empirically demonstrate the utility of our approach.",https://github.com/SagiLevanon1/GSC,-1
a45102d3-5747-4d95-9be6-20c212dae0ea,Unsupervised Unlearning of Concept Drift with Autoencoders,0.562066,"Concept drift refers to a change in the data distribution affecting the data
stream of future samples. Consequently, learning models operating on the data
stream might become obsolete, and need costly and difficult adjustments such as
retraining or adaptation. Existing methods usually implement a local concept
drift adaptation scheme, where either incremental learning of the models is
used, or the models are completely retrained when a drift detection mechanism
triggers an alarm. This paper proposes an alternative approach in which an
unsupervised and model-agnostic concept drift adaptation method at the global
level is introduced, based on autoencoders. Specifically, the proposed method
aims to ``unlearn'' the concept drift without having to retrain or adapt any of
the learning models operating on the data. An extensive experimental evaluation
is conducted in two application domains. We consider a realistic water
distribution network with more than 30 models in-place, from which we create
200 simulated data sets / scenarios. We further consider an image-related task
to demonstrate the effectiveness of our method.",https://github.com/HammerLabML/UnsupervisedUnlearningConceptDriftAutoencoders,-1
e69bb05f-b3fb-4ca9-9167-b286a746f075,A Fast Post-Training Pruning Framework for Transformers,0.615454,"Pruning is an effective way to reduce the huge inference cost of Transformer
models. However, prior work on pruning Transformers requires retraining the
models. This can add high training cost and high complexity to model
deployment, making it difficult to use in many practical situations. To address
this, we propose a fast post-training pruning framework for Transformers that
does not require any retraining. Given a resource constraint and a sample
dataset, our framework automatically prunes the Transformer model using
structured sparsity methods. To retain high accuracy without retraining, we
introduce three novel techniques: (i) a lightweight mask search algorithm that
finds which heads and filters to prune based on the Fisher information; (ii)
mask rearrangement that complements the search algorithm; and (iii) mask tuning
that reconstructs the output activations for each layer. We apply our method to
BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD
benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x
speedup in inference latency, while maintaining < 1% loss in accuracy.
Importantly, our framework prunes Transformers in less than 3 minutes on a
single GPU, which is over two orders of magnitude faster than existing pruning
approaches that retrain the models.",https://github.com/WoosukKwon/retraining-free-pruning,-1
957160d6-536a-42e7-8314-a6aafcf06745,SCVRL: Shuffled Contrastive Video Representation Learning,0.507478,"We propose SCVRL, a novel contrastive-based framework for self-supervised
learning for videos. Differently from previous contrast learning based methods
that mostly focus on learning visual semantics (e.g., CVRL), SCVRL is capable
of learning both semantic and motion patterns. For that, we reformulate the
popular shuffling pretext task within a modern contrastive learning paradigm.
We show that our transformer-based network has a natural capacity to learn
motion in self-supervised settings and achieves strong performance,
outperforming CVRL on four benchmarks.",None,-1
80297dcb-0adb-431c-8614-0ba3f1b690d7,AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation,0.296505,"Training a generalizable 3D part segmentation network is quite challenging
but of great importance in real-world applications. To tackle this problem,
some works design task-specific solutions by translating human understanding of
the task to machine's learning process, which faces the risk of missing the
optimal strategy since machines do not necessarily understand in the exact
human way. Others try to use conventional task-agnostic approaches designed for
domain generalization problems with no task prior knowledge considered. To
solve the above issues, we propose AutoGPart, a generic method enabling
training generalizable 3D part segmentation networks with the task prior
considered. AutoGPart builds a supervision space with geometric prior knowledge
encoded, and lets the machine to search for the optimal supervisions from the
space for a specific segmentation task automatically. Extensive experiments on
three generalizable 3D part segmentation tasks are conducted to demonstrate the
effectiveness and versatility of AutoGPart. We demonstrate that the performance
of segmentation networks using simple backbones can be significantly improved
when trained with supervisions searched by our method.",https://autogpart.github.io,-1
f93831f8-1ad4-48f4-bc46-34cf97cde3fb,Vision Transformer Compression with Structured Pruning and Low Rank Approximation,0.0567005,"Transformer architecture has gained popularity due to its ability to scale
with large dataset. Consequently, there is a need to reduce the model size and
latency, especially for on-device deployment. We focus on vision transformer
proposed for image recognition task (Dosovitskiy et al., 2021), and explore the
application of different compression techniques such as low rank approximation
and pruning for this purpose. Specifically, we investigate a structured pruning
method proposed recently in Zhu et al. (2021) and find that mostly feedforward
blocks are pruned with this approach, that too, with severe degradation in
accuracy. We propose a hybrid compression approach to mitigate this where we
compress the attention blocks using low rank approximation and use the
previously mentioned pruning with a lower rate for feedforward blocks in each
transformer layer. Our technique results in 50% compression with 14% relative
increase in classification error whereas we obtain 44% compression with 20%
relative increase in error when only pruning is applied. We propose further
enhancements to bridge the accuracy gap but leave it as a future work.",None,-1
87dcb339-659a-4c1d-98d7-979bebb2df8c,MetricBERT: Text Representation Learning via Self-Supervised Triplet Training,0.215452,"We present MetricBERT, a BERT-based model that learns to embed text under a
well-defined similarity metric while simultaneously adhering to the
``traditional'' masked-language task. We focus on downstream tasks of learning
similarities for recommendations where we show that MetricBERT outperforms
state-of-the-art alternatives, sometimes by a substantial margin. We conduct
extensive evaluations of our method and its different variants, showing that
our training objective is highly beneficial over a traditional contrastive
loss, a standard cosine similarity objective, and six other baselines. As an
additional contribution, we publish a dataset of video games descriptions along
with a test set of similarity annotations crafted by a domain expert.",None,-1
72e3689a-37c7-433b-af3a-1b697c4ac6c3,Domain Adaptation meets Individual Fairness. And they get along,0.540242,"Many instances of algorithmic bias are caused by distributional shifts. For
example, machine learning (ML) models often perform worse on demographic groups
that are underrepresented in the training data. In this paper, we leverage this
connection between algorithmic fairness and distribution shifts to show that
algorithmic fairness interventions can help ML models overcome distribution
shifts, and that domain adaptation methods (for overcoming distribution shifts)
can mitigate algorithmic biases. In particular, we show that (i) enforcing
suitable notions of individual fairness (IF) can improve the
out-of-distribution accuracy of ML models under the covariate shift assumption
and that (ii) it is possible to adapt representation alignment methods for
domain adaptation to enforce individual fairness. The former is unexpected
because IF interventions were not developed with distribution shifts in mind.
The latter is also unexpected because representation alignment is not a common
approach in the individual fairness literature.",None,-1
6896140c-10ee-498a-b607-b3e16f10e6b9,I see what you hear: a vision-inspired method to localize words,0.0426192,"This paper explores the possibility of using visual object detection
techniques for word localization in speech data. Object detection has been
thoroughly studied in the contemporary literature for visual data. Noting that
an audio can be interpreted as a 1-dimensional image, object localization
techniques can be fundamentally useful for word localization. Building upon
this idea, we propose a lightweight solution for word detection and
localization. We use bounding box regression for word localization, which
enables our model to detect the occurrence, offset, and duration of keywords in
a given audio stream. We experiment with LibriSpeech and train a model to
localize 1000 words. Compared to existing work, our method reduces model size
by 94%, and improves the F1 score by 6.5\%.",None,-1
135084e0-e05f-48eb-b4bb-d727a5ca82d3,Data-Efficiency with a Single GPU: An Exploration of Transfer Methods for Small Language Models,0.0247512,"Multi-task learning (MTL), instruction tuning, and prompting have recently
been shown to improve the generalizability of large language models to new
tasks. However, the benefits of such methods are less well-documented in
smaller language models, with some studies finding contradictory results. In
this work, we explore and isolate the effects of (i) model size, (ii) general
purpose MTL, (iii) in-domain MTL, (iv) instruction tuning, and (v) few-shot
fine-tuning for models with fewer than 500 million parameters. Our experiments
in the zero-shot setting demonstrate that models gain 31% relative improvement,
on average, from general purpose MTL, with an additional 37.6% relative gain
from in-domain MTL. Contradictory to prior works on large models, we find that
instruction tuning provides a modest 2% performance improvement for small
models.",None,-1
c2bdc759-1dec-449f-82ed-b078447ce7a2,Learning to Act with Affordance-Aware Multimodal Neural SLAM,0.260727,"Recent years have witnessed an emerging paradigm shift toward embodied
artificial intelligence, in which an agent must learn to solve challenging
tasks by interacting with its environment. There are several challenges in
solving embodied multimodal tasks, including long-horizon planning,
vision-and-language grounding, and efficient exploration. We focus on a
critical bottleneck, namely the performance of planning and navigation. To
tackle this challenge, we propose a Neural SLAM approach that, for the first
time, utilizes several modalities for exploration, predicts an affordance-aware
semantic map, and plans over it at the same time. This significantly improves
exploration efficiency, leads to robust long-horizon planning, and enables
effective vision-and-language grounding. With the proposed Affordance-aware
Multimodal Neural SLAM (AMSLAM) approach, we obtain more than 40% improvement
over prior published work on the ALFRED benchmark and set a new
state-of-the-art generalization performance at a success rate of 23.48% on the
test unseen scenes.",https://github.com/amazon-research/multimodal-neuralslam,-1
3a7a93a7-c72e-4ae7-b8c2-73ddf98c02ee,Vector Quantized Semantic Communication System,0.343774,"Although analog semantic communication systems have received considerable
attention in the literature, there is less work on digital semantic
communication systems. In this paper, we develop a deep learning (DL)-enabled
vector quantized (VQ) semantic communication system for image transmission,
named VQ-DeepSC. Specifically, we propose a convolutional neural network
(CNN)-based transceiver to extract multi-scale semantic features of images and
introduce multi-scale semantic embedding spaces to perform semantic feature
quantization, rendering the data compatible with digital communication systems.
Furthermore, we employ adversarial training to improve the quality of received
images by introducing a PatchGAN discriminator. Experimental results
demonstrate that the proposed VQ-DeepSC is more robustness than BPG in digital
communication systems and has comparable MS-SSIM performance to the DeepJSCC
method.",None,10261
4caf18b3-5f6a-4e35-9d76-74b4dd3fbbdc,Can language models handle recursively nested grammatical structures? A case study on comparing models and humans,0.997492,"How should we compare the capabilities of language models (LMs) and humans? I
draw inspiration from comparative psychology to highlight some challenges. In
particular, I consider a case study: processing of recursively nested
grammatical structures. Prior work suggests that LMs cannot handle these
structures as reliably as humans can. However, the humans were provided with
instructions and training, while the LMs were evaluated zero-shot. I therefore
match the evaluation more closely. Providing large LMs with a simple prompt --
substantially less content than the human training -- allows the LMs to
consistently outperform the human results, and even to extrapolate to more
deeply nested conditions than were tested with humans. Further, reanalyzing the
prior human data suggests that the humans may not perform above chance at the
difficult structures initially. Thus, large LMs may indeed process recursively
nested grammatical structures as reliably as humans. This case study highlights
how discrepancies in the evaluation can confound comparisons of language models
and humans. I therefore reflect on the broader challenge of comparing human and
model capabilities, and highlight an important difference between evaluating
cognitive models and foundation models.",https://github.com/google/BIG-bench/,-1
8ec51ac3-17b4-4f91-8427-46d727665f4d,Q-ViT: Fully Differentiable Quantization for Vision Transformer,0.808892,"In this paper, we propose a fully differentiable quantization method for
vision transformer (ViT) named as Q-ViT, in which both of the quantization
scales and bit-widths are learnable parameters. Specifically, based on our
observation that heads in ViT display different quantization robustness, we
leverage head-wise bit-width to squeeze the size of Q-ViT while preserving
performance. In addition, we propose a novel technique named switchable scale
to resolve the convergence problem in the joint training of quantization scales
and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to
3-bit without heavy performance drop. Moreover, we analyze the quantization
robustness of every architecture component of ViT and show that the Multi-head
Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key
aspects for ViT quantization. This study provides some insights for further
research about ViT quantization. Extensive experiments on different ViT models,
such as DeiT and Swin Transformer show the effectiveness of our quantization
method. In particular, our method outperforms the state-of-the-art uniform
quantization method by 1.5% on DeiT-Tiny.",https://github.com/zhexinli/Q-ViT-DeiT,-1
c9c7e888-cb49-431c-9f19-7434e14802fa,Boundary Smoothing for Named Entity Recognition,0.903493,"Neural named entity recognition (NER) models may easily encounter the
over-confidence issue, which degrades the performance and calibration. Inspired
by label smoothing and driven by the ambiguity of boundary annotation in NER
engineering, we propose boundary smoothing as a regularization technique for
span-based neural NER models. It re-assigns entity probabilities from annotated
spans to the surrounding ones. Built on a simple but strong baseline, our model
achieves results better than or competitive with previous state-of-the-art
systems on eight well-known NER benchmarks. Further empirical analysis suggests
that boundary smoothing effectively mitigates over-confidence, improves model
calibration, and brings flatter neural minima and more smoothed loss
landscapes.",https://github.com/syuoni/eznlp,-1
864c39c7-b81c-4171-a0d3-b811c0378890,Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models,0.150708,"With a growing number of BERTology work analyzing different components of
pre-trained language models, we extend this line of research through an
in-depth analysis of discourse information in pre-trained and fine-tuned
language models. We move beyond prior work along three dimensions: First, we
describe a novel approach to infer discourse structures from arbitrarily long
documents. Second, we propose a new type of analysis to explore where and how
accurately intrinsic discourse is captured in the BERT and BART models.
Finally, we assess how similar the generated structures are to a variety of
baselines as well as their distribution within and between models.",https://github.com/Wendy-Xiao/summ_guided_disco_parser,-1
a2f2fc30-06f8-4253-9051-54a8ac19d71f,Few-shot Query-Focused Summarization with Prefix-Merging,0.375631,"Query-focused summarization has been considered as an important extension for
text summarization. It aims to generate a concise highlight for a given query.
Different from text summarization, query-focused summarization has long been
plagued by the problem of lacking high-quality large-scale datasets. In this
paper, we investigate the idea that whether we can integrate and transfer the
knowledge of text summarization and question answering to assist the few-shot
learning in query-focused summarization. Here, we propose prefix-merging, a
prefix-based pretraining strategy for few-shot learning in query-focused
summarization. Drawn inspiration from prefix-tuning, we are allowed to
integrate the task knowledge from text summarization and question answering
into a properly designed prefix and apply the merged prefix to query-focused
summarization. With only a small amount of trainable parameters, prefix-merging
outperforms fine-tuning on query-focused summarization. We further discuss the
influence of different prefix designs and propose a visualized explanation for
how prefix-merging works.",None,-1
36b20226-8da5-4f20-ba60-25cff1f4b142,Improving Low-Resource Speech Recognition with Pretrained Speech Models: Continued Pretraining vs. Semi-Supervised Training,0.553124,"Self-supervised Transformer based models, such as wav2vec 2.0 and HuBERT,
have produced significant improvements over existing approaches to automatic
speech recognition (ASR). This is evident in the performance of the wav2vec 2.0
based pretrained XLSR-53 model across many languages when fine-tuned with
available labeled data. However, the performance from finetuning these models
can be dependent on the amount of in-language or similar-to-in-language data
included in the pretraining dataset. In this paper we investigate continued
pretraining (CoPT) with unlabeled in-language audio data on the XLSR-53
pretrained model in several low-resource languages. CoPT is more
computationally efficient than semi-supervised training (SST), the standard
approach of utilizing unlabeled data in ASR, since it omits the need for
pseudo-labeling of the unlabeled data. We show CoPT results in word error rates
(WERs), equal to or slightly better than using SST. In addition, we show that
using the CoPT model for pseudo-labeling, and using these labels in SST,
results in further improvements in WER.",None,604
dfe8387d-8e92-4d49-b36d-0dee4e47add2,Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search,0.509918,"Complex reasoning problems contain states that vary in the computational cost
required to determine a good action plan. Taking advantage of this property, we
propose Adaptive Subgoal Search (AdaSubS), a search method that adaptively
adjusts the planning horizon. To this end, AdaSubS generates diverse sets of
subgoals at different distances. A verification mechanism is employed to filter
out unreachable subgoals swiftly, allowing to focus on feasible further
subgoals. In this way, AdaSubS benefits from the efficiency of planning with
longer subgoals and the fine control with the shorter ones, and thus scales
well to difficult planning problems. We show that AdaSubS significantly
surpasses hierarchical planning algorithms on three complex reasoning tasks:
Sokoban, the Rubik's Cube, and inequality proving benchmark INT.",https://github.com/AdaptiveSubgoalSearch/adaptive_subs,-1
2c13be68-8645-409e-a159-af41b02f88b3,Causality for Inherently Explainable Transformers: CAT-XPLAIN,0.0911663,"There have been several post-hoc explanation approaches developed to explain
pre-trained black-box neural networks. However, there is still a gap in
research efforts toward designing neural networks that are inherently
explainable. In this paper, we utilize a recently proposed instance-wise
post-hoc causal explanation method to make an existing transformer architecture
inherently explainable. Once trained, our model provides an explanation in the
form of top-$k$ regions in the input space of the given instance contributing
to its decision. We evaluate our method on binary classification tasks using
three image datasets: MNIST, FMNIST, and CIFAR. Our results demonstrate that
compared to the causality-based post-hoc explainer model, our inherently
explainable model achieves better explainability results while eliminating the
need of training a separate explainer model. Our code is available at
https://github.com/mvrl/CAT-XPLAIN.",https://github.com/mvrl/CAT-XPLAIN,-1
68b79004-588d-482d-9d9f-f10955d49727,Tone prediction and orthographic conversion for Basaa,0.421675,"In this paper, we present a seq2seq approach for transliterating missionary
Basaa orthographies into the official orthography. Our model uses pre-trained
Basaa missionary and official orthography corpora using BERT. Since Basaa is a
low-resource language, we have decided to use the mT5 model for our project.
Before training our model, we pre-processed our corpora by eliminating
one-to-one correspondences between spellings and unifying characters variably
containing either one to two characters into single-character form. Our best
mT5 model achieved a CER equal to 12.6747 and a WER equal to 40.1012.",None,-1
5be1c56b-f6e5-4c0d-9715-bec8f4981a62,End-to-End Speech to Intent Prediction to improve E-commerce Customer Support Voicebot in Hindi and English,0.2387,"Automation of on-call customer support relies heavily on accurate and
efficient speech-to-intent (S2I) systems. Building such systems using
multi-component pipelines can pose various challenges because they require
large annotated datasets, have higher latency, and have complex deployment.
These pipelines are also prone to compounding errors. To overcome these
challenges, we discuss an end-to-end (E2E) S2I model for customer support
voicebot task in a bilingual setting. We show how we can solve E2E intent
classification by leveraging a pre-trained automatic speech recognition (ASR)
model with slight modification and fine-tuning on small annotated datasets.
Experimental results show that our best E2E model outperforms a conventional
pipeline by a relative ~27% on the F1 score.",None,-1
eff0cfe7-acfc-4b9d-8c23-e1d928455504,Automatic Speech Recognition for Speech Assessment of Persian Preschool Children,0.287628,"Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about children's growth and development. The COVID-19
pandemic has highlighted the necessity of online assessment for preschool
children. One of the areas that should be tested is their ability to speak.
Employing an Automatic Speech Recognition (ASR) system would not help since
they are pre-trained on voices that differ from children's in terms of
frequency and amplitude. Because most of these are pre-trained with data in a
specific range of amplitude, their objectives do not make them ready for voices
in different amplitudes. To overcome this issue, we added a new objective to
the masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch
(RFP). In addition, we used our newly introduced dataset to fine-tune our model
for Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using
masking in concatenation with RFP outperforms the masking objective of Wav2Vec
2.0 by reaching a Word Error Rate (WER) of 1.35. Our new approach reaches a WER
of 6.45 on the Persian section of the CommonVoice dataset. Furthermore, our
novel methodology produces positive outcomes in zero- and few-shot scenarios.",https://github.com/AmirAbaskohi/Automatic-Speech-recognition-for-Speech-Assessment-of-Persian-Preschool-Children,-1
7042bee2-08a8-473b-9bcb-486c69e5c3f0,Excavating RoI Attention for Underwater Object Detection,0.30269,"Self-attention is one of the most successful designs in deep learning, which
calculates the similarity of different tokens and reconstructs the feature
based on the attention matrix. Originally designed for NLP, self-attention is
also popular in computer vision, and can be categorized into pixel-level
attention and patch-level attention. In object detection, RoI features can be
seen as patches from base feature maps. This paper aims to apply the attention
module to RoI features to improve performance. Instead of employing an original
self-attention module, we choose the external attention module, a modified
self-attention with reduced parameters. With the proposed double head structure
and the Positional Encoding module, our method can achieve promising
performance in object detection. The comprehensive experiments show that it
achieves promising performance, especially in the underwater object detection
dataset. The code will be avaiable in:
https://github.com/zsyasd/Excavating-RoI-Attention-for-Underwater-Object-Detection",https://github.com/zsyasd/Excavating-RoI-Attention-for-Underwater-Object-Detection,-1
6c0a1eb5-4a8a-4f5f-a670-c9ce564cfbb6,Revisiting Self-Supervised Contrastive Learning for Facial Expression Recognition,0.645136,"The success of most advanced facial expression recognition works relies
heavily on large-scale annotated datasets. However, it poses great challenges
in acquiring clean and consistent annotations for facial expression datasets.
On the other hand, self-supervised contrastive learning has gained great
popularity due to its simple yet effective instance discrimination training
strategy, which can potentially circumvent the annotation issue. Nevertheless,
there remain inherent disadvantages of instance-level discrimination, which are
even more challenging when faced with complicated facial representations. In
this paper, we revisit the use of self-supervised contrastive learning and
explore three core strategies to enforce expression-specific representations
and to minimize the interference from other facial attributes, such as identity
and face styling. Experimental results show that our proposed method
outperforms the current state-of-the-art self-supervised learning methods, in
terms of both categorical and dimensional facial expression recognition tasks.",None,-1
ad0389ef-fc31-4585-a4e6-2e7a808277fd,"A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes",0.574235,"While datasets with single-label supervision have propelled rapid advances in
image classification, additional annotations are necessary in order to
quantitatively assess how models make predictions. To this end, for a subset of
ImageNet samples, we collect segmentation masks for the entire object and $18$
informative attributes. We call this dataset RIVAL10 (RIch Visual Attributes
with Localization), consisting of roughly $26k$ instances over $10$ classes.
Using RIVAL10, we evaluate the sensitivity of a broad set of models to noise
corruptions in foregrounds, backgrounds and attributes. In our analysis, we
consider diverse state-of-the-art architectures (ResNets, Transformers) and
training procedures (CLIP, SimCLR, DeiT, Adversarial Training). We find that,
somewhat surprisingly, in ResNets, adversarial training makes models more
sensitive to the background compared to foreground than standard training.
Similarly, contrastively-trained models also have lower relative foreground
sensitivity in both transformers and ResNets. Lastly, we observe intriguing
adaptive abilities of transformers to increase relative foreground sensitivity
as corruption level increases. Using saliency methods, we automatically
discover spurious features that drive the background sensitivity of models and
assess alignment of saliency maps with foregrounds. Finally, we quantitatively
study the attribution problem for neural features by comparing feature saliency
with ground-truth localization of semantic attributes.",None,-1
b26488b7-daca-4885-b7ab-ab59e0ac8397,A Reference Model for Common Understanding of Capabilities and Skills in Manufacturing,0.944496,"In manufacturing, many use cases of Industry 4.0 require vendor-neutral and
machine-readable information models to describe, implement and execute resource
functions. Such models have been researched under the terms capabilities and
skills. Standardization of such models is required, but currently not
available. This paper presents a reference model developed jointly by members
of various organizations in a working group of the Plattform Industrie 4.0.
This model covers definitions of most important aspects of capabilities and
skills. It can be seen as a basis for further standardization efforts.",None,-1
540fa43e-ad28-4942-afba-ece2ca0c8578,Linguistic Elements of Engaging Customer Service Discourse on Social Media,0.0872859,"Customers are rapidly turning to social media for customer support. While
brand agents on these platforms are motivated and well-intentioned to help and
engage with customers, their efforts are often ignored if their initial
response to the customer does not match a specific tone, style, or topic the
customer is aiming to receive. The length of a conversation can reflect the
effort and quality of the initial response made by a brand toward collaborating
and helping consumers, even when the overall sentiment of the conversation
might not be very positive. Thus, through this study, we aim to bridge this
critical gap in the existing literature by analyzing language's content and
stylistic aspects such as expressed empathy, psycho-linguistic features,
dialogue tags, and metrics for quantifying personalization of the utterances
that can influence the engagement of an interaction. This paper demonstrates
that we can predict engagement using initial customer and brand posts.",None,-1
094a6299-1de2-4cb4-8ca9-5f09fb0041de,1st Place Solution to the EPIC-Kitchens Action Anticipation Challenge 2022,0.0394145,"In this report, we describe the technical details of our submission to the
EPIC-Kitchens Action Anticipation Challenge 2022. In this competition, we
develop the following two approaches. 1) Anticipation Time Knowledge
Distillation using the soft labels learned by the teacher model as knowledge to
guide the student network to learn the information of anticipation time; 2)
Verb-Noun Relation Module for building the relationship between verbs and
nouns. Our method achieves state-of-the-art results on the testing set of
EPIC-Kitchens Action Anticipation Challenge 2022.",None,-1
2abf45d1-2f1b-4b57-97ec-1aeae74ce57d,GaIA: Graphical Information Gain based Attention Network for Weakly Supervised Point Cloud Semantic Segmentation,0.735116,"While point cloud semantic segmentation is a significant task in 3D scene
understanding, this task demands a time-consuming process of fully annotating
labels. To address this problem, recent studies adopt a weakly supervised
learning approach under the sparse annotation. Different from the existing
studies, this study aims to reduce the epistemic uncertainty measured by the
entropy for a precise semantic segmentation. We propose the graphical
information gain based attention network called GaIA, which alleviates the
entropy of each point based on the reliable information. The graphical
information gain discriminates the reliable point by employing relative entropy
between target point and its neighborhoods. We further introduce anchor-based
additive angular margin loss, ArcPoint. The ArcPoint optimizes the unlabeled
points containing high entropy towards semantically similar classes of the
labeled points on hypersphere space. Experimental results on S3DIS and
ScanNet-v2 datasets demonstrate our framework outperforms the existing weakly
supervised methods. We have released GaIA at https://github.com/Karel911/GaIA.",https://github.com/Karel911/GaIA,-1
a0482eb6-22be-4dab-815b-bc1e382fd9d9,ARST: Auto-Regressive Surgical Transformer for Phase Recognition from Laparoscopic Videos,0.83607,"Phase recognition plays an essential role for surgical workflow analysis in
computer assisted intervention. Transformer, originally proposed for sequential
data modeling in natural language processing, has been successfully applied to
surgical phase recognition. Existing works based on transformer mainly focus on
modeling attention dependency, without introducing auto-regression. In this
work, an Auto-Regressive Surgical Transformer, referred as ARST, is first
proposed for on-line surgical phase recognition from laparoscopic videos,
modeling the inter-phase correlation implicitly by conditional probability
distribution. To reduce inference bias and to enhance phase consistency, we
further develop a consistency constraint inference strategy based on
auto-regression. We conduct comprehensive validations on a well-known public
dataset Cholec80. Experimental results show that our method outperforms the
state-of-the-art methods both quantitatively and qualitatively, and achieves an
inference rate of 66 frames per second (fps).",None,-1
135155b2-74a0-45aa-9ad8-a700769bc8d1,Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval,0.578017,"Multi-document summarization (MDS) assumes a set of topic-related documents
are provided as input. In practice, this document set is not always available;
it would need to be retrieved given an information need, i.e. a question or
topic statement, a setting we dub ""open-domain"" MDS. We study this more
challenging setting by formalizing the task and bootstrapping it using existing
datasets, retrievers and summarizers. Via extensive automatic and human
evaluation, we determine: (1) state-of-the-art summarizers suffer large
reductions in performance when applied to open-domain MDS, (2) additional
training in the open-domain setting can reduce this sensitivity to imperfect
retrieval, and (3) summarizers are insensitive to the retrieval of duplicate
documents and the order of retrieved documents, but highly sensitive to other
errors, like the retrieval of irrelevant documents. Based on our results, we
provide practical guidelines to enable future work on open-domain MDS, e.g. how
to choose the number of retrieved documents to summarize. Our results suggest
that new retrieval and summarization methods and annotated resources for
training and evaluation are necessary for further progress in the open-domain
setting.",https://github.com/allenai/open-mds,13874
769e9773-b7ef-4597-aa18-2f5af363700a,Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion,0.80512,"We present a framework for modeling interactional communication in dyadic
conversations: given multimodal inputs of a speaker, we autoregressively output
multiple possibilities of corresponding listener motion. We combine the motion
and speech audio of the speaker using a motion-audio cross attention
transformer. Furthermore, we enable non-deterministic prediction by learning a
discrete latent representation of realistic listener motion with a novel
motion-encoding VQ-VAE. Our method organically captures the multimodal and
non-deterministic nature of nonverbal dyadic interactions. Moreover, it
produces realistic 3D listener facial motion synchronous with the speaker (see
video). We demonstrate that our method outperforms baselines qualitatively and
quantitatively via a rich suite of experiments. To facilitate this line of
research, we introduce a novel and large in-the-wild dataset of dyadic
conversations. Code, data, and videos available at
https://evonneng.github.io/learning2listen/.",None,-1
27b4550d-2af8-4c8c-be57-5b58979e4a47,Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors,0.997949,"The main question this work aims at answering is: ""can morphing attack
detection (MAD) solutions be successfully developed based on synthetic data?"".
Towards that, this work introduces the first synthetic-based MAD development
dataset, namely the Synthetic Morphing Attack Detection Development dataset
(SMDD). This dataset is utilized successfully to train three MAD backbones
where it proved to lead to high MAD performance, even on completely unknown
attack types. Additionally, an essential aspect of this work is the detailed
legal analyses of the challenges of using and sharing real biometric data,
rendering our proposed SMDD dataset extremely essential. The SMDD dataset,
consisting of 30,000 attack and 50,000 bona fide samples, is publicly available
for research purposes.",https://github.com/naserdamer/SMDD-Synthetic-Face-Morphing-Attack-Detection-Development-dataset,4711
80194f22-b9dd-4c7f-8b90-c7343d81d652,Affective Idiosyncratic Responses to Music,0.370868,"Affective responses to music are highly personal. Despite consensus that
idiosyncratic factors play a key role in regulating how listeners emotionally
respond to music, precisely measuring the marginal effects of these variables
has proved challenging. To address this gap, we develop computational methods
to measure affective responses to music from over 403M listener comments on a
Chinese social music platform. Building on studies from music psychology in
systematic and quasi-causal analyses, we test for musical, lyrical, contextual,
demographic, and mental health effects that drive listener affective responses.
Finally, motivated by the social phenomenon known as w\v{a}ng-y\`i-y\'un, we
identify influencing factors of platform user self-disclosures, the social
support they receive, and notable differences in discloser user activity.",https://github.com/skychwang/music-emotions,-1
d017837a-9de4-48e1-bdd6-5b0bfe9e874c,Don't Be So Sure! Boosting ASR Decoding via Confidence Relaxation,0.0923952,"Automatic Speech Recognition (ASR) systems frequently use a search-based
decoding strategy aiming to find the best attainable transcript by considering
multiple candidates. One prominent speech recognition decoding heuristic is
beam search, which seeks the transcript with the greatest likelihood computed
using the predicted distribution. While showing substantial performance gains
in various tasks, beam search loses some of its effectiveness when the
predicted probabilities are highly confident, i.e., the predicted distribution
is massed for a single or very few classes. We show that recently proposed
Self-Supervised Learning (SSL)-based ASR models tend to yield exceptionally
confident predictions that may hamper beam search from truly considering a
diverse set of candidates. We perform a layer analysis to reveal and visualize
how predictions evolve, and propose a decoding procedure that improves the
performance of fine-tuned ASR models. Our proposed approach does not require
further training beyond the original fine-tuning, nor additional model
parameters. In fact, we find that our proposed method requires significantly
less inference computation than current approaches. We propose aggregating the
top M layers, potentially leveraging useful information encoded in intermediate
layers, and relaxing model confidence. We demonstrate the effectiveness of our
approach by conducting an empirical study on varying amounts of labeled
resources and different model sizes, showing consistent improvements in
particular when applied to low-resource scenarios.",None,-1
996ba743-87e1-4c68-85de-e5c343e94cd9,RA-Depth: Resolution Adaptive Self-Supervised Monocular Depth Estimation,0.906333,"Existing self-supervised monocular depth estimation methods can get rid of
expensive annotations and achieve promising results. However, these methods
suffer from severe performance degradation when directly adopting a model
trained on a fixed resolution to evaluate at other different resolutions. In
this paper, we propose a resolution adaptive self-supervised monocular depth
estimation method (RA-Depth) by learning the scale invariance of the scene
depth. Specifically, we propose a simple yet efficient data augmentation method
to generate images with arbitrary scales for the same scene. Then, we develop a
dual high-resolution network that uses the multi-path encoder and decoder with
dense interactions to aggregate multi-scale features for accurate depth
inference. Finally, to explicitly learn the scale invariance of the scene
depth, we formulate a cross-scale depth consistency loss on depth predictions
with different scales. Extensive experiments on the KITTI, Make3D and NYU-V2
datasets demonstrate that RA-Depth not only achieves state-of-the-art
performance, but also exhibits a good ability of resolution adaptation.",https://github.com/hmhemu/RA-Depth,-1
3b528929-ed6e-4d13-8ead-fe87fff92200,On Web-based Visual Corpus Construction for Visual Document Understanding,0.48695,"In recent years, research on visual document understanding (VDU) has grown
significantly, with a particular emphasis on the development of self-supervised
learning methods. However, one of the significant challenges faced in this
field is the limited availability of publicly accessible visual corpora or
extensive collections of images with detailed text annotations, particularly
for non-Latin or resource-scarce languages. To address this challenge, we
propose Web-based Visual Corpus Builder (Webvicob), a dataset generator engine
capable of constructing large-scale, multilingual visual corpora from raw
Wikipedia HTML dumps. Our experiments demonstrate that the data generated by
Webvicob can be used to train robust VDU models that perform well on various
downstream tasks, such as DocVQA and post-OCR parsing. Furthermore, when using
a dataset of 1 million images generated by Webvicob, we observed an improvement
of over 13% on the DocVQA Task 3 compared to a dataset of 11 million images
from the IIT-CDIP. The implementation of our engine is publicly available on
https://github.com/clovaai/webvicob",https://github.com/clovaai/webvicob,5053
886ab7ac-2dcc-4dd3-a951-ae7755820d7b,Learn what matters: cross-domain imitation learning with task-relevant embeddings,0.613829,"We study how an autonomous agent learns to perform a task from demonstrations
in a different domain, such as a different environment or different agent. Such
cross-domain imitation learning is required to, for example, train an
artificial agent from demonstrations of a human expert. We propose a scalable
framework that enables cross-domain imitation learning without access to
additional demonstrations or further domain knowledge. We jointly train the
learner agent's policy and learn a mapping between the learner and expert
domains with adversarial training. We effect this by using a mutual information
criterion to find an embedding of the expert's state space that contains
task-relevant information and is invariant to domain specifics. This step
significantly simplifies estimating the mapping between the learner and expert
domains and hence facilitates end-to-end learning. We demonstrate successful
transfer of policies between considerably different domains, without extra
supervision such as additional demonstrations, and in situations where other
methods fail.",https://github.com/HumanCompatibleAI/seals,-1
4d1ccc47-6576-4f28-974b-2b52529b9fe4,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,0.351071,"Does prompting a large language model (LLM) like GPT-3 with explanations
improve in-context learning? We study this question on two NLP tasks that
involve reasoning over text, namely question answering and natural language
inference. We test the performance of four LLMs on three textual reasoning
datasets using prompts that include explanations in multiple different styles.
For these tasks, we find that including explanations in the prompts for OPT,
GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to
moderate accuracy improvements over standard few-show learning. However,
text-davinci-002 is able to benefit more substantially.
  We further show that explanations generated by the LLMs may not entail the
models' predictions nor be factually grounded in the input, even on simple
tasks with extractive explanations. However, these flawed explanations can
still be useful as a way to verify LLMs' predictions post-hoc. Through analysis
in our three settings, we show that explanations judged by humans to be
good--logically consistent with the input and the prediction--more likely
cooccur with accurate predictions. Following these observations, we train
calibrators using automatically extracted scores that assess the reliability of
explanations, allowing us to improve performance post-hoc across all of our
datasets.",https://github.com/xiye17/TextualExplInContext,-1
79579089-a605-487c-a895-d3c71a8e1a46,Contrastive Representation Learning for Gaze Estimation,0.333111,"Self-supervised learning (SSL) has become prevalent for learning
representations in computer vision. Notably, SSL exploits contrastive learning
to encourage visual representations to be invariant under various image
transformations. The task of gaze estimation, on the other hand, demands not
just invariance to various appearances but also equivariance to the geometric
transformations. In this work, we propose a simple contrastive representation
learning framework for gaze estimation, named Gaze Contrastive Learning
(GazeCLR). GazeCLR exploits multi-view data to promote equivariance and relies
on selected data augmentation techniques that do not alter gaze directions for
invariance learning. Our experiments demonstrate the effectiveness of GazeCLR
for several settings of the gaze estimation task. Particularly, our results
show that GazeCLR improves the performance of cross-domain gaze estimation and
yields as high as 17.2% relative improvement. Moreover, the GazeCLR framework
is competitive with state-of-the-art representation learning methods for
few-shot evaluation. The code and pre-trained models are available at
https://github.com/jswati31/gazeclr.",None,-1
3c510574-9abc-49b8-9cc3-57c67d4ff62c,Multi-Layer Modeling of Dense Vegetation from Aerial LiDAR Scans,0.245636,"The analysis of the multi-layer structure of wild forests is an important
challenge of automated large-scale forestry. While modern aerial LiDARs offer
geometric information across all vegetation layers, most datasets and methods
focus only on the segmentation and reconstruction of the top of canopy. We
release WildForest3D, which consists of 29 study plots and over 2000 individual
trees across 47 000m2 with dense 3D annotation, along with occupancy and height
maps for 3 vegetation layers: ground vegetation, understory, and overstory. We
propose a 3D deep network architecture predicting for the first time both 3D
point-wise labels and high-resolution layer occupancy rasters simultaneously.
This allows us to produce a precise estimation of the thickness of each
vegetation layer as well as the corresponding watertight meshes, therefore
meeting most forestry purposes. Both the dataset and the model are released in
open access: https://github.com/ekalinicheva/multi_layer_vegetation.",https://github.com/ekalinicheva/multi_layer_vegetation,-1
221108f1-234b-404a-bcb1-ea51c6be75e9,Polarimetric Multi-View Inverse Rendering,0.434152,"A polarization camera has great potential for 3D reconstruction since the
angle of polarization (AoP) and the degree of polarization (DoP) of reflected
light are related to an object's surface normal. In this paper, we propose a
novel 3D reconstruction method called Polarimetric Multi-View Inverse Rendering
(Polarimetric MVIR) that effectively exploits geometric, photometric, and
polarimetric cues extracted from input multi-view color-polarization images. We
first estimate camera poses and an initial 3D model by geometric reconstruction
with a standard structure-from-motion and multi-view stereo pipeline. We then
refine the initial model by optimizing photometric rendering errors and
polarimetric errors using multi-view RGB, AoP, and DoP images, where we propose
a novel polarimetric cost function that enables an effective constraint on the
estimated surface normal of each vertex, while considering four possible
ambiguous azimuth angles revealed from the AoP measurement. The weight for the
polarimetric cost is effectively determined based on the DoP measurement, which
is regarded as the reliability of polarimetric information. Experimental
results using both synthetic and real data demonstrate that our Polarimetric
MVIR can reconstruct a detailed 3D shape without assuming a specific surface
material and lighting condition.",None,-1
30687daf-db87-42a7-a961-2ace5481751b,Vulnerability Prioritization: An Offensive Security Approach,0.133851,"Organizations struggle to handle sheer number of vulnerabilities in their
cloud environments. The de facto methodology used for prioritizing
vulnerabilities is to use Common Vulnerability Scoring System (CVSS). However,
CVSS has inherent limitations that makes it not ideal for prioritization. In
this work, we propose a new way of prioritizing vulnerabilities. Our approach
is inspired by how offensive security practitioners perform penetration
testing. We evaluate our approach with a real world case study for a large
client, and the accuracy of machine learning to automate the process end to
end.",None,112
371a912e-9481-40ee-8d9b-6b31f658ce96,Specialized Re-Ranking: A Novel Retrieval-Verification Framework for Cloth Changing Person Re-Identification,0.70915,"Cloth changing person re-identification(Re-ID) can work under more
complicated scenarios with higher security than normal Re-ID and biometric
techniques and is therefore extremely valuable in applications. Meanwhile,
higher flexibility in appearance always leads to more similar-looking confusing
images, which is the weakness of the widely used retrieval methods. In this
work, we shed light on how to handle these similar images. Specifically, we
propose a novel retrieval-verification framework. Given an image, the retrieval
module can search for similar images quickly. Our proposed verification network
will then compare the input image and the candidate images by contrasting those
local details and give a similarity score. An innovative ranking strategy is
also introduced to take a good balance between retrieval and verification
results. Comprehensive experiments are conducted to show the effectiveness of
our framework and its capability in improving the state-of-the-art methods
remarkably on both synthetic and realistic datasets.",None,-1
b3bb9e92-9917-4a14-b0c4-2b1e5efd5ece,An Energy-aware and Fault-tolerant Deep Reinforcement Learning based approach for Multi-agent Patrolling Problems,0.139771,"Autonomous vehicles are suited for continuous area patrolling problems.
However, finding an optimal patrolling strategy can be challenging for many
reasons. Firstly, patrolling environments are often complex and can include
unknown environmental factors, such as wind or landscape. Secondly, autonomous
vehicles can have failures or hardware constraints, such as limited battery
life. Importantly, patrolling large areas often requires multiple agents that
need to collectively coordinate their actions. In this work, we consider these
limitations and propose an approach based on model-free, deep multi-agent
reinforcement learning. In this approach, the agents are trained to patrol an
environment with various unknown dynamics and factors. They can automatically
recharge themselves to support continuous collective patrolling. A distributed
homogeneous multi-agent architecture is proposed, where all patrolling agents
execute identical policies locally based on their local observations and shared
location information. This architecture provides a patrolling system that can
tolerate agent failures and allow supplementary agents to be added to replace
failed agents or to increase the overall patrol performance. The solution is
validated through simulation experiments from multiple perspectives, including
the overall patrol performance, the efficiency of battery recharging
strategies, the overall fault tolerance, and the ability to cooperate with
supplementary agents.",None,-1
7699ffd0-64ca-4274-9525-20642bd60a76,Offline Supervised Learning V.S. Online Direct Policy Optimization: A Comparative Study and A Unified Training Paradigm for Neural Network-Based Optimal Feedback Control,0.510333,"This work is concerned with solving neural network-based feedback controllers
efficiently for optimal control problems. We first conduct a comparative study
of two prevalent approaches: offline supervised learning and online direct
policy optimization. Albeit the training part of the supervised learning
approach is relatively easy, the success of the method heavily depends on the
optimal control dataset generated by open-loop optimal control solvers. In
contrast, direct policy optimization turns the optimal control problem into an
optimization problem directly without any requirement of pre-computing, but the
dynamics-related objective can be hard to optimize when the problem is
complicated. Our results underscore the superiority of offline supervised
learning in terms of both optimality and training time. To overcome the main
challenges, dataset and optimization, in the two approaches respectively, we
complement them and propose the Pre-train and Fine-tune strategy as a unified
training paradigm for optimal feedback control, which further improves the
performance and robustness significantly. Our code is accessible at
https://github.com/yzhao98/DeepOptimalControl.",https://github.com/yzhao98/DeepOptimalControl,-1
1f6089a0-58b2-498a-b2bb-0d2a2e8ffcb4,The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,0.983989,"Reward hacking -- where RL agents exploit gaps in misspecified reward
functions -- has been widely observed, but not yet systematically studied. To
understand how reward hacking arises, we construct four RL environments with
misspecified rewards. We investigate reward hacking as a function of agent
capabilities: model capacity, action space resolution, observation space noise,
and training time. More capable agents often exploit reward misspecifications,
achieving higher proxy reward and lower true reward than less capable agents.
Moreover, we find instances of phase transitions: capability thresholds at
which the agent's behavior qualitatively shifts, leading to a sharp decrease in
the true reward. Such phase transitions pose challenges to monitoring the
safety of ML systems. To address this, we propose an anomaly detection task for
aberrant policies and offer several baseline detectors.",https://github.com/aypan17/reward-misspecification,-1
2d29d91f-6299-4c06-8c83-6e15a700030f,Pyramid Frequency Network with Spatial Attention Residual Refinement Module for Monocular Depth Estimation,0.449916,"Deep-learning-based approaches to depth estimation are rapidly advancing,
offering superior performance over existing methods. To estimate the depth in
real-world scenarios, depth estimation models require the robustness of various
noise environments. In this work, a Pyramid Frequency Network(PFN) with Spatial
Attention Residual Refinement Module(SARRM) is proposed to deal with the weak
robustness of existing deep-learning methods. To reconstruct depth maps with
accurate details, the SARRM constructs a residual fusion method with an
attention mechanism to refine the blur depth. The frequency division strategy
is designed, and the frequency pyramid network is developed to extract features
from multiple frequency bands. With the frequency strategy, PFN achieves better
visual accuracy than state-of-the-art methods in both indoor and outdoor scenes
on Make3D, KITTI depth, and NYUv2 datasets. Additional experiments on the noisy
NYUv2 dataset demonstrate that PFN is more reliable than existing deep-learning
methods in high-noise scenes.",None,-1
04d87ead-1cac-402a-97c4-0d130802f891,Pretrained Domain-Specific Language Model for General Information Retrieval Tasks in the AEC Domain,0.539876,"As an essential task for the architecture, engineering, and construction
(AEC) industry, information retrieval (IR) from unstructured textual data based
on natural language processing (NLP) is gaining increasing attention. Although
various deep learning (DL) models for IR tasks have been investigated in the
AEC domain, it is still unclear how domain corpora and domain-specific
pretrained DL models can improve performance in various IR tasks. To this end,
this work systematically explores the impacts of domain corpora and various
transfer learning techniques on the performance of DL models for IR tasks and
proposes a pretrained domain-specific language model for the AEC domain. First,
both in-domain and close-domain corpora are developed. Then, two types of
pretrained models, including traditional wording embedding models and
BERT-based models, are pretrained based on various domain corpora and transfer
learning strategies. Finally, several widely used DL models for IR tasks are
further trained and tested based on various configurations and pretrained
models. The result shows that domain corpora have opposite effects on
traditional word embedding models for text classification and named entity
recognition tasks but can further improve the performance of BERT-based models
in all tasks. Meanwhile, BERT-based models dramatically outperform traditional
methods in all IR tasks, with maximum improvements of 5.4% and 10.1% in the F1
score, respectively. This research contributes to the body of knowledge in two
ways: 1) demonstrating the advantages of domain corpora and pretrained DL
models and 2) opening the first domain-specific dataset and pretrained language
model for the AEC domain, to the best of our knowledge. Thus, this work sheds
light on the adoption and application of pretrained models in the AEC domain.",None,1701
47ffb4f2-dbcb-40cb-904c-f40e60595b7c,SDF-StyleGAN: Implicit SDF-Based StyleGAN for 3D Shape Generation,0.976614,"We present a StyleGAN2-based deep learning approach for 3D shape generation,
called SDF-StyleGAN, with the aim of reducing visual and geometric
dissimilarity between generated shapes and a shape collection. We extend
StyleGAN2 to 3D generation and utilize the implicit signed distance function
(SDF) as the 3D shape representation, and introduce two novel global and local
shape discriminators that distinguish real and fake SDF values and gradients to
significantly improve shape geometry and visual quality. We further complement
the evaluation metrics of 3D generative models with the shading-image-based
Fr\'echet inception distance (FID) scores to better assess visual quality and
shape distribution of the generated shapes. Experiments on shape generation
demonstrate the superior performance of SDF-StyleGAN over the state-of-the-art.
We further demonstrate the efficacy of SDF-StyleGAN in various tasks based on
GAN inversion, including shape reconstruction, shape completion from partial
point clouds, single-view image-based shape generation, and shape style
editing. Extensive ablation studies justify the efficacy of our framework
design. Our code and trained models are available at
https://github.com/Zhengxinyang/SDF-StyleGAN.",https://github.com/Zhengxinyang/SDF-StyleGAN,-1
0ecec887-f046-427e-bd26-63ff775758c0,Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography,0.513725,"Displacement estimation is a critical step of virtually all Ultrasound
Elastography (USE) techniques. Two main features make this task unique compared
to the general optical flow problem: the high-frequency nature of ultrasound
radio-frequency (RF) data and the governing laws of physics on the displacement
field. Recently, the architecture of the optical flow networks has been
modified to be able to use RF data. Also, semi-supervised and unsupervised
techniques have been employed for USE by considering prior knowledge of
displacement continuity in the form of the first- and second-derivative
regularizers. Despite these attempts, no work has considered the tissue
compression pattern, and displacements in axial and lateral directions have
been assumed to be independent. However, tissue motion pattern is governed by
laws of physics in USE, rendering the axial and the lateral displacements
highly correlated. In this paper, we propose Physically Inspired ConsTraint for
Unsupervised Regularized Elastography (PICTURE), where we impose constraints on
the Poisson's ratio to improve lateral displacement estimates. Experiments on
phantom and in vivo data show that PICTURE substantially improves the quality
of the lateral displacement estimation.",None,-1
d11a5a80-237b-4b37-a681-9754d6dc0c9d,Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild,0.540495,"Talking face generation with great practical significance has attracted more
attention in recent audio-visual studies. How to achieve accurate lip
synchronization is a long-standing challenge to be further investigated.
Motivated by xxx, in this paper, an AttnWav2Lip model is proposed by
incorporating spatial attention module and channel attention module into
lip-syncing strategy. Rather than focusing on the unimportant regions of the
face image, the proposed AttnWav2Lip model is able to pay more attention on the
lip region reconstruction. To our limited knowledge, this is the first attempt
to introduce attention mechanism to the scheme of talking face generation. An
extensive experiments have been conducted to evaluate the effectiveness of the
proposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a
superior performance has been demonstrated on the benchmark lip synthesis
datasets, including LRW, LRS2 and LRS3.",None,-1
1c5e1fd0-8051-4296-8de0-4ce0f6a6a8bb,Design of High-Throughput Mixed-Precision CNN Accelerators on FPGA,0.52301,"Convolutional Neural Networks (CNNs) reach high accuracies in various
application domains, but require large amounts of computation and incur costly
data movements. One method to decrease these costs while trading accuracy is
weight and/or activation word-length reduction. Thereby, layer-wise
mixed-precision quantization allows for more efficient results while inflating
the design space. In this work, we present an in-depth quantitative methodology
to efficiently explore the design space considering the limited hardware
resources of a given FPGA. Our holistic exploration approach vertically
traverses the various design entry levels from the architectural down to the
logic level, and laterally covers optimization from processing elements to
dataflow for an efficient mixed-precision CNN accelerator. Our resulting
hardware accelerators implement truly mixed-precision operations that enable
efficient execution of layer-wise and channel-wise quantized CNNs. Mapping
feed-forward and identity-shortcut-connection mixed-precision CNNs result in
competitive accuracy-throughout trade-offs: 245 frames/s with 87.48% Top-5
accuracy for ResNet-18 and 92.9% Top-5 accuracy with 1.13 TOps/s for
ResNet-152, respectively. Thereby, the required memory footprint for parameters
is reduced by 4.9x and 9.4x compared to the respective floating-point baseline.",None,699
241ca2a6-fb0e-40fa-b293-aec986b5684b,SeqOT: A Spatial-Temporal Transformer Network for Place Recognition Using Sequential LiDAR Data,0.997881,"Place recognition is an important component for autonomous vehicles to
achieve loop closing or global localization. In this paper, we tackle the
problem of place recognition based on sequential 3D LiDAR scans obtained by an
onboard LiDAR sensor. We propose a transformer-based network named SeqOT to
exploit the temporal and spatial information provided by sequential range
images generated from the LiDAR data. It uses multi-scale transformers to
generate a global descriptor for each sequence of LiDAR range images in an
end-to-end fashion. During online operation, our SeqOT finds similar places by
matching such descriptors between the current query sequence and those stored
in the map. We evaluate our approach on four datasets collected with different
types of LiDAR sensors in different environments. The experimental results show
that our method outperforms the state-of-the-art LiDAR-based place recognition
methods and generalizes well across different environments. Furthermore, our
method operates online faster than the frame rate of the sensor. The
implementation of our method is released as open source at:
https://github.com/BIT-MJY/SeqOT.",https://github.com/BIT-MJY/SeqOT,108
84dac465-a679-4e5a-a494-17d1f2a6bb41,Limitations of the NTK for Understanding Generalization in Deep Learning,0.632274,"The ``Neural Tangent Kernel'' (NTK) (Jacot et al 2018), and its empirical
variants have been proposed as a proxy to capture certain behaviors of real
neural networks. In this work, we study NTKs through the lens of scaling laws,
and demonstrate that they fall short of explaining important aspects of neural
network generalization. In particular, we demonstrate realistic settings where
finite-width neural networks have significantly better data scaling exponents
as compared to their corresponding empirical and infinite NTKs at
initialization. This reveals a more fundamental difference between the real
networks and NTKs, beyond just a few percentage points of test accuracy.
Further, we show that even if the empirical NTK is allowed to be pre-trained on
a constant number of samples, the kernel scaling does not catch up to the
neural network scaling. Finally, we show that the empirical NTK continues to
evolve throughout most of the training, in contrast with prior work which
suggests that it stabilizes after a few epochs of training. Altogether, our
work establishes concrete limitations of the NTK approach in understanding
generalization of real networks on natural datasets.",None,-1
5821d329-767d-4828-b16c-39ccee9714e8,Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models,0.280011,"Relations between words are governed by hierarchical structure rather than
linear ordering. Sequence-to-sequence (seq2seq) models, despite their success
in downstream NLP applications, often fail to generalize in a
hierarchy-sensitive manner when performing syntactic transformations - for
example, transforming declarative sentences into questions. However, syntactic
evaluations of seq2seq models have only observed models that were not
pre-trained on natural language data before being trained to perform syntactic
transformations, in spite of the fact that pre-training has been found to
induce hierarchical linguistic generalizations in language models; in other
words, the syntactic capabilities of seq2seq models may have been greatly
understated. We address this gap using the pre-trained seq2seq models T5 and
BART, as well as their multilingual variants mT5 and mBART. We evaluate whether
they generalize hierarchically on two transformations in two languages:
question formation and passivization in English and German. We find that
pre-trained seq2seq models generalize hierarchically when performing syntactic
transformations, whereas models trained from scratch on syntactic
transformations do not. This result presents evidence for the learnability of
hierarchical syntactic information from non-annotated natural language text
while also demonstrating that seq2seq models are capable of syntactic
generalization, though only after exposure to much more language data than
human learners receive.",https://github.com/sebschu/multilingual-transformations,-1
87a379ef-c5da-42c4-8baf-d3e08794ecdf,MMGL: Multi-Scale Multi-View Global-Local Contrastive learning for Semi-supervised Cardiac Image Segmentation,0.509484,"With large-scale well-labeled datasets, deep learning has shown significant
success in medical image segmentation. However, it is challenging to acquire
abundant annotations in clinical practice due to extensive expertise
requirements and costly labeling efforts. Recently, contrastive learning has
shown a strong capacity for visual representation learning on unlabeled data,
achieving impressive performance rivaling supervised learning in many domains.
In this work, we propose a novel multi-scale multi-view global-local
contrastive learning (MMGL) framework to thoroughly explore global and local
features from different scales and views for robust contrastive learning
performance, thereby improving segmentation performance with limited
annotations. Extensive experiments on the MM-WHS dataset demonstrate the
effectiveness of MMGL framework on semi-supervised cardiac image segmentation,
outperforming the state-of-the-art contrastive learning methods by a large
margin.",None,-1
648b08dd-6306-4272-b5f7-3cc6d2259d32,ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping,0.169812,"In this paper, a novel solution is introduced for visual Simultaneous
Localization and Mapping (vSLAM) that is built up of Deep Learning components.
The proposed architecture is a highly modular framework in which each component
offers state of the art results in their respective fields of vision-based deep
learning solutions. The paper shows that with the synergic integration of these
individual building blocks, a functioning and efficient all-through deep neural
(ATDN) vSLAM system can be created. The Embedding Distance Loss function is
introduced and using it the ATDN architecture is trained. The resulting system
managed to achieve 4.4% translation and 0.0176 deg/m rotational error on a
subset of the KITTI dataset. The proposed architecture can be used for
efficient and low-latency autonomous driving (AD) aiding database creation as
well as a basis for autonomous vehicle (AV) control.",None,-1
8be82d13-348d-404a-9063-7bcd4e08b06b,Discovering Nonlinear PDEs from Scarce Data with Physics-encoded Learning,0.364339,"There have been growing interests in leveraging experimental measurements to
discover the underlying partial differential equations (PDEs) that govern
complex physical phenomena. Although past research attempts have achieved great
success in data-driven PDE discovery, the robustness of the existing methods
cannot be guaranteed when dealing with low-quality measurement data. To
overcome this challenge, we propose a novel physics-encoded discrete learning
framework for discovering spatiotemporal PDEs from scarce and noisy data. The
general idea is to (1) firstly introduce a novel deep convolutional-recurrent
network, which can encode prior physics knowledge (e.g., known PDE terms,
assumed PDE structure, initial/boundary conditions, etc.) while remaining
flexible on representation capability, to accurately reconstruct high-fidelity
data, and (2) perform sparse regression with the reconstructed data to identify
the explicit form of the governing PDEs. We validate our method on three
nonlinear PDE systems. The effectiveness and superiority of the proposed method
over baseline models are demonstrated.",None,-1
3b75a105-d79a-47c3-8405-a7fbd3fcffa8,Computational historical linguistics and language diversity in South Asia,0.837177,"South Asia is home to a plethora of languages, many of which severely lack
access to new language technologies. This linguistic diversity also results in
a research environment conducive to the study of comparative, contact, and
historical linguistics -- fields which necessitate the gathering of extensive
data from many languages. We claim that data scatteredness (rather than
scarcity) is the primary obstacle in the development of South Asian language
technology, and suggest that the study of language history is uniquely aligned
with surmounting this obstacle. We review recent developments in and at the
intersection of South Asian NLP and historical-comparative linguistics,
describing our and others' current efforts in this area. We also offer new
strategies towards breaking the data barrier.",None,-1
0551bcd2-aa86-4d9d-9370-5d53c5dee033,UC-OWOD: Unknown-Classified Open World Object Detection,0.879053,"Open World Object Detection (OWOD) is a challenging computer vision problem
that requires detecting unknown objects and gradually learning the identified
unknown classes. However, it cannot distinguish unknown instances as multiple
unknown classes. In this work, we propose a novel OWOD problem called
Unknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to
detect unknown instances and classify them into different unknown classes.
Besides, we formulate the problem and devise a two-stage object detector to
solve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative
classification head are used to detect known and unknown objects. Then,
similarity-based unknown classification and unknown clustering refinement
modules are constructed to distinguish multiple unknown classes. Moreover, two
novel evaluation protocols are designed to evaluate unknown-class detection.
Abundant experiments and visualizations prove the effectiveness of the proposed
method. Code is available at https://github.com/JohnWuzh/UC-OWOD.",https://github.com/JohnWuzh/UC-OWOD,11164
73b77607-2e69-42c0-8e70-295a3707bbf1,A General Framework for Modelling Conditional Reasoning -- Preliminary Report,0.0295116,"We introduce and investigate here a formalisation for conditionals that
allows the definition of a broad class of reasoning systems. This framework
covers the most popular kinds of conditional reasoning in logic-based KR: the
semantics we propose is appropriate for a structural analysis of those
conditionals that do not satisfy closure properties associated to classical
logics.",None,-1
6046fbd8-0219-4832-9afb-0aeff2582a1e,MLP-Hash: Protecting Face Templates via Hashing of Randomized Multi-Layer Perceptron,0.396402,"Applications of face recognition systems for authentication purposes are
growing rapidly. Although state-of-the-art (SOTA) face recognition systems have
high recognition accuracy, the features which are extracted for each user and
are stored in the system's database contain privacy-sensitive information.
Accordingly, compromising this data would jeopardize users' privacy. In this
paper, we propose a new cancelable template protection method, dubbed MLP-hash,
which generates protected templates by passing the extracted features through a
user-specific randomly-weighted multi-layer perceptron (MLP) and binarizing the
MLP output. We evaluated the unlinkability, irreversibility, and recognition
accuracy of our proposed biometric template protection method to fulfill the
ISO/IEC 30136 standard requirements. Our experiments with SOTA face recognition
systems on the MOBIO and LFW datasets show that our method has competitive
performance with the BioHashing and IoM Hashing (IoM-GRP and IoM-URP) template
protection algorithms. We provide an open-source implementation of all the
experiments presented in this paper so that other researchers can verify our
findings and build upon our work.",https://gitlab.idiap.ch/bob/bob.bio.face,-1
23fe0311-87e0-484b-852e-582d90febbb7,Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model,0.53828,"Contextual ASR, which takes a list of bias terms as input along with audio,
has drawn recent interest as ASR use becomes more widespread. We are releasing
contextual biasing lists to accompany the Earnings21 dataset, creating a public
benchmark for this task. We present baseline results on this benchmark using a
pretrained end-to-end ASR model from the WeNet toolkit. We show results for
shallow fusion contextual biasing applied to two different decoding algorithms.
Our baseline results confirm observations that end-to-end models struggle in
particular with words that are rarely or never seen during training, and that
existing shallow fusion techniques do not adequately address this problem. We
propose an alternate spelling prediction model that improves recall of rare
words by 34.7% relative and of out-of-vocabulary words by 97.2% relative,
compared to contextual biasing without alternate spellings. This model is
conceptually similar to ones used in prior work, but is simpler to implement as
it does not rely on either a pronunciation dictionary or an existing
text-to-speech system.",https://github.com/revdotcom/speech-datasets/tree/main/earnings21,143
1a7a4a11-955a-4a76-8bed-fcc083699044,How to Fine-Tune Vision Models with SGD,0.414509,"SGD and AdamW are the two most used optimizers for fine-tuning large neural
networks in computer vision. When the two methods perform the same, SGD is
preferable because it uses less memory (12 bytes/parameter with momentum and 8
bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite
of downstream tasks, especially those with distribution shifts, we find that
fine-tuning with AdamW performs substantially better than SGD on modern Vision
Transformer and ConvNeXt models. We find that large gaps in performance between
SGD and AdamW occur when the fine-tuning gradients in the first ""embedding""
layer are much larger than in the rest of the model. Our analysis suggests an
easy fix that works consistently across datasets and models: freezing the
embedding layer (less than 1% of the parameters) leads to SGD with or without
momentum performing slightly better than AdamW while using less memory (e.g.,
on ViT-L, SGD uses 33% less GPU memory). Our insights result in
state-of-the-art accuracies on five popular distribution shift benchmarks:
WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.",None,4318
31145420-1ff6-43de-ac0c-ab3ed37a2550,"Knowledge Representation for Conceptual, Motivational, and Affective Processes in Natural Language Communication",0.177579,"Natural language communication is an intricate and complex process. The
speaker usually begins with an intention and motivation of what is to be
communicated, and what effects are expected from the communication, while
taking into consideration the listener's mental model to concoct an appropriate
sentence. The listener likewise has to interpret what the speaker means, and
respond accordingly, also with the speaker's mental state in mind. To do this
successfully, conceptual, motivational, and affective processes have to be
represented appropriately to drive the language generation and understanding
processes. Language processing has succeeded well with the big data approach in
applications such as chatbots and machine translation. However, in human-robot
collaborative social communication and in using natural language for delivering
precise instructions to robots, a deeper representation of the conceptual,
motivational, and affective processes is needed. This paper capitalizes on the
UGALRS (Unified General Autonomous and Language Reasoning System) framework and
the CD+ (Conceptual Representation Plus) representational scheme to illustrate
how social communication through language is supported by a knowledge
representational scheme that handles conceptual, motivational, and affective
processes in a deep and general way. Though a small set of concepts,
motivations, and emotions is treated in this paper, its main contribution is in
articulating a general framework of knowledge representation and processing to
link these aspects together in serving the purpose of natural language
communication for an intelligent system.",None,-1
7ffe6fd7-3487-4ca6-b445-25dcc2437814,DeepPrivacy2: Towards Realistic Full-Body Anonymization,0.882599,"Generative Adversarial Networks (GANs) are widely adapted for anonymization
of human figures. However, current state-of-the-art limit anonymization to the
task of face anonymization. In this paper, we propose a novel anonymization
framework (DeepPrivacy2) for realistic anonymization of human figures and
faces. We introduce a new large and diverse dataset for human figure synthesis,
which significantly improves image quality and diversity of generated images.
Furthermore, we propose a style-based GAN that produces high quality, diverse
and editable anonymizations. We demonstrate that our full-body anonymization
framework provides stronger privacy guarantees than previously proposed
methods.",https://github.com/hukkelas/deep-privacy2,-1
0ab51e42-9af2-4c81-ae32-1f9771c1a248,How Powerful are Spectral Graph Neural Networks,0.987573,"Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based
on graph signal filters. Some models able to learn arbitrary spectral filters
have emerged recently. However, few works analyze the expressive power of
spectral GNNs. This paper studies spectral GNNs' expressive power
theoretically. We first prove that even spectral GNNs without nonlinearity can
produce arbitrary graph signals and give two conditions for reaching
universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2)
no missing frequency components in node features. We also establish a
connection between the expressive power of spectral GNNs and Graph Isomorphism
(GI) testing, the latter of which is often used to characterize spatial GNNs'
expressive power. Moreover, we study the difference in empirical performance
among different spectral GNNs with the same expressive power from an
optimization perspective, and motivate the use of an orthogonal basis whose
weight function corresponds to the graph signal density in the spectrum.
Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to
its orthogonality and flexibility to adapt to a wide range of weight functions.
JacobiConv deserts nonlinearity while outperforming all baselines on both
synthetic and real-world datasets.",https://github.com/GraphPKU/JacobiConv,-1
fcebc4a8-a20b-4fa8-a0bc-80dd48519bf2,FastRE: Towards Fast Relation Extraction with Convolutional Encoder and Improved Cascade Binary Tagging Framework,0.297536,"Recent work for extracting relations from texts has achieved excellent
performance. However, most existing methods pay less attention to the
efficiency, making it still challenging to quickly extract relations from
massive or streaming text data in realistic scenarios. The main efficiency
bottleneck is that these methods use a Transformer-based pre-trained language
model for encoding, which heavily affects the training speed and inference
speed. To address this issue, we propose a fast relation extraction model
(FastRE) based on convolutional encoder and improved cascade binary tagging
framework. Compared to previous work, FastRE employs several innovations to
improve efficiency while also keeping promising performance. Concretely, FastRE
adopts a novel convolutional encoder architecture combined with dilated
convolution, gated unit and residual connection, which significantly reduces
the computation cost of training and inference, while maintaining the
satisfactory performance. Moreover, to improve the cascade binary tagging
framework, FastRE first introduces a type-relation mapping mechanism to
accelerate tagging efficiency and alleviate relation redundancy, and then
utilizes a position-dependent adaptive thresholding strategy to obtain higher
tagging accuracy and better model generalization. Experimental results
demonstrate that FastRE is well balanced between efficiency and performance,
and achieves 3-10x training speed, 7-15x inference speed faster, and 1/100
parameters compared to the state-of-the-art models, while the performance is
still competitive.",https://github.com/seukgcode/FastRE,-1
84df5505-bb87-408e-b79e-99d7ac92bf40,Generative Multiplane Images: Making a 2D GAN 3D-Aware,0.814304,"What is really needed to make an existing 2D GAN 3D-aware? To answer this
question, we modify a classical GAN, i.e., StyleGANv2, as little as possible.
We find that only two modifications are absolutely necessary: 1) a multiplane
image style generator branch which produces a set of alpha maps conditioned on
their depth; 2) a pose-conditioned discriminator. We refer to the generated
output as a 'generative multiplane image' (GMPI) and emphasize that its
renderings are not only high-quality but also guaranteed to be view-consistent,
which makes GMPIs different from many prior works. Importantly, the number of
alpha maps can be dynamically adjusted and can differ between training and
inference, alleviating memory concerns and enabling fast training of GMPIs in
less than half a day at a resolution of $1024^2$. Our findings are consistent
across three challenging and common high-resolution datasets, including FFHQ,
AFHQv2, and MetFaces.",https://github.com/apple/ml-gmpi,-1
78185b44-30d2-42ec-bbf4-4425b6c39985,PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model,0.647594,"In this paper, we introduce PanGu-Bot, a Chinese pre-trained open-domain
dialogue generation model based on a large pre-trained language model (PLM)
PANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue
models trained over a massive amount of dialogue data from scratch, we aim to
build a powerful dialogue model with relatively fewer data and computation
costs by inheriting valuable language capabilities and knowledge from PLMs. To
this end, we train PanGu-Bot from the large PLM PANGU-alpha, which has been
proven well-performed on a variety of Chinese natural language tasks. We
investigate different aspects of responses generated by PanGu-Bot, including
response quality, knowledge, and safety. We show that PanGu-Bot outperforms
state-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA
(Zhou et al., 2021), EVA2.0 (Gu et al., 2022)) w.r.t. the above three aspects.
We also demonstrate that PanGu-Bot can be easily deployed to generate emotional
responses without further training. Throughout our empirical analysis, we also
point out that the PanGu-Bot response quality, knowledge correctness, and
safety are still far from perfect, and further explorations are indispensable
to building reliable and smart dialogue systems. Our model and code will be
available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot
soon.",https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot,-1
7efab64b-17c8-4be3-894c-83b9896a3df2,On the focusing of thermal images,0.426022,"In this paper we present a new thermographic image database suitable for the
analysis of automatic focus measures. This database consists of 8 different
sets of scenes, where each scene contains one image for 96 different focus
positions. Using this database we evaluate the usefulness of six focus measures
with the goal to determine the optimal focus position. Experimental results
reveal that an accurate automatic detection of optimal focus position is
possible, even with a low computational burden. We also present an acquisition
tool able to help the acquisition of thermal images. To the best of our
knowledge, this is the first study about automatic focus of thermal images.",None,-1
c7075f76-0387-4d7c-8274-0eb45ac9d736,Generating Executable Action Plans with Environmentally-Aware Language Models,0.905127,"Large Language Models (LLMs) trained using massive text datasets have
recently shown promise in generating action plans for robotic agents from high
level text queries. However, these models typically do not consider the robot's
environment, resulting in generated plans that may not actually be executable,
due to ambiguities in the planned actions or environmental constraints. In this
paper, we propose an approach to generate environmentally-aware action plans
that agents are better able to execute. Our approach involves integrating
environmental objects and object relations as additional inputs into LLM action
plan generation to provide the system with an awareness of its surroundings,
resulting in plans where each generated action is mapped to objects present in
the scene. We also design a novel scoring function that, along with generating
the action steps and associating them with objects, helps the system
disambiguate among object instances and take into account their states. We
evaluated our approach using the VirtualHome simulator and the ActivityPrograms
knowledge base and found that action plans generated from our system had a 310%
improvement in executability and a 147% improvement in correctness over prior
work. The complete code and a demo of our method is publicly available at
https://github.com/hri-ironlab/scene_aware_language_planner.",https://github.com/hri-ironlab/scene_aware_language_planner,-1
6553def9-5034-45fb-8372-8925eb3b8151,How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning,0.832809,"To avoid collapse in self-supervised learning (SSL), a contrastive loss is
widely used but often requires a large number of negative samples. Without
negative samples yet achieving competitive performance, a recent work has
attracted significant attention for providing a minimalist simple Siamese
(SimSiam) method to avoid collapse. However, the reason for how it avoids
collapse without negative samples remains not fully clear and our investigation
starts by revisiting the explanatory claims in the original SimSiam. After
refuting their claims, we introduce vector decomposition for analyzing the
collapse based on the gradient analysis of the $l_2$-normalized representation
vector. This yields a unified perspective on how negative samples and SimSiam
alleviate collapse. Such a unified perspective comes timely for understanding
the recent progress in SSL.",None,-1
57a7912f-5fe4-4085-960d-66a851c12eef,Gradient-Guided Importance Sampling for Learning Binary Energy-Based Models,0.864665,"Learning energy-based models (EBMs) is known to be difficult especially on
discrete data where gradient-based learning strategies cannot be applied
directly. Although ratio matching is a sound method to learn discrete EBMs, it
suffers from expensive computation and excessive memory requirements, thereby
resulting in difficulties in learning EBMs on high-dimensional data. Motivated
by these limitations, in this study, we propose ratio matching with
gradient-guided importance sampling (RMwGGIS). Particularly, we use the
gradient of the energy function w.r.t. the discrete data space to approximately
construct the provably optimal proposal distribution, which is subsequently
used by importance sampling to efficiently estimate the original ratio matching
objective. We perform experiments on density modeling over synthetic discrete
data, graph generation, and training Ising models to evaluate our proposed
method. The experimental results demonstrate that our method can significantly
alleviate the limitations of ratio matching, perform more effectively in
practice, and scale to high-dimensional problems. Our implementation is
available at https://github.com/divelab/RMwGGIS.",https://github.com/divelab/RMwGGIS,-1
48e2481f-ff52-4c83-8bf2-88b965db5749,What do tokens know about their characters and how do they know it?,0.873198,"Pre-trained language models (PLMs) that use subword tokenization schemes can
succeed at a variety of language tasks that require character-level
information, despite lacking explicit access to the character composition of
tokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe),
we probe what word pieces encode about character-level information by training
classifiers to predict the presence or absence of a particular alphabetical
character in a token, based on its embedding (e.g., probing whether the model
embedding for ""cat"" encodes that it contains the character ""a""). We find that
these models robustly encode character-level information and, in general,
larger models perform better at the task. We show that these results generalize
to characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic).
Then, through a series of experiments and analyses, we investigate the
mechanisms through which PLMs acquire English-language character information
during training and argue that this knowledge is acquired through multiple
phenomena, including a systematic relationship between particular characters
and particular parts of speech, as well as natural variability in the
tokenization of related strings.",https://github.com/ayushk4/character-probing-pytorch,-1
51c76dc8-15a9-4f90-9f20-bb53d8a7c171,Assessment of Massively Multilingual Sentiment Classifiers,0.0522617,"Models are increasing in size and complexity in the hunt for SOTA. But what
if those 2\% increase in performance does not make a difference in a production
use case? Maybe benefits from a smaller, faster model outweigh those slight
performance gains. Also, equally good performance across languages in
multilingual tasks is more important than SOTA results on a single one. We
present the biggest, unified, multilingual collection of sentiment analysis
datasets. We use these to assess 11 models and 80 high-quality sentiment
datasets (out of 342 raw datasets collected) in 27 languages and included
results on the internally annotated datasets. We deeply evaluate multiple
setups, including fine-tuning transformer-based models for measuring
performance. We compare results in numerous dimensions addressing the imbalance
in both languages coverage and dataset sizes. Finally, we present some best
practices for working with such a massive collection of datasets and models
from a multilingual perspective.",None,-1
844e19c2-e347-4ec2-bd46-9f00c239759b,Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding,0.69234,"Inspired by the impressive performance of recent face image editing methods,
several studies have been naturally proposed to extend these methods to the
face video editing task. One of the main challenges here is temporal
consistency among edited frames, which is still unresolved. To this end, we
propose a novel face video editing framework based on diffusion autoencoders
that can successfully extract the decomposed features - for the first time as a
face video editing model - of identity and motion from a given video. This
modeling allows us to edit the video by simply manipulating the temporally
invariant feature to the desired direction for the consistency. Another unique
strength of our model is that, since our model is based on diffusion models, it
can satisfy both reconstruction and edit capabilities at the same time, and is
robust to corner cases in wild face videos (e.g. occluded faces) unlike the
existing GAN-based methods.",https://diff-video-ae.github.io,-1
cf654840-8e8c-40d9-81c4-43d4d145cb4e,Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation,0.896592,"Personalized dialogue systems explore the problem of generating responses
that are consistent with the user's personality, which has raised much
attention in recent years. Existing personalized dialogue systems have tried to
extract user profiles from dialogue history to guide personalized response
generation. Since the dialogue history is usually long and noisy, most existing
methods truncate the dialogue history to model the user's personality. Such
methods can generate some personalized responses, but a large part of dialogue
history is wasted, leading to sub-optimal performance of personalized response
generation. In this work, we propose to refine the user dialogue history on a
large scale, based on which we can handle more dialogue history and obtain more
abundant and accurate persona information. Specifically, we design an MSP model
which consists of three personal information refiners and a personalized
response generator. With these multi-level refiners, we can sparsely extract
the most valuable information (tokens) from the dialogue history and leverage
other similar users' data to enhance personalization. Experimental results on
two real-world datasets demonstrate the superiority of our model in generating
more informative and personalized responses.",https://github.com/bangbangbang12315/MSP/tree/release,-1
0205ad22-7323-4cab-9513-bb809623a0ad,Learning Visual Explanations for DCNN-Based Image Classifiers Using an Attention Mechanism,0.438454,"In this paper two new learning-based eXplainable AI (XAI) methods for deep
convolutional neural network (DCNN) image classifiers, called L-CAM-Fm and
L-CAM-Img, are proposed. Both methods use an attention mechanism that is
inserted in the original (frozen) DCNN and is trained to derive class
activation maps (CAMs) from the last convolutional layer's feature maps. During
training, CAMs are applied to the feature maps (L-CAM-Fm) or the input image
(L-CAM-Img) forcing the attention mechanism to learn the image regions
explaining the DCNN's outcome. Experimental evaluation on ImageNet shows that
the proposed methods achieve competitive results while requiring a single
forward pass at the inference stage. Moreover, based on the derived
explanations a comprehensive qualitative analysis is performed providing
valuable insight for understanding the reasons behind classification errors,
including possible dataset biases affecting the trained classifier.",https://github.com/eclique/RISE,6033
eca89768-9ebc-4e9a-a6fd-731a7199cdfa,On the Decentralization of Blockchain-enabled Asynchronous Federated Learning,0.279853,"Federated learning (FL), thanks in part to the emergence of the edge
computing paradigm, is expected to enable true real-time applications in
production environments. However, its original dependence on a central server
for orchestration raises several concerns in terms of security, privacy, and
scalability. To solve some of these worries, blockchain technology is expected
to bring decentralization, robustness, and enhanced trust to FL. The
empowerment of FL through blockchain (also referred to as FLchain), however,
has some implications in terms of ledger inconsistencies and age of information
(AoI), which are naturally inherited from the blockchain's fully decentralized
operation. Such issues stem from the fact that, given the temporary ledger
versions in the blockchain, FL devices may use different models for training,
and that, given the asynchronicity of the FL operation, stale local updates
(computed using outdated models) may be generated. In this paper, we shed light
on the implications of the FLchain setting and study the effect that both the
AoI and ledger inconsistencies have on the FL performance. To that end, we
provide a faithful simulation tool that allows capturing the decentralized and
asynchronous nature of the FLchain operation.",https://gitlab.cttc.es/supercom/blockFLsim/-/tree/BlockFLsim,-1
67d21991-c4b5-46ac-a714-308442be0ec2,Meta-Learning Sparse Compression Networks,0.604255,"Recent work in Deep Learning has re-imagined the representation of data as
functions mapping from a coordinate space to an underlying continuous signal.
When such functions are approximated by neural networks this introduces a
compelling alternative to the more common multi-dimensional array
representation. Recent work on such Implicit Neural Representations (INRs) has
shown that - following careful architecture search - INRs can outperform
established compression methods such as JPEG (e.g. Dupont et al., 2021). In
this paper, we propose crucial steps towards making such ideas scalable:
Firstly, we employ state-of-the-art network sparsification techniques to
drastically improve compression. Secondly, introduce the first method allowing
for sparsification to be employed in the inner-loop of commonly used
Meta-Learning algorithms, drastically improving both compression and the
computational cost of learning INRs. The generality of this formalism allows us
to present results on diverse data modalities such as images, manifolds, signed
distance functions, 3D shapes and scenes, several of which establish new
state-of-the-art results.",None,-1
b829d694-5d1d-47ba-a46c-6163037ac961,Entity Linking in Tabular Data Needs the Right Attention,0.151271,"Understanding the semantic meaning of tabular data requires Entity Linking
(EL), in order to associate each cell value to a real-world entity in a
Knowledge Base (KB). In this work, we focus on end-to-end solutions for EL on
tabular data that do not rely on fact lookup in the target KB. Tabular data
contains heterogeneous and sparse context, including column headers, cell
values and table captions. We experiment with various models to generate a
vector representation for each cell value to be linked. Our results show that
it is critical to apply an attention mechanism as well as an attention mask, so
that the model can only attend to the most relevant context and avoid
information dilution. The most relevant context includes: same-row cells,
same-column cells, headers and caption. Computational complexity, however,
grows quadratically with the size of tabular data for such a complex model. We
achieve constant memory usage by introducing a Tabular Entity Linking Lite
model (TELL ) that generates vector representation for a cell based only on its
value, the table headers and the table caption. TELL achieves 80.8% accuracy on
Wikipedia tables, which is only 0.1% lower than the state-of-the-art model with
quadratic memory usage.",https://github.com/jcklie/wikimapper,-1
d26346b9-2cf8-4996-af1b-5e7f7e93bb8c,Modeling Dual Read/Write Paths for Simultaneous Machine Translation,0.689124,"Simultaneous machine translation (SiMT) outputs translation while reading
source sentence and hence requires a policy to decide whether to wait for the
next source word (READ) or generate a target word (WRITE), the actions of which
form a read/write path. Although the read/write path is essential to SiMT
performance, no direct supervision is given to the path in the existing
methods. In this paper, we propose a method of dual-path SiMT which introduces
duality constraints to direct the read/write path. According to duality
constraints, the read/write path in source-to-target and target-to-source SiMT
models can be mapped to each other. As a result, the two SiMT models can be
optimized jointly by forcing their read/write paths to satisfy the mapping.
Experiments on En-Vi and De-En tasks show that our method can outperform strong
baselines under all latency.",https://github.com/ictnlp/,-1
3833e505-303d-43a1-88af-21a111256340,A Comparative Study of Graph Neural Networks for Shape Classification in Neuroimaging,0.0849553,"Graph neural networks have emerged as a promising approach for the analysis
of non-Euclidean data such as meshes. In medical imaging, mesh-like data plays
an important role for modelling anatomical structures, and shape classification
can be used in computer aided diagnosis and disease detection. However, with a
plethora of options, the best architectural choices for medical shape analysis
using GNNs remain unclear. We conduct a comparative analysis to provide
practitioners with an overview of the current state-of-the-art in geometric
deep learning for shape classification in neuroimaging. Using biological sex
classification as a proof-of-concept task, we find that using FPFH as node
features substantially improves GNN performance and generalisation to
out-of-distribution data; we compare the performance of three alternative
convolutional layers; and we reinforce the importance of data augmentation for
graph based learning. We then confirm these results hold for a clinically
relevant task, using the classification of Alzheimer's disease.",https://github.com/biomedia-mira/medmesh,41086
9b83deb0-8fbe-43a0-a937-aa95f8e8aa91,Lifelong Bandit Optimization: No Prior and No Regret,0.488216,"Machine learning algorithms are often repeatedly applied to problems with
similar structure over and over again. We focus on solving a sequence of bandit
optimization tasks and develop LIBO, an algorithm which adapts to the
environment by learning from past experience and becomes more sample-efficient
in the process. We assume a kernelized structure where the kernel is unknown
but shared across all tasks. LIBO sequentially meta-learns a kernel that
approximates the true kernel and solves the incoming tasks with the latest
kernel estimate. Our algorithm can be paired with any kernelized or linear
bandit algorithm and guarantees oracle optimal performance, meaning that as
more tasks are solved, the regret of LIBO on each task converges to the regret
of the bandit algorithm with oracle knowledge of the true kernel. Naturally, if
paired with a sublinear bandit algorithm, LIBO yields a sublinear lifelong
regret. We also show that direct access to the data from each task is not
necessary for attaining sublinear regret. We propose F-LIBO, which solves the
lifelong problem in a federated manner.",None,-1
5e9a6c65-76fd-44c7-b4d4-d7cf3d6b23df,Real Time Egocentric Segmentation for Video-self Avatar in Mixed Reality,0.6145,"In this work we present our real-time egocentric body segmentation algorithm.
Our algorithm achieves a frame rate of 66 fps for an input resolution of
640x480, thanks to our shallow network inspired in Thundernet's architecture.
Besides, we put a strong emphasis on the variability of the training data. More
concretely, we describe the creation process of our Egocentric Bodies
(EgoBodies) dataset, composed of almost 10,000 images from three datasets,
created both from synthetic methods and real capturing. We conduct experiments
to understand the contribution of the individual datasets; compare Thundernet
model trained with EgoBodies with simpler and more complex previous approaches
and discuss their corresponding performance in a real-life setup in terms of
segmentation quality and inference times. The described trained semantic
segmentation algorithm is already integrated in an end-to-end system for Mixed
Reality (MR), making it possible for users to see his/her own body while being
immersed in a MR scene.",None,-1
1552e543-087e-4d94-ace1-0d4a720e8a51,Dense Prediction Transformer for Scale Estimation in Monocular Visual Odometry,0.142444,"Monocular visual odometry consists of the estimation of the position of an
agent through images of a single camera, and it is applied in autonomous
vehicles, medical robots, and augmented reality. However, monocular systems
suffer from the scale ambiguity problem due to the lack of depth information in
2D frames. This paper contributes by showing an application of the dense
prediction transformer model for scale estimation in monocular visual odometry
systems. Experimental results show that the scale drift problem of monocular
systems can be reduced through the accurate estimation of the depth map by this
model, achieving competitive state-of-the-art performance on a visual odometry
benchmark.",https://github.com/isl-org/DPT,-1
3cc3d539-518c-4c36-aec7-d2c0f7ea41d5,ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection,0.977506,"Existing approaches for unsupervised point cloud pre-training are constrained
to either scene-level or point/voxel-level instance discrimination. Scene-level
methods tend to lose local details that are crucial for recognizing the road
objects, while point/voxel-level methods inherently suffer from limited
receptive field that is incapable of perceiving large objects or context
environments. Considering region-level representations are more suitable for 3D
object detection, we devise a new unsupervised point cloud pre-training
framework, called ProposalContrast, that learns robust 3D representations by
contrasting region proposals. Specifically, with an exhaustive set of region
proposals sampled from each point cloud, geometric point relations within each
proposal are modeled for creating expressive proposal representations. To
better accommodate 3D detection properties, ProposalContrast optimizes with
both inter-cluster and inter-proposal separation, i.e., sharpening the
discriminativeness of proposal representations across semantic classes and
object instances. The generalizability and transferability of ProposalContrast
are verified on various 3D detectors (i.e., PV-RCNN, CenterPoint, PointPillars
and PointRCNN) and datasets (i.e., KITTI, Waymo and ONCE).",https://github.com/yinjunbo/ProposalContrast,-1
42177f69-5ec8-4c25-81c9-8365df475146,Understanding Zero-Shot Adversarial Robustness for Large-Scale Models,0.731581,"Pretrained large-scale vision-language models like CLIP have exhibited strong
generalization over unseen tasks. Yet imperceptible adversarial perturbations
can significantly reduce CLIP's performance on new tasks. In this work, we
identify and explore the problem of \emph{adapting large-scale models for
zero-shot adversarial robustness}. We first identify two key factors during
model adaption -- training losses and adaptation methods -- that affect the
model's zero-shot adversarial robustness. We then propose a text-guided
contrastive adversarial training loss, which aligns the text embeddings and the
adversarial visual features with contrastive learning on a small set of
training data. We apply this training loss to two adaption methods, model
finetuning and visual prompt tuning. We find that visual prompt tuning is more
effective in the absence of texts, while finetuning wins in the existence of
text guidance. Overall, our approach significantly improves the zero-shot
adversarial robustness over CLIP, seeing an average improvement of over 31
points over ImageNet and 15 zero-shot datasets. We hope this work can shed
light on understanding the zero-shot adversarial robustness of large-scale
models.",https://github.com/cvlab-columbia/ZSRobust4FoundationModel,-1
35aa6837-bf60-4559-bc69-49a97c93bb9b,Emotion Analysis using Multi-Layered Networks for Graphical Representation of Tweets,0.527191,"Anticipating audience reaction towards a certain piece of text is integral to
several facets of society ranging from politics, research, and commercial
industries. Sentiment analysis (SA) is a useful natural language processing
(NLP) technique that utilizes both lexical/statistical and deep learning
methods to determine whether different sized texts exhibit a positive,
negative, or neutral emotion. However, there is currently a lack of tools that
can be used to analyse groups of independent texts and extract the primary
emotion from the whole set. Therefore, the current paper proposes a novel
algorithm referred to as the Multi-Layered Tweet Analyzer (MLTA) that
graphically models social media text using multi-layered networks (MLNs) in
order to better encode relationships across independent sets of tweets. Graph
structures are capable of capturing meaningful relationships in complex
ecosystems compared to other representation methods. State of the art Graph
Neural Networks (GNNs) are used to extract information from the Tweet-MLN and
make predictions based on the extracted graph features. Results show that not
only does the MLTA predict from a larger set of possible emotions, delivering a
more accurate sentiment compared to the standard positive, negative or neutral,
it also allows for accurate group-level predictions of Twitter data.",None,-1
3fcff545-8435-403c-80ff-e7ab761c35e5,Depth-aware Neural Style Transfer using Instance Normalization,0.454876,"Neural Style Transfer (NST) is concerned with the artistic stylization of
visual media. It can be described as the process of transferring the style of
an artistic image onto an ordinary photograph. Recently, a number of studies
have considered the enhancement of the depth-preserving capabilities of the NST
algorithms to address the undesired effects that occur when the input content
images include numerous objects at various depths. Our approach uses a deep
residual convolutional network with instance normalization layers that utilizes
an advanced depth prediction network to integrate depth preservation as an
additional loss function to content and style. We demonstrate results that are
effective in retaining the depth and global structure of content images. Three
different evaluation processes show that our system is capable of preserving
the structure of the stylized results while exhibiting style-capture
capabilities and aesthetic qualities comparable or superior to state-of-the-art
methods. Project page:
https://ioannoue.github.io/depth-aware-nst-using-in.html.",https://ioannoue.github.io/depth-aware-nst-using-in.html,97
ef002ab3-db60-4218-9d9c-c35a4b5de99e,Uncertainty estimation for Cross-dataset performance in Trajectory prediction,0.633854,"While a lot of work has been carried on developing trajectory prediction
methods, and various datasets have been proposed for benchmarking this task,
little study has been done so far on the generalizability and the
transferability of these methods across dataset. In this paper, we observe the
performance of two of the latest state-of-the-art trajectory prediction methods
across four different datasets (Argoverse, NuScenes, Interaction, Shifts). This
analysis allows to gain some insights on the generalizability proprieties of
most recent trajectory prediction models and to analyze which dataset is more
representative of real driving scenes and therefore enables better
transferability. Furthermore we present a novel method to estimate prediction
uncertainty and show how it could be used to achieve better performance across
datasets.",None,-1
848828e0-60e2-4525-add5-c4af75893609,Panoptic-PHNet: Towards Real-Time and High-Precision LiDAR Panoptic Segmentation via Clustering Pseudo Heatmap,0.897188,"As a rising task, panoptic segmentation is faced with challenges in both
semantic segmentation and instance segmentation. However, in terms of speed and
accuracy, existing LiDAR methods in the field are still limited. In this paper,
we propose a fast and high-performance LiDAR-based framework, referred to as
Panoptic-PHNet, with three attractive aspects: 1) We introduce a clustering
pseudo heatmap as a new paradigm, which, followed by a center grouping module,
yields instance centers for efficient clustering without object-level learning
tasks. 2) A knn-transformer module is proposed to model the interaction among
foreground points for accurate offset regression. 3) For backbone design, we
fuse the fine-grained voxel features and the 2D Bird's Eye View (BEV) features
with different receptive fields to utilize both detailed and global
information. Extensive experiments on both SemanticKITTI dataset and nuScenes
dataset show that our Panoptic-PHNet surpasses state-of-the-art methods by
remarkable margins with a real-time speed. We achieve the 1st place on the
public leaderboard of SemanticKITTI and leading performance on the recently
released leaderboard of nuScenes.",None,-1
a0abac83-c20f-4704-bb52-932fb441761e,Scaling Up Probabilistic Circuits by Latent Variable Distillation,0.881855,"Probabilistic Circuits (PCs) are a unified framework for tractable
probabilistic models that support efficient computation of various
probabilistic queries (e.g., marginal probabilities). One key challenge is to
scale PCs to model large and high-dimensional real-world datasets: we observe
that as the number of parameters in PCs increases, their performance
immediately plateaus. This phenomenon suggests that the existing optimizers
fail to exploit the full expressive power of large PCs. We propose to overcome
such bottleneck by latent variable distillation: we leverage the less tractable
but more expressive deep generative models to provide extra supervision over
the latent variables of PCs. Specifically, we extract information from
Transformer-based generative models to assign values to latent variables of
PCs, providing guidance to PC optimizers. Experiments on both image and
language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent
variable distillation substantially boosts the performance of large PCs
compared to their counterparts without latent variable distillation. In
particular, on the image modeling benchmarks, PCs achieve competitive
performance against some of the widely-used deep generative models, including
variational autoencoders and flow-based models, opening up new avenues for
tractable generative modeling.",https://github.com/facebookresearch/mae,-1
73b4951c-69d3-446f-a97d-8eb88fd16a3e,Modeling Human Behavior Part I -- Learning and Belief Approaches,0.165002,"There is a clear desire to model and comprehend human behavior. Trends in
research covering this topic show a clear assumption that many view human
reasoning as the presupposed standard in artificial reasoning. As such, topics
such as game theory, theory of mind, machine learning, etc. all integrate
concepts which are assumed components of human reasoning. These serve as
techniques to attempt to both replicate and understand the behaviors of humans.
In addition, next generation autonomous and adaptive systems will largely
include AI agents and humans working together as teams. To make this possible,
autonomous agents will require the ability to embed practical models of human
behavior, which allow them not only to replicate human models as a technique to
""learn"", but to to understand the actions of users and anticipate their
behavior, so as to truly operate in symbiosis with them. The main objective of
this paper it to provide a succinct yet systematic review of the most important
approaches in two areas dealing with quantitative models of human behaviors.
Specifically, we focus on (i) techniques which learn a model or policy of
behavior through exploration and feedback, such as Reinforcement Learning, and
(ii) directly model mechanisms of human reasoning, such as beliefs and bias,
without going necessarily learning via trial-and-error.",None,-1
2f8ba7fb-77fb-48d4-a8f2-8c076bf1bd22,Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks,0.199169,"Logical approaches to representing language have developed and evaluated
computational models of quantifier words since the 19th century, but today's
NLU models still struggle to capture their semantics. We rely on Generalized
Quantifier Theory for language-independent representations of the semantics of
quantifier words, to quantify their contribution to the errors of NLU models.
We find that quantifiers are pervasive in NLU benchmarks, and their occurrence
at test time is associated with performance drops. Multilingual models also
exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse
for non-English languages. To facilitate directly-targeted probing, we present
an adversarial generalized quantifier NLI task (GQNLI) and show that
pre-trained language models have a clear lack of robustness in generalized
quantifier reasoning.",None,-1
25b2a6ab-2f8d-4a81-be77-f09dac7c990b,Facing Changes: Continual Entity Alignment for Growing Knowledge Graphs,0.427999,"Entity alignment is a basic and vital technique in knowledge graph (KG)
integration. Over the years, research on entity alignment has resided on the
assumption that KGs are static, which neglects the nature of growth of
real-world KGs. As KGs grow, previous alignment results face the need to be
revisited while new entity alignment waits to be discovered. In this paper, we
propose and dive into a realistic yet unexplored setting, referred to as
continual entity alignment. To avoid retraining an entire model on the whole
KGs whenever new entities and triples come, we present a continual alignment
method for this task. It reconstructs an entity's representation based on
entity adjacency, enabling it to generate embeddings for new entities quickly
and inductively using their existing neighbors. It selects and replays partial
pre-aligned entity pairs to train only parts of KGs while extracting
trustworthy alignment for knowledge augmentation. As growing KGs inevitably
contain non-matchable entities, different from previous works, the proposed
method employs bidirectional nearest neighbor matching to find new entity
alignment and update old alignment. Furthermore, we also construct new datasets
by simulating the growth of multilingual DBpedia. Extensive experiments
demonstrate that our continual alignment method is more effective than
baselines based on retraining or inductive learning.",https://github.com/nju-websoft/ContEA,-1
26b4889b-49f3-4da5-a1da-ee74d9c9bc11,Learning to Reverse DNNs from AI Programs Automatically,0.702398,"With the privatization deployment of DNNs on edge devices, the security of
on-device DNNs has raised significant concern. To quantify the model leakage
risk of on-device DNNs automatically, we propose NNReverse, the first
learning-based method which can reverse DNNs from AI programs without domain
knowledge. NNReverse trains a representation model to represent the semantics
of binary code for DNN layers. By searching the most similar function in our
database, NNReverse infers the layer type of a given function's binary code. To
represent assembly instructions semantics precisely, NNReverse proposes a more
fine-grained embedding model to represent the textual and structural-semantic
of assembly functions.",None,-1
63acf935-f064-4611-909f-fbc18a310e7e,Neural Topic Modeling of Psychotherapy Sessions,0.488786,"In this work, we compare different neural topic modeling methods in learning
the topical propensities of different psychiatric conditions from the
psychotherapy session transcripts parsed from speech recordings. We also
incorporate temporal modeling to put this additional interpretability to action
by parsing out topic similarities as a time series in a turn-level resolution.
We believe this topic modeling framework can offer interpretable insights for
the therapist to optimally decide his or her strategy and improve psychotherapy
effectiveness.",https://github.com/zll17/Neural Topic Models,-1
716ceaa6-267c-4ef1-a5b6-a198b7902dd9,LingMess: Linguistically Informed Multi Expert Scorers for Coreference Resolution,0.882429,"While coreference resolution typically involves various linguistic
challenges, recent models are based on a single pairwise scorer for all types
of pairs. We present LingMess, a new coreference model that defines different
categories of coreference cases and optimize multiple pairwise scorers, where
each scorer learns a specific set of linguistic challenges. Our model
substantially improves pairwise scores for most categories and outperforms
cluster-level performance on Ontonotes and 5 additional datasets. Our model is
available in https://github.com/shon-otmazgin/lingmess-coref",https://github.com/shon-otmazgin/lingmess-coref,-1
8f9b0ce1-6a0d-4fd9-b00a-ebca75dda37a,COVIBOT: A Smart Chatbot for Assistance and E-Awareness during COVID-19 Pandemic,0.208444,"The coronavirus pandemic has spread over the past two years in our highly
connected and information-dense society. Nonetheless, disseminating accurate
and up-to-date information on the spread of this pandemic remains a challenge.
In this context, opting for a solution based on conversational artificial
intelligence, also known under the name of the chatbot, is proving to be an
unavoidable solution, especially since it has already shown its effectiveness
in fighting the coronavirus crisis in several countries. This work proposes to
design and implement a smart chatbot on the theme of COVID-19, called COVIBOT,
which will be useful in the context of Saudi Arabia. COVIBOT is a
generative-based contextual chatbot, which is built using machine learning APIs
that are offered by the cloud-based Azure Cognitive Services. Two versions of
COVIBOT are offered: English and Arabic versions. Use cases of COVIBOT are
tested and validated using a scenario-based approach.",None,-1
a61f1267-548a-4f08-a932-cacaf23c8e08,A Holistic Framework for Analyzing the COVID-19 Vaccine Debate,0.821673,"The Covid-19 pandemic has led to infodemic of low quality information leading
to poor health decisions. Combating the outcomes of this infodemic is not only
a question of identifying false claims, but also reasoning about the decisions
individuals make. In this work we propose a holistic analysis framework
connecting stance and reason analysis, and fine-grained entity level moral
sentiment analysis. We study how to model the dependencies between the
different level of analysis and incorporate human insights into the learning
process. Experiments show that our framework provides reliable predictions even
in the low-supervision settings.",https://gitlab.com/mlpacheco/covid-moral-foundations,-1
8ddcddf2-3977-4958-88b3-de95db9afcd4,Sequence-to-Sequence Knowledge Graph Completion and Question Answering,0.986637,"Knowledge graph embedding (KGE) models represent each entity and relation of
a knowledge graph (KG) with low-dimensional embedding vectors. These methods
have recently been applied to KG link prediction and question answering over
incomplete KGs (KGQA). KGEs typically create an embedding for each entity in
the graph, which results in large model sizes on real-world graphs with
millions of entities. For downstream tasks these atomic entity representations
often need to be integrated into a multi stage pipeline, limiting their
utility. We show that an off-the-shelf encoder-decoder Transformer model can
serve as a scalable and versatile KGE model obtaining state-of-the-art results
for KG link prediction and incomplete KG question answering. We achieve this by
posing KG link prediction as a sequence-to-sequence task and exchange the
triple scoring approach taken by prior KGE methods with autoregressive
decoding. Such a simple but powerful method reduces the model size up to 98%
compared to conventional KGE models while keeping inference time tractable.
After finetuning this model on the task of KGQA over incomplete KGs, our
approach outperforms baselines on multiple large-scale datasets without
extensive hyperparameter tuning.",https://github.com/apoorvumang/kgt5,-1
481537be-afff-446f-922b-690bf36849b3,PP-YOLOE: An evolved version of YOLO,0.87966,"In this report, we present PP-YOLOE, an industrial state-of-the-art object
detector with high performance and friendly deployment. We optimize on the
basis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful
backbone and neck equipped with CSPRepResStage, ET-head and dynamic label
assignment algorithm TAL. We provide s/m/l/x models for different practice
scenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1
FPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35% speed
up) and (+1.3 AP, +24.96% speed up), compared to the previous state-of-the-art
industrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference
speed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct
extensive experiments to verify the effectiveness of our designs. Source code
and pre-trained models are available at
https://github.com/PaddlePaddle/PaddleDetection.",https://github.com/PaddlePaddle/PaddleDetection,-1
150ac79e-4f77-447c-b440-b5e9718f499b,Revisiting End-to-End Speech-to-Text Translation From Scratch,0.97047,"End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining
its encoder and/or decoder using source transcripts via speech recognition or
text translation tasks, without which translation performance drops
substantially. However, transcripts are not always available, and how
significant such pretraining is for E2E ST has rarely been studied in the
literature. In this paper, we revisit this question and explore the extent to
which the quality of E2E ST trained on speech-translation pairs alone can be
improved. We reexamine several techniques proven beneficial to ST previously,
and offer a set of best practices that biases a Transformer-based E2E ST system
toward training from scratch. Besides, we propose parameterized distance
penalty to facilitate the modeling of locality in the self-attention model for
speech. On four benchmarks covering 23 languages, our experiments show that,
without using any transcripts or pretraining, the proposed system reaches and
even outperforms previous studies adopting pretraining, although the gap
remains in (extremely) low-resource settings. Finally, we discuss neural
acoustic feature modeling, where a neural model is designed to extract acoustic
features from raw speech signals directly, with the goal to simplify inductive
biases and add freedom to the model in describing speech. For the first time,
we demonstrate its feasibility and show encouraging results on ST tasks.",None,21498
12747e94-d016-495e-96c9-1cec7857d1e8,"A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks",0.229217,"Predictive coding networks are neuroscience-inspired models with roots in
both Bayesian statistics and neuroscience. Training such models, however, is
quite inefficient and unstable. In this work, we show how by simply changing
the temporal scheduling of the update rule for the synaptic weights leads to an
algorithm that is much more efficient and stable than the original one, and has
theoretical guarantees in terms of convergence. The proposed algorithm, that we
call incremental predictive coding (iPC) is also more biologically plausible
than the original one, as it it fully automatic. In an extensive set of
experiments, we show that iPC constantly performs better than the original
formulation on a large number of benchmarks for image classification, as well
as for the training of both conditional and masked language models, in terms of
test accuracy, efficiency, and convergence with respect to a large set of
hyperparameters.",None,12571
a31be4ae-72af-42e6-a767-dd7693aa9331,Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos,0.870179,"Understanding dynamic hand motions and actions from egocentric RGB videos is
a fundamental yet challenging task due to self-occlusion and ambiguity. To
address occlusion and ambiguity, we develop a transformer-based framework to
exploit temporal information for robust estimation. Noticing the different
temporal granularity of and the semantic correlation between hand pose
estimation and action recognition, we build a network hierarchy with two
cascaded transformer encoders, where the first one exploits the short-term
temporal cue for hand pose estimation, and the latter aggregates per-frame pose
and object information over a longer time span to recognize the action. Our
approach achieves competitive results on two first-person hand action
benchmarks, namely FPHA and H2O. Extensive ablation studies verify our design
choices.",https://github.com/fylwen/HTT,-1
24bf5c63-6665-4d45-9a4e-85d136e74ad3,Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation,0.942198,"Sparsely annotated semantic segmentation (SASS) aims to train a segmentation
network with coarse-grained (i.e., point-, scribble-, and block-wise)
supervisions, where only a small proportion of pixels are labeled in each
image. In this paper, we propose a novel tree energy loss for SASS by providing
semantic guidance for unlabeled pixels. The tree energy loss represents images
as minimum spanning trees to model both low-level and high-level pair-wise
affinities. By sequentially applying these affinities to the network
prediction, soft pseudo labels for unlabeled pixels are generated in a
coarse-to-fine manner, achieving dynamic online self-training. The tree energy
loss is effective and easy to be incorporated into existing frameworks by
combining it with a traditional segmentation loss. Compared with previous SASS
methods, our method requires no multistage training strategies, alternating
optimization procedures, additional supervised data, or time-consuming
post-processing while outperforming them in all SASS settings. Code is
available at https://github.com/megvii-research/TreeEnergyLoss.",https://github.com/megvii-research/TreeEnergyLoss,-1
8f1466b1-cf34-4282-b1dd-a92ba71dc91f,RobustLoc: Robust Camera Pose Regression in Challenging Driving Environments,0.775592,"Camera relocalization has various applications in autonomous driving.
Previous camera pose regression models consider only ideal scenarios where
there is little environmental perturbation. To deal with challenging driving
environments that may have changing seasons, weather, illumination, and the
presence of unstable objects, we propose RobustLoc, which derives its
robustness against perturbations from neural differential equations. Our model
uses a convolutional neural network to extract feature maps from multi-view
images, a robust neural differential equation diffusion block module to diffuse
information interactively, and a branched pose decoder with multi-layer
training to estimate the vehicle poses. Experiments demonstrate that RobustLoc
surpasses current state-of-the-art camera pose regression models and achieves
robust performance in various environments. Our code is released at:
https://github.com/sijieaaa/RobustLoc",https://github.com/sijieaaa/RobustLoc,-1
afb7d361-fe45-4714-aeca-b4f60d957552,Jitter Does Matter: Adapting Gaze Estimation to New Domains,0.444182,"Deep neural networks have demonstrated superior performance on
appearance-based gaze estimation tasks. However, due to variations in person,
illuminations, and background, performance degrades dramatically when applying
the model to a new domain. In this paper, we discover an interesting gaze
jitter phenomenon in cross-domain gaze estimation, i.e., the gaze predictions
of two similar images can be severely deviated in target domain. This is
closely related to cross-domain gaze estimation tasks, but surprisingly, it has
not been noticed yet previously. Therefore, we innovatively propose to utilize
the gaze jitter to analyze and optimize the gaze domain adaptation task. We
find that the high-frequency component (HFC) is an important factor that leads
to jitter. Based on this discovery, we add high-frequency components to input
images using the adversarial attack and employ contrastive learning to
encourage the model to obtain similar representations between original and
perturbed data, which reduces the impacts of HFC. We evaluate the proposed
method on four cross-domain gaze estimation tasks, and experimental results
demonstrate that it significantly reduces the gaze jitter and improves the gaze
estimation performance in target domains.",None,-1
05a5fd7b-2e2d-4f8c-b2d0-08b809ede502,Self-Supervised Visual Place Recognition by Mining Temporal and Feature Neighborhoods,0.42838,"Visual place recognition (VPR) using deep networks has achieved
state-of-the-art performance. However, most of them require a training set with
ground truth sensor poses to obtain positive and negative samples of each
observation's spatial neighborhood for supervised learning. When such
information is unavailable, temporal neighborhoods from a sequentially
collected data stream could be exploited for self-supervised training, although
we find its performance suboptimal. Inspired by noisy label learning, we
propose a novel self-supervised framework named \textit{TF-VPR} that uses
temporal neighborhoods and learnable feature neighborhoods to discover unknown
spatial neighborhoods. Our method follows an iterative training paradigm which
alternates between: (1) representation learning with data augmentation, (2)
positive set expansion to include the current feature space neighbors, and (3)
positive set contraction via geometric verification. We conduct comprehensive
experiments on both simulated and real datasets, with either RGB images or
point clouds as inputs. The results show that our method outperforms our
baselines in recall rate, robustness, and heading diversity, a novel metric we
propose for VPR. Our code and datasets can be found at
https://ai4ce.github.io/TF-VPR/.",https://ai4ce.github.io/TF-VPR/,-1
b9b258f3-f5db-4227-bf5d-f3865c5a4427,Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models,0.215754,"Prompt learning is a new learning paradigm which reformulates downstream
tasks as similar pretraining tasks on pretrained models by leveraging textual
prompts. Recent works have demonstrated that prompt learning is particularly
useful for few-shot learning, where there is limited training data. Depending
on the granularity of prompts, those methods can be roughly divided into
task-level prompting and instance-level prompting. Task-level prompting methods
learn one universal prompt for all input samples, which is efficient but
ineffective to capture subtle differences among different classes.
Instance-level prompting methods learn a specific prompt for each input, though
effective but inefficient. In this work, we develop a novel prototype-based
prompt learning method to overcome the above limitations. In particular, we
focus on few-shot image recognition tasks on pretrained vision-language models
(PVLMs) and develop a method of prompting through prototype (PTP), where we
define $K$ image prototypes and $K$ prompt prototypes. In PTP, the image
prototype represents a centroid of a certain image cluster in the latent space
and a prompt prototype is defined as a soft prompt in the continuous space. The
similarity between a query image and an image prototype determines how much
this prediction relies on the corresponding prompt prototype. Hence, in PTP,
similar images will utilize similar prompting ways. Through extensive
experiments on seven real-world benchmarks, we show that PTP is an effective
method to leverage the latent knowledge and adaptive to various PVLMs.
Moreover, through detailed analysis, we discuss pros and cons for prompt
learning and parameter-efficient fine-tuning under the context of few-shot
learning.",None,-1
c0dd0f00-159d-4e01-8159-c079f56880eb,TVLT: Textless Vision-Language Transformer,0.365865,"In this work, we present the Textless Vision-Language Transformer (TVLT),
where homogeneous transformer blocks take raw visual and audio inputs for
vision-and-language representation learning with minimal modality-specific
design, and do not use text-specific modules such as tokenization or automatic
speech recognition (ASR). TVLT is trained by reconstructing masked patches of
continuous video frames and audio spectrograms (masked autoencoding) and
contrastive modeling to align video and audio. TVLT attains performance
comparable to its text-based counterpart on various multimodal tasks, such as
visual question answering, image retrieval, video retrieval, and multimodal
sentiment analysis, with 28x faster inference speed and only 1/3 of the
parameters. Our findings suggest the possibility of learning compact and
efficient visual-linguistic representations from low-level visual and audio
signals without assuming the prior existence of text. Our code and checkpoints
are available at: https://github.com/zinengtang/TVLT",https://github.com/zinengtang/TVLT,-1
9d9f7d99-7ac4-42f1-81fa-8e07913bd8b0,"""My nose is running.""""Are you also coughing?"": Building A Medical Diagnosis Agent with Interpretable Inquiry Logics",0.358487,"With the rise of telemedicine, the task of developing Dialogue Systems for
Medical Diagnosis (DSMD) has received much attention in recent years. Different
from early researches that needed to rely on extra human resources and
expertise to help construct the system, recent researches focused on how to
build DSMD in a purely data-driven manner. However, the previous data-driven
DSMD methods largely overlooked the system interpretability, which is critical
for a medical application, and they also suffered from the data sparsity issue
at the same time. In this paper, we explore how to bring interpretability to
data-driven DSMD. Specifically, we propose a more interpretable decision
process to implement the dialogue manager of DSMD by reasonably mimicking real
doctors' inquiry logics, and we devise a model with highly transparent
components to conduct the inference. Moreover, we collect a new DSMD dataset,
which has a much larger scale, more diverse patterns and is of higher quality
than the existing ones. The experiments show that our method obtains 7.7%,
10.0%, 3.0% absolute improvement in diagnosis accuracy respectively on three
datasets, demonstrating the effectiveness of its rational decision process and
model design. Our codes and the GMD-12 dataset are available at
https://github.com/lwgkzl/BR-Agent.",https://github.com/lwgkzl/BR-Agent,-1
a1d78bc0-c9f9-43b2-9096-9bf9304002c5,Multi-View Object Pose Refinement With Differentiable Renderer,0.491179,"This paper introduces a novel multi-view 6 DoF object pose refinement
approach focusing on improving methods trained on synthetic data. It is based
on the DPOD detector, which produces dense 2D-3D correspondences between the
model vertices and the image pixels in each frame. We have opted for the use of
multiple frames with known relative camera transformations, as it allows
introduction of geometrical constraints via an interpretable ICP-like loss
function. The loss function is implemented with a differentiable renderer and
is optimized iteratively. We also demonstrate that a full detection and
refinement pipeline, which is trained solely on synthetic data, can be used for
auto-labeling real data. We perform quantitative evaluation on LineMOD,
Occlusion, Homebrewed and YCB-V datasets and report excellent performance in
comparison to the state-of-the-art methods trained on the synthetic and real
data. We demonstrate empirically that our approach requires only a few frames
and is robust to close camera locations and noise in extrinsic camera
calibration, making its practical usage easier and more ubiquitous.",None,-1
9aa32590-9c26-4d60-93c7-1f239280ddb4,TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval,0.919095,"Text-Video retrieval is a task of great practical value and has received
increasing attention, among which learning spatial-temporal video
representation is one of the research hotspots. The video encoders in the
state-of-the-art video retrieval models usually directly adopt the pre-trained
vision backbones with the network structure fixed, they therefore can not be
further improved to produce the fine-grained spatial-temporal video
representation. In this paper, we propose Token Shift and Selection Network
(TS2-Net), a novel token shift and selection transformer architecture, which
dynamically adjusts the token sequence and selects informative tokens in both
temporal and spatial dimensions from input video samples. The token shift
module temporally shifts the whole token features back-and-forth across
adjacent frames, to preserve the complete token representation and capture
subtle movements. Then the token selection module selects tokens that
contribute most to local spatial semantics. Based on thorough experiments, the
proposed TS2-Net achieves state-of-the-art performance on major text-video
retrieval benchmarks, including new records on MSRVTT, VATEX, LSMDC,
ActivityNet, and DiDeMo.",https://github.com/yuqi657/ts2_net,-1
0afc3560-1a16-47e8-b96a-c552005af16d,Implicit Two-Tower Policies,0.307883,"We present a new class of structured reinforcement learning
policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are
chosen based on the attention scores of their learnable latent representations
with those of the input states. By explicitly disentangling action from state
processing in the policy stack, we achieve two main goals: substantial
computational gains and better performance. Our architectures are compatible
with both: discrete and continuous action spaces. By conducting tests on 15
environments from OpenAI Gym and DeepMind Control Suite, we show that
ITT-architectures are particularly suited for blackbox/evolutionary
optimization and the corresponding policy training algorithms outperform their
vanilla unstructured implicit counterparts as well as commonly used explicit
policies. We complement our analysis by showing how techniques such as hashing
and lazy tower updates, critically relying on the two-tower structure of ITTs,
can be applied to obtain additional computational improvements.",https://anonymous.4open.science/r/itt-9881/README.md,17269
817bf9f0-b922-47ea-936d-74f675a30252,Gradient-Based Constrained Sampling from Language Models,0.452526,"Large pretrained language models generate fluent text but are notoriously
hard to controllably sample from. In this work, we study constrained sampling
from such language models: generating text that satisfies user-defined
constraints, while maintaining fluency and the model's performance in a
downstream task. We propose MuCoLa -- a sampling procedure that combines the
log-likelihood of the language model with arbitrary (differentiable)
constraints in a single energy function, and then generates samples in a
non-autoregressive manner. Specifically, it initializes the entire output
sequence with noise and follows a Markov chain defined by Langevin Dynamics
using the gradients of the energy function. We evaluate MuCoLa on text
generation with soft and hard constraints as well as their combinations
obtaining significant improvements over competitive baselines for toxicity
avoidance, sentiment control, and keyword-guided generation.",https://github.com/Sachin19/mucoco/tree/sampling,-1
e39381b1-ab57-421e-acc2-5ec255ef2d80,IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation,0.47226,"The T5 model and its unified text-to-text paradigm contributed in advancing
the state-of-the-art for many natural language processing tasks. While some
multilingual variants of the T5 model have recently been introduced, their
performances were found to provide suboptimal performances for languages other
than English if compared to monolingual variants. We are motivated by these
findings to introduce IT5, the first family of encoder-decoder transformer
models pretrained specifically on Italian. We perform a thorough cleaning of a
web-crawled Italian corpus including more than 40 billion words and use it to
pretrain three IT5 models of different sizes. The performance of IT5 models and
their multilingual counterparts is then evaluated on a broad range of natural
language understanding and generation benchmarks for Italian. We find the
monolingual IT5 models to provide the best scale-to-performance ratio across
tested models, consistently outperforming their multilingual counterparts and
setting a new state-of-the-art for most Italian conditional language generation
tasks.",https://github.com/gsarti/it5,-1
2d505e44-a54c-4b2e-8fb5-3daa01ae8740,Multi-level Consistency Learning for Semi-supervised Domain Adaptation,0.586545,"Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from
a fully labeled source domain to a scarcely labeled target domain. In this
paper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA.
Specifically, our MCL regularizes the consistency of different views of target
domain samples at three levels: (i) at inter-domain level, we robustly and
accurately align the source and target domains using a prototype-based optimal
transport method that utilizes the pros and cons of different views of target
samples; (ii) at intra-domain level, we facilitate the learning of both
discriminative and compact target feature representations by proposing a novel
class-wise contrastive clustering loss; (iii) at sample level, we follow
standard practice and improve the prediction accuracy by conducting a
consistency-based self-training. Empirically, we verified the effectiveness of
our MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet,
and Office-Home datasets, and the experimental results demonstrate that our MCL
framework achieves the state-of-the-art performance.",https://github.com/chester256/MCL,30637
8994705f-8376-4655-bd18-163d5ebf12d5,McQueen: a Benchmark for Multimodal Conversational Query Rewrite,0.291658,"The task of query rewrite aims to convert an in-context query to its
fully-specified version where ellipsis and coreference are completed and
referred-back according to the history context. Although much progress has been
made, less efforts have been paid to real scenario conversations that involve
drawing information from more than one modalities. In this paper, we propose
the task of multimodal conversational query rewrite (McQR), which performs
query rewrite under the multimodal visual conversation setting. We collect a
large-scale dataset named McQueen based on manual annotation, which contains
15k visual conversations and over 80k queries where each one is associated with
a fully-specified rewrite version. In addition, for entities appearing in the
rewrite, we provide the corresponding image box annotation. We then use the
McQueen dataset to benchmark a state-of-the-art method for effectively tackling
the McQR task, which is based on a multimodal pre-trained model with pointer
generator. Extensive experiments are performed to demonstrate the effectiveness
of our model on this task\footnote{The dataset and code of this paper are both
available in \url{https://github.com/yfyuan01/MQR}",None,-1
a2250ee1-4771-4e71-b0bb-f159fc92359c,Reference Resolution and Context Change in Multimodal Situated Dialogue for Exploring Data Visualizations,0.0663935,"Reference resolution, which aims to identify entities being referred to by a
speaker, is more complex in real world settings: new referents may be created
by processes the agents engage in and/or be salient only because they belong to
the shared physical setting. Our focus is on resolving references to
visualizations on a large screen display in multimodal dialogue; crucially,
reference resolution is directly involved in the process of creating new
visualizations. We describe our annotations for user references to
visualizations appearing on a large screen via language and hand gesture and
also new entity establishment, which results from executing the user request to
create a new visualization. We also describe our reference resolution pipeline
which relies on an information-state architecture to maintain dialogue context.
We report results on detecting and resolving references, effectiveness of
contextual information on the model, and under-specified requests for creating
visualizations. We also experiment with conventional CRF and deep learning /
transformer models (BiLSTM-CRF and BERT-CRF) for tagging references in user
utterance text. Our results show that transfer learning significantly boost
performance of the deep learning methods, although CRF still out-performs them,
suggesting that conventional methods may generalize better for low resource
data.",None,-1
54715a7b-a30f-4adb-9056-81143c0ef40d,Learning to Ask Like a Physician,0.378173,"Existing question answering (QA) datasets derived from electronic health
records (EHR) are artificially generated and consequently fail to capture
realistic physician information needs. We present Discharge Summary Clinical
Questions (DiSCQ), a newly curated question dataset composed of 2,000+
questions paired with the snippets of text (triggers) that prompted each
question. The questions are generated by medical experts from 100+ MIMIC-III
discharge summaries. We analyze this dataset to characterize the types of
information sought by medical experts. We also train baseline models for
trigger detection and question generation (QG), paired with unsupervised answer
retrieval over EHRs. Our baseline model is able to generate high quality
questions in over 62% of cases when prompted with human selected triggers. We
release this dataset (and all code to reproduce baseline model results) to
facilitate further research into realistic clinical QA and QG:
https://github.com/elehman16/discq.",https://github.com/elehman16/discq,-1
24874699-eda5-4a44-ae5f-d3bf28be6195,Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization,0.776092,"In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D .",https://github.com/YxZhxn/Ray3D,-1
068f2be8-c023-415b-a69b-0487eef55c78,Non-rigid Point Cloud Registration with Neural Deformation Pyramid,0.839812,"Non-rigid point cloud registration is a key component in many computer vision
and computer graphics applications. The high complexity of the unknown
non-rigid motion make this task a challenging problem. In this paper, we break
down this problem via hierarchical motion decomposition. Our method called
Neural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid
architecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),
takes as input a sinusoidally encoded 3D point and outputs its motion
increments from the previous level. The sinusoidal function starts with a low
input frequency and gradually increases when the pyramid level goes down. This
allows a multi-level rigid to nonrigid motion decomposition and also speeds up
the solving by 50 times compared to the existing MLP-based approach. Our method
achieves advanced partialto-partial non-rigid point cloud registration results
on the 4DMatch/4DLoMatch benchmark under both no-learned and supervised
settings.",https://github.com/rabbityl/DeformationPyramid,-1
9680b0f1-4221-4296-a74c-3a274f3339b0,Generalized Differentiable RANSAC,0.871734,"We propose $\nabla$-RANSAC, a generalized differentiable RANSAC that allows
learning the entire randomized robust estimation pipeline. The proposed
approach enables the use of relaxation techniques for estimating the gradients
in the sampling distribution, which are then propagated through a
differentiable solver. The trainable quality function marginalizes over the
scores from all the models estimated within $\nabla$-RANSAC to guide the
network learning accurate and useful inlier probabilities or to train feature
detection and matching networks. Our method directly maximizes the probability
of drawing a good hypothesis, allowing us to learn better sampling
distributions. We test $\nabla$-RANSAC on various real-world scenarios on
fundamental and essential matrix estimation, and 3D point cloud registration,
outdoors and indoors, with handcrafted and learning-based features. It is
superior to the state-of-the-art in terms of accuracy while running at a
similar speed to its less accurate alternatives. The code and trained models
are available at https://github.com/weitong8591/differentiable_ransac.",https://github.com/weitong8591/differentiable_ransac,-1
8ce7dc71-7b86-47a2-aceb-9bca1e67cb69,MPA: MultiPath++ Based Architecture for Motion Prediction,0.668082,"Autonomous driving technology is developing rapidly and nowadays first
autonomous rides are being provided in city areas. This requires the highest
standards for the safety and reliability of the technology. Motion prediction
part of the general self-driving pipeline plays a crucial role in providing
these qualities. In this work we present one of the solutions for Waymo Motion
Prediction Challenge 2022 based on MultiPath++ ranked the 3rd as of May, 26
2022. Our source code is publicly available on GitHub.",https://github.com/stepankonev/waymo-motion-prediction-challenge-2022-multipath-plus-plus,-1
9d84fd9f-05f2-4882-8cae-5aff3ada75b6,Improving Large-scale Paraphrase Acquisition and Generation,0.578161,"This paper addresses the quality issues in existing Twitter-based paraphrase
datasets, and discusses the necessity of using two separate definitions of
paraphrase for identification and generation tasks. We present a new
Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of
130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert
(MultiPIT_expert) annotations using two different paraphrase definitions for
paraphrase identification, in addition to a multi-reference test set
(MultiPIT_NMR) and a large automatically constructed training set
(MultiPIT_Auto) for paraphrase generation. With improved data annotation
quality and task-specific paraphrase definition, the best pre-trained language
model fine-tuned on our dataset achieves the state-of-the-art performance of
84.2 F1 for automatic paraphrase identification. Furthermore, our empirical
results also demonstrate that the paraphrase generation models trained on
MultiPIT_Auto generate more diverse and high-quality paraphrases compared to
their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and
ParaNMT.",https://www.github.com/Tiiiger/bert_score,-1
b2ad9197-bb19-4fe3-879d-f8afd35d8848,"YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception",0.969665,"Over the last decade, multi-tasking learning approaches have achieved
promising results in solving panoptic driving perception problems, providing
both high-precision and high-efficiency performance. It has become a popular
paradigm when designing networks for real-time practical autonomous driving
system, where computation resources are limited. This paper proposed an
effective and efficient multi-task learning network to simultaneously perform
the task of traffic object detection, drivable road area segmentation and lane
detection. Our model achieved the new state-of-the-art (SOTA) performance in
terms of accuracy and speed on the challenging BDD100K dataset. Especially, the
inference time is reduced by half compared to the previous SOTA model. Code
will be released in the near future.",None,-1
6fb43ccb-c97e-4656-b36f-30edd5521763,Responsible AI Implementation: A Human-centered Framework for Accelerating the Innovation Process,0.157796,"There is still a significant gap between expectations and the successful
adoption of AI to innovate and improve businesses. Due to the emergence of deep
learning, AI adoption is more complex as it often incorporates big data and the
internet of things, affecting data privacy. Existing frameworks have identified
the need to focus on human-centered design, combining technical and
business/organizational perspectives. However, trust remains a critical issue
that needs to be designed from the beginning. The proposed framework expands
from the human-centered design approach, emphasizing and maintaining the trust
that underpins the process. This paper proposes a theoretical framework for
responsible artificial intelligence (AI) implementation. The proposed framework
emphasizes a synergistic business technology approach for the agile co-creation
process. The aim is to streamline the adoption process of AI to innovate and
improve business by involving all stakeholders throughout the project so that
the AI technology is designed, developed, and deployed in conjunction with
people and not in isolation. The framework presents a fresh viewpoint on
responsible AI implementation based on analytical literature review, conceptual
framework design, and practitioners' mediating expertise. The framework
emphasizes establishing and maintaining trust throughout the human-centered
design and agile development of AI. This human-centered approach is aligned
with and enabled by the privacy by design principle. The creators of the
technology and the end-users are working together to tailor the AI solution
specifically for the business requirements and human characteristics. An
illustrative case study on adopting AI for assisting planning in a hospital
will demonstrate that the proposed framework applies to real-life applications.",None,-1
c0db5b3d-0e4e-4695-84d6-8acb96ee63e5,Understanding Translationese in Cross-Lingual Summarization,0.864011,"Given a document in a source language, cross-lingual summarization (CLS) aims
at generating a concise summary in a different target language. Unlike
monolingual summarization (MS), naturally occurring source-language documents
paired with target-language summaries are rare. To collect large-scale CLS
data, existing datasets typically involve translation in their creation.
However, the translated text is distinguished from the text originally written
in that language, i.e., translationese. In this paper, we first confirm that
different approaches of constructing CLS datasets will lead to different
degrees of translationese. Then we systematically investigate how
translationese affects CLS model evaluation and performance when it appears in
source documents or target summaries. In detail, we find that (1) the
translationese in documents or summaries of test sets might lead to the
discrepancy between human judgment and automatic evaluation; (2) the
translationese in training sets would harm model performance in real-world
applications; (3) though machine-translated documents involve translationese,
they are very useful for building CLS systems on low-resource languages under
specific training strategies. Lastly, we give suggestions for future CLS
research including dataset and model developments. We hope that our work could
let researchers notice the phenomenon of translationese in CLS and take it into
account in the future.",https://github.com/xcfcode/MSAMSum,-1
94f8bccf-6883-4863-a56b-97f35010bdc8,Multimodal Hate Speech Detection from Bengali Memes and Texts,0.888433,"Numerous machine learning (ML) and deep learning (DL)-based approaches have
been proposed to utilize textual data from social media for anti-social
behavior analysis like cyberbullying, fake news detection, and identification
of hate speech mainly for highly-resourced languages such as English. However,
despite having a lot of diversity and millions of native speakers, some
languages like Bengali are under-resourced, which is due to a lack of
computational resources for natural language processing (NLP). Similar to other
languages, Bengali social media contents also include images along with texts
(e.g., multimodal memes are posted by embedding short texts into images on
Facebook). Therefore, only the textual data is not enough to judge them since
images might give extra context to make a proper judgement. This paper is about
hate speech detection from multimodal Bengali memes and texts. We prepared the
only multimodal hate speech dataset for-a-kind of problem for Bengali, which we
use to train state-of-the-art neural architectures (e.g., Bi-LSTM/Conv-LSTM
with word embeddings, ConvNets + pre-trained language models, e.g., monolingual
Bangla BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) to jointly
analyze textual and visual information for hate speech detection. Conv-LSTM and
XLM-RoBERTa models performed best for texts, yielding F1 scores of 0.78 and
0.82, respectively. As of memes, ResNet-152 and DenseNet-161 models yield F1
scores of 0.78 and 0.79, respectively. As for multimodal fusion, XLM-RoBERTa +
DenseNet-161 performed the best, yielding an F1 score of 0.83. Our study
suggests that text modality is most useful for hate speech detection, while
memes are moderately useful.",https://github.com/rezacsedu/Multimodal-Hate-Speech-Bengali,-1
a341ad76-41f3-4a50-a4ea-cf46305f2777,Practical Deepfake Detection: Vulnerabilities in Global Contexts,0.0357506,"Recent advances in deep learning have enabled realistic digital alterations
to videos, known as deepfakes. This technology raises important societal
concerns regarding disinformation and authenticity, galvanizing the development
of numerous deepfake detection algorithms. At the same time, there are
significant differences between training data and in-the-wild video data, which
may undermine their practical efficacy. We simulate data corruption techniques
and examine the performance of a state-of-the-art deepfake detection algorithm
on corrupted variants of the FaceForensics++ dataset.
  While deepfake detection models are robust against video corruptions that
align with training-time augmentations, we find that they remain vulnerable to
video corruptions that simulate decreases in video quality. Indeed, in the
controversial case of the video of Gabonese President Bongo's new year address,
the algorithm, which confidently authenticates the original video, judges
highly corrupted variants of the video to be fake. Our work opens up both
technical and ethical avenues of exploration into practical deepfake detection
in global contexts.",https://github.com/deepfakes/faceswap,-1
1f8da53c-0d30-4ad6-be1b-ce42347542bb,Uncertainty-aware Panoptic Segmentation,0.689514,"Reliable scene understanding is indispensable for modern autonomous systems.
Current learning-based methods typically try to maximize their performance
based on segmentation metrics that only consider the quality of the
segmentation. However, for the safe operation of a system in the real world it
is crucial to consider the uncertainty in the prediction as well. In this work,
we introduce the novel task of uncertainty-aware panoptic segmentation, which
aims to predict per-pixel semantic and instance segmentations, together with
per-pixel uncertainty estimates. We define two novel metrics to facilitate its
quantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the
panoptic Expected Calibration Error (pECE). We further propose the novel
top-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.
Our architecture employs a simple yet effective panoptic fusion module that
leverages the predicted uncertainties. Furthermore, we provide several strong
baselines combining state-of-the-art panoptic segmentation networks with
sampling-free uncertainty estimation techniques. Extensive evaluations show
that our EvPSNet achieves the new state-of-the-art for the standard Panoptic
Quality (PQ), as well as for our uncertainty-aware panoptic metrics. We make
the code available at: \url{https://github.com/kshitij3112/EvPSNet}",https://github.com/kshitij3112/EvPSNet,115008
2deeed2b-53d8-45aa-830f-572f0a8a4c8d,Vision Transformer Equipped with Neural Resizer on Facial Expression Recognition Task,0.52048,"When it comes to wild conditions, Facial Expression Recognition is often
challenged with low-quality data and imbalanced, ambiguous labels. This field
has much benefited from CNN based approaches; however, CNN models have
structural limitation to see the facial regions in distant. As a remedy,
Transformer has been introduced to vision fields with global receptive field,
but requires adjusting input spatial size to the pretrained models to enjoy
their strong inductive bias at hands. We herein raise a question whether using
the deterministic interpolation method is enough to feed low-resolution data to
Transformer. In this work, we propose a novel training framework, Neural
Resizer, to support Transformer by compensating information and downscaling in
a data-driven manner trained with loss function balancing the noisiness and
imbalance. Experiments show our Neural Resizer with F-PDLS loss function
improves the performance with Transformer variants in general and nearly
achieves the state-of-the-art performance.",None,-1
7bb93bd2-c073-486b-9460-c1295ba530c4,Dialogue Evaluation with Offline Reinforcement Learning,0.0754686,"Task-oriented dialogue systems aim to fulfill user goals through natural
language interactions. They are ideally evaluated with human users, which
however is unattainable to do at every iteration of the development phase.
Simulated users could be an alternative, however their development is
nontrivial. Therefore, researchers resort to offline metrics on existing
human-human corpora, which are more practical and easily reproducible. They are
unfortunately limited in reflecting real performance of dialogue systems. BLEU
for instance is poorly correlated with human judgment, and existing
corpus-based metrics such as success rate overlook dialogue context mismatches.
There is still a need for a reliable metric for task-oriented systems with good
generalization and strong correlation with human judgements. In this paper, we
propose the use of offline reinforcement learning for dialogue evaluation based
on a static corpus. Such an evaluator is typically called a critic and utilized
for policy optimization. We go one step further and show that offline RL
critics can be trained on a static corpus for any dialogue system as external
evaluators, allowing dialogue performance comparisons across various types of
systems. This approach has the benefit of being corpus- and model-independent,
while attaining strong correlation with human judgements, which we confirm via
an interactive user trial.",https://gitlab.cs.uni-duesseldorf.de/general/dsml/lava-plas-public,-1
7e7b5137-c78e-40e3-b6af-e5ac5eae21ed,NSNet: A General Neural Probabilistic Framework for Satisfiability Problems,0.151134,"We present the Neural Satisfiability Network (NSNet), a general neural
framework that models satisfiability problems as probabilistic inference and
meanwhile exhibits proper explainability. Inspired by the Belief Propagation
(BP), NSNet uses a novel graph neural network (GNN) to parameterize BP in the
latent space, where its hidden representations maintain the same probabilistic
interpretation as BP. NSNet can be flexibly configured to solve both SAT and
#SAT problems by applying different learning objectives. For SAT, instead of
directly predicting a satisfying assignment, NSNet performs marginal inference
among all satisfying solutions, which we empirically find is more feasible for
neural networks to learn. With the estimated marginals, a satisfying assignment
can be efficiently generated by rounding and executing a stochastic local
search. For #SAT, NSNet performs approximate model counting by learning the
Bethe approximation of the partition function. Our evaluations show that NSNet
achieves competitive results in terms of inference accuracy and time efficiency
on multiple SAT and #SAT datasets.",https://github.com/zhaoyu-li/NSNet,-1
c766c5f2-7a24-4f15-8e9c-c78fa806cc27,UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes,0.635325,"We introduce UViM, a unified approach capable of modeling a wide range of
computer vision tasks. In contrast to previous models, UViM has the same
functional form for all tasks; it requires no task-specific modifications which
require extensive human expertise. The approach involves two components: (I) a
base model (feed-forward) which is trained to directly predict raw vision
outputs, guided by a learned discrete code and (II) a language model
(autoregressive) that is trained to generate the guiding code. These components
complement each other: the language model is well-suited to modeling structured
interdependent data, while the base model is efficient at dealing with
high-dimensional outputs. We demonstrate the effectiveness of UViM on three
diverse and challenging vision tasks: panoptic segmentation, depth prediction
and image colorization, where we achieve competitive and near state-of-the-art
results. Our experimental results suggest that UViM is a promising candidate
for a unified modeling approach in computer vision.",https://github.com/google-research/big_vision,-1
a71d7002-1db3-44bd-89c6-a5bdcd0a8dc6,Visual Haptic Reasoning: Estimating Contact Forces by Observing Deformable Object Interactions,0.312554,"Robotic manipulation of highly deformable cloth presents a promising
opportunity to assist people with several daily tasks, such as washing dishes;
folding laundry; or dressing, bathing, and hygiene assistance for individuals
with severe motor impairments. In this work, we introduce a formulation that
enables a collaborative robot to perform visual haptic reasoning with cloth --
the act of inferring the location and magnitude of applied forces during
physical interaction. We present two distinct model representations, trained in
physics simulation, that enable haptic reasoning using only visual and robot
kinematic observations. We conducted quantitative evaluations of these models
in simulation for robot-assisted dressing, bathing, and dish washing tasks, and
demonstrate that the trained models can generalize across different tasks with
varying interactions, human body sizes, and object shapes. We also present
results with a real-world mobile manipulator, which used our simulation-trained
models to estimate applied contact forces while performing physically assistive
tasks with cloth. Videos can be found at our project webpage.",None,-1
29810f6a-b44c-4195-ae9c-efda13b57312,Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification,0.242043,"Few-shot text classification aims to classify the text under the few-shot
scenario. Most of the previous methods adopt optimization-based meta learning
to obtain task distribution. However, due to the neglect of matching between
the few amount of samples and complicated models, as well as the distinction
between useful and useless task features, these methods suffer from the
overfitting issue. To address this issue, we propose a novel Adaptive
Meta-learner via Gradient Similarity (AMGS) method to improve the model
generalization ability to a new task. Specifically, the proposed AMGS
alleviates the overfitting based on two aspects: (i) acquiring the potential
semantic representation of samples and improving model generalization through
the self-supervised auxiliary task in the inner loop, (ii) leveraging the
adaptive meta-learner via gradient similarity to add constraints on the
gradient obtained by base-learner in the outer loop. Moreover, we make a
systematic analysis of the influence of regularization on the entire framework.
Experimental results on several benchmarks demonstrate that the proposed AMGS
consistently improves few-shot text classification performance compared with
the state-of-the-art optimization-based meta-learning approaches.",https://github.com/Tianyi-Lei,3279
23dd3364-8c88-4608-9460-fad0ae90fa89,How Well Do Self-Supervised Methods Perform in Cross-Domain Few-Shot Learning?,0.0646932,"Cross-domain few-shot learning (CDFSL) remains a largely unsolved problem in
the area of computer vision, while self-supervised learning presents a
promising solution. Both learning methods attempt to alleviate the dependency
of deep networks on the requirement of large-scale labeled data. Although
self-supervised methods have recently advanced dramatically, their utility on
CDFSL is relatively unexplored. In this paper, we investigate the role of
self-supervised representation learning in the context of CDFSL via a thorough
evaluation of existing methods. It comes as a surprise that even with shallow
architectures or small training datasets, self-supervised methods can perform
favorably compared to the existing SOTA methods. Nevertheless, no single
self-supervised approach dominates all datasets indicating that existing
self-supervised methods are not universally applicable. In addition, we find
that representations extracted from self-supervised methods exhibit stronger
robustness than the supervised method. Intriguingly, whether self-supervised
representations perform well on the source domain has little correlation with
their applicability on the target domain. As part of our study, we conduct an
objective measurement of the performance for six kinds of representative
classifiers. The results suggest Prototypical Classifier as the standard
evaluation recipe for CDFSL.",None,-1
a26cc44f-e244-438c-a77c-2a53daf6e244,"""The World Is Its Own Best Model"": Robust Real-World Manipulation Through Online Behavior Selection",0.0316469,"Robotic manipulation behavior should be robust to disturbances that violate
high-level task-structure. Such robustness can be achieved by constantly
monitoring the environment to observe the discrete high-level state of the
task. This is possible because different phases of a task are characterized by
different sensor patterns and by monitoring these patterns a robot can decide
which controllers to execute in the moment. This relaxes assumptions about the
temporal sequence of those controllers and makes behavior robust to unforeseen
disturbances. We implement this idea as probabilistic filter over discrete
states where each state is direcly associated with a controller. Based on this
framework we present a robotic system that is able to open a drawer and grasp
tennis balls from it in a surprisingly robust way.",None,-1
a8b295a1-1969-4022-b81a-5f8c5480ccd5,"Despite ""super-human"" performance, current LLMs are unsuited for decisions about ethics and safety",0.211458,"Large language models (LLMs) have exploded in popularity in the past few
years and have achieved undeniably impressive results on benchmarks as varied
as question answering and text summarization. We provide a simple new prompting
strategy that leads to yet another supposedly ""super-human"" result, this time
outperforming humans at common sense ethical reasoning (as measured by accuracy
on a subset of the ETHICS dataset). Unfortunately, we find that relying on
average performance to judge capabilities can be highly misleading. LLM errors
differ systematically from human errors in ways that make it easy to craft
adversarial examples, or even perturb existing examples to flip the output
label. We also observe signs of inverse scaling with model size on some
examples, and show that prompting models to ""explain their reasoning"" often
leads to alarming justifications of unethical actions. Our results highlight
how human-like performance does not necessarily imply human-like understanding
or reasoning.",None,-1
6175db5b-174e-47c4-b10d-763b8d264c64,Paraphrasing Techniques for Maritime QA system,0.137731,"There has been an increasing interest in incorporating Artificial
Intelligence (AI) into Defence and military systems to complement and augment
human intelligence and capabilities. However, much work still needs to be done
toward achieving an effective human-machine partnership. This work is aimed at
enhancing human-machine communications by developing a capability for
automatically translating human natural language into a machine-understandable
language (e.g., SQL queries). Techniques toward achieving this goal typically
involve building a semantic parser trained on a very large amount of
high-quality manually-annotated data. However, in many real-world Defence
scenarios, it is not feasible to obtain such a large amount of training data.
To the best of our knowledge, there are few works trying to explore the
possibility of training a semantic parser with limited manually-paraphrased
data, in other words, zero-shot. In this paper, we investigate how to exploit
paraphrasing methods for the automated generation of large-scale training
datasets (in the form of paraphrased utterances and their corresponding logical
forms in SQL format) and present our experimental results using real-world data
in the maritime domain.",https://github.com/pytorch/fairseq,-1
21686820-0fba-4d90-bf4a-b356417d98c7,A Dynamic Graph Interactive Framework with Label-Semantic Injection for Spoken Language Understanding,0.4308,"Multi-intent detection and slot filling joint models are gaining increasing
traction since they are closer to complicated real-world scenarios. However,
existing approaches (1) focus on identifying implicit correlations between
utterances and one-hot encoded labels in both tasks while ignoring explicit
label characteristics; (2) directly incorporate multi-intent information for
each token, which could lead to incorrect slot prediction due to the
introduction of irrelevant intent. In this paper, we propose a framework termed
DGIF, which first leverages the semantic information of labels to give the
model additional signals and enriched priors. Then, a multi-grain interactive
graph is constructed to model correlations between intents and slots.
Specifically, we propose a novel approach to construct the interactive graph
based on the injection of label semantics, which can automatically update the
graph to better alleviate error propagation. Experimental results show that our
framework significantly outperforms existing approaches, obtaining a relative
improvement of 13.7% over the previous best model on the MixATIS dataset in
overall accuracy.",https://github.com/Zhihong-Zhu/DGIF,7453
a52dc337-db0b-46d7-a694-5b3bad76867d,An Ultra-low Power TinyML System for Real-time Visual Processing at Edge,0.430048,"Tiny machine learning (TinyML), executing AI workloads on resource and power
strictly restricted systems, is an important and challenging topic. This brief
firstly presents an extremely tiny backbone to construct high efficiency CNN
models for various visual tasks. Then, a specially designed neural co-processor
(NCP) is interconnected with MCU to build an ultra-low power TinyML system,
which stores all features and weights on chip and completely removes both of
latency and power consumption in off-chip memory access. Furthermore, an
application specific instruction-set is further presented for realizing agile
development and rapid deployment. Extensive experiments demonstrate that the
proposed TinyML system based on our model, NCP and instruction set yields
considerable accuracy and achieves a record ultra-low power of 160mW while
implementing object detection and recognition at 30FPS. The demo video is
available on \url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}.",None,-1
5aeda20e-da53-4f55-8879-770299a1b788,"""I'm sorry to hear that"": Finding New Biases in Language Models with a Holistic Descriptor Dataset",0.855002,"As language models grow in popularity, it becomes increasingly important to
clearly measure all possible markers of demographic identity in order to avoid
perpetuating existing societal harms. Many datasets for measuring bias
currently exist, but they are restricted in their coverage of demographic axes
and are commonly used with preset bias tests that presuppose which types of
biases models can exhibit. In this work, we present a new, more inclusive bias
measurement dataset, HolisticBias, which includes nearly 600 descriptor terms
across 13 different demographic axes. HolisticBias was assembled in a
participatory process including experts and community members with lived
experience of these terms. These descriptors combine with a set of bias
measurement templates to produce over 450,000 unique sentence prompts, which we
use to explore, identify, and reduce novel forms of bias in several generative
models. We demonstrate that HolisticBias is effective at measuring previously
undetectable biases in token likelihoods from language models, as well as in an
offensiveness classifier. We will invite additions and amendments to the
dataset, which we hope will serve as a basis for more easy-to-use and
standardized methods for evaluating bias in NLP models.",https://github.com/facebookresearch/ResponsibleNLP/tree/main/holistic_bias,-1
4e9f5f0d-3169-499f-9051-50c3d977dbcf,Detect Hate Speech in Unseen Domains using Multi-Task Learning: A Case Study of Political Public Figures,0.345046,"Automatic identification of hateful and abusive content is vital in combating
the spread of harmful online content and its damaging effects. Most existing
works evaluate models by examining the generalization error on train-test
splits on hate speech datasets. These datasets often differ in their
definitions and labeling criteria, leading to poor model performance when
predicting across new domains and datasets. In this work, we propose a new
Multi-task Learning (MTL) pipeline that utilizes MTL to train simultaneously
across multiple hate speech datasets to construct a more encompassing
classification model. We simulate evaluation on new previously unseen datasets
by adopting a leave-one-out scheme in which we omit a target dataset from
training and jointly train on the other datasets. Our results consistently
outperform a large sample of existing work. We show strong results when
examining generalization error in train-test splits and substantial
improvements when predicting on previously unseen datasets. Furthermore, we
assemble a novel dataset, dubbed PubFigs, focusing on the problematic speech of
American Public Political Figures. We automatically detect problematic speech
in the $305,235$ tweets in PubFigs, and we uncover insights into the posting
behaviors of public figures.",None,-1
74af7c23-30f9-460c-84d2-1254cf6a31a3,Generate rather than Retrieve: Large Language Models are Strong Context Generators,0.815401,"Knowledge-intensive tasks, such as open-domain question answering (QA),
require access to a large amount of world or domain knowledge. A common
approach for knowledge-intensive tasks is to employ a retrieve-then-read
pipeline that first retrieves a handful of relevant contextual documents from
an external corpus such as Wikipedia and then predicts an answer conditioned on
the retrieved documents. In this paper, we present a novel perspective for
solving knowledge-intensive tasks by replacing document retrievers with large
language model generators. We call our method generate-then-read (GenRead),
which first prompts a large language model to generate contextutal documents
based on a given question, and then reads the generated documents to produce
the final answer. Furthermore, we propose a novel clustering-based prompting
method that selects distinct prompts, resulting in the generated documents that
cover different perspectives, leading to better recall over acceptable answers.
We conduct extensive experiments on three different knowledge-intensive tasks,
including open-domain QA, fact checking, and dialogue system. Notably, GenRead
achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly
outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0
and +3.9, without retrieving any documents from any external knowledge source.
Lastly, we demonstrate the model performance can be further improved by
combining retrieval and generation. Our code and generated documents can be
found at https://github.com/wyu97/GenRead.",https://github.com/wyu97/GenRead,-1
10678165-3079-4a6d-be34-5cae74d5fbc9,Automated Reinforcement Learning: An Overview,0.296383,"Reinforcement Learning and recently Deep Reinforcement Learning are popular
methods for solving sequential decision making problems modeled as Markov
Decision Processes. RL modeling of a problem and selecting algorithms and
hyper-parameters require careful considerations as different configurations may
entail completely different performances. These considerations are mainly the
task of RL experts; however, RL is progressively becoming popular in other
fields where the researchers and system designers are not RL experts. Besides,
many modeling decisions, such as defining state and action space, size of
batches and frequency of batch updating, and number of timesteps are typically
made manually. For these reasons, automating different components of RL
framework is of great importance and it has attracted much attention in recent
years. Automated RL provides a framework in which different components of RL
including MDP modeling, algorithm selection and hyper-parameter optimization
are modeled and defined automatically. In this article, we explore the
literature and present recent work that can be used in automated RL. Moreover,
we discuss the challenges, open questions and research directions in AutoRL.",None,-1
44390952-34ad-4a2c-9b1c-0d15ab13eefc,Data Selection Curriculum for Neural Machine Translation,0.221039,"Neural Machine Translation (NMT) models are typically trained on
heterogeneous data that are concatenated and randomly shuffled. However, not
all of the training data are equally useful to the model. Curriculum training
aims to present the data to the NMT models in a meaningful order. In this work,
we introduce a two-stage curriculum training framework for NMT where we
fine-tune a base NMT model on subsets of data, selected by both deterministic
scoring using pre-trained methods and online scoring that considers prediction
scores of the emerging NMT model. Through comprehensive experiments on six
language pairs comprising low- and high-resource languages from WMT'21, we have
shown that our curriculum strategies consistently demonstrate better quality
(up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer
updates).",https://github.com/google/sentencepiece,-1
11231e47-c96f-46cc-895a-d7a1ca8ca1f1,Unsupervised multi-branch Capsule for Hyperspectral and LiDAR classification,0.133495,"With the convenient availability of remote sensing data, how to make models
to interpret complex remote sensing data attracts wide attention. In remote
sensing data, hyperspectral images contain spectral information and LiDAR
contains elevation information. Hence, more explorations are warranted to
better fuse the features of different source data. In this paper, we introduce
semantic understanding to dynamically fuse data from two different sources,
extract features of HSI and LiDAR through different capsule network branches
and improve self-supervised loss and random rigid rotation in Canonical Capsule
to a high-dimensional situation. Canonical Capsule computes the capsule
decomposition of objects by permutation-equivariant attention and the process
is self-supervised by training pairs of randomly rotated objects. After fusing
the features of HSI and LiDAR with semantic understanding, the unsupervised
extraction of spectral-spatial-elevation fusion features is achieved. With two
real-world examples of HSI and LiDAR fused, the experimental results show that
the proposed multi-branch high-dimensional canonical capsule algorithm can be
effective for semantic understanding of HSI and LiDAR. It indicates that the
model can extract HSI and LiDAR data features effectively as opposed to
existing models for unsupervised extraction of multi-source RS data.",None,-1
a9ac386b-687a-4098-af67-f9617ddf2d29,Covariance Matrix Adaptation MAP-Annealing,0.43636,"Single-objective optimization algorithms search for the single
highest-quality solution with respect to an objective. Quality diversity (QD)
optimization algorithms, such as Covariance Matrix Adaptation MAP-Elites
(CMA-ME), search for a collection of solutions that are both high-quality with
respect to an objective and diverse with respect to specified measure
functions. However, CMA-ME suffers from three major limitations highlighted by
the QD community: prematurely abandoning the objective in favor of exploration,
struggling to explore flat objectives, and having poor performance for
low-resolution archives. We propose a new quality diversity algorithm,
Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), that addresses all three
limitations. We provide theoretical justifications for the new algorithm with
respect to each limitation. Our theory informs our experiments, which support
the theory and show that CMA-MAE achieves state-of-the-art performance and
robustness.",https://github.com/icaros-usc/pyribs,-1
544f247a-7d6a-471f-bc35-783aa2309acc,"Classification of multi-frequency RF signals by extreme learning, using magnetic tunnel junctions as neurons and synapses",0.269929,"Extracting information from radiofrequency (RF) signals using artificial
neural networks at low energy cost is a critical need for a wide range of
applications from radars to health. These RF inputs are composed of multiples
frequencies. Here we show that magnetic tunnel junctions can process analogue
RF inputs with multiple frequencies in parallel and perform synaptic
operations. Using a backpropagation-free method called extreme learning, we
classify noisy images encoded by RF signals, using experimental data from
magnetic tunnel junctions functioning as both synapses and neurons. We achieve
the same accuracy as an equivalent software neural network. These results are a
key step for embedded radiofrequency artificial intelligence.",None,-1
8269f409-a906-498f-a154-58f80753c6e7,A Data Cartography based MixUp for Pre-trained Language Models,0.150975,"MixUp is a data augmentation strategy where additional samples are generated
during training by combining random pairs of training samples and their labels.
However, selecting random pairs is not potentially an optimal choice. In this
work, we propose TDMixUp, a novel MixUp strategy that leverages Training
Dynamics and allows more informative samples to be combined for generating new
data samples. Our proposed TDMixUp first measures confidence, variability,
(Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al.,
2020) to identify the characteristics of training samples (e.g., as
easy-to-learn or ambiguous samples), and then interpolates these characterized
samples. We empirically validate that our method not only achieves competitive
performance using a smaller subset of the training data compared with strong
baselines, but also yields lower expected calibration error on the pre-trained
language model, BERT, on both in-domain and out-of-domain settings in a wide
range of NLP tasks. We publicly release our code.",https://github.com/seoyeon-p/TDMixUp,-1
74b6a21c-7e9a-4b20-b6f9-5a5d62be4ea0,"1Cademy at Semeval-2022 Task 1: Investigating the Effectiveness of Multilingual, Multitask, and Language-Agnostic Tricks for the Reverse Dictionary Task",0.138362,"This paper describes our system for the SemEval2022 task of matching
dictionary glosses to word embeddings. We focus on the Reverse Dictionary Track
of the competition, which maps multilingual glosses to reconstructed vector
representations. More specifically, models convert the input of sentences to
three types of embeddings: SGNS, Char, and Electra. We propose several
experiments for applying neural network cells, general multilingual and
multitask structures, and language-agnostic tricks to the task. We also provide
comparisons over different types of word embeddings and ablation studies to
suggest helpful strategies. Our initial transformer-based model achieves
relatively low performance. However, trials on different retokenization
methodologies indicate improved performance. Our proposed Elmobased monolingual
model achieves the highest outcome, and its multitask, and multilingual
varieties show competitive results as well.",https://github.com/ravenouse/Revdict_1Cademy,-1
387644ff-93ef-47c0-87ca-c34ea832e1e5,User-Centric Gender Rewriting,0.563221,"In this paper, we define the task of gender rewriting in contexts involving
two users (I and/or You) - first and second grammatical persons with
independent grammatical gender preferences. We focus on Arabic, a
gender-marking morphologically rich language. We develop a multi-step system
that combines the positive aspects of both rule-based and neural rewriting
models. Our results successfully demonstrate the viability of this approach on
a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5
on a blind test set. Our proposed system improves over previous work on the
first-person-only version of this task, by 3.05 absolute increase in M2 F0.5.
We demonstrate a use case of our gender rewriting system by using it to
post-edit the output of a commercial MT system to provide personalized outputs
based on the users' grammatical gender preferences. We make our code, data, and
models publicly available.",https://github.com/CAMeL-Lab/gender-rewriting/,16583
1ed5d240-933c-40c0-9137-2d3e51e39388,Transfer Learning based Search Space Design for Hyperparameter Tuning,0.694246,"The tuning of hyperparameters becomes increasingly important as machine
learning (ML) models have been extensively applied in data mining applications.
Among various approaches, Bayesian optimization (BO) is a successful
methodology to tune hyper-parameters automatically. While traditional methods
optimize each tuning task in isolation, there has been recent interest in
speeding up BO by transferring knowledge across previous tasks. In this work,
we introduce an automatic method to design the BO search space with the aid of
tuning history from past tasks. This simple yet effective approach can be used
to endow many existing BO methods with transfer learning capabilities. In
addition, it enjoys the three advantages: universality, generality, and
safeness. The extensive experiments show that our approach considerably boosts
BO by designing a promising and compact search space instead of using the
entire space, and outperforms the state-of-the-arts on a wide range of
benchmarks, including machine learning and deep learning tuning tasks, and
neural architecture search.",None,10542
ab0255e7-b25b-40f3-bed2-1d241ca2c607,CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation,0.537378,"Empathetic conversation is psychologically supposed to be the result of
conscious alignment and interaction between the cognition and affection of
empathy. However, existing empathetic dialogue models usually consider only the
affective aspect or treat cognition and affection in isolation, which limits
the capability of empathetic response generation. In this work, we propose the
CASE model for empathetic dialogue generation. It first builds upon a
commonsense cognition graph and an emotional concept graph and then aligns the
user's cognition and affection at both the coarse-grained and fine-grained
levels. Through automatic and manual evaluation, we demonstrate that CASE
outperforms state-of-the-art baselines of empathetic dialogues and can generate
more empathetic and informative responses.",https://github.com/jfzhouyoo/CASE,-1
24e91ce4-7034-4a65-8b49-588c20035512,Energy-Based Residual Latent Transport for Unsupervised Point Cloud Completion,0.178686,"Unsupervised point cloud completion aims to infer the whole geometry of a
partial object observation without requiring partial-complete correspondence.
Differing from existing deterministic approaches, we advocate generative
modeling based unsupervised point cloud completion to explore the missing
correspondence. Specifically, we propose a novel framework that performs
completion by transforming a partial shape encoding into a complete one using a
latent transport module, and it is designed as a latent-space energy-based
model (EBM) in an encoder-decoder architecture, aiming to learn a probability
distribution conditioned on the partial shape encoding. To train the latent
code transport module and the encoder-decoder network jointly, we introduce a
residual sampling strategy, where the residual captures the domain gap between
partial and complete shape latent spaces. As a generative model-based
framework, our method can produce uncertainty maps consistent with human
perception, leading to explainable unsupervised point cloud completion. We
experimentally show that the proposed method produces high-fidelity completion
results, outperforming state-of-the-art models by a significant margin.",None,-1
a0020df3-c1ec-4cbb-b415-377f82258332,Development of a rule-based lemmatization algorithm through Finite State Machine for Uzbek language,0.712893,"Lemmatization is one of the core concepts in natural language processing,
thus creating a lemmatization tool is an important task. This paper discusses
the construction of a lemmatization algorithm for the Uzbek language. The main
purpose of the work is to remove affixes of words in the Uzbek language by
means of the finite state machine and to identify a lemma (a word that can be
found in the dictionary) of the word. The process of removing affixes uses a
database of affixes and part of speech knowledge. This lemmatization consists
of the general rules and a part of speech data of the Uzbek language, affixes,
classification of affixes, removing affixes on the basis of the finite state
machine for each class, as well as a definition of this word lemma.",None,-1
199d1841-285a-42ce-9af0-a4194ad2a974,DiffusER: Discrete Diffusion via Edit-based Reconstruction,0.422795,"In text generation, models that generate text from scratch one token at a
time are currently the dominant paradigm. Despite being performant, these
models lack the ability to revise existing text, which limits their usability
in many practical scenarios. We look to address this, with DiffusER (Diffusion
via Edit-based Reconstruction), a new edit-based generative model for text
based on denoising diffusion models -- a class of models that use a Markov
chain of denoising steps to incrementally generate data. DiffusER is not only a
strong generative model in general, rivalling autoregressive models on several
tasks spanning machine translation, summarization, and style transfer; it can
also perform other varieties of generation that standard autoregressive models
are not well-suited for. For instance, we demonstrate that DiffusER makes it
possible for a user to condition generation on a prototype, or an incomplete
sequence, and continue revising based on previous edit steps.",https://github.com/machelreid/diffuser,-1
9ba86610-b414-4b00-aff7-59aa00e0ac65,Discontinuous Constituency and BERT: A Case Study of Dutch,0.169392,"In this paper, we set out to quantify the syntactic capacity of BERT in the
evaluation regime of non-context free patterns, as occurring in Dutch. We
devise a test suite based on a mildly context-sensitive formalism, from which
we derive grammars that capture the linguistic phenomena of control verb
nesting and verb raising. The grammars, paired with a small lexicon, provide us
with a large collection of naturalistic utterances, annotated with verb-subject
pairings, that serve as the evaluation test bed for an attention-based span
selection probe. Our results, backed by extensive analysis, suggest that the
models investigated fail in the implicit acquisition of the dependencies
examined.",https://github.com/gijswijnholds/discontinuous-probing,-1
076a86d6-ab8b-4e2a-a1c7-ccd282b8da4e,InducT-GCN: Inductive Graph Convolutional Networks for Text Classification,0.73218,"Text classification aims to assign labels to textual units by making use of
global information. Recent studies have applied graph neural network (GNN) to
capture the global word co-occurrence in a corpus. Existing approaches require
that all the nodes (training and test) in a graph are present during training,
which are transductive and do not naturally generalise to unseen nodes. To make
those models inductive, they use extra resources, like pretrained word
embedding. However, high-quality resource is not always available and hard to
train. Under the extreme settings with no extra resource and limited amount of
training set, can we still learn an inductive graph-based text classification
model? In this paper, we introduce a novel inductive graph-based text
classification framework, InducT-GCN (InducTive Graph Convolutional Networks
for Text classification). Compared to transductive models that require test
documents in training, we construct a graph based on the statistics of training
documents only and represent document vectors with a weighted sum of word
vectors. We then conduct one-directional GCN propagation during testing. Across
five text classification benchmarks, our InducT-GCN outperformed
state-of-the-art methods that are either transductive in nature or pre-trained
additional resources. We also conducted scalability testing by gradually
increasing the data size and revealed that our InducT-GCN can reduce the time
and space complexity. The code is available on:
https://github.com/usydnlp/InductTGCN.",https://github.com/usydnlp/InductTGCN,3559
63c94076-9488-45e7-88dc-b1d22ec31ef0,Spatio-Temporal Dynamic Graph Relation Learning for Urban Metro Flow Prediction,0.847838,"Urban metro flow prediction is of great value for metro operation scheduling,
passenger flow management and personal travel planning. However, it faces two
main challenges. First, different metro stations, e.g. transfer stations and
non-transfer stations, have unique traffic patterns. Second, it is challenging
to model complex spatio-temporal dynamic relation of metro stations. To address
these challenges, we develop a spatio-temporal dynamic graph relational
learning model (STDGRL) to predict urban metro station flow. First, we propose
a spatio-temporal node embedding representation module to capture the traffic
patterns of different stations. Second, we employ a dynamic graph relationship
learning module to learn dynamic spatial relationships between metro stations
without a predefined graph adjacency matrix. Finally, we provide a
transformer-based long-term relationship prediction module for long-term metro
flow prediction. Extensive experiments are conducted based on metro data in
Beijing, Shanghai, Chongqing and Hangzhou. Experimental results show the
advantages of our method beyond 11 baselines for urban metro flow prediction.",None,-1
6cbaa189-8cb5-4be0-9e4c-7b8b7b981a59,Weakly-supervised Pre-training for 3D Human Pose Estimation via Perspective Knowledge,0.474528,"Modern deep learning-based 3D pose estimation approaches require plenty of 3D
pose annotations. However, existing 3D datasets lack diversity, which limits
the performance of current methods and their generalization ability. Although
existing methods utilize 2D pose annotations to help 3D pose estimation, they
mainly focus on extracting 2D structural constraints from 2D poses, ignoring
the 3D information hidden in the images. In this paper, we propose a novel
method to extract weak 3D information directly from 2D images without 3D pose
supervision. Firstly, we utilize 2D pose annotations and perspective prior
knowledge to generate the relationship of that keypoint is closer or farther
from the camera, called relative depth. We collect a 2D pose dataset (MCPC) and
generate relative depth labels. Based on MCPC, we propose a weakly-supervised
pre-training (WSP) strategy to distinguish the depth relationship between two
points in an image. WSP enables the learning of the relative depth of two
keypoints on lots of in-the-wild images, which is more capable of predicting
depth and generalization ability for 3D human pose estimation. After
fine-tuning on 3D pose datasets, WSP achieves state-of-the-art results on two
widely-used benchmarks.",None,-1
a1656c56-9ffb-4e4f-a9db-4b2dc9fc325b,Overview of Test Coverage Criteria for Test Case Generation from Finite State Machines Modelled as Directed Graphs,0.210364,"Test Coverage criteria are an essential concept for test engineers when
generating the test cases from a System Under Test model. They are routinely
used in test case generation for user interfaces, middleware, and back-end
system parts for software, electronics, or Internet of Things (IoT) systems.
Test Coverage criteria define the number of actions or combinations by which a
system is tested, informally determining a potential ""strength"" of a test set.
As no previous study summarized all commonly used test coverage criteria for
Finite State Machines and comprehensively discussed them regarding their
subsumption, equivalence, or non-comparability, this paper provides this
overview. In this study, 14 most common test coverage criteria and seven of
their synonyms for Finite State Machines defined via a directed graph are
summarized and compared. The results give researchers and industry testing
engineers a helpful overview when setting a software-based or IoT system test
strategy.",None,-1
6d5b8728-2a0b-47c8-ab0f-6638ab6931e1,Instance-based Learning for Knowledge Base Completion,0.278136,"In this paper, we propose a new method for knowledge base completion (KBC):
instance-based learning (IBL). For example, to answer (Jill Biden, lived city,?
), instead of going directly to Washington D.C., our goal is to find Joe Biden,
who has the same lived city as Jill Biden. Through prototype entities, IBL
provides interpretability. We develop theories for modeling prototypes and
combining IBL with translational models. Experiments on various tasks confirmed
the IBL model's effectiveness and interpretability.
  In addition, IBL shed light on the mechanism of rule-based KBC models.
Previous research has generally agreed that rule-based models provide rules
with semantically compatible premises and hypotheses. We challenge this view.
We begin by demonstrating that some logical rules represent {\it instance-based
equivalence} (i.e. prototypes) rather than semantic compatibility. These are
denoted as {\it IBL rules}. Surprisingly, despite occupying only a small
portion of the rule space, IBL rules outperform non-IBL rules in all four
benchmarks. We use a variety of experiments to demonstrate that rule-based
models work because they have the ability to represent instance-based
equivalence via IBL rules. The findings provide new insights of how rule-based
models work and how to interpret their rules.",https://github.com/chenxran/InstanceBasedLearning,-1
9cf99867-c081-4ea5-83db-8b41905c6b4a,Teaching Where to Look: Attention Similarity Knowledge Distillation for Low Resolution Face Recognition,0.817373,"Deep learning has achieved outstanding performance for face recognition
benchmarks, but performance reduces significantly for low resolution (LR)
images. We propose an attention similarity knowledge distillation approach,
which transfers attention maps obtained from a high resolution (HR) network as
a teacher into an LR network as a student to boost LR recognition performance.
Inspired by humans being able to approximate an object's region from an LR
image based on prior knowledge obtained from HR images, we designed the
knowledge distillation loss using the cosine similarity to make the student
network's attention resemble the teacher network's attention. Experiments on
various LR face related benchmarks confirmed the proposed method generally
improved recognition performances on LR settings, outperforming
state-of-the-art results by simply transferring well-constructed attention
maps. The code and pretrained models are publicly available in the
https://github.com/gist-ailab/teaching-where-to-look.",https://github.com/gist-ailab/teaching-where-to-look,-1
4d7465dd-82cb-4ee8-87f1-35628fce5478,UzbekStemmer: Development of a Rule-Based Stemming Algorithm for Uzbek Language,0.409933,"In this paper we present a rule-based stemming algorithm for the Uzbek
language. Uzbek is an agglutinative language, so many words are formed by
adding suffixes, and the number of suffixes is also large. For this reason, it
is difficult to find a stem of words. The methodology is proposed for doing the
stemming of the Uzbek words with an affix stripping approach whereas not
including any database of the normal word forms of the Uzbek language. Word
affixes are classified into fifteen classes and designed as finite state
machines (FSMs) for each class according to morphological rules. We created
fifteen FSMs and linked them together to create the Basic FSM. A lexicon of
affixes in XML format was created and a stemming application for Uzbek words
has been developed based on the FSMs.",https://www.higithub.com/MaksudSharipov/repo/UzbekStemmer,-1
1652d8de-21f8-4af8-b4a1-d4a9f458f59f,Thalamus: a brain-inspired algorithm for biologically-plausible continual learning and disentangled representations,0.105476,"Animals thrive in a constantly changing environment and leverage the temporal
structure to learn well-factorized causal representations. In contrast,
traditional neural networks suffer from forgetting in changing environments and
many methods have been proposed to limit forgetting with different trade-offs.
Inspired by the brain thalamocortical circuit, we introduce a simple algorithm
that uses optimization at inference time to generate internal representations
of the current task dynamically. The algorithm alternates between updating the
model weights and a latent task embedding, allowing the agent to parse the
stream of temporal experience into discrete events and organize learning about
them. On a continual learning benchmark, it achieves competitive end average
accuracy by mitigating forgetting, but importantly, by requiring the model to
adapt through latent updates, it organizes knowledge into flexible structures
with a cognitive interface to control them. Tasks later in the sequence can be
solved through knowledge transfer as they become reachable within the
well-factorized latent space. The algorithm meets many of the desiderata of an
ideal continually learning agent in open-ended environments, and its simplicity
suggests fundamental computations in circuits with abundant feedback control
loops such as the thalamocortical circuits in the brain.",None,-1
2f0ca11c-0740-4fee-82de-548bf35c7129,Learning to Revise References for Faithful Summarization,0.708969,"In real-world scenarios with naturally occurring datasets, reference
summaries are noisy and may contain information that cannot be inferred from
the source text. On large news corpora, removing low quality samples has been
shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora,
filtering is detrimental to performance. To improve reference quality while
retaining all data, we propose a new approach: to selectively re-write
unsupported reference sentences to better reflect source data. We automatically
generate a synthetic dataset of positive and negative revisions by corrupting
supported sentences and learn to revise reference sentences with contrastive
learning. The intensity of revisions is treated as a controllable attribute so
that, at inference, diverse candidates can be over-generated-then-rescored to
balance faithfulness and abstraction. To test our methods, we extract noisy
references from publicly available MIMIC-III discharge summaries for the task
of hospital-course summarization, and vary the data on which models are
trained. According to metrics and human evaluation, models trained on revised
clinical references are much more faithful, informative, and fluent than models
trained on original or filtered data.",https://github.com/amazon-research/summary-reference-revision,30354
3e52b90f-e576-47ae-ac8a-74f4b69af953,BIC: Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency,0.893296,"Twitter bots are automatic programs operated by malicious actors to
manipulate public opinion and spread misinformation. Research efforts have been
made to automatically identify bots based on texts and networks on social
media. Existing methods only leverage texts or networks alone, and while few
works explored the shallow combination of the two modalities, we hypothesize
that the interaction and information exchange between texts and graphs could be
crucial for holistically evaluating bot activities on social media. In
addition, according to a recent survey (Cresci, 2020), Twitter bots are
constantly evolving while advanced bots steal genuine users' tweets and dilute
their malicious content to evade detection. This results in greater
inconsistency across the timeline of novel Twitter bots, which warrants more
attention. In light of these challenges, we propose BIC, a Twitter Bot
detection framework with text-graph Interaction and semantic Consistency.
Specifically, in addition to separately modeling the two modalities on social
media, BIC employs a text-graph interaction module to enable information
exchange across modalities in the learning process. In addition, given the
stealing behavior of novel Twitter bots, BIC proposes to model semantic
consistency in tweets based on attention weights while using it to augment the
decision process. Extensive experiments demonstrate that BIC consistently
outperforms state-of-the-art baselines on two widely adopted datasets. Further
analyses reveal that text-graph interactions and modeling semantic consistency
are essential improvements and help combat bot evolution.",https://github.com/Bjarten/early-stopping-pytorch,-1
ea31f4db-d5c3-42d8-8c06-187d6f2a962a,mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark,0.0698883,"Robust 2004 is an information retrieval benchmark whose large number of
judgments per query make it a reliable evaluation dataset. In this paper, we
present mRobust04, a multilingual version of Robust04 that was translated to 8
languages using Google Translate. We also provide results of three different
multilingual retrievers on this dataset. The dataset is available at
https://huggingface.co/datasets/unicamp-dl/mrobust",None,-1
30270393-a615-4489-89eb-6e9fa2b89478,CarFi: Rider Localization Using Wi-Fi CSI,0.629042,"With the rise of hailing services, people are increasingly relying on shared
mobility (e.g., Uber, Lyft) drivers to pick up for transportation. However,
such drivers and riders have difficulties finding each other in urban areas as
GPS signals get blocked by skyscrapers, in crowded environments (e.g., in
stadiums, airports, and bars), at night, and in bad weather. It wastes their
time, creates a bad user experience, and causes more CO2 emissions due to idle
driving. In this work, we explore the potential of Wi-Fi to help drivers to
determine the street side of the riders. Our proposed system is called CarFi
that uses Wi-Fi CSI from two antennas placed inside a moving vehicle and a
data-driven technique to determine the street side of the rider. By collecting
real-world data in realistic and challenging settings by blocking the signal
with other people and other parked cars, we see that CarFi is 95.44% accurate
in rider-side determination in both line of sight (LoS) and non-line of sight
(nLoS) conditions, and can be run on an embedded GPU in real-time.",None,2064
d26f83ac-ab6f-413c-b978-694d0cf00a18,Sar Ship Detection based on Swin Transformer and Feature Enhancement Feature Pyramid Network,0.784527,"With the booming of Convolutional Neural Networks (CNNs), CNNs such as VGG-16
and ResNet-50 widely serve as backbone in SAR ship detection. However, CNN
based backbone is hard to model long-range dependencies, and causes the lack of
enough high-quality semantic information in feature maps of shallow layers,
which leads to poor detection performance in complicated background and
small-sized ships cases. To address these problems, we propose a SAR ship
detection method based on Swin Transformer and Feature Enhancement Feature
Pyramid Network (FEFPN). Swin Transformer serves as backbone to model
long-range dependencies and generates hierarchical features maps. FEFPN is
proposed to further improve the quality of feature maps by gradually enhancing
the semantic information of feature maps at all levels, especially feature maps
in shallow layers. Experiments conducted on SAR ship detection dataset (SSDD)
reveal the advantage of our proposed methods.",None,-1
00f66d96-6cc4-4acb-8649-63f6a86fdd85,The Importance of Credo in Multiagent Learning,0.189882,"We propose a model for multi-objective optimization, a credo, for agents in a
system that are configured into multiple groups (i.e., teams). Our model of
credo regulates how agents optimize their behavior for the groups they belong
to. We evaluate credo in the context of challenging social dilemmas with
reinforcement learning agents. Our results indicate that the interests of
teammates, or the entire system, are not required to be fully aligned for
achieving globally beneficial outcomes. We identify two scenarios without full
common interest that achieve high equality and significantly higher mean
population rewards compared to when the interests of all agents are aligned.",https://github.com/eugenevinitsky/sequential_social_dilemma_games/,-1
1267bccf-dc64-47f1-9e3a-a20e3aa62f10,Quantifying Harm,0.136782,"In a companion paper (Beckers et al. 2022), we defined a qualitative notion
of harm: either harm is caused, or it is not. For practical applications, we
often need to quantify harm; for example, we may want to choose the lest
harmful of a set of possible interventions. We first present a quantitative
definition of harm in a deterministic context involving a single individual,
then we consider the issues involved in dealing with uncertainty regarding the
context and going from a notion of harm for a single individual to a notion of
""societal harm"", which involves aggregating the harm to individuals. We show
that the ""obvious"" way of doing this (just taking the expected harm for an
individual and then summing the expected harm over all individuals can lead to
counterintuitive or inappropriate answers, and discuss alternatives, drawing on
work from the decision-theory literature.",None,2215
4dcc2878-1fcd-4996-883c-f3b9227c28ee,E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text,0.928236,"Identifying named entities such as a person, location or organization, in
documents can highlight key information to readers. Training Named Entity
Recognition (NER) models requires an annotated data set, which can be a
time-consuming labour-intensive task. Nevertheless, there are publicly
available NER data sets for general English. Recently there has been interest
in developing NER for legal text. However, prior work and experimental results
reported here indicate that there is a significant degradation in performance
when NER methods trained on a general English data set are applied to legal
text. We describe a publicly available legal NER data set, called E-NER, based
on legal company filings available from the US Securities and Exchange
Commission's EDGAR data set. Training a number of different NER algorithms on
the general English CoNLL-2003 corpus but testing on our test collection
confirmed significant degradations in accuracy, as measured by the F1-score, of
between 29.4\% and 60.4\%, compared to training and testing on the E-NER
collection.",https://github.com/terenceau2/E-NER-Dataset,-1
4b20e9fe-9962-4174-a838-bd3f17aa85b2,A Multi-turn Machine Reading Comprehension Framework with Rethink Mechanism for Emotion-Cause Pair Extraction,0.646688,"Emotion-cause pair extraction (ECPE) is an emerging task in emotion cause
analysis, which extracts potential emotion-cause pairs from an emotional
document. Most recent studies use end-to-end methods to tackle the ECPE task.
However, these methods either suffer from a label sparsity problem or fail to
model complicated relations between emotions and causes. Furthermore, they all
do not consider explicit semantic information of clauses. To this end, we
transform the ECPE task into a document-level machine reading comprehension
(MRC) task and propose a Multi-turn MRC framework with Rethink mechanism
(MM-R). Our framework can model complicated relations between emotions and
causes while avoiding generating the pairing matrix (the leading cause of the
label sparsity problem). Besides, the multi-turn structure can fuse explicit
semantic information flow between emotions and causes. Extensive experiments on
the benchmark emotion cause corpus demonstrate the effectiveness of our
proposed framework, which outperforms existing state-of-the-art methods.",https://github.com/zhoucz97/ECPE-MM-R,-1
6fc22088-9dbf-4b60-8488-1bde75773156,JNMR: Joint Non-linear Motion Regression for Video Frame Interpolation,0.175508,"Video frame interpolation (VFI) aims to generate predictive frames by warping
learnable motions from the bidirectional historical references. Most existing
works utilize spatio-temporal semantic information extractor to realize motion
estimation and interpolation modeling. However, they insufficiently consider
the real mechanistic rationality of generated middle motions. In this paper, we
reformulate VFI as a Joint Non-linear Motion Regression (JNMR) strategy to
model the complicated motions of inter-frame. Specifically, the motion
trajectory between the target frame and the multiple reference frames is
regressed by a temporal concatenation of multi-stage quadratic models. ConvLSTM
is adopted to construct this joint distribution of complete motions in temporal
dimension. Moreover, the feature learning network is designed to optimize for
the joint regression modeling. A coarse-to-fine synthesis enhancement module is
also conducted to learn visual dynamics at different resolutions through
repetitive regression and interpolation. Experimental results on VFI show that
the effectiveness and significant improvement of joint motion regression
compared with the state-of-the-art methods. The code is available at
https://github.com/ruhig6/JNMR.",https://github.com/ruhig6/JNMR,-1
2457391f-bfb1-419f-bb54-3495885435bd,What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured,0.197272,"Transformer-based models are now predominant in NLP. They outperform
approaches based on static models in many respects. This success has in turn
prompted research that reveals a number of biases in the language models
generated by transformers. In this paper we utilize this research on biases to
investigate to what extent transformer-based language models allow for
extracting knowledge about object relations (X occurs in Y; X consists of Z;
action A involves using X). To this end, we compare contextualized models with
their static counterparts. We make this comparison dependent on the application
of a number of similarity measures and classifiers. Our results are threefold:
Firstly, we show that the models combined with the different similarity
measures differ greatly in terms of the amount of knowledge they allow for
extracting. Secondly, our results suggest that similarity measures perform much
worse than classifier-based approaches. Thirdly, we show that, surprisingly,
static models perform almost as well as contextualized models -- in some cases
even better.",None,3603
85e91acd-e147-4151-983c-a70f6e8528df,Transductive Decoupled Variational Inference for Few-Shot Classification,0.276674,"The versatility to learn from a handful of samples is the hallmark of human
intelligence. Few-shot learning is an endeavour to transcend this capability
down to machines. Inspired by the promise and power of probabilistic deep
learning, we propose a novel variational inference network for few-shot
classification (coined as TRIDENT) to decouple the representation of an image
into semantic and label latent variables, and simultaneously infer them in an
intertwined fashion. To induce task-awareness, as part of the inference
mechanics of TRIDENT, we exploit information across both query and support
images of a few-shot task using a novel built-in attention-based transductive
feature extraction module (we call AttFEX). Our extensive experimental results
corroborate the efficacy of TRIDENT and demonstrate that, using the simplest of
backbones, it sets a new state-of-the-art in the most commonly adopted datasets
miniImageNet and tieredImageNet (offering up to 4% and 5% improvements,
respectively), as well as for the recent challenging cross-domain miniImagenet
--> CUB scenario offering a significant margin (up to 20% improvement) beyond
the best existing cross-domain baselines. Code and experimentation can be found
in our GitHub repository: https://github.com/anujinho/trident",https://github.com/anujinho/trident,-1
33690b7a-0b2a-45d2-a9a7-ef85984464a3,Housekeep: Tidying Virtual Households using Commonsense Reasoning,0.719296,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the
home for embodied AI. In Housekeep, an embodied agent must tidy a house by
rearranging misplaced objects without explicit instructions specifying which
objects need to be rearranged. Instead, the agent must learn from and is
evaluated against human preferences of which objects belong where in a tidy
house. Specifically, we collect a dataset of where humans typically place
objects in tidy and untidy houses constituting 1799 objects, 268 object
categories, 585 placements, and 105 rooms. Next, we propose a modular baseline
approach for Housekeep that integrates planning, exploration, and navigation.
It leverages a fine-tuned large language model (LLM) trained on an internet
text corpus for effective planning. We show that our baseline agent generalizes
to rearranging unseen objects in unknown environments. See our webpage for more
details: https://yashkant.github.io/housekeep/",None,-1
1b4332d3-2407-4461-9ce1-801e23388bf9,CSL: A Large-scale Chinese Scientific Literature Dataset,0.923326,"Scientific literature serves as a high-quality corpus, supporting a lot of
Natural Language Processing (NLP) research. However, existing datasets are
centered around the English language, which restricts the development of
Chinese scientific NLP. In this work, we present CSL, a large-scale Chinese
Scientific Literature dataset, which contains the titles, abstracts, keywords
and academic fields of 396k papers. To our knowledge, CSL is the first
scientific document dataset in Chinese. The CSL can serve as a Chinese corpus.
Also, this semi-structured data is a natural annotation that can constitute
many supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the
performance of models across scientific domain tasks, i.e., summarization,
keyword generation and text classification. We analyze the behavior of existing
text-to-text models on the evaluation tasks and reveal the challenges for
Chinese scientific NLP tasks, which provides a valuable reference for future
research. Data and code are available at https://github.com/ydli-ai/CSL",https://github.com/ydli-ai/CSL,57782
58e4acb7-ffc2-487f-8d67-120cc280502c,Neural apparent BRDF fields for multiview photometric stereo,0.44494,"We propose to tackle the multiview photometric stereo problem using an
extension of Neural Radiance Fields (NeRFs), conditioned on light source
direction. The geometric part of our neural representation predicts surface
normal direction, allowing us to reason about local surface reflectance. The
appearance part of our neural representation is decomposed into a neural
bidirectional reflectance function (BRDF), learnt as part of the fitting
process, and a shadow prediction network (conditioned on light source
direction) allowing us to model the apparent BRDF. This balance of learnt
components with inductive biases based on physical image formation models
allows us to extrapolate far from the light source and viewer directions
observed during training. We demonstrate our approach on a multiview
photometric stereo benchmark and show that competitive performance can be
obtained with the neural density representation of a NeRF.",None,-1
4a3f3218-9de6-4796-94df-6c743f654c18,Robust Task-Oriented Dialogue Generation with Contrastive Pre-training and Adversarial Filtering,0.217116,"Data artifacts incentivize machine learning models to learn non-transferable
generalizations by taking advantage of shortcuts in the data, and there is
growing evidence that data artifacts play a role for the strong results that
deep learning models achieve in recent natural language processing benchmarks.
In this paper, we focus on task-oriented dialogue and investigate whether
popular datasets such as MultiWOZ contain such data artifacts. We found that by
only keeping frequent phrases in the training examples, state-of-the-art models
perform similarly compared to the variant trained with full data, suggesting
they exploit these spurious correlations to solve the task. Motivated by this,
we propose a contrastive learning based framework to encourage the model to
ignore these cues and focus on learning generalisable patterns. We also
experiment with adversarial filtering to remove ""easy"" training instances so
that the model would focus on learning from the ""harder"" instances. We conduct
a number of generalization experiments -- e.g., cross-domain/dataset and
adversarial tests -- to assess the robustness of our approach and found that it
works exceptionally well.",None,-1
391c379d-03b6-4b4e-8ac9-7066853d21f1,Bilinear value networks,0.41405,"The dominant framework for off-policy multi-goal reinforcement learning
involves estimating goal conditioned Q-value function. When learning to achieve
multiple goals, data efficiency is intimately connected with the generalization
of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a,
g) using monolithic neural networks. To improve the generalization of the
Q-function, we propose a bilinear decomposition that represents the Q-value via
a low-rank approximation in the form of a dot product between two vector
fields. The first vector field, f(s, a), captures the environment's local
dynamics at the state s; whereas the second component, {\phi}(s, g), captures
the global relationship between the current state and the goal. We show that
our bilinear decomposition scheme substantially improves data efficiency, and
has superior transfer to out-of-distribution goals compared to prior methods.
Empirical evidence is provided on the simulated Fetch robot task-suite and
dexterous manipulation with a Shadow hand.",https://github.com/Improbable-AI/bvn,-1
226d764b-efc9-4997-9adb-7f6851d12dfe,A Classical-Quantum Convolutional Neural Network for Detecting Pneumonia from Chest Radiographs,0.221785,"While many quantum computing techniques for machine learning have been
proposed, their performance on real-world datasets remains to be studied. In
this paper, we explore how a variational quantum circuit could be integrated
into a classical neural network for the problem of detecting pneumonia from
chest radiographs. We substitute one layer of a classical convolutional neural
network with a variational quantum circuit to create a hybrid neural network.
We train both networks on an image dataset containing chest radiographs and
benchmark their performance. To mitigate the influence of different sources of
randomness in network training, we sample the results over multiple rounds. We
show that the hybrid network outperforms the classical network on different
performance measures, and that these improvements are statistically
significant. Our work serves as an experimental demonstration of the potential
of quantum computing to significantly improve neural network performance for
real-world, non-trivial problems relevant to society and industry.",None,-1
205845e4-c425-4ccd-a0de-f4ac56a856f0,Table-based Fact Verification with Self-adaptive Mixture of Experts,0.876011,"The table-based fact verification task has recently gained widespread
attention and yet remains to be a very challenging problem. It inherently
requires informative reasoning over natural language together with different
numerical and logical reasoning on tables (e.g., count, superlative,
comparative). Considering that, we exploit mixture-of-experts and present in
this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE).
Specifically, we have developed a mixture-of-experts neural network to
recognize and execute different types of reasoning -- the network is composed
of multiple experts, each handling a specific part of the semantics for
reasoning, whereas a management module is applied to decide the contribution of
each expert network to the verification result. A self-adaptive method is
developed to teach the management module combining results of different experts
more efficiently without external knowledge. The experimental results
illustrate that our framework achieves 85.1% accuracy on the benchmark dataset
TabFact, comparable with the previous state-of-the-art models. We hope our
framework can serve as a new baseline for table-based verification. Our code is
available at https://github.com/THUMLP/SaMoE.",https://github.com/THUMLP/SaMoE,2921
ea1ea9b6-16d1-4934-a394-09a2b0bde4a9,BLIND: Bias Removal With No Demographics,0.661212,"Models trained on real-world data tend to imitate and amplify social biases.
Common methods to mitigate biases require prior information on the types of
biases that should be mitigated (e.g., gender or racial bias) and the social
groups associated with each data sample. In this work, we introduce BLIND, a
method for bias removal with no prior knowledge of the demographics in the
dataset. While training a model on a downstream task, BLIND detects biased
samples using an auxiliary model that predicts the main model's success, and
down-weights those samples during the training process. Experiments with racial
and gender biases in sentiment classification and occupation classification
tasks demonstrate that BLIND mitigates social biases without relying on a
costly demographic annotation process. Our method is competitive with other
methods that require demographic information and sometimes even surpasses them.",https://github.com/technion-cs-nlp/BLIND,-1
1e1bb6a6-8dc9-493c-8d4c-147922395cd9,YFACC: A Yorb speech-image dataset for cross-lingual keyword localisation through visual grounding,0.459567,"Visually grounded speech (VGS) models are trained on images paired with
unlabelled spoken captions. Such models could be used to build speech systems
in settings where it is impossible to get labelled data, e.g. for documenting
unwritten languages. However, most VGS studies are in English or other
high-resource languages. This paper attempts to address this shortcoming. We
collect and release a new single-speaker dataset of audio captions for 6k
Flickr images in Yor\`ub\'a -- a real low-resource language spoken in Nigeria.
We train an attention-based VGS model where images are automatically tagged
with English visual labels and paired with Yor\`ub\'a utterances. This enables
cross-lingual keyword localisation: a written English query is detected and
located in Yor\`ub\'a speech. To quantify the effect of the smaller dataset, we
compare to English systems trained on similar and more data. We hope that this
new dataset will stimulate research in the use of VGS models for real
low-resource languages.",https://github.com/kayodeolaleye/keyword_localisation_speech,-1
ca8b1921-d49d-48f8-b9b7-32f3d133a676,Points2NeRF: Generating Neural Radiance Fields from 3D point cloud,0.294958,"Contemporary registration devices for 3D visual information, such as LIDARs
and various depth cameras, capture data as 3D point clouds. In turn, such
clouds are challenging to be processed due to their size and complexity.
Existing methods address this problem by fitting a mesh to the point cloud and
rendering it instead. This approach, however, leads to the reduced fidelity of
the resulting visualization and misses color information of the objects crucial
in computer graphics applications. In this work, we propose to mitigate this
challenge by representing 3D objects as Neural Radiance Fields (NeRFs). We
leverage a hypernetwork paradigm and train the model to take a 3D point cloud
with the associated color values and return a NeRF network's weights that
reconstruct 3D objects from input 2D images. Our method provides efficient 3D
object representation and offers several advantages over the existing
approaches, including the ability to condition NeRFs and improved
generalization beyond objects seen in training. The latter we also confirmed in
the results of our empirical evaluation.",https://github.com/gmum/points2nerf,-1
0dd5136e-aaae-4aab-b0b7-977b098bc38e,Interacting Hand-Object Pose Estimation via Dense Mutual Attention,0.64656,"3D hand-object pose estimation is the key to the success of many computer
vision applications. The main focus of this task is to effectively model the
interaction between the hand and an object. To this end, existing works either
rely on interaction constraints in a computationally-expensive iterative
optimization, or consider only a sparse correlation between sampled hand and
object keypoints. In contrast, we propose a novel dense mutual attention
mechanism that is able to model fine-grained dependencies between the hand and
the object. Specifically, we first construct the hand and object graphs
according to their mesh structures. For each hand node, we aggregate features
from every object node by the learned attention and vice versa for each object
node. Thanks to such dense mutual attention, our method is able to produce
physically plausible poses with high quality and real-time inference speed.
Extensive quantitative and qualitative experiments on large benchmark datasets
show that our method outperforms state-of-the-art methods. The code is
available at https://github.com/rongakowang/DenseMutualAttention.git.",https://github.com/rongakowang/DenseMutualAttention.git,24
09f1ce99-ed5c-4c73-8eb4-ff7c053a9ea8,Comparison of biomedical relationship extraction methods and models for knowledge graph creation,0.868319,"Biomedical research is growing at such an exponential pace that scientists,
researchers, and practitioners are no more able to cope with the amount of
published literature in the domain. The knowledge presented in the literature
needs to be systematized in such a way that claims and hypotheses can be easily
found, accessed, and validated. Knowledge graphs can provide such a framework
for semantic knowledge representation from literature. However, in order to
build a knowledge graph, it is necessary to extract knowledge as relationships
between biomedical entities and normalize both entities and relationship types.
In this paper, we present and compare few rule-based and machine learning-based
(Naive Bayes, Random Forests as examples of traditional machine learning
methods and DistilBERT, PubMedBERT, T5 and SciFive-based models as examples of
modern deep learning transformers) methods for scalable relationship extraction
from biomedical literature, and for the integration into the knowledge graphs.
We examine how resilient are these various methods to unbalanced and fairly
small datasets. Our experiments show that transformer-based models handle well
both small (due to pre-training on a large dataset) and unbalanced datasets.
The best performing model was the PubMedBERT-based model fine-tuned on balanced
data, with a reported F1-score of 0.92. DistilBERT-based model followed with
F1-score of 0.89, performing faster and with lower resource requirements.
BERT-based models performed better then T5-based generative models.",None,-1
4e1be1eb-0741-4bd6-b2f7-86d3ef45c7c4,The Glass Ceiling of Automatic Evaluation in Natural Language Generation,0.202771,"Automatic evaluation metrics capable of replacing human judgments are
critical to allowing fast development of new methods. Thus, numerous research
efforts have focused on crafting such metrics. In this work, we take a step
back and analyze recent progress by comparing the body of existing automatic
metrics and human metrics altogether. As metrics are used based on how they
rank systems, we compare metrics in the space of system rankings. Our extensive
statistical analysis reveals surprising findings: automatic metrics -- old and
new -- are much more similar to each other than to humans. Automatic metrics
are not complementary and rank systems similarly. Strikingly, human metrics
predict each other much better than the combination of all automatic metrics
used to predict a human metric. It is surprising because human metrics are
often designed to be independent, to capture different aspects of quality, e.g.
content fidelity or readability. We provide a discussion of these findings and
recommendations for future work in the field of evaluation.",https://github.com/xxx,-1
a36f896c-4dbd-416f-b841-6818b3a3182f,Towards Responsible AI for Financial Transactions,0.230077,"The application of AI in finance is increasingly dependent on the principles
of responsible AI. These principles - explainability, fairness, privacy,
accountability, transparency and soundness form the basis for trust in future
AI systems. In this study, we address the first principle by providing an
explanation for a deep neural network that is trained on a mixture of
numerical, categorical and textual inputs for financial transaction
classification. The explanation is achieved through (1) a feature importance
analysis using Shapley additive explanations (SHAP) and (2) a hybrid approach
of text clustering and decision tree classifiers. We then test the robustness
of the model by exposing it to a targeted evasion attack, leveraging the
knowledge we gained about the model through the extracted explanation.",None,-1
9eddd61c-0380-4749-95a4-845534ee0377,L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT models,0.919792,"Social media platforms are used by a large number of people prominently to
express their thoughts and opinions. However, these platforms have contributed
to a substantial amount of hateful and abusive content as well. Therefore, it
is important to curb the spread of hate speech on these platforms. In India,
Marathi is one of the most popular languages used by a wide audience. In this
work, we present L3Cube-MahaHate, the first major Hate Speech Dataset in
Marathi. The dataset is curated from Twitter, annotated manually. Our dataset
consists of over 25000 distinct tweets labeled into four major classes i.e
hate, offensive, profane, and not. We present the approaches used for
collecting and annotating the data and the challenges faced during the process.
Finally, we present baseline classification results using deep learning models
based on CNN, LSTM, and Transformers. We explore mono-lingual and multi-lingual
variants of BERT like MahaBERT, IndicBERT, mBERT, and xlm-RoBERTa and show that
mono-lingual models perform better than their multi-lingual counterparts. The
MahaBERT model provides the best results on L3Cube-MahaHate Corpus. The data
and models are available at https://github.com/l3cube-pune/MarathiNLP .",https://github.com/l3cube-pune/MarathiNLP,-1
0dd78bcb-1eae-48dd-a9a1-d87a94a944fc,On three types of $L$-fuzzy $$-covering-based rough sets,0.372911,"In this paper, we mainly construct three types of $L$-fuzzy
$\beta$-covering-based rough set models and study the axiom sets, matrix
representations and interdependency of these three pairs of $L$-fuzzy
$\beta$-covering-based rough approximation operators. Firstly, we propose three
pairs of $L$-fuzzy $\beta$-covering-based rough approximation operators by
introducing the concepts such as $\beta$-degree of intersection and
$\beta$-subsethood degree, which are generalizations of degree of intersection
and subsethood degree, respectively. And then, the axiom set for each of these
$L$-fuzzy $\beta$-covering-based rough approximation operator is investigated.
Thirdly, we give the matrix representations of three types of $L$-fuzzy
$\beta$-covering-based rough approximation operators, which make it valid to
calculate the $L$-fuzzy $\beta$-covering-based lower and upper rough
approximation operators through operations on matrices. Finally, the
interdependency of the three pairs of rough approximation operators based on
$L$-fuzzy $\beta$-covering is studied by using the notion of reducible elements
and independent elements. In other words, we present the necessary and
sufficient conditions under which two $L$-fuzzy $\beta$-coverings can generate
the same lower and upper rough approximation operations.",None,7646
082b69dc-5440-4c8a-af26-b87c3c7b58e8,Extending Process Discovery with Model Complexity Optimization and Cyclic States Identification: Application to Healthcare Processes,0.0467913,"Within Process mining, discovery techniques had made it possible to construct
business process models automatically from event logs. However, results often
do not achieve the balance between model complexity and its fitting accuracy,
so there is a need for manual model adjusting. The paper presents an approach
to process mining providing semi-automatic support to model optimization based
on the combined assessment of the model complexity and fitness. To balance
between the two ingredients, a model simplification approach is proposed, which
essentially abstracts the raw model at the desired granularity. Additionally,
we introduce a concept of meta-states, a cycle collapsing in the model, which
can potentially simplify the model and interpret it. We aim to demonstrate the
capabilities of the technological solution using three datasets from different
applications in the healthcare domain. They are remote monitoring process for
patients with arterial hypertension and workflows of healthcare workers during
the COVID-19 pandemic. A case study also investigates the use of various
complexity measures and different ways of solution application providing
insights on better practices in improving interpretability and
complexity/fitness balance in process models.",None,12
778f3c43-91b7-45a2-a632-9f932c45e7e6,Enhanced Bi-directional Motion Estimation for Video Frame Interpolation,0.83706,"We present a novel simple yet effective algorithm for motion-based video
frame interpolation. Existing motion-based interpolation methods typically rely
on a pre-trained optical flow model or a U-Net based pyramid network for motion
estimation, which either suffer from large model size or limited capacity in
handling complex and large motion cases. In this work, by carefully integrating
intermediateoriented forward-warping, lightweight feature encoder, and
correlation volume into a pyramid recurrent framework, we derive a compact
model to simultaneously estimate the bidirectional motion between input frames.
It is 15 times smaller in size than PWC-Net, yet enables more reliable and
flexible handling of challenging motion cases. Based on estimated
bi-directional motion, we forward-warp input frames and their context features
to intermediate frame, and employ a synthesis network to estimate the
intermediate frame from warped representations. Our method achieves excellent
performance on a broad range of video frame interpolation benchmarks. Code and
trained models are available at \url{https://github.com/srcn-ivl/EBME}.",https://github.com/srcn-ivl/EBME,-1
fa0a7fa4-210f-42d9-8fc1-05ab69e9061d,Motif-topology and Reward-learning improved Spiking Neural Network for Efficient Multi-sensory Integration,0.518163,"Network architectures and learning principles are key in forming complex
functions in artificial neural networks (ANNs) and spiking neural networks
(SNNs). SNNs are considered the new-generation artificial networks by
incorporating more biological features than ANNs, including dynamic spiking
neurons, functionally specified architectures, and efficient learning
paradigms. In this paper, we propose a Motif-topology and Reward-learning
improved SNN (MR-SNN) for efficient multi-sensory integration. MR-SNN contains
13 types of 3-node Motif topologies which are first extracted from independent
single-sensory learning paradigms and then integrated for multi-sensory
classification. The experimental results showed higher accuracy and stronger
robustness of the proposed MR-SNN than other conventional SNNs without using
Motifs. Furthermore, the proposed reward learning paradigm was biologically
plausible and can better explain the cognitive McGurk effect caused by
incongruent visual and auditory sensory signals.",https://github.com/thomasaimondy/Motif-SNN,-1
a12920e9-1684-4ecf-8ab5-dad097bd35f0,Towards automatic generation of Piping and Instrumentation Diagrams (P&IDs) with Artificial Intelligence,0.736403,"Developing Piping and Instrumentation Diagrams (P&IDs) is a crucial step
during the development of chemical processes. Currently, this is a tedious,
manual, and time-consuming task. We propose a novel, completely data-driven
method for the prediction of control structures. Our methodology is inspired by
end-to-end transformer-based human language translation models. We cast the
control structure prediction as a translation task where Process Flow Diagrams
(PFDs) are translated to P&IDs. To use established transformer-based language
translation models, we represent the P&IDs and PFDs as strings using our
recently proposed SFILES 2.0 notation. Model training is performed in a
transfer learning approach. Firstly, we pre-train our model using generated
P&IDs to learn the grammatical structure of the process diagrams. Thereafter,
the model is fine-tuned leveraging transfer learning on real P&IDs. The model
achieved a top-5 accuracy of 74.8% on 10,000 generated P&IDs and 89.2% on
100,000 generated P&IDs. These promising results show great potential for
AI-assisted process engineering. The tests on a dataset of 312 real P&IDs
indicate the need of a larger P&IDs dataset for industry applications.",None,-1
02fbc889-b510-4674-a13c-8e706ca25941,Incorporating Multi-armed Bandit with Local Search for MaxSAT,0.284802,"Partial MaxSAT (PMS) and Weighted PMS (WPMS) are two practical
generalizations of the MaxSAT problem. In this paper, we propose a local search
algorithm for these problems, called BandHS, which applies two multi-armed
bandits to guide the search directions when escaping local optima. One bandit
is combined with all the soft clauses to help the algorithm select to satisfy
appropriate soft clauses, and the other bandit with all the literals in hard
clauses to help the algorithm select appropriate literals to satisfy the hard
clauses. These two bandits can improve the algorithm's search ability in both
feasible and infeasible solution spaces. We further propose an initialization
method for (W)PMS that prioritizes both unit and binary clauses when producing
the initial solutions. Extensive experiments demonstrate the excellent
performance and generalization capability of our proposed methods, that greatly
boost the state-of-the-art local search algorithm, SATLike3.0, and the
state-of-the-art SAT-based incomplete solver, NuWLS-c.",https://github.com/JHL-HUST/BandHS/,5393
92cc7aa9-ff98-4c56-aae7-6acc8ec54c0b,A Multimodal Corpus for Emotion Recognition in Sarcasm,0.757536,"While sentiment and emotion analysis have been studied extensively, the
relationship between sarcasm and emotion has largely remained unexplored. A
sarcastic expression may have a variety of underlying emotions. For example, ""I
love being ignored"" belies sadness, while ""my mobile is fabulous with a battery
backup of only 15 minutes!"" expresses frustration. Detecting the emotion behind
a sarcastic expression is non-trivial yet an important task. We undertake the
task of detecting the emotion in a sarcastic statement, which to the best of
our knowledge, is hitherto unexplored. We start with the recently released
multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions.
We identify and correct 343 incorrect emotion labels (out of 690). We double
the size of the dataset, label it with emotions along with valence and arousal
which are important indicators of emotional intensity. Finally, we label each
sarcastic utterance with one of the four sarcasm types-Propositional, Embedded,
Likeprefixed and Illocutionary, with the goal of advancing sarcasm detection
research. Exhaustive experimentation with multimodal (text, audio, and video)
fusion models establishes a benchmark for exact emotion recognition in sarcasm
and outperforms the state-of-art sarcasm detection. We release the dataset
enriched with various annotations and the code for research purposes:
https://github.com/apoorva-nunna/MUStARD_Plus_Plus",https://github.com/apoorva-nunna/MUStARD_Plus_Plus,-1
bedb5828-1b56-48a2-9dfb-deb0d7dbcae3,Truth Set Algebra: A New Way to Prove Undefinability,0.872864,"The article proposes a new technique for proving the undefinability of
logical connectives through each other and illustrates the technique with
several examples. Some of the obtained results are new proofs of the existing
theorems, others are original to this work.",None,-1
d00e37d3-d9a1-4176-b4b8-5b93e7299b37,What does Transformer learn about source code?,0.271273,"In the field of source code processing, the transformer-based representation
models have shown great powerfulness and have achieved state-of-the-art (SOTA)
performance in many tasks. Although the transformer models process the
sequential source code, pieces of evidence show that they may capture the
structural information (\eg, in the syntax tree, data flow, control flow, \etc)
as well. We propose the aggregated attention score, a method to investigate the
structural information learned by the transformer. We also put forward the
aggregated attention graph, a new way to extract program graphs from the
pre-trained models automatically. We measure our methods from multiple
perspectives. Furthermore, based on our empirical findings, we use the
automatically extracted graphs to replace those ingenious manual designed
graphs in the Variable Misuse task. Experimental results show that the semantic
graphs we extracted automatically are greatly meaningful and effective, which
provide a new perspective for us to understand and use the information
contained in the model.",None,-1
65ec3c18-ee97-4ab7-baf0-b0a8387a9c86,Portrait Segmentation Using Deep Learning,0.0381225,"A portrait is a painting, drawing, photograph, or engraving of a person,
especially one depicting only the face or head and shoulders. In the digital
world the portrait of a person is captured by having the person as a subject in
the image and capturing the image of the person such that the background is
blurred. DSLRs generally do it by reducing the aperture to focus on very close
regions of interest and automatically blur the background. In this paper I have
come up with a novel approach to replicate the portrait mode from DSLR using
any smartphone to generate high quality portrait images.",https://github.com/matterport/Mask_RCNN,-1
2eeade2d-2b54-4a94-884c-ec36fce54aba,PSDoodle: Searching for App Screens via Interactive Sketching,0.592237,"Keyword-based mobile screen search does not account for screen content and
fails to operate as a universal tool for all levels of users. Visual searching
(e.g., image, sketch) is structured and easy to adopt. Current visual search
approaches count on a complete screen and are therefore slow and tedious.
PSDoodle employs a deep neural network to recognize partial screen element
drawings instantly on a digital drawing interface and shows results in
real-time. PSDoodle is the first tool that utilizes partial sketches and
searches for screens in an interactive iterative way. PSDoodle supports
different drawing styles and retrieves search results that are relevant to the
user's sketch query. A short video demonstration is available online at:
https://youtu.be/3cVLHFm5pY4",https://github.com/soumikmohianuta/PSDoodle,2830
0dc3429e-a395-4072-972b-14cfe60887e4,Cross Attention Based Style Distribution for Controllable Person Image Synthesis,0.900514,"Controllable person image synthesis task enables a wide range of applications
through explicit control over body pose and appearance. In this paper, we
propose a cross attention based style distribution module that computes between
the source semantic styles and target pose for pose transfer. The module
intentionally selects the style represented by each semantic and distributes
them according to the target pose. The attention matrix in cross attention
expresses the dynamic similarities between the target pose and the source
styles for all semantics. Therefore, it can be utilized to route the color and
texture from the source image, and is further constrained by the target parsing
map to achieve a clearer objective. At the same time, to encode the source
appearance accurately, the self attention among different semantic styles is
also added. The effectiveness of our model is validated quantitatively and
qualitatively on pose transfer and virtual try-on tasks.",https://github.com/xyzhouo/CASD,-1
523d5a1a-2f0a-4684-af88-6191dcf9f717,Probing Causes of Hallucinations in Neural Machine Translations,0.0834535,"Hallucination, one kind of pathological translations that bothers Neural
Machine Translation, has recently drawn much attention. In simple terms,
hallucinated translations are fluent sentences but barely related to source
inputs. Arguably, it remains an open problem how hallucination occurs. In this
paper, we propose to use probing methods to investigate the causes of
hallucinations from the perspective of model architecture, aiming to avoid such
problems in future architecture designs. By conducting experiments over various
NMT datasets, we find that hallucination is often accompanied by the deficient
encoder, especially embeddings, and vulnerable cross-attentions, while,
interestingly, cross-attention mitigates some errors caused by the encoder.",None,-1
76da099d-c212-447f-a143-ec9270d19f43,Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks,0.428203,"Facial expression in-the-wild is essential for various interactive computing
domains. Especially, ""Learning from Synthetic Data"" (LSD) is an important topic
in the facial expression recognition task. In this paper, we propose a
multi-task learning-based facial expression recognition approach which consists
of emotion and appearance learning branches that can share all face
information, and present preliminary results for the LSD challenge introduced
in the 4th affective behavior analysis in-the-wild (ABAW) competition. Our
method achieved the mean F1 score of 0.71.",None,-1
522b5895-8730-49fc-ba67-77f8c966dac1,Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification,0.675348,"To learn camera-view invariant features for person Re-IDentification (Re-ID),
the cross-camera image pairs of each person play an important role. However,
such cross-view training samples could be unavailable under the ISolated Camera
Supervised (ISCS) setting, e.g., a surveillance system deployed across distant
scenes. To handle this challenging problem, a new pipeline is introduced by
synthesizing the cross-camera samples in the feature space for model training.
Specifically, the feature encoder and generator are end-to-end optimized under
a novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint
learning procedure raises concern on the stability of generative model
training. Therefore, a new feature generator, $\sigma$-Regularized Conditional
Variational Autoencoder ($\sigma$-Reg.~CVAE), is proposed with theoretical and
experimental analysis on its robustness. Extensive experiments on two ISCS
person Re-ID datasets demonstrate the superiority of our CCSFG to the
competitors.",https://github.com/ftd-Wuchao/CCSFG,-1
ece17447-ee85-4bd9-be8c-81878fd2d070,Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis,0.95448,"Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a
specific aspect in the given sentence. While pre-trained language models such
as BERT have achieved great success, incorporating dynamic semantic changes
into ABSA remains challenging. To this end, in this paper, we propose to
address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method
designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we
first take the Stack-BERT layers as a primary encoder to grasp the overall
semantic of the sentence and then fine-tune it by incorporating a lightweight
Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention
to a small region of the sentences at each step and re-weigh the vitally
important words for better aspect-aware sentiment understanding. Finally,
experimental results on three benchmark datasets demonstrate the effectiveness
and the rationality of our proposed model and provide good interpretable
insights for future semantic modeling.",None,-1
cceac6cc-8a49-4ea9-aa19-8b448655490b,Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET,0.801211,"Neural metrics have achieved impressive correlation with human judgements in
the evaluation of machine translation systems, but before we can safely
optimise towards such metrics, we should be aware of (and ideally eliminate)
biases toward bad translations that receive high scores. Our experiments show
that sample-based Minimum Bayes Risk decoding can be used to explore and
quantify such weaknesses. When applying this strategy to COMET for en-de and
de-en, we find that COMET models are not sensitive enough to discrepancies in
numbers and named entities. We further show that these biases are hard to fully
remove by simply training on additional synthetic data and release our code and
data for facilitating further experiments.",https://github.com/Unbabel/COMET,-1
be893813-1bca-4bb2-b432-bab4c507292c,Semantic Guided Level-Category Hybrid Prediction Network for Hierarchical Image Classification,0.163349,"Hierarchical classification (HC) assigns each object with multiple labels
organized into a hierarchical structure. The existing deep learning based HC
methods usually predict an instance starting from the root node until a leaf
node is reached. However, in the real world, images interfered by noise,
occlusion, blur, or low resolution may not provide sufficient information for
the classification at subordinate levels. To address this issue, we propose a
novel semantic guided level-category hybrid prediction network (SGLCHPN) that
can jointly perform the level and category prediction in an end-to-end manner.
SGLCHPN comprises two modules: a visual transformer that extracts feature
vectors from the input images, and a semantic guided cross-attention module
that uses categories word embeddings as queries to guide learning
category-specific representations. In order to evaluate the proposed method, we
construct two new datasets in which images are at a broad range of quality and
thus are labeled to different levels (depths) in the hierarchy according to
their individual quality. Experimental results demonstrate the effectiveness of
our proposed HC method.",None,-1
f074ee75-3059-4b78-8529-ac8e50691251,Cross-Lingual Speaker Identification Using Distant Supervision,0.0734195,"Speaker identification, determining which character said each utterance in
literary text, benefits many downstream tasks. Most existing approaches use
expert-defined rules or rule-based features to directly approach this task, but
these approaches come with significant drawbacks, such as lack of contextual
reasoning and poor cross-lingual generalization. In this work, we propose a
speaker identification framework that addresses these issues. We first extract
large-scale distant supervision signals in English via general-purpose tools
and heuristics, and then apply these weakly-labeled instances with a focus on
encouraging contextual reasoning to train a cross-lingual language model. We
show that the resulting model outperforms previous state-of-the-art methods on
two English speaker identification benchmarks by up to 9% in accuracy and 5%
with only distant supervision, as well as two Chinese speaker identification
datasets by up to 4.7%.",https://github.com/Slash0BZ/speaker-identification,-1
f040203b-767d-4970-b7a0-1771dbcaa7ea,Exploring Effective Information Utilization in Multi-Turn Topic-Driven Conversations,0.416355,"Conversations are always related to certain topics. However, it is
challenging to fuse dialogue history and topic information from various sources
at the same time in current dialogue generation models because of the input
length limit of pre-trained language models (PLMs). In order to expand the
information that PLMs can utilize, we encode topic and dialogue history
information using certain prompts with multiple channels of Fusion-in-Decoder
(FiD) and explore the influence of three different channel settings. In this
paper, our experiments focus on a specific Chinese dataset named NaturalConv,
where the conversation revolves around a piece of recent news. We thoroughly
compared different dialogue models and different FiD channel settings.
Empirical results show that by combining our proposed whole passage channel
with additional history channel, our methods can achieve competitive
performance on NaturalConv, making it possible to encode various information
from excessively long texts.",https://github.com/ZhuiyiTechnology/t5-pegasus,-1
3a986eb8-d807-4c64-848b-108ae350a980,Jam or Cream First? Modeling Ambiguity in Neural Machine Translation with SCONES,0.459886,"The softmax layer in neural machine translation is designed to model the
distribution over mutually exclusive tokens. Machine translation, however, is
intrinsically uncertain: the same source sentence can have multiple
semantically equivalent translations. Therefore, we propose to replace the
softmax activation with a multi-label classification layer that can model
ambiguity more effectively. We call our loss function Single-label Contrastive
Objective for Non-Exclusive Sequences (SCONES). We show that the multi-label
output layer can still be trained on single reference training data using the
SCONES loss function. SCONES yields consistent BLEU score gains across six
translation directions, particularly for medium-resource language pairs and
small beam sizes. By using smaller beam sizes we can speed up inference by a
factor of 3.9x and still match or improve the BLEU score obtained using
softmax. Furthermore, we demonstrate that SCONES can be used to train NMT
models that assign the highest probability to adequate translations, thus
mitigating the ""beam search curse"". Additional experiments on synthetic
language pairs with varying levels of uncertainty suggest that the improvements
from SCONES can be attributed to better handling of ambiguity.",None,-1
73ff77cc-5177-4607-a568-61106ac15e45,Optimizing Test-Time Query Representations for Dense Retrieval,0.092549,"Recent developments of dense retrieval rely on quality representations of
queries and contexts from pre-trained query and context encoders. In this
paper, we introduce TOUR (Test-Time Optimization of Query Representations),
which further optimizes instance-level query representations guided by signals
from test-time retrieval results. We leverage a cross-encoder re-ranker to
provide fine-grained pseudo labels over retrieval results and iteratively
optimize query representations with gradient descent. Our theoretical analysis
reveals that TOUR can be viewed as a generalization of the classical Rocchio
algorithm for pseudo relevance feedback, and we present two variants that
leverage pseudo-labels as hard binary or soft continuous labels. We first apply
TOUR on phrase retrieval with our proposed phrase re-ranker, and also evaluate
its effectiveness on passage retrieval with an off-the-shelf re-ranker. TOUR
greatly improves end-to-end open-domain question answering accuracy, as well as
passage retrieval performance. TOUR also consistently improves direct
re-ranking by up to 2.0% while running 1.3-2.4x faster with an efficient
implementation.",https://github.com/dmis-lab/TouR,-1
1ee40cec-def7-4e9c-b2f0-07e6d132f1d0,Investigating Reasons for Disagreement in Natural Language Inference,0.526118,"We investigate how disagreement in natural language inference (NLI)
annotation arises. We developed a taxonomy of disagreement sources with 10
categories spanning 3 high-level classes. We found that some disagreements are
due to uncertainty in the sentence meaning, others to annotator biases and task
artifacts, leading to different interpretations of the label distribution. We
explore two modeling approaches for detecting items with potential
disagreement: a 4-way classification with a ""Complicated"" label in addition to
the three standard NLI labels, and a multilabel classification approach. We
found that the multilabel classification is more expressive and gives better
recall of the possible interpretations in the data.",https://github.com/njjiang/NLI_disagreement_taxonomy,-1
8c6a5066-7922-42b7-ae05-a7b5998891bb,"Streaming, fast and accurate on-device Inverse Text Normalization for Automatic Speech Recognition",0.536658,"Automatic Speech Recognition (ASR) systems typically yield output in lexical
form. However, humans prefer a written form output. To bridge this gap, ASR
systems usually employ Inverse Text Normalization (ITN).
  In previous works, Weighted Finite State Transducers (WFST) have been
employed to do ITN. WFSTs are nicely suited to this task but their size and
run-time costs can make deployment on embedded applications challenging.
  In this paper, we describe the development of an on-device ITN system that is
streaming, lightweight & accurate. At the core of our system is a streaming
transformer tagger, that tags lexical tokens from ASR. The tag informs which
ITN category might be applied, if at all. Following that, we apply an
ITN-category-specific WFST, only on the tagged text, to reliably perform the
ITN conversion. We show that the proposed ITN solution performs equivalent to
strong baselines, while being significantly smaller in size and retaining
customization capabilities.",None,-1
3f41ed8c-161d-4f8f-96e1-acbe2e363598,Overlapping Word Removal is All You Need: Revisiting Data Imbalance in Hope Speech Detection,0.0845855,"Hope Speech Detection, a task of recognizing positive expressions, has made
significant strides recently. However, much of the current works focus on model
development without considering the issue of inherent imbalance in the data.
Our work revisits this issue in hope-speech detection by introducing focal
loss, data augmentation, and pre-processing strategies. Accordingly, we find
that introducing focal loss as part of Multilingual-BERT's (M-BERT) training
process mitigates the effect of class imbalance and improves overall F1-Macro
by 0.11. At the same time, contextual and back-translation-based word
augmentation with M-BERT improves results by 0.10 over baseline despite
imbalance. Finally, we show that overlapping word removal based on
pre-processing, though simple, improves F1-Macro by 0.28. In due process, we
present detailed studies depicting various behaviors of each of these
strategies and summarize key findings from our empirical results for those
interested in getting the most out of M-BERT for hope speech detection under
real-world conditions of data imbalance.",None,4224
36b2f52d-f5d6-41fe-aec0-75c6c6b7b6c7,TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter,0.999787,"Pre-trained language models (PLMs) are fundamental for natural language
processing applications. Most existing PLMs are not tailored to the noisy
user-generated text on social media, and the pre-training does not factor in
the valuable social engagement logs available in a social network. We present
TwHIN-BERT, a multilingual language model productionized at Twitter, trained on
in-domain data from the popular social network. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages,
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on various multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We open-source TwHIN-BERT and our
curated hashtag prediction and social engagement benchmark datasets to the
research community.",None,255449
ae00bdba-1f4a-4ad2-a757-17eb420d7dfe,A Robust Document Image Watermarking Scheme using Deep Neural Network,0.610994,"Watermarking is an important copyright protection technology which generally
embeds the identity information into the carrier imperceptibly. Then the
identity can be extracted to prove the copyright from the watermarked carrier
even after suffering various attacks. Most of the existing watermarking
technologies take the nature images as carriers. Different from the natural
images, document images are not so rich in color and texture, and thus have
less redundant information to carry watermarks. This paper proposes an
end-to-end document image watermarking scheme using the deep neural network.
Specifically, an encoder and a decoder are designed to embed and extract the
watermark. A noise layer is added to simulate the various attacks that could be
encountered in reality, such as the Cropout, Dropout, Gaussian blur, Gaussian
noise, Resize, and JPEG Compression. A text-sensitive loss function is designed
to limit the embedding modification on characters. An embedding strength
adjustment strategy is proposed to improve the quality of watermarked image
with little loss of extraction accuracy. Experimental results show that the
proposed document image watermarking technology outperforms three
state-of-the-arts in terms of the robustness and image quality.",https://github.com/gslxr/Document-image-watermarking,-1
9eb1045c-724a-4b5c-a6f5-2c1d139058e5,GeoSegNet: Point Cloud Semantic Segmentation via Geometric Encoder-Decoder Modeling,0.247484,"Semantic segmentation of point clouds, aiming to assign each point a semantic
category, is critical to 3D scene understanding.Despite of significant advances
in recent years, most of existing methods still suffer from either the
object-level misclassification or the boundary-level ambiguity. In this paper,
we present a robust semantic segmentation network by deeply exploring the
geometry of point clouds, dubbed GeoSegNet. Our GeoSegNet consists of a
multi-geometry based encoder and a boundary-guided decoder. In the encoder, we
develop a new residual geometry module from multi-geometry perspectives to
extract object-level features. In the decoder, we introduce a contrastive
boundary learning module to enhance the geometric representation of boundary
points. Benefiting from the geometric encoder-decoder modeling, our GeoSegNet
can infer the segmentation of objects effectively while making the
intersections (boundaries) of two or more objects clear. Experiments show
obvious improvements of our method over its competitors in terms of the overall
segmentation accuracy and object boundary clearness. Code is available at
https://github.com/Chen-yuiyui/GeoSegNet.",https://github.com/Chen-yuiyui/GeoSegNet,-1
dd41cdad-0a8a-4d33-b51d-45958e5edc8e,Tractable Boolean and Arithmetic Circuits,0.69657,"Tractable Boolean and arithmetic circuits have been studied extensively in AI
for over two decades now. These circuits were initially proposed as ""compiled
objects,"" meant to facilitate logical and probabilistic reasoning, as they
permit various types of inference to be performed in linear-time and a
feed-forward fashion like neural networks. In more recent years, the role of
tractable circuits has significantly expanded as they became a computational
and semantical backbone for some approaches that aim to integrate knowledge,
reasoning and learning. In this article, we review the foundations of tractable
circuits and some associated milestones, while focusing on their core
properties and techniques that make them particularly useful for the broad aims
of neuro-symbolic AI.",None,-1
5425464b-302b-461d-ab9c-6416cc68a7f2,Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling,0.956349,"Existing research generally treats Chinese character as a minimum unit for
representation. However, such Chinese character representation will suffer two
bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich
internal features (e.g., radicals and strokes); and 2) Parameter bottleneck,
each individual character has to be represented by a unique vector. In this
paper, we introduce a novel representation method for Chinese characters to
break the bottlenecks, namely StrokeNet, which represents a Chinese character
by a Latinized stroke sequence (e.g., ""ao1 (concave)"" to ""ajaie"" and ""tu1
(convex)"" to ""aeaqe""). Specifically, StrokeNet maps each stroke to a specific
Latin character, thus allowing similar Chinese characters to have similar Latin
representations. With the introduction of StrokeNet to neural machine
translation (NMT), many powerful but not applicable techniques to non-Latin
languages (e.g., shared subword vocabulary learning and ciphertext-based data
augmentation) can now be perfectly implemented. Experiments on the widely-used
NIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT
tasks show that StrokeNet can provide a significant performance boost over the
strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17
Chinese-English task which is better than any previously reported results
without using monolingual data. Code and scripts are freely available at
https://github.com/zjwang21/StrokeNet.",https://github.com/zjwang21/StrokeNet,-1
a482421f-ac67-490f-9578-fb8cbe8e0162,The Phenomenon of Policy Churn,0.480338,"We identify and study the phenomenon of policy churn, that is, the rapid
change of the greedy policy in value-based reinforcement learning. Policy churn
operates at a surprisingly rapid pace, changing the greedy action in a large
fraction of states within a handful of learning updates (in a typical deep RL
set-up such as DQN on Atari). We characterise the phenomenon empirically,
verifying that it is not limited to specific algorithm or environment
properties. A number of ablations help whittle down the plausible explanations
on why churn occurs to just a handful, all related to deep learning. Finally,
we hypothesise that policy churn is a beneficial but overlooked form of
implicit exploration that casts $\epsilon$-greedy exploration in a fresh light,
namely that $\epsilon$-noise plays a much smaller role than expected.",https://github.com/deepmind/deepmind-research/tree/master/tandem_dqn,-1
4e177044-70d7-4389-942f-3287c7488268,polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics,0.979912,"Polymers are a vital part of everyday life. Their chemical universe is so
large that it presents unprecedented opportunities as well as significant
challenges to identify suitable application-specific candidates. We present a
complete end-to-end machine-driven polymer informatics pipeline that can search
this space for suitable candidates at unprecedented speed and accuracy. This
pipeline includes a polymer chemical fingerprinting capability called polyBERT
(inspired by Natural Language Processing concepts), and a multitask learning
approach that maps the polyBERT fingerprints to a host of properties. polyBERT
is a chemical linguist that treats the chemical structure of polymers as a
chemical language. The present approach outstrips the best presently available
concepts for polymer property prediction based on handcrafted fingerprint
schemes in speed by two orders of magnitude while preserving accuracy, thus
making it a strong candidate for deployment in scalable architectures including
cloud infrastructures.",https://github.com/Ramprasad-Group/polyBERT,-1
3807b5eb-5d43-4950-ae73-7468edab6e71,TODE-Trans: Transparent Object Depth Estimation with Transformer,0.547453,"Transparent objects are widely used in industrial automation and daily life.
However, robust visual recognition and perception of transparent objects have
always been a major challenge. Currently, most commercial-grade depth cameras
are still not good at sensing the surfaces of transparent objects due to the
refraction and reflection of light. In this work, we present a
transformer-based transparent object depth estimation approach from a single
RGB-D input. We observe that the global characteristics of the transformer make
it easier to extract contextual information to perform depth estimation of
transparent areas. In addition, to better enhance the fine-grained features, a
feature fusion module (FFM) is designed to assist coherent prediction. Our
empirical evidence demonstrates that our model delivers significant
improvements in recent popular datasets, e.g., 25% gain on RMSE and 21% gain on
REL compared to previous state-of-the-art convolutional-based counterparts in
ClearGrasp dataset. Extensive results show that our transformer-based model
enables better aggregation of the object's RGB and inaccurate depth information
to obtain a better depth representation. Our code and the pre-trained model
will be available at https://github.com/yuchendoudou/TODE.",None,6668
f540e077-c824-4f70-8646-b4a397ef2c91,End-to-end model for named entity recognition from speech without paired training data,0.753686,"Recent works showed that end-to-end neural approaches tend to become very
popular for spoken language understanding (SLU). Through the term end-to-end,
one considers the use of a single model optimized to extract semantic
information directly from the speech signal. A major issue for such models is
the lack of paired audio and textual data with semantic annotation. In this
paper, we propose an approach to build an end-to-end neural model to extract
semantic information in a scenario in which zero paired audio data is
available. Our approach is based on the use of an external model trained to
generate a sequence of vectorial representations from text. These
representations mimic the hidden representations that could be generated inside
an end-to-end automatic speech recognition (ASR) model by processing a speech
signal. An SLU neural module is then trained using these representations as
input and the annotated text as output. Last, the SLU module replaces the top
layers of the ASR model to achieve the construction of the end-to-end model.
Our experiments on named entity recognition, carried out on the QUAERO corpus,
show that this approach is very promising, getting better results than a
comparable cascade approach or than the use of synthetic voices.",https://github.com/mdhaffar/Named-Entity-Recognition,-1
df81dc3c-4a1d-456d-badd-51eedfc31e5f,Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields,0.144263,"Comprehensive 3D scene understanding, both geometrically and semantically, is
important for real-world applications such as robot perception. Most of the
existing work has focused on developing data-driven discriminative models for
scene understanding. This paper provides a new approach to scene understanding,
from a synthesis model perspective, by leveraging the recent progress on
implicit 3D representation and neural rendering. Building upon the great
success of Neural Radiance Fields (NeRFs), we introduce Scene-Property
Synthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic
RGB images from novel viewpoints, but also render various accurate scene
properties (e.g., appearance, geometry, and semantics). By doing so, we
facilitate addressing a variety of scene understanding tasks under a unified
framework, including semantic segmentation, surface normal estimation,
reshading, keypoint detection, and edge detection. Our SS-NeRF framework can be
a powerful tool for bridging generative learning and discriminative learning,
and thus be beneficial to the investigation of a wide range of interesting
problems, such as studying task relationships within a synthesis paradigm,
transferring knowledge to novel tasks, facilitating downstream discriminative
tasks as ways of data augmentation, and serving as auto-labeller for data
creation.",None,-1
245016da-c8b4-4eb2-8b53-b09b9a3da036,CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,0.444112,"Knowledge-intensive language tasks (KILT) usually require a large body of
information to provide correct answers. A popular paradigm to solve this
problem is to combine a search system with a machine reader, where the former
retrieves supporting evidences and the latter examines them to produce answers.
Recently, the reader component has witnessed significant advances with the help
of large-scale pre-trained generative models. Meanwhile most existing solutions
in the search component rely on the traditional ``index-retrieve-then-rank''
pipeline, which suffers from large memory footprint and difficulty in
end-to-end optimization. Inspired by recent efforts in constructing model-based
IR models, we propose to replace the traditional multi-step search pipeline
with a novel single-step generative model, which can dramatically simplify the
search process and be optimized in an end-to-end manner. We show that a strong
generative retrieval model can be learned with a set of adequately designed
pre-training tasks, and be adopted to improve a variety of downstream KILT
tasks with further fine-tuning. We name the pre-trained generative retrieval
model as CorpusBrain as all information about the corpus is encoded in its
parameters without the need of constructing additional index. Empirical results
show that CorpusBrain can significantly outperform strong baselines for the
retrieval task on the KILT benchmark and establish new state-of-the-art
downstream performances. We also show that CorpusBrain works well under zero-
and low-resource settings.",https://github.com/ict-bigdatalab/CorpusBrain,-1
a5edbd52-3d2d-4369-9af8-20a66e0f6df6,Feature-Level Debiased Natural Language Understanding,0.324004,"Natural language understanding (NLU) models often rely on dataset biases
rather than intended task-relevant features to achieve high performance on
specific datasets. As a result, these models perform poorly on datasets outside
the training distribution. Some recent studies address this issue by reducing
the weights of biased samples during the training process. However, these
methods still encode biased latent features in representations and neglect the
dynamic nature of bias, which hinders model prediction. We propose an NLU
debiasing method, named debiasing contrastive learning (DCT), to simultaneously
alleviate the above problems based on contrastive learning. We devise a
debiasing, positive sampling strategy to mitigate biased latent features by
selecting the least similar biased positive samples. We also propose a dynamic
negative sampling strategy to capture the dynamic influence of biases by
employing a bias-only model to dynamically select the most similar biased
negative samples. We conduct experiments on three NLU benchmark datasets.
Experimental results show that DCT outperforms state-of-the-art baselines on
out-of-distribution datasets while maintaining in-distribution performance. We
also verify that DCT can reduce biased latent features from the model's
representation.",https://github.com/youganglyu/DCT,-1
2265fbdc-b28a-4bb5-b618-6254c5c60e78,Towards a Secure and Reliable Federated Learning using Blockchain,0.431615,"Federated learning (FL) is a distributed machine learning (ML) technique that
enables collaborative training in which devices perform learning using a local
dataset while preserving their privacy. This technique ensures privacy,
communication efficiency, and resource conservation. Despite these advantages,
FL still suffers from several challenges related to reliability (i.e.,
unreliable participating devices in training), tractability (i.e., a large
number of trained models), and anonymity. To address these issues, we propose a
secure and trustworthy blockchain framework (SRB-FL) tailored to FL, which uses
blockchain features to enable collaborative model training in a fully
distributed and trustworthy manner. In particular, we design a secure FL based
on the blockchain sharding that ensures data reliability, scalability, and
trustworthiness. In addition, we introduce an incentive mechanism to improve
the reliability of FL devices using subjective multi-weight logic. The results
show that our proposed SRB-FL framework is efficient and scalable, making it a
promising and suitable solution for federated learning.",None,-1
3e93e5de-7fd0-4bcb-9077-dfab3dc359b6,Multi-sensor large-scale dataset for multi-view 3D reconstruction,0.293736,"We present a new multi-sensor dataset for multi-view 3D surface
reconstruction. It includes registered RGB and depth data from sensors of
different resolutions and modalities: smartphones, Intel RealSense, Microsoft
Kinect, industrial cameras, and structured-light scanner. The scenes are
selected to emphasize a diverse set of material properties challenging for
existing algorithms. We provide around 1.4 million images of 107 different
scenes acquired from 100 viewing directions under 14 lighting conditions. We
expect our dataset will be useful for evaluation and training of 3D
reconstruction algorithms and for related tasks. The dataset is available at
skoltech3d.appliedai.tech.",None,15328
5c347b9a-4a75-436f-b03c-a31fc575c2f1,Evaluating the Knowledge Dependency of Questions,0.0391349,"The automatic generation of Multiple Choice Questions (MCQ) has the potential
to reduce the time educators spend on student assessment significantly.
However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE,
and METEOR, focus on the n-gram based similarity of the generated MCQ to the
gold sample in the dataset and disregard their educational value. They fail to
evaluate the MCQ's ability to assess the student's knowledge of the
corresponding target fact. To tackle this issue, we propose a novel automatic
evaluation metric, coined Knowledge Dependent Answerability (KDA), which
measures the MCQ's answerability given knowledge of the target fact.
Specifically, we first show how to measure KDA based on student responses from
a human survey. Then, we propose two automatic evaluation metrics, KDA_disc and
KDA_cont, that approximate KDA by leveraging pre-trained language models to
imitate students' problem-solving behavior. Through our human studies, we show
that KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2)
usability in an actual classroom setting, labeled by experts. Furthermore, when
combined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown
to have a strong predictive power for various expert-labeled MCQ quality
measures.",https://github.com/riiid/question-score,-1
52aa9c64-f13a-41c1-b11d-9240477535b2,Help Me Explore: Minimal Social Interventions for Graph-Based Autotelic Agents,0.572333,"In the quest for autonomous agents learning open-ended repertoires of skills,
most works take a Piagetian perspective: learning trajectories are the results
of interactions between developmental agents and their physical environment.
The Vygotskian perspective, on the other hand, emphasizes the centrality of the
socio-cultural environment: higher cognitive functions emerge from
transmissions of socio-cultural processes internalized by the agent. This paper
argues that both perspectives could be coupled within the learning of autotelic
agents to foster their skill acquisition. To this end, we make two
contributions: 1) a novel social interaction protocol called Help Me Explore
(HME), where autotelic agents can benefit from both individual and socially
guided exploration. In social episodes, a social partner suggests goals at the
frontier of the learning agent knowledge. In autotelic episodes, agents can
either learn to master their own discovered goals or autonomously rehearse
failed social goals; 2) GANGSTR, a graph-based autotelic agent for manipulation
domains capable of decomposing goals into sequences of intermediate sub-goals.
We show that when learning within HME, GANGSTR overcomes its individual
learning limits by mastering the most complex configurations (e.g. stacks of 5
blocks) with only few social interventions.",https://github.com/akakzia/gangstr,-1
ceec8b3e-636d-4229-9dcd-17c46097a2fa,Simplifying Multilingual News Clustering Through Projection From a Shared Space,0.451759,"The task of organizing and clustering multilingual news articles for media
monitoring is essential to follow news stories in real time. Most approaches to
this task focus on high-resource languages (mostly English), with low-resource
languages being disregarded. With that in mind, we present a much simpler
online system that is able to cluster an incoming stream of documents without
depending on language-specific features. We empirically demonstrate that the
use of multilingual contextual embeddings as the document representation
significantly improves clustering quality. We challenge previous crosslingual
approaches by removing the precondition of building monolingual clusters. We
model the clustering process as a set of linear classifiers to aggregate
similar documents, and correct closely-related multilingual clusters through
merging in an online fashion. Our system achieves state-of-the-art results on a
multilingual news stream clustering dataset, and we introduce a new evaluation
for zero-shot news clustering in multiple languages. We make our code available
as open-source.",https://github.com/Priberam/projected-news-clustering,-1
4da1de8b-9c8e-466b-a6a7-75016ee64165,Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering,0.664556,"Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a
two-stage framework that first retrieves external knowledge given the visual
question and then predicts the answer based on the retrieved content. However,
the retrieved knowledge is often inadequate. Retrievals are frequently too
general and fail to cover specific knowledge needed to answer the question.
Also, the naturally available supervision (whether the passage contains the
correct answer) is weak and does not guarantee question relevancy. To address
these issues, we propose an Entity-Focused Retrieval (EnFoRe) model that
provides stronger supervision during training and recognizes question-relevant
entities to help retrieve more specific knowledge. Experiments show that our
EnFoRe model achieves superior retrieval performance on OK-VQA, the currently
largest outside-knowledge VQA dataset. We also combine the retrieved knowledge
with state-of-the-art VQA models, and achieve a new state-of-the-art
performance on OK-VQA.",https://github.com/jialinwu17/EnFoRe.git,-1
7fa0b7e0-9241-4d50-8dd5-06fcf59aeaae,Offline Reinforcement Learning with Differential Privacy,0.571326,"The offline reinforcement learning (RL) problem is often motivated by the
need to learn data-driven decision policies in financial, legal and healthcare
applications. However, the learned policy could retain sensitive information of
individuals in the training data (e.g., treatment and outcome of patients),
thus susceptible to various privacy risks. We design offline RL algorithms with
differential privacy guarantees which provably prevent such risks. These
algorithms also enjoy strong instance-dependent learning bounds under both
tabular and linear Markov decision process (MDP) settings. Our theory and
simulation suggest that the privacy guarantee comes at (almost) no drop in
utility comparing to the non-private counterpart for a medium-size dataset.",None,-1
4c2a73f9-486a-47f6-8342-2ea0c048a54f,Extreme Masking for Learning Instance and Distributed Visual Representations,0.524127,"The paper presents a scalable approach for learning spatially distributed
visual representations over individual tokens and a holistic instance
representation simultaneously. We use self-attention blocks to represent
spatially distributed tokens, followed by cross-attention blocks to aggregate
the holistic image instance. The core of the approach is the use of extremely
large token masking (75\%-90\%) as the data augmentation for supervision. Our
model, named ExtreMA, follows the plain BYOL approach where the instance
representation from the unmasked subset is trained to predict that from the
intact input. Instead of encouraging invariance across inputs, the model is
required to capture informative variations in an image. The paper makes three
contributions: 1) It presents random masking as a strong and computationally
efficient data augmentation for siamese representation learning. 2) With
multiple sampling per instance, extreme masking greatly speeds up learning and
improves performance with more data. 3) ExtreMA obtains stronger linear probing
performance than masked modeling methods, and better transfer performance than
prior contrastive models.",None,-1
31c1b0cc-4147-4ced-b092-4db142725521,$\mathcal{X}$-Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing,0.976658,"This paper presents a generic probabilistic framework for estimating the
statistical dependency and finding the anatomical correspondences among an
arbitrary number of medical images. The method builds on a novel formulation of
the $N$-dimensional joint intensity distribution by representing the common
anatomy as latent variables and estimating the appearance model with
nonparametric estimators. Through connection to maximum likelihood and the
expectation-maximization algorithm, an information\hyp{}theoretic metric called
$\mathcal{X}$-metric and a co-registration algorithm named $\mathcal{X}$-CoReg
are induced, allowing groupwise registration of the $N$ observed images with
computational complexity of $\mathcal{O}(N)$. Moreover, the method naturally
extends for a weakly-supervised scenario where anatomical labels of certain
images are provided. This leads to a combined\hyp{}computing framework
implemented with deep learning, which performs registration and segmentation
simultaneously and collaboratively in an end-to-end fashion. Extensive
experiments were conducted to demonstrate the versatility and applicability of
our model, including multimodal groupwise registration, motion correction for
dynamic contrast enhanced magnetic resonance images, and deep combined
computing for multimodal medical images. Results show the superiority of our
method in various applications in terms of both accuracy and efficiency,
highlighting the advantage of the proposed representation of the imaging
process.",https://zmiclab.github.io/projects.html,5501
5ef2d88c-0a15-49bc-b5fd-4c586714e51a,On the Role of Field of View for Occlusion Removal with Airborne Optical Sectioning,0.312833,"Occlusion caused by vegetation is an essential problem for remote sensing
applications in areas, such as search and rescue, wildfire detection, wildlife
observation, surveillance, border control, and others. Airborne Optical
Sectioning (AOS) is an optical, wavelength-independent synthetic aperture
imaging technique that supports computational occlusion removal in real-time.
It can be applied with manned or unmanned aircrafts, such as drones. In this
article, we demonstrate a relationship between forest density and field of view
(FOV) of applied imaging systems. This finding was made with the help of a
simulated procedural forest model which offers the consideration of more
realistic occlusion properties than our previous statistical model. While AOS
has been explored with automatic and autonomous research prototypes in the
past, we present a free AOS integration for DJI systems. It enables bluelight
organizations and others to use and explore AOS with compatible, manually
operated, off-the-shelf drones. The (digitally cropped) default FOV for this
implementation was chosen based on our new finding.",https://github.com/tensorware/aos-simulation,-1
75a2bc5d-e545-4a1d-be33-d2426fecd16b,MIRROR: Differentiable Deep Social Projection for Assistive Human-Robot Communication,0.68292,"Communication is a hallmark of intelligence. In this work, we present MIRROR,
an approach to (i) quickly learn human models from human demonstrations, and
(ii) use the models for subsequent communication planning in assistive
shared-control settings. MIRROR is inspired by social projection theory, which
hypothesizes that humans use self-models to understand others. Likewise, MIRROR
leverages self-models learned using reinforcement learning to bootstrap human
modeling. Experiments with simulated humans show that this approach leads to
rapid learning and more robust models compared to existing behavioral cloning
and state-of-the-art imitation learning methods. We also present a
human-subject study using the CARLA simulator which shows that (i) MIRROR is
able to scale to complex domains with high-dimensional observations and
complicated world physics and (ii) provides effective assistive communication
that enabled participants to drive more safely in adverse weather conditions.",https://github.com/clear-nus/mirror,-1
f29f7727-b399-405d-aaf0-6605940b783f,Lightweight Long-Range Generative Adversarial Networks,0.153562,"In this paper, we introduce novel lightweight generative adversarial
networks, which can effectively capture long-range dependencies in the image
generation process, and produce high-quality results with a much simpler
architecture. To achieve this, we first introduce a long-range module, allowing
the network to dynamically adjust the number of focused sampling pixels and to
also augment sampling locations. Thus, it can break the limitation of the fixed
geometric structure of the convolution operator, and capture long-range
dependencies in both spatial and channel-wise directions. Also, the proposed
long-range module can highlight negative relations between pixels, working as a
regularization to stabilize training. Furthermore, we propose a new generation
strategy through which we introduce metadata into the image generation process
to provide basic information about target images, which can stabilize and speed
up the training process. Our novel long-range module only introduces few
additional parameters and is easily inserted into existing models to capture
long-range dependencies. Extensive experiments demonstrate the competitive
performance of our method with a lightweight architecture.",None,-1
93ef2775-d407-4697-88e1-de7e95a4abf2,SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds,0.818192,"With the recent availability and affordability of commercial depth sensors
and 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets
have been publicized to facilitate research in 3D computer vision. However,
existing datasets either cover relatively small areas or have limited semantic
annotations. Fine-grained understanding of urban-scale 3D scenes is still in
its infancy. In this paper, we introduce SensatUrban, an urban-scale UAV
photogrammetry point cloud dataset consisting of nearly three billion points
collected from three UK cities, covering 7.6 km^2. Each point in the dataset
has been labelled with fine-grained semantic annotations, resulting in a
dataset that is three times the size of the previous existing largest
photogrammetric point cloud dataset. In addition to the more commonly
encountered categories such as road and vegetation, urban-level categories
including rail, bridge, and river are also included in our dataset. Based on
this dataset, we further build a benchmark to evaluate the performance of
state-of-the-art segmentation algorithms. In particular, we provide a
comprehensive analysis and identify several key challenges limiting urban-scale
point cloud understanding. The dataset is available at
http://point-cloud-analysis.cs.ox.ac.uk.",None,-1
fcdd4009-f61b-47a7-a832-e56453e5e8fa,VCNet: A self-explaining model for realistic counterfactual generation,0.136481,"Counterfactual explanation is a common class of methods to make local
explanations of machine learning decisions. For a given instance, these methods
aim to find the smallest modification of feature values that changes the
predicted decision made by a machine learning model. One of the challenges of
counterfactual explanation is the efficient generation of realistic
counterfactuals. To address this challenge, we propose VCNet-Variational
Counter Net-a model architecture that combines a predictor and a counterfactual
generator that are jointly trained, for regression or classification tasks.
VCNet is able to both generate predictions, and to generate counterfactual
explanations without having to solve another minimisation problem. Our
contribution is the generation of counterfactuals that are close to the
distribution of the predicted class. This is done by learning a variational
autoencoder conditionally to the output of the predictor in a join-training
fashion. We present an empirical evaluation on tabular datasets and across
several interpretability metrics. The results are competitive with the
state-of-the-art method.",https://github.com/bkghz-orange-blue/CounterNet,-1
97f8b4f5-2567-4b0e-bfac-b2af898d0ddc,A Critical Analysis of Image-based Camera Pose Estimation Techniques,0.275271,"Camera, and associated with its objects within the field of view,
localization could benefit many computer vision fields, such as autonomous
driving, robot navigation, and augmented reality (AR). In this survey, we first
introduce specific application areas and the evaluation metrics for camera
localization pose according to different sub-tasks (learning-based 2D-2D task,
feature-based 2D-3D task, and 3D-3D task). Then, we review common methods for
structure-based camera pose estimation approaches, absolute pose regression and
relative pose regression approaches by critically modelling the methods to
inspire further improvements in their algorithms such as loss functions, neural
network structures. Furthermore, we summarise what are the popular datasets
used for camera localization and compare the quantitative and qualitative
results of these methods with detailed performance metrics. Finally, we discuss
future research possibilities and applications.",None,-1
7f8f491e-d248-47d0-a657-b70abd0e94e4,Look at Adjacent Frames: Video Anomaly Detection without Offline Training,0.125617,"We propose a solution to detect anomalous events in videos without the need
to train a model offline. Specifically, our solution is based on a
randomly-initialized multilayer perceptron that is optimized online to
reconstruct video frames, pixel-by-pixel, from their frequency information.
Based on the information shifts between adjacent frames, an incremental learner
is used to update parameters of the multilayer perceptron after observing each
frame, thus allowing to detect anomalous events along the video stream.
Traditional solutions that require no offline training are limited to operating
on videos with only a few abnormal frames. Our solution breaks this limit and
achieves strong performance on benchmark datasets.",None,-1
9513c5a0-0076-4372-ad79-7ac38967b309,Democratizing Neural Machine Translation with OPUS-MT,0.11469,"This paper presents the OPUS ecosystem with a focus on the development of
open machine translation models and tools, and their integration into end-user
applications, development platforms and professional workflows. We discuss our
on-going mission of increasing language coverage and translation quality, and
also describe on-going work on the development of modular translation models
and speed-optimized compact solutions for real-time translation on regular
desktops and small devices.",None,-1
08feb40c-5081-4971-86c8-d74ee81d6608,SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup Training,0.132541,"The conventional success of textual classification relies on annotated data,
and the new paradigm of pre-trained language models (PLMs) still requires a few
labeled data for downstream tasks. However, in real-world applications, label
noise inevitably exists in training data, damaging the effectiveness,
robustness, and generalization of the models constructed on such data.
Recently, remarkable achievements have been made to mitigate this dilemma in
visual data, while only a few explore textual data. To fill this gap, we
present SelfMix, a simple yet effective method, to handle label noise in text
classification tasks. SelfMix uses the Gaussian Mixture Model to separate
samples and leverages semi-supervised learning. Unlike previous works requiring
multiple models, our method utilizes the dropout mechanism on a single model to
reduce the confirmation bias in self-training and introduces a textual-level
mixup training strategy. Experimental results on three text classification
benchmarks with different types of text show that the performance of our
proposed method outperforms these strong baselines designed for both textual
and visual data under different noise ratios and noise types. Our code is
available at https://github.com/noise-learning/SelfMix.",https://github.com/noise-learning/SelfMix,-1
d8099f1f-420c-4a96-a838-2856cd070ef1,A logical theory for conditional weak ontic necessity based on context update,0.251691,"Weak ontic necessity is the ontic necessity expressed by ``should'' or
``ought to'' in English. An example of it is ``I should be dead by now''. A
feature of this necessity is whether it holds does not have anything to do with
whether its prejacent holds. In this paper, we present a logical theory for
conditional weak ontic necessity based on context update. A context is a set of
ordered defaults, determining expected possible states of the present world.
Sentences are evaluated with respect to contexts. When evaluating the
conditional weak ontic necessity with respect to a context, we first update the
context with the antecedent, then check whether the consequent holds with
respect to the updated context. The logic is complete. Our theory combines
premise semantics and update semantics for conditionals.",None,-1
ab687e16-d50f-47e2-b829-b46a8d2c0661,"Layout-Aware Information Extraction for Document-Grounded Dialogue: Dataset, Method and Demonstration",0.562066,"Building document-grounded dialogue systems have received growing interest as
documents convey a wealth of human knowledge and commonly exist in enterprises.
Wherein, how to comprehend and retrieve information from documents is a
challenging research problem. Previous work ignores the visual property of
documents and treats them as plain text, resulting in incomplete modality. In
this paper, we propose a Layout-aware document-level Information Extraction
dataset, LIE, to facilitate the study of extracting both structural and
semantic knowledge from visually rich documents (VRDs), so as to generate
accurate responses in dialogue systems. LIE contains 62k annotations of three
extraction tasks from 4,061 pages in product and official documents, becoming
the largest VRD-based information extraction dataset to the best of our
knowledge. We also develop benchmark methods that extend the token-based
language model to consider layout features like humans. Empirical results show
that layout is critical for VRD-based extraction, and system demonstration also
verifies that the extracted knowledge can help locate the answers that users
care about.",https://github.com/jsvine/pdfplumber,-1
6b21496d-b3bd-45f2-9694-da039744334f,TCN Mapping Optimization for Ultra-Low Power Time-Series Edge Inference,0.293544,"Temporal Convolutional Networks (TCNs) are emerging lightweight Deep Learning
models for Time Series analysis. We introduce an automated exploration approach
and a library of optimized kernels to map TCNs on Parallel Ultra-Low Power
(PULP) microcontrollers. Our approach minimizes latency and energy by
exploiting a layer tiling optimizer to jointly find the tiling dimensions and
select among alternative implementations of the causal and dilated
1D-convolution operations at the core of TCNs. We benchmark our approach on a
commercial PULP device, achieving up to 103X lower latency and 20.3X lower
energy than the Cube-AI toolkit executed on the STM32L4 and from 2.9X to 26.6X
lower energy compared to commercial closed-source and academic open-source
approaches on the same hardware target.",None,-1
01039c3a-8003-4619-9079-f55622a3c7c7,Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates,0.32746,"Calibration strengthens the trustworthiness of black-box models by producing
better accurate confidence estimates on given examples. However, little is
known about if model explanations can help confidence calibration. Intuitively,
humans look at important features attributions and decide whether the model is
trustworthy. Similarly, the explanations can tell us when the model may or may
not know. Inspired by this, we propose a method named CME that leverages model
explanations to make the model less confident with non-inductive attributions.
The idea is that when the model is not highly confident, it is difficult to
identify strong indications of any class, and the tokens accordingly do not
have high attribution scores for any class and vice versa. We conduct extensive
experiments on six datasets with two popular pre-trained language models in the
in-domain and out-of-domain settings. The results show that CME improves
calibration performance in all settings. The expected calibration errors are
further reduced when combined with temperature scaling. Our findings highlight
that model explanations can help calibrate posterior estimates.",https://github.com/crazyofapple/CME-EMNLP2022/,-1
578cd393-dba6-4c06-88dc-4a8f09fbaff8,Cross-Modal Mutual Learning for Cued Speech Recognition,0.500924,"Automatic Cued Speech Recognition (ACSR) provides an intelligent
human-machine interface for visual communications, where the Cued Speech (CS)
system utilizes lip movements and hand gestures to code spoken language for
hearing-impaired people. Previous ACSR approaches often utilize direct feature
concatenation as the main fusion paradigm. However, the asynchronous modalities
i.e., lip, hand shape and hand position) in CS may cause interference for
feature concatenation. To address this challenge, we propose a transformer
based cross-modal mutual learning framework to prompt multi-modal interaction.
Compared with the vanilla self-attention, our model forces modality-specific
information of different modalities to pass through a modality-invariant
codebook, collating linguistic representations for tokens of each modality.
Then the shared linguistic knowledge is used to re-synchronize multi-modal
sequences. Moreover, we establish a novel large-scale multi-speaker CS dataset
for Mandarin Chinese. To our knowledge, this is the first work on ACSR for
Mandarin Chinese. Extensive experiments are conducted for different languages
i.e., Chinese, French, and British English). Results demonstrate that our model
exhibits superior recognition performance to the state-of-the-art by a large
margin.",None,-1
0c409e0a-63f2-4779-8841-a33fa73793e5,MTEB: Massive Text Embedding Benchmark,0.985442,"Text embeddings are commonly evaluated on a small set of datasets from a
single task not covering their possible applications to other tasks. It is
unclear whether state-of-the-art embeddings on semantic textual similarity
(STS) can be equally well applied to other tasks like clustering or reranking.
This makes progress in the field difficult to track, as various models are
constantly being proposed without proper evaluation. To solve this problem, we
introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding
tasks covering a total of 58 datasets and 112 languages. Through the
benchmarking of 33 models on MTEB, we establish the most comprehensive
benchmark of text embeddings to date. We find that no particular text embedding
method dominates across all tasks. This suggests that the field has yet to
converge on a universal text embedding method and scale it up sufficiently to
provide state-of-the-art results on all embedding tasks. MTEB comes with
open-source code and a public leaderboard at
https://github.com/embeddings-benchmark/mteb.",None,-1
e8e04a72-0850-488d-b952-ecdb6c18b295,Optimizing LLVM Pass Sequences with Shackleton: A Linear Genetic Programming Framework,0.566381,"In this paper we introduce Shackleton as a generalized framework enabling the
application of linear genetic programming -- a technique under the umbrella of
evolutionary algorithms -- to a variety of use cases. We also explore here a
novel application for this class of methods: optimizing sequences of LLVM
optimization passes. The algorithm underpinning Shackleton is discussed, with
an emphasis on the effects of different features unique to the framework when
applied to LLVM pass sequences. Combined with analysis of different
hyperparameter settings, we report the results on automatically optimizing pass
sequences using Shackleton for two software applications at differing
complexity levels. Finally, we reflect on the advantages and limitations of our
current implementation and lay out a path for further improvements. These
improvements aim to surpass hand-crafted solutions with an automatic discovery
method for an optimal pass sequence.",None,-1
b7bca2f2-c121-45d0-b4ef-73861e5e8432,Disentangling Identity and Pose for Facial Expression Recognition,0.798104,"Facial expression recognition (FER) is a challenging problem because the
expression component is always entangled with other irrelevant factors, such as
identity and head pose. In this work, we propose an identity and pose
disentangled facial expression recognition (IPD-FER) model to learn more
discriminative feature representation. We regard the holistic facial
representation as the combination of identity, pose and expression. These three
components are encoded with different encoders. For identity encoder, a well
pre-trained face recognition model is utilized and fixed during training, which
alleviates the restriction on specific expression training data in previous
works and makes the disentanglement practicable on in-the-wild datasets. At the
same time, the pose and expression encoder are optimized with corresponding
labels. Combining identity and pose feature, a neutral face of input individual
should be generated by the decoder. When expression feature is added, the input
image should be reconstructed. By comparing the difference between synthesized
neutral and expressional images of the same individual, the expression
component is further disentangled from identity and pose. Experimental results
verify the effectiveness of our method on both lab-controlled and in-the-wild
databases and we achieve state-of-the-art recognition performance.",https://github.com/WIKI2020/FacePose,-1
2e5b4230-8051-4763-8a56-1a691deb7382,The rise of the lottery heroes: why zero-shot pruning is hard,0.098798,"Recent advances in deep learning optimization showed that just a subset of
parameters are really necessary to successfully train a model. Potentially,
such a discovery has broad impact from the theory to application; however, it
is known that finding these trainable sub-network is a typically costly
process. This inhibits practical applications: can the learned sub-graph
structures in deep learning models be found at training time? In this work we
explore such a possibility, observing and motivating why common approaches
typically fail in the extreme scenarios of interest, and proposing an approach
which potentially enables training with reduced computational effort. The
experiments on either challenging architectures and datasets suggest the
algorithmic accessibility over such a computational gain, and in particular a
trade-off between accuracy achieved and training complexity deployed emerges.",https://github.com/ShowLo/MobileNetV3,-1
2e50db5f-23dc-4ec3-a239-5e2f604deb1c,Event Collapse in Contrast Maximization Frameworks,0.560296,"Contrast maximization (CMax) is a framework that provides state-of-the-art
results on several event-based computer vision tasks, such as ego-motion or
optical flow estimation. However, it may suffer from a problem called event
collapse, which is an undesired solution where events are warped into too few
pixels. As prior works have largely ignored the issue or proposed workarounds,
it is imperative to analyze this phenomenon in detail. Our work demonstrates
event collapse in its simplest form and proposes collapse metrics by using
first principles of space-time deformation based on differential geometry and
physics. We experimentally show on publicly available datasets that the
proposed metrics mitigate event collapse and do not harm well-posed warps. To
the best of our knowledge, regularizers based on the proposed metrics are the
only effective solution against event collapse in the experimental settings
considered, compared with other methods. We hope that this work inspires
further research to tackle more complex warp models.",None,7090
906f3362-3402-4ad2-a527-7a2b6c473d11,Weakly Supervised 3D Point Cloud Segmentation via Multi-Prototype Learning,0.519184,"Addressing the annotation challenge in 3D Point Cloud segmentation has
inspired research into weakly supervised learning. Existing approaches mainly
focus on exploiting manifold and pseudo-labeling to make use of large unlabeled
data points. A fundamental challenge here lies in the large intra-class
variations of local geometric structure, resulting in subclasses within a
semantic class. In this work, we leverage this intuition and opt for
maintaining an individual classifier for each subclass. Technically, we design
a multi-prototype classifier, each prototype serves as the classifier weights
for one subclass. To enable effective updating of multi-prototype classifier
weights, we propose two constraints respectively for updating the prototypes
w.r.t. all point features and for encouraging the learning of diverse
prototypes. Experiments on weakly supervised 3D point cloud segmentation tasks
validate the efficacy of proposed method in particular at low-label regime. Our
hypothesis is also verified given the consistent discovery of semantic
subclasses at no cost of additional annotations.",None,-1
bef53db0-6075-4a0d-a72e-3d57bc82afbe,Phone2Proc: Bringing Robust Robots Into Our Chaotic World,0.514767,"Training embodied agents in simulation has become mainstream for the embodied
AI community. However, these agents often struggle when deployed in the
physical world due to their inability to generalize to real-world environments.
In this paper, we present Phone2Proc, a method that uses a 10-minute phone scan
and conditional procedural generation to create a distribution of training
scenes that are semantically similar to the target environment. The generated
scenes are conditioned on the wall layout and arrangement of large objects from
the scan, while also sampling lighting, clutter, surface textures, and
instances of smaller objects with randomized placement and materials.
Leveraging just a simple RGB camera, training with Phone2Proc shows massive
improvements from 34.7% to 70.7% success rate in sim-to-real ObjectNav
performance across a test suite of over 200 trials in diverse real-world
environments, including homes, offices, and RoboTHOR. Furthermore, Phone2Proc's
diverse distribution of generated scenes makes agents remarkably robust to
changes in the real world, such as human movement, object rearrangement,
lighting changes, or clutter.",https://allenai.org/project/phone2proc,-1
9ee27393-3dd0-47c9-8d40-ecfaf922da80,Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis,0.793283,"Recent literature focuses on utilizing the entity information in the
sentence-level relation extraction (RE), but this risks leaking superficial and
spurious clues of relations. As a result, RE still suffers from unintended
entity bias, i.e., the spurious correlation between entity mentions (names) and
relations. Entity bias can mislead the RE models to extract the relations that
do not exist in the text. To combat this issue, some previous work masks the
entity mentions to prevent the RE models from overfitting entity mentions.
However, this strategy degrades the RE performance because it loses the
semantic information of entities. In this paper, we propose the CORE
(Counterfactual Analysis based Relation Extraction) debiasing method that
guides the RE models to focus on the main effects of textual context without
losing the entity information. We first construct a causal graph for RE, which
models the dependencies between variables in RE models. Then, we propose to
conduct counterfactual analysis on our causal graph to distill and mitigate the
entity bias, that captures the causal effects of specific entity mentions in
each instance. Note that our CORE method is model-agnostic to debias existing
RE systems during inference without changing their training processes.
Extensive experimental results demonstrate that our CORE yields significant
gains on both effectiveness and generalization for RE. The source code is
provided at: https://github.com/vanoracai/CoRE.",https://github.com/vanoracai/CoRE,-1
9986ebfd-3408-4d2e-a0de-ae01ae7f2edf,Satellite Image and Machine Learning based Knowledge Extraction in the Poverty and Welfare Domain,0.156348,"Recent advances in artificial intelligence and machine learning have created
a step change in how to measure human development indicators, in particular
asset based poverty. The combination of satellite imagery and machine learning
has the capability to estimate poverty at a level similar to what is achieved
with workhorse methods such as face-to-face interviews and household surveys.
An increasingly important issue beyond static estimations is whether this
technology can contribute to scientific discovery and consequently new
knowledge in the poverty and welfare domain. A foundation for achieving
scientific insights is domain knowledge, which in turn translates into
explainability and scientific consistency. We review the literature focusing on
three core elements relevant in this context: transparency, interpretability,
and explainability and investigate how they relates to the poverty, machine
learning and satellite imagery nexus. Our review of the field shows that the
status of the three core elements of explainable machine learning
(transparency, interpretability and domain knowledge) is varied and does not
completely fulfill the requirements set up for scientific insights and
discoveries. We argue that explainability is essential to support wider
dissemination and acceptance of this research, and explainability means more
than just interpretability.",None,-1
e6fb63e0-a40e-45f4-8e7e-22f4c0624350,g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin,0.705446,"Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have approached this
problem using pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we
propose a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments show that learning a soft-weighting function for the candidate
phonemes benefits performance. In addition, our proposed g2pW does not require
extra pre-trained POS tagging models while using POS tags as auxiliary features
since we train the POS tagging model simultaneously with the unified encoder.
Experimental results show that our g2pW outperforms existing methods on the
public CPP dataset. All codes, model weights, and a user-friendly package are
publicly available.",https://github.com/GitYCC/g2pW,-1
ce9fc796-c984-463d-9a91-ec358d18e25c,MMFN: Multi-Modal-Fusion-Net for End-to-End Driving,0.708153,"Inspired by the fact that humans use diverse sensory organs to perceive the
world, sensors with different modalities are deployed in end-to-end driving to
obtain the global context of the 3D scene. In previous works, camera and LiDAR
inputs are fused through transformers for better driving performance. These
inputs are normally further interpreted as high-level map information to assist
navigation tasks. Nevertheless, extracting useful information from the complex
map input is challenging, for redundant information may mislead the agent and
negatively affect driving performance. We propose a novel approach to
efficiently extract features from vectorized High-Definition (HD) maps and
utilize them in the end-to-end driving tasks. In addition, we design a new
expert to further enhance the model performance by considering multi-road
rules. Experimental results prove that both of the proposed improvements enable
our agent to achieve superior performance compared with other methods.",https://github.com/Kin-Zhang/mmfn,-1
81e8c72c-a2f6-4f1b-bc18-2417787fae71,Deep Surface Reconstruction from Point Clouds with Visibility Information,0.303985,"Most current neural networks for reconstructing surfaces from point clouds
ignore sensor poses and only operate on raw point locations. Sensor visibility,
however, holds meaningful information regarding space occupancy and surface
orientation. In this paper, we present two simple ways to augment raw point
clouds with visibility information, so it can directly be leveraged by surface
reconstruction networks with minimal adaptation. Our proposed modifications
consistently improve the accuracy of generated surfaces as well as the
generalization ability of the networks to unseen shape domains. Our code and
data is available at https://github.com/raphaelsulzer/dsrv-data.",https://github.com/raphaelsulzer/dsrv-data,-1
155d1dc8-692a-4857-a32a-785699e3d157,UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture,0.944973,"We present UnrealEgo, i.e., a new large-scale naturalistic dataset for
egocentric 3D human pose estimation. UnrealEgo is based on an advanced concept
of eyeglasses equipped with two fisheye cameras that can be used in
unconstrained environments. We design their virtual prototype and attach them
to 3D human models for stereo view capture. We next generate a large corpus of
human motions. As a consequence, UnrealEgo is the first dataset to provide
in-the-wild stereo images with the largest variety of motions among existing
egocentric datasets. Furthermore, we propose a new benchmark method with a
simple but effective idea of devising a 2D keypoint estimation module for
stereo inputs to improve 3D human pose estimation. The extensive experiments
show that our approach outperforms the previous state-of-the-art methods
qualitatively and quantitatively. UnrealEgo and our source codes are available
on our project web page.",None,42457
3df3fd2b-5bcb-4e99-b057-e30f427a4619,Balancing Discriminability and Transferability for Source-Free Domain Adaptation,0.977451,"Conventional domain adaptation (DA) techniques aim to improve domain
transferability by learning domain-invariant representations; while
concurrently preserving the task-discriminability knowledge gathered from the
labeled source data. However, the requirement of simultaneous access to labeled
source and unlabeled target renders them unsuitable for the challenging
source-free DA setting. The trivial solution of realizing an effective original
to generic domain mapping improves transferability but degrades task
discriminability. Upon analyzing the hurdles from both theoretical and
empirical standpoints, we derive novel insights to show that a mixup between
original and corresponding translated generic samples enhances the
discriminability-transferability trade-off while duly respecting the
privacy-oriented source-free setting. A simple but effective realization of the
proposed insights on top of the existing source-free DA approaches yields
state-of-the-art performance with faster convergence. Beyond single-source, we
also outperform multi-source prior-arts across both classification and semantic
segmentation benchmarks.",https://github.com/iver56/audiomentations,13740
c3a7f587-5d09-45a1-a896-1923001ee4a4,Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,0.305748,"This paper studies multi-task training of retrieval-augmented generation
models for knowledge-intensive tasks. We propose to clean the training set by
utilizing a distinct property of knowledge-intensive generation: The connection
of query-answer pairs to items in the knowledge base. We filter training
examples via a threshold of confidence on the relevance labels, whether a pair
is answerable by the knowledge base or not. We train a single Fusion-in-Decoder
(FiD) generator on seven combined tasks of the KILT benchmark. The experimental
results suggest that our simple yet effective approach substantially improves
competitive baselines on two strongly imbalanced tasks; and shows either
smaller improvements or no significant regression on the remaining tasks.
Furthermore, we demonstrate our multi-task training with relevance label
sampling scales well with increased model capacity and achieves
state-of-the-art results in five out of seven KILT tasks.",None,-1
a30acb43-eb85-4430-b762-b279e70553e6,Diverse Multiple Trajectory Prediction Using a Two-stage Prediction Network Trained with Lane Loss,0.630542,"Prior arts in the field of motion predictions for autonomous driving tend to
focus on finding a trajectory that is close to the ground truth trajectory.
Such problem formulations and approaches, however, frequently lead to loss of
diversity and biased trajectory predictions. Therefore, they are unsuitable for
real-world autonomous driving where diverse and road-dependent multimodal
trajectory predictions are critical for safety. To this end, this study
proposes a novel loss function, \textit{Lane Loss}, that ensures map-adaptive
diversity and accommodates geometric constraints. A two-stage trajectory
prediction architecture with a novel trajectory candidate proposal module,
\textit{Trajectory Prediction Attention (TPA)}, is trained with Lane Loss
encourages multiple trajectories to be diversely distributed, covering feasible
maneuvers in a map-aware manner. Furthermore, considering that the existing
trajectory performance metrics are focusing on evaluating the accuracy based on
the ground truth future trajectory, a quantitative evaluation metric is also
suggested to evaluate the diversity of predicted multiple trajectories. The
experiments performed on the Argoverse dataset show that the proposed method
significantly improves the diversity of the predicted trajectories without
sacrificing the prediction accuracy.",None,-1
d7660443-73c0-465c-b185-1b918cc1964b,ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding,1.0,"Recently, learned image compression techniques have achieved remarkable
performance, even surpassing the best manually designed lossy image coders.
They are promising to be large-scale adopted. For the sake of practicality, a
thorough investigation of the architecture design of learned image compression,
regarding both compression performance and running speed, is essential. In this
paper, we first propose uneven channel-conditional adaptive coding, motivated
by the observation of energy compaction in learned image compression. Combining
the proposed uneven grouping model with existing context models, we obtain a
spatial-channel contextual adaptive model to improve the coding performance
without damage to running speed. Then we study the structure of the main
transform and propose an efficient model, ELIC, to achieve state-of-the-art
speed and compression ability. With superior performance, the proposed model
also supports extremely fast preview decoding and progressive decoding, which
makes the coming application of learning-based image compression more
promising.",https://github.com/InterDigitalInc/CompressAI/blob/v1.1.8/results/kodak/vtm.json,-1
4e0feaf8-6b8e-4c67-b41a-bff53abf61c3,End-to-end Multilingual Coreference Resolution with Mention Head Prediction,0.650769,"This paper describes our approach to the CRAC 2022 Shared Task on
Multilingual Coreference Resolution. Our model is based on a state-of-the-art
end-to-end coreference resolution system. Apart from joined multilingual
training, we improved our results with mention head prediction. We also tried
to integrate dependency information into our model. Our system ended up in
$3^{rd}$ place. Moreover, we reached the best performance on two datasets out
of 13.",https://github.com/ufal/corefud-scorer,-1
6c5b9fee-a3e0-480e-940b-6567977446b6,Causal Fairness Analysis,0.794597,"Decision-making systems based on AI and machine learning have been used
throughout a wide range of real-world scenarios, including healthcare, law
enforcement, education, and finance. It is no longer far-fetched to envision a
future where autonomous systems will be driving entire business decisions and,
more broadly, supporting large-scale decision-making infrastructure to solve
society's most challenging problems. Issues of unfairness and discrimination
are pervasive when decisions are being made by humans, and remain (or are
potentially amplified) when decisions are made using machines with little
transparency, accountability, and fairness. In this paper, we introduce a
framework for \textit{causal fairness analysis} with the intent of filling in
this gap, i.e., understanding, modeling, and possibly solving issues of
fairness in decision-making settings. The main insight of our approach will be
to link the quantification of the disparities present on the observed data with
the underlying, and often unobserved, collection of causal mechanisms that
generate the disparity in the first place, challenge we call the Fundamental
Problem of Causal Fairness Analysis (FPCFA). In order to solve the FPCFA, we
study the problem of decomposing variations and empirical measures of fairness
that attribute such variations to structural mechanisms and different units of
the population. Our effort culminates in the Fairness Map, which is the first
systematic attempt to organize and explain the relationship between different
criteria found in the literature. Finally, we study which causal assumptions
are minimally needed for performing causal fairness analysis and propose a
Fairness Cookbook, which allows data scientists to assess the existence of
disparate impact and disparate treatment.",None,-1
57cce177-4e92-4c4e-b406-774e3cda46f1,Arbitrary-Scale Image Synthesis,0.453418,"Positional encodings have enabled recent works to train a single adversarial
network that can generate images of different scales. However, these approaches
are either limited to a set of discrete scales or struggle to maintain good
perceptual quality at the scales for which the model is not trained explicitly.
We propose the design of scale-consistent positional encodings invariant to our
generator's layers transformations. This enables the generation of
arbitrary-scale images even at scales unseen during training. Moreover, we
incorporate novel inter-scale augmentations into our pipeline and partial
generation training to facilitate the synthesis of consistent images at
arbitrary scales. Lastly, we show competitive results for a continuum of scales
on various commonly used datasets for image synthesis.",https://github.com/open-mmlab/mmgeneration,-1
ea5187eb-18fe-4cc7-b90f-3d7842a49bbb,RankGen: Improving Text Generation with Large Ranking Models,0.585302,"Given an input sequence (or prefix), modern language models often assign high
probabilities to output sequences that are repetitive, incoherent, or
irrelevant to the prefix; as such, model-generated text also contains such
artifacts. To address these issues we present RankGen, a 1.2B parameter encoder
model for English that scores model generations given a prefix. RankGen can be
flexibly incorporated as a scoring function in beam search and used to decode
from any pretrained language model. We train RankGen using large-scale
contrastive learning to map a prefix close to the ground-truth sequence that
follows it and far away from two types of negatives: (1) random sequences from
the same document as the prefix, and (2) sequences generated from a large
language model conditioned on the prefix. Experiments across four different
language models (345M-11B parameters) and two domains show that RankGen
significantly outperforms decoding algorithms like nucleus, top-k, and typical
sampling, as well as contrastive decoding and search, on both automatic metrics
(85.0 vs 77.3 MAUVE over nucleus) as well as human evaluations with English
writers (74.5% human preference over nucleus sampling). Analysis reveals that
RankGen outputs are more relevant to the prefix and improve continuity and
coherence compared to baselines. We release our model checkpoints, code, and
human preference data with explanations to facilitate future research.",https://github.com/martiansideofthemoon/rankgen,-1
0686f247-acf5-4fa5-8f0c-6d34ed4ea4ce,A Diversity-Aware Domain Development Methodology,0.403651,"The development of domain ontological models, though being a mature research
arena backed by well-established methodologies, still suffer from two key
shortcomings. Firstly, the issues concerning the semantic persistency of
ontology concepts and their flexible reuse in domain development employing
existing approaches. Secondly, due to the difficulty in understanding and
reusing top-level concepts in existing foundational ontologies, the obfuscation
regarding the semantic nature of domain representations. The paper grounds the
aforementioned shortcomings in representation diversity and proposes a
three-fold solution - (i) a pipeline for rendering concepts reuse-ready, (ii) a
first characterization of a minimalistic foundational knowledge model, named
foundational teleology, semantically explicating foundational distinctions
enforcing the static as well as dynamic nature of domain representations, and
(iii) a flexible, reuse-native methodology for diversity-aware domain
development exploiting solutions (i) and (ii). The preliminary work reported
validates the potentiality of the solution components.",None,-1
bf7c4496-303b-48be-a5f9-9c94e7ce54c1,Latent Image Animator: Learning to Animate Images via Latent Space Navigation,0.95365,"Due to the remarkable progress of deep generative models, animating images
has become increasingly efficient, whereas associated results have become
increasingly realistic. Current animation-approaches commonly exploit structure
representation extracted from driving videos. Such structure representation is
instrumental in transferring motion from driving videos to still images.
However, such approaches fail in case the source image and driving video
encompass large appearance variation. Moreover, the extraction of structure
information requires additional modules that endow the animation-model with
increased complexity. Deviating from such models, we here introduce the Latent
Image Animator (LIA), a self-supervised autoencoder that evades need for
structure representation. LIA is streamlined to animate images by linear
navigation in the latent space. Specifically, motion in generated video is
constructed by linear displacement of codes in the latent space. Towards this,
we learn a set of orthogonal motion directions simultaneously, and use their
linear combination, in order to represent any displacement in the latent space.
Extensive quantitative and qualitative analysis suggests that our model
systematically and significantly outperforms state-of-art methods on VoxCeleb,
Taichi and TED-talk datasets w.r.t. generated quality.",https://wyhsirius.github.io/LIA-project/,-1
0f2d7158-41c0-416e-8350-99ba28039377,Uncertainty Quantification for Competency Assessment of Autonomous Agents,0.205691,"For safe and reliable deployment in the real world, autonomous agents must
elicit appropriate levels of trust from human users. One method to build trust
is to have agents assess and communicate their own competencies for performing
given tasks. Competency depends on the uncertainties affecting the agent,
making accurate uncertainty quantification vital for competency assessment. In
this work, we show how ensembles of deep generative models can be used to
quantify the agent's aleatoric and epistemic uncertainties when forecasting
task outcomes as part of competency assessment.",None,-1
c06b4141-066b-4ce4-bfb5-5338157e2423,Speeding Up Question Answering Task of Language Models via Inverted Index,0.0382962,"Natural language processing applications, such as conversational agents and
their question-answering capabilities, are widely used in the real world.
Despite the wide popularity of large language models (LLMs), few real-world
conversational agents take advantage of LLMs. Extensive resources consumed by
LLMs disable developers from integrating them into end-user applications. In
this study, we leverage an inverted indexing mechanism combined with LLMs to
improve the efficiency of question-answering models for closed-domain
questions. Our experiments show that using the index improves the average
response time by 97.44%. In addition, due to the reduced search scope, the
average BLEU score improved by 0.23 while using the inverted index.",None,-1
f24147dd-8817-45a1-a1b8-38d0f81500bb,"Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",0.672362,"As natural language processing systems become more widespread, it is
necessary to address fairness issues in their implementation and deployment to
ensure that their negative impacts on society are understood and minimized.
However, there is limited work that studies fairness using a multilingual and
intersectional framework or on downstream tasks. In this paper, we introduce
four multilingual Equity Evaluation Corpora, supplementary test sets designed
to measure social biases, and a novel statistical framework for studying
unisectional and intersectional social biases in natural language processing.
We use these tools to measure gender, racial, ethnic, and intersectional social
biases across five models trained on emotion regression tasks in English,
Spanish, and Arabic. We find that many systems demonstrate statistically
significant unisectional and intersectional social biases.",https://github.com/ascamara/,-1
83531106-4d5e-42d8-8a1c-786d04f7c38c,FisheyeHDK: Hyperbolic Deformable Kernel Learning for Ultra-Wide Field-of-View Image Recognition,0.793149,"Conventional convolution neural networks (CNNs) trained on narrow
Field-of-View (FoV) images are the state-of-the-art approaches for object
recognition tasks. Some methods proposed the adaptation of CNNs to ultra-wide
FoV images by learning deformable kernels. However, they are limited by the
Euclidean geometry and their accuracy degrades under strong distortions caused
by fisheye projections. In this work, we demonstrate that learning the shape of
convolution kernels in non-Euclidean spaces is better than existing deformable
kernel methods. In particular, we propose a new approach that learns deformable
kernel parameters (positions) in hyperbolic space. FisheyeHDK is a hybrid CNN
architecture combining hyperbolic and Euclidean convolution layers for
positions and features learning. First, we provide an intuition of hyperbolic
space for wide FoV images. Using synthetic distortion profiles, we demonstrate
the effectiveness of our approach. We select two datasets - Cityscapes and
BDD100K 2020 - of perspective images which we transform to fisheye equivalents
at different scaling factors (analog to focal lengths). Finally, we provide an
experiment on data collected by a real fisheye camera. Validations and
experiments show that our approach improves existing deformable kernel methods
for CNN adaptation on fisheye images.",None,-1
9f7b2106-b114-433b-9755-5ba22b693893,Intelligent Painter: Picture Composition With Resampling Diffusion Model,0.364044,"Have you ever thought that you can be an intelligent painter? This means that
you can paint a picture with a few expected objects in mind, or with a
desirable scene. This is different from normal inpainting approaches for which
the location of specific objects cannot be determined. In this paper, we
present an intelligent painter that generate a person's imaginary scene in one
go, given explicit hints. We propose a resampling strategy for Denoising
Diffusion Probabilistic Model (DDPM) to intelligently compose unconditional
harmonized pictures according to the input subjects at specific locations. By
exploiting the diffusion property, we resample efficiently to produce realistic
pictures. Experimental results show that our resampling method favors the
semantic meaning of the generated output efficiently and generates less blurry
output. Quantitative analysis of image quality assessment shows that our method
produces higher perceptual quality images compared with the state-of-the-art
methods.",https://github.com/vinesmsuic/ipainter-diffusion,-1
4dc6069d-b75f-4bd4-b14a-53cb45073003,Efficient Knowledge Distillation from Model Checkpoints,0.716099,"Knowledge distillation is an effective approach to learn compact models
(students) with the supervision of large and strong models (teachers). As
empirically there exists a strong correlation between the performance of
teacher and student models, it is commonly believed that a high performing
teacher is preferred. Consequently, practitioners tend to use a well trained
network or an ensemble of them as the teacher. In this paper, we make an
intriguing observation that an intermediate model, i.e., a checkpoint in the
middle of the training procedure, often serves as a better teacher compared to
the fully converged model, although the former has much lower accuracy. More
surprisingly, a weak snapshot ensemble of several intermediate models from a
same training trajectory can outperform a strong ensemble of independently
trained and fully converged models, when they are used as teachers. We show
that this phenomenon can be partially explained by the information bottleneck
principle: the feature representations of intermediate models can have higher
mutual information regarding the input, and thus contain more ""dark knowledge""
for effective distillation. We further propose an optimal intermediate teacher
selection algorithm based on maximizing the total task-related mutual
information. Experiments verify its effectiveness and applicability.",https://github.com/LeapLabTHU/CheckpointKD,-1
8f10882c-e54b-4a6f-b643-1ae86514f2d1,MDMLP: Image Classification from Scratch on Small Datasets with MLP,0.178967,"The attention mechanism has become a go-to technique for natural language
processing and computer vision tasks. Recently, the MLP-Mixer and other
MLP-based architectures, based simply on multi-layer perceptrons (MLPs), are
also powerful compared to CNNs and attention techniques and raises a new
research direction. However, the high capability of the MLP-based networks
severely relies on large volume of training data, and lacks of explanation
ability compared to the Vision Transformer (ViT) or ConvNets. When trained on
small datasets, they usually achieved inferior results than ConvNets. To
resolve it, we present (i) multi-dimensional MLP (MDMLP), a conceptually simple
and lightweight MLP-based architecture yet achieves SOTA when training from
scratch on small-size datasets; (ii) multi-dimension MLP Attention Tool
(MDAttnTool), a novel and efficient attention mechanism based on MLPs. Even
without strong data augmentation, MDMLP achieves 90.90% accuracy on CIFAR10
with only 0.3M parameters, while the well-known MLP-Mixer achieves 85.45% with
17.1M parameters. In addition, the lightweight MDAttnTool highlights objects in
images, indicating its explanation power. Our code is available at
https://github.com/Amoza-Theodore/MDMLP.",https://github.com/Amoza-Theodore/MDMLP,-1
4fc6eb27-1eb2-4976-bb5b-dff343b13577,Is Vanilla Policy Gradient Overlooked? Analyzing Deep Reinforcement Learning for Hanabi,0.0688755,"In pursuit of enhanced multi-agent collaboration, we analyze several
on-policy deep reinforcement learning algorithms in the recently published
Hanabi benchmark. Our research suggests a perhaps counter-intuitive finding,
where Proximal Policy Optimization (PPO) is outperformed by Vanilla Policy
Gradient over multiple random seeds in a simplified environment of the
multi-agent cooperative card game. In our analysis of this behavior we look
into Hanabi-specific metrics and hypothesize a reason for PPO's plateau. In
addition, we provide proofs for the maximum length of a perfect game (71 turns)
and any game (89 turns). Our code can be found at:
https://github.com/bramgrooten/DeepRL-for-Hanabi",https://github.com/bramgrooten/DeepRL-for-Hanabi,-1
7732ff66-f07c-4a9e-ba52-5b986e39a1d6,Energy-Aware Edge Association for Cluster-based Personalized Federated Learning,0.457948,"Federated Learning (FL) over wireless network enables data-conscious services
by leveraging the ubiquitous intelligence at network edge for
privacy-preserving model training. As the proliferation of context-aware
services, the diversified personal preferences causes disagreeing conditional
distributions among user data, which leads to poor inference performance. In
this sense, clustered federated learning is proposed to group user devices with
similar preference and provide each cluster with a personalized model. This
calls for innovative design in edge association that involves user clustering
and also resource management optimization. We formulate an accuracy-cost
trade-off optimization problem by jointly considering model accuracy,
communication resource allocation and energy consumption. To comply with
parameter encryption techniques in FL, we propose an iterative solution
procedure which employs deep reinforcement learning based approach at cloud
server for edge association. The reward function consists of minimized energy
consumption at each base station and the averaged model accuracy of all users.
Under our proposed solution, multiple edge base station are fully exploited to
realize cost efficient personalized federated learning without any prior
knowledge on model parameters. Simulation results show that our proposed
strategy outperforms existing strategies in achieving accurate learning at low
energy cost.",None,-1
20a0e3a9-3357-4187-b381-7e44c08597c6,Efficient Data-Plane Memory Scheduling for In-Network Aggregation,0.299729,"As the scale of distributed training grows, communication becomes a
bottleneck. To accelerate the communication, recent works introduce In-Network
Aggregation (INA), which moves the gradients summation into network
middle-boxes, e.g., programmable switches to reduce the traffic volume.
However, switch memory is scarce compared to the volume of gradients
transmitted in distributed training. Although literature applies methods like
pool-based streaming or dynamic sharing to tackle the mismatch, switch memory
is still a potential performance bottleneck. Furthermore, we observe the
under-utilization of switch memory due to the synchronization requirement for
aggregator deallocation in recent works. To improve the switch memory
utilization, we propose ESA, an $\underline{E}$fficient Switch Memory
$\underline{S}$cheduler for In-Network $\underline{A}$ggregation. At its cores,
ESA enforces the preemptive aggregator allocation primitive and introduces
priority scheduling at the data-plane, which improves the switch memory
utilization and average job completion time (JCT). Experiments show that ESA
can improve the average JCT by up to $1.35\times$.",https://github.com/in-ATP/ATP,35836
cac84599-4755-4957-9f6a-fa41c58b55da,Implementing Deep Learning-Based Approaches for Article Summarization in Indian Languages,0.440309,"The research on text summarization for low-resource Indian languages has been
limited due to the availability of relevant datasets. This paper presents a
summary of various deep-learning approaches used for the ILSUM 2022 Indic
language summarization datasets. The ISUM 2022 dataset consists of news
articles written in Indian English, Hindi, and Gujarati respectively, and their
ground-truth summarizations. In our work, we explore different pre-trained
seq2seq models and fine-tune those with the ILSUM 2022 datasets. In our case,
the fine-tuned SoTA PEGASUS model worked the best for English, the fine-tuned
IndicBART model with augmented data for Hindi, and again fine-tuned PEGASUS
model along with a translation mapping-based approach for Gujarati. Our scores
on the obtained inferences were evaluated using ROUGE-1, ROUGE-2, and ROUGE-4
as the evaluation metrics.",None,-1
4d1dc8df-c046-4199-af10-d99d8acde633,Consistency-based Self-supervised Learning for Temporal Anomaly Localization,0.233521,"This work tackles Weakly Supervised Anomaly detection, in which a predictor
is allowed to learn not only from normal examples but also from a few labeled
anomalies made available during training. In particular, we deal with the
localization of anomalous activities within the video stream: this is a very
challenging scenario, as training examples come only with video-level
annotations (and not frame-level). Several recent works have proposed various
regularization terms to address it i.e. by enforcing sparsity and smoothness
constraints over the weakly-learned frame-level anomaly scores. In this work,
we get inspired by recent advances within the field of self-supervised learning
and ask the model to yield the same scores for different augmentations of the
same video sequence. We show that enforcing such an alignment improves the
performance of the model on XD-Violence.",None,-1
ece30420-e35c-4c70-9480-367ff4ef4dfc,On the State of the Art in Authorship Attribution and Authorship Verification,0.834999,"Despite decades of research on authorship attribution (AA) and authorship
verification (AV), inconsistent dataset splits/filtering and mismatched
evaluation methods make it difficult to assess the state of the art. In this
paper, we present a survey of the fields, resolve points of confusion,
introduce Valla that standardizes and benchmarks AA/AV datasets and metrics,
provide a large-scale empirical evaluation, and provide apples-to-apples
comparisons between existing methods. We evaluate eight promising methods on
fifteen datasets (including distribution-shifted challenge sets) and introduce
a new large-scale dataset based on texts archived by Project Gutenberg.
Surprisingly, we find that a traditional Ngram-based model performs best on 5
(of 7) AA tasks, achieving an average macro-accuracy of $76.50\%$ (compared to
$66.71\%$ for a BERT-based model). However, on the two AA datasets with the
greatest number of words per author, as well as on the AV datasets, BERT-based
models perform best. While AV methods are easily applied to AA, they are seldom
included as baselines in AA papers. We show that through the application of
hard-negative mining, AV methods are competitive alternatives to AA methods.
Valla and all experiment code can be found here:
https://github.com/JacobTyo/Valla",https://github.com/JacobTyo/Valla,-1
3a26708a-469e-46ae-92c4-f8f7eb91d56d,BILP-Q: Quantum Coalition Structure Generation,0.687694,"Quantum AI is an emerging field that uses quantum computing to solve typical
complex problems in AI. In this work, we propose BILP-Q, the first-ever general
quantum approach for solving the Coalition Structure Generation problem (CSGP),
which is notably NP-hard. In particular, we reformulate the CSGP in terms of a
Quadratic Binary Combinatorial Optimization (QUBO) problem to leverage existing
quantum algorithms (e.g., QAOA) to obtain the best coalition structure. Thus,
we perform a comparative analysis in terms of time complexity between the
proposed quantum approach and the most popular classical baselines.
Furthermore, we consider standard benchmark distributions for coalition values
to test the BILP-Q on small-scale experiments using the IBM Qiskit environment.
Finally, since QUBO problems can be solved operating with quantum annealing, we
run BILP-Q on medium-size problems using a real quantum annealer (D-Wave).",https://github.com/supreethmv/BILP-Q,-1
0da71ac1-12dd-4616-b624-3f90d09a7a84,On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence,0.98567,"Ten years into the revival of deep networks and artificial intelligence, we
propose a theoretical framework that sheds light on understanding deep networks
within a bigger picture of Intelligence in general. We introduce two
fundamental principles, Parsimony and Self-consistency, that address two
fundamental questions regarding Intelligence: what to learn and how to learn,
respectively. We believe the two principles are the cornerstones for the
emergence of Intelligence, artificial or natural. While these two principles
have rich classical roots, we argue that they can be stated anew in entirely
measurable and computable ways. More specifically, the two principles lead to
an effective and efficient computational framework, compressive closed-loop
transcription, that unifies and explains the evolution of modern deep networks
and many artificial intelligence practices. While we mainly use modeling of
visual data as an example, we believe the two principles will unify
understanding of broad families of autonomous intelligent systems and provide a
framework for understanding the brain.",None,-1
c8df3495-e001-4021-8b5f-fd99748f233f,FedQAS: Privacy-aware machine reading comprehension with federated learning,0.414064,"Machine reading comprehension (MRC) of text data is one important task in
Natural Language Understanding. It is a complex NLP problem with a lot of
ongoing research fueled by the release of the Stanford Question Answering
Dataset (SQuAD) and Conversational Question Answering (CoQA). It is considered
to be an effort to teach computers how to ""understand"" a text, and then to be
able to answer questions about it using deep learning. However, until now
large-scale training on private text data and knowledge sharing has been
missing for this NLP task. Hence, we present FedQAS, a privacy-preserving
machine reading system capable of leveraging large-scale private data without
the need to pool those datasets in a central location. The proposed approach
combines transformer models and federated learning technologies. The system is
developed using the FEDn framework and deployed as a proof-of-concept alliance
initiative. FedQAS is flexible, language-agnostic, and allows intuitive
participation and execution of local model training. In addition, we present
the architecture and implementation of the system, as well as provide a
reference evaluation based on the SQUAD dataset, to showcase how it overcomes
data privacy issues and enables knowledge sharing between alliance members in a
Federated learning setting.",https://github.com/aitmlouk/FEDn-client-FedQAS-tf.git,-1
222ac523-6497-46fa-9661-8f1674dafe80,An Online Approach to Solve the Dynamic Vehicle Routing Problem with Stochastic Trip Requests for Paratransit Services,0.601083,"Many transit agencies operating paratransit and microtransit services have to
respond to trip requests that arrive in real-time, which entails solving hard
combinatorial and sequential decision-making problems under uncertainty. To
avoid decisions that lead to significant inefficiency in the long term,
vehicles should be allocated to requests by optimizing a non-myopic utility
function or by batching requests together and optimizing a myopic utility
function. While the former approach is typically offline, the latter can be
performed online. We point out two major issues with such approaches when
applied to paratransit services in practice. First, it is difficult to batch
paratransit requests together as they are temporally sparse. Second, the
environment in which transit agencies operate changes dynamically (e.g.,
traffic conditions), causing estimates that are learned offline to become
stale. To address these challenges, we propose a fully online approach to solve
the dynamic vehicle routing problem (DVRP) with time windows and stochastic
trip requests that is robust to changing environmental dynamics by
construction. We focus on scenarios where requests are relatively sparse - our
problem is motivated by applications to paratransit services. We formulate DVRP
as a Markov decision process and use Monte Carlo tree search to evaluate
actions for any given state. Accounting for stochastic requests while
optimizing a non-myopic utility function is computationally challenging;
indeed, the action space for such a problem is intractably large in practice.
To tackle the large action space, we leverage the structure of the problem to
design heuristics that can sample promising actions for the tree search. Our
experiments using real-world data from our partner agency show that the
proposed approach outperforms existing state-of-the-art approaches both in
terms of performance and robustness.",https://github.com/smarttransit-ai/iccps-2022-paratransit-public,-1
35711953-ecd6-4d3a-8351-7b5ae9758d6d,Efficient yet Competitive Speech Translation: FBK@IWSLT2022,0.688462,"The primary goal of this FBK's systems submission to the IWSLT 2022 offline
and simultaneous speech translation tasks is to reduce model training costs
without sacrificing translation quality. As such, we first question the need of
ASR pre-training, showing that it is not essential to achieve competitive
results. Second, we focus on data filtering, showing that a simple method that
looks at the ratio between source and target characters yields a quality
improvement of 1 BLEU. Third, we compare different methods to reduce the
detrimental effect of the audio segmentation mismatch between training data
manually segmented at sentence level and inference data that is automatically
segmented. Towards the same goal of training cost reduction, we participate in
the simultaneous task with the same model trained for offline ST. The
effectiveness of our lightweight training strategy is shown by the high score
obtained on the MuST-C en-de corpus (26.7 BLEU) and is confirmed in
high-resource data conditions by a 1.6 BLEU improvement on the IWSLT2020 test
set over last year's winning system.",https://github.com/hlt-mt/FBK-fairseq,-1
348304cd-73d0-477f-a6fc-191f22c9d12c,StegaNeRF: Embedding Invisible Information within Neural Radiance Fields,0.581078,"Recent advances in neural rendering imply a future of widespread visual data
distributions through sharing NeRF model weights. However, while common visual
data (images and videos) have standard approaches to embed ownership or
copyright information explicitly or subtly, the problem remains unexplored for
the emerging NeRF format. We present StegaNeRF, a method for steganographic
information embedding in NeRF renderings. We design an optimization framework
allowing accurate hidden information extractions from images rendered by NeRF,
while preserving its original visual quality. We perform experimental
evaluations of our method under several potential deployment scenarios, and we
further discuss the insights discovered through our analysis. StegaNeRF
signifies an initial exploration into the novel problem of instilling
customizable, imperceptible, and recoverable information to NeRF renderings,
with minimal impact to rendered images. Project page:
https://xggnet.github.io/StegaNeRF/.",None,-1
9e3793a9-cbc9-4636-b0e0-6f65b73ac2a8,Learning a General Clause-to-Clause Relationships for Enhancing Emotion-Cause Pair Extraction,0.761617,"Emotion-cause pair extraction (ECPE) is an emerging task aiming to extract
potential pairs of emotions and corresponding causes from documents. Previous
approaches have focused on modeling the pair-to-pair relationship and achieved
promising results. However, the clause-to-clause relationship, which
fundamentally symbolizes the underlying structure of a document, has still been
in its research infancy. In this paper, we define a novel clause-to-clause
relationship. To learn it applicably, we propose a general clause-level
encoding model named EA-GAT comprising E-GAT and Activation Sort. E-GAT is
designed to aggregate information from different types of clauses; Activation
Sort leverages the individual emotion/cause prediction and the sort-based
mapping to propel the clause to a more favorable representation. Since EA-GAT
is a clause-level encoding model, it can be broadly integrated with any
previous approach. Experimental results show that our approach has a
significant advantage over all current approaches on the Chinese and English
benchmark corpus, with an average of $2.1\%$ and $1.03\%$.",None,-1
89ca2831-dc14-4a5d-a670-7de627f72575,The THUEE System Description for the IARPA OpenASR21 Challenge,0.0597153,"This paper describes the THUEE team's speech recognition system for the IARPA
Open Automatic Speech Recognition Challenge (OpenASR21), with further
experiment explorations. We achieve outstanding results under both the
Constrained and Constrained-plus training conditions. For the Constrained
training condition, we construct our basic ASR system based on the standard
hybrid architecture. To alleviate the Out-Of-Vocabulary (OOV) problem, we
extend the pronunciation lexicon using Grapheme-to-Phoneme (G2P) techniques for
both OOV and potential new words. Standard acoustic model structures such as
CNN-TDNN-F and CNN-TDNN-F-A are adopted. In addition, multiple data
augmentation techniques are applied. For the Constrained-plus training
condition, we use the self-supervised learning framework wav2vec2.0. We
experiment with various fine-tuning techniques with the Connectionist Temporal
Classification (CTC) criterion on top of the publicly available pre-trained
model XLSR-53. We find that the frontend feature extractor plays an important
role when applying the wav2vec2.0 pre-trained model to the encoder-decoder
based CTC/Attention ASR architecture. Extra improvements can be achieved by
using the CTC model finetuned in the target language as the frontend feature
extractor.",https://github.com/pytorch/fairseq,-1
cbad936b-914a-4aed-b01f-3f034a457785,Continuous Scene Representations for Embodied AI,0.861507,"We propose Continuous Scene Representations (CSR), a scene representation
constructed by an embodied agent navigating within a space, where objects and
their relationships are modeled by continuous valued embeddings. Our method
captures feature relationships between objects, composes them into a graph
structure on-the-fly, and situates an embodied agent within the representation.
Our key insight is to embed pair-wise relationships between objects in a latent
space. This allows for a richer representation compared to discrete relations
(e.g., [support], [next-to]) commonly used for building scene representations.
CSR can track objects as the agent moves in a scene, update the representation
accordingly, and detect changes in room configurations. Using CSR, we
outperform state-of-the-art approaches for the challenging downstream task of
visual room rearrangement, without any task specific training. Moreover, we
show the learned embeddings capture salient spatial details of the scene and
show applicability to real world data. A summery video and code is available at
https://prior.allenai.org/projects/csr.",https://github.com/facebookresearch/detectron2,-1
fc45b8ff-4714-40e2-bed6-8f05001e164c,Handwritten Arabic Character Recognition for Children Writ-ing Using Convolutional Neural Network and Stroke Identification,0.891498,"Automatic Arabic handwritten recognition is one of the recently studied
problems in the field of Machine Learning. Unlike Latin languages, Arabic is a
Semitic language that forms a harder challenge, especially with variability of
patterns caused by factors such as writer age. Most of the studies focused on
adults, with only one recent study on children. Moreover, much of the recent
Machine Learning methods focused on using Convolutional Neural Networks, a
powerful class of neural networks that can extract complex features from
images. In this paper we propose a convolutional neural network (CNN) model
that recognizes children handwriting with an accuracy of 91% on the Hijja
dataset, a recent dataset built by collecting images of the Arabic characters
written by children, and 97% on Arabic Handwritten Character Dataset. The
results showed a good improvement over the proposed model from the Hijja
dataset authors, yet it reveals a bigger challenge to solve for children Arabic
handwritten character recognition. Moreover, we proposed a new approach using
multi models instead of single model based on the number of strokes in a
character, and merged Hijja with AHCD which reached an averaged prediction
accuracy of 96%.",None,-1
410ce9cf-9cd9-49b5-8b49-baf764f56707,Better plain ViT baselines for ImageNet-1k,0.668979,"It is commonly accepted that the Vision Transformer model requires
sophisticated regularization techniques to excel at ImageNet-1k scale data.
Surprisingly, we find this is not the case and standard data augmentation is
sufficient. This note presents a few minor modifications to the original Vision
Transformer (ViT) vanilla training setting that dramatically improve the
performance of plain ViT models. Notably, 90 epochs of training surpass 76%
top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic
ResNet50 baseline, and 300 epochs of training reach 80% in less than one day.",https://github.com/google-research/big_vision,-1
7ba53a1c-3231-416b-a8c3-9e75c8656a85,GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator,0.652066,"Pre-trained models have achieved remarkable success in natural language
processing (NLP). However, existing pre-training methods underutilize the
benefits of language understanding for generation. Inspired by the idea of
Generative Adversarial Networks (GANs), we propose a GAN-style model for
encoder-decoder pre-training by introducing an auxiliary discriminator,
unifying the ability of language understanding and generation in a single
model. Our model, named as GanLM, is trained with two pre-training objectives:
replaced token detection and replaced token denoising. Specifically, given
masked source sentences, the generator outputs the target distribution and the
discriminator predicts whether the target sampled tokens from distribution are
incorrect. The target sentence is replaced with misclassified tokens to
construct noisy previous context, which is used to generate the gold sentence.
In general, both tasks improve the ability of language understanding and
generation by selectively using the denoising data. Extensive experiments in
language generation benchmarks show that GanLM with the powerful language
understanding capability outperforms various strong pre-trained language models
(PLMs) and achieves state-of-the-art performance.",https://github.com/CSJianYang/GanLM,-1
0bda1525-d03a-4786-82d4-9e7d9576fe93,Training Deep Learning Algorithms on Synthetic Forest Images for Tree Detection,0.362788,"Vision-based segmentation in forested environments is a key functionality for
autonomous forestry operations such as tree felling and forwarding. Deep
learning algorithms demonstrate promising results to perform visual tasks such
as object detection. However, the supervised learning process of these
algorithms requires annotations from a large diversity of images. In this work,
we propose to use simulated forest environments to automatically generate 43 k
realistic synthetic images with pixel-level annotations, and use it to train
deep learning algorithms for tree detection. This allows us to address the
following questions: i) what kind of performance should we expect from deep
learning in harsh synthetic forest environments, ii) which annotations are the
most important for training, and iii) what modality should be used between RGB
and depth. We also report the promising transfer learning capability of
features learned on our synthetic dataset by directly predicting bounding box,
segmentation masks and keypoints on real images. Code available on GitHub
(https://github.com/norlab-ulaval/PercepTreeV1).",https://github.com/norlab-ulaval/PercepTreeV1,-1
959932ef-a077-4188-aa89-e4eaf7aa0e61,"""Does it come in black?"" CLIP-like models are zero-shot recommenders",0.111777,"Product discovery is a crucial component for online shopping. However,
item-to-item recommendations today do not allow users to explore changes along
selected dimensions: given a query item, can a model suggest something similar
but in a different color? We consider item recommendations of the comparative
nature (e.g. ""something darker"") and show how CLIP-based models can support
this use case in a zero-shot manner. Leveraging a large model built for
fashion, we introduce GradREC and its industry potential, and offer a first
rounded assessment of its strength and weaknesses.",https://github.com/patrickjohncyh/gradient-recs,-1
1d57a122-612d-4023-b32c-95676c999df1,What You See is What You Classify: Black Box Attributions,0.202878,"An important step towards explaining deep image classifiers lies in the
identification of image regions that contribute to individual class scores in
the model's output. However, doing this accurately is a difficult task due to
the black-box nature of such networks. Most existing approaches find such
attributions either using activations and gradients or by repeatedly perturbing
the input. We instead address this challenge by training a second deep network,
the Explainer, to predict attributions for a pre-trained black-box classifier,
the Explanandum. These attributions are provided in the form of masks that only
show the classifier-relevant parts of an image, masking out the rest. Our
approach produces sharper and more boundary-precise masks when compared to the
saliency maps generated by other methods. Moreover, unlike most existing
approaches, ours is capable of directly generating very distinct class-specific
masks in a single forward pass. This makes the proposed method very efficient
during inference. We show that our attributions are superior to established
methods both visually and quantitatively with respect to the PASCAL VOC-2007
and Microsoft COCO-2014 datasets.",https://github.com/stevenstalder/NN-Explainer,-1
9fed0e45-06aa-47e8-8283-704f35c7865c,Robot Vitals and Robot Health: Towards Systematically Quantifying Runtime Performance Degradation in Robots Under Adverse Conditions,0.594093,"This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.",https://github.com/anikethramesh/robotVitals,-1
57d36da5-7438-4db7-832a-a54eb4e0abb1,Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features,0.660481,"We consider the problem of category-level 6D pose estimation from a single
RGB image. Our approach represents an object category as a cuboid mesh and
learns a generative model of the neural feature activations at each mesh vertex
to perform pose estimation through differentiable rendering. A common problem
of rendering-based approaches is that they rely on bounding box proposals,
which do not convey information about the 3D rotation of the object and are not
reliable when objects are partially occluded. Instead, we introduce a
coarse-to-fine optimization strategy that utilizes the rendering process to
estimate a sparse set of 6D object proposals, which are subsequently refined
with gradient-based optimization. The key to enabling the convergence of our
approach is a neural feature representation that is trained to be scale- and
rotation-invariant using contrastive learning. Our experiments demonstrate an
enhanced category-level 6D pose estimation performance compared to prior work,
particularly under strong partial occlusion.",None,-1
773fc692-d06b-4c4a-87d2-dcf24f9052e8,EAPruning: Evolutionary Pruning for Vision Transformers and CNNs,0.232521,"Structured pruning greatly eases the deployment of large neural networks in
resource-constrained environments. However, current methods either involve
strong domain expertise, require extra hyperparameter tuning, or are restricted
only to a specific type of network, which prevents pervasive industrial
applications. In this paper, we undertake a simple and effective approach that
can be easily applied to both vision transformers and convolutional neural
networks. Specifically, we consider pruning as an evolution process of
sub-network structures that inherit weights through reconstruction techniques.
We achieve a 50% FLOPS reduction for ResNet50 and MobileNetV1, leading to 1.37x
and 1.34x speedup respectively. For DeiT-Base, we reach nearly 40% FLOPs
reduction and 1.4x speedup. Our code will be made available.",None,-1
8d95c39b-fa9c-42b3-95d2-bb0d28c23393,Towards High-Fidelity Single-view Holistic Reconstruction of Indoor Scenes,0.783021,"We present a new framework to reconstruct holistic 3D indoor scenes including
both room background and indoor objects from single-view images. Existing
methods can only produce 3D shapes of indoor objects with limited geometry
quality because of the heavy occlusion of indoor scenes. To solve this, we
propose an instance-aligned implicit function (InstPIFu) for detailed object
reconstruction. Combining with instance-aligned attention module, our method is
empowered to decouple mixed local features toward the occluded instances.
Additionally, unlike previous methods that simply represents the room
background as a 3D bounding box, depth map or a set of planes, we recover the
fine geometry of the background via implicit representation. Extensive
experiments on the SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT datasets
demonstrate that our method outperforms existing approaches in both background
and foreground object reconstruction. Our code and model will be made publicly
available.",None,-1
30b93130-91ea-484c-9623-bd5fcac4954d,Normal and Visibility Estimation of Human Face from a Single Image,0.0309371,"Recent work on the intrinsic image of humans starts to consider the
visibility of incident illumination and encodes the light transfer function by
spherical harmonics. In this paper, we show that such a light transfer function
can be further decomposed into visibility and cosine terms related to surface
normal. Such decomposition allows us to recover the surface normal in addition
to visibility. We propose a deep learning-based approach with a reconstruction
loss for training on real-world images. Results show that compared with
previous works, the reconstruction of human face from our method better reveals
the surface normal and shading details especially around regions where
visibility effect is strong.",None,-1
7caf0eba-e000-4298-8a65-8037d738b438,An Empirical Study on Cross-X Transfer for Legal Judgment Prediction,0.749591,"Cross-lingual transfer learning has proven useful in a variety of Natural
Language Processing (NLP) tasks, but it is understudied in the context of legal
NLP, and not at all in Legal Judgment Prediction (LJP). We explore transfer
learning techniques on LJP using the trilingual Swiss-Judgment-Prediction
dataset, including cases written in three languages. We find that cross-lingual
transfer improves the overall results across languages, especially when we use
adapter-based fine-tuning. Finally, we further improve the model's performance
by augmenting the training dataset with machine-translated versions of the
original documents, using a 3x larger training corpus. Further on, we perform
an analysis exploring the effect of cross-domain and cross-regional transfer,
i.e., train a model across domains (legal areas), or regions. We find that in
both settings (legal areas, origin regions), models trained across all groups
perform overall better, while they also have improved results in the worst-case
scenarios. Finally, we report improved results when we ambitiously apply
cross-jurisdiction transfer, where we further augment our dataset with Indian
legal cases.",https://github.com/UKPLab/EasyNMT,-1
d425139e-e4f5-4865-93cc-1fb367568ebe,Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion,0.353761,"Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task.",None,-1
229d5883-55c5-4c98-91aa-594c68c1de40,GAN You Hear Me? Reclaiming Unconditional Speech Synthesis from Diffusion Models,0.414735,"We propose AudioStyleGAN (ASGAN), a new generative adversarial network (GAN)
for unconditional speech synthesis. As in the StyleGAN family of image
synthesis models, ASGAN maps sampled noise to a disentangled latent vector
which is then mapped to a sequence of audio features so that signal aliasing is
suppressed at every layer. To successfully train ASGAN, we introduce a number
of new techniques, including a modification to adaptive discriminator
augmentation to probabilistically skip discriminator updates. ASGAN achieves
state-of-the-art results in unconditional speech synthesis on the Google Speech
Commands dataset. It is also substantially faster than the top-performing
diffusion models. Through a design that encourages disentanglement, ASGAN is
able to perform voice conversion and speech editing without being explicitly
trained to do so. ASGAN demonstrates that GANs are still highly competitive
with diffusion models. Code, models, samples:
https://github.com/RF5/simple-asgan/.",https://github.com/RF5/simple-asgan/,-1
4257c839-9a06-4099-bcd3-500628137e22,Learning Visuo-Haptic Skewering Strategies for Robot-Assisted Feeding,0.996701,"Acquiring food items with a fork poses an immense challenge to a
robot-assisted feeding system, due to the wide range of material properties and
visual appearances present across food groups. Deformable foods necessitate
different skewering strategies than firm ones, but inferring such
characteristics for several previously unseen items on a plate remains
nontrivial. Our key insight is to leverage visual and haptic observations
during interaction with an item to rapidly and reactively plan skewering
motions. We learn a generalizable, multimodal representation for a food item
from raw sensory inputs which informs the optimal skewering strategy. Given
this representation, we propose a zero-shot framework to sense visuo-haptic
properties of a previously unseen item and reactively skewer it, all within a
single interaction. Real-robot experiments with foods of varying levels of
visual and textural diversity demonstrate that our multimodal policy
outperforms baselines which do not exploit both visual and haptic cues or do
not reactively plan. Across 6 plates of different food items, our proposed
framework achieves 71% success over 69 skewering attempts total. Supplementary
material, datasets, code, and videos are available on our website:
https://sites.google.com/view/hapticvisualnet-corl22/home",None,-1
ab30587f-0cde-4d61-bb0f-fbbe9acf790a,Few-Shot Segmentation via Rich Prototype Generation and Recurrent Prediction Enhancement,0.0230787,"Prototype learning and decoder construction are the keys for few-shot
segmentation. However, existing methods use only a single prototype generation
mode, which can not cope with the intractable problem of objects with various
scales. Moreover, the one-way forward propagation adopted by previous methods
may cause information dilution from registered features during the decoding
process. In this research, we propose a rich prototype generation module (RPGM)
and a recurrent prediction enhancement module (RPEM) to reinforce the prototype
learning paradigm and build a unified memory-augmented decoder for few-shot
segmentation, respectively. Specifically, the RPGM combines superpixel and
K-means clustering to generate rich prototype features with complementary scale
relationships and adapt the scale gap between support and query images. The
RPEM utilizes the recurrent mechanism to design a round-way propagation
decoder. In this way, registered features can provide object-aware information
continuously. Experiments show that our method consistently outperforms other
competitors on two popular benchmarks PASCAL-${{5}^{i}}$ and COCO-${{20}^{i}}$.",None,-1
0fa93757-e7e2-44b0-bc6c-470b5585cc7e,Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,0.694067,"Despite their strong performance on many tasks, pre-trained language models
have been shown to struggle on out-of-distribution compositional
generalization. Meanwhile, recent work has shown considerable improvements on
many NLP tasks from model scaling. Can scaling up model size also improve
compositional generalization in semantic parsing? We evaluate encoder-decoder
models up to 11B parameters and decoder-only models up to 540B parameters, and
compare model scaling curves for three different methods for applying a
pre-trained language model to a new task: fine-tuning all parameters, prompt
tuning, and in-context learning. We observe that fine-tuning generally has flat
or negative scaling curves on out-of-distribution compositional generalization
in semantic parsing evaluations. In-context learning has positive scaling
curves, but is generally outperformed by much smaller fine-tuned models.
Prompt-tuning can outperform fine-tuning, suggesting further potential
improvements from scaling as it exhibits a more positive scaling curve.
Additionally, we identify several error trends that vary with model scale. For
example, larger models are generally better at modeling the syntax of the
output space, but are also more prone to certain types of overfitting. Overall,
our study highlights limitations of current techniques for effectively
leveraging model scale for compositional generalization, while our analysis
also suggests promising directions for future work.",https://github.com/microsoft/compositional-generalization-span-level-attention,20836
8ed12b45-8386-432b-882f-99ee3f608386,SPACE: Speech-driven Portrait Animation with Controllable Expression,0.911058,"Animating portraits using speech has received growing attention in recent
years, with various creative and practical use cases. An ideal generated video
should have good lip sync with the audio, natural facial expressions and head
motions, and high frame quality. In this work, we present SPACE, which uses
speech and a single image to generate high-resolution, and expressive videos
with realistic head pose, without requiring a driving video. It uses a
multi-stage approach, combining the controllability of facial landmarks with
the high-quality synthesis power of a pretrained face generator. SPACE also
allows for the control of emotions and their intensities. Our method
outperforms prior methods in objective metrics for image quality and facial
motions and is strongly preferred by users in pair-wise comparisons. The
project website is available at https://deepimagination.cc/SPACE/",None,-1
28d47079-9609-4bc9-9fad-29fb4cf1ec20,Rxn Hypergraph: a Hypergraph Attention Model for Chemical Reaction Representation,0.405678,"It is fundamental for science and technology to be able to predict chemical
reactions and their properties. To achieve such skills, it is important to
develop good representations of chemical reactions, or good deep learning
architectures that can learn such representations automatically from the data.
There is currently no universal and widely adopted method for robustly
representing chemical reactions. Most existing methods suffer from one or more
drawbacks, such as: (1) lacking universality; (2) lacking robustness; (3)
lacking interpretability; or (4) requiring excessive manual pre-processing.
Here we exploit graph-based representations of molecular structures to develop
and test a hypergraph attention neural network approach to solve at once the
reaction representation and property-prediction problems, alleviating the
aforementioned drawbacks. We evaluate this hypergraph representation in three
experiments using three independent data sets of chemical reactions. In all
experiments, the hypergraph-based approach matches or outperforms other
representations and their corresponding models of chemical reactions while
yielding interpretable multi-level representations.",None,-1
52606f6e-5948-4df4-9550-2a4c41ecb9c1,Unsupervised Domain Adaptation for Point Cloud Semantic Segmentation via Graph Matching,0.34518,"Unsupervised domain adaptation for point cloud semantic segmentation has
attracted great attention due to its effectiveness in learning with unlabeled
data. Most of existing methods use global-level feature alignment to transfer
the knowledge from the source domain to the target domain, which may cause the
semantic ambiguity of the feature space. In this paper, we propose a
graph-based framework to explore the local-level feature alignment between the
two domains, which can reserve semantic discrimination during adaptation.
Specifically, in order to extract local-level features, we first dynamically
construct local feature graphs on both domains and build a memory bank with the
graphs from the source domain. In particular, we use optimal transport to
generate the graph matching pairs. Then, based on the assignment matrix, we can
align the feature distributions between the two domains with the graph-based
local feature loss. Furthermore, we consider the correlation between the
features of different categories and formulate a category-guided contrastive
loss to guide the segmentation model to learn discriminative features on the
target domain. Extensive experiments on different synthetic-to-real and
real-to-real domain adaptation scenarios demonstrate that our method can
achieve state-of-the-art performance.",https://github.com/BianYikai/PointUDA,-1
27030c5e-4ded-4ea3-b39c-003d78e672a7,Probing for the Usage of Grammatical Number,0.913343,"A central quest of probing is to uncover how pre-trained models encode a
linguistic property within their representations. An encoding, however, might
be spurious-i.e., the model might not rely on it when making predictions. In
this paper, we try to find encodings that the model actually uses, introducing
a usage-based probing setup. We first choose a behavioral task which cannot be
solved without using the linguistic property. Then, we attempt to remove the
property by intervening on the model's representations. We contend that, if an
encoding is used by the model, its removal should harm the performance on the
chosen behavioral task. As a case study, we focus on how BERT encodes
grammatical number, and on how it uses this encoding to solve the number
agreement task. Experimentally, we find that BERT relies on a linear encoding
of grammatical number to produce the correct behavioral output. We also find
that BERT uses a separate encoding of grammatical number for nouns and verbs.
Finally, we identify in which layers information about grammatical number is
transferred from a noun to its head verb.",None,-1
b709658b-9107-488e-a7e5-bac3038156d2,Local Relighting of Real Scenes,0.0529478,"We introduce the task of local relighting, which changes a photograph of a
scene by switching on and off the light sources that are visible within the
image. This new task differs from the traditional image relighting problem, as
it introduces the challenge of detecting light sources and inferring the
pattern of light that emanates from them. We propose an approach for local
relighting that trains a model without supervision of any novel image dataset
by using synthetically generated image pairs from another model. Concretely, we
collect paired training images from a stylespace-manipulated GAN; then we use
these images to train a conditional image-to-image model. To benchmark local
relighting, we introduce Lonoff, a collection of 306 precisely aligned images
taken in indoor spaces with different combinations of lights switched on. We
show that our method significantly outperforms baseline methods based on GAN
inversion. Finally, we demonstrate extensions of our method that control
different light sources separately. We invite the community to tackle this new
task of local relighting.",None,-1
1ca05380-3bec-498a-8d0d-3d849f426ee6,Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation,0.464593,"3D Human body pose and shape estimation within a temporal sequence can be
quite critical for understanding human behavior. Despite the significant
progress in human pose estimation in the recent years, which are often based on
single images or videos, human motion estimation on live stream videos is still
a rarely-touched area considering its special requirements for real-time output
and temporal consistency. To address this problem, we present a temporally
embedded 3D human body pose and shape estimation (TePose) method to improve the
accuracy and temporal consistency of pose estimation in live stream videos.
TePose uses previous predictions as a bridge to feedback the error for better
estimation in the current frame and to learn the correspondence between data
frames and predictions in the history. A multi-scale spatio-temporal graph
convolutional network is presented as the motion discriminator for adversarial
training using datasets without any 3D labeling. We propose a sequential data
loading strategy to meet the special start-to-end data processing requirement
of live stream. We demonstrate the importance of each proposed module with
extensive experiments. The results show the effectiveness of TePose on
widely-used human pose benchmarks with state-of-the-art performance.",https://github.com/ostadabbas/TePose,2464
c01b4f42-f79c-4035-a1e4-263bda180195,CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds,0.987915,"We present a novel two-stage fully sparse convolutional 3D object detection
framework, named CAGroup3D. Our proposed method first generates some
high-quality 3D proposals by leveraging the class-aware local group strategy on
the object surface voxels with the same semantic predictions, which considers
semantic consistency and diverse locality abandoned in previous bottom-up
approaches. Then, to recover the features of missed voxels due to incorrect
voxel-wise segmentation, we build a fully sparse convolutional RoI pooling
module to directly aggregate fine-grained spatial information from backbone for
further proposal refinement. It is memory-and-computation efficient and can
better encode the geometry-specific features of each 3D proposal. Our model
achieves state-of-the-art 3D detection performance with remarkable gains of
+\textit{3.6\%} on ScanNet V2 and +\textit{2.6}\% on SUN RGB-D in term of
mAP@0.25. Code will be available at https://github.com/Haiyang-W/CAGroup3D.",https://github.com/Haiyang-W/CAGroup3D,-1
11af33cb-251e-49aa-8db2-48d57bb6557f,Fault-Aware Design and Training to Enhance DNNs Reliability with Zero-Overhead,0.655846,"Deep Neural Networks (DNNs) enable a wide series of technological
advancements, ranging from clinical imaging, to predictive industrial
maintenance and autonomous driving. However, recent findings indicate that
transient hardware faults may corrupt the models prediction dramatically. For
instance, the radiation-induced misprediction probability can be so high to
impede a safe deployment of DNNs models at scale, urging the need for efficient
and effective hardening solutions. In this work, we propose to tackle the
reliability issue both at training and model design time. First, we show that
vanilla models are highly affected by transient faults, that can induce a
performances drop up to 37%. Hence, we provide three zero-overhead solutions,
based on DNN re-design and re-train, that can improve DNNs reliability to
transient faults up to one order of magnitude. We complement our work with
extensive ablation studies to quantify the gain in performances of each
hardening component.",None,-1
ad06944a-e472-45b7-be79-b7a2b5a8eae9,Human-Object Interaction Detection via Disentangled Transformer,0.830849,"Human-Object Interaction Detection tackles the problem of joint localization
and classification of human object interactions. Existing HOI transformers
either adopt a single decoder for triplet prediction, or utilize two parallel
decoders to detect individual objects and interactions separately, and compose
triplets by a matching process. In contrast, we decouple the triplet prediction
into human-object pair detection and interaction classification. Our main
motivation is that detecting the human-object instances and classifying
interactions accurately needs to learn representations that focus on different
regions. To this end, we present Disentangled Transformer, where both encoder
and decoder are disentangled to facilitate learning of two sub-tasks. To
associate the predictions of disentangled decoders, we first generate a unified
representation for HOI triplets with a base decoder, and then utilize it as
input feature of each disentangled decoder. Extensive experiments show that our
method outperforms prior work on two public HOI benchmarks by a sizeable
margin. Code will be available.",https://github.com/facebookresearch/detectron2,-1
8d8afad9-ff69-40c7-8bdf-55d641bd9717,A Novel Neural Network Training Method for Autonomous Driving Using Semi-Pseudo-Labels and 3D Data Augmentations,0.0548285,"Training neural networks to perform 3D object detection for autonomous
driving requires a large amount of diverse annotated data. However, obtaining
training data with sufficient quality and quantity is expensive and sometimes
impossible due to human and sensor constraints. Therefore, a novel solution is
needed for extending current training methods to overcome this limitation and
enable accurate 3D object detection. Our solution for the above-mentioned
problem combines semi-pseudo-labeling and novel 3D augmentations. For
demonstrating the applicability of the proposed method, we have designed a
convolutional neural network for 3D object detection which can significantly
increase the detection range in comparison with the training data distribution.",None,-1
536305f6-58c2-44b0-bf08-c689ea358bb5,Transformer-based Entity Typing in Knowledge Graphs,0.741669,"We investigate the knowledge graph entity typing task which aims at inferring
plausible entity types. In this paper, we propose a novel Transformer-based
Entity Typing (TET) approach, effectively encoding the content of neighbors of
an entity. More precisely, TET is composed of three different mechanisms: a
local transformer allowing to infer missing types of an entity by independently
encoding the information provided by each of its neighbors; a global
transformer aggregating the information of all neighbors of an entity into a
single long sequence to reason about more complex entity types; and a context
transformer integrating neighbors content based on their contribution to the
type inference through information exchange between neighbor pairs.
Furthermore, TET uses information about class membership of types to
semantically strengthen the representation of an entity. Experiments on two
real-world datasets demonstrate the superior performance of TET compared to the
state-of-the-art.",https://github.com/zhiweihu1103/ET-TET,-1
fbc07ad2-050e-4b45-a236-3684151258ae,Non-Contrastive Learning Meets Language-Image Pre-Training,0.288636,"Contrastive language-image pre-training (CLIP) serves as a de-facto standard
to align images and texts. Nonetheless, the loose correlation between images
and texts of web-crawled data renders the contrastive objective data
inefficient and craving for a large training batch size. In this work, we
explore the validity of non-contrastive language-image pre-training (nCLIP),
and study whether nice properties exhibited in visual self-supervised models
can emerge. We empirically observe that the non-contrastive objective nourishes
representation learning while sufficiently underperforming under zero-shot
recognition. Based on the above study, we further introduce xCLIP, a
multi-tasking framework combining CLIP and nCLIP, and show that nCLIP aids CLIP
in enhancing feature semantics. The synergy between two objectives lets xCLIP
enjoy the best of both worlds: superior performance in both zero-shot transfer
and representation learning. Systematic evaluation is conducted spanning a wide
variety of downstream tasks including zero-shot classification, out-of-domain
classification, retrieval, visual representation learning, and textual
representation learning, showcasing a consistent performance gain and
validating the effectiveness of xCLIP.",None,-1
31cf8f20-0551-4787-9c1d-af396ae3aee4,Entity Type Prediction Leveraging Graph Walks and Entity Descriptions,0.398018,"The entity type information in Knowledge Graphs (KGs) such as DBpedia,
Freebase, etc. is often incomplete due to automated generation or human
curation. Entity typing is the task of assigning or inferring the semantic type
of an entity in a KG. This paper presents \textit{GRAND}, a novel approach for
entity typing leveraging different graph walk strategies in RDF2vec together
with textual entity descriptions. RDF2vec first generates graph walks and then
uses a language model to obtain embeddings for each node in the graph. This
study shows that the walk generation strategy and the embedding model have a
significant effect on the performance of the entity typing task. The proposed
approach outperforms the baseline approaches on the benchmark datasets DBpedia
and FIGER for entity typing in KGs for both fine-grained and coarse-grained
classes. The results show that the combination of order-aware RDF2vec variants
together with the contextual embeddings of the textual entity descriptions
achieve the best results.",None,-1
a9d92ac7-8b61-4ace-be82-476e50b02ab4,Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos,0.998213,"Unsupervised object-centric learning aims to represent the modular,
compositional, and causal structure of a scene as a set of object
representations and thereby promises to resolve many critical limitations of
traditional single-vector representations such as poor systematic
generalization. Although there have been many remarkable advances in recent
years, one of the most critical problems in this direction has been that
previous methods work only with simple and synthetic scenes but not with
complex and naturalistic images or videos. In this paper, we propose STEVE, an
unsupervised model for object-centric learning in videos. Our proposed model
makes a significant advancement by demonstrating its effectiveness on various
complex and naturalistic videos unprecedented in this line of research.
Interestingly, this is achieved by neither adding complexity to the model
architecture nor introducing a new objective or weak supervision. Rather, it is
achieved by a surprisingly simple architecture that uses a transformer-based
image decoder conditioned on slots and the learning objective is simply to
reconstruct the observation. Our experiment results on various complex and
naturalistic videos show significant improvements compared to the previous
state-of-the-art.",https://sites.google.com/view/slot-transformer-for-videos,-1
bae0eeec-9b7c-44b1-965d-327b7f889bad,A Transparency Index Framework for AI in Education,0.469098,"Numerous AI ethics checklists and frameworks have been proposed focusing on
different dimensions of ethical AI such as fairness, explainability, and
safety. Yet, no such work has been done on developing transparent AI systems
for real-world educational scenarios. This paper presents a Transparency Index
framework that has been iteratively co-designed with different stakeholders of
AI in education, including educators, ed-tech experts, and AI practitioners. We
map the requirements of transparency for different categories of stakeholders
of AI in education and demonstrate that transparency considerations are
embedded in the entire AI development process from the data collection stage
until the AI system is deployed in the real world and iteratively improved. We
also demonstrate how transparency enables the implementation of other ethical
AI dimensions in Education like interpretability, accountability, and safety.
In conclusion, we discuss the directions for future research in this newly
emerging field. The main contribution of this study is that it highlights the
importance of transparency in developing AI-powered educational technologies
and proposes an index framework for its conceptualization for AI in education.",None,-1
d2f92683-258f-4c39-a9d5-cf53a359923d,Natural Language Inference with Self-Attention for Veracity Assessment of Pandemic Claims,0.319835,"We present a comprehensive work on automated veracity assessment from dataset
creation to developing novel methods based on Natural Language Inference (NLI),
focusing on misinformation related to the COVID-19 pandemic. We first describe
the construction of the novel PANACEA dataset consisting of heterogeneous
claims on COVID-19 and their respective information sources. The dataset
construction includes work on retrieval techniques and similarity measurements
to ensure a unique set of claims. We then propose novel techniques for
automated veracity assessment based on Natural Language Inference including
graph convolutional networks and attention based approaches. We have carried
out experiments on evidence retrieval and veracity assessment on the dataset
using the proposed techniques and found them competitive with SOTA methods, and
provided a detailed discussion.",None,-1
8f09970d-1bc0-4e72-8a85-616b0947e97e,YOLO-FaceV2: A Scale and Occlusion Aware Face Detector,0.711309,"In recent years, face detection algorithms based on deep learning have made
great progress. These algorithms can be generally divided into two categories,
i.e. two-stage detector like Faster R-CNN and one-stage detector like YOLO.
Because of the better balance between accuracy and speed, one-stage detectors
have been widely used in many applications. In this paper, we propose a
real-time face detector based on the one-stage detector YOLOv5, named
YOLO-FaceV2. We design a Receptive Field Enhancement module called RFE to
enhance receptive field of small face, and use NWD Loss to make up for the
sensitivity of IoU to the location deviation of tiny objects. For face
occlusion, we present an attention module named SEAM and introduce Repulsion
Loss to solve it. Moreover, we use a weight function Slide to solve the
imbalance between easy and hard samples and use the information of the
effective receptive field to design the anchor. The experimental results on
WiderFace dataset show that our face detector outperforms YOLO and its variants
can be find in all easy, medium and hard subsets. Source code in
https://github.com/Krasjet-Yu/YOLO-FaceV2",https://github.com/Krasjet-Yu/YOLO-FaceV2,-1
72188aa2-5952-4583-9ce5-f596464eb638,RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states,0.583682,"Multi-agent deep reinforcement learning makes optimal decisions dependent on
system states observed by agents, but any uncertainty on the observations may
mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement
learning (MFAC) is well-known in the multi-agent field since it can effectively
handle a scalability problem. However, it is sensitive to state perturbations
that can significantly degrade the team rewards. This work proposes a Robust
Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two
innovations: 1) a new objective function of training actors, composed of a
\emph{policy gradient function} that is related to the expected cumulative
discount reward on sampled clean states and an \emph{action loss function} that
represents the difference between actions taken on clean and adversarial
states; and 2) a repetitive regularization of the action loss, ensuring the
trained actors to obtain excellent performance. Furthermore, this work proposes
a game model named a State-Adversarial Stochastic Game (SASG). Despite the Nash
equilibrium of SASG may not exist, adversarial perturbations to states in the
RoMFAC are proven to be defensible based on SASG. Experimental results show
that RoMFAC is robust against adversarial perturbations while maintaining its
competitive performance in environments without perturbations.",None,-1
022a23de-afeb-46b8-b669-bada8c805031,BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis,0.969153,"Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to align aspects and corresponding sentiments for
aspect-specific sentiment polarity inference. It is challenging because a
sentence may contain multiple aspects or complicated (e.g., conditional,
coordinating, or adversative) relations. Recently, exploiting dependency syntax
information with graph neural networks has been the most popular trend. Despite
its success, methods that heavily rely on the dependency tree pose challenges
in accurately modeling the alignment of the aspects and their words indicative
of sentiment, since the dependency tree may provide noisy signals of unrelated
associations (e.g., the ""conj"" relation between ""great"" and ""dreadful"" in
Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax
aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully
exploits the syntax information (e.g., phrase segmentation and hierarchical
structure) of the constituent tree of a sentence to model the sentiment-aware
context of every single aspect (called intra-context) and the sentiment
relations across aspects (called inter-context) for learning. Experiments on
four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the
state-of-the-art methods consistently.",https://github.com/yzhangcs/parser,147980
ee317e08-ef11-4090-bfa1-ccacfbc33fe6,Improving Policy Learning via Language Dynamics Distillation,0.397633,"Recent work has shown that augmenting environments with language descriptions
improves policy learning. However, for environments with complex language
abstractions, learning how to ground language to observations is difficult due
to sparse, delayed rewards. We propose Language Dynamics Distillation (LDD),
which pretrains a model to predict environment dynamics given demonstrations
with language descriptions, and then fine-tunes these language-aware pretrained
representations via reinforcement learning (RL). In this way, the model is
trained to both maximize expected reward and retain knowledge about how
language relates to environment dynamics. On SILG, a benchmark of five tasks
with language descriptions that evaluate distinct generalization challenges on
unseen environments (NetHack, ALFWorld, RTFM, Messenger, and Touchdown), LDD
outperforms tabula-rasa RL, VAE pretraining, and methods that learn from
unlabeled demonstrations in inverse RL and reward shaping with pretrained
experts. In our analyses, we show that language descriptions in demonstrations
improve sample-efficiency and generalization across environments, and that
dynamics modelling with expert demonstrations is more effective than with
non-experts.",https://github.com/vzhong/language-dynamics-distillation,-1
f3a819c0-4196-4b69-84e5-e6106be5cd98,A Proposal for Foley Sound Synthesis Challenge,0.838957,"""Foley"" refers to sound effects that are added to multimedia during
post-production to enhance its perceived acoustic properties, e.g., by
simulating the sounds of footsteps, ambient environmental sounds, or visible
objects on the screen. While foley is traditionally produced by foley artists,
there is increasing interest in automatic or machine-assisted techniques
building upon recent advances in sound synthesis and generative models. To
foster more participation in this growing research area, we propose a challenge
for automatic foley synthesis. Through case studies on successful previous
challenges in audio and machine learning, we set the goals of the proposed
challenge: rigorous, unified, and efficient evaluation of different foley
synthesis systems, with an overarching goal of drawing active participation
from the research community. We outline the details and design considerations
of a foley sound synthesis challenge, including task definition, dataset
requirements, and evaluation criteria.",None,-1
49c45e7b-2335-40a4-be1b-643268e9801c,Stop Filtering: Multi-View Attribute-Enhanced Dialogue Learning,0.183505,"There is a growing interest in improving the conversational ability of models
by filtering the raw dialogue corpora. Previous filtering strategies usually
rely on a scoring method to assess and discard samples from one perspective,
enabling the model to enhance the corresponding dialogue attributes (e.g.,
consistency) more easily. However, the discarded samples may obtain high scores
in other perspectives and can provide regularization effects on the model
learning, which causes the performance improvement to be sensitive to the
filtering ratio. In this work, we propose a multi-view attribute-enhanced
dialogue learning framework that strengthens the attribute-related features
more robustly and comprehensively. Instead of filtering the raw dataset to
train the model, our framework first pre-trains the model on the raw dataset
and then fine-tunes it through adapters on the selected sub-sets, which also
enhances certain attributes of responses but without suffering from the
problems mentioned above. Considering the variety of the dialogue attribute, we
further design a multi-view enhancement mechanism, including multi-view
selection and inter-view fusion. It groups the high-quality samples from
multiple perspectives, respectively, and enhances different attributes of
responses with the corresponding sample sets and adapters, keeping knowledge
independent and allowing flexible integration. Empirical results and analysis
show that our framework can improve the performance significantly in terms of
enhancing dialogue attributes and fusing view-specific knowledge.",None,-1
c3f8643d-8746-4eac-8a75-8559bc6893bd,HiSMatch: Historical Structure Matching based Temporal Knowledge Graph Reasoning,0.825023,"A Temporal Knowledge Graph (TKG) is a sequence of KGs with respective
timestamps, which adopts quadruples in the form of (\emph{subject},
\emph{relation}, \emph{object}, \emph{timestamp}) to describe dynamic facts.
TKG reasoning has facilitated many real-world applications via answering such
queries as (\emph{query entity}, \emph{query relation}, \emph{?}, \emph{future
timestamp}) about future. This is actually a matching task between a query and
candidate entities based on their historical structures, which reflect
behavioral trends of the entities at different timestamps. In addition, recent
KGs provide background knowledge of all the entities, which is also helpful for
the matching. Thus, in this paper, we propose the \textbf{Hi}storical
\textbf{S}tructure \textbf{Match}ing (\textbf{HiSMatch}) model. It applies two
structure encoders to capture the semantic information contained in the
historical structures of the query and candidate entities. Besides, it adopts
another encoder to integrate the background knowledge into the model. TKG
reasoning experiments on six benchmark datasets demonstrate the significant
improvement of the proposed HiSMatch model, with up to 5.6\% performance
improvement in MRR, compared to the state-of-the-art baselines.",None,-1
e148c606-66a7-44ef-8e89-37978e3da01b,Dynamic Local Aggregation Network with Adaptive Clusterer for Anomaly Detection,0.612392,"Existing methods for anomaly detection based on memory-augmented autoencoder
(AE) have the following drawbacks: (1) Establishing a memory bank requires
additional memory space. (2) The fixed number of prototypes from subjective
assumptions ignores the data feature differences and diversity. To overcome
these drawbacks, we introduce DLAN-AC, a Dynamic Local Aggregation Network with
Adaptive Clusterer, for anomaly detection. First, The proposed DLAN can
automatically learn and aggregate high-level features from the AE to obtain
more representative prototypes, while freeing up extra memory space. Second,
The proposed AC can adaptively cluster video data to derive initial prototypes
with prior information. In addition, we also propose a dynamic redundant
clustering strategy (DRCS) to enable DLAN for automatically eliminating feature
clusters that do not contribute to the construction of prototypes. Extensive
experiments on benchmarks demonstrate that DLAN-AC outperforms most existing
methods, validating the effectiveness of our method. Our code is publicly
available at https://github.com/Beyond-Zw/DLAN-AC.",https://github.com/Beyond-Zw/DLAN-AC,-1
ee2fc9c6-e28c-49af-86ae-2027d6ddea43,A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation,0.760263,"We propose a simple, yet powerful approach for unsupervised object
segmentation in videos. We introduce an objective function whose minimum
represents the mask of the main salient object over the input sequence. It only
relies on independent image features and optical flows, which can be obtained
using off-the-shelf self-supervised methods. It scales with the length of the
sequence with no need for superpixels or sparsification, and it generalizes to
different datasets without any specific training. This objective function can
actually be derived from a form of spectral clustering applied to the entire
video. Our method achieves on-par performance with the state of the art on
standard benchmarks (DAVIS2016, SegTrack-v2, FBMS59), while being conceptually
and practically much simpler. Code is available at
https://ponimatkin.github.io/ssl-vos.",https://ponimatkin.github.io/ssl-vos,-1
a976f2b4-c767-4d72-9381-0df205df655b,Visualizing Automatic Speech Recognition -- Means for a Better Understanding?,0.225079,"Automatic speech recognition (ASR) is improving ever more at mimicking human
speech processing. The functioning of ASR, however, remains to a large extent
obfuscated by the complex structure of the deep neural networks (DNNs) they are
based on. In this paper, we show how so-called attribution methods, that we
import from image recognition and suitably adapt to handle audio data, can help
to clarify the working of ASR. Taking DeepSpeech, an end-to-end model for ASR,
as a case study, we show how these techniques help to visualize which features
of the input are the most influential in determining the output. We focus on
three visualization techniques: Layer-wise Relevance Propagation (LRP),
Saliency Maps, and Shapley Additive Explanations (SHAP). We compare these
methods and discuss potential further applications, such as in the detection of
adversarial examples.",https://github.com/marcoancona/DeepExplain,-1
1759771a-821d-407a-83ef-d2ae8a0432ca,MorisienMT: A Dataset for Mauritian Creole Machine Translation,0.811124,"In this paper, we describe MorisienMT, a dataset for benchmarking machine
translation quality of Mauritian Creole. Mauritian Creole (Morisien) is the
lingua franca of the Republic of Mauritius and is a French-based creole
language. MorisienMT consists of a parallel corpus between English and
Morisien, French and Morisien and a monolingual corpus for Morisien. We first
give an overview of Morisien and then describe the steps taken to create the
corpora and, from it, the training and evaluation splits. Thereafter, we
establish a variety of baseline models using the created parallel corpora as
well as large French--English corpora for transfer learning. We release our
datasets publicly for research purposes and hope that this spurs research for
Morisien machine translation.",https://huggingface.co/datasets/prajdabre/MorisienMT,1758
35512f5d-e004-4448-a2ef-9d6682fc6a80,Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization,0.875828,"This paper presents Z-Code++, a new pre-trained language model optimized for
abstractive text summarization. The model extends the state of the art
encoder-decoder model using three techniques. First, we use a two-phase
pre-training process to improve model's performance on low-resource
summarization tasks. The model is first pre-trained using text corpora for
language understanding, and then is continually pre-trained on summarization
corpora for grounded text generation. Second, we replace self-attention layers
in the encoder with disentangled attention layers, where each word is
represented using two vectors that encode its content and position,
respectively. Third, we use fusion-in-encoder, a simple yet effective method of
encoding long sequences in a hierarchical manner. Z-Code++ creates new state of
the art on 9 out of 13 text summarization tasks across 5 languages. Our model
is parameter-efficient in that it outperforms the 600x larger PaLM-540B on
XSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and
few-shot settings, our model substantially outperforms the competing models.",https://github.com/microsoft/DeBERTa,-1
56d69a82-8149-4844-9daf-d8b39030133d,Adaptive Discrete Communication Bottlenecks with Dynamic Vector Quantization,0.539109,"Vector Quantization (VQ) is a method for discretizing latent representations
and has become a major part of the deep learning toolkit. It has been
theoretically and empirically shown that discretization of representations
leads to improved generalization, including in reinforcement learning where
discretization can be used to bottleneck multi-agent communication to promote
agent specialization and robustness. The discretization tightness of most
VQ-based methods is defined by the number of discrete codes in the
representation vector and the codebook size, which are fixed as
hyperparameters. In this work, we propose learning to dynamically select
discretization tightness conditioned on inputs, based on the hypothesis that
data naturally contains variations in complexity that call for different levels
of representational coarseness. We show that dynamically varying tightness in
communication bottlenecks can improve model performance on visual reasoning and
reinforcement learning tasks.",None,-1
26d57db3-67d8-421e-9180-9fa2f75181ae,From Multi-agent to Multi-robot: A Scalable Training and Evaluation Platform for Multi-robot Reinforcement Learning,0.362493,"Multi-agent reinforcement learning (MARL) has been gaining extensive
attention from academia and industries in the past few decades. One of the
fundamental problems in MARL is how to evaluate different approaches
comprehensively. Most existing MARL methods are evaluated in either video games
or simplistic simulated scenarios. It remains unknown how these methods perform
in real-world scenarios, especially multi-robot systems. This paper introduces
a scalable emulation platform for multi-robot reinforcement learning (MRRL)
called SMART to meet this need. Precisely, SMART consists of two components: 1)
a simulation environment that provides a variety of complex interaction
scenarios for training and 2) a real-world multi-robot system for realistic
performance evaluation. Besides, SMART offers agent-environment APIs that are
plug-and-play for algorithm implementation. To illustrate the practicality of
our platform, we conduct a case study on the cooperative driving lane change
scenario. Building off the case study, we summarize several unique challenges
of MRRL, which are rarely considered previously. Finally, we open-source the
simulation environments, associated benchmark tasks, and state-of-the-art
baselines to encourage and empower MRRL research.",None,27290
85c37304-4c83-4264-a797-a136c478d046,Video Prediction by Efficient Transformers,0.830643,"Video prediction is a challenging computer vision task that has a wide range
of applications. In this work, we present a new family of Transformer-based
models for video prediction. Firstly, an efficient local spatial-temporal
separation attention mechanism is proposed to reduce the complexity of standard
Transformers. Then, a full autoregressive model, a partial autoregressive model
and a non-autoregressive model are developed based on the new efficient
Transformer. The partial autoregressive model has a similar performance with
the full autoregressive model but a faster inference speed. The
non-autoregressive model not only achieves a faster inference speed but also
mitigates the quality degradation problem of the autoregressive counterparts,
but it requires additional parameters and loss function for learning. Given the
same attention mechanism, we conducted a comprehensive study to compare the
proposed three video prediction variants. Experiments show that the proposed
video prediction models are competitive with more complex state-of-the-art
convolutional-LSTM based models. The source code is available at
https://github.com/XiYe20/VPTR.",https://github.com/XiYe20/VPTR,-1
30202ac7-f79f-499f-beeb-4b210bce45a2,Logic-based Reward Shaping for Multi-Agent Reinforcement Learning,0.142259,"Reinforcement learning (RL) relies heavily on exploration to learn from its
environment and maximize observed rewards. Therefore, it is essential to design
a reward function that guarantees optimal learning from the received
experience. Previous work has combined automata and logic based reward shaping
with environment assumptions to provide an automatic mechanism to synthesize
the reward function based on the task. However, there is limited work on how to
expand logic-based reward shaping to Multi-Agent Reinforcement Learning (MARL).
The environment will need to consider the joint state in order to keep track of
other agents if the task requires cooperation, thus suffering from the curse of
dimensionality with respect to the number of agents. This project explores how
logic-based reward shaping for MARL can be designed for different scenarios and
tasks. We present a novel method for semi-centralized logic-based MARL reward
shaping that is scalable in the number of agents and evaluate it in multiple
scenarios.",https://github.com/IngyN/macsrl,2832
fa644fe9-e5f4-48b7-aab4-e5bdf933051b,Extracting Medication Changes in Clinical Narratives using Pre-trained Language Models,0.159174,"An accurate and detailed account of patient medications, including medication
changes within the patient timeline, is essential for healthcare providers to
provide appropriate patient care. Healthcare providers or the patients
themselves may initiate changes to patient medication. Medication changes take
many forms, including prescribed medication and associated dosage modification.
These changes provide information about the overall health of the patient and
the rationale that led to the current care. Future care can then build on the
resulting state of the patient. This work explores the automatic extraction of
medication change information from free-text clinical notes. The Contextual
Medication Event Dataset (CMED) is a corpus of clinical notes with annotations
that characterize medication changes through multiple change-related
attributes, including the type of change (start, stop, increase, etc.),
initiator of the change, temporality, change likelihood, and negation. Using
CMED, we identify medication mentions in clinical text and propose three novel
high-performing BERT-based systems that resolve the annotated medication change
characteristics. We demonstrate that our proposed systems improve medication
change classification performance over the initial work exploring CMED.",None,-1
3798ff4b-6c9b-483f-8bc9-d7fd7645a381,Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study),0.728461,"Test Input Prioritizers (TIP) for Deep Neural Networks (DNN) are an important
technique to handle the typically very large test datasets efficiently, saving
computation and labeling costs. This is particularly true for large-scale,
deployed systems, where inputs observed in production are recorded to serve as
potential test or training data for the next versions of the system. Feng et.
al. propose DeepGini, a very fast and simple TIP, and show that it outperforms
more elaborate techniques such as neuron- and surprise coverage. In a
large-scale study (4 case studies, 8 test datasets, 32'200 trained models) we
verify their findings. However, we also find that other comparable or even
simpler baselines from the field of uncertainty quantification, such as the
predicted softmax likelihood or the entropy of the predicted softmax
likelihoods perform equally well as DeepGini.",https://github.com/testingautomated-usi/simple-tip,-1
6c7b6f8e-d84d-487e-af23-dd616f17f5a9,Private Quantiles Estimation in the Presence of Atoms,0.597973,"We consider the differentially private estimation of multiple quantiles (MQ)
of a distribution from a dataset, a key building block in modern data analysis.
We apply the recent non-smoothed Inverse Sensitivity (IS) mechanism to this
specific problem. We establish that the resulting method is closely related to
the recently published ad hoc algorithm JointExp. In particular, they share the
same computational complexity and a similar efficiency. We prove the
statistical consistency of these two algorithms for continuous distributions.
Furthermore, we demonstrate both theoretically and empirically that this method
suffers from an important lack of performance in the case of peaked
distributions, which can degrade up to a potentially catastrophic impact in the
presence of atoms. Its smoothed version (i.e. by applying a max kernel to its
output density) would solve this problem, but remains an open challenge to
implement. As a proxy, we propose a simple and numerically efficient method
called Heuristically Smoothed JointExp (HSJointExp), which is endowed with
performance guarantees for a broad class of distributions and achieves results
that are orders of magnitude better on problematic datasets.",https://github.com/opendp/smartnoise-core,-1
3b4a7c63-ea68-4c1e-b23f-6088fee1824e,Low-resource Neural Machine Translation with Cross-modal Alignment,0.279801,"How to achieve neural machine translation with limited parallel data?
Existing techniques often rely on large-scale monolingual corpora, which is
impractical for some low-resource languages. In this paper, we turn to connect
several low-resource languages to a particular high-resource one by additional
visual modality. Specifically, we propose a cross-modal contrastive learning
method to learn a shared space for all languages, where both a coarse-grained
sentence-level objective and a fine-grained token-level one are introduced.
Experimental results and further analysis show that our method can effectively
learn the cross-modal and cross-lingual alignment with a small amount of
image-text pairs and achieves significant improvements over the text-only
baseline under both zero-shot and few-shot scenarios.",https://github.com/ictnlp/LNMT-CA,6142
b0800d32-58cc-4beb-ab38-7417fb332ae5,Machine Learning-Based User Scheduling in Integrated Satellite-HAPS-Ground Networks,0.339288,"Integrated space-air-ground networks promise to offer a valuable solution
space for empowering the sixth generation of communication networks (6G),
particularly in the context of connecting the unconnected and ultraconnecting
the connected. Such digital inclusion thrive makes resource management
problems, especially those accounting for load-balancing considerations, of
particular interest. The conventional model-based optimization methods,
however, often fail to meet the real-time processing and quality-of-service
needs, due to the high heterogeneity of the space-air-ground networks, and the
typical complexity of the classical algorithms. Given the premises of
artificial intelligence at automating wireless networks design and the
large-scale heterogeneity of non-terrestrial networks, this paper focuses on
showcasing the prospects of machine learning in the context of user scheduling
in integrated space-air-ground communications. The paper first overviews the
most relevant state-of-the art in the context of machine learning applications
to the resource allocation problems, with a dedicated attention to
space-air-ground networks. The paper then proposes, and shows the benefit of,
one specific use case that uses ensembling deep neural networks for optimizing
the user scheduling policies in integrated space-high altitude platform station
(HAPS)-ground networks. Finally, the paper sheds light on the challenges and
open issues that promise to spur the integration of machine learning in
space-air-ground networks, namely, online HAPS power adaptation, learning-based
channel sensing, data-driven multi-HAPSs resource management, and intelligent
flying taxis-empowered systems.",None,-1
f50d1a22-c91f-4df9-9e19-437d56cc46d7,Structural Bias for Aspect Sentiment Triplet Extraction,0.8039,"Structural bias has recently been exploited for aspect sentiment triplet
extraction (ASTE) and led to improved performance. On the other hand, it is
recognized that explicitly incorporating structural bias would have a negative
impact on efficiency, whereas pretrained language models (PLMs) can already
capture implicit structures. Thus, a natural question arises: Is structural
bias still a necessity in the context of PLMs? To answer the question, we
propose to address the efficiency issues by using an adapter to integrate
structural bias in the PLM and using a cheap-to-compute relative position
structure in place of the syntactic dependency structure. Benchmarking
evaluation is conducted on the SemEval datasets. The results show that our
proposed structural adapter is beneficial to PLMs and achieves state-of-the-art
performance over a range of strong baselines, yet with a light parameter demand
and low latency. Meanwhile, we give rise to the concern that the current
evaluation default with data of small scale is under-confident. Consequently,
we release a large-scale dataset for ASTE. The results on the new dataset hint
that the structural adapter is confidently effective and efficient to a large
scale. Overall, we draw the conclusion that structural bias shall still be a
necessity even with PLMs.",https://github.com/GeneZC/StructBias,-1
9d235447-5757-4323-b5df-3ab213258cfc,UAV-aided Wireless Node Localization Using Hybrid Radio Channel Models,0.328558,"This paper considers the problem of ground user localization based on
received signal strength (RSS) measurements obtained by an unmanned aerial
vehicle (UAV). We treat UAV-user link channel model parameters and antenna
radiation pattern of the UAV as unknowns that need to be estimated. A hybrid
channel model is proposed that consists of a traditional path loss model
combined with a neural network approximating the UAV antenna gain function.
With this model and a set of offline RSS measurements, the unknown parameters
are estimated. We then employ the particle swarm optimization (PSO) technique
which utilizes the learned hybrid channel model along with a 3D map of the
environment to accurately localize the ground users. The performance of the
developed algorithm is evaluated through simulations and also real-world
experiments.",None,-1
a45b2ff4-cb1d-41a5-8ca4-cac8d79da30d,"Unsupervised Learning of Depth, Camera Pose and Optical Flow from Monocular Video",0.0406232,"We propose DFPNet -- an unsupervised, joint learning system for monocular
Depth, Optical Flow and egomotion (Camera Pose) estimation from monocular image
sequences. Due to the nature of 3D scene geometry these three components are
coupled. We leverage this fact to jointly train all the three components in an
end-to-end manner. A single composite loss function -- which involves image
reconstruction-based loss for depth & optical flow, bidirectional consistency
checks and smoothness loss components -- is used to train the network. Using
hyperparameter tuning, we are able to reduce the model size to less than 5%
(8.4M parameters) of state-of-the-art DFP models. Evaluation on KITTI and
Cityscapes driving datasets reveals that our model achieves results comparable
to state-of-the-art in all of the three tasks, even with the significantly
smaller model size.",None,-1
f1cc7615-2d0b-4358-9dc4-c88c704b8659,LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension,0.614527,"The application of Natural Language Processing (NLP) to specialized domains,
such as the law, has recently received a surge of interest. As many legal
services rely on processing and analyzing large collections of documents,
automating such tasks with NLP tools emerges as a key challenge. Many popular
language models, such as BERT or RoBERTa, are general-purpose models, which
have limitations on processing specialized legal terminology and syntax. In
addition, legal documents may contain specialized vocabulary from other
domains, such as medical terminology in personal injury text. Here, we propose
LegalRelectra, a legal-domain language model that is trained on mixed-domain
legal and medical corpora. We show that our model improves over general-domain
and single-domain medical and legal language models when processing
mixed-domain (personal injury) text. Our training architecture implements the
Electra framework, but utilizes Reformer instead of BERT for its generator and
discriminator. We show that this improves the model's performance on processing
long passages and results in better long-range text comprehension.",None,-1
cd5f14eb-60ab-43d7-b32f-3da9f482d88f,"Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2",0.884732,"The field of image synthesis has made great strides in the last couple of
years. Recent models are capable of generating images with astonishing quality.
Fine-grained evaluation of these models on some interesting categories such as
faces is still missing. Here, we conduct a quantitative comparison of three
popular systems including Stable Diffusion, Midjourney, and DALL-E 2 in their
ability to generate photorealistic faces in the wild. We find that Stable
Diffusion generates better faces than the other systems, according to the FID
score. We also introduce a dataset of generated faces in the wild dubbed GFW,
including a total of 15,076 faces. Furthermore, we hope that our study spurs
follow-up research in assessing the generative models and improving them. Data
and code are available at data and code, respectively.",None,-1
dac811d1-d7f4-4838-9a29-02fec254390c,Constants of motion network,0.444285,"The beauty of physics is that there is usually a conserved quantity in an
always-changing system, known as the constant of motion. Finding the constant
of motion is important in understanding the dynamics of the system, but
typically requires mathematical proficiency and manual analytical work. In this
paper, we present a neural network that can simultaneously learn the dynamics
of the system and the constants of motion from data. By exploiting the
discovered constants of motion, it can produce better predictions on dynamics
and can work on a wider range of systems than Hamiltonian-based neural
networks. In addition, the training progresses of our method can be used as an
indication of the number of constants of motion in a system which could be
useful in studying a novel physical system.",https://github.com/machine-discovery/comet/,-1
0c6dfb0c-8de0-411b-9827-da11ff916ef3,LiDAR-MIMO: Efficient Uncertainty Estimation for LiDAR-based 3D Object Detection,0.161925,"The estimation of uncertainty in robotic vision, such as 3D object detection,
is an essential component in developing safe autonomous systems aware of their
own performance. However, the deployment of current uncertainty estimation
methods in 3D object detection remains challenging due to timing and
computational constraints. To tackle this issue, we propose LiDAR-MIMO, an
adaptation of the multi-input multi-output (MIMO) uncertainty estimation method
to the LiDAR-based 3D object detection task. Our method modifies the original
MIMO by performing multi-input at the feature level to ensure the detection,
uncertainty estimation, and runtime performance benefits are retained despite
the limited capacity of the underlying detector and the large computational
costs of point cloud processing. We compare LiDAR-MIMO with MC dropout and
ensembles as baselines and show comparable uncertainty estimation results with
only a small number of output heads. Further, LiDAR-MIMO can be configured to
be twice as fast as MC dropout and ensembles, while achieving higher mAP than
MC dropout and approaching that of ensembles.",https://github.com/open-mmlab/OpenPCDet,12580
479bb8c4-bc98-433f-908e-52a52fdd83b9,Procedural Image Programs for Representation Learning,0.750991,"Learning image representations using synthetic data allows training neural
networks without some of the concerns associated with real images, such as
privacy and bias. Existing work focuses on a handful of curated generative
processes which require expert knowledge to design, making it hard to scale up.
To overcome this, we propose training with a large dataset of twenty-one
thousand programs, each one generating a diverse set of synthetic images. These
programs are short code snippets, which are easy to modify and fast to execute
using OpenGL. The proposed dataset can be used for both supervised and
unsupervised representation learning, and reduces the gap between pre-training
with real and procedurally generated images by 38%.",https://github.com/mbaradad/shaders21k,-1
01fc01e3-16e9-4d07-bf96-b087ed5e01cc,Human Interpretation of Saliency-based Explanation Over Text,0.796287,"While a lot of research in explainable AI focuses on producing effective
explanations, less work is devoted to the question of how people understand and
interpret the explanation. In this work, we focus on this question through a
study of saliency-based explanations over textual data. Feature-attribution
explanations of text models aim to communicate which parts of the input text
were more influential than others towards the model decision. Many current
explanation methods, such as gradient-based or Shapley value-based methods,
provide measures of importance which are well-understood mathematically. But
how does a person receiving the explanation (the explainee) comprehend it? And
does their understanding match what the explanation attempted to communicate?
We empirically investigate the effect of various factors of the input, the
feature-attribution explanation, and visualization procedure, on laypeople's
interpretation of the explanation. We query crowdworkers for their
interpretation on tasks in English and German, and fit a GAMM model to their
responses considering the factors of interest. We find that people often
mis-interpret the explanations: superficial and unrelated factors, such as word
length, influence the explainees' importance assignment despite the explanation
communicating importance directly. We then show that some of this distortion
can be attenuated: we propose a method to adjust saliencies based on model
estimates of over- and under-perception, and explore bar charts as an
alternative to heatmap saliency visualization. We find that both approaches can
attenuate the distorting effect of specific factors, leading to
better-calibrated understanding of the explanation.",None,-1
89deb13a-4324-45f9-8c83-c6580c812ac2,arXivEdits: Understanding the Human Revision Process in Scientific Writing,0.434219,"Scientific publications are the primary means to communicate research
discoveries, where the writing quality is of crucial importance. However, prior
work studying the human editing process in this domain mainly focused on the
abstract or introduction sections, resulting in an incomplete picture. In this
work, we provide a complete computational framework for studying text revision
in scientific writing. We first introduce arXivEdits, a new annotated corpus of
751 full papers from arXiv with gold sentence alignment across their multiple
versions of revision, as well as fine-grained span-level edits and their
underlying intentions for 1,000 sentence pairs. It supports our data-driven
analysis to unveil the common strategies practiced by researchers for revising
their papers. To scale up the analysis, we also develop automatic methods to
extract revision at document-, sentence-, and word-levels. A neural CRF
sentence alignment model trained on our corpus achieves 93.8 F1, enabling the
reliable matching of sentences between different versions. We formulate the
edit extraction task as a span alignment problem, and our proposed method
extracts more fine-grained and explainable edits, compared to the commonly used
diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1
on the fine-grained intent classification task. Our data and system are
released at tiny.one/arxivedits.",https://tiny.one/arxivedits,-1
145bf36e-147c-47dc-b838-a1811baf7149,MolScribe: Robust Molecular Structure Recognition with Image-To-Graph Generation,0.669867,"Molecular structure recognition is the task of translating a molecular image
into its graph structure. Significant variation in drawing styles and
conventions exhibited in chemical literature poses a significant challenge for
automating this task. In this paper, we propose MolScribe, a novel
image-to-graph generation model that explicitly predicts atoms and bonds, along
with their geometric layouts, to construct the molecular structure. Our model
flexibly incorporates symbolic chemistry constraints to recognize chirality and
expand abbreviated structures. We further develop data augmentation strategies
to enhance the model robustness against domain shifts. In experiments on both
synthetic and realistic molecular images, MolScribe significantly outperforms
previous models, achieving 76-93% accuracy on public benchmarks. Chemists can
also easily verify MolScribe's prediction, informed by its confidence
estimation and atom-level alignment with the input image. MolScribe is publicly
available through Python and web interfaces:
https://github.com/thomas0809/MolScribe.",https://github.com/thomas0809/MolScribe,-1
0e07b8bb-57e5-4deb-9072-6cf613dc2caa,Danish Airs and Grounds: A Dataset for Aerial-to-Street-Level Place Recognition and Localization,0.736403,"Place recognition and visual localization are particularly challenging in
wide baseline configurations. In this paper, we contribute with the
\emph{Danish Airs and Grounds} (DAG) dataset, a large collection of
street-level and aerial images targeting such cases. Its main challenge lies in
the extreme viewing-angle difference between query and reference images with
consequent changes in illumination and perspective. The dataset is larger and
more diverse than current publicly available data, including more than 50 km of
road in urban, suburban and rural areas. All images are associated with
accurate 6-DoF metadata that allows the benchmarking of visual localization
methods.
  We also propose a map-to-image re-localization pipeline, that first estimates
a dense 3D reconstruction from the aerial images and then matches query
street-level images to street-level renderings of the 3D model. The dataset can
be downloaded at: https://frederikwarburg.github.io/DAG",None,-1
04cd98da-df11-44f4-91d5-8634648421b7,mSLAM: Massively multilingual joint pre-training for speech and text,0.998731,"We present mSLAM, a multilingual Speech and LAnguage Model that learns
cross-lingual cross-modal representations of speech and text by pre-training
jointly on large amounts of unlabeled speech and text in multiple languages.
mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on
character-level text, along with Connectionist Temporal Classification (CTC)
losses on paired speech and transcript data, to learn a single model capable of
learning from and representing both speech and text signals in a shared
representation space. We evaluate mSLAM on several downstream speech
understanding tasks and find that joint pre-training with text improves quality
on speech translation, speech intent classification and speech language-ID
while being competitive on multilingual ASR, when compared against speech-only
pre-training. Our speech translation model demonstrates zero-shot text
translation without seeing any text translation data, providing evidence for
cross-modal alignment of representations. mSLAM also benefits from multi-modal
fine-tuning, further improving the quality of speech translation by directly
leveraging text translation data during the fine-tuning process. Our empirical
analysis highlights several opportunities and challenges arising from
large-scale multimodal pre-training, suggesting directions for future research.",None,18269
41f9d4a2-9ae7-481c-b1bf-33a10a66e34f,Human-Centered Concept Explanations for Neural Networks,0.508133,"Understanding complex machine learning models such as deep neural networks
with explanations is crucial in various applications. Many explanations stem
from the model perspective, and may not necessarily effectively communicate why
the model is making its predictions at the right level of abstraction. For
example, providing importance weights to individual pixels in an image can only
express which parts of that particular image are important to the model, but
humans may prefer an explanation which explains the prediction by concept-based
thinking. In this work, we review the emerging area of concept based
explanations. We start by introducing concept explanations including the class
of Concept Activation Vectors (CAV) which characterize concepts using vectors
in appropriate spaces of neural activations, and discuss different properties
of useful concepts, and approaches to measure the usefulness of concept
vectors. We then discuss approaches to automatically extract concepts, and
approaches to address some of their caveats. Finally, we discuss some case
studies that showcase the utility of such concept-based explanations in
synthetic settings and real world applications.",None,-1
2c363bc1-5a9d-4d41-a9f8-a77a89978b2b,Enhanced Training of Query-Based Object Detection via Selective Query Recollection,0.555931,"This paper investigates a phenomenon where query-based object detectors
mispredict at the last decoding stage while predicting correctly at an
intermediate stage. We review the training process and attribute the overlooked
phenomenon to two limitations: lack of training emphasis and cascading errors
from decoding sequence. We design and present Selective Query Recollection
(SQR), a simple and effective training strategy for query-based object
detectors. It cumulatively collects intermediate queries as decoding stages go
deeper and selectively forwards the queries to the downstream stages aside from
the sequential structure. Such-wise, SQR places training emphasis on later
stages and allows later stages to work with intermediate queries from earlier
stages directly. SQR can be easily plugged into various query-based object
detectors and significantly enhances their performance while leaving the
inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,
and Deformable-DETR across various settings (backbone, number of queries,
schedule) and consistently brings 1.4-2.8 AP improvement.",https://github.com/Fangyi-Chen/SQR,14250
2185e60b-32a0-4913-870b-db376155504c,Join-Chain Network: A Logical Reasoning View of the Multi-head Attention in Transformer,0.0270334,"Developing neural architectures that are capable of logical reasoning has
become increasingly important for a wide range of applications (e.g., natural
language processing). Towards this grand objective, we propose a symbolic
reasoning architecture that chains many join operators together to model output
logical expressions. In particular, we demonstrate that such an ensemble of
join-chains can express a broad subset of ''tree-structured'' first-order
logical expressions, named FOET, which is particularly useful for modeling
natural languages. To endow it with differentiable learning capability, we
closely examine various neural operators for approximating the symbolic
join-chains. Interestingly, we find that the widely used multi-head
self-attention module in transformer can be understood as a special neural
operator that implements the union bound of the join operator in probabilistic
predicate space. Our analysis not only provides a new perspective on the
mechanism of the pretrained models such as BERT for natural language
understanding but also suggests several important future improvement
directions.",None,620
7bfb33dd-5e09-4ab2-a53e-395275348e7c,Fast and Data Efficient Reinforcement Learning from Pixels via Non-Parametric Value Approximation,0.0426646,"We present Nonparametric Approximation of Inter-Trace returns (NAIT), a
Reinforcement Learning algorithm for discrete action, pixel-based environments
that is both highly sample and computation efficient. NAIT is a lazy-learning
approach with an update that is equivalent to episodic Monte-Carlo on episode
completion, but that allows the stable incorporation of rewards while an
episode is ongoing. We make use of a fixed domain-agnostic representation,
simple distance based exploration and a proximity graph-based lookup to
facilitate extremely fast execution. We empirically evaluate NAIT on both the
26 and 57 game variants of ATARI100k where, despite its simplicity, it achieves
competitive performance in the online setting with greater than 100x speedup in
wall-time.",https://github.com/nmslib/hnswlib,-1
5a51f404-3bc2-4eb4-8d32-35a2ced7559e,Learning logic programs by combining programs,0.62766,"The goal of inductive logic programming is to induce a logic program (a set
of logical rules) that generalises training examples. Inducing programs with
many rules and literals is a major challenge. To tackle this challenge, we
introduce an approach where we learn small non-separable programs and combine
them. We implement our approach in a constraint-driven ILP system. Our approach
can learn optimal and recursive programs and perform predicate invention. Our
experiments on multiple domains, including game playing and program synthesis,
show that our approach can drastically outperform existing approaches in terms
of predictive accuracies and learning times, sometimes reducing learning times
from over an hour to a few seconds.",https://github.com/logic-and-learning-lab/ecai23-combo,-1
de7f2809-91d2-473b-8a8a-0221266ac00e,Deformable Butterfly: A Highly Structured and Sparse Linear Transform,0.715186,"We introduce a new kind of linear transform named Deformable Butterfly
(DeBut) that generalizes the conventional butterfly matrices and can be adapted
to various input-output dimensions. It inherits the fine-to-coarse-grained
learnable hierarchy of traditional butterflies and when deployed to neural
networks, the prominent structures and sparsity in a DeBut layer constitutes a
new way for network compression. We apply DeBut as a drop-in replacement of
standard fully connected and convolutional layers, and demonstrate its
superiority in homogenizing a neural network and rendering it favorable
properties such as light weight and low inference complexity, without
compromising accuracy. The natural complexity-accuracy tradeoff arising from
the myriad deformations of a DeBut layer also opens up new rooms for analytical
and practical research. The codes and Appendix are publicly available at:
https://github.com/ruilin0212/DeBut.",https://github.com/ruilin0212/DeBut,-1
164683b4-638d-42ed-957a-160f582ebe9b,Learning First-Order Symbolic Planning Representations That Are Grounded,0.456069,"Two main approaches have been developed for learning first-order planning
(action) models from unstructured data: combinatorial approaches that yield
crisp action schemas from the structure of the state space, and deep learning
approaches that produce action schemas from states represented by images. A
benefit of the former approach is that the learned action schemas are similar
to those that can be written by hand; a benefit of the latter is that the
learned representations (predicates) are grounded on the images, and as a
result, new instances can be given in terms of images. In this work, we develop
a new formulation for learning crisp first-order planning models that are
grounded on parsed images, a step to combine the benefits of the two
approaches. Parsed images are assumed to be given in a simple O2D language
(objects in 2D) that involves a small number of unary and binary predicates
like ""left"", ""above"", ""shape"", etc. After learning, new planning instances can
be given in terms of pairs of parsed images, one for the initial situation and
the other for the goal. Learning and planning experiments are reported for
several domains including Blocks, Sokoban, IPC Grid, and Hanoi.",None,12924
210219ff-fe21-4d75-9c75-db0795fa686c,Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task,0.676742,"Although transformer-based Neural Language Models demonstrate impressive
performance on a variety of tasks, their generalization abilities are not well
understood. They have been shown to perform strongly on subject-verb number
agreement in a wide array of settings, suggesting that they learned to track
syntactic dependencies during their training even without explicit supervision.
In this paper, we examine the extent to which BERT is able to perform
lexically-independent subject-verb number agreement (NA) on targeted syntactic
templates. To do so, we disrupt the lexical patterns found in naturally
occurring stimuli for each targeted structure in a novel fine-grained analysis
of BERT's behavior. Our results on nonce sentences suggest that the model
generalizes well for simple templates, but fails to perform
lexically-independent syntactic generalization when as little as one attractor
is present.",https://github.com/karimlasri/does-bert-really-agree,8215
70f5806a-35af-427f-b03b-fe4168a4ff77,Egocentric Human-Object Interaction Detection Exploiting Synthetic Data,0.762107,"We consider the problem of detecting Egocentric HumanObject Interactions
(EHOIs) in industrial contexts. Since collecting and labeling large amounts of
real images is challenging, we propose a pipeline and a tool to generate
photo-realistic synthetic First Person Vision (FPV) images automatically
labeled for EHOI detection in a specific industrial scenario. To tackle the
problem of EHOI detection, we propose a method that detects the hands, the
objects in the scene, and determines which objects are currently involved in an
interaction. We compare the performance of our method with a set of
state-of-the-art baselines. Results show that using a synthetic dataset
improves the performance of an EHOI detection system, especially when few real
data are available. To encourage research on this topic, we publicly release
the proposed dataset at the following url:
https://iplab.dmi.unict.it/EHOI_SYNTH/.",https://github.com/cocodataset/cocoapi,-1
5d9fbf1c-c5db-4b8d-9925-3f45ca4be55d,Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents,0.402222,"We argue that disentangling content selection from the budget used to cover
salient content improves the performance and applicability of abstractive
summarizers. Our method, FactorSum, does this disentanglement by factorizing
summarization into two steps through an energy function: (1) generation of
abstractive summary views; (2) combination of these views into a final summary,
following a budget and content guidance. This guidance may come from different
sources, including from an advisor model such as BART or BigBird, or in oracle
mode -- from the reference. This factorization achieves significantly higher
ROUGE scores on multiple benchmarks for long document summarization, namely
PubMed, arXiv, and GovReport. Most notably, our model is effective for domain
adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1
score on arXiv, which indicates a strong performance due to more flexible
budget adaptation and content selection less dependent on domain-specific
textual structure.",https://github.com/thefonseca/factorsum,-1
a761e557-5d89-43fa-81b7-c8a7d71a2a9c,Full-Text Argumentation Mining on Scientific Publications,0.476315,"Scholarly Argumentation Mining (SAM) has recently gained attention due to its
potential to help scholars with the rapid growth of published scientific
literature. It comprises two subtasks: argumentative discourse unit recognition
(ADUR) and argumentative relation extraction (ARE), both of which are
challenging since they require e.g. the integration of domain knowledge, the
detection of implicit statements, and the disambiguation of argument structure.
While previous work focused on dataset construction and baseline methods for
specific document sections, such as abstract or results, full-text scholarly
argumentation mining has seen little progress. In this work, we introduce a
sequential pipeline model combining ADUR and ARE for full-text SAM, and provide
a first analysis of the performance of pretrained language models (PLMs) on
both subtasks. We establish a new SotA for ADUR on the Sci-Arg corpus,
outperforming the previous best reported result by a large margin (+7% F1). We
also present the first results for ARE, and thus for the full AM pipeline, on
this benchmark dataset. Our detailed error analysis reveals that non-contiguous
ADUs as well as the interpretation of discourse connectors pose major
challenges and that data annotation needs to be more consistent.",https://github.com/DFKI-NLP/sam,-1
8666778c-e195-4fc8-9fca-7bf75649a730,RBC: Rectifying the Biased Context in Continual Semantic Segmentation,0.323235,"Recent years have witnessed a great development of Convolutional Neural
Networks in semantic segmentation, where all classes of training images are
simultaneously available. In practice, new images are usually made available in
a consecutive manner, leading to a problem called Continual Semantic
Segmentation (CSS). Typically, CSS faces the forgetting problem since previous
training images are unavailable, and the semantic shift problem of the
background class. Considering the semantic segmentation as a context-dependent
pixel-level classification task, we explore CSS from a new perspective of
context analysis in this paper. We observe that the context of old-class pixels
in the new images is much more biased on new classes than that in the old
images, which can sharply aggravate the old-class forgetting and new-class
overfitting. To tackle the obstacle, we propose a biased-context-rectified CSS
framework with a context-rectified image-duplet learning scheme and a
biased-context-insensitive consistency loss. Furthermore, we propose an
adaptive re-weighting class-balanced learning strategy for the biased class
distribution. Our approach outperforms state-of-the-art methods by a large
margin in existing CSS scenarios.",https://github.com/arthurdouillard/CVPR2021,-1
15da53dc-43b0-4e4f-83d4-4ee941c69be9,SVNet: Where SO(3) Equivariance Meets Binarization on Point Cloud Representation,0.697056,"Efficiency and robustness are increasingly needed for applications on 3D
point clouds, with the ubiquitous use of edge devices in scenarios like
autonomous driving and robotics, which often demand real-time and reliable
responses. The paper tackles the challenge by designing a general framework to
construct 3D learning architectures with SO(3) equivariance and network
binarization. However, a naive combination of equivariant networks and
binarization either causes sub-optimal computational efficiency or geometric
ambiguity. We propose to locate both scalar and vector features in our networks
to avoid both cases. Precisely, the presence of scalar features makes the major
part of the network binarizable, while vector features serve to retain rich
structural information and ensure SO(3) equivariance. The proposed approach can
be applied to general backbones like PointNet and DGCNN. Meanwhile, experiments
on ModelNet40, ShapeNet, and the real-world dataset ScanObjectNN, demonstrated
that the method achieves a great trade-off between efficiency, rotation
robustness, and accuracy. The codes are available at
https://github.com/zhuoinoulu/svnet.",https://github.com/zhuoinoulu/svnet,-1
7dbcbc35-341f-470a-b5f8-2e1e37a9b55a,Online Continual Learning with Contrastive Vision Transformer,0.948235,"Online continual learning (online CL) studies the problem of learning
sequential tasks from an online data stream without task boundaries, aiming to
adapt to new data while alleviating catastrophic forgetting on the past tasks.
This paper proposes a framework Contrastive Vision Transformer (CVT), which
designs a focal contrastive learning strategy based on a transformer
architecture, to achieve a better stability-plasticity trade-off for online CL.
Specifically, we design a new external attention mechanism for online CL that
implicitly captures previous tasks' information. Besides, CVT contains
learnable focuses for each class, which could accumulate the knowledge of
previous classes to alleviate forgetting. Based on the learnable focuses, we
design a focal contrastive loss to rebalance contrastive learning between new
and past classes and consolidate previously learned representations. Moreover,
CVT contains a dual-classifier structure for decoupling learning current
classes and balancing all observed classes. The extensive experimental results
show that our approach achieves state-of-the-art performance with even fewer
parameters on online CL benchmarks and effectively alleviates the catastrophic
forgetting.",None,-1
56841793-693e-4ad5-a7f7-733ecb5d41fe,Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection,0.629676,"Pre-training large neural language models, such as BERT, has led to
impressive gains on many natural language processing (NLP) tasks. Although this
method has proven to be effective for many domains, it might not always provide
desirable benefits. In this paper, we study the effects of hateful pre-training
on low-resource hate speech classification tasks. While previous studies on the
English language have emphasized its importance, we aim to augment their
observations with some non-obvious insights. We evaluate different variations
of tweet-based BERT models pre-trained on hateful, non-hateful, and mixed
subsets of a 40M tweet dataset. This evaluation is carried out for the Indian
languages Hindi and Marathi. This paper is empirical evidence that hateful
pre-training is not the best pre-training option for hate speech detection. We
show that pre-training on non-hateful text from the target domain provides
similar or better results. Further, we introduce HindTweetBERT and
MahaTweetBERT, the first publicly available BERT models pre-trained on Hindi
and Marathi tweets, respectively. We show that they provide state-of-the-art
performance on hate speech classification tasks. We also release hateful BERT
for the two languages and a gold hate speech evaluation benchmark HateEval-Hi
and HateEval-Mr consisting of manually labeled 2000 tweets each. The models and
data are available at https://github.com/l3cube-pune/MarathiNLP .",https://github.com/l3cube-pune/MarathiNLP,-1
8de18786-4648-4b44-baee-cb32adc06e90,Abstraction for Deep Reinforcement Learning,0.524873,"We characterise the problem of abstraction in the context of deep
reinforcement learning. Various well established approaches to analogical
reasoning and associative memory might be brought to bear on this issue, but
they present difficulties because of the need for end-to-end differentiability.
We review developments in AI and machine learning that could facilitate their
adoption.",None,-1
d6f2667d-bc5b-43c3-b949-d138aab1d8bf,NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,0.849796,"Given the ubiquitous nature of numbers in text, reasoning with numbers to
perform simple calculations is an important skill of AI systems. While many
datasets and models have been developed to this end, state-of-the-art AI
systems are brittle; failing to perform the underlying mathematical reasoning
when they appear in a slightly different scenario. Drawing inspiration from
GLUE that was proposed in the context of natural language understanding, we
propose NumGLUE, a multi-task benchmark that evaluates the performance of AI
systems on eight different tasks, that at their core require simple arithmetic
understanding. We show that this benchmark is far from being solved with neural
models including state-of-the-art large-scale language models performing
significantly worse than humans (lower by 46.4%). Further, NumGLUE promotes
sharing knowledge across tasks, especially those with limited training data as
evidenced by the superior performance (average gain of 3.4% on each task) when
a model is jointly trained on all the tasks as opposed to task-specific
modeling. Finally, we hope that NumGLUE will encourage systems that perform
robust and general arithmetic reasoning within language, a first step towards
being able to perform more complex mathematical reasoning.",None,-1
c772a2ad-9ae0-4efd-82ee-c11f96194720,BabyBear: Cheap inference triage for expensive language models,0.114082,"Transformer language models provide superior accuracy over previous models
but they are computationally and environmentally expensive. Borrowing the
concept of model cascading from computer vision, we introduce BabyBear, a
framework for cascading models for natural language processing (NLP) tasks to
minimize cost. The core strategy is inference triage, exiting early when the
least expensive model in the cascade achieves a sufficiently high-confidence
prediction. We test BabyBear on several open source data sets related to
document classification and entity recognition. We find that for common NLP
tasks a high proportion of the inference load can be accomplished with cheap,
fast models that have learned by observing a deep learning model. This allows
us to reduce the compute cost of large-scale classification jobs by more than
50% while retaining overall accuracy. For named entity recognition, we save 33%
of the deep learning compute while maintaining an F1 score higher than 95% on
the CoNLL benchmark.",https://github.com/PrimerAI/primer-research/tree/main/babybear,-1
26cb51d0-ae19-4d4e-978d-ecb166cfb7a3,Self-distilled Knowledge Delegator for Exemplar-free Class Incremental Learning,0.0786251,"Exemplar-free incremental learning is extremely challenging due to
inaccessibility of data from old tasks. In this paper, we attempt to exploit
the knowledge encoded in a previously trained classification model to handle
the catastrophic forgetting problem in continual learning. Specifically, we
introduce a so-called knowledge delegator, which is capable of transferring
knowledge from the trained model to a randomly re-initialized new model by
generating informative samples. Given the previous model only, the delegator is
effectively learned using a self-distillation mechanism in a data-free manner.
The knowledge extracted by the delegator is then utilized to maintain the
performance of the model on old tasks in incremental learning. This simple
incremental learning framework surpasses existing exemplar-free methods by a
large margin on four widely used class incremental benchmarks, namely
CIFAR-100, ImageNet-Subset, Caltech-101 and Flowers-102. Notably, we achieve
comparable performance to some exemplar-based methods without accessing any
exemplars.",https://github.com/google/deepdream,8286
8b78a4f3-adfc-46c7-81c9-9734b75eb676,Generating Sequences by Learning to Self-Correct,0.998268,"Sequence generation applications require satisfying semantic constraints,
such as ensuring that programs are correct, using certain keywords, or avoiding
undesirable content. Language models, whether fine-tuned or prompted with
few-shot demonstrations, frequently violate these constraints, and lack a
mechanism to iteratively revise their outputs. Moreover, some powerful language
models are of extreme scale or inaccessible, making it inefficient, if not
infeasible, to update their parameters for task-specific adaptation. We present
Self-Correction, an approach that decouples an imperfect base generator (an
off-the-shelf language model or supervised sequence-to-sequence model) from a
separate corrector that learns to iteratively correct imperfect generations. To
train the corrector, we propose an online training procedure that can use
either scalar or natural language feedback on intermediate imperfect
generations. We show that Self-Correction improves upon the base generator in
three diverse generation tasks - mathematical program synthesis,
lexically-constrained generation, and toxicity control - even when the
corrector is much smaller than the base generator.",None,-1
39a788bf-7bfa-480e-acb5-be8e99103f54,An Overview of Structural Coverage Metrics for Testing Neural Networks,0.402777,"Deep neural network (DNN) models, including those used in safety-critical
domains, need to be thoroughly tested to ensure that they can reliably perform
well in different scenarios. In this article, we provide an overview of
structural coverage metrics for testing DNN models, including neuron coverage
(NC), k-multisection neuron coverage (kMNC), top-k neuron coverage (TKNC),
neuron boundary coverage (NBC), strong neuron activation coverage (SNAC) and
modified condition/decision coverage (MC/DC). We evaluate the metrics on
realistic DNN models used for perception tasks (including LeNet-1, LeNet-4,
LeNet-5, and ResNet20) as well as on networks used in autonomy (TaxiNet). We
also provide a tool, DNNCov, which can measure the testing coverage for all
these metrics. DNNCov outputs an informative coverage report to enable
researchers and practitioners to assess the adequacy of DNN testing, compare
different coverage measures, and to more conveniently inspect the model's
internals during testing.",https://github.com/DNNCov/DNNCov,-1
9bef7aa0-a9ae-4267-868c-5711f9635592,How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image,0.406163,"This study investigates how fake news uses a thumbnail for a news article
with a focus on whether a news article's thumbnail represents the news content
correctly. A news article shared with an irrelevant thumbnail can mislead
readers into having a wrong impression of the issue, especially in social media
environments where users are less likely to click the link and consume the
entire content. We propose to capture the degree of semantic incongruity in the
multimodal relation by using the pretrained CLIP representation. From a
source-level analysis, we found that fake news employs a more incongruous image
to the main content than general news. Going further, we attempted to detect
news articles with image-text incongruity. Evaluation experiments suggest that
CLIP-based methods can successfully detect news articles in which the thumbnail
is semantically irrelevant to news text. This study contributes to the research
by providing a novel view on tackling online fake news and misinformation. Code
and datasets are available at
https://github.com/ssu-humane/fake-news-thumbnail.",https://github.com/su-humane/fake-news-thumbnail,-1
57fd8608-eb53-408a-80dd-35851c654e2e,UnCommonSense: Informative Negative Knowledge about Everyday Concepts,0.732947,"Commonsense knowledge about everyday concepts is an important asset for AI
applications, such as question answering and chatbots. Recently, we have seen
an increasing interest in the construction of structured commonsense knowledge
bases (CSKBs). An important part of human commonsense is about properties that
do not apply to concepts, yet existing CSKBs only store positive statements.
Moreover, since CSKBs operate under the open-world assumption, absent
statements are considered to have unknown truth rather than being invalid. This
paper presents the UNCOMMONSENSE framework for materializing informative
negative commonsense statements. Given a target concept, comparable concepts
are identified in the CSKB, for which a local closed-world assumption is
postulated. This way, positive statements about comparable concepts that are
absent for the target concept become seeds for negative statement candidates.
The large set of candidates is then scrutinized, pruned and ranked by
informativeness. Intrinsic and extrinsic evaluations show that our method
significantly outperforms the state-of-the-art. A large dataset of informative
negations is released as a resource for future research.",https://github.com/tsafavi/NegatER,-1
8d1672cd-a5be-4933-81c1-b373f547dc69,Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient Classification,0.104263,"Language Models pretrained on large textual data have been shown to encode
different types of knowledge simultaneously. Traditionally, only the features
from the last layer are used when adapting to new tasks or data. We put forward
that, when using or finetuning deep pretrained models, intermediate layer
features that may be relevant to the downstream task are buried too deep to be
used efficiently in terms of needed samples or steps. To test this, we propose
a new layer fusion method: Depth-Wise Attention (DWAtt), to help re-surface
signals from non-final layers. We compare DWAtt to a basic concatenation-based
layer fusion method (Concat), and compare both to a deeper model baseline --
all kept within a similar parameter budget. Our findings show that DWAtt and
Concat are more step- and sample-efficient than the baseline, especially in the
few-shot setting. DWAtt outperforms Concat on larger data sizes. On CoNLL-03
NER, layer fusion shows 3.68--9.73% F1 gain at different few-shot sizes. The
layer fusion models presented significantly outperform the baseline in various
training scenarios with different data sizes, architectures, and training
constraints.",https://github.com/munael/dwatt-depth_wise_attention-lrec_coling_2024,-1
24a64b6c-e196-45a0-81b7-66990166e8a4,Revisiting Neural Scaling Laws in Language and Vision,0.722463,"The remarkable progress in deep learning in recent years is largely driven by
improvements in scale, where bigger models are trained on larger datasets for
longer schedules. To predict the benefit of scale empirically, we argue for a
more rigorous methodology based on the extrapolation loss, instead of reporting
the best-fitting (interpolating) parameters. We then present a recipe for
estimating scaling law parameters reliably from learning curves. We demonstrate
that it extrapolates more accurately than previous methods in a wide range of
architecture families across several domains, including image classification,
neural machine translation (NMT) and language modeling, in addition to tasks
from the BIG-Bench evaluation benchmark. Finally, we release a benchmark
dataset comprising of 90 evaluation tasks to facilitate research in this
domain.",https://github.com/google-research/revisiting_neural_scaling_laws,-1
e0882a55-576f-43ee-a013-2a10ba4eb49f,Self-Supervised Super-Resolution for Multi-Exposure Push-Frame Satellites,0.292937,"Modern Earth observation satellites capture multi-exposure bursts of
push-frame images that can be super-resolved via computational means. In this
work, we propose a super-resolution method for such multi-exposure sequences, a
problem that has received very little attention in the literature. The proposed
method can handle the signal-dependent noise in the inputs, process sequences
of any length, and be robust to inaccuracies in the exposure times.
Furthermore, it can be trained end-to-end with self-supervision, without
requiring ground truth high resolution frames, which makes it especially suited
to handle real data. Central to our method are three key contributions: i) a
base-detail decomposition for handling errors in the exposure times, ii) a
noise-level-aware feature encoding for improved fusion of frames with varying
signal-to-noise ratio and iii) a permutation invariant fusion strategy by
temporal pooling operators. We evaluate the proposed method on synthetic and
real data and show that it outperforms by a significant margin existing
single-exposure approaches that we adapted to the multi-exposure case.",None,-1
d64a1ec8-b19b-4f0b-89e1-d00103da33a4,Immersive Text Game and Personality Classification,0.0120734,"We designed and built a game called \textit{Immersive Text Game}, which
allows the player to choose a story and a character, and interact with other
characters in the story in an immersive manner of dialogues. The game is based
on several latest models, including text generation language model, information
extraction model, commonsense reasoning model, and psychology evaluation model.
In the past, similar text games usually let players choose from limited actions
instead of answering on their own, and not every time what characters said are
determined by the player. Through the combination of these models and elaborate
game mechanics and modes, the player will find some novel experiences as driven
through the storyline.",None,-1
2bee3bd7-dfc1-498d-90e4-0297a9335ec2,Cost-Effective Online Contextual Model Selection,0.583542,"How can we collect the most useful labels to learn a model selection policy,
when presented with arbitrary heterogeneous data streams? In this paper, we
formulate this task as an online contextual active model selection problem,
where at each round the learner receives an unlabeled data point along with a
context. The goal is to output the best model for any given context without
obtaining an excessive amount of labels. In particular, we focus on the task of
selecting pre-trained classifiers, and propose a contextual active model
selection algorithm (CAMS), which relies on a novel uncertainty sampling query
criterion defined on a given policy class for adaptive model selection. In
comparison to prior art, our algorithm does not assume a globally optimal
model. We provide rigorous theoretical analysis for the regret and query
complexity under both adversarial and stochastic settings. Our experiments on
several benchmark classification datasets demonstrate the algorithm's
effectiveness in terms of both regret and query complexity. Notably, to achieve
the same accuracy, CAMS incurs less than 10% of the label cost when compared to
the best online model selection baselines on CIFAR10.",None,-1
a5e39904-b69d-407e-8ef5-9a33f47ff4bc,Efficient Training of Language Models to Fill in the Middle,0.719577,"We show that autoregressive language models can learn to infill text after we
apply a straightforward transformation to the dataset, which simply moves a
span of text from the middle of a document to its end. While this data
augmentation has garnered much interest in recent years, we provide extensive
evidence that training models with a large fraction of data transformed in this
way does not harm the original left-to-right generative capability, as measured
by perplexity and sampling evaluations across a wide range of scales. Given the
usefulness, simplicity, and efficiency of training models to fill-in-the-middle
(FIM), we suggest that future autoregressive language models be trained with
FIM by default. To this end, we run a series of ablations on key
hyperparameters, such as the data transformation frequency, the structure of
the transformation, and the method of selecting the infill span. We use these
ablations to prescribe strong default settings and best practices to train FIM
models. We have released our best infilling model trained with best practices
in our API, and release our infilling benchmarks to aid future research.",None,-1
a53804c9-1af7-4ad0-b07d-148a76886f7f,Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking,0.962422,"Exploiting a general-purpose neural architecture to replace hand-wired
designs or inductive biases has recently drawn extensive interest. However,
existing tracking approaches rely on customized sub-modules and need prior
knowledge for architecture selection, hindering the tracking development in a
more general system. This paper presents a Simplified Tracking architecture
(SimTrack) by leveraging a transformer backbone for joint feature extraction
and interaction. Unlike existing Siamese trackers, we serialize the input
images and concatenate them directly before the one-branch backbone. Feature
interaction in the backbone helps to remove well-designed interaction modules
and produce a more efficient and effective framework. To reduce the information
loss from down-sampling in vision transformers, we further propose a foveal
window strategy, providing more diverse input patches with acceptable
computational costs. Our SimTrack improves the baseline with 2.5%/2.6% AUC
gains on LaSOT/TNL2K and gets results competitive with other specialized
tracking algorithms without bells and whistles.",https://github.com/LPXTT/SimTrack,-1
bcbd311b-30dd-4fbe-b14c-c4ba655aaeee,RuCoLA: Russian Corpus of Linguistic Acceptability,0.738787,"Linguistic acceptability (LA) attracts the attention of the research
community due to its many uses, such as testing the grammatical knowledge of
language models and filtering implausible texts with acceptability classifiers.
However, the application scope of LA in languages other than English is limited
due to the lack of high-quality resources. To this end, we introduce the
Russian Corpus of Linguistic Acceptability (RuCoLA), built from the ground up
under the well-established binary LA approach. RuCoLA consists of $9.8$k
in-domain sentences from linguistic publications and $3.6$k out-of-domain
sentences produced by generative models. The out-of-domain set is created to
facilitate the practical use of acceptability for improving language
generation. Our paper describes the data collection protocol and presents a
fine-grained analysis of acceptability classification experiments with a range
of baseline approaches. In particular, we demonstrate that the most widely used
language models still fall behind humans by a large margin, especially when
detecting morphological and semantic errors. We release RuCoLA, the code of
experiments, and a public leaderboard (rucola-benchmark.com) to assess the
linguistic competence of language models for Russian.",https://github.com/RussianNLP/RuCoLA,-1
6b24d6ca-3cbe-472a-a5cd-58b1a069aeca,Fantastic Data and How to Query Them,0.0762508,"It is commonly acknowledged that the availability of the huge amount of
(training) data is one of the most important factors for many recent advances
in Artificial Intelligence (AI). However, datasets are often designed for
specific tasks in narrow AI sub areas and there is no unified way to manage and
access them. This not only creates unnecessary overheads when training or
deploying Machine Learning models but also limits the understanding of the
data, which is very important for data-centric AI. In this paper, we present
our vision about a unified framework for different datasets so that they can be
integrated and queried easily, e.g., using standard query languages. We
demonstrate this in our ongoing work to create a framework for datasets in
Computer Vision and show its advantages in different scenarios. Our
demonstration is available at https://vision.semkg.org.",None,-1
31ad7945-93a3-4f0f-b006-b5ecba67a4a6,Medical Dataset Classification for Kurdish Short Text over Social Media,0.0932752,"The Facebook application is used as a resource for collecting the comments of
this dataset, The dataset consists of 6756 comments to create a Medical Kurdish
Dataset (MKD). The samples are comments of users, which are gathered from
different posts of pages (Medical, News, Economy, Education, and Sport). Six
steps as a preprocessing technique are performed on the raw dataset to clean
and remove noise in the comments by replacing characters. The comments (short
text) are labeled for positive class (medical comment) and negative class
(non-medical comment) as text classification. The percentage ratio of the
negative class is 55% while the positive class is 45%.",None,133
24be0d41-b9a5-4148-a3f0-8b094fc833c9,A Novel Approach for Neuromorphic Vision Data Compression based on Deep Belief Network,0.116124,"A neuromorphic camera is an image sensor that emulates the human eyes
capturing only changes in local brightness levels. They are widely known as
event cameras, silicon retinas or dynamic vision sensors (DVS). DVS records
asynchronous per-pixel brightness changes, resulting in a stream of events that
encode the brightness change's time, location, and polarity. DVS consumes
little power and can capture a wider dynamic range with no motion blur and
higher temporal resolution than conventional frame-based cameras. Although this
method of event capture results in a lower bit rate than traditional video
capture, it is further compressible. This paper proposes a novel deep
learning-based compression scheme for event data. Using a deep belief network
(DBN), the high dimensional event data is reduced into a latent representation
and later encoded using an entropy-based coding technique. The proposed scheme
is among the first to incorporate deep learning for event compression. It
achieves a high compression ratio while maintaining good reconstruction quality
outperforming state-of-the-art event data coders and other lossless benchmark
techniques.",None,-1
466fa5af-fd76-4534-80a3-eedb16da6d5f,Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks,0.149399,"Graph neural networks (GNNs) have become compelling models designed to
perform learning and inference on graph-structured data. However, little work
has been done to understand the fundamental limitations of GNNs for scaling to
larger graphs and generalizing to out-of-distribution (OOD) inputs. In this
paper, we use a random graph generator to systematically investigate how the
graph size and structural properties affect the predictive performance of GNNs.
We present specific evidence that the average node degree is a key feature in
determining whether GNNs can generalize to unseen graphs, and that the use of
multiple node update functions can improve the generalization performance of
GNNs when dealing with graphs of multimodal degree distributions. Accordingly,
we propose a multi-module GNN framework that allows the network to adapt
flexibly to new graphs by generalizing a single canonical nonlinear
transformation over aggregated inputs. Our results show that the multi-module
GNNs improve the OOD generalization on a variety of inference tasks in the
direction of diverse structural features.",None,-1
9845c274-2798-4382-b70d-493cdda8bc59,MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model,0.999818,"Human motion modeling is important for many modern graphics applications,
which typically require professional skills. In order to remove the skill
barriers for laymen, recent motion generation methods can directly generate
human motions conditioned on natural languages. However, it remains challenging
to achieve diverse and fine-grained motion generation with various text inputs.
To address this problem, we propose MotionDiffuse, the first diffusion
model-based text-driven motion generation framework, which demonstrates several
desired properties over existing methods. 1) Probabilistic Mapping. Instead of
a deterministic language-motion mapping, MotionDiffuse generates motions
through a series of denoising steps in which variations are injected. 2)
Realistic Synthesis. MotionDiffuse excels at modeling complicated data
distribution and generating vivid motion sequences. 3) Multi-Level
Manipulation. MotionDiffuse responds to fine-grained instructions on body
parts, and arbitrary-length motion synthesis with time-varied text prompts. Our
experiments show MotionDiffuse outperforms existing SoTA methods by convincing
margins on text-driven motion generation and action-conditioned motion
generation. A qualitative analysis further demonstrates MotionDiffuse's
controllability for comprehensive motion generation. Homepage:
https://mingyuan-zhang.github.io/projects/MotionDiffuse.html",https://mingyuan-zhang.github.io/projects/MotionDiffuse.html,-1
46f727a5-facc-4722-9031-c58eae5fafb4,Motion Matters: A Novel Motion Modeling For Cross-View Gait Feature Learning,0.240797,"As a unique biometric that can be perceived at a distance, gait has broad
applications in person authentication, social security, and so on. Existing
gait recognition methods suffer from changes in viewpoint and clothing and
barely consider extracting diverse motion features, a fundamental
characteristic in gaits, from gait sequences. This paper proposes a novel
motion modeling method to extract the discriminative and robust representation.
Specifically, we first extract the motion features from the encoded motion
sequences in the shallow layer. Then we continuously enhance the motion feature
in deep layers. This motion modeling approach is independent of mainstream work
in building network architectures. As a result, one can apply this motion
modeling method to any backbone to improve gait recognition performance. In
this paper, we combine motion modeling with one commonly used backbone~(GaitGL)
as GaitGL-M to illustrate motion modeling. Extensive experimental results on
two commonly-used cross-view gait datasets demonstrate the superior performance
of GaitGL-M over existing state-of-the-art methods.",None,-1
9d04bd7f-a5ee-4ea4-97cc-58d2c99b4419,Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media,0.454069,"Social media is a breeding ground for threat narratives and related
conspiracy theories. In these, an outside group threatens the integrity of an
inside group, leading to the emergence of sharply defined group identities:
Insiders -- agents with whom the authors identify and Outsiders -- agents who
threaten the insiders. Inferring the members of these groups constitutes a
challenging new NLP task: (i) Information is distributed over many
poorly-constructed posts; (ii) Threats and threat agents are highly contextual,
with the same post potentially having multiple agents assigned to membership in
either group; (iii) An agent's identity is often implicit and transitive; and
(iv) Phrases used to imply Outsider status often do not follow common negative
sentiment patterns. To address these challenges, we define a novel
Insider-Outsider classification task. Because we are not aware of any
appropriate existing datasets or attendant models, we introduce a labeled
dataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages
pretrained language modeling to classify Insiders and Outsiders. NP2IO is shown
to be robust, generalizing to noun phrases not seen during training, and
exceeding the performance of non-trivial baseline models by $20\%$.",https://github.com/heartexlabs/label-studio,-1
52b6b3c1-7d20-4b5a-8e4f-e5fff30d7cfa,Dense Teacher: Dense Pseudo-Labels for Semi-supervised Object Detection,0.966766,"To date, the most powerful semi-supervised object detectors (SS-OD) are based
on pseudo-boxes, which need a sequence of post-processing with fine-tuned
hyper-parameters. In this work, we propose replacing the sparse pseudo-boxes
with the dense prediction as a united and straightforward form of pseudo-label.
Compared to the pseudo-boxes, our Dense Pseudo-Label (DPL) does not involve any
post-processing method, thus retaining richer information. We also introduce a
region selection technique to highlight the key information while suppressing
the noise carried by dense labels. We name our proposed SS-OD algorithm that
leverages the DPL as Dense Teacher. On COCO and VOC, Dense Teacher shows
superior performance under various settings compared with the pseudo-box-based
methods.",https://github.com/Megvii-BaseDetection/DenseTeacher,-1
57a5ca9b-a7f7-4cea-bf54-a58a29a6bc84,Solving Bilevel Knapsack Problem using Graph Neural Networks,0.152328,"The Bilevel Optimization Problem is a hierarchical optimization problem with
two agents, a leader and a follower. The leader make their own decisions first,
and the followers make the best choices accordingly. The leader knows the
information of the followers, and the goal of the problem is to find the
optimal solution by considering the reactions of the followers from the
leader's point of view. For the Bilevel Optimization Problem, there are no
general and efficient algorithms or commercial solvers to get an optimal
solution, and it is very difficult to get a good solution even for a simple
problem. In this paper, we propose a deep learning approach using Graph Neural
Networks to solve the bilevel knapsack problem. We train the model to predict
the leader's solution and use it to transform the hierarchical optimization
problem into a single-level optimization problem to get the solution. Our model
found the feasible solution that was about 500 times faster than the exact
algorithm with $1.7\%$ optimal gap. Also, our model performed well on problems
of different size from the size it was trained on.",None,-1
889286c8-9a04-42a4-ba02-bff11bae6b2c,OpenTAL: Towards Open Set Temporal Action Localization,0.842421,"Temporal Action Localization (TAL) has experienced remarkable success under
the supervised learning paradigm. However, existing TAL methods are rooted in
the closed set assumption, which cannot handle the inevitable unknown actions
in open-world scenarios. In this paper, we, for the first time, step toward the
Open Set TAL (OSTAL) problem and propose a general framework OpenTAL based on
Evidential Deep Learning (EDL). Specifically, the OpenTAL consists of
uncertainty-aware action classification, actionness prediction, and temporal
location regression. With the proposed importance-balanced EDL method,
classification uncertainty is learned by collecting categorical evidence
majorly from important samples. To distinguish the unknown actions from
background video frames, the actionness is learned by the positive-unlabeled
learning. The classification uncertainty is further calibrated by leveraging
the guidance from the temporal localization quality. The OpenTAL is general to
enable existing TAL models for open set scenarios, and experimental results on
THUMOS14 and ActivityNet1.3 benchmarks show the effectiveness of our method.
The code and pre-trained models are released at
https://www.rit.edu/actionlab/opental.",https://www.rit.edu/actionlab/opental,-1
79969bf1-70d6-41f3-93d6-714977313f8e,Stubborn: A Strong Baseline for Indoor Object Navigation,0.852072,"We present a strong baseline that surpasses the performance of previously
published methods on the Habitat Challenge task of navigating to a target
object in indoor environments. Our method is motivated from primary failure
modes of prior state-of-the-art: poor exploration, inaccurate object
identification, and agent getting trapped due to imprecise map construction. We
make three contributions to mitigate these issues: (i) First, we show that
existing map-based methods fail to effectively use semantic clues for
exploration. We present a semantic-agnostic exploration strategy (called
Stubborn) without any learning that surprisingly outperforms prior work. (ii)
We propose a strategy for integrating temporal information to improve object
identification. (iii) Lastly, due to inaccurate depth observation the agent
often gets trapped in small regions. We develop a multi-scale collision map for
obstacle identification that mitigates this issue.",https://github.com/Improbable-AI/Stubborn,-1
12251c1e-ff1e-40e9-b2e4-4cfdefa02dff,Crowd Counting on Heavily Compressed Images with Curriculum Pre-Training,0.105017,"JPEG image compression algorithm is a widely used technique for image size
reduction in edge and cloud computing settings. However, applying such lossy
compression on images processed by deep neural networks can lead to significant
accuracy degradation. Inspired by the curriculum learning paradigm, we propose
a training approach called curriculum pre-training (CPT) for crowd counting on
compressed images, which alleviates the drop in accuracy resulting from lossy
compression. We verify the effectiveness of our approach by extensive
experiments on three crowd counting datasets, two crowd counting DNN models and
various levels of compression. The proposed training method is not overly
sensitive to hyper-parameters, and reduces the error, particularly for heavily
compressed images, by up to 19.70%.",None,-1
05a0ffd7-7000-40c1-88f0-b7418b5326d5,An Item Response Theory Framework for Persuasion,0.524879,"In this paper, we apply Item Response Theory, popular in education and
political science research, to the analysis of argument persuasiveness in
language. We empirically evaluate the model's performance on three datasets,
including a novel dataset in the area of political advocacy. We show the
advantages of separating these components under several style and content
representations, including evaluating the ability of the speaker embeddings
generated by the model to parallel real-world observations about
persuadability.",https://github.com/akornilo/IRT_Persuasion,266
1343bddc-7d30-4649-b97d-5084f326bf86,Towards a Cleaner Document-Oriented Multilingual Crawled Corpus,0.99851,"The need for raw large raw corpora has dramatically increased in recent years
with the introduction of transfer learning and semi-supervised learning methods
to Natural Language Processing. And while there have been some recent attempts
to manually curate the amount of data necessary to train large language models,
the main way to obtain this data is still through automatic web crawling. In
this paper we take the existing multilingual web corpus OSCAR and its pipeline
Ungoliant that extracts and classifies data from Common Crawl at the line
level, and propose a set of improvements and automatic annotations in order to
produce a new document-oriented version of OSCAR that could prove more suitable
to pre-train large generative language models as well as hopefully other
applications in Natural Language Processing and Digital Humanities.",https://github.com/oscar-corpus/oscar-tools,9732
344727f5-9170-4ba8-b4f2-c83bf8ab3f6a,"RARR: Researching and Revising What Language Models Say, Using Language Models",0.986403,"Language models (LMs) now excel at many tasks such as few-shot learning,
question answering, reasoning, and dialog. However, they sometimes generate
unsupported or misleading content. A user cannot easily determine whether their
outputs are trustworthy or not, because most LMs do not have any built-in
mechanism for attribution to external evidence. To enable attribution while
still preserving all the powerful advantages of recent generation models, we
propose RARR (Retrofit Attribution using Research and Revision), a system that
1) automatically finds attribution for the output of any text generation model
and 2) post-edits the output to fix unsupported content while preserving the
original output as much as possible. When applied to the output of several
state-of-the-art LMs on a diverse set of generation tasks, we find that RARR
significantly improves attribution while otherwise preserving the original
input to a much greater degree than previously explored edit models.
Furthermore, the implementation of RARR requires only a handful of training
examples, a large language model, and standard web search.",https://github.com/anthonywchen/RARR,-1
e39c8c53-bb81-499b-b842-249b2cc36f41,NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night,0.505583,"The semantic segmentation of nighttime scenes is a challenging problem that
is key to impactful applications like self-driving cars. Yet, it has received
little attention compared to its daytime counterpart. In this paper, we propose
NightLab, a novel nighttime segmentation framework that leverages multiple deep
learning models imbued with night-aware features to yield State-of-The-Art
(SoTA) performance on multiple night segmentation benchmarks. Notably, NightLab
contains models at two levels of granularity, i.e. image and regional, and each
level is composed of light adaptation and segmentation modules. Given a
nighttime image, the image level model provides an initial segmentation
estimate while, in parallel, a hardness detection module identifies regions and
their surrounding context that need further analysis. A regional level model
focuses on these difficult regions to provide a significantly improved
segmentation. All the models in NightLab are trained end-to-end using a set of
proposed night-aware losses without handcrafted heuristics. Extensive
experiments on the NightCity and BDD100K datasets show NightLab achieves SoTA
performance compared to concurrent methods.",https://github.com/xdeng7/NightLab,-1
2b3726f0-7b0e-4e91-85b7-332e24e504a1,Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding,0.518345,"ASR systems designed for native English (L1) usually underperform on
non-native English (L2). To address this performance gap, \textbf{(i)} we
extend our previous work to investigate fine-tuning of a pre-trained wav2vec
2.0 model \cite{baevski2020wav2vec,xu2021self} under a rich set of L1 and L2
training conditions. We further \textbf{(ii)} incorporate language model
decoding in the ASR system, along with the fine-tuning method. Quantifying
gains acquired from each of these two approaches separately and an error
analysis allows us to identify different sources of improvement within our
models. We find that while the large self-trained wav2vec 2.0 may be
internalizing sufficient decoding knowledge for clean L1 speech
\cite{xu2021self}, this does not hold for L2 speech and accounts for the
utility of employing language model decoding on L2 data.",https://github.com/UBC-NLP/L2ASR,-1
8e975648-7c5c-4478-b2a9-53b18933f6a1,From Indoor To Outdoor: Unsupervised Domain Adaptive Gait Recognition,0.540927,"Gait recognition is an important AI task, which has been progressed rapidly
with the development of deep learning. However, existing learning based gait
recognition methods mainly focus on the single domain, especially the
constrained laboratory environment. In this paper, we study a new problem of
unsupervised domain adaptive gait recognition (UDA-GR), that learns a gait
identifier with supervised labels from the indoor scenes (source domain), and
is applied to the outdoor wild scenes (target domain). For this purpose, we
develop an uncertainty estimation and regularization based UDA-GR method.
Specifically, we investigate the characteristic of gaits in the indoor and
outdoor scenes, for estimating the gait sample uncertainty, which is used in
the unsupervised fine-tuning on the target domain to alleviate the noises of
the pseudo labels. We also establish a new benchmark for the proposed problem,
experimental results on which show the effectiveness of the proposed method. We
will release the benchmark and source code in this work to the public.",None,-1
4053632c-370f-4de9-a5c0-720a079d8ebf,LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding,0.999998,"Structured document understanding has attracted considerable attention and
made significant progress recently, owing to its crucial role in intelligent
document processing. However, most existing related models can only deal with
the document data of specific language(s) (typically English) included in the
pre-training collection, which is extremely limited. To address this issue, we
propose a simple yet effective Language-independent Layout Transformer (LiLT)
for structured document understanding. LiLT can be pre-trained on the
structured documents of a single language and then directly fine-tuned on other
languages with the corresponding off-the-shelf monolingual/multilingual
pre-trained textual models. Experimental results on eight languages have shown
that LiLT can achieve competitive or even superior performance on diverse
widely-used downstream benchmarks, which enables language-independent benefit
from the pre-training of document layout structure. Code and model are publicly
available at https://github.com/jpWang/LiLT.",https://github.com/jpWang/LiLT,-1
49893058-eaff-489a-a39a-37762c9d86d8,Path-Specific Objectives for Safer Agent Incentives,0.56309,"We present a general framework for training safe agents whose naive
incentives are unsafe. As an example, manipulative or deceptive behaviour can
improve rewards but should be avoided. Most approaches fail here: agents
maximize expected return by any means necessary. We formally describe settings
with 'delicate' parts of the state which should not be used as a means to an
end. We then train agents to maximize the causal effect of actions on the
expected return which is not mediated by the delicate parts of state, using
Causal Influence Diagram analysis. The resulting agents have no incentive to
control the delicate state. We further show how our framework unifies and
generalizes existing proposals.",None,-1
1dbdec8c-5af8-4ba6-b68c-1dc191b1c801,Placing Human Animations into 3D Scenes by Learning Interaction- and Geometry-Driven Keyframes,0.342415,"We present a novel method for placing a 3D human animation into a 3D scene
while maintaining any human-scene interactions in the animation. We use the
notion of computing the most important meshes in the animation for the
interaction with the scene, which we call ""keyframes."" These keyframes allow us
to better optimize the placement of the animation into the scene such that
interactions in the animations (standing, laying, sitting, etc.) match the
affordances of the scene (e.g., standing on the floor or laying in a bed). We
compare our method, which we call PAAK, with prior approaches, including POSA,
PROX ground truth, and a motion synthesis method, and highlight the benefits of
our method with a perceptual study. Human raters preferred our PAAK method over
the PROX ground truth data 64.6\% of the time. Additionally, in direct
comparisons, the raters preferred PAAK over competing methods including 61.5\%
compared to POSA.",https://gamma.umd.edu/paak/,60988
a1b99188-04bb-4940-bcff-6717cbc570bd,Reasoning with fuzzy and uncertain evidence using epistemic random fuzzy sets: general framework and practical models,0.550851,"We introduce a general theory of epistemic random fuzzy sets for reasoning
with fuzzy or crisp evidence. This framework generalizes both the
Dempster-Shafer theory of belief functions, and possibility theory. Independent
epistemic random fuzzy sets are combined by the generalized
product-intersection rule, which extends both Dempster's rule for combining
belief functions, and the product conjunctive combination of possibility
distributions. We introduce Gaussian random fuzzy numbers and their
multi-dimensional extensions, Gaussian random fuzzy vectors, as practical
models for quantifying uncertainty about scalar or vector quantities.
Closed-form expressions for the combination, projection and vacuous extension
of Gaussian random fuzzy numbers and vectors are derived.",None,-1
0a703c6b-1dde-4fdf-83c6-25af6eb36f64,RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction,0.908302,"In document-level event extraction (DEE) task, event arguments always scatter
across sentences (across-sentence issue) and multiple events may lie in one
document (multi-event issue). In this paper, we argue that the relation
information of event arguments is of great significance for addressing the
above two issues, and propose a new DEE framework which can model the relation
dependencies, called Relation-augmented Document-level Event Extraction
(ReDEE). More specifically, this framework features a novel and tailored
transformer, named as Relation-augmented Attention Transformer (RAAT). RAAT is
scalable to capture multi-scale and multi-amount argument relations. To further
leverage relation information, we introduce a separate event relation
prediction task and adopt multi-task learning method to explicitly enhance
event extraction performance. Extensive experiments demonstrate the
effectiveness of the proposed method, which can achieve state-of-the-art
performance on two public datasets. Our code is available at https://github.
com/TencentYoutuResearch/RAAT.",https://github.com/TencentYoutuResearch/RAAT,-1
d27a005d-1a26-49bf-ab06-e37798e492bd,PSP-HDRI$+$: A Synthetic Dataset Generator for Pre-Training of Human-Centric Computer Vision Models,0.733332,"We introduce a new synthetic data generator PSP-HDRI$+$ that proves to be a
superior pre-training alternative to ImageNet and other large-scale synthetic
data counterparts. We demonstrate that pre-training with our synthetic data
will yield a more general model that performs better than alternatives even
when tested on out-of-distribution (OOD) sets. Furthermore, using ablation
studies guided by person keypoint estimation metrics with an off-the-shelf
model architecture, we show how to manipulate our synthetic data generator to
further improve model performance.",https://github.com/Unity-Technologies/PeopleSansPeople,-1
bda9aa11-8aa8-49c9-8314-cf3019d796b9,AWADA: Attention-Weighted Adversarial Domain Adaptation for Object Detection,0.1685,"Object detection networks have reached an impressive performance level, yet a
lack of suitable data in specific applications often limits it in practice.
Typically, additional data sources are utilized to support the training task.
In these, however, domain gaps between different data sources pose a challenge
in deep learning. GAN-based image-to-image style-transfer is commonly applied
to shrink the domain gap, but is unstable and decoupled from the object
detection task. We propose AWADA, an Attention-Weighted Adversarial Domain
Adaptation framework for creating a feedback loop between style-transformation
and detection task. By constructing foreground object attention maps from
object detector proposals, we focus the transformation on foreground object
regions and stabilize style-transfer training. In extensive experiments and
ablation studies, we show that AWADA reaches state-of-the-art unsupervised
domain adaptation object detection performance in the commonly used benchmarks
for tasks such as synthetic-to-real, adverse weather and cross-camera
adaptation.",None,-1
a684ab87-abda-4a99-b993-5ec1a5e37a11,Diffusion models for missing value imputation in tabular data,0.991869,"Missing value imputation in machine learning is the task of estimating the
missing values in the dataset accurately using available information. In this
task, several deep generative modeling methods have been proposed and
demonstrated their usefulness, e.g., generative adversarial imputation
networks. Recently, diffusion models have gained popularity because of their
effectiveness in the generative modeling task in images, texts, audio, etc. To
our knowledge, less attention has been paid to the investigation of the
effectiveness of diffusion models for missing value imputation in tabular data.
Based on recent development of diffusion models for time-series data
imputation, we propose a diffusion model approach called ""Conditional
Score-based Diffusion Models for Tabular data"" (TabCSDI). To effectively handle
categorical variables and numerical variables simultaneously, we investigate
three techniques: one-hot encoding, analog bits encoding, and feature
tokenization. Experimental results on benchmark datasets demonstrated the
effectiveness of TabCSDI compared with well-known existing methods, and also
emphasized the importance of the categorical embedding techniques.",https://github.com/vanderschaarlab/hyperimpute,-1
416d6687-a08a-4556-b592-f2908a031c0f,TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving,0.999975,"How should we integrate representations from complementary sensors for
autonomous driving? Geometry-based fusion has shown promise for perception
(e.g. object detection, motion forecasting). However, in the context of
end-to-end driving, we find that imitation learning based on existing sensor
fusion methods underperforms in complex driving scenarios with a high density
of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate
image and LiDAR representations using self-attention. Our approach uses
transformer modules at multiple resolutions to fuse perspective view and bird's
eye view feature maps. We experimentally validate its efficacy on a challenging
new benchmark with long routes and dense traffic, as well as the official
leaderboard of the CARLA urban driving simulator. At the time of submission,
TransFuser outperforms all prior work on the CARLA leaderboard in terms of
driving score by a large margin. Compared to geometry-based fusion, TransFuser
reduces the average collisions per kilometer by 48%.",https://github.com/autonomousvision/transfuser,-1
399d97fd-771e-4e17-9d1c-5f32d8928330,Progressively Generating Better Initial Guesses Towards Next Stages for High-Quality Human Motion Prediction,0.936725,"This paper presents a high-quality human motion prediction method that
accurately predicts future human poses given observed ones. Our method is based
on the observation that a good initial guess of the future poses is very
helpful in improving the forecasting accuracy. This motivates us to propose a
novel two-stage prediction framework, including an init-prediction network that
just computes the good guess and then a formal-prediction network that predicts
the target future poses based on the guess. More importantly, we extend this
idea further and design a multi-stage prediction framework where each stage
predicts initial guess for the next stage, which brings more performance gain.
To fulfill the prediction task at each stage, we propose a network comprising
Spatial Dense Graph Convolutional Networks (S-DGCN) and Temporal Dense Graph
Convolutional Networks (T-DGCN). Alternatively executing the two networks helps
extract spatiotemporal features over the global receptive field of the whole
pose sequence. All the above design choices cooperating together make our
method outperform previous approaches by large margins: 6%-7% on Human3.6M,
5%-10% on CMU-MoCap, and 13%-16% on 3DPW.",https://github.com/705062791/PGBIG,-1
7bbe9ef3-7a1f-42f8-a794-c22514e3c536,Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations,0.793808,"We propose an unsupervised method for 3D geometry-aware representation
learning of articulated objects, in which no image-pose pairs or foreground
masks are used for training. Though photorealistic images of articulated
objects can be rendered with explicit pose control through existing 3D neural
representations, these methods require ground truth 3D pose and foreground
masks for training, which are expensive to obtain. We obviate this need by
learning the representations with GAN training. The generator is trained to
produce realistic images of articulated objects from random poses and latent
vectors by adversarial training. To avoid a high computational cost for GAN
training, we propose an efficient neural representation for articulated objects
based on tri-planes and then present a GAN-based framework for its unsupervised
training. Experiments demonstrate the efficiency of our method and show that
GAN-based training enables the learning of controllable 3D representations
without paired supervision.",https://github.com/open-mmlab/mmpose,-1
a65d371d-e97c-40df-82f6-df7401b7bd85,Omnifont Persian OCR System Using Primitives,0.715982,"In this paper, we introduce a model-based omnifont Persian OCR system. The
system uses a set of 8 primitive elements as structural features for
recognition. First, the scanned document is preprocessed. After normalizing the
preprocessed image, text rows and sub-words are separated and then thinned.
After recognition of dots in sub-words, strokes are extracted and primitive
elements of each sub-word are recognized using the strokes. Finally, the
primitives are compared with a predefined set of character identification
vectors in order to identify sub-word characters. The separation and
recognition steps of the system are concurrent, eliminating unavoidable errors
of independent separation of letters. The system has been tested on documents
with 14 standard Persian fonts in 6 sizes. The achieved precision is 97.06%.",None,-1
aa245415-4308-462c-a0ad-fa05fbd2c8f2,GaitMM: Multi-Granularity Motion Sequence Learning for Gait Recognition,0.0370478,"Gait recognition aims to identify individual-specific walking patterns by
observing the different periodic movements of each body part. However, most
existing methods treat each part equally and fail to account for the data
redundancy caused by the different step frequencies and sampling rates of gait
sequences. In this study, we propose a multi-granularity motion representation
network (GaitMM) for gait sequence learning. In GaitMM, we design a combined
full-body and fine-grained sequence learning module (FFSL) to explore
part-independent spatio-temporal representations. Moreover, we utilize a
frame-wise compression strategy, referred to as multi-scale motion aggregation
(MSMA), to capture discriminative information in the gait sequence. Experiments
on two public datasets, CASIA-B and OUMVLP, show that our approach reaches
state-of-the-art performances.",None,-1
52876021-920b-4db9-9387-b476a0e87c35,"Improving Human-AI Partnerships in Child Welfare: Understanding Worker Practices, Challenges, and Desires for Algorithmic Decision Support",0.980745,"AI-based decision support tools (ADS) are increasingly used to augment human
decision-making in high-stakes, social contexts. As public sector agencies
begin to adopt ADS, it is critical that we understand workers' experiences with
these systems in practice. In this paper, we present findings from a series of
interviews and contextual inquiries at a child welfare agency, to understand
how they currently make AI-assisted child maltreatment screening decisions.
Overall, we observe how workers' reliance upon the ADS is guided by (1) their
knowledge of rich, contextual information beyond what the AI model captures,
(2) their beliefs about the ADS's capabilities and limitations relative to
their own, (3) organizational pressures and incentives around the use of the
ADS, and (4) awareness of misalignments between algorithmic predictions and
their own decision-making objectives. Drawing upon these findings, we discuss
design implications towards supporting more effective human-AI decision-making.",None,-1
965abf95-a8ec-4831-8c1f-a9a3497f3374,Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in Dynamic Environments,0.692367,"We tackle the problem of perceptive locomotion in dynamic environments. In
this problem, a quadrupedal robot must exhibit robust and agile walking
behaviors in response to environmental clutter and moving obstacles. We present
a hierarchical learning framework, named PRELUDE, which decomposes the problem
of perceptive locomotion into high-level decision-making to predict navigation
commands and low-level gait generation to realize the target commands. In this
framework, we train the high-level navigation controller with imitation
learning on human demonstrations collected on a steerable cart and the
low-level gait controller with reinforcement learning (RL). Therefore, our
method can acquire complex navigation behaviors from human supervision and
discover versatile gaits from trial and error. We demonstrate the effectiveness
of our approach in simulation and with hardware experiments. Videos and code
can be found at the project page: https://ut-austin-rpl.github.io/PRELUDE.",https://ut-austin-rpl.github.io/PRELUDE,-1
694b4ff1-f788-483a-9da5-3a120d6f197d,Decoding Attention from Gaze: A Benchmark Dataset and End-to-End Models,0.267441,"Eye-tracking has potential to provide rich behavioral data about human
cognition in ecologically valid environments. However, analyzing this rich data
is often challenging. Most automated analyses are specific to simplistic
artificial visual stimuli with well-separated, static regions of interest,
while most analyses in the context of complex visual stimuli, such as most
natural scenes, rely on laborious and time-consuming manual annotation. This
paper studies using computer vision tools for ""attention decoding"", the task of
assessing the locus of a participant's overt visual attention over time. We
provide a publicly available Multiple Object Eye-Tracking (MOET) dataset,
consisting of gaze data from participants tracking specific objects, annotated
with labels and bounding boxes, in crowded real-world videos, for training and
evaluating attention decoding algorithms. We also propose two end-to-end deep
learning models for attention decoding and compare these to state-of-the-art
heuristic methods.",https://github.com/karan-uppal3/decoding-attention,-1
b63b693f-d73d-4f5e-8d49-07c9c9fcd116,Class-Incremental Learning via Knowledge Amalgamation,0.338542,"Catastrophic forgetting has been a significant problem hindering the
deployment of deep learning algorithms in the continual learning setting.
Numerous methods have been proposed to address the catastrophic forgetting
problem where an agent loses its generalization power of old tasks while
learning new tasks. We put forward an alternative strategy to handle the
catastrophic forgetting with knowledge amalgamation (CFA), which learns a
student network from multiple heterogeneous teacher models specializing in
previous tasks and can be applied to current offline methods. The knowledge
amalgamation process is carried out in a single-head manner with only a
selected number of memorized samples and no annotations. The teachers and
students do not need to share the same network structure, allowing
heterogeneous tasks to be adapted to a compact or sparse data representation.
We compare our method with competitive baselines from different strategies,
demonstrating our approach's advantages.",https://github.com/Ivsucram/CFA,-1
b5752d33-2d6c-41a4-bd0c-4089df54fe8b,Differentially Private Decoding in Large Language Models,0.898162,"Recent large-scale natural language processing (NLP) systems use a
pre-trained Large Language Model (LLM) on massive and diverse corpora as a
headstart. In practice, the pre-trained model is adapted to a wide array of
tasks via fine-tuning on task-specific datasets. LLMs, while effective, have
been shown to memorize instances of training data thereby potentially revealing
private information processed during pre-training. The potential leakage might
further propagate to the downstream tasks for which LLMs are fine-tuned. On the
other hand, privacy-preserving algorithms usually involve retraining from
scratch, which is prohibitively expensive for LLMs. In this work, we propose a
simple, easy to interpret, and computationally lightweight perturbation
mechanism to be applied to an already trained model at the decoding stage. Our
perturbation mechanism is model-agnostic and can be used in conjunction with
any LLM. We provide theoretical analysis showing that the proposed mechanism is
differentially private, and experimental results showing a privacy-utility
trade-off.",https://github.com/kingoflolz/mesh-transformer-jax,-1
e85e3fbc-d8f3-4b73-9665-f246fa39a6a6,Using 3D Shadows to Detect Object Hiding Attacks on Autonomous Vehicle Perception,0.0401455,"Autonomous Vehicles (AVs) are mostly reliant on LiDAR sensors which enable
spatial perception of their surroundings and help make driving decisions.
Recent works demonstrated attacks that aim to hide objects from AV perception,
which can result in severe consequences. 3D shadows, are regions void of
measurements in 3D point clouds which arise from occlusions of objects in a
scene. 3D shadows were proposed as a physical invariant valuable for detecting
spoofed or fake objects. In this work, we leverage 3D shadows to locate
obstacles that are hidden from object detectors. We achieve this by searching
for void regions and locating the obstacles that cause these shadows. Our
proposed methodology can be used to detect an object that has been hidden by an
adversary as these objects, while hidden from 3D object detectors, still induce
shadow artifacts in 3D point clouds, which we use for obstacle detection. We
show that using 3D shadows for obstacle detection can achieve high accuracy in
matching shadows to their object and provide precise prediction of an
obstacle's distance from the ego-vehicle.",None,-1
34dfd8c2-c760-4ddd-a8a5-70e49370699d,Mapping Husserlian phenomenology onto active inference,0.612881,"Phenomenology is the rigorous descriptive study of conscious experience.
Recent attempts to formalize Husserlian phenomenology provide us with a
mathematical model of perception as a function of prior knowledge and
expectation. In this paper, we re-examine elements of Husserlian phenomenology
through the lens of active inference. In doing so, we aim to advance the
project of computational phenomenology, as recently outlined by proponents of
active inference. We propose that key aspects of Husserl's descriptions of
consciousness can be mapped onto aspects of the generative models associated
with the active inference approach. We first briefly review active inference.
We then discuss Husserl's phenomenology, with a focus on time consciousness.
Finally, we present our mapping from Husserlian phenomenology to active
inference.",None,-1
72c22cba-2ca6-4e38-8f06-4db3b8ba1f99,Random Rank: The One and Only Strategyproof and Proportionally Fair Randomized Facility Location Mechanism,0.178221,"Proportionality is an attractive fairness concept that has been applied to a
range of problems including the facility location problem, a classic problem in
social choice. In our work, we propose a concept called Strong Proportionality,
which ensures that when there are two groups of agents at different locations,
both groups incur the same total cost. We show that although Strong
Proportionality is a well-motivated and basic axiom, there is no deterministic
strategyproof mechanism satisfying the property. We then identify a randomized
mechanism called Random Rank (which uniformly selects a number $k$ between $1$
to $n$ and locates the facility at the $k$'th highest agent location) which
satisfies Strong Proportionality in expectation. Our main theorem characterizes
Random Rank as the unique mechanism that achieves universal truthfulness,
universal anonymity, and Strong Proportionality in expectation among all
randomized mechanisms. Finally, we show via the AverageOrRandomRank mechanism
that even stronger ex-post fairness guarantees can be achieved by weakening
universal truthfulness to strategyproofness in expectation.",None,-1
58a1341e-9371-46bb-b319-6f0fa9e098f4,MnTTS2: An Open-Source Multi-Speaker Mongolian Text-to-Speech Synthesis Dataset,0.632121,"Text-to-Speech (TTS) synthesis for low-resource languages is an attractive
research issue in academia and industry nowadays. Mongolian is the official
language of the Inner Mongolia Autonomous Region and a representative
low-resource language spoken by over 10 million people worldwide. However,
there is a relative lack of open-source datasets for Mongolian TTS. Therefore,
we make public an open-source multi-speaker Mongolian TTS dataset, named
MnTTS2, for the benefit of related researchers. In this work, we prepare the
transcription from various topics and invite three professional Mongolian
announcers to form a three-speaker TTS dataset, in which each announcer records
10 hours of speeches in Mongolian, resulting 30 hours in total. Furthermore, we
build the baseline system based on the state-of-the-art FastSpeech2 model and
HiFi-GAN vocoder. The experimental results suggest that the constructed MnTTS2
dataset is sufficient to build robust multi-speaker TTS models for real-world
applications. The MnTTS2 dataset, training recipe, and pretrained models are
released at: \url{https://github.com/ssmlkl/MnTTS2}",https://github.com/ssmlkl/MnTTS2,-1
c1df873f-b2a8-42f2-8587-88704f62aa08,The Third International Verification of Neural Networks Competition (VNN-COMP 2022): Summary and Results,0.857211,"This report summarizes the 3rd International Verification of Neural Networks
Competition (VNN-COMP 2022), held as a part of the 5th Workshop on Formal
Methods for ML-Enabled Autonomous Systems (FoMLAS), which was collocated with
the 34th International Conference on Computer-Aided Verification (CAV).
VNN-COMP is held annually to facilitate the fair and objective comparison of
state-of-the-art neural network verification tools, encourage the
standardization of tool interfaces, and bring together the neural network
verification community. To this end, standardized formats for networks (ONNX)
and specification (VNN-LIB) were defined, tools were evaluated on equal-cost
hardware (using an automatic evaluation pipeline based on AWS instances), and
tool parameters were chosen by the participants before the final test sets were
made public. In the 2022 iteration, 11 teams participated on a diverse set of
12 scored benchmarks. This report summarizes the rules, benchmarks,
participating tools, results, and lessons learned from this iteration of this
competition.",None,-1
0a9e7569-1d8c-4687-88d4-7c76828232d9,Will Large-scale Generative Models Corrupt Future Datasets?,0.298103,"Recently proposed large-scale text-to-image generative models such as
DALL$\cdot$E 2, Midjourney, and StableDiffusion can generate high-quality and
realistic images from users' prompts. Not limited to the research community,
ordinary Internet users enjoy these generative models, and consequently, a
tremendous amount of generated images have been shared on the Internet.
Meanwhile, today's success of deep learning in the computer vision field owes a
lot to images collected from the Internet. These trends lead us to a research
question: ""\textbf{will such generated images impact the quality of future
datasets and the performance of computer vision models positively or
negatively?}"" This paper empirically answers this question by simulating
contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using
a state-of-the-art generative model and evaluate models trained with
""contaminated"" datasets on various tasks, including image classification and
image generation. Throughout experiments, we conclude that generated images
negatively affect downstream performance, while the significance depends on
tasks and the amount of generated images. The generated datasets and the codes
for experiments will be publicly released for future research. Generated
datasets and source codes are available from
\url{https://github.com/moskomule/dataset-contamination}.",https://github.com/moskomule/dataset-contamination,694
00110da2-8471-46ed-b4d0-572862c3d17e,Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation,0.641134,"Federated learning (FL) can be essential in knowledge representation,
reasoning, and data mining applications over multi-source knowledge graphs
(KGs). A recent study FedE first proposes an FL framework that shares entity
embeddings of KGs across all clients. However, entity embedding sharing from
FedE would incur a severe privacy leakage. Specifically, the known entity
embedding can be used to infer whether a specific relation between two entities
exists in a private client. In this paper, we introduce a novel attack method
that aims to recover the original data based on the embedding information,
which is further used to evaluate the vulnerabilities of FedE. Furthermore, we
propose a Federated learning paradigm with privacy-preserving Relation
embedding aggregation (FedR) to tackle the privacy issue in FedE. Besides,
relation embedding sharing can significantly reduce the communication cost due
to its smaller size of queries. We conduct extensive experiments to evaluate
FedR with five different KG embedding models and three datasets. Compared to
FedE, FedR achieves similar utility and significant improvements regarding
privacy-preserving effect and communication efficiency on the link prediction
task.",None,23079
ae0c3382-fe9b-4593-8a09-53cf9dcd6db5,DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling,0.0732905,"Retrieval-based conversational systems learn to rank response candidates for
a given dialogue context by computing the similarity between their vector
representations. However, training on a single textual form of the multi-turn
context limits the ability of a model to learn representations that generalize
to natural perturbations seen during inference. In this paper we propose a
framework that incorporates augmented versions of a dialogue context into the
learning objective. We utilize contrastive learning as an auxiliary objective
to learn robust dialogue context representations that are invariant to
perturbations injected through the augmentation method. We experiment with four
benchmark dialogue datasets and demonstrate that our framework combines well
with existing augmentation methods and can significantly improve over baseline
BERT-based ranking architectures. Furthermore, we propose a novel data
augmentation method, ConMix, that adds token level perturbations through
stochastic mixing of tokens from other contexts in the batch. We show that our
proposed augmentation method outperforms previous data augmentation approaches,
and provides dialogue representations that are more robust to common
perturbations seen during inference.",https://github.com/rkadlec/ubuntu-ranking-dataset-,-1
93267a5e-59d7-418c-ab85-8f47c464b841,SAT: Improving Semi-Supervised Text Classification with Simple Instance-Adaptive Self-Training,0.331893,"Self-training methods have been explored in recent years and have exhibited
great performance in improving semi-supervised learning. This work presents a
Simple instance-Adaptive self-Training method (SAT) for semi-supervised text
classification. SAT first generates two augmented views for each unlabeled data
and then trains a meta-learner to automatically identify the relative strength
of augmentations based on the similarity between the original view and the
augmented views. The weakly-augmented view is fed to the model to produce a
pseudo-label and the strongly-augmented view is used to train the model to
predict the same pseudo-label. We conducted extensive experiments and analyses
on three text classification datasets and found that with varying sizes of
labeled training data, SAT consistently shows competitive performance compared
to existing semi-supervised learning methods. Our code can be found at
\url{https://github.com/declare-lab/SAT.git}.",https://github.com/declare-lab/SAT.git,-1
b750f8c0-8fea-49a0-9b9e-deed41bcb5f2,Compression of Generative Pre-trained Language Models via Quantization,0.980507,"The increasing size of generative Pre-trained Language Models (PLMs) has
greatly increased the demand for model compression. Despite various methods to
compress BERT or its variants, there are few attempts to compress generative
PLMs, and the underlying difficulty remains unclear. In this paper, we compress
generative PLMs by quantization. We find that previous quantization methods
fail on generative tasks due to the \textit{homogeneous word embeddings} caused
by reduced capacity, and \textit{varied distribution of weights}.
Correspondingly, we propose a token-level contrastive distillation to learn
distinguishable word embeddings, and a module-wise dynamic scaling to make
quantizers adaptive to different modules. Empirical results on various tasks
show that our proposed method outperforms the state-of-the-art compression
methods on generative PLMs by a clear margin. With comparable performance with
the full-precision models, we achieve 14.4x and 13.4x compression rates on
GPT-2 and BART, respectively.",None,-1
0eebf932-de8f-4e1d-994c-83aa27dbdfc0,Argus++: Robust Real-time Activity Detection for Unconstrained Video Streams with Overlapping Cube Proposals,0.502194,"Activity detection is one of the attractive computer vision tasks to exploit
the video streams captured by widely installed cameras. Although achieving
impressive performance, conventional activity detection algorithms are usually
designed under certain constraints, such as using trimmed and/or
object-centered video clips as inputs. Therefore, they failed to deal with the
multi-scale multi-instance cases in real-world unconstrained video streams,
which are untrimmed and have large field-of-views. Real-time requirements for
streaming analysis also mark brute force expansion of them unfeasible.
  To overcome these issues, we propose Argus++, a robust real-time activity
detection system for analyzing unconstrained video streams. The design of
Argus++ introduces overlapping spatio-temporal cubes as an intermediate concept
of activity proposals to ensure coverage and completeness of activity detection
through over-sampling. The overall system is optimized for real-time processing
on standalone consumer-level hardware. Extensive experiments on different
surveillance and driving scenarios demonstrated its superior performance in a
series of activity detection benchmarks, including CVPR ActivityNet ActEV 2021,
NIST ActEV SDL UF/KF, TRECVID ActEV 2020/2021, and ICCV ROAD 2021.",None,233
154476b5-ed35-4eab-9d75-b2200ddc352c,Structured Pruning Learns Compact and Accurate Models,0.999305,"The growing size of neural language models has led to increased attention in
model compression. The two predominant approaches are pruning, which gradually
removes weights from a pre-trained model, and distillation, which trains a
smaller compact model to match a larger one. Pruning methods can significantly
reduce the model size but hardly achieve large speedups as distillation.
However, distillation methods require large amounts of unlabeled data and are
expensive to train. In this work, we propose a task-specific structured pruning
method CoFi (Coarse- and Fine-grained Pruning), which delivers highly
parallelizable subnetworks and matches the distillation methods in both
accuracy and latency, without resorting to any unlabeled data. Our key insight
is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads
and hidden units) modules, which controls the pruning decision of each
parameter with masks of different granularity. We also devise a layerwise
distillation strategy to transfer knowledge from unpruned to pruned models
during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi
yields models with over 10x speedups with a small accuracy drop, showing its
effectiveness and efficiency compared to previous pruning and distillation
approaches.",https://github.com/princeton-nlp/CoFiPruning,-1
139c1a2d-8408-4a36-b4fe-b0d9ec0cff14,VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting,0.839875,"We propose VoLux-GAN, a generative framework to synthesize 3D-aware faces
with convincing relighting. Our main contribution is a volumetric HDRI
relighting method that can efficiently accumulate albedo, diffuse and specular
lighting contributions along each 3D ray for any desired HDR environmental map.
Additionally, we show the importance of supervising the image decomposition
process using multiple discriminators. In particular, we propose a data
augmentation technique that leverages recent advances in single image portrait
relighting to enforce consistent geometry, albedo, diffuse and specular
components. Multiple experiments and comparisons with other generative
frameworks show how our model is a step forward towards photorealistic
relightable 3D generative models.",None,10370
b4911c70-1f03-485a-8899-e6215622e72f,Improving Neural Machine Translation by Denoising Training,0.245912,"We present a simple and effective pretraining strategy {D}en{o}ising
{T}raining DoT for neural machine translation. Specifically, we update the
model parameters with source- and target-side denoising tasks at the early
stage and then tune the model normally. Notably, our approach does not increase
any parameters or training steps, requiring the parallel data merely.
Experiments show that DoT consistently improves the neural machine translation
performance across 12 bilingual and 16 multilingual directions (data size
ranges from 80K to 20M). In addition, we show that DoT can complement existing
data manipulation strategies, i.e. curriculum learning, knowledge distillation,
data diversification, bidirectional training, and back-translation.
Encouragingly, we found that DoT outperforms costly pretrained model mBART in
high-resource settings. Analyses show DoT is a novel in-domain cross-lingual
pretraining strategy and could offer further improvements with task-relevant
self-supervisions.",https://github.com/pytorch/fairseq/,-1
b19e95ad-96f1-426b-b9bc-36879d1798d2,Automatic Tabulation in Constraint Models,0.22003,"The performance of a constraint model can often be improved by converting a
subproblem into a single table constraint. In this paper we study heuristics
for identifying promising candidate subproblems, where converting the candidate
into a table constraint is likely to improve solver performance. We propose a
small set of heuristics to identify common cases, such as expressions that will
propagate weakly. The process of discovering promising subproblems and
tabulating them is entirely automated in the constraint modelling tool Savile
Row. Caches are implemented to avoid tabulating equivalent subproblems many
times. We give a simple algorithm to generate table constraints directly from a
constraint expression in \savilerow. We demonstrate good performance on the
benchmark problems used in earlier work on tabulation, and also for several new
problem classes. In some cases, the entirely automated process leads to orders
of magnitude improvements in solver performance.",https://github.com/stacs-cp/tabulation-exp,-1
109111b3-658e-4ab3-beed-040d2aa03a31,QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars,0.98732,"Real-time tracking of human body motion is crucial for interactive and
immersive experiences in AR/VR. However, very limited sensor data about the
body is available from standalone wearable devices such as HMDs (Head Mounted
Devices) or AR glasses. In this work, we present a reinforcement learning
framework that takes in sparse signals from an HMD and two controllers, and
simulates plausible and physically valid full body motions. Using high quality
full body motion as dense supervision during training, a simple policy network
can learn to output appropriate torques for the character to balance, walk, and
jog, while closely following the input signals. Our results demonstrate
surprisingly similar leg motions to ground truth without any observations of
the lower body, even when the input is only the 6D transformations of the HMD.
We also show that a single policy can be robust to diverse locomotion styles,
different body sizes, and novel environments.",None,912
01cb360b-00a0-483f-b46b-230a1fe746dc,An Attention-based Method for Action Unit Detection at the 3rd ABAW Competition,0.636091,"Facial Action Coding System is an approach for modeling the complexity of
human emotional expression. Automatic action unit (AU) detection is a crucial
research area in human-computer interaction. This paper describes our
submission to the third Affective Behavior Analysis in-the-wild (ABAW)
competition 2022. We proposed a method for detecting facial action units in the
video. At the first stage, a lightweight CNN-based feature extractor is
employed to extract the feature map from each video frame. Then, an attention
module is applied to refine the attention map. The attention encoded vector is
derived using a weighted sum of the feature map and the attention scores later.
Finally, the sigmoid function is used at the output layer to make the
prediction suitable for multi-label AUs detection. We achieved a macro F1 score
of 0.48 on the ABAW challenge validation set compared to 0.39 from the baseline
model.",None,-1
060dab27-b589-41c6-917c-71bb0a7feb3c,CNLL: A Semi-supervised Approach For Continual Noisy Label Learning,0.421929,"The task of continual learning requires careful design of algorithms that can
tackle catastrophic forgetting. However, the noisy label, which is inevitable
in a real-world scenario, seems to exacerbate the situation. While very few
studies have addressed the issue of continual learning under noisy labels, long
training time and complicated training schemes limit their applications in most
cases. In contrast, we propose a simple purification technique to effectively
cleanse the online data stream that is both cost-effective and more accurate.
After purification, we perform fine-tuning in a semi-supervised fashion that
ensures the participation of all available samples. Training in this fashion
helps us learn a better representation that results in state-of-the-art (SOTA)
performance. Through extensive experimentation on 3 benchmark datasets, MNIST,
CIFAR10 and CIFAR100, we show the effectiveness of our proposed approach. We
achieve a 24.8% performance gain for CIFAR10 with 20% noise over previous SOTA
methods. Our code is publicly available.",https://github.com/nazmul-karim170/CNLL,-1
ff9d5096-3f50-4475-b1b3-fbc61bcc546a,Multiple Object Tracking from appearance by hierarchically clustering tracklets,0.435813,"Current approaches in Multiple Object Tracking (MOT) rely on the
spatio-temporal coherence between detections combined with object appearance to
match objects from consecutive frames. In this work, we explore MOT using
object appearances as the main source of association between objects in a
video, using spatial and temporal priors as weighting factors. We form initial
tracklets by leveraging on the idea that instances of an object that are close
in time should be similar in appearance, and build the final object tracks by
fusing the tracklets in a hierarchical fashion. We conduct extensive
experiments that show the effectiveness of our method over three different MOT
benchmarks, MOT17, MOT20, and DanceTrack, being competitive in MOT17 and MOT20
and establishing state-of-the-art results in DanceTrack.",https://github.com/NII-Satoh-Lab/MOT_FCG,-1
e356f915-c1e5-4b4b-9e65-2f01d3b6a22f,On the Relation between Sensitivity and Accuracy in In-context Learning,0.820593,"In-context learning (ICL) suffers from oversensitivity to the prompt, making
it unreliable in real-world scenarios. We study the sensitivity of ICL with
respect to multiple perturbation types. First, we find that label bias obscures
the true sensitivity, and therefore prior work may have significantly
underestimated ICL sensitivity. Second, we observe a strong negative
correlation between ICL sensitivity and accuracy: predictions sensitive to
perturbations are less likely to be correct. Motivated by these findings, we
propose \textsc{SenSel}, a few-shot selective prediction method that abstains
from sensitive predictions. Experiments on ten classification datasets show
that \textsc{SenSel} consistently outperforms two commonly used
confidence-based and entropy-based baselines on abstention decisions.",https://github.com/yandachen/ICLSensitivity,-1
641544e1-f9a1-4b74-9bea-e97692099ab2,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,0.265463,"IR models using a pretrained language model significantly outperform lexical
approaches like BM25. In particular, SPLADE, which encodes texts to sparse
vectors, is an effective model for practical use because it shows robustness to
out-of-domain datasets. However, SPLADE still struggles with exact matching of
low-frequency words in training data. In addition, domain shifts in vocabulary
and word frequencies deteriorate the IR performance of SPLADE. Because
supervision data are scarce in the target domain, addressing the domain shifts
without supervision data is necessary. This paper proposes an unsupervised
domain adaptation method by filling vocabulary and word-frequency gaps. First,
we expand a vocabulary and execute continual pretraining with a masked language
model on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse
vectors by inverse document frequency weights to consider the importance of
documents with lowfrequency words. We conducted experiments using our method on
datasets with a large vocabulary gap from a source domain. We show that our
method outperforms the present stateof-the-art domain adaptation method. In
addition, our method achieves state-of-the-art results, combined with BM25.",https://github.com/meshidenn/CAI.git,-1
2df16605-57c3-4974-b39b-30ca62f6bd35,CIRCLe: Color Invariant Representation Learning for Unbiased Classification of Skin Lesions,0.952044,"While deep learning based approaches have demonstrated expert-level
performance in dermatological diagnosis tasks, they have also been shown to
exhibit biases toward certain demographic attributes, particularly skin types
(e.g., light versus dark), a fairness concern that must be addressed. We
propose CIRCLe, a skin color invariant deep representation learning method for
improving fairness in skin lesion classification. CIRCLe is trained to classify
images by utilizing a regularization loss that encourages images with the same
diagnosis but different skin types to have similar latent representations.
Through extensive evaluation and ablation studies, we demonstrate CIRCLe's
superior performance over the state-of-the-art when evaluated on 16k+ images
spanning 6 Fitzpatrick skin types and 114 diseases, using classification
accuracy, equal opportunity difference (for light versus dark groups), and
normalized accuracy range, a new measure we propose to assess fairness on
multiple skin type groups.",https://github.com/arezou-pakzad/CIRCLe,13990
5bf8001e-4c5b-4953-9425-dbcf6962fc81,PromptBERT: Improving BERT Sentence Embeddings with Prompts,0.932574,"We propose PromptBERT, a novel contrastive learning method for learning
better sentence representation. We firstly analyze the drawback of current
sentence embedding from original BERT and find that it is mainly due to the
static token embedding bias and ineffective BERT layers. Then we propose the
first prompt-based sentence embeddings method and discuss two prompt
representing methods and three prompt searching methods to make BERT achieve
better sentence embeddings. Moreover, we propose a novel unsupervised training
objective by the technology of template denoising, which substantially shortens
the performance gap between the supervised and unsupervised settings. Extensive
experiments show the effectiveness of our method. Compared to SimCSE,
PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and
RoBERTa in the unsupervised setting.",https://github.com/kongds/Prompt-BERT,-1
a6c7e885-bc4f-4f98-83bb-c7f931025213,Transformers are Sample-Efficient World Models,0.792866,"Deep reinforcement learning agents are notoriously sample inefficient, which
considerably limits their application to real-world problems. Recently, many
model-based methods have been designed to address this issue, with learning in
the imagination of a world model being one of the most prominent approaches.
However, while virtually unlimited interaction with a simulated environment
sounds appealing, the world model has to be accurate over extended periods of
time. Motivated by the success of Transformers in sequence modeling tasks, we
introduce IRIS, a data-efficient agent that learns in a world model composed of
a discrete autoencoder and an autoregressive Transformer. With the equivalent
of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean
human normalized score of 1.046, and outperforms humans on 10 out of 26 games,
setting a new state of the art for methods without lookahead search. To foster
future research on Transformers and world models for sample-efficient
reinforcement learning, we release our code and models at
https://github.com/eloialonso/iris.",https://github.com/eloialonso/iris,-1
83ed3f04-4d21-42fd-a13d-325f769c1c27,Continual Spatio-Temporal Graph Convolutional Networks,0.472025,"Graph-based reasoning over skeleton data has emerged as a promising approach
for human action recognition. However, the application of prior graph-based
methods, which predominantly employ whole temporal sequences as their input, to
the setting of online inference entails considerable computational redundancy.
In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph
Convolutional Neural Network as a Continual Inference Network, which can
perform step-by-step predictions in time without repeat frame processing. To
evaluate our method, we create a continual version of ST-GCN, CoST-GCN,
alongside two derived methods with different self-attention mechanisms, CoAGCN
and CoS-TR. We investigate weight transfer strategies and architectural
modifications for inference acceleration, and perform experiments on the NTU
RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar
predictive accuracy, we observe up to 109x reduction in time complexity,
on-hardware accelerations of 26x, and reductions in maximum allocated memory of
52% during online inference.",https://github.com/lukashedegaard/continual-skeletons,-1
17f88e41-72e1-472c-981a-fb019ca41dd0,Contrastive Learning of Coarse-Grained Force Fields,0.644812,"Coarse-grained models have proven helpful for simulating complex systems over
long timescales to provide molecular insights into various processes.
Methodologies for systematic parameterization of the underlying energy
function, or force field that describes the interactions among different
components of the system are of great interest for ensuring simulation
accuracy. We present a new method, potential contrasting, to enable efficient
learning of force fields that can accurately reproduce the conformational
distribution produced with all-atom simulations. Potential contrasting
generalizes the noise contrastive estimation method with umbrella sampling to
better learn the complex energy landscape of molecular systems. When applied to
the Trp-cage protein, we found that the technique produces force fields that
thoroughly capture the thermodynamics of the folding process despite the use of
only $\alpha$-Carbons in the coarse-grained model. We further showed that
potential contrasting could be applied over large datasets that combine the
conformational ensembles of many proteins to ensure the transferability of
coarse-grained force fields. We anticipate potential contrasting to be a
powerful tool for building general-purpose coarse-grained force fields.",None,-1
c52ffe74-1679-46bf-b38b-ad337f172b14,Deep Span Representations for Named Entity Recognition,0.0525431,"Span-based models are one of the most straightforward methods for named
entity recognition (NER). Existing span-based NER systems shallowly aggregate
the token representations to span representations. However, this typically
results in significant ineffectiveness for long-span entities, a coupling
between the representations of overlapping spans, and ultimately a performance
degradation. In this study, we propose DSpERT (Deep Span Encoder
Representations from Transformers), which comprises a standard Transformer and
a span Transformer. The latter uses low-layered span representations as
queries, and aggregates the token representations as keys and values, layer by
layer from bottom to top. Thus, DSpERT produces span representations of deep
semantics.
  With weight initialization from pretrained language models, DSpERT achieves
performance higher than or competitive with recent state-of-the-art systems on
eight NER benchmarks. Experimental results verify the importance of the depth
for span representations, and show that DSpERT performs particularly well on
long-span entities and nested structures. Further, the deep span
representations are well structured and easily separable in the feature space.",https://github.com/syuoni/eznlp,-1
05b2b505-605b-44a6-9fad-c3ad19482a7b,Parallel Instance Query Network for Named Entity Recognition,0.533566,"Named entity recognition (NER) is a fundamental task in natural language
processing. Recent works treat named entity recognition as a reading
comprehension task, constructing type-specific queries manually to extract
entities. This paradigm suffers from three issues. First, type-specific queries
can only extract one type of entities per inference, which is inefficient.
Second, the extraction for different types of entities is isolated, ignoring
the dependencies between them. Third, query construction relies on external
knowledge and is difficult to apply to realistic scenarios with hundreds of
entity types. To deal with them, we propose Parallel Instance Query Network
(PIQN), which sets up global and learnable instance queries to extract entities
from a sentence in a parallel manner. Each instance query predicts one entity,
and by feeding all instance queries simultaneously, we can query all entities
in parallel. Instead of being constructed from external knowledge, instance
queries can learn their different query semantics during training. For training
the model, we treat label assignment as a one-to-many Linear Assignment Problem
(LAP) and dynamically assign gold entities to instance queries with minimal
assignment cost. Experiments on both nested and flat NER datasets demonstrate
that our proposed method outperforms previous state-of-the-art models.",https://github.com/tricktreat/piqn,-1
256b1027-601f-496b-86e5-e39895a4a29e,Exploration with Multi-Sample Target Values for Distributional Reinforcement Learning,0.0344912,"Distributional reinforcement learning (RL) aims to learn a value-network that
predicts the full distribution of the returns for a given state, often modeled
via a quantile-based critic. This approach has been successfully integrated
into common RL methods for continuous control, giving rise to algorithms such
as Distributional Soft Actor-Critic (DSAC). In this paper, we introduce
multi-sample target values (MTV) for distributional RL, as a principled
replacement for single-sample target value estimation, as commonly employed in
current practice. The improved distributional estimates further lend themselves
to UCB-based exploration. These two ideas are combined to yield our
distributional RL algorithm, E2DC (Extra Exploration with Distributional
Critics). We evaluate our approach on a range of continuous control tasks and
demonstrate state-of-the-art model-free performance on difficult tasks such as
Humanoid control. We provide further insight into the method via visualization
and analysis of the learned distributions and their evolution during training.",https://github.com/vitchyr/rlkit,-1
24be20c6-2878-4525-859d-761940dafc36,A New Knowledge Distillation Network for Incremental Few-Shot Surface Defect Detection,0.197953,"Surface defect detection is one of the most essential processes for
industrial quality inspection. Deep learning-based surface defect detection
methods have shown great potential. However, the well-performed models usually
require large training data and can only detect defects that appeared in the
training stage. When facing incremental few-shot data, defect detection models
inevitably suffer from catastrophic forgetting and misclassification problem.
To solve these problems, this paper proposes a new knowledge distillation
network, called Dual Knowledge Align Network (DKAN). The proposed DKAN method
follows a pretraining-finetuning transfer learning paradigm and a knowledge
distillation framework is designed for fine-tuning. Specifically, an
Incremental RCNN is proposed to achieve decoupled stable feature representation
of different categories. Under this framework, a Feature Knowledge Align (FKA)
loss is designed between class-agnostic feature maps to deal with catastrophic
forgetting problems, and a Logit Knowledge Align (LKA) loss is deployed between
logit distributions to tackle misclassification problems. Experiments have been
conducted on the incremental Few-shot NEU-DET dataset and results show that
DKAN outperforms other methods on various few-shot scenes, up to 6.65% on the
mean Average Precision metric, which proves the effectiveness of the proposed
method.",None,-1
07789a37-09d3-45d8-aef6-636c90f7f1d8,Dual Progressive Transformations for Weakly Supervised Semantic Segmentation,0.307241,"Weakly supervised semantic segmentation (WSSS), which aims to mine the object
regions by merely using class-level labels, is a challenging task in computer
vision. The current state-of-the-art CNN-based methods usually adopt
Class-Activation-Maps (CAMs) to highlight the potential areas of the object,
however, they may suffer from the part-activated issues. To this end, we try an
early attempt to explore the global feature attention mechanism of vision
transformer in WSSS task. However, since the transformer lacks the inductive
bias as in CNN models, it can not boost the performance directly and may yield
the over-activated problems. To tackle these drawbacks, we propose a
Convolutional Neural Networks Refined Transformer (CRT) to mine a globally
complete and locally accurate class activation maps in this paper. To validate
the effectiveness of our proposed method, extensive experiments are conducted
on PASCAL VOC 2012 and CUB-200-2011 datasets. Experimental evaluations show
that our proposed CRT achieves the new state-of-the-art performance on both the
weakly supervised semantic segmentation task the weakly supervised object
localization task, which outperform others by a large margin.",https://github.com/huodongjian0603/crt,5685
35711a57-c9ce-4e25-b96a-fcb35bf05308,On the Identifiability of Nonlinear ICA: Sparsity and Beyond,0.994915,"Nonlinear independent component analysis (ICA) aims to recover the underlying
independent latent sources from their observable nonlinear mixtures. How to
make the nonlinear ICA model identifiable up to certain trivial indeterminacies
is a long-standing problem in unsupervised learning. Recent breakthroughs
reformulate the standard independence assumption of sources as conditional
independence given some auxiliary variables (e.g., class labels and/or
domain/time indexes) as weak supervision or inductive bias. However, nonlinear
ICA with unconditional priors cannot benefit from such developments. We explore
an alternative path and consider only assumptions on the mixing process, such
as Structural Sparsity. We show that under specific instantiations of such
constraints, the independent latent sources can be identified from their
nonlinear mixtures up to a permutation and a component-wise transformation,
thus achieving nontrivial identifiability of nonlinear ICA without auxiliary
variables. We provide estimation methods and validate the theoretical results
experimentally. The results on image data suggest that our conditions may hold
in a number of practical data generating processes.",https://github.com/VLL-HD/GIN,-1
9e707ee6-deb8-4bf8-8e4e-d00e61040dfb,DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata,0.615552,"Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic
videos. However, they require a slow iterative optimization process to
synthesize a single fixed-size short video, and they do not offer any
post-training control over the synthesis process. We propose Dynamic Neural
Cellular Automata (DyNCA), a framework for real-time and controllable dynamic
texture synthesis. Our method is built upon the recently introduced NCA models
and can synthesize infinitely long and arbitrary-sized realistic video textures
in real time. We quantitatively and qualitatively evaluate our model and show
that our synthesized videos appear more realistic than the existing results. We
improve the SOTA DyTS performance by $2\sim 4$ orders of magnitude. Moreover,
our model offers several real-time video controls including motion speed,
motion direction, and an editing brush tool. We exhibit our trained models in
an online interactive demo that runs on local hardware and is accessible on
personal computers and smartphones.",None,-1
a77e94cf-ddc1-4f64-aab0-2c46fa485c62,CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation,0.40863,"Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed
inputs during training -- helps reduce model reliance on spurious correlations
and improves generalization to out-of-distribution (OOD) data. Prior work on
generating counterfactuals only considered restricted classes of perturbations,
limiting their effectiveness. We present COunterfactual Generation via
Retrieval and Editing (CORE), a retrieval-augmented generation framework for
creating diverse counterfactual perturbations for CDA. For each training
example, CORE first performs a dense retrieval over a task-related unlabeled
text corpus using a learned bi-encoder and extracts relevant counterfactual
excerpts. CORE then incorporates these into prompts to a large language model
with few-shot learning capabilities, for counterfactual editing. Conditioning
language model edits on naturally occurring data results in diverse
perturbations. Experiments on natural language inference and sentiment analysis
benchmarks show that CORE counterfactuals are more effective at improving
generalization to OOD data compared to other DA approaches. We also show that
the CORE retrieval framework can be used to encourage diversity in manually
authored perturbations",https://github.com/tanay2001/CORE,-1
020d0364-6fa6-4811-aa5c-81c65273996e,A Deep-Discrete Learning Framework for Spherical Surface Registration,0.778236,"Cortical surface registration is a fundamental tool for neuroimaging analysis
that has been shown to improve the alignment of functional regions relative to
volumetric approaches. Classically, image registration is performed by
optimizing a complex objective similarity function, leading to long run times.
This contributes to a convention for aligning all data to a global average
reference frame that poorly reflects the underlying cortical heterogeneity. In
this paper, we propose a novel unsupervised learning-based framework that
converts registration to a multi-label classification problem, where each point
in a low-resolution control grid deforms to one of fixed, finite number of
endpoints. This is learned using a spherical geometric deep learning
architecture, in an end-to-end unsupervised way, with regularization imposed
using a deep Conditional Random Field (CRF). Experiments show that our proposed
framework performs competitively, in terms of similarity and areal distortion,
relative to the most popular classical surface registration algorithms and
generates smoother deformations than other learning-based surface registration
methods, even in subjects with atypical cortical morphology.",https://github.com/ThomasYeoLab/CBIG,10746
6ddfb2b4-9b7d-44f5-bcb7-7930e150948e,Prompting Is Programming: A Query Language for Large Language Models,0.741358,"Large language models have demonstrated outstanding performance on a wide
range of tasks such as question answering and code generation. On a high level,
given an input, a language model can be used to automatically complete the
sequence in a statistically-likely way. Based on this, users prompt these
models with language instructions or examples, to implement a variety of
downstream tasks. Advanced prompting methods can even imply interaction between
the language model, a user, and external tools such as calculators. However, to
obtain state-of-the-art performance or adapt language models for specific
tasks, complex task- and model-specific programs have to be implemented, which
may still require ad-hoc interaction.
  Based on this, we present the novel idea of Language Model Programming (LMP).
LMP generalizes language model prompting from pure text prompts to an intuitive
combination of text prompting and scripting. Additionally, LMP allows
constraints to be specified over the language model output. This enables easy
adaption to many tasks while abstracting language model internals and providing
high-level semantics.
  To enable LMP, we implement LMQL(short for Language Model Query Language),
which leverages the constraints and control flow from an LMP prompt to generate
an efficient inference procedure that minimizes the number of expensive calls
to the underlying language model.
  We show that LMQL can capture a wide range of state-of-the-art prompting
methods in an intuitive way, especially facilitating interactive flows that are
challenging to implement with existing high-level APIs. Our evaluation shows
that we retain or increase the accuracy on several downstream tasks, while also
significantly reducing the required amount of computation or cost in the case
of pay-to-use APIs (26-85% cost savings).",https://github.com/eth-sri/lmql,-1
a516e3cc-c80f-48ff-900c-75a2e38b348c,BadPrompt: Backdoor Attacks on Continuous Prompts,0.861906,"The prompt-based learning paradigm has gained much research attention
recently. It has achieved state-of-the-art performance on several NLP tasks,
especially in the few-shot scenarios. While steering the downstream tasks, few
works have been reported to investigate the security problems of the
prompt-based models. In this paper, we conduct the first study on the
vulnerability of the continuous prompt learning algorithm to backdoor attacks.
We observe that the few-shot scenarios have posed a great challenge to backdoor
attacks on the prompt-based models, limiting the usability of existing NLP
backdoor methods. To address this challenge, we propose BadPrompt, a
lightweight and task-adaptive algorithm, to backdoor attack continuous prompts.
Specially, BadPrompt first generates candidate triggers which are indicative
for predicting the targeted label and dissimilar to the samples of the
non-targeted labels. Then, it automatically selects the most effective and
invisible trigger for each sample with an adaptive trigger optimization
algorithm. We evaluate the performance of BadPrompt on five datasets and two
continuous prompt models. The results exhibit the abilities of BadPrompt to
effectively attack continuous prompts while maintaining high performance on the
clean test sets, outperforming the baseline models by a large margin. The
source code of BadPrompt is publicly available at
https://github.com/papersPapers/BadPrompt.",https://github.com/papersPapers/BadPrompt,-1
03b6e914-5ce2-42ff-8b5b-677c83f16bbc,Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP),0.983459,"Contrastively trained language-image models such as CLIP, ALIGN, and BASIC
have demonstrated unprecedented robustness to multiple challenging natural
distribution shifts. Since these language-image models differ from previous
training approaches in several ways, an important question is what causes the
large robustness gains. We answer this question via a systematic experimental
investigation. Concretely, we study five different possible causes for the
robustness gains: (i) the training set size, (ii) the training distribution,
(iii) language supervision at training time, (iv) language supervision at test
time, and (v) the contrastive loss function. Our experiments show that the more
diverse training distribution is the main cause for the robustness gains, with
the other factors contributing little to no robustness. Beyond our experimental
results, we also introduce ImageNet-Captions, a version of ImageNet with
original text annotations from Flickr, to enable further controlled experiments
of language-image training.",None,-1
14dd4ad4-0719-4a40-8368-4e1e186119af,User Experience Design for Automatic Credibility Assessment of News Content About COVID-19,0.114874,"The increasingly rapid spread of information about COVID-19 on the web calls
for automatic measures of quality assurance. In that context, we check the
credibility of news content using selected linguistic features. We present two
empirical studies to evaluate the usability of graphical interfaces that offer
such credibility assessment. In a moderated qualitative interview with six
participants, we identify rating scale, sub-criteria and algorithm authorship
as important predictors of the usability. A subsequent quantitative online
survey with 50 participants reveals a conflict between transparency and
conciseness in the interface design, as well as a perceived hierarchy of
metadata: the authorship of a news text is more important than the authorship
of the credibility algorithm used to assess the content quality. Finally, we
make suggestions for future research, such as proactively documenting
credibility-related metadata for Natural Language Processing and Language
Technology services and establishing an explicit hierarchical taxonomy of
usability predictors for automatic credibility assessment.",https://github.com/konstantinschulz/credible-covid-ux,-1
92f9aab8-33e7-4a22-8600-aad59ef06a70,Aligned with Whom? Direct and social goals for AI systems,0.105199,"As artificial intelligence (AI) becomes more powerful and widespread, the AI
alignment problem - how to ensure that AI systems pursue the goals that we want
them to pursue - has garnered growing attention. This article distinguishes two
types of alignment problems depending on whose goals we consider, and analyzes
the different solutions necessitated by each. The direct alignment problem
considers whether an AI system accomplishes the goals of the entity operating
it. In contrast, the social alignment problem considers the effects of an AI
system on larger groups or on society more broadly. In particular, it also
considers whether the system imposes externalities on others. Whereas solutions
to the direct alignment problem center around more robust implementation,
social alignment problems typically arise because of conflicts between
individual and group-level goals, elevating the importance of AI governance to
mediate such conflicts. Addressing the social alignment problem requires both
enforcing existing norms on their developers and operators and designing new
norms that apply directly to AI systems.",None,-1
603efffb-2e0f-4961-99e6-3d75e1325362,PointSCNet: Point Cloud Structure and Correlation Learning Based on Space Filling Curve-Guided Sampling,0.422746,"Geometrical structures and the internal local region relationship, such as
symmetry, regular array, junction, etc., are essential for understanding a 3D
shape. This paper proposes a point cloud feature extraction network named
PointSCNet, to capture the geometrical structure information and local region
correlation information of a point cloud. The PointSCNet consists of three main
modules: the space-filling curve-guided sampling module, the information fusion
module, and the channel-spatial attention module. The space-filling
curve-guided sampling module uses Z-order curve coding to sample points that
contain geometrical correlation. The information fusion module uses a
correlation tensor and a set of skip connections to fuse the structure and
correlation information. The channel-spatial attention module enhances the
representation of key points and crucial feature channels to refine the
network. The proposed PointSCNet is evaluated on shape classification and part
segmentation tasks. The experimental results demonstrate that the PointSCNet
outperforms or is on par with state-of-the-art methods by learning the
structure and correlation of point clouds effectively.",https://github.com/Chenguoz/PointSCNet,-1
862b9d41-4b8c-4dbc-bb74-3eecbe864fab,CITRIS: Causal Identifiability from Temporal Intervened Sequences,0.804722,"Understanding the latent causal factors of a dynamical system from visual
observations is considered a crucial step towards agents reasoning in complex
environments. In this paper, we propose CITRIS, a variational autoencoder
framework that learns causal representations from temporal sequences of images
in which underlying causal factors have possibly been intervened upon. In
contrast to the recent literature, CITRIS exploits temporality and observing
intervention targets to identify scalar and multidimensional causal factors,
such as 3D rotation angles. Furthermore, by introducing a normalizing flow,
CITRIS can be easily extended to leverage and disentangle representations
obtained by already pretrained autoencoders. Extending previous results on
scalar causal factors, we prove identifiability in a more general setting, in
which only some components of a causal factor are affected by interventions. In
experiments on 3D rendered image sequences, CITRIS outperforms previous methods
on recovering the underlying causal variables. Moreover, using pretrained
autoencoders, CITRIS can even generalize to unseen instantiations of causal
factors, opening future research areas in sim-to-real generalization for causal
representation learning.",https://github.com/phlippe/CITRIS,-1
6682195f-dddd-4abe-8199-09192a81d097,Panoptic Segmentation using Synthetic and Real Data,0.0412218,"Being able to understand the relations between the user and the surrounding
environment is instrumental to assist users in a worksite. For instance,
understanding which objects a user is interacting with from images and video
collected through a wearable device can be useful to inform the worker on the
usage of specific objects in order to improve productivity and prevent
accidents. Despite modern vision systems can rely on advanced algorithms for
object detection, semantic and panoptic segmentation, these methods still
require large quantities of domain-specific labeled data, which can be
difficult to obtain in industrial scenarios. Motivated by this observation, we
propose a pipeline which allows to generate synthetic images from 3D models of
real environments and real objects. The generated images are automatically
labeled and hence effortless to obtain. Exploiting the proposed pipeline, we
generate a dataset comprising synthetic images automatically labeled for
panoptic segmentation. This set is complemented by a small number of manually
labeled real images for fine-tuning. Experiments show that the use of synthetic
images allows to drastically reduce the number of real images needed to obtain
reasonable panoptic segmentation performance.",https://github.com/facebookresearch/detectron2,-1
085504ff-656e-4c1b-b1e2-8529b4d0f40c,Font Representation Learning via Paired-glyph Matching,0.0480929,"Fonts can convey profound meanings of words in various forms of glyphs.
Without typography knowledge, manually selecting an appropriate font or
designing a new font is a tedious and painful task. To allow users to explore
vast font styles and create new font styles, font retrieval and font style
transfer methods have been proposed. These tasks increase the need for learning
high-quality font representations. Therefore, we propose a novel font
representation learning scheme to embed font styles into the latent space. For
the discriminative representation of a font from others, we propose a
paired-glyph matching-based font representation learning model that attracts
the representations of glyphs in the same font to one another, but pushes away
those of other fonts. Through evaluations on font retrieval with query glyphs
on new fonts, we show our font representation learning scheme achieves better
generalization performance than the existing font representation learning
techniques. Finally on the downstream font style transfer and generation tasks,
we confirm the benefits of transfer learning with the proposed method. The
source code is available at https://github.com/junhocho/paired-glyph-matching.",https://github.com/junhocho/paired-glyph-matching,645
93c6596c-730f-4b2e-af39-4db10a1107a7,Polyglot Prompt: Multilingual Multitask PrompTraining,0.31484,"This paper aims for a potential architectural improvement for multilingual
learning and asks: Can different tasks from different languages be modeled in a
monolithic framework, i.e. without any task/language-specific module? The
benefit of achieving this could open new doors for future multilingual
research, including allowing systems trained on low resources to be further
assisted by other languages as well as other tasks. We approach this goal by
developing a learning framework named Polyglot Prompting to exploit prompting
methods for learning a unified semantic space for different languages and tasks
with multilingual prompt engineering. We performed a comprehensive evaluation
of 6 tasks, namely topic classification, sentiment classification, named entity
recognition, question answering, natural language inference, and summarization,
covering 24 datasets and 49 languages. The experimental results demonstrated
the efficacy of multilingual multitask prompt-based learning and led to
inspiring observations. We also present an interpretable multilingual
evaluation methodology and show how the proposed framework, multilingual
multitask prompt training, works. We release all datasets prompted in the best
setting and code.",https://github.com/jinlanfu/Polyglot_Prompt,-1
a36eafd6-dede-40d3-a197-f40163a582a7,Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation,0.521025,"Neural machine translation (NMT) has become the de-facto standard in
real-world machine translation applications. However, NMT models can
unpredictably produce severely pathological translations, known as
hallucinations, that seriously undermine user trust. It becomes thus crucial to
implement effective preventive strategies to guarantee their proper
functioning. In this paper, we address the problem of hallucination detection
in NMT by following a simple intuition: as hallucinations are detached from the
source content, they exhibit encoder-decoder attention patterns that are
statistically different from those of good quality translations. We frame this
problem with an optimal transport formulation and propose a fully unsupervised,
plug-in detector that can be used with any attention-based NMT model.
Experimental results show that our detector not only outperforms all previous
model-based detectors, but is also competitive with detectors that employ large
models trained on millions of samples.",https://github.com/deep-spin/,-1
bc48f9a2-c6d7-4551-9116-19a118ad9532,Multi-View Document Representation Learning for Open-Domain Dense Retrieval,0.783766,"Dense retrieval has achieved impressive advances in first-stage retrieval
from a large-scale document collection, which is built on bi-encoder
architecture to produce single vector representation of query and document.
However, a document can usually answer multiple potential queries from
different views. So the single vector representation of a document is hard to
match with multi-view queries, and faces a semantic mismatch problem. This
paper proposes a multi-view document representation learning framework, aiming
to produce multi-view embeddings to represent documents and enforce them to
align with different queries. First, we propose a simple yet effective method
of generating multiple embeddings through viewers. Second, to prevent
multi-view embeddings from collapsing to the same one, we further propose a
global-local loss with annealed temperature to encourage the multiple viewers
to better align with different potential queries. Experiments show our method
outperforms recent works and achieves state-of-the-art results.",None,-1
bb1d6353-0682-49a8-bc3d-bd6ecc831d09,TimeLMs: Diachronic Language Models from Twitter,0.849355,"Despite its importance, the time variable has been largely neglected in the
NLP and language model literature. In this paper, we present TimeLMs, a set of
language models specialized on diachronic Twitter data. We show that a
continual learning strategy contributes to enhancing Twitter-based language
models' capacity to deal with future and out-of-distribution tweets, while
making them competitive with standardized and more monolithic benchmarks. We
also perform a number of qualitative analyses showing how they cope with trends
and peaks in activity involving specific named entities or concept drift.",https://github.com/cardiffnlp/timelms,-1
4a55587e-8397-44cd-aabb-c88824a43724,pymdp: A Python library for active inference in discrete state spaces,0.480415,"Active inference is an account of cognition and behavior in complex systems
which brings together action, perception, and learning under the theoretical
mantle of Bayesian inference. Active inference has seen growing applications in
academic research, especially in fields that seek to model human or animal
behavior. While in recent years, some of the code arising from the active
inference literature has been written in open source languages like Python and
Julia, to-date, the most popular software for simulating active inference
agents is the DEM toolbox of SPM, a MATLAB library originally developed for the
statistical analysis and modelling of neuroimaging data. Increasing interest in
active inference, manifested both in terms of sheer number as well as
diversifying applications across scientific disciplines, has thus created a
need for generic, widely-available, and user-friendly code for simulating
active inference in open-source scientific computing languages like Python. The
Python package we present here, pymdp (see
https://github.com/infer-actively/pymdp), represents a significant step in this
direction: namely, we provide the first open-source package for simulating
active inference with partially-observable Markov Decision Processes or POMDPs.
We review the package's structure and explain its advantages like modular
design and customizability, while providing in-text code blocks along the way
to demonstrate how it can be used to build and run active inference processes
with ease. We developed pymdp to increase the accessibility and exposure of the
active inference framework to researchers, engineers, and developers with
diverse disciplinary backgrounds. In the spirit of open-source software, we
also hope that it spurs new innovation, development, and collaboration in the
growing active inference community.",https://pymdp-rtd.readthedocs.io/,-1
6d0edee9-44e4-4c7b-a6d6-5ae89fdc232a,Natural Language Syntax Complies with the Free-Energy Principle,0.225491,"Natural language syntax yields an unbounded array of hierarchically
structured expressions. We claim that these are used in the service of active
inference in accord with the free-energy principle (FEP). While conceptual
advances alongside modelling and simulation work have attempted to connect
speech segmentation and linguistic communication with the FEP, we extend this
program to the underlying computations responsible for generating syntactic
objects. We argue that recently proposed principles of economy in language
design - such as ""minimal search"" criteria from theoretical syntax - adhere to
the FEP. This affords a greater degree of explanatory power to the FEP - with
respect to higher language functions - and offers linguistics a grounding in
first principles with respect to computability. We show how both tree-geometric
depth and a Kolmogorov complexity estimate (recruiting a Lempel-Ziv compression
algorithm) can be used to accurately predict legal operations on syntactic
workspaces, directly in line with formulations of variational free energy
minimization. This is used to motivate a general principle of language design
that we term Turing-Chomsky Compression (TCC). We use TCC to align concerns of
linguists with the normative account of self-organization furnished by the FEP,
by marshalling evidence from theoretical linguistics and psycholinguistics to
ground core principles of efficient syntactic computation within active
inference.",None,-1
14a4291b-e57c-450d-bac1-c4f402359ecc,Geometric Features Informed Multi-person Human-object Interaction Recognition in Videos,0.327975,"Human-Object Interaction (HOI) recognition in videos is important for
analyzing human activity. Most existing work focusing on visual features
usually suffer from occlusion in the real-world scenarios. Such a problem will
be further complicated when multiple people and objects are involved in HOIs.
Consider that geometric features such as human pose and object position provide
meaningful information to understand HOIs, we argue to combine the benefits of
both visual and geometric features in HOI recognition, and propose a novel
Two-level Geometric feature-informed Graph Convolutional Network (2G-GCN). The
geometric-level graph models the interdependency between geometric features of
humans and objects, while the fusion-level graph further fuses them with visual
features of humans and objects. To demonstrate the novelty and effectiveness of
our method in challenging scenarios, we propose a new multi-person HOI dataset
(MPHOI-72). Extensive experiments on MPHOI-72 (multi-person HOI), CAD-120
(single-human HOI) and Bimanual Actions (two-hand HOI) datasets demonstrate our
superior performance compared to state-of-the-arts.",https://github.com/tanqiu98/2G-GCN,6272
0cb4bf0e-726a-4a1d-ba6e-8de05300ea91,DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing,0.327003,"In the field of representation learning on knowledge graphs (KGs), a
hyper-relational fact consists of a main triple and several auxiliary
attribute-value descriptions, which is considered more comprehensive and
specific than a triple-based fact. However, currently available
hyper-relational KG embedding methods in a single view are limited in
application because they weaken the hierarchical structure that represents the
affiliation between entities. To overcome this limitation, we propose a
dual-view hyper-relational KG structure (DH-KG) that contains a
hyper-relational instance view for entities and a hyper-relational ontology
view for concepts that are abstracted hierarchically from the entities. This
paper defines link prediction and entity typing tasks on DH-KG for the first
time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and
HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding
model based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms
baseline models on DH-KG, according to experimental results. Finally, we
provide an example of how this technology can be used to treat hypertension.
Our model and new datasets are publicly available.",https://github.com/LHRLAB/DHGE,1107
3d6f4750-1f41-46ba-8b9f-aad3c2e068e7,CEU-Net: Ensemble Semantic Segmentation of Hyperspectral Images Using Clustering,0.207823,"Most semantic segmentation approaches of Hyperspectral images (HSIs) use and
require preprocessing steps in the form of patching to accurately classify
diversified land cover in remotely sensed images. These approaches use patching
to incorporate the rich neighborhood information in images and exploit the
simplicity and segmentability of the most common HSI datasets. In contrast,
most landmasses in the world consist of overlapping and diffused classes,
making neighborhood information weaker than what is seen in common HSI
datasets. To combat this issue and generalize the segmentation models to more
complex and diverse HSI datasets, in this work, we propose our novel flagship
model: Clustering Ensemble U-Net (CEU-Net). CEU-Net uses the ensemble method to
combine spectral information extracted from convolutional neural network (CNN)
training on a cluster of landscape pixels. Our CEU-Net model outperforms
existing state-of-the-art HSI semantic segmentation methods and gets
competitive performance with and without patching when compared to baseline
models. We highlight CEU-Net's high performance across Botswana, KSC, and
Salinas datasets compared to HybridSN and AeroRIT methods.",None,-1
2909748d-765e-4e4d-9018-bc63a46bc9b7,StyLandGAN: A StyleGAN based Landscape Image Synthesis using Depth-map,0.0238567,"Despite recent success in conditional image synthesis, prevalent input
conditions such as semantics and edges are not clear enough to express `Linear
(Ridges)' and `Planar (Scale)' representations. To address this problem, we
propose a novel framework StyLandGAN, which synthesizes desired landscape
images using a depth map which has higher expressive power. Our StyleLandGAN is
extended from the unconditional generation model to accept input conditions. We
also propose a '2-phase inference' pipeline which generates diverse depth maps
and shifts local parts so that it can easily reflect user's intend. As a
comparison, we modified the existing semantic image synthesis models to accept
a depth map as well. Experimental results show that our method is superior to
existing methods in quality, diversity, and depth-accuracy.",None,966
975f6083-99fe-4377-a00c-2715a6d93f72,Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation,0.928404,"Despite recent advances in detecting fake news generated by neural models,
their results are not readily applicable to effective detection of
human-written disinformation. What limits the successful transfer between them
is the sizable gap between machine-generated fake news and human-authored ones,
including the notable differences in terms of style and underlying intent. With
this in mind, we propose a novel framework for generating training examples
that are informed by the known styles and strategies of human-authored
propaganda. Specifically, we perform self-critical sequence training guided by
natural language inference to ensure the validity of the generated articles,
while also incorporating propaganda techniques, such as appeal to authority and
loaded language. In particular, we create a new training dataset, PropaNews,
with 2,256 examples, which we release for future use. Our experimental results
show that fake news detectors trained on PropaNews are better at detecting
human-written disinformation by 3.62 - 7.69% F1 score on two public datasets.",https://github.com/khuangaf/FakingFakeNews,-1
f7b6183e-bf03-4f72-bcd5-5c6491391adc,NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression Recognition,0.958367,"Dynamic facial expression recognition (DFER) in the wild is an extremely
challenging task, due to a large number of noisy frames in the video sequences.
Previous works focus on extracting more discriminative features, but ignore
distinguishing the key frames from the noisy frames. To tackle this problem, we
propose a noise-robust dynamic facial expression recognition network
(NR-DFERNet), which can effectively reduce the interference of noisy frames on
the DFER task. Specifically, at the spatial stage, we devise a dynamic-static
fusion module (DSF) that introduces dynamic features to static features for
learning more discriminative spatial features. To suppress the impact of target
irrelevant frames, we introduce a novel dynamic class token (DCT) for the
transformer at the temporal stage. Moreover, we design a snippet-based filter
(SF) at the decision stage to reduce the effect of too many neutral frames on
non-neutral sequence classification. Extensive experimental results demonstrate
that our NR-DFERNet outperforms the state-of-the-art methods on both the DFEW
and AFEW benchmarks.",None,-1
ae7a01f0-9bc7-4997-b9a2-c476460f3406,Learning Joint Representation of Human Motion and Language,0.0890966,"In this work, we present MoLang (a Motion-Language connecting model) for
learning joint representation of human motion and language, leveraging both
unpaired and paired datasets of motion and language modalities. To this end, we
propose a motion-language model with contrastive learning, empowering our model
to learn better generalizable representations of the human motion domain.
Empirical results show that our model learns strong representations of human
motion data through navigating language modality. Our proposed method is able
to perform both action recognition and motion retrieval tasks with a single
model where it outperforms state-of-the-art approaches on a number of action
recognition benchmarks.",None,-1
95b8d517-5984-462b-b196-01c1a1c68614,Semantic optical fiber communication system,0.572202,"The current optical communication systems minimize bit or symbol errors
without considering the semantic meaning behind digital bits, thus transmitting
a lot of unnecessary information. We propose and experimentally demonstrate a
semantic optical fiber communication (SOFC) system. Instead of encoding
information into bits for transmission, semantic information is extracted from
the source using deep learning. The generated semantic symbols are then
directly transmitted through an optical fiber. Compared with the bit-based
structure, the SOFC system achieved higher information compression and a more
stable performance, especially in the low received optical power regime, and
enhanced the robustness against optical link impairments. This work introduces
an intelligent optical communication system at the human analytical thinking
level, which is a significant step toward a breakthrough in the current optical
communication architecture.",None,-1
46dd9a15-5e88-4613-9975-614c4e650807,Backdoor Attack is a Devil in Federated GAN-based Medical Image Synthesis,0.564873,"Deep Learning-based image synthesis techniques have been applied in
healthcare research for generating medical images to support open research.
Training generative adversarial neural networks (GAN) usually requires large
amounts of training data. Federated learning (FL) provides a way of training a
central model using distributed data from different medical institutions while
keeping raw data locally. However, FL is vulnerable to backdoor attack, an
adversarial by poisoning training data, given the central server cannot access
the original data directly. Most backdoor attack strategies focus on
classification models and centralized domains. In this study, we propose a way
of attacking federated GAN (FedGAN) by treating the discriminator with a
commonly used data poisoning strategy in backdoor attack classification models.
We demonstrate that adding a small trigger with size less than 0.5 percent of
the original image size can corrupt the FL-GAN model. Based on the proposed
attack, we provide two effective defense strategies: global malicious detection
and local training regularization. We show that combining the two defense
strategies yields a robust medical image generation.",None,-1
153edd75-c37d-4a58-b83c-a5da58c01e93,A Multibranch Convolutional Neural Network for Hyperspectral Unmixing,0.632268,"Hyperspectral unmixing remains one of the most challenging tasks in the
analysis of such data. Deep learning has been blooming in the field and proved
to outperform other classic unmixing techniques, and can be effectively
deployed onboard Earth observation satellites equipped with hyperspectral
imagers. In this letter, we follow this research pathway and propose a
multi-branch convolutional neural network that benefits from fusing spectral,
spatial, and spectral-spatial features in the unmixing process. The results of
our experiments, backed up with the ablation study, revealed that our
techniques outperform others from the literature and lead to higher-quality
fractional abundance estimation. Also, we investigated the influence of
reducing the training sets on the capabilities of all algorithms and their
robustness against noise, as capturing large and representative ground-truth
sets is time-consuming and costly in practice, especially in emerging Earth
observation scenarios.",https://gitlab.com/jnalepa/mbhuto,-1
f00e13a3-54e1-44be-9d29-32b124063cdf,BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing,0.999006,"Training and evaluating language models increasingly requires the
construction of meta-datasets --diverse collections of curated data with clear
provenance. Natural language prompting has recently lead to improved zero-shot
generalization by transforming existing, supervised datasets into a diversity
of novel pretraining tasks, highlighting the benefits of meta-dataset curation.
While successful in general-domain text, translating these data-centric
approaches to biomedical language modeling remains challenging, as labeled
biomedical datasets are significantly underrepresented in popular data hubs. To
address this challenge, we introduce BigBIO a community library of 126+
biomedical NLP datasets, currently covering 12 task categories and 10+
languages. BigBIO facilitates reproducible meta-dataset curation via
programmatic access to datasets and their metadata, and is compatible with
current platforms for prompt engineering and end-to-end few/zero shot language
model evaluation. We discuss our process for task schema harmonization, data
auditing, contribution guidelines, and outline two illustrative use cases:
zero-shot evaluation of biomedical prompts and large-scale, multi-task
learning. BigBIO is an ongoing community effort and is available at
https://github.com/bigscience-workshop/biomedical",https://github.com/bigscience-workshop/biomedical,-1
4c95bfd6-83ed-4297-8cf5-b37a688e6c50,Real3D-Aug: Point Cloud Augmentation by Placing Real Objects with Occlusion Handling for 3D Detection and Segmentation,0.133501,"Object detection and semantic segmentation with the 3D lidar point cloud data
require expensive annotation. We propose a data augmentation method that takes
advantage of already annotated data multiple times. We propose an augmentation
framework that reuses real data, automatically finds suitable placements in the
scene to be augmented, and handles occlusions explicitly. Due to the usage of
the real data, the scan points of newly inserted objects in augmentation
sustain the physical characteristics of the lidar, such as intensity and
raydrop. The pipeline proves competitive in training top-performing models for
3D object detection and semantic segmentation. The new augmentation provides a
significant performance gain in rare and essential classes, notably 6.65%
average precision gain for ""Hard"" pedestrian class in KITTI object detection or
2.14 mean IoU gain in the SemanticKITTI segmentation challenge over the state
of the art.",https://github.com/ctu-vras/,-1
65f33aa7-5cfb-48a6-baf5-7afbb7eeea11,Molecular Substructure-Aware Network for Drug-Drug Interaction Prediction,0.609175,"Concomitant administration of drugs can cause drug-drug interactions (DDIs).
Some drug combinations are beneficial, but other ones may cause negative
effects which are previously unrecorded. Previous works on DDI prediction
usually rely on hand-engineered domain knowledge, which is laborious to obtain.
In this work, we propose a novel model, Molecular Substructure-Aware Network
(MSAN), to effectively predict potential DDIs from molecular structures of drug
pairs. We adopt a Transformer-like substructure extraction module to acquire a
fixed number of representative vectors that are associated with various
substructure patterns of the drug molecule. Then, interaction strength between
the two drugs' substructures will be captured by a similarity-based interaction
module. We also perform a substructure dropping augmentation before graph
encoding to alleviate overfitting. Experimental results from a real-world
dataset reveal that our proposed model achieves the state-of-the-art
performance. We also show that the predictions of our model are highly
interpretable through a case study.",https://github.com/Hienyriux/MSAN,-1
09654c07-794b-44d7-b576-ba3c3f6ef34c,Data Augmentation by Selecting Mixed Classes Considering Distance Between Classes,0.0982838,"Data augmentation is an essential technique for improving recognition
accuracy in object recognition using deep learning. Methods that generate mixed
data from multiple data sets, such as mixup, can acquire new diversity that is
not included in the training data, and thus contribute significantly to
accuracy improvement. However, since the data selected for mixing are randomly
sampled throughout the training process, there are cases where appropriate
classes or data are not selected. In this study, we propose a data augmentation
method that calculates the distance between classes based on class
probabilities and can select data from suitable classes to be mixed in the
training process. Mixture data is dynamically adjusted according to the
training trend of each class to facilitate training. The proposed method is
applied in combination with conventional methods for generating mixed data.
Evaluation experiments show that the proposed method improves recognition
performance on general and long-tailed image recognition datasets.",None,9497
c59930cf-8a0d-437f-8066-a68598bbfc4b,Gradient-based Uncertainty for Monocular Depth Estimation,0.808156,"In monocular depth estimation, disturbances in the image context, like moving
objects or reflecting materials, can easily lead to erroneous predictions. For
that reason, uncertainty estimates for each pixel are necessary, in particular
for safety-critical applications such as automated driving. We propose a post
hoc uncertainty estimation approach for an already trained and thus fixed depth
estimation model, represented by a deep neural network. The uncertainty is
estimated with the gradients which are extracted with an auxiliary loss
function. To avoid relying on ground-truth information for the loss definition,
we present an auxiliary loss function based on the correspondence of the depth
prediction for an image and its horizontally flipped counterpart. Our approach
achieves state-of-the-art uncertainty estimation results on the KITTI and NYU
Depth V2 benchmarks without the need to retrain the neural network. Models and
code are publicly available at https://github.com/jhornauer/GrUMoDepth.",https://github.com/jhornauer/GrUMoDepth,-1
0ea03d3a-43d1-4906-8fae-b5468a31b3da,A knowledge graph representation learning approach to predict novel kinase-substrate interactions,0.479691,"The human proteome contains a vast network of interacting kinases and
substrates. Even though some kinases have proven to be immensely useful as
therapeutic targets, a majority are still understudied. In this work, we
present a novel knowledge graph representation learning approach to predict
novel interaction partners for understudied kinases. Our approach uses a
phosphoproteomic knowledge graph constructed by integrating data from iPTMnet,
Protein Ontology, Gene Ontology and BioKG. The representation of kinases and
substrates in this knowledge graph are learned by performing directed random
walks on triples coupled with a modified SkipGram or CBOW model. These
representations are then used as an input to a supervised classification model
to predict novel interactions for understudied kinases. We also present a
post-predictive analysis of the predicted interactions and an ablation study of
the phosphoproteomic knowledge graph to gain an insight into the biology of the
understudied kinases.",https://github.com/udel-cbcb/ikg_v2_public.git,-1
3c24eff8-4b91-47a7-ac69-e6489f3f68b4,Multielement polynomial chaos Kriging-based metamodelling for Bayesian inference of non-smooth systems,0.695532,"This paper presents a surrogate modelling technique based on domain
partitioning for Bayesian parameter inference of highly nonlinear engineering
models. In order to alleviate the computational burden typically involved in
Bayesian inference applications, a multielement Polynomial Chaos Expansion
based Kriging metamodel is proposed. The developed surrogate model combines in
a piecewise function an array of local Polynomial Chaos based Kriging
metamodels constructed on a finite set of non-overlapping subdomains of the
stochastic input space. Therewith, the presence of non-smoothness in the
response of the forward model (e.g.~ nonlinearities and sparseness) can be
reproduced by the proposed metamodel with minimum computational costs owing to
its local adaptation capabilities. The model parameter inference is conducted
through a Markov chain Monte Carlo approach comprising adaptive exploration and
delayed rejection. The efficiency and accuracy of the proposed approach are
validated through two case studies, including an analytical benchmark and a
numerical case study. The latter relates the partial differential equation
governing the hydrogen diffusion phenomenon of metallic materials in Thermal
Desorption Spectroscopy tests.",None,-1
d4164fb9-a4a3-4419-8e4a-9df84e7a4c9f,Uncertainty-aware Personal Assistant for Making Personalized Privacy Decisions,0.683977,"Many software systems, such as online social networks enable users to share
information about themselves. While the action of sharing is simple, it
requires an elaborate thought process on privacy: what to share, with whom to
share, and for what purposes. Thinking about these for each piece of content to
be shared is tedious. Recent approaches to tackle this problem build personal
assistants that can help users by learning what is private over time and
recommending privacy labels such as private or public to individual content
that a user considers sharing. However, privacy is inherently ambiguous and
highly personal. Existing approaches to recommend privacy decisions do not
address these aspects of privacy sufficiently. Ideally, a personal assistant
should be able to adjust its recommendation based on a given user, considering
that user's privacy understanding. Moreover, the personal assistant should be
able to assess when its recommendation would be uncertain and let the user make
the decision on her own. Accordingly, this paper proposes a personal assistant
that uses evidential deep learning to classify content based on its privacy
label. An important characteristic of the personal assistant is that it can
model its uncertainty in its decisions explicitly, determine that it does not
know the answer, and delegate from making a recommendation when its uncertainty
is high. By factoring in the user's own understanding of privacy, such as risk
factors or own labels, the personal assistant can personalize its
recommendations per user. We evaluate our proposed personal assistant using a
well-known data set. Our results show that our personal assistant can
accurately identify uncertain cases, personalize them to its user's needs, and
thus helps users preserve their privacy well.",None,-1
7fcb9221-1f09-468c-a7d6-6a2c5dfd9c18,Stitch it in Time: GAN-Based Facial Editing of Real Videos,0.998332,"The ability of Generative Adversarial Networks to encode rich semantics
within their latent space has been widely adopted for facial image editing.
However, replicating their success with videos has proven challenging. Sets of
high-quality facial videos are lacking, and working with videos introduces a
fundamental barrier to overcome - temporal coherency. We propose that this
barrier is largely artificial. The source video is already temporally coherent,
and deviations from this state arise in part due to careless treatment of
individual components in the editing pipeline. We leverage the natural
alignment of StyleGAN and the tendency of neural networks to learn low
frequency functions, and demonstrate that they provide a strongly consistent
prior. We draw on these insights and propose a framework for semantic editing
of faces in videos, demonstrating significant improvements over the current
state-of-the-art. Our method produces meaningful face manipulations, maintains
a higher degree of temporal consistency, and can be applied to challenging,
high quality, talking head videos which current methods struggle with.",https://stitch-time.github.io/,-1
a2a68e70-58f5-4c9c-aee2-bf1a9997b0e9,Hardware-agnostic Computation for Large-scale Knowledge Graph Embeddings,0.287943,"Knowledge graph embedding research has mainly focused on learning continuous
representations of knowledge graphs towards the link prediction problem.
Recently developed frameworks can be effectively applied in research related
applications. Yet, these frameworks do not fulfill many requirements of
real-world applications. As the size of the knowledge graph grows, moving
computation from a commodity computer to a cluster of computers in these
frameworks becomes more challenging. Finding suitable hyperparameter settings
w.r.t. time and computational budgets are left to practitioners. In addition,
the continual learning aspect in knowledge graph embedding frameworks is often
ignored, although continual learning plays an important role in many real-world
(deep) learning-driven applications. Arguably, these limitations explain the
lack of publicly available knowledge graph embedding models for large knowledge
graphs. We developed a framework based on the frameworks DASK, Pytorch
Lightning and Hugging Face to compute embeddings for large-scale knowledge
graphs in a hardware-agnostic manner, which is able to address real-world
challenges pertaining to the scale of real application. We provide an
open-source version of our framework along with a hub of pre-trained models
having more than 11.4 B parameters.",https://github.com/dice-group/dice-embeddings,-1
85edecd9-c494-46b8-a01f-de717dda9fc2,Learning-Assisted Algorithm Unrolling for Online Optimization with Budget Constraints,0.102922,"Online optimization with multiple budget constraints is challenging since the
online decisions over a short time horizon are coupled together by strict
inventory constraints. The existing manually-designed algorithms cannot achieve
satisfactory average performance for this setting because they often need a
large number of time steps for convergence and/or may violate the inventory
constraints. In this paper, we propose a new machine learning (ML) assisted
unrolling approach, called LAAU (Learning-Assisted Algorithm Unrolling), which
unrolls the online decision pipeline and leverages an ML model for updating the
Lagrangian multiplier online. For efficient training via backpropagation, we
derive gradients of the decision pipeline over time. We also provide the
average cost bounds for two cases when training data is available offline and
collected online, respectively. Finally, we present numerical results to
highlight that LAAU can outperform the existing baselines.",None,-1
33c2bdb9-08ad-4613-b46c-a37812cac3c2,Improving Sentiment Analysis By Emotion Lexicon Approach on Vietnamese Texts,0.304787,"The sentiment analysis task has various applications in practice. In the
sentiment analysis task, words and phrases that represent positive and negative
emotions are important. Finding out the words that represent the emotion from
the text can improve the performance of the classification models for the
sentiment analysis task. In this paper, we propose a methodology that combines
the emotion lexicon with the classification model to enhance the accuracy of
the models. Our experimental results show that the emotion lexicon combined
with the classification model improves the performance of models.",None,-1
011ab30b-d59d-4eac-b8fe-117d127a4fe8,Data augmentation for efficient learning from parametric experts,0.0731654,"We present a simple, yet powerful data-augmentation technique to enable
data-efficient learning from parametric experts for reinforcement and imitation
learning. We focus on what we call the policy cloning setting, in which we use
online or offline queries of an expert or expert policy to inform the behavior
of a student policy. This setting arises naturally in a number of problems, for
instance as variants of behavior cloning, or as a component of other algorithms
such as DAGGER, policy distillation or KL-regularized RL. Our approach,
augmented policy cloning (APC), uses synthetic states to induce
feedback-sensitivity in a region around sampled trajectories, thus dramatically
reducing the environment interactions required for successful cloning of the
expert. We achieve highly data-efficient transfer of behavior from an expert to
a student policy for high-degrees-of-freedom control problems. We demonstrate
the benefit of our method in the context of several existing and widely used
algorithms that include policy cloning as a constituent part. Moreover, we
highlight the benefits of our approach in two practically relevant settings (a)
expert compression, i.e. transfer to a student with fewer parameters; and (b)
transfer from privileged experts, i.e. where the expert has a different
observation space than the student, usually including access to privileged
information.",None,49708
a5f4e0cb-8799-4fa1-ad27-5df78c050706,A Question-Answer Driven Approach to Reveal Affirmative Interpretations from Verbal Negations,0.0801253,"This paper explores a question-answer driven approach to reveal affirmative
interpretations from verbal negations (i.e., when a negation cue grammatically
modifies a verb). We create a new corpus consisting of 4,472 verbal negations
and discover that 67.1% of them convey that an event actually occurred.
Annotators generate and answer 7,277 questions for the 3,001 negations that
convey an affirmative interpretation. We first cast the problem of revealing
affirmative interpretations from negations as a natural language inference
(NLI) classification task. Experimental results show that state-of-the-art
transformers trained with existing NLI corpora are insufficient to reveal
affirmative interpretations. We also observe, however, that fine-tuning brings
small improvements. In addition to NLI classification, we also explore the more
realistic task of generating affirmative interpretations directly from
negations with the T5 transformer. We conclude that the generation task remains
a challenge as T5 substantially underperforms humans.",https://github.com/mosharafhossain/AFIN,-1
48421967-1e08-4b79-ad8c-0b215eabf889,SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model,0.677774,"Data-driven speech processing models usually perform well with a large amount
of text supervision, but collecting transcribed speech data is costly.
Therefore, we propose SpeechCLIP, a novel framework bridging speech and text
through images to enhance speech models without transcriptions. We leverage
state-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images
and spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior
state-of-the-art on image-speech retrieval and performs zero-shot speech-text
retrieval without direct supervision from transcriptions. Moreover, SpeechCLIP
can directly retrieve semantically related keywords from speech.",https://github.com/atosystem/SpeechCLIP,-1
793e9535-7750-4e48-8bc6-4f39f989112d,Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective,0.303055,"Unsupervised video domain adaptation is a practical yet challenging task. In
this work, for the first time, we tackle it from a disentanglement view. Our
key idea is to handle the spatial and temporal domain divergence separately
through disentanglement. Specifically, we consider the generation of
cross-domain videos from two sets of latent factors, one encoding the static
information and another encoding the dynamic information. A Transfer Sequential
VAE (TranSVAE) framework is then developed to model such generation. To better
serve for adaptation, we propose several objectives to constrain the latent
factors. With these constraints, the spatial divergence can be readily removed
by disentangling the static domain-specific information out, and the temporal
divergence is further reduced from both frame- and video-levels through
adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and
Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE
compared with several state-of-the-art approaches. Code is publicly available.",https://github.com/ldkong1205/TranSVAE,-1
941523a7-3a30-4d52-b4e1-eb42d6b043b0,Tuning Synaptic Connections instead of Weights by Genetic Algorithm in Spiking Policy Network,0.350677,"Learning from the interaction is the primary way biological agents know about
the environment and themselves. Modern deep reinforcement learning (DRL)
explores a computational approach to learning from interaction and has
significantly progressed in solving various tasks. However, the powerful DRL is
still far from biological agents in energy efficiency. Although the underlying
mechanisms are not fully understood, we believe that the integration of spiking
communication between neurons and biologically-plausible synaptic plasticity
plays a prominent role. Following this biological intuition, we optimize a
spiking policy network (SPN) by a genetic algorithm as an energy-efficient
alternative to DRL. Our SPN mimics the sensorimotor neuron pathway of insects
and communicates through event-based spikes. Inspired by biological research
that the brain forms memories by forming new synaptic connections and rewires
these connections based on new experiences, we tune the synaptic connections
instead of weights in SPN to solve given tasks. Experimental results on several
robotic control tasks show that our method can achieve the performance level of
mainstream DRL methods and exhibit significantly higher energy efficiency.",https://github.com/BladeDancer957/SPN-GA,-1
2162b3b6-d1fc-4a86-87ab-fdb3fa111d70,Nested Named Entity Recognition as Latent Lexicalized Constituency Parsing,0.977522,"Nested named entity recognition (NER) has been receiving increasing
attention. Recently, (Fu et al, 2021) adapt a span-based constituency parser to
tackle nested NER. They treat nested entities as partially-observed
constituency trees and propose the masked inside algorithm for partial
marginalization. However, their method cannot leverage entity heads, which have
been shown useful in entity mention detection and entity typing. In this work,
we resort to more expressive structures, lexicalized constituency trees in
which constituents are annotated by headwords, to model nested entities. We
leverage the Eisner-Satta algorithm to perform partial marginalization and
inference efficiently. In addition, we propose to use (1) a two-stage strategy
(2) a head regularization loss and (3) a head-aware labeling loss in order to
enhance the performance. We make a thorough ablation study to investigate the
functionality of each component. Experimentally, our method achieves the
state-of-the-art performance on ACE2004, ACE2005 and NNE, and competitive
performance on GENIA, and meanwhile has a fast inference speed.",https://github.com/LouChao98/nner_as_parsing,-1
f5eeed21-818b-4443-a55e-f1c4fc89509b,Speeding Up Action Recognition Using Dynamic Accumulation of Residuals in Compressed Domain,0.182805,"With the widespread use of installed cameras, video-based monitoring
approaches have seized considerable attention for different purposes like
assisted living. Temporal redundancy and the sheer size of raw videos are the
two most common problematic issues related to video processing algorithms. Most
of the existing methods mainly focused on increasing accuracy by exploring
consecutive frames, which is laborious and cannot be considered for real-time
applications. Since videos are mostly stored and transmitted in compressed
format, these kinds of videos are available on many devices. Compressed videos
contain a multitude of beneficial information, such as motion vectors and
quantized coefficients. Proper use of this available information can greatly
improve the video understanding methods' performance. This paper presents an
approach for using residual data, available in compressed videos directly,
which can be obtained by a light partially decoding procedure. In addition, a
method for accumulating similar residuals is proposed, which dramatically
reduces the number of processed frames for action recognition. Applying neural
networks exclusively for accumulated residuals in the compressed domain
accelerates performance, while the classification results are highly
competitive with raw video approaches.",None,-1
a5368b10-d909-4ed3-bfd4-8e6bd2ae9661,Text2LIVE: Text-Driven Layered Image and Video Editing,0.999996,"We present a method for zero-shot, text-driven appearance manipulation in
natural images and videos. Given an input image or video and a target text
prompt, our goal is to edit the appearance of existing objects (e.g., object's
texture) or augment the scene with visual effects (e.g., smoke, fire) in a
semantically meaningful manner. We train a generator using an internal dataset
of training examples, extracted from a single input (image or video and target
text prompt), while leveraging an external pre-trained CLIP model to establish
our losses. Rather than directly generating the edited output, our key idea is
to generate an edit layer (color+opacity) that is composited over the original
input. This allows us to constrain the generation process and maintain high
fidelity to the original input via novel text-driven losses that are applied
directly to the edit layer. Our method neither relies on a pre-trained
generator nor requires user-provided edit masks. We demonstrate localized,
semantic edits on high-resolution natural images and videos across a variety of
objects and scenes.",None,-1
4ca56bd8-897e-4f65-894e-a6750052ead1,CliMedBERT: A Pre-trained Language Model for Climate and Health-related Text,0.77687,"Climate change is threatening human health in unprecedented orders and many
ways. These threats are expected to grow unless effective and evidence-based
policies are developed and acted upon to minimize or eliminate them. Attaining
such a task requires the highest degree of the flow of knowledge from science
into policy. The multidisciplinary, location-specific, and vastness of
published science makes it challenging to keep track of novel work in this
area, as well as making the traditional knowledge synthesis methods inefficient
in infusing science into policy. To this end, we consider developing multiple
domain-specific language models (LMs) with different variations from Climate-
and Health-related information, which can serve as a foundational step toward
capturing available knowledge to enable solving different tasks, such as
detecting similarities between climate- and health-related concepts,
fact-checking, relation extraction, evidence of health effects to policy text
generation, and more. To our knowledge, this is the first work that proposes
developing multiple domain-specific language models for the considered domains.
We will make the developed models, resources, and codebase available for the
researchers.",None,-1
60fd210e-d2a6-4fc1-8511-8bb39ac457b0,Multimodal data matters: language model pre-training over structured and unstructured electronic health records,0.473879,"As two important textual modalities in electronic health records (EHR), both
structured data (clinical codes) and unstructured data (clinical narratives)
have recently been increasingly applied to the healthcare domain. Most existing
EHR-oriented studies, however, either focus on a particular modality or
integrate data from different modalities in a straightforward manner, which
usually treats structured and unstructured data as two independent sources of
information about patient admission and ignore the intrinsic interactions
between them. In fact, the two modalities are documented during the same
encounter where structured data inform the documentation of unstructured data
and vice versa. In this paper, we proposed a Medical Multimodal Pre-trained
Language Model, named MedM-PLM, to learn enhanced EHR representations over
structured and unstructured data and explore the interaction of two modalities.
In MedM-PLM, two Transformer-based neural network components are firstly
adopted to learn representative characteristics from each modality. A
cross-modal module is then introduced to model their interactions. We
pre-trained MedM-PLM on the MIMIC-III dataset and verified the effectiveness of
the model on three downstream clinical tasks, i.e., medication recommendation,
30-day readmission prediction and ICD coding. Extensive experiments demonstrate
the power of MedM-PLM compared with state-of-the-art methods. Further analyses
and visualizations show the robustness of our model, which could potentially
provide more comprehensive interpretations for clinical decision-making.",https://git.openi.org.cn/liusc/3-6-li-usicen-multi-modal-pretrain,-1
12388b5f-ea2b-4eb6-b04e-0ce1cf3992f8,METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals,0.323764,"We present an efficient method of pretraining large-scale autoencoding
language models using training signals generated by an auxiliary model.
Originated in ELECTRA, this training strategy has demonstrated
sample-efficiency to pretrain models at the scale of hundreds of millions of
parameters. In this work, we conduct a comprehensive empirical study, and
propose a recipe, namely ""Model generated dEnoising TRaining Objective""
(METRO), which incorporates some of the best modeling techniques developed
recently to speed up, stabilize, and enhance pretrained language models without
compromising model effectiveness. The resultant models, METRO-LM, consisting of
up to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,
SuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in
that they often outperform previous large models with significantly smaller
model sizes and lower pretraining cost.",https://github.com/namisan/mt-dnn/tree/master/experiments/superglue,82331
b7a290e5-a5bc-429e-8703-c7d0f8325095,Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations,0.149377,"We suggest a simple practical method to combine the human and artificial
intelligence to both learn best investment practices of fund managers, and
provide recommendations to improve them. Our approach is based on a combination
of Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns
the intent of fund managers as suggested by their trading history, and recovers
their implied reward function. At the second step, this reward function is used
by a direct RL algorithm to optimize asset allocation decisions. We show that
our method is able to improve over the performance of individual fund managers.",None,-1
76cc1f31-a6db-4ee4-9d16-d653b91fc026,Mapless Navigation of a Hybrid Aerial Underwater Vehicle with Deep Reinforcement Learning Through Environmental Generalization,0.46597,"Previous works showed that Deep-RL can be applied to perform mapless
navigation, including the medium transition of Hybrid Unmanned Aerial
Underwater Vehicles (HUAUVs). This paper presents new approaches based on the
state-of-the-art actor-critic algorithms to address the navigation and medium
transition problems for a HUAUV. We show that a double critic Deep-RL with
Recurrent Neural Networks improves the navigation performance of HUAUVs using
solely range data and relative localization. Our Deep-RL approaches achieved
better navigation and transitioning capabilities with a solid generalization of
learning through distinct simulated scenarios, outperforming previous
approaches.",None,-1
2dbe8fd4-e6f9-4ff0-9790-687aad1590a1,Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of AI,0.726568,"As artificial intelligence becomes more powerful and a ubiquitous presence in
daily life, it is imperative to understand and manage the impact of AI systems
on our lives and decisions. Modern ML systems often change user behavior (e.g.
personalized recommender systems learn user preferences to deliver
recommendations that change online behavior). An externality of behavior change
is preference change. This article argues for the establishment of a
multidisciplinary endeavor focused on understanding how AI systems change
preference: Preference Science. We operationalize preference to incorporate
concepts from various disciplines, outlining the importance of meta-preferences
and preference-change preferences, and proposing a preliminary framework for
how preferences change. We draw a distinction between preference change,
permissible preference change, and outright preference manipulation. A
diversity of disciplines contribute unique insights to this framework.",None,2326
016ae676-d168-48c1-918b-8210cbdc35af,Controllable Dynamic Multi-Task Architectures,0.549198,"Multi-task learning commonly encounters competition for resources among
tasks, specifically when model capacity is limited. This challenge motivates
models which allow control over the relative importance of tasks and total
compute cost during inference time. In this work, we propose such a
controllable multi-task network that dynamically adjusts its architecture and
weights to match the desired task preference as well as the resource
constraints. In contrast to the existing dynamic multi-task approaches that
adjust only the weights within a fixed architecture, our approach affords the
flexibility to dynamically control the total computational cost and match the
user-preferred task importance better. We propose a disentangled training of
two hypernetworks, by exploiting task affinity and a novel branching
regularized loss, to take input preferences and accordingly predict
tree-structured models with adapted weights. Experiments on three multi-task
benchmarks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of
our approach. Project page is available at https://www.nec-labs.com/~mas/DYMU.",https://www.nec-labs.com/mas/DYMU,-1
7318d6cb-1209-4292-bf04-af963c2b9ee1,Annotating Norwegian Language Varieties on Twitter for Part-of-Speech,0.739761,"Norwegian Twitter data poses an interesting challenge for Natural Language
Processing (NLP) tasks. These texts are difficult for models trained on
standardized text in one of the two Norwegian written forms (Bokm{\aa}l and
Nynorsk), as they contain both the typical variation of social media text, as
well as a large amount of dialectal variety. In this paper we present a novel
Norwegian Twitter dataset annotated with POS-tags. We show that models trained
on Universal Dependency (UD) data perform worse when evaluated against this
dataset, and that models trained on Bokm{\aa}l generally perform better than
those trained on Nynorsk. We also see that performance on dialectal tweets is
comparable to the written standards for some models. Finally we perform a
detailed analysis of the errors that models commonly make on this data.",https://github.com/noklesta/,-1
0d9f5c4a-f99e-427f-9a09-380214c6e181,A Low-Shot Object Counting Network With Iterative Prototype Adaptation,0.336799,"We consider low-shot counting of arbitrary semantic categories in the image
using only few annotated exemplars (few-shot) or no exemplars (no-shot). The
standard few-shot pipeline follows extraction of appearance queries from
exemplars and matching them with image features to infer the object counts.
Existing methods extract queries by feature pooling which neglects the shape
information (e.g., size and aspect) and leads to a reduced object localization
accuracy and count estimates. We propose a Low-shot Object Counting network
with iterative prototype Adaptation (LOCA). Our main contribution is the new
object prototype extraction module, which iteratively fuses the exemplar shape
and appearance information with image features. The module is easily adapted to
zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot
counting problems. LOCA outperforms all recent state-of-the-art methods on
FSC147 benchmark by 20-30% in RMSE on one-shot and few-shot and achieves
state-of-the-art on zero-shot scenarios, while demonstrating better
generalization capabilities.",https://github.com/djukicn/loca,11587
a846b18f-1b3c-43a4-9777-06581bd94895,OTPose: Occlusion-Aware Transformer for Pose Estimation in Sparsely-Labeled Videos,0.363362,"Although many approaches for multi-human pose estimation in videos have shown
profound results, they require densely annotated data which entails excessive
man labor. Furthermore, there exists occlusion and motion blur that inevitably
lead to poor estimation performance. To address these problems, we propose a
method that leverages an attention mask for occluded joints and encodes
temporal dependency between frames using transformers. First, our framework
composes different combinations of sparsely annotated frames that denote the
track of the overall joint movement. We propose an occlusion attention mask
from these combinations that enable encoding occlusion-aware heatmaps as a
semi-supervised task. Second, the proposed temporal encoder employs transformer
architecture to effectively aggregate the temporal relationship and
keypoint-wise attention from each time step and accurately refines the target
frame's final pose estimation. We achieve state-of-the-art pose estimation
results for PoseTrack2017 and PoseTrack2018 datasets and demonstrate the
robustness of our approach to occlusion and motion blur in sparsely annotated
video data.",None,-1
167014be-c5c2-4c44-8e53-49f65a80f57a,A Secure Clustering Protocol with Fuzzy Trust Evaluation and Outlier Detection for Industrial Wireless Sensor Networks,0.881122,"Security is one of the major concerns in Industrial Wireless Sensor Networks
(IWSNs). To assure the security in clustered IWSNs, this paper presents a
secure clustering protocol with fuzzy trust evaluation and outlier detection
(SCFTO). Firstly, to deal with the transmission uncertainty in an open wireless
medium, an interval type-2 fuzzy logic controller is adopted to estimate the
trusts. And then a density based outlier detection mechanism is introduced to
acquire an adaptive trust threshold used to isolate the malicious nodes from
being cluster heads. Finally, a fuzzy based cluster heads election method is
proposed to achieve a balance between energy saving and security assurance, so
that a normal sensor node with more residual energy or less confidence on other
nodes has higher probability to be the cluster head. Extensive experiments
verify that our secure clustering protocol can effectively defend the network
against attacks from internal malicious or compromised nodes.",None,-1
43760f44-cb2a-4ca1-87e9-ae37cc28afe3,Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions,0.390827,"We explore zero-shot adaptation, where a general-domain model has access to
customer or domain specific parallel data at inference time, but not during
training. We build on the idea of Retrieval Augmented Translation (RAT) where
top-k in-domain fuzzy matches are found for the source sentence, and
target-language translations of those fuzzy-matched sentences are provided to
the translation model at inference time. We propose a novel architecture to
control interactions between a source sentence and the top-k fuzzy
target-language matches, and compare it to architectures from prior work. We
conduct experiments in two language pairs (En-De and En-Fr) by training models
on WMT data and testing them with five and seven multi-domain datasets,
respectively. Our approach consistently outperforms the alternative
architectures, improving BLEU across language pair, domain, and number k of
fuzzy matches.",https://github.com/elastic/elasticsearch-py,-1
3bec2f73-f49b-4f7b-b994-08cb60a4acfa,An Effective Scheme for Maize Disease Recognition based on Deep Networks,0.109605,"In the last decades, the area under cultivation of maize products has
increased because of its essential role in the food cycle for humans,
livestock, and poultry. Moreover, the diseases of plants impact food safety and
can significantly reduce both the quality and quantity of agricultural
products. There are many challenges to accurate and timely diagnosis of the
disease. This research presents a novel scheme based on a deep neural network
to overcome the mentioned challenges. Due to the limited number of data, the
transfer learning technique is employed with the help of two well-known
architectures. In this way, a new effective model is adopted by a combination
of pre-trained MobileNetV2 and Inception Networks due to their effective
performance on object detection problems. The convolution layers of MoblieNetV2
and Inception modules are parallelly arranged as earlier layers to extract
crucial features. In addition, the imbalance problem of classes has been solved
by an augmentation strategy. The proposed scheme has a superior performance
compared to other state-of-the-art models published in recent years. The
accuracy of the model reaches 97%, approximately. In summary, experimental
results prove the method's validity and significant performance in diagnosing
disease in plant leaves.",None,-1
6cd9fd7d-7c51-4c06-9539-36b9d909f027,Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries,0.854259,"Learning under a continuously changing data distribution with incorrect
labels is a desirable real-world problem yet challenging. A large body of
continual learning (CL) methods, however, assumes data streams with clean
labels, and online learning scenarios under noisy data streams are yet
underexplored. We consider a more practical CL task setup of an online learning
from blurry data stream with corrupted labels, where existing CL methods
struggle. To address the task, we first argue the importance of both diversity
and purity of examples in the episodic memory of continual learning models. To
balance diversity and purity in the episodic memory, we propose a novel
strategy to manage and use the memory by a unified approach of label noise
aware diverse sampling and robust learning with semi-supervised learning. Our
empirical validations on four real-world or synthetic noise datasets (CIFAR10
and 100, mini-WebVision, and Food-101N) exhibit that our method significantly
outperforms prior arts in this realistic and challenging continual learning
scenario. Code and data splits are available in
https://github.com/clovaai/puridiver.",https://github.com/clovaai/puridiver,-1
bddd40b6-8a0b-483a-8426-f602ad785921,DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation,0.691153,"Task-oriented dialogue generation is challenging since the underlying
knowledge is often dynamic and effectively incorporating knowledge into the
learning process is hard. It is particularly challenging to generate both
human-like and informative responses in this setting. Recent research primarily
focused on various knowledge distillation methods where the underlying
relationship between the facts in a knowledge base is not effectively captured.
In this paper, we go one step further and demonstrate how the structural
information of a knowledge graph can improve the system's inference
capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue
system that effectively incorporates knowledge into a language model. Our
proposed system views relational knowledge as a knowledge graph and introduces
(1) a structure-aware knowledge embedding technique, and (2) a knowledge
graph-weighted attention masking strategy to facilitate the system selecting
relevant information during the dialogue generation. An empirical evaluation
demonstrates the effectiveness of DialoKG over state-of-the-art methods on
several standard benchmark datasets.",https://github.com/rashad101/DialoKG,-1
12690778-e838-4891-93f0-ebc6e3a79489,Analyzing Wrap-Up Effects through an Information-Theoretic Lens,0.679312,"Numerous analyses of reading time (RT) data have been implemented -- all in
an effort to better understand the cognitive processes driving reading
comprehension. However, data measured on words at the end of a sentence -- or
even at the end of a clause -- is often omitted due to the confounding factors
introduced by so-called ""wrap-up effects,"" which manifests as a skewed
distribution of RTs for these words. Consequently, the understanding of the
cognitive processes that might be involved in these wrap-up effects is limited.
In this work, we attempt to learn more about these processes by examining the
relationship between wrap-up effects and information-theoretic quantities, such
as word and context surprisals. We find that the distribution of information in
prior contexts is often predictive of sentence- and clause-final RTs (while not
of sentence-medial RTs). This lends support to several prior hypotheses about
the processes involved in wrap-up effects.",https://github.com/rycolab/wrap-up-effects,-1
41f45234-0aa4-49f4-9dab-768a999330cd,DocBed: A Multi-Stage OCR Solution for Documents with Complex Layouts,0.716781,"Digitization of newspapers is of interest for many reasons including
preservation of history, accessibility and search ability, etc. While
digitization of documents such as scientific articles and magazines is
prevalent in literature, one of the main challenges for digitization of
newspaper lies in its complex layout (e.g. articles spanning multiple columns,
text interrupted by images) analysis, which is necessary to preserve human
read-order. This work provides a major breakthrough in the digitization of
newspapers on three fronts: first, releasing a dataset of 3000 fully-annotated,
real-world newspaper images from 21 different U.S. states representing an
extensive variety of complex layouts for document layout analysis; second,
proposing layout segmentation as a precursor to existing optical character
recognition (OCR) engines, where multiple state-of-the-art image segmentation
models and several post-processing methods are explored for document layout
segmentation; third, providing a thorough and structured evaluation protocol
for isolated layout segmentation and end-to-end OCR.",https://github.com/facebookresearch/detectron2,-1
e1806389-d28d-4fe2-a67f-fdd9da8bfb24,Combining Lipschitz and RBF Surrogate Models for High-dimensional Computationally Expensive Problems,0.747803,"Standard evolutionary optimization algorithms assume that the evaluation of
the objective and constraint functions is straightforward and computationally
cheap. However, in many real-world optimization problems, these evaluations
involve computationally expensive numerical simulations or physical
experiments. Surrogate-assisted evolutionary algorithms (SAEAs) have recently
gained increased attention for their performance in solving these types of
problems. The main idea of SAEAs is the integration of an evolutionary
algorithm with a selected surrogate model that approximates the computationally
expensive function. In this paper, we propose a surrogate model based on a
Lipschitz underestimation and use it to develop a differential evolution-based
algorithm. The algorithm, called Lipschitz Surrogate-assisted Differential
Evolution (LSADE), utilizes the Lipschitz-based surrogate model, along with a
standard radial basis function surrogate model and a local search procedure.
The experimental results on seven benchmark functions of dimensions 30, 50,
100, and 200 show that the proposed LSADE algorithm is competitive compared
with the state-of-the-art algorithms under a limited computational budget,
being especially effective for the very complicated benchmark functions in high
dimensions.",https://github.com/JakubKudela89/LSADE,-1
4aba1579-a53b-4886-a3d3-bacd9aa82244,Feedback Gradient Descent: Efficient and Stable Optimization with Orthogonality for DNNs,0.596777,"The optimization with orthogonality has been shown useful in training deep
neural networks (DNNs). To impose orthogonality on DNNs, both computational
efficiency and stability are important. However, existing methods utilizing
Riemannian optimization or hard constraints can only ensure stability while
those using soft constraints can only improve efficiency. In this paper, we
propose a novel method, named Feedback Gradient Descent (FGD), to our
knowledge, the first work showing high efficiency and stability simultaneously.
FGD induces orthogonality based on the simple yet indispensable Euler
discretization of a continuous-time dynamical system on the tangent bundle of
the Stiefel manifold. In particular, inspired by a numerical integration method
on manifolds called Feedback Integrators, we propose to instantiate it on the
tangent bundle of the Stiefel manifold for the first time. In the extensive
image classification experiments, FGD comprehensively outperforms the existing
state-of-the-art methods in terms of accuracy, efficiency, and stability.",None,-1
88cf7035-7027-4939-8f85-7f2a4aa331ec,"CGiS-Net: Aggregating Colour, Geometry and Implicit Semantic Features for Indoor Place Recognition",0.550614,"We describe a novel approach to indoor place recognition from RGB point
clouds based on aggregating low-level colour and geometry features with
high-level implicit semantic features. It uses a 2-stage deep learning
framework, in which the first stage is trained for the auxiliary task of
semantic segmentation and the second stage uses features from layers in the
first stage to generate discriminate descriptors for place recognition. The
auxiliary task encourages the features to be semantically meaningful, hence
aggregating the geometry and colour in the RGB point cloud data with implicit
semantic information. We use an indoor place recognition dataset derived from
the ScanNet dataset for training and evaluation, with a test set comprising
3,608 point clouds generated from 100 different rooms. Comparison with a
traditional feature-based method and four state-of-the-art deep learning
methods demonstrate that our approach significantly outperforms all five
methods, achieving, for example, a top-3 average recall rate of 75% compared
with 41% for the closest rival method. Our code is available at:
https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition",https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition,-1
5997d07e-fbac-446b-94e7-1a0b8653627d,Fine-Grained Visual Classification using Self Assessment Classifier,0.39428,"Extracting discriminative features plays a crucial role in the fine-grained
visual classification task. Most of the existing methods focus on developing
attention or augmentation mechanisms to achieve this goal. However, addressing
the ambiguity in the top-k prediction classes is not fully investigated. In
this paper, we introduce a Self Assessment Classifier, which simultaneously
leverages the representation of the image and top-k prediction classes to
reassess the classification results. Our method is inspired by continual
learning with coarse-grained and fine-grained classifiers to increase the
discrimination of features in the backbone and produce attention maps of
informative areas on the image. In practice, our method works as an auxiliary
branch and can be easily integrated into different architectures. We show that
by effectively addressing the ambiguity in the top-k prediction classes, our
method achieves new state-of-the-art results on CUB200-2011, Stanford Dog, and
FGVC Aircraft datasets. Furthermore, our method also consistently improves the
accuracy of different existing fine-grained classifiers with a unified setup.",https://github.com/aioz-ai/SAC,-1
485b5810-0b85-4657-a237-4deacd0bbc2e,ViTOL: Vision Transformer for Weakly Supervised Object Localization,0.735191,"Weakly supervised object localization (WSOL) aims at predicting object
locations in an image using only image-level category labels. Common challenges
that image classification models encounter when localizing objects are, (a)
they tend to look at the most discriminative features in an image that confines
the localization map to a very small region, (b) the localization maps are
class agnostic, and the models highlight objects of multiple classes in the
same image and, (c) the localization performance is affected by background
noise. To alleviate the above challenges we introduce the following simple
changes through our proposed method ViTOL. We leverage the vision-based
transformer for self-attention and introduce a patch-based attention dropout
layer (p-ADL) to increase the coverage of the localization map and a gradient
attention rollout mechanism to generate class-dependent attention maps. We
conduct extensive quantitative, qualitative and ablation experiments on the
ImageNet-1K and CUB datasets. We achieve state-of-the-art MaxBoxAcc-V2
localization scores of 70.47% and 73.17% on the two datasets respectively. Code
is available on https://github.com/Saurav-31/ViTOL",https://github.com/Saurav-31/ViTOL,-1
49a8400a-a4a2-4e8c-ab98-2734ade3613c,Learning to Reuse Distractors to support Multiple Choice Question Generation in Education,0.48607,"Multiple choice questions (MCQs) are widely used in digital learning systems,
as they allow for automating the assessment process. However, due to the
increased digital literacy of students and the advent of social media
platforms, MCQ tests are widely shared online, and teachers are continuously
challenged to create new questions, which is an expensive and time-consuming
task. A particularly sensitive aspect of MCQ creation is to devise relevant
distractors, i.e., wrong answers that are not easily identifiable as being
wrong. This paper studies how a large existing set of manually created answers
and distractors for questions over a variety of domains, subjects, and
languages can be leveraged to help teachers in creating new MCQs, by the smart
reuse of existing distractors. We built several data-driven models based on
context-aware question and distractor representations, and compared them with
static feature-based models. The proposed models are evaluated with automated
metrics and in a realistic user test with teachers. Both automatic and human
evaluations indicate that context-aware models consistently outperform a static
feature-based approach. For our best-performing context-aware model, on average
3 distractors out of the 10 shown to teachers were rated as high-quality
distractors. We create a performance benchmark, and make it public, to enable
comparison between different approaches and to introduce a more standardized
evaluation of the task. The benchmark contains a test of 298 educational
questions covering multiple subjects & languages and a 77k multilingual pool of
distractor vocabulary for future research.",https://github.com/semerekiros/dist-retrieval,-1
8e785d1f-de24-47ef-aea4-9155bdd97556,FFC-SE: Fast Fourier Convolution for Speech Enhancement,0.590635,"Fast Fourier convolution (FFC) is the recently proposed neural operator
showing promising performance in several computer vision problems. The FFC
operator allows employing large receptive field operations within early layers
of the neural network. It was shown to be especially helpful for inpainting of
periodic structures which are common in audio processing. In this work, we
design neural network architectures which adapt FFC for speech enhancement. We
hypothesize that a large receptive field allows these networks to produce more
coherent phases than vanilla convolutional models, and validate this hypothesis
experimentally. We found that neural networks based on Fast Fourier convolution
outperform analogous convolutional models and show better or comparable results
with other speech enhancement baselines.",None,-1
d8e92c1e-19f3-4cc7-a626-9ad0cb51d889,Memory transformers for full context and high-resolution 3D Medical Segmentation,0.25046,"Transformer models achieve state-of-the-art results for image segmentation.
However, achieving long-range attention, necessary to capture global context,
with high-resolution 3D images is a fundamental challenge. This paper
introduces the Full resolutIoN mEmory (FINE) transformer to overcome this
issue. The core idea behind FINE is to learn memory tokens to indirectly model
full range interactions while scaling well in both memory and computational
costs. FINE introduces memory tokens at two levels: the first one allows full
interaction between voxels within local image regions (patches), the second one
allows full interactions between all regions of the 3D volume. Combined, they
allow full attention over high resolution images, e.g. 512 x 512 x 256 voxels
and above. Experiments on the BCV image segmentation dataset shows better
performances than state-of-the-art CNN and transformer baselines, highlighting
the superiority of our full attention mechanism compared to recent transformer
baselines, e.g. CoTr, and nnFormer.",None,-1
ae67d3b7-8941-4ea3-b68d-e4ecbcea72ca,Densely Constrained Depth Estimator for Monocular 3D Object Detection,0.687387,"Estimating accurate 3D locations of objects from monocular images is a
challenging problem because of lacking depth. Previous work shows that
utilizing the object's keypoint projection constraints to estimate multiple
depth candidates boosts the detection performance. However, the existing
methods can only utilize vertical edges as projection constraints for depth
estimation. So these methods only use a small number of projection constraints
and produce insufficient depth candidates, leading to inaccurate depth
estimation. In this paper, we propose a method that utilizes dense projection
constraints from edges of any direction. In this way, we employ much more
projection constraints and produce considerable depth candidates. Besides, we
present a graph matching weighting module to merge the depth candidates. The
proposed method DCD (Densely Constrained Detector) achieves state-of-the-art
performance on the KITTI and WOD benchmarks. Code is released at
https://github.com/BraveGroup/DCD.",https://github.com/BraveGroup/DCD,-1
d01e3fdc-df92-47e9-aad3-8faeeaaecaa7,Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text,0.759209,"As text generated by large language models proliferates, it becomes vital to
understand how humans engage with such text, and whether or not they are able
to detect when the text they are reading did not originate with a human writer.
Prior work on human detection of generated text focuses on the case where an
entire passage is either human-written or machine-generated. In this paper, we
study a more realistic setting where text begins as human-written and
transitions to being generated by state-of-the-art neural language models. We
show that, while annotators often struggle at this task, there is substantial
variance in annotator skill and that given proper incentives, annotators can
improve at this task over time. Furthermore, we conduct a detailed comparison
study and analyze how a variety of variables (model size, decoding strategy,
fine-tuning, prompt genre, etc.) affect human detection performance. Finally,
we collect error annotations from our participants and use them to show that
certain textual genres influence models to make different types of errors and
that certain sentence-level features correlate highly with annotator selection.
We release the RoFT dataset: a collection of over 21,000 human annotations
paired with error classifications to encourage future work in human detection
and evaluation of generated text.",https://github.com/liamdugan/human-detection,-1
a1702cce-fe28-432a-ac04-3fbd5881ee75,Sequential Causal Imitation Learning with Unobserved Confounders,0.903047,"""Monkey see monkey do"" is an age-old adage, referring to na\""ive imitation
without a deep understanding of a system's underlying mechanics. Indeed, if a
demonstrator has access to information unavailable to the imitator (monkey),
such as a different set of sensors, then no matter how perfectly the imitator
models its perceived environment (See), attempting to reproduce the
demonstrator's behavior (Do) can lead to poor outcomes. Imitation learning in
the presence of a mismatch between demonstrator and imitator has been studied
in the literature under the rubric of causal imitation learning (Zhang et al.,
2020), but existing solutions are limited to single-stage decision-making. This
paper investigates the problem of causal imitation learning in sequential
settings, where the imitator must make multiple decisions per episode. We
develop a graphical criterion that is necessary and sufficient for determining
the feasibility of causal imitation, providing conditions when an imitator can
match a demonstrator's performance despite differing capabilities. Finally, we
provide an efficient algorithm for determining imitability and corroborate our
theory with simulations.",None,-1
064a1ff5-cad8-4338-bc7a-e5a951408ebb,Sentiment Analysis of Covid-related Reddits,0.23726,"This paper focuses on Sentiment Analysis of Covid-19 related messages from
the r/Canada and r/Unitedkingdom subreddits of Reddit. We apply manual
annotation and three Machine Learning algorithms to analyze sentiments conveyed
in those messages. We use VADER and TextBlob to label messages for Machine
Learning experiments. Our results show that removal of shortest and longest
messages improves VADER and TextBlob agreement on positive sentiments and
F-score of sentiment classification by all the three algorithms",None,-1
0778424a-9750-4dd2-bf99-db4820254ffd,DexTransfer: Real World Multi-fingered Dexterous Grasping with Minimal Human Demonstrations,0.626191,"Teaching a multi-fingered dexterous robot to grasp objects in the real world
has been a challenging problem due to its high dimensional state and action
space. We propose a robot-learning system that can take a small number of human
demonstrations and learn to grasp unseen object poses given partially occluded
observations. Our system leverages a small motion capture dataset and generates
a large dataset with diverse and successful trajectories for a multi-fingered
robot gripper. By adding domain randomization, we show that our dataset
provides robust grasping trajectories that can be transferred to a policy
learner. We train a dexterous grasping policy that takes the point clouds of
the object as input and predicts continuous actions to grasp objects from
different initial robot states. We evaluate the effectiveness of our system on
a 22-DoF floating Allegro Hand in simulation and a 23-DoF Allegro robot hand
with a KUKA arm in real world. The policy learned from our dataset can
generalize well on unseen object poses in both simulation and the real world",https://github.com/mmatl/pyrender,-1
799ccd80-12f6-45b9-a1ad-2234f58edcc7,Open Vocabulary Extreme Classification Using Generative Models,0.241337,"The extreme multi-label classification (XMC) task aims at tagging content
with a subset of labels from an extremely large label set. The label vocabulary
is typically defined in advance by domain experts and assumed to capture all
necessary tags. However in real world scenarios this label set, although large,
is often incomplete and experts frequently need to refine it. To develop
systems that simplify this process, we introduce the task of open vocabulary
XMC (OXMC): given a piece of content, predict a set of labels, some of which
may be outside of the known tag set. Hence, in addition to not having training
data for some labels - as is the case in zero-shot classification - models need
to invent some labels on-the-fly. We propose GROOV, a fine-tuned seq2seq model
for OXMC that generates the set of labels as a flat sequence and is trained
using a novel loss independent of predicted label order. We show the efficacy
of the approach, experimenting with popular XMC datasets for which GROOV is
able to predict meaningful labels outside the given vocabulary while performing
on par with state-of-the-art solutions for known labels.",None,-1
309bb2ae-1f56-4514-8062-d6da9f44c0f4,DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection,0.814973,"Graph Anomaly Detection (GAD) has recently become a hot research spot due to
its practicability and theoretical value. Since GAD emphasizes the application
and the rarity of anomalous samples, enriching the varieties of its datasets is
fundamental work. Thus, this paper present DGraph, a real-world dynamic graph
in the finance domain. DGraph overcomes many limitations of current GAD
datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth
nodes. We provide a comprehensive observation of DGraph, revealing that
anomalous nodes and normal nodes generally have different structures, neighbor
distribution, and temporal dynamics. Moreover, it suggests that unlabeled nodes
are also essential for detecting fraudsters. Furthermore, we conduct extensive
experiments on DGraph. Observation and experiments demonstrate that DGraph is
propulsive to advance GAD research and enable in-depth exploration of anomalous
nodes.",https://github.com/hxttkl/DGraph_Experiments,-1
f60f6280-a9d7-455a-a1d6-dc032e80c5e8,Developmental Negation Processing in Transformer Language Models,0.111208,"Reasoning using negation is known to be difficult for transformer-based
language models. While previous studies have used the tools of
psycholinguistics to probe a transformer's ability to reason over negation,
none have focused on the types of negation studied in developmental psychology.
We explore how well transformers can process such categories of negation, by
framing the problem as a natural language inference (NLI) task. We curate a set
of diagnostic questions for our target categories from popular NLI datasets and
evaluate how well a suite of models reason over them. We find that models
perform consistently better only on certain categories, suggesting clear
distinctions in how they are processed.",https://github.com/Advancing-Machine-Human-Reasoning-Lab/negation-processing-ACL-2022,-1
661853c9-c5b2-43dc-9fc2-a9bd04e6ab20,IGLU 2022: Interactive Grounded Language Understanding in a Collaborative Environment at NeurIPS 2022,0.697186,"Human intelligence has the remarkable ability to adapt to new tasks and
environments quickly. Starting from a very young age, humans acquire new skills
and learn how to solve new tasks either by imitating the behavior of others or
by following provided natural language instructions. To facilitate research in
this direction, we propose IGLU: Interactive Grounded Language Understanding in
a Collaborative Environment. The primary goal of the competition is to approach
the problem of how to develop interactive embodied agents that learn to solve a
task while provided with grounded natural language instructions in a
collaborative environment. Understanding the complexity of the challenge, we
split it into sub-tasks to make it feasible for participants.
  This research challenge is naturally related, but not limited, to two fields
of study that are highly relevant to the NeurIPS community: Natural Language
Understanding and Generation (NLU/G) and Reinforcement Learning (RL).
Therefore, the suggested challenge can bring two communities together to
approach one of the crucial challenges in AI. Another critical aspect of the
challenge is the dedication to perform a human-in-the-loop evaluation as a
final evaluation for the agents developed by contestants.",None,-1
a00f56c7-2516-4f66-b537-3df03bc5f2a0,DisenHCN: Disentangled Hypergraph Convolutional Networks for Spatiotemporal Activity Prediction,0.518827,"Spatiotemporal activity prediction, aiming to predict user activities at a
specific location and time, is crucial for applications like urban planning and
mobile advertising. Existing solutions based on tensor decomposition or graph
embedding suffer from the following two major limitations: 1) ignoring the
fine-grained similarities of user preferences; 2) user's modeling is entangled.
In this work, we propose a hypergraph neural network model called DisenHCN to
bridge the above gaps. In particular, we first unify the fine-grained user
similarity and the complex matching between user preferences and spatiotemporal
activity into a heterogeneous hypergraph. We then disentangle the user
representations into different aspects (location-aware, time-aware, and
activity-aware) and aggregate corresponding aspect's features on the
constructed hypergraph, capturing high-order relations from different aspects
and disentangles the impact of each aspect for final prediction. Extensive
experiments show that our DisenHCN outperforms the state-of-the-art methods by
14.23% to 18.10% on four real-world datasets. Further studies also convincingly
verify the rationality of each component in our DisenHCN.",None,-1
1e80e521-599f-4cee-a59b-0d657aca0f45,ROAD: Learning an Implicit Recursive Octree Auto-Decoder to Efficiently Encode 3D Shapes,0.351916,"Compact and accurate representations of 3D shapes are central to many
perception and robotics tasks. State-of-the-art learning-based methods can
reconstruct single objects but scale poorly to large datasets. We present a
novel recursive implicit representation to efficiently and accurately encode
large datasets of complex 3D shapes by recursively traversing an implicit
octree in latent space. Our implicit Recursive Octree Auto-Decoder (ROAD)
learns a hierarchically structured latent space enabling state-of-the-art
reconstruction results at a compression ratio above 99%. We also propose an
efficient curriculum learning scheme that naturally exploits the coarse-to-fine
properties of the underlying octree spatial representation. We explore the
scaling law relating latent space dimension, dataset size, and reconstruction
accuracy, showing that increasing the latent space dimension is enough to scale
to large shape datasets. Finally, we show that our learned latent space encodes
a coarse-to-fine hierarchical structure yielding reusable latents across
different levels of details, and we provide qualitative evidence of
generalization to novel shapes outside the training set.",https://zakharos.github.io/projects/road/,-1
77f96f24-fbb9-417f-85de-debb33d81d91,Multimodal learning with graphs,0.808843,"Artificial intelligence for graphs has achieved remarkable success in
modeling complex systems, ranging from dynamic networks in biology to
interacting particle systems in physics. However, the increasingly
heterogeneous graph datasets call for multimodal methods that can combine
different inductive biases: the set of assumptions that algorithms use to make
predictions for inputs they have not encountered during training. Learning on
multimodal datasets presents fundamental challenges because the inductive
biases can vary by data modality and graphs might not be explicitly given in
the input. To address these challenges, multimodal graph AI methods combine
different modalities while leveraging cross-modal dependencies using graphs.
Diverse datasets are combined using graphs and fed into sophisticated
multimodal architectures, specified as image-intensive, knowledge-grounded and
language-intensive models. Using this categorization, we introduce a blueprint
for multimodal graph learning, use it to study existing methods and provide
guidelines to design new models.",https://yashaektefaie.github.io/mgl,-1
dada3361-847b-406f-b171-b961eb5fd7b2,SegViT: Semantic Segmentation with Plain Vision Transformers,0.905516,"We explore the capability of plain Vision Transformers (ViTs) for semantic
segmentation and propose the SegVit. Previous ViT-based segmentation networks
usually learn a pixel-level representation from the output of the ViT.
Differently, we make use of the fundamental component -- attention mechanism,
to generate masks for semantic segmentation. Specifically, we propose the
Attention-to-Mask (ATM) module, in which the similarity maps between a set of
learnable class tokens and the spatial feature maps are transferred to the
segmentation masks. Experiments show that our proposed SegVit using the ATM
module outperforms its counterparts using the plain ViT backbone on the ADE20K
dataset and achieves new state-of-the-art performance on COCO-Stuff-10K and
PASCAL-Context datasets. Furthermore, to reduce the computational cost of the
ViT backbone, we propose query-based down-sampling (QD) and query-based
up-sampling (QU) to build a Shrunk structure. With the proposed Shrunk
structure, the model can save up to $40\%$ computations while maintaining
competitive performance.",https://github.com/zbwxp/SegVit,-1
82206b4a-a332-4b86-8520-4bff11a6a242,Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation From Monocular RGB Image,0.746049,"Recently, RGBD-based category-level 6D object pose estimation has achieved
promising improvement in performance, however, the requirement of depth
information prohibits broader applications. In order to relieve this problem,
this paper proposes a novel approach named Object Level Depth reconstruction
Network (OLD-Net) taking only RGB images as input for category-level 6D object
pose estimation. We propose to directly predict object-level depth from a
monocular RGB image by deforming the category-level shape prior into
object-level depth and the canonical NOCS representation. Two novel modules
named Normalized Global Position Hints (NGPH) and Shape-aware Decoupled Depth
Reconstruction (SDDR) module are introduced to learn high fidelity object-level
depth and delicate shape representations. At last, the 6D object pose is solved
by aligning the predicted canonical representation with the back-projected
object-level depth. Extensive experiments on the challenging CAMERA25 and
REAL275 datasets indicate that our model, though simple, achieves
state-of-the-art performance.",None,-1
c729353b-5ee2-46ed-af4c-85706ab426c8,Learning Rationalizable Equilibria in Multiplayer Games,0.242386,"A natural goal in multiagent learning besides finding equilibria is to learn
rationalizable behavior, where players learn to avoid iteratively dominated
actions. However, even in the basic setting of multiplayer general-sum games,
existing algorithms require a number of samples exponential in the number of
players to learn rationalizable equilibria under bandit feedback. This paper
develops the first line of efficient algorithms for learning rationalizable
Coarse Correlated Equilibria (CCE) and Correlated Equilibria (CE) whose sample
complexities are polynomial in all problem parameters including the number of
players. To achieve this result, we also develop a new efficient algorithm for
the simpler task of finding one rationalizable action profile (not necessarily
an equilibrium), whose sample complexity substantially improves over the best
existing results of Wu et al. (2021). Our algorithms incorporate several novel
techniques to guarantee rationalizability and no (swap-)regret simultaneously,
including a correlated exploration scheme and adaptive learning rates, which
may be of independent interest. We complement our results with a sample
complexity lower bound showing the sharpness of our guarantees.",None,-1
5bd2626f-2c1a-43cc-9bd9-5b62203973a8,Flowstorm: Open-Source Platform with Hybrid Dialogue Architecture,0.0765542,"This paper presents a conversational AI platform called Flowstorm. Flowstorm
is an open-source SaaS project suitable for creating, running, and analyzing
conversational applications. Thanks to the fast and fully automated build
process, the dialogues created within the platform can be executed in seconds.
Furthermore, we propose a novel dialogue architecture that uses a combination
of tree structures with generative models. The tree structures are also used
for training NLU models suitable for specific dialogue scenarios. However, the
generative models are globally used across applications and extend the
functionality of the dialogue trees. Moreover, the platform functionality
benefits from out-of-the-box components, such as the one responsible for
extracting data from utterances or working with crawled data. Additionally, it
can be extended using a custom code directly in the platform. One of the
essential features of the platform is the possibility to reuse the created
assets across applications. There is a library of prepared assets where each
developer can contribute. All of the features are available through a
user-friendly visual editor.",None,-1
cebeac50-5704-4fc6-b18b-3e4951b911b5,Reward Learning with Trees: Methods and Evaluation,0.0557825,"Recent efforts to learn reward functions from human feedback have tended to
use deep neural networks, whose lack of transparency hampers our ability to
explain agent behaviour or verify alignment. We explore the merits of learning
intrinsically interpretable tree models instead. We develop a recently proposed
method for learning reward trees from preference labels, and show it to be
broadly competitive with neural networks on challenging high-dimensional tasks,
with good robustness to limited or corrupted data. Having found that reward
tree learning can be done effectively in complex settings, we then consider why
it should be used, demonstrating that the interpretable reward structure gives
significant scope for traceability, verification and explanation.",None,-1
985509d4-2532-4ba5-a568-5c248446f13a,AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models,0.229715,"Knowledge distillation (KD) methods compress large models into smaller
students with manually-designed student architectures given pre-specified
computational cost. This requires several trials to find a viable student, and
further repeating the process for each student or computational budget change.
We use Neural Architecture Search (NAS) to automatically distill several
compressed students with variable cost from a large model. Current works train
a single SuperLM consisting of millions of subnetworks with weight-sharing,
resulting in interference between subnetworks of different sizes. Our framework
AutoDistil addresses above challenges with the following steps: (a)
Incorporates inductive bias and heuristics to partition Transformer search
space into K compact sub-spaces (K=3 for typical student sizes of base, small
and tiny); (b) Trains one SuperLM for each sub-space using task-agnostic
objective (e.g., self-attention distillation) with weight-sharing of students;
(c) Lightweight search for the optimal student without re-training. Fully
task-agnostic training and search allow students to be reused for fine-tuning
on any downstream task. Experiments on GLUE benchmark against state-of-the-art
KD and NAS methods demonstrate AutoDistil to outperform leading compression
techniques with upto 2.7x reduction in computational cost and negligible loss
in task performance.",None,-1
4fa029ac-391e-4355-a39f-dbfcd88e4fb6,Using Deep Mixture-of-Experts to Detect Word Meaning Shift for TempoWiC,0.461566,"This paper mainly describes the dma submission to the TempoWiC task, which
achieves a macro-F1 score of 77.05% and attains the first place in this task.
We first explore the impact of different pre-trained language models. Then we
adopt data cleaning, data augmentation, and adversarial training strategies to
enhance the model generalization and robustness. For further improvement, we
integrate POS information and word semantic representation using a
Mixture-of-Experts (MoE) approach. The experimental results show that MoE can
overcome the feature overuse issue and combine the context, POS, and word
semantic features well. Additionally, we use a model ensemble method for the
final prediction, which has been proven effective by many research works.",None,-1
1f5a7c2a-b950-4e1f-84a2-723723065cc0,"Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data",0.77021,"Automatic player identification is an essential and complex task in sports
video analysis. Different strategies have been devised over the years, but
identification based on jersey numbers is one of the most common approaches
given its versatility and relative simplicity. However, automatic detection of
jersey numbers is still challenging due to changing camera angles, low video
resolution, small object size in wide-range shots and transient changes in the
player's posture and movement. In this paper we present a novel approach for
jersey number identification in a small, highly imbalanced dataset from the
Seattle Seahawks practice videos. Our results indicate that simple models can
achieve an acceptable performance on the jersey number detection task and that
synthetic data can improve the performance dramatically (accuracy increase of
~9% overall, ~18% on low frequency numbers) making our approach achieve state
of the art results.",None,-1
9822d281-645f-451d-b7b0-e6f857289ed0,Abstractive Summarization Guided by Latent Hierarchical Document Structure,0.257712,"Sequential abstractive neural summarizers often do not use the underlying
structure in the input article or dependencies between the input sentences.
This structure is essential to integrate and consolidate information from
different parts of the text. To address this shortcoming, we propose a
hierarchy-aware graph neural network (HierGNN) which captures such dependencies
through three main steps: 1) learning a hierarchical document structure through
a latent structure tree learned by a sparse matrix-tree computation; 2)
propagating sentence information over this structure using a novel
message-passing node propagation mechanism to identify salient information; 3)
using graph-level attention to concentrate the decoder on salient information.
Experiments confirm HierGNN improves strong sequence models such as BART, with
a 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further
human evaluation demonstrates that summaries produced by our model are more
relevant and less redundant than the baselines, into which HierGNN is
incorporated. We also find HierGNN synthesizes summaries by fusing multiple
source sentences more, rather than compressing a single source sentence, and
that it processes long inputs more effectively.",https://github.com/yfqiu-nlp/hiergnn,-1
04f0dbf3-b322-4c97-bdec-e0696057f456,Linear Transformations for Cross-lingual Sentiment Analysis,0.310732,"This paper deals with cross-lingual sentiment analysis in Czech, English and
French languages. We perform zero-shot cross-lingual classification using five
linear transformations combined with LSTM and CNN based classifiers. We compare
the performance of the individual transformations, and in addition, we confront
the transformation-based approach with existing state-of-the-art BERT-like
models. We show that the pre-trained embeddings from the target domain are
crucial to improving the cross-lingual classification results, unlike in the
monolingual classification, where the effect is not so distinctive.",https://github.com/pauli31/linear-transformation-4-cs-sa,-1
0ff3ae91-b922-4bec-9cd2-421a13a845bb,"S5CL: Unifying Fully-Supervised, Self-Supervised, and Semi-Supervised Learning Through Hierarchical Contrastive Learning",0.150944,"In computational pathology, we often face a scarcity of annotations and a
large amount of unlabeled data. One method for dealing with this is
semi-supervised learning which is commonly split into a self-supervised pretext
task and a subsequent model fine-tuning. Here, we compress this two-stage
training into one by introducing S5CL, a unified framework for
fully-supervised, self-supervised, and semi-supervised learning. With three
contrastive losses defined for labeled, unlabeled, and pseudo-labeled images,
S5CL can learn feature representations that reflect the hierarchy of distance
relationships: similar images and augmentations are embedded the closest,
followed by different looking images of the same class, while images from
separate classes have the largest distance. Moreover, S5CL allows us to
flexibly combine these losses to adapt to different scenarios. Evaluations of
our framework on two public histopathological datasets show strong improvements
in the case of sparse labels: for a H&E-stained colorectal cancer dataset, the
accuracy increases by up to 9% compared to supervised cross-entropy loss; for a
highly imbalanced dataset of single white blood cells from leukemia patient
blood smears, the F1-score increases by up to 6%.",https://github.com/manuel-tran/s5cl,-1
744f3038-35a8-4d4b-9ea9-3385c1786776,Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems,0.618841,"Task-oriented dialogue systems (TDSs) are assessed mainly in an offline
setting or through human evaluation. The evaluation is often limited to
single-turn or is very time-intensive. As an alternative, user simulators that
mimic user behavior allow us to consider a broad set of user goals to generate
human-like conversations for simulated evaluation. Employing existing user
simulators to evaluate TDSs is challenging as user simulators are primarily
designed to optimize dialogue policies for TDSs and have limited evaluation
capabilities. Moreover, the evaluation of user simulators is an open challenge.
  In this work, we propose a metaphorical user simulator for end-to-end TDS
evaluation, where we define a simulator to be metaphorical if it simulates
user's analogical thinking in interactions with systems. We also propose a
tester-based evaluation framework to generate variants, i.e., dialogue systems
with different capabilities. Our user simulator constructs a metaphorical user
model that assists the simulator in reasoning by referring to prior knowledge
when encountering new items. We estimate the quality of simulators by checking
the simulated interactions between simulators and variants. Our experiments are
conducted using three TDS datasets. The proposed user simulator demonstrates
better consistency with manual evaluation than an agenda-based simulator and a
seq2seq model on three datasets; our tester framework demonstrates efficiency
and has been tested on multiple tasks, such as conversational recommendation
and e-commerce dialogues.",https://github.com/Superbooming/simtester,-1
f015a043-8ff9-4425-9915-9648dcdfab7a,Wavelet Feature Maps Compression for Image-to-Image CNNs,0.0652204,"Convolutional Neural Networks (CNNs) are known for requiring extensive
computational resources, and quantization is among the best and most common
methods for compressing them. While aggressive quantization (i.e., less than
4-bits) performs well for classification, it may cause severe performance
degradation in image-to-image tasks such as semantic segmentation and depth
estimation. In this paper, we propose Wavelet Compressed Convolution (WCC) -- a
novel approach for high-resolution activation maps compression integrated with
point-wise convolutions, which are the main computational cost of modern
architectures. To this end, we use an efficient and hardware-friendly
Haar-wavelet transform, known for its effectiveness in image compression, and
define the convolution on the compressed activation map. We experiment with
various tasks that benefit from high-resolution input. By combining WCC with
light quantization, we achieve compression rates equivalent to 1-4bit
activation quantization with relatively small and much more graceful
degradation in performance. Our code is available at
https://github.com/BGUCompSci/WaveletCompressedConvolution.",https://github.com/BGUCompSci/WaveletCompressedConvolution,-1
49f434ef-0dde-40d9-8a5f-1c42e6be4e54,The distribution of syntactic dependency distances,0.236007,"The syntactic structure of a sentence can be represented as a graph where
vertices are words and edges indicate syntactic dependencies between them. In
this setting, the distance between two syntactically linked words can be
defined as the difference between their positions. Here we want to contribute
to the characterization of the actual distribution of syntactic dependency
distances, and unveil its relationship with short-term memory limitations. We
propose a new double-exponential model in which decay in probability is allowed
to change after a break-point. This transition could mirror the transition from
the processing of words chunks to higher-level structures. We find that a
two-regime model -- where the first regime follows either an exponential or a
power-law decay -- is the most likely one in all 20 languages we considered,
independently of sentence length and annotation style. Moreover, the
break-point is fairly stable across languages and averages values of 4-5 words,
suggesting that the amount of words that can be simultaneously processed
abstracts from the specific language to a high degree. Finally, we give an
account of the relation between the best estimated model and the closeness of
syntactic dependencies, as measured by a recently introduced optimality score.",None,-1
1d08830b-c52a-4fee-b824-3fee31e477de,Self-supervised models of audio effectively explain human cortical responses to speech,0.815524,"Self-supervised language models are very effective at predicting high-level
cortical responses during language comprehension. However, the best current
models of lower-level auditory processing in the human brain rely on either
hand-constructed acoustic filters or representations from supervised audio
neural networks. In this work, we capitalize on the progress of self-supervised
speech representation learning (SSL) to create new state-of-the-art models of
the human auditory system. Compared against acoustic baselines, phonemic
features, and supervised models, representations from the middle layers of
self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently
yield the best prediction performance for fMRI recordings within the auditory
cortex (AC). Brain areas involved in low-level auditory processing exhibit a
preference for earlier SSL model layers, whereas higher-level semantic areas
prefer later layers. We show that these trends are due to the models' ability
to encode information at multiple linguistic levels (acoustic, phonetic, and
lexical) along their representation depth. Overall, these results show that
self-supervised models effectively capture the hierarchy of information
relevant to different stages of speech processing in human cortex.",None,589
42532919-0e90-4b27-8326-16aa653e5751,Thutmose Tagger: Single-pass neural model for Inverse Text Normalization,0.0240769,"Inverse text normalization (ITN) is an essential post-processing step in
automatic speech recognition (ASR). It converts numbers, dates, abbreviations,
and other semiotic classes from the spoken form generated by ASR to their
written forms. One can consider ITN as a Machine Translation task and use
neural sequence-to-sequence models to solve it. Unfortunately, such neural
models are prone to hallucinations that could lead to unacceptable errors. To
mitigate this issue, we propose a single-pass token classifier model that
regards ITN as a tagging task. The model assigns a replacement fragment to
every input token or marks it for deletion or copying without changes. We
present a dataset preparation method based on the granular alignment of ITN
examples. The proposed model is less prone to hallucination errors. The model
is trained on the Google Text Normalization dataset and achieves
state-of-the-art sentence accuracy on both English and Russian test sets.
One-to-one correspondence between tags and input words improves the
interpretability of the model's predictions, simplifies debugging, and allows
for post-processing corrections. The model is simpler than sequence-to-sequence
models and easier to optimize in production settings. The model and the code to
prepare the dataset is published as part of NeMo project.",https://github.com/NVIDIA/NeMo,-1
6e71f843-854d-4200-8e65-3a399d21fe67,Deep Learning-Based Discrete Calibrated Survival Prediction,0.0722009,"Deep neural networks for survival prediction outper-form classical approaches
in discrimination, which is the ordering of patients according to their
time-of-event. Conversely, classical approaches like the Cox Proportional
Hazards model display much better calibration, the correct temporal prediction
of events of the underlying distribution. Especially in the medical domain,
where it is critical to predict the survival of a single patient, both
discrimination and calibration are important performance metrics. Here we
present Discrete Calibrated Survival (DCS), a novel deep neural network for
discriminated and calibrated survival prediction that outperforms competing
survival models in discrimination on three medical datasets, while achieving
best calibration among all discrete time models. The enhanced performance of
DCS can be attributed to two novel features, the variable temporal output node
spacing and the novel loss term that optimizes the use of uncensored and
censored patient data. We believe that DCS is an important step towards
clinical application of deep-learning-based survival prediction with
state-of-the-art discrimination and good calibration.",https://github.com/imsb-uke/dcsurv,-1
82f69fb6-7d5c-4c26-97a8-86e1f0e2ec18,Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields,0.512924,"Image translation and manipulation have gain increasing attention along with
the rapid development of deep generative models. Although existing approaches
have brought impressive results, they mainly operated in 2D space. In light of
recent advances in NeRF-based 3D-aware generative models, we introduce a new
task, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene
modelled by NeRF, conditioned on one single-view semantic mask as input. To
kick-off this novel task, we propose the Sem2NeRF framework. In particular,
Sem2NeRF addresses the highly challenging task by encoding the semantic mask
into the latent code that controls the 3D scene representation of a pre-trained
decoder. To further improve the accuracy of the mapping, we integrate a new
region-aware learning strategy into the design of both the encoder and the
decoder. We verify the efficacy of the proposed Sem2NeRF and demonstrate that
it outperforms several strong baselines on two benchmark datasets. Code and
video are available at https://donydchen.github.io/sem2nerf/",https://donydchen.github.io/sem2nerf/,-1
97f81bd5-e905-4233-b453-adf9695e440c,Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks,0.775642,"Recently, contrastive learning-based image translation methods have been
proposed, which contrasts different spatial locations to enhance the spatial
correspondence. However, the methods often ignore the diverse semantic relation
within the images. To address this, here we propose a novel semantic relation
consistency (SRC) regularization along with the decoupled contrastive learning,
which utilize the diverse semantics by focusing on the heterogeneous semantics
between the image patches of a single image. To further improve the
performance, we present a hard negative mining by exploiting the semantic
relation. We verified our method for three tasks: single-modal and multi-modal
image translations, and GAN compression task for image translation.
Experimental results confirmed the state-of-art performance of our method in
all the three tasks.",https://github.com/mit-han-lab/gan-compression,-1
8b863fa1-c62b-40ba-9c48-f664823c79b9,Wireless Ad Hoc Federated Learning: A Fully Distributed Cooperative Machine Learning,0.334296,"Privacy-sensitive data is stored in autonomous vehicles, smart devices, or
sensor nodes that can move around with making opportunistic contact with each
other. Federation among such nodes was mainly discussed in the context of
federated learning with a centralized mechanism in many works. However, because
of multi-vendor issues, those nodes do not want to rely on a specific server
operated by a third party for this purpose. In this paper, we propose a
wireless ad hoc federated learning (WAFL) -- a fully distributed cooperative
machine learning organized by the nodes physically nearby. WAFL can develop
generalized models from Non-IID datasets stored in distributed nodes locally by
exchanging and aggregating them with each other over opportunistic node-to-node
contacts. In our benchmark-based evaluation with various opportunistic
networks, WAFL has achieved higher accuracy of 94.8-96.3% than the
self-training case of 84.7%. All our evaluation results show that WAFL can
train and converge the model parameters from highly-partitioned Non-IID
datasets over opportunistic networks without any centralized mechanisms.",None,-1
3f9b1714-323c-4f4f-ae34-97fbba49f5a9,FvOR: Robust Joint Shape and Pose Optimization for Few-view Object Reconstruction,0.816554,"Reconstructing an accurate 3D object model from a few image observations
remains a challenging problem in computer vision. State-of-the-art approaches
typically assume accurate camera poses as input, which could be difficult to
obtain in realistic settings. In this paper, we present FvOR, a learning-based
object reconstruction method that predicts accurate 3D models given a few
images with noisy input poses. The core of our approach is a fast and robust
multi-view reconstruction algorithm to jointly refine 3D geometry and camera
pose estimation using learnable neural network modules. We provide a thorough
benchmark of state-of-the-art approaches for this problem on ShapeNet. Our
approach achieves best-in-class results. It is also two orders of magnitude
faster than the recent optimization-based approach IDR. Our code is released at
\url{https://github.com/zhenpeiyang/FvOR/}",https://github.com/zhenpeiyang/FvOR/,-1
3a115f40-130d-4b7a-84b5-fb3f6b83f168,Neural-Symbolic Entangled Framework for Complex Query Answering,0.835968,"Answering complex queries over knowledge graphs (KG) is an important yet
challenging task because of the KG incompleteness issue and cascading errors
during reasoning. Recent query embedding (QE) approaches to embed the entities
and relations in a KG and the first-order logic (FOL) queries into a low
dimensional space, answering queries by dense similarity search. However,
previous works mainly concentrate on the target answers, ignoring intermediate
entities' usefulness, which is essential for relieving the cascading error
problem in logical query answering. In addition, these methods are usually
designed with their own geometric or distributional embeddings to handle
logical operators like union, intersection, and negation, with the sacrifice of
the accuracy of the basic operator - projection, and they could not absorb
other embedding methods to their models. In this work, we propose a Neural and
Symbolic Entangled framework (ENeSy) for complex query answering, which enables
the neural and symbolic reasoning to enhance each other to alleviate the
cascading error and KG incompleteness. The projection operator in ENeSy could
be any embedding method with the capability of link prediction, and the other
FOL operators are handled without parameters. With both neural and symbolic
reasoning results contained, ENeSy answers queries in ensembles. ENeSy achieves
the SOTA performance on several benchmarks, especially in the setting of the
training model only with the link prediction task.",None,-1
30885cf3-56b3-4c27-a973-85635f62cee0,An Intermediate-level Attack Framework on The Basis of Linear Regression,0.420282,"This paper substantially extends our work published at ECCV, in which an
intermediate-level attack was proposed to improve the transferability of some
baseline adversarial examples. Specifically, we advocate a framework in which a
direct linear mapping from the intermediate-level discrepancies (between
adversarial features and benign features) to prediction loss of the adversarial
example is established. By delving deep into the core components of such a
framework, we show that 1) a variety of linear regression models can all be
considered in order to establish the mapping, 2) the magnitude of the finally
obtained intermediate-level adversarial discrepancy is correlated with the
transferability, 3) further boost of the performance can be achieved by
performing multiple runs of the baseline attack with random initialization. In
addition, by leveraging these findings, we achieve new state-of-the-arts on
transfer-based $\ell_\infty$ and $\ell_2$ attacks. Our code is publicly
available at https://github.com/qizhangli/ila-plus-plus-lr.",https://github.com/qizhangli/ila-plus-plus-lr,60691
9501737a-b4b5-453e-be78-67224436afaf,Self-Supervision Can Be a Good Few-Shot Learner,0.756771,"Existing few-shot learning (FSL) methods rely on training with a large
labeled dataset, which prevents them from leveraging abundant unlabeled data.
From an information-theoretic perspective, we propose an effective unsupervised
FSL method, learning representations with self-supervision. Following the
InfoMax principle, our method learns comprehensive representations by capturing
the intrinsic structure of the data. Specifically, we maximize the mutual
information (MI) of instances and their representations with a low-bias MI
estimator to perform self-supervised pre-training. Rather than supervised
pre-training focusing on the discriminable features of the seen classes, our
self-supervised model has less bias toward the seen classes, resulting in
better generalization for unseen classes. We explain that supervised
pre-training and self-supervised pre-training are actually maximizing different
MI objectives. Extensive experiments are further conducted to analyze their FSL
performance with various training settings. Surprisingly, the results show that
self-supervised pre-training can outperform supervised pre-training under the
appropriate conditions. Compared with state-of-the-art FSL methods, our
approach achieves comparable performance on widely used FSL benchmarks without
any labels of the base classes.",None,-1
89274b28-cbb8-4f97-837c-4349cfab7d9e,LibertyMFD: A Lexicon to Assess the Moral Foundation of Liberty,0.239324,"Quantifying the moral narratives expressed in the user-generated text, news,
or public discourses is fundamental for understanding individuals' concerns and
viewpoints and preventing violent protests and social polarisation. The Moral
Foundation Theory (MFT) was developed to operationalise morality in a
five-dimensional scale system. Recent developments of the theory urged for the
introduction of a new foundation, the Liberty Foundation. Being only recently
added to the theory, there are no available linguistic resources to assess
whether liberty is present in text corpora. Given its importance to current
social issues such as the vaccination debate, we propose two data-driven
approaches, deriving two candidate lexicons generated based on aligned
documents from online news sources with different worldviews. After extensive
experimentation, we contribute to the research community a novel lexicon that
assesses the liberty moral foundation in the way individuals with contrasting
viewpoints express themselves through written text. The LibertyMFD dictionary
can be a valuable tool for policymakers to understand diverse viewpoints on
controversial social issues such as vaccination, abortion, or even uprisings,
as they happen and on a large scale.",None,-1
b854bdac-427e-4d55-a647-b0e496d29f73,Forest and Water Bodies Segmentation Through Satellite Images Using U-Net,0.378105,"Global environment monitoring is a task that requires additional attention in
the contemporary rapid climate change environment. This includes monitoring the
rate of deforestation and areas affected by flooding. Satellite imaging has
greatly helped monitor the earth, and deep learning techniques have helped to
automate this monitoring process. This paper proposes a solution for observing
the area covered by the forest and water. To achieve this task UNet model has
been proposed, which is an image segmentation model. The model achieved a
validation accuracy of 82.55% and 82.92% for the segmentation of areas covered
by forest and water, respectively.",None,-1
1f52bfd0-52b5-40d3-829e-077a6cedb91f,Neural Volumetric Object Selection,0.960087,"We introduce an approach for selecting objects in neural volumetric 3D
representations, such as multi-plane images (MPI) and neural radiance fields
(NeRF). Our approach takes a set of foreground and background 2D user scribbles
in one view and automatically estimates a 3D segmentation of the desired
object, which can be rendered into novel views. To achieve this result, we
propose a novel voxel feature embedding that incorporates the neural volumetric
3D representation and multi-view image features from all input views. To
evaluate our approach, we introduce a new dataset of human-provided
segmentation masks for depicted objects in real-world multi-view scene
captures. We show that our approach out-performs strong baselines, including 2D
segmentation and 3D segmentation approaches adapted to our task.",https://jason718.github.io/nvos,-1
6d419bb8-73a7-4400-a233-5d3755488359,Neural Forecasting of the Italian Sovereign Bond Market with Economic News,0.101968,"In this paper we employ economic news within a neural network framework to
forecast the Italian 10-year interest rate spread. We use a big, open-source,
database known as Global Database of Events, Language and Tone to extract
topical and emotional news content linked to bond markets dynamics. We deploy
such information within a probabilistic forecasting framework with
autoregressive recurrent networks (DeepAR). Our findings suggest that a deep
learning network based on Long-Short Term Memory cells outperforms classical
machine learning techniques and provides a forecasting performance that is over
and above that obtained by using conventional determinants of interest rates
alone.",None,-1
b31d7c4a-5535-42c5-8547-e408cec2a64e,Differentially Private Counterfactuals via Functional Mechanism,0.264414,"Counterfactual, serving as one emerging type of model explanation, has
attracted tons of attentions recently from both industry and academia.
Different from the conventional feature-based explanations (e.g.,
attributions), counterfactuals are a series of hypothetical samples which can
flip model decisions with minimal perturbations on queries. Given valid
counterfactuals, humans are capable of reasoning under ``what-if''
circumstances, so as to better understand the model decision boundaries.
However, releasing counterfactuals could be detrimental, since it may
unintentionally leak sensitive information to adversaries, which brings about
higher risks on both model security and data privacy. To bridge the gap, in
this paper, we propose a novel framework to generate differentially private
counterfactual (DPC) without touching the deployed model or explanation set,
where noises are injected for protection while maintaining the explanation
roles of counterfactual. In particular, we train an autoencoder with the
functional mechanism to construct noisy class prototypes, and then derive the
DPC from the latent prototypes based on the post-processing immunity of
differential privacy. Further evaluations demonstrate the effectiveness of the
proposed framework, showing that DPC can successfully relieve the risks on both
extraction and inference attacks.",None,-1
1b138b93-2446-4b6d-a060-990cd718d8dd,Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition,0.0769842,"The choice of modeling units is crucial for automatic speech recognition
(ASR) tasks. In mandarin scenarios, the Chinese characters represent meaning
but are not directly related to the pronunciation. Thus only considering the
writing of Chinese characters as modeling units is insufficient to capture
speech features. In this paper, we present a novel method involves with
multi-level modeling units, which integrates multi-level information for
mandarin speech recognition. Specifically, the encoder block considers
syllables as modeling units and the decoder block deals with character-level
modeling units. To facilitate the incremental conversion from syllable features
to character features, we design an auxiliary task that applies cross-entropy
(CE) loss to intermediate decoder layers. During inference, the input feature
sequences are converted into syllable sequences by the encoder block and then
converted into Chinese characters by the decoder block. Experiments on the
widely used AISHELL-1 corpus demonstrate that our method achieves promising
results with CER of 4.1%/4.6% and 4.6%/5.2%, using the Conformer and the
Transformer backbones respectively.",https://github.com/mozillazg/python-pinyin,11
f8edd77b-fdd6-41cf-91ad-e008f55ac441,Deep Learning for Hate Speech Detection: A Comparative Study,0.919224,"Automated hate speech detection is an important tool in combating the spread
of hate speech, particularly in social media. Numerous methods have been
developed for the task, including a recent proliferation of deep-learning based
approaches. A variety of datasets have also been developed, exemplifying
various manifestations of the hate-speech detection problem. We present here a
large-scale empirical comparison of deep and shallow hate-speech detection
methods, mediated through the three most commonly used datasets. Our goal is to
illuminate progress in the area, and identify strengths and weaknesses in the
current state-of-the-art. We particularly focus our analysis on measures of
practical performance, including detection accuracy, computational efficiency,
capability in using pre-trained models, and domain generalization. In doing so
we aim to provide guidance as to the use of hate-speech detection in practice,
quantify the state-of-the-art, and identify future research directions. Code
and dataset are available at
https://github.com/jmjmalik22/Hate-Speech-Detection.",https://github.com/jmjmalik22/Hate-Speech-Detection,-1
fc15111a-d762-46d8-a36e-48bbcf117e6a,Context Sensing Attention Network for Video-based Person Re-identification,0.31867,"Video-based person re-identification (ReID) is challenging due to the
presence of various interferences in video frames. Recent approaches handle
this problem using temporal aggregation strategies. In this work, we propose a
novel Context Sensing Attention Network (CSA-Net), which improves both the
frame feature extraction and temporal aggregation steps. First, we introduce
the Context Sensing Channel Attention (CSCA) module, which emphasizes responses
from informative channels for each frame. These informative channels are
identified with reference not only to each individual frame, but also to the
content of the entire sequence. Therefore, CSCA explores both the individuality
of each frame and the global context of the sequence. Second, we propose the
Contrastive Feature Aggregation (CFA) module, which predicts frame weights for
temporal aggregation. Here, the weight for each frame is determined in a
contrastive manner: i.e., not only by the quality of each individual frame, but
also by the average quality of the other frames in a sequence. Therefore, it
effectively promotes the contribution of relatively good frames. Extensive
experimental results on four datasets show that CSA-Net consistently achieves
state-of-the-art performance.",None,-1
7a32b707-3675-4118-88b5-3c45f68e244f,Entailment Graph Learning with Textual Entailment and Soft Transitivity,0.306216,"Typed entailment graphs try to learn the entailment relations between
predicates from text and model them as edges between predicate nodes. The
construction of entailment graphs usually suffers from severe sparsity and
unreliability of distributional similarity. We propose a two-stage method,
Entailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns
local entailment relations by recognizing possible textual entailment between
template sentences formed by typed CCG-parsed predicates. Based on the
generated local graph, EGT2 then uses three novel soft transitivity constraints
to consider the logical transitivity in entailment structures. Experiments on
benchmark datasets show that EGT2 can well model the transitivity in entailment
graph to alleviate the sparsity issue, and lead to significant improvement over
current state-of-the-art methods.",https://github.com/ZacharyChenpk/EGT2,-1
d42457a6-fac7-402c-8139-41cb857cf41a,OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction,0.750208,"Learning how humans manipulate objects requires machines to acquire knowledge
from two perspectives: one for understanding object affordances and the other
for learning human's interactions based on the affordances. Even though these
two knowledge bases are crucial, we find that current databases lack a
comprehensive awareness of them. In this work, we propose a multi-modal and
rich-annotated knowledge repository, OakInk, for visual and cognitive
understanding of hand-object interactions. We start to collect 1,800 common
household objects and annotate their affordances to construct the first
knowledge base: Oak. Given the affordance, we record rich human interactions
with 100 selected objects in Oak. Finally, we transfer the interactions on the
100 recorded objects to their virtual counterparts through a novel method:
Tink. The recorded and transferred hand-object interactions constitute the
second knowledge base: Ink. As a result, OakInk contains 50,000 distinct
affordance-aware and intent-oriented hand-object interactions. We benchmark
OakInk on pose estimation and grasp generation tasks. Moreover, we propose two
practical applications of OakInk: intent-based interaction generation and
handover generation. Our datasets and source code are publicly available at
https://github.com/lixiny/OakInk.",https://github.com/lixiny/OakInk,-1
46e3d9db-71dd-437b-9840-4c570567179f,Contrastive Bayesian Analysis for Deep Metric Learning,0.42565,"Recent methods for deep metric learning have been focusing on designing
different contrastive loss functions between positive and negative pairs of
samples so that the learned feature embedding is able to pull positive samples
of the same class closer and push negative samples from different classes away
from each other. In this work, we recognize that there is a significant
semantic gap between features at the intermediate feature layer and class
labels at the final output layer. To bridge this gap, we develop a contrastive
Bayesian analysis to characterize and model the posterior probabilities of
image labels conditioned by their features similarity in a contrastive learning
setting. This contrastive Bayesian analysis leads to a new loss function for
deep metric learning. To improve the generalization capability of the proposed
method onto new classes, we further extend the contrastive Bayesian loss with a
metric variance constraint. Our experimental results and ablation studies
demonstrate that the proposed contrastive Bayesian metric learning method
significantly improves the performance of deep metric learning in both
supervised and pseudo-supervised scenarios, outperforming existing methods by a
large margin.",https://github.com/kanshichao/CBML,-1
7f8cddeb-07a1-4d70-964e-06e547f0bd0d,Assessing Digital Language Support on a Global Scale,0.551968,"The users of endangered languages struggle to thrive in a digitally-mediated
world. We have developed an automated method for assessing how well every
language recognized by ISO 639 is faring in terms of digital language support.
The assessment is based on scraping the names of supported languages from the
websites of 143 digital tools selected to represent a full range of ways that
digital technology can support languages. The method uses Mokken scale analysis
to produce an explainable model for quantifying digital language support and
monitoring it on a global scale.",https://github.com/sil-ai/dls-results,-1
1eff7b31-2e1a-4c7d-9e93-7d6d1f71ccb6,OCR Improves Machine Translation for Low-Resource Languages,0.789373,"We aim to investigate the performance of current OCR systems on low resource
languages and low resource scripts. We introduce and make publicly available a
novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with
noise, for 60 low-resource languages in low resource scripts. We evaluate
state-of-the-art OCR systems on our benchmark and analyse most common errors.
We show that OCR monolingual data is a valuable resource that can increase
performance of Machine Translation models, when used in backtranslation. We
then perform an ablation study to investigate how OCR errors impact Machine
Translation performance and determine what is the minimum level of OCR quality
needed for the monolingual data to be useful for Machine Translation.",https://github.com/facebookresearch/flores,-1
6b0e9330-8faf-4191-a7d6-e248657bc590,Learning Feynman Diagrams using Graph Neural Networks,0.0457752,"In the wake of the growing popularity of machine learning in particle
physics, this work finds a new application of geometric deep learning on
Feynman diagrams to make accurate and fast matrix element predictions with the
potential to be used in analysis of quantum field theory. This research uses
the graph attention layer which makes matrix element predictions to 1
significant figure accuracy above 90% of the time. Peak performance was
achieved in making predictions to 3 significant figure accuracy over 10% of the
time with less than 200 epochs of training, serving as a proof of concept on
which future works can build upon for better performance. Finally, a procedure
is suggested, to use the network to make advancements in quantum field theory
by constructing Feynman diagrams with effective particles that represent
non-perturbative calculations.",https://github.com/Clearbloo/Feynman_GNN.git,-1
7949e384-e5d6-4ae3-99e8-4fc6e1e06196,Imagination-Augmented Natural Language Understanding,0.86367,"Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances.",https://github.com/YujieLu10/IACE-NLU,-1
62451656-fd5d-49ac-b489-a1315f005cd2,Are Neighbors Enough? Multi-Head Neural n-gram can be Alternative to Self-attention,0.139292,"Impressive performance of Transformer has been attributed to self-attention,
where dependencies between entire input in a sequence are considered at every
position. In this work, we reform the neural $n$-gram model, which focuses on
only several surrounding representations of each position, with the multi-head
mechanism as in Vaswani et al.(2017). Through experiments on
sequence-to-sequence tasks, we show that replacing self-attention in
Transformer with multi-head neural $n$-gram can achieve comparable or better
performance than Transformer. From various analyses on our proposed method, we
find that multi-head neural $n$-gram is complementary to self-attention, and
their combinations can further improve performance of vanilla Transformer.",None,-1
4c561c1e-0c94-42ec-84e5-cc5f27e4f03f,Comparative layer-wise analysis of self-supervised speech models,0.837552,"Many self-supervised speech models, varying in their pre-training objective,
input modality, and pre-training data, have been proposed in the last few
years. Despite impressive successes on downstream tasks, we still have a
limited understanding of the properties encoded by the models and the
differences across models. In this work, we examine the intermediate
representations for a variety of recent models. Specifically, we measure
acoustic, phonetic, and word-level properties encoded in individual layers,
using a lightweight analysis tool based on canonical correlation analysis
(CCA). We find that these properties evolve across layers differently depending
on the model, and the variations relate to the choice of pre-training
objective. We further investigate the utility of our analyses for downstream
tasks by comparing the property trends with performance on speech recognition
and spoken language understanding tasks. We discover that CCA trends provide
reliable guidance to choose layers of interest for downstream tasks and that
single-layer performance often matches or improves upon using all layers,
suggesting implications for more efficient use of pre-trained models.",https://github.com/ankitapasad/layerwise-analysis/,-1
af87a928-bf44-45c3-8c01-ce2cebe6f6a4,Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences,0.635942,"Generating complex behaviors that satisfy the preferences of non-expert users
is a crucial requirement for AI agents. Interactive reward learning from
trajectory comparisons (a.k.a. RLHF) is one way to allow non-expert users to
convey complex objectives by expressing preferences over short clips of agent
behaviors. Even though this parametric method can encode complex tacit
knowledge present in the underlying tasks, it implicitly assumes that the human
is unable to provide richer feedback than binary preference labels, leading to
intolerably high feedback complexity and poor user experience. While providing
a detailed symbolic closed-form specification of the objectives might be
tempting, it is not always feasible even for an expert user. However, in most
cases, humans are aware of how the agent should change its behavior along
meaningful axes to fulfill their underlying purpose, even if they are not able
to fully specify task objectives symbolically. Using this as motivation, we
introduce the notion of Relative Behavioral Attributes, which allows the users
to tweak the agent behavior through symbolic concepts (e.g., increasing the
softness or speed of agents' movement). We propose two practical methods that
can learn to model any kind of behavioral attributes from ordered behavior
clips. We demonstrate the effectiveness of our methods on four tasks with nine
different behavioral attributes, showing that once the attributes are learned,
end users can produce desirable agent behaviors relatively effortlessly, by
providing feedback just around ten times. This is over an order of magnitude
less than that required by the popular learning-from-human-preferences
baselines. The supplementary video and source code are available at:
https://guansuns.github.io/pages/rba.",None,-1
5c3b46b5-9356-4f0d-9a70-14bf3f86a0fe,InterTrack: Interaction Transformer for 3D Multi-Object Tracking,0.388691,"3D multi-object tracking (MOT) is a key problem for autonomous vehicles,
required to perform well-informed motion planning in dynamic environments.
Particularly for densely occupied scenes, associating existing tracks to new
detections remains challenging as existing systems tend to omit critical
contextual information. Our proposed solution, InterTrack, introduces the
Interaction Transformer for 3D MOT to generate discriminative object
representations for data association. We extract state and shape features for
each track and detection, and efficiently aggregate global information via
attention. We then perform a learned regression on each track/detection feature
pair to estimate affinities, and use a robust two-stage data association and
track management approach to produce the final tracks. We validate our approach
on the nuScenes 3D MOT benchmark, where we observe significant improvements,
particularly on classes with small physical sizes and clustered objects. As of
submission, InterTrack ranks 1st in overall AMOTA among methods using
CenterPoint detections.",https://github.com/open-mmlab/OpenPCDet,-1
1f6a84e8-9ae6-4319-ba78-1301306a047d,FS6D: Few-Shot 6D Pose Estimation of Novel Objects,0.955106,"6D object pose estimation networks are limited in their capability to scale
to large numbers of object instances due to the close-set assumption and their
reliance on high-fidelity object CAD models. In this work, we study a new open
set problem; the few-shot 6D object poses estimation: estimating the 6D pose of
an unknown object by a few support views without extra training. To tackle the
problem, we point out the importance of fully exploring the appearance and
geometric relationship between the given support views and query scene patches
and propose a dense prototypes matching framework by extracting and matching
dense RGBD prototypes with transformers. Moreover, we show that the priors from
diverse appearances and shapes are crucial to the generalization capability
under the problem setting and thus propose a large-scale RGBD photorealistic
dataset (ShapeNet6D) for network pre-training. A simple and effective online
texture blending approach is also introduced to eliminate the domain gap from
the synthesis dataset, which enriches appearance diversity at a low cost.
Finally, we discuss possible solutions to this problem and establish benchmarks
on popular datasets to facilitate future research. The project page is at
\url{https://fs6d.github.io/}.",None,-1
de2d3667-e702-4c9f-929a-49acad8a0fff,SecureBERT: A Domain-Specific Language Model for Cybersecurity,0.424602,"Natural Language Processing (NLP) has recently gained wide attention in
cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber
automation. Increased connection and automation have revolutionized the world's
economic and cultural infrastructures, while they have introduced risks in
terms of cyber attacks. CTI is information that helps cybersecurity analysts
make intelligent security decisions, that is often delivered in the form of
natural language text, which must be transformed to machine readable format
through an automated procedure before it can be used for automated security
measures.
  This paper proposes SecureBERT, a cybersecurity language model capable of
capturing text connotations in cybersecurity text (e.g., CTI) and therefore
successful in automation for many critical cybersecurity tasks that would
otherwise rely on human expertise and time-consuming manual efforts. SecureBERT
has been trained using a large corpus of cybersecurity text.To make SecureBERT
effective not just in retaining general English understanding, but also when
applied to text with cybersecurity implications, we developed a customized
tokenizer as well as a method to alter pre-trained weights. The SecureBERT is
evaluated using the standard Masked Language Model (MLM) test as well as two
additional standard NLP tasks. Our evaluation studies show that
SecureBERT\footnote{\url{https://github.com/ehsanaghaei/SecureBERT}}
outperforms existing similar models, confirming its capability for solving
crucial NLP tasks in cybersecurity.",https://github.com/ehsanaghaei/SecureBERT,-1
cc1df209-a0ac-4201-8802-52c951c585f8,V-Doc : Visual questions answers with Documents,0.453017,"We propose V-Doc, a question-answering tool using document images and PDF,
mainly for researchers and general non-deep learning experts looking to
generate, process, and understand the document visual question answering tasks.
The V-Doc supports generating and using both extractive and abstractive
question-answer pairs using documents images. The extractive QA selects a
subset of tokens or phrases from the document contents to predict the answers,
while the abstractive QA recognises the language in the content and generates
the answer based on the trained model. Both aspects are crucial to
understanding the documents, especially in an image format. We include a
detailed scenario of question generation for the abstractive QA task. V-Doc
supports a wide range of datasets and models, and is highly extensible through
a declarative, framework-agnostic platform.",https://github.com/usydnlp/vdoc,-1
8481af7d-e8df-4264-944e-51a11b8a26fe,On the Role of Bidirectionality in Language Model Pre-Training,0.557968,"Prior work on language model pre-training has explored different
architectures and learning objectives, but differences in data, hyperparameters
and evaluation make a principled comparison difficult. In this work, we focus
on bidirectionality as a key factor that differentiates existing approaches,
and present a comprehensive study of its role in next token prediction, text
infilling, zero-shot priming and fine-tuning. We propose a new framework that
generalizes prior approaches, including fully unidirectional models like GPT,
fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.
Our framework distinguishes between two notions of bidirectionality
(bidirectional context and bidirectional attention) and allows us to control
each of them separately. We find that the optimal configuration is largely
application-dependent (e.g., bidirectional attention is beneficial for
fine-tuning and infilling, but harmful for next token prediction and zero-shot
priming). We train models with up to 6.7B parameters, and find differences to
remain consistent at scale. While prior work on scaling has focused on
left-to-right autoregressive models, our results suggest that this approach
comes with some trade-offs, and it might be worthwhile to develop very large
bidirectional models.",None,-1
1f8d4153-213d-4bea-9f16-94651ec273b3,Cyberbullying detection across social media platforms via platform-aware adversarial encoding,0.808821,"Despite the increasing interest in cyberbullying detection, existing efforts
have largely been limited to experiments on a single platform and their
generalisability across different social media platforms have received less
attention. We propose XP-CB, a novel cross-platform framework based on
Transformers and adversarial learning. XP-CB can enhance a Transformer
leveraging unlabelled data from the source and target platforms to come up with
a common representation while preventing platform-specific training. To
validate our proposed framework, we experiment on cyberbullying datasets from
three different platforms through six cross-platform configurations, showing
its effectiveness with both BERT and RoBERTa as the underlying Transformer
models.",None,-1
39cf20bc-bed7-4504-b38e-ac83ba63987a,Knowledge Base Question Answering: A Semantic Parsing Perspective,0.56986,"Recent advances in deep learning have greatly propelled the research on
semantic parsing. Improvement has since been made in many downstream tasks,
including natural language interface to web APIs, text-to-SQL generation, among
others. However, despite the close connection shared with these tasks, research
on question answering over knowledge bases (KBQA) has comparatively been
progressing slowly. We identify and attribute this to two unique challenges of
KBQA, schema-level complexity and fact-level complexity. In this survey, we
situate KBQA in the broader literature of semantic parsing and give a
comprehensive account of how existing KBQA approaches attempt to address the
unique challenges. Regardless of the unique challenges, we argue that we can
still take much inspiration from the literature of semantic parsing, which has
been overlooked by existing research on KBQA. Based on our discussion, we can
better understand the bottleneck of current KBQA research and shed light on
promising directions for KBQA to keep up with the literature of semantic
parsing, particularly in the era of pre-trained language models.",None,-1
29c7ba75-7e78-45b6-ac08-3605c87f9a0b,Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes,0.0237169,"Policy gradient (PG) is a reinforcement learning (RL) approach that optimizes
a parameterized policy model for an expected return using gradient ascent.
Given a well-parameterized policy model, such as a neural network model, with
appropriate initial parameters, the PG algorithms work well even when
environment does not have the Markov property. Otherwise, they can be trapped
on a plateau or suffer from peakiness effects. As another successful RL
approach, algorithms based on Monte-Carlo Tree Search (MCTS), which include
AlphaZero, have obtained groundbreaking results especially on the board game
playing domain. They are also suitable to be applied to non-Markov decision
processes. However, since the standard MCTS does not have the ability to learn
state representation, the size of the tree-search space can be too large to
search. In this work, we examine a mixture policy of PG and MCTS to complement
each other's difficulties and take advantage of them. We derive conditions for
asymptotic convergence with results of a two-timescale stochastic approximation
and propose an algorithm that satisfies these conditions. The effectivity of
the proposed methods is verified through numerical experiments on non-Markov
decision processes.",None,1194
82f7f845-bf9c-415d-821a-81eec7763cfb,Face Detection on Mobile: Five Implementations and Analysis,0.114448,"In many practical cases face detection on smartphones or other highly
portable devices is a necessity. Applications include mobile face access
control systems, driver status tracking, emotion recognition, etc. Mobile
devices have limited processing power and should have long-enough battery life
even with face detection application running. Thus, striking the right balance
between algorithm quality and complexity is crucial. In this work we adapt 5
algorithms to mobile. These algorithms are based on handcrafted or
neural-network-based features and include: Viola-Jones (Haar cascade), LBP,
HOG, MTCNN, BlazeFace. We analyze inference time of these algorithms on
different devices with different input image resolutions. We provide guidance,
which algorithms are the best fit for mobile face access control systems and
potentially other mobile applications. Interestingly, we note that cascaded
algorithms perform faster on scenes without faces, while BlazeFace is slower on
empty scenes. Exploiting this behavior might be useful in practice.",None,-1
13506174-90db-42a8-8d10-4d3bd3dfd8a2,Explaining Causal Models with Argumentation: the Case of Bi-variate Reinforcement,0.0662542,"Causal models are playing an increasingly important role in machine learning,
particularly in the realm of explainable AI. We introduce a conceptualisation
for generating argumentation frameworks (AFs) from causal models for the
purpose of forging explanations for the models' outputs. The conceptualisation
is based on reinterpreting desirable properties of semantics of AFs as
explanation moulds, which are means for characterising the relations in the
causal model argumentatively. We demonstrate our methodology by reinterpreting
the property of bi-variate reinforcement as an explanation mould to forge
bipolar AFs as explanations for the outputs of causal models. We perform a
theoretical evaluation of these argumentative explanations, examining whether
they satisfy a range of desirable explanatory and argumentative properties.",None,12536
7fc7e745-a713-4452-8c21-4093e8c7e7fa,Reconciling Security and Communication Efficiency in Federated Learning,0.220262,"Cross-device Federated Learning is an increasingly popular machine learning
setting to train a model by leveraging a large population of client devices
with high privacy and security guarantees. However, communication efficiency
remains a major bottleneck when scaling federated learning to production
environments, particularly due to bandwidth constraints during uplink
communication. In this paper, we formalize and address the problem of
compressing client-to-server model updates under the Secure Aggregation
primitive, a core component of Federated Learning pipelines that allows the
server to aggregate the client updates without accessing them individually. In
particular, we adapt standard scalar quantization and pruning methods to Secure
Aggregation and propose Secure Indexing, a variant of Secure Aggregation that
supports quantization for extreme compression. We establish state-of-the-art
results on LEAF benchmarks in a secure Federated Learning setup with up to
40$\times$ compression in uplink communication with no meaningful loss in
utility compared to uncompressed baselines.",https://github.com/facebookresearch/SecureFLCompression,-1
a0ffac54-fffa-4609-a59e-54dc05fe105a,MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure,0.127134,"In this paper, we propose a comprehensive benchmark to investigate models'
logical reasoning capabilities in complex real-life scenarios. Current
explanation datasets often employ synthetic data with simple reasoning
structures. Therefore, it cannot express more complex reasoning processes, such
as the rebuttal to a reasoning step and the degree of certainty of the
evidence. To this end, we propose a comprehensive logical reasoning explanation
form. Based on the multi-hop chain of reasoning, the explanation form includes
three main components: (1) The condition of rebuttal that the reasoning node
can be challenged; (2) Logical formulae that uncover the internal texture of
reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The
fine-grained structure conforms to the real logical reasoning scenario, better
fitting the human cognitive process but, simultaneously, is more challenging
for the current models. We evaluate the current best models' performance on
this new explanation form. The experimental results show that generating
reasoning graphs remains a challenging task for current models, even with the
help of giant pre-trained language models.",https://github.com/tencent-ailab/MetaLogic,-1
6d20e9f5-536f-43d5-9c9f-c96b77794487,Delta Distillation for Efficient Video Processing,0.279359,"This paper aims to accelerate video stream processing, such as object
detection and semantic segmentation, by leveraging the temporal redundancies
that exist between video frames. Instead of propagating and warping features
using motion alignment, such as optical flow, we propose a novel knowledge
distillation schema coined as Delta Distillation. In our proposal, the student
learns the variations in the teacher's intermediate features over time. We
demonstrate that these temporal variations can be effectively distilled due to
the temporal redundancies within video frames. During inference, both teacher
and student cooperate for providing predictions: the former by providing
initial representations extracted only on the key-frame, and the latter by
iteratively estimating and applying deltas for the successive frames. Moreover,
we consider various design choices to learn optimal student architectures
including an end-to-end learnable architecture search. By extensive experiments
on a wide range of architectures, including the most efficient ones, we
demonstrate that delta distillation sets a new state of the art in terms of
accuracy vs. efficiency trade-off for semantic segmentation and object
detection in videos. Finally, we show that, as a by-product, delta distillation
improves the temporal consistency of the teacher model.",None,-1
250d3dd7-2e16-459f-9a21-7e637a662ad6,Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms,0.646242,"Attention has become one of the most commonly used mechanisms in deep
learning approaches. The attention mechanism can help the system focus more on
the feature space's critical regions. For example, high amplitude regions can
play an important role for Speech Emotion Recognition (SER). In this paper, we
identify misalignments between the attention and the signal amplitude in the
existing multi-head self-attention. To improve the attention area, we propose
to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA)
mechanism in combination with the multi-head self-attention. Through the FA
mechanism, the network can detect the largest amplitude part in the segment. By
employing the CA mechanism, the network can modulate the information flow by
assigning different weights to each attention head and improve the utilization
of surrounding contexts. To evaluate the proposed method, experiments are
performed with the IEMOCAP and RAVDESS datasets. Experimental results show that
the proposed framework significantly outperforms the state-of-the-art
approaches on both datasets.",None,5129
55b6a54f-43a3-4016-9d17-020ba64b6a96,Do LSTMs See Gender? Probing the Ability of LSTMs to Learn Abstract Syntactic Rules,0.292948,"LSTMs trained on next-word prediction can accurately perform linguistic tasks
that require tracking long-distance syntactic dependencies. Notably, model
accuracy approaches human performance on number agreement tasks (Gulordava et
al., 2018). However, we do not have a mechanistic understanding of how LSTMs
perform such linguistic tasks. Do LSTMs learn abstract grammatical rules, or do
they rely on simple heuristics? Here, we test gender agreement in French which
requires tracking both hierarchical syntactic structures and the inherent
gender of lexical units. Our model is able to reliably predict long-distance
gender agreement in two subject-predicate contexts: noun-adjective and
noun-passive-verb agreement. The model showed more inaccuracies on plural noun
phrases with gender attractors compared to singular cases, suggesting a
reliance on clues from gendered articles for agreement. Overall, our study
highlights key ways in which LSTMs deviate from human behaviour and questions
whether LSTMs genuinely learn abstract syntactic rules and categories. We
propose using gender agreement as a useful probe to investigate the underlying
mechanisms, internal representations, and linguistic capabilities of LSTM
language models.",None,-1
27bac5df-412a-4e75-9f92-def875c1a26a,Editable Indoor Lighting Estimation,0.638021,"We present a method for estimating lighting from a single perspective image
of an indoor scene. Previous methods for predicting indoor illumination usually
focus on either simple, parametric lighting that lack realism, or on richer
representations that are difficult or even impossible to understand or modify
after prediction. We propose a pipeline that estimates a parametric light that
is easy to edit and allows renderings with strong shadows, alongside with a
non-parametric texture with high-frequency information necessary for realistic
rendering of specular objects. Once estimated, the predictions obtained with
our model are interpretable and can easily be modified by an artist/user with a
few mouse clicks. Quantitative and qualitative results show that our approach
makes indoor lighting estimation easier to handle by a casual user, while still
producing competitive results.",None,6586
8a1104d4-4357-4a8d-b626-53344f8f5735,Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning,0.798955,"Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.",https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt,-1
5bf1c502-090f-4e14-ba52-096a3a859027,Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning,0.599205,"Non-exemplar class-incremental learning is to recognize both the old and new
classes when old class samples cannot be saved. It is a challenging task since
representation optimization and feature retention can only be achieved under
supervision from new classes. To address this problem, we propose a novel
self-sustaining representation expansion scheme. Our scheme consists of a
structure reorganization strategy that fuses main-branch expansion and
side-branch updating to maintain the old features, and a main-branch
distillation scheme to transfer the invariant knowledge. Furthermore, a
prototype selection mechanism is proposed to enhance the discrimination between
the old and new classes by selectively incorporating new samples into the
distillation process. Extensive experiments on three benchmarks demonstrate
significant incremental performance, outperforming the state-of-the-art methods
by a margin of 3%, 3% and 6%, respectively.",None,-1
ea81c571-4437-4e9e-89df-14209afff4c2,Error Correction Code Transformer,0.469642,"Error correction code is a major part of the communication physical layer,
ensuring the reliable transfer of data over noisy channels. Recently, neural
decoders were shown to outperform classical decoding techniques. However, the
existing neural approaches present strong overfitting due to the exponential
training complexity, or a restrictive inductive bias due to reliance on Belief
Propagation. Recently, Transformers have become methods of choice in many
applications thanks to their ability to represent complex interactions between
elements. In this work, we propose to extend for the first time the Transformer
architecture to the soft decoding of linear codes at arbitrary block lengths.
We encode each channel's output dimension to high dimension for better
representation of the bits information to be processed separately. The
element-wise processing allows the analysis of the channel output reliability,
while the algebraic code and the interaction between the bits are inserted into
the model via an adapted masked self-attention module. The proposed approach
demonstrates the extreme power and flexibility of Transformers and outperforms
existing state-of-the-art neural decoders by large margins at a fraction of
their time complexity.",https://github.com/yoniLc/ECCT,-1
bcd3b38f-2439-44cd-b9f0-944ade76c2fe,Brain tumor detection using artificial convolutional neural networks,0.565486,"In this paper, a convolutional neural network (CNN) was used to classify NMR
images of human brains with 4 different types of tumors: meningioma, glioma and
pituitary gland tumors. During the training phase of this project, an accuracy
of 100% was obtained, meanwhile, in the evaluation phase the precision was 96%.",None,-1
d04f4984-d888-4d8c-b812-abd6a15832c6,Factual Error Correction for Abstractive Summaries Using Entity Retrieval,0.320511,"Despite the recent advancements in abstractive summarization systems
leveraged from large-scale datasets and pre-trained language models, the
factual correctness of the summary is still insufficient. One line of trials to
mitigate this problem is to include a post-editing process that can detect and
correct factual errors in the summary. In building such a post-editing system,
it is strongly required that 1) the process has a high success rate and
interpretability and 2) has a fast running time. Previous approaches focus on
regeneration of the summary using the autoregressive models, which lack
interpretability and require high computing resources. In this paper, we
propose an efficient factual error correction system RFEC based on entities
retrieval post-editing process. RFEC first retrieves the evidence sentences
from the original document by comparing the sentences with the target summary.
This approach greatly reduces the length of text for a system to analyze. Next,
RFEC detects the entity-level errors in the summaries by considering the
evidence sentences and substitutes the wrong entities with the accurate
entities from the evidence sentences. Experimental results show that our
proposed error correction system shows more competitive performance than
baseline methods in correcting the factual errors with a much faster speed.",None,-1
24764b26-5158-43fb-a389-d0f7608d3f88,A BERT-based Deep Learning Approach for Reputation Analysis in Social Media,0.333257,"Social media has become an essential part of the modern lifestyle, with its
usage being highly prevalent. This has resulted in unprecedented amounts of
data generated from users in social media, such as users' attitudes, opinions,
interests, purchases, and activities across various aspects of their lives.
Therefore, in a world of social media, where its power has shifted to users,
actions taken by companies and public figures are subject to constantly being
under scrutiny by influential global audiences. As a result, reputation
management in social media has become essential as companies and public figures
need to maintain their reputation to preserve their reputation capital.
However, domain experts still face the challenge of lacking appropriate
solutions to automate reliable online reputation analysis. To tackle this
challenge, we proposed a novel reputation analysis approach based on the
popular language model BERT (Bidirectional Encoder Representations from
Transformers). The proposed approach was evaluated on the reputational polarity
task using RepLab 2013 dataset. Compared to previous works, we achieved 5.8%
improvement in accuracy, 26.9% improvement in balanced accuracy, and 21.8%
improvement in terms of F-score.",None,12055
142d3b66-481f-4d57-946c-9b733e3d4e07,InstructPix2Pix: Learning to Follow Image Editing Instructions,1.0,"We propose a method for editing images from human instructions: given an
input image and a written instruction that tells the model what to do, our
model follows these instructions to edit the image. To obtain training data for
this problem, we combine the knowledge of two large pretrained models -- a
language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to
generate a large dataset of image editing examples. Our conditional diffusion
model, InstructPix2Pix, is trained on our generated data, and generalizes to
real images and user-written instructions at inference time. Since it performs
edits in the forward pass and does not require per example fine-tuning or
inversion, our model edits images quickly, in a matter of seconds. We show
compelling editing results for a diverse collection of input images and written
instructions.",None,-1
e538b5d1-44b4-49c1-9e02-721ba6ee4a8c,Testing predictive automated driving systems: lessons learned and future recommendations,0.638137,"Conventional vehicles are certified through classical approaches, where
different physical certification tests are set up on test tracks to assess
required safety levels. These approaches are well suited for vehicles with
limited complexity and limited interactions with other entities as last-second
resources. However, these approaches do not allow to evaluate safety with real
behaviors for critical and edge cases, nor to evaluate the ability to
anticipate them in the mid or long term. This is particularly relevant for
automated and autonomous driving functions that make use of advanced predictive
systems to anticipate future actions and motions to be considered in the path
planning layer. In this paper, we present and analyze the results of physical
tests on proving grounds of several predictive systems in automated driving
functions developed within the framework of the BRAVE project. Based on our
experience in testing predictive automated driving functions, we identify the
main limitations of current physical testing approaches when dealing with
predictive systems, analyze the main challenges ahead, and provide a set of
practical actions and recommendations to consider in future physical testing
procedures for automated and autonomous driving functions.",None,-1
5cfc2302-dd0d-4db4-a254-919cb1926b40,Efficient Remote Photoplethysmography with Temporal Derivative Modules and Time-Shift Invariant Loss,0.66878,"We present a lightweight neural model for remote heart rate estimation
focused on the efficient spatio-temporal learning of facial
photoplethysmography (PPG) based on i) modelling of PPG dynamics by
combinations of multiple convolutional derivatives, and ii) increased
flexibility of the model to learn possible offsets between the facial video PPG
and the ground truth. PPG dynamics are modelled by a Temporal Derivative Module
(TDM) constructed by the incremental aggregation of multiple convolutional
derivatives, emulating a Taylor series expansion up to the desired order.
Robustness to ground truth offsets is handled by the introduction of TALOS
(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based
models. We verify the effectiveness of our model by reporting accuracy and
efficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to
existing models, our approach shows competitive heart rate estimation accuracy
with a much lower number of parameters and lower computational cost.",None,-1
5bdf465f-27c0-497a-9400-8e0a5d7e280c,Box2Seg: Learning Semantics of 3D Point Clouds with Box-Level Supervision,0.46582,"Learning dense point-wise semantics from unstructured 3D point clouds with
fewer labels, although a realistic problem, has been under-explored in
literature. While existing weakly supervised methods can effectively learn
semantics with only a small fraction of point-level annotations, we find that
the vanilla bounding box-level annotation is also informative for semantic
segmentation of large-scale 3D point clouds. In this paper, we introduce a
neural architecture, termed Box2Seg, to learn point-level semantics of 3D point
clouds with bounding box-level supervision. The key to our approach is to
generate accurate pseudo labels by exploring the geometric and topological
structure inside and outside each bounding box. Specifically, an
attention-based self-training (AST) technique and Point Class Activation
Mapping (PCAM) are utilized to estimate pseudo-labels. The network is further
trained and refined with pseudo labels. Experiments on two large-scale
benchmarks including S3DIS and ScanNet demonstrate the competitive performance
of the proposed method. In particular, the proposed network can be trained with
cheap, or even off-the-shelf bounding box-level annotations and subcloud-level
tags.",None,-1
26de8a97-12c7-477a-96a3-b721a444b99e,Semantic Enhanced Text-to-SQL Parsing via Iteratively Learning Schema Linking Graph,0.763441,"The generalizability to new databases is of vital importance to Text-to-SQL
systems which aim to parse human utterances into SQL statements. Existing works
achieve this goal by leveraging the exact matching method to identify the
lexical matching between the question words and the schema items. However,
these methods fail in other challenging scenarios, such as the synonym
substitution in which the surface form differs between the corresponding
question words and schema items. In this paper, we propose a framework named
ISESL-SQL to iteratively build a semantic enhanced schema-linking graph between
question tokens and database schemas. First, we extract a schema linking graph
from PLMs through a probing procedure in an unsupervised manner. Then the
schema linking graph is further optimized during the training process through a
deep graph learning method. Meanwhile, we also design an auxiliary task called
graph regularization to improve the schema information mentioned in the
schema-linking graph. Extensive experiments on three benchmarks demonstrate
that ISESL-SQL could consistently outperform the baselines and further
investigations show its generalizability and robustness.",https://github.com/THU-BPM/ISESL-SQL,7471
421abf39-52c2-42ed-9bab-7ee4edb69a24,Hierarchies of Reward Machines,0.289378,"Reward machines (RMs) are a recent formalism for representing the reward
function of a reinforcement learning task through a finite-state machine whose
edges encode subgoals of the task using high-level events. The structure of RMs
enables the decomposition of a task into simpler and independently solvable
subtasks that help tackle long-horizon and/or sparse reward tasks. We propose a
formalism for further abstracting the subtask structure by endowing an RM with
the ability to call other RMs, thus composing a hierarchy of RMs (HRM). We
exploit HRMs by treating each call to an RM as an independently solvable
subtask using the options framework, and describe a curriculum-based method to
learn HRMs from traces observed by the agent. Our experiments reveal that
exploiting a handcrafted HRM leads to faster convergence than with a flat HRM,
and that learning an HRM is feasible in cases where its equivalent flat
representation is not.",https://github.com/ertsiger/hrm-learning,-1
33d514fc-4f1f-4755-80e6-cf7e2e103f3f,Generating Scientific Claims for Zero-Shot Scientific Fact Checking,0.994545,"Automated scientific fact checking is difficult due to the complexity of
scientific language and a lack of significant amounts of training data, as
annotation requires domain expertise. To address this challenge, we propose
scientific claim generation, the task of generating one or more atomic and
verifiable claims from scientific sentences, and demonstrate its usefulness in
zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new
supervised method for generating claims supported by the literature, as well as
KBIN, a novel method for generating claim negations. Additionally, we adapt an
existing unsupervised entity-centric method of claim generation to biomedical
claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking
demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN,
achieve up to 90% performance of fully supervised models trained on manually
annotated claims and evidence. A rigorous evaluation study demonstrates
significant improvement in generated claim and negation quality over existing
baselines",https://github.com/allenai/scientic-claim-generation,-1
b6847298-b260-4c53-9fb4-d11abe39d77c,Computational linguistics and Natural Language Processing,0.0619643,"This chapter provides an introduction to computational linguistics methods,
with focus on their applications to the practice and study of translation. It
covers computational models, methods and tools for collection, storage,
indexing and analysis of linguistic data in the context of translation, and
discusses the main methodological issues and challenges in this field. While an
exhaustive review of existing computational linguistics methods and tools is
beyond the scope of this chapter, we describe the most representative
approaches, and illustrate them with descriptions of typical applications.",None,-1
67c4de12-a718-41d8-a4e4-48423cad0369,Storehouse: a Reinforcement Learning Environment for Optimizing Warehouse Management,0.374434,"Warehouse Management Systems have been evolving and improving thanks to new
Data Intelligence techniques. However, many current optimizations have been
applied to specific cases or are in great need of manual interaction. Here is
where Reinforcement Learning techniques come into play, providing
automatization and adaptability to current optimization policies. In this
paper, we present Storehouse, a customizable environment that generalizes the
definition of warehouse simulations for Reinforcement Learning. We also
validate this environment against state-of-the-art reinforcement learning
algorithms and compare these results to human and random policies.",https://github.com/JulenCestero/storehouse,-1
7e4cfba2-e1ec-4fbe-b5b8-4381d7861c4e,The Alberta Plan for AI Research,0.747482,"Herein we describe our approach to artificial intelligence research, which we
call the Alberta Plan. The Alberta Plan is pursued within our research groups
in Alberta and by others who are like minded throughout the world. We welcome
all who would join us in this pursuit.",None,-1
a5b88d15-9753-4a7c-8cee-8924424732d2,F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models,0.996434,"We present F-VLM, a simple open-vocabulary object detection method built upon
Frozen Vision and Language Models. F-VLM simplifies the current multi-stage
training pipeline by eliminating the need for knowledge distillation or
detection-tailored pretraining. Surprisingly, we observe that a frozen VLM: 1)
retains the locality-sensitive features necessary for detection, and 2) is a
strong region classifier. We finetune only the detector head and combine the
detector and VLM outputs for each region at inference time. F-VLM shows
compelling scaling behavior and achieves +6.5 mask AP improvement over the
previous state of the art on novel categories of LVIS open-vocabulary detection
benchmark. In addition, we demonstrate very competitive results on COCO
open-vocabulary detection benchmark and cross-dataset transfer detection, in
addition to significant training speed-up and compute savings. Code will be
released at the https://sites.google.com/view/f-vlm/home",https://github.com/facebookresearch/detectron2,-1
41e9b19e-5c85-4c7f-9d7f-695c57bdeb1a,Locality-aware Attention Network with Discriminative Dynamics Learning for Weakly Supervised Anomaly Detection,0.45271,"Video anomaly detection is recently formulated as a multiple instance
learning task under weak supervision, in which each video is treated as a bag
of snippets to be determined whether contains anomalies. Previous efforts
mainly focus on the discrimination of the snippet itself without modeling the
temporal dynamics, which refers to the variation of adjacent snippets.
Therefore, we propose a Discriminative Dynamics Learning (DDL) method with two
objective functions, i.e., dynamics ranking loss and dynamics alignment loss.
The former aims to enlarge the score dynamics gap between positive and negative
bags while the latter performs temporal alignment of the feature dynamics and
score dynamics within the bag. Moreover, a Locality-aware Attention Network
(LA-Net) is constructed to capture global correlations and re-calibrate the
location preference across snippets, followed by a multilayer perceptron with
causal convolution to obtain anomaly scores. Experimental results show that our
method achieves significant improvements on two challenging benchmarks, i.e.,
UCF-Crime and XD-Violence.",None,-1
f5d2d50a-142b-4b4b-81d3-fa63f937d2a7,BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision,0.999951,"We present a novel bird's-eye-view (BEV) detector with perspective
supervision, which converges faster and better suits modern image backbones.
Existing state-of-the-art BEV detectors are often tied to certain depth
pre-trained backbones like VoVNet, hindering the synergy between booming image
backbones and BEV detectors. To address this limitation, we prioritize easing
the optimization of BEV detectors by introducing perspective space supervision.
To this end, we propose a two-stage BEV detector, where proposals from the
perspective head are fed into the bird's-eye-view head for final predictions.
To evaluate the effectiveness of our model, we conduct extensive ablation
studies focusing on the form of supervision and the generality of the proposed
detector. The proposed method is verified with a wide spectrum of traditional
and modern image backbones and achieves new SoTA results on the large-scale
nuScenes dataset. The code shall be released soon.",None,-1
cd4bb8aa-eda2-4cb3-8c35-56cfddc1d3b6,MatteFormer: Transformer-Based Image Matting via Prior-Tokens,0.953863,"In this paper, we propose a transformer-based image matting model called
MatteFormer, which takes full advantage of trimap information in the
transformer block. Our method first introduces a prior-token which is a global
representation of each trimap region (e.g. foreground, background and unknown).
These prior-tokens are used as global priors and participate in the
self-attention mechanism of each block. Each stage of the encoder is composed
of PAST (Prior-Attentive Swin Transformer) block, which is based on the Swin
Transformer block, but differs in a couple of aspects: 1) It has PA-WSA
(Prior-Attentive Window Self-Attention) layer, performing self-attention not
only with spatial-tokens but also with prior-tokens. 2) It has prior-memory
which saves prior-tokens accumulatively from the previous blocks and transfers
them to the next block. We evaluate our MatteFormer on the commonly used image
matting datasets: Composition-1k and Distinctions-646. Experiment results show
that our proposed method achieves state-of-the-art performance with a large
margin. Our codes are available at https://github.com/webtoon/matteformer.",https://github.com/webtoon/matteformer,-1
98741425-96fb-4b4f-b5ac-1780ecdf9815,DeepGen: Diverse Search Ad Generation and Real-Time Customization,0.314656,"We present DeepGen, a system deployed at web scale for automatically creating
sponsored search advertisements (ads) for BingAds customers. We leverage
state-of-the-art natural language generation (NLG) models to generate fluent
ads from advertiser's web pages in an abstractive fashion and solve practical
issues such as factuality and inference speed. In addition, our system creates
a customized ad in real-time in response to the user's search query, therefore
highlighting different aspects of the same product based on what the user is
looking for. To achieve this, our system generates a diverse choice of smaller
pieces of the ad ahead of time and, at query time, selects the most relevant
ones to be stitched into a complete ad. We improve generation diversity by
training a controllable NLG model to generate multiple ads for the same web
page highlighting different selling points. Our system design further improves
diversity horizontally by first running an ensemble of generation models
trained with different objectives and then using a diversity sampling algorithm
to pick a diverse subset of generation results for online selection.
Experimental results show the effectiveness of our proposed system design. Our
system is currently deployed in production, serving ${\sim}4\%$ of global ads
served in Bing.",None,-1
a300483d-1197-4af8-bae7-47089ef80aae,ASL Video Corpora & Sign Bank: Resources Available through the American Sign Language Linguistic Research Project (ASLLRP),0.313826,"The American Sign Language Linguistic Research Project (ASLLRP) provides
Internet access to high-quality ASL video data, generally including front and
side views and a close-up of the face. The manual and non-manual components of
the signing have been linguistically annotated using SignStream(R). The
recently expanded video corpora can be browsed and searched through the Data
Access Interface (DAI 2) we have designed; it is possible to carry out complex
searches. The data from our corpora can also be downloaded; annotations are
available in an XML export format. We have also developed the ASLLRP Sign Bank,
which contains almost 6,000 sign entries for lexical signs, with distinct
English-based glosses, with a total of 41,830 examples of lexical signs (in
addition to about 300 gestures, over 1,000 fingerspelled signs, and 475
classifier examples). The Sign Bank is likewise accessible and searchable on
the Internet; it can also be accessed from within SignStream(R) (software to
facilitate linguistic annotation and analysis of visual language data) to make
annotations more accurate and efficient. Here we describe the available
resources. These data have been used for many types of research in linguistics
and in computer-based sign language recognition from video; examples of such
research are provided in the latter part of this article.",None,-1
04d79125-8613-46eb-8e10-bde679d4b5bf,Multi-Spectral Image Classification with Ultra-Lean Complex-Valued Models,0.147378,"Multi-spectral imagery is invaluable for remote sensing due to different
spectral signatures exhibited by materials that often appear identical in
greyscale and RGB imagery. Paired with modern deep learning methods, this
modality has great potential utility in a variety of remote sensing
applications, such as humanitarian assistance and disaster recovery efforts.
State-of-the-art deep learning methods have greatly benefited from large-scale
annotations like in ImageNet, but existing MSI image datasets lack annotations
at a similar scale. As an alternative to transfer learning on such data with
few annotations, we apply complex-valued co-domain symmetric models to classify
real-valued MSI images. Our experiments on 8-band xView data show that our
ultra-lean model trained on xView from scratch without data augmentations can
outperform ResNet with data augmentation and modified transfer learning on
xView. Our work is the first to demonstrate the value of complex-valued deep
learning on real-valued MSI data.",None,12734
07c611be-7af9-477d-805e-065fa21c1af4,HyperNST: Hyper-Networks for Neural Style Transfer,0.401853,"We present HyperNST; a neural style transfer (NST) technique for the artistic
stylization of images, based on Hyper-networks and the StyleGAN2 architecture.
Our contribution is a novel method for inducing style transfer parameterized by
a metric space, pre-trained for style-based visual search (SBVS). We show for
the first time that such space may be used to drive NST, enabling the
application and interpolation of styles from an SBVS system. The technical
contribution is a hyper-network that predicts weight updates to a StyleGAN2
pre-trained over a diverse gamut of artistic content (portraits), tailoring the
style parameterization on a per-region basis using a semantic map of the facial
regions. We show HyperNST to exceed state of the art in content preservation
for our stylized content while retaining good style transfer performance.",None,33989
4c432962-725e-4425-94d8-597bcb66e101,Approaching Reflex Predictions as a Classification Problem Using Extended Phonological Alignments,0.0141751,"This work describes an implementation of the ""extended alignment"" (or
""multitiers"") approach for cognate reflex prediction, submitted to ""Prediction
of Cognate Reflexes"" shared task. Similarly to List2022d, the technique
involves an automatic extension of sequence alignments with multilayered
vectors that encode informational tiers on both site-specific traits, such as
sound classes and distinctive features, as well as contextual and
suprasegmental ones, conveyed by cross-site referrals and replication. The
method allows to generalize the problem of cognate reflex prediction as a
classification problem, with models trained using a parallel corpus of cognate
sets. A model using random forests is trained and evaluated on the shared task
for reflex prediction, and the experimental results are presented and discussed
along with some differences to other implementations.",https://github.com/tresoldi/maniphono,-1
5f3c319c-f24a-4d17-a042-b9c9ab83a14b,Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models,0.87582,"We present a novel way of conditioning a pretrained denoising diffusion
speech model to produce speech in the voice of a novel person unseen during
training. The method requires a short (~3 seconds) sample from the target
person, and generation is steered at inference time, without any training
steps. At the heart of the method lies a sampling process that combines the
estimation of the denoising model with a low-pass version of the new speaker's
sample. The objective and subjective evaluations show that our sampling method
can generate a voice similar to that of the target speaker in terms of
frequency, with an accuracy comparable to state-of-the-art methods, and without
training.",None,-1
d8da0d36-7db5-4f54-bd30-bb312e5861e5,FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation,0.361167,"The speaker-follower models have proven to be effective in
vision-and-language navigation, where a speaker model is used to synthesize new
instructions to augment the training data for a follower navigation model.
However, in many of the previous methods, the generated instructions are not
directly trained to optimize the performance of the follower. In this paper, we
present \textsc{foam}, a \textsc{Fo}llower-\textsc{a}ware speaker
\textsc{M}odel that is constantly updated given the follower feedback, so that
the generated instructions can be more suitable to the current learning state
of the follower. Specifically, we optimize the speaker using a bi-level
optimization framework and obtain its training signals by evaluating the
follower on labeled data. Experimental results on the Room-to-Room and
Room-across-Room datasets demonstrate that our methods can outperform strong
baseline models across settings. Analyses also reveal that our generated
instructions are of higher quality than the baselines.",https://github.com/PlusLabNLP/follower_aware_speaker,-1
e04ac15d-7513-4878-b5d8-9494280eb5f6,Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images,0.984599,"We present Neural Strands, a novel learning framework for modeling accurate
hair geometry and appearance from multi-view image inputs. The learned hair
model can be rendered in real-time from any viewpoint with high-fidelity
view-dependent effects. Our model achieves intuitive shape and style control
unlike volumetric counterparts. To enable these properties, we propose a novel
hair representation based on a neural scalp texture that encodes the geometry
and appearance of individual strands at each texel location. Furthermore, we
introduce a novel neural rendering framework based on rasterization of the
learned hair strands. Our neural rendering is strand-accurate and anti-aliased,
making the rendering view-consistent and photorealistic. Combining appearance
with a multi-view geometric prior, we enable, for the first time, the joint
learning of appearance and explicit hair geometry from a multi-view setup. We
demonstrate the efficacy of our approach in terms of fidelity and efficiency
for various hairstyles.",None,-1
82b0b244-0c99-4d7c-b0f6-9ab32db2af02,UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition,0.955418,"Multimodal sentiment analysis (MSA) and emotion recognition in conversation
(ERC) are key research topics for computers to understand human behaviors. From
a psychological perspective, emotions are the expression of affect or feelings
during a short period, while sentiments are formed and held for a longer
period. However, most existing works study sentiment and emotion separately and
do not fully exploit the complementary knowledge behind the two. In this paper,
we propose a multimodal sentiment knowledge-sharing framework (UniMSE) that
unifies MSA and ERC tasks from features, labels, and models. We perform
modality fusion at the syntactic and semantic levels and introduce contrastive
learning between modalities and samples to better capture the difference and
consistency between sentiments and emotions. Experiments on four public
benchmark datasets, MOSI, MOSEI, MELD, and IEMOCAP, demonstrate the
effectiveness of the proposed method and achieve consistent improvements
compared with state-of-the-art methods.",https://github.com/LeMei/UniMSE,-1
4d69ae99-d55f-4e52-84e9-84a40e3c294f,Performance of different machine learning methods on activity recognition and pose estimation datasets,0.111187,"With advancements in computer vision taking place day by day, recently a lot
of light is being shed on activity recognition. With the range for real-world
applications utilizing this field of study increasing across a multitude of
industries such as security and healthcare, it becomes crucial for businesses
to distinguish which machine learning methods perform better than others in the
area. This paper strives to aid in this predicament i.e. building upon previous
related work, it employs both classical and ensemble approaches on rich pose
estimation (OpenPose) and HAR datasets. Making use of appropriate metrics to
evaluate the performance for each model, the results show that overall, random
forest yields the highest accuracy in classifying ADLs. Relatively all the
models have excellent performance across both datasets, except for logistic
regression and AdaBoost perform poorly in the HAR one. With the limitations of
this paper also discussed in the end, the scope for further research is vast,
which can use this paper as a base in aims of producing better results.",None,-1
51cc4f51-f069-4c4e-afc4-94a6174fb33a,HieNet: Bidirectional Hierarchy Framework for Automated ICD Coding,0.173965,"International Classification of Diseases (ICD) is a set of classification
codes for medical records. Automated ICD coding, which assigns unique
International Classification of Diseases codes with each medical record, is
widely used recently for its efficiency and error-prone avoidance. However,
there are challenges that remain such as heterogeneity, label unbalance, and
complex relationships between ICD codes. In this work, we proposed a novel
Bidirectional Hierarchy Framework(HieNet) to address the challenges.
Specifically, a personalized PageRank routine is developed to capture the
co-relation of codes, a bidirectional hierarchy passage encoder to capture the
codes' hierarchical representations, and a progressive predicting method is
then proposed to narrow down the semantic searching space of prediction. We
validate our method on two widely used datasets. Experimental results on two
authoritative public datasets demonstrate that our proposed method boosts
state-of-the-art performance by a large margin.",None,-1
e9ef850c-8536-4160-8eb7-aef97a1f8503,Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds,0.882995,"Dense captioning in 3D point clouds is an emerging vision-and-language task
involving object-level 3D scene understanding. Apart from coarse semantic class
prediction and bounding box regression as in traditional 3D object detection,
3D dense captioning aims at producing a further and finer instance-level label
of natural language description on visual appearance and spatial relations for
each scene object of interest. To detect and describe objects in a scene,
following the spirit of neural machine translation, we propose a
transformer-based encoder-decoder architecture, namely SpaCap3D, to transform
objects into descriptions, where we especially investigate the relative
spatiality of objects in 3D scenes and design a spatiality-guided encoder via a
token-to-token spatial relation learning objective and an object-centric
decoder for precise and spatiality-enhanced object caption generation.
Evaluated on two benchmark datasets, ScanRefer and ReferIt3D, our proposed
SpaCap3D outperforms the baseline method Scan2Cap by 4.94% and 9.61% in
CIDEr@0.5IoU, respectively. Our project page with source code and supplementary
files is available at https://SpaCap3D.github.io/ .",https://github.com/heng-hw/SpaCap3D,-1
b5e76f54-9550-4f1e-a145-92e9231c6e01,Open-Ended Reinforcement Learning with Neural Reward Functions,0.181302,"Inspired by the great success of unsupervised learning in Computer Vision and
Natural Language Processing, the Reinforcement Learning community has recently
started to focus more on unsupervised discovery of skills. Most current
approaches, like DIAYN or DADS, optimize some form of mutual information
objective. We propose a different approach that uses reward functions encoded
by neural networks. These are trained iteratively to reward more complex
behavior. In high-dimensional robotic environments our approach learns a wide
range of interesting skills including front-flips for Half-Cheetah and
one-legged running for Humanoid. In the pixel-based Montezuma's Revenge
environment our method also works with minimal changes and it learns complex
skills that involve interacting with items and visiting diverse locations. The
implementation of our approach can be found in this link:
https://github.com/amujika/Open-Ended-Reinforcement-Learning-with-Neural-Reward-Functions.",https://github.com/coax-dev/coax,-1
cfce1c54-541f-401a-9fe7-55c7ff93f4ec,Privacy-Preserving Image Classification Using Vision Transformer,0.864702,"In this paper, we propose a privacy-preserving image classification method
that is based on the combined use of encrypted images and the vision
transformer (ViT). The proposed method allows us not only to apply images
without visual information to ViT models for both training and testing but to
also maintain a high classification accuracy. ViT utilizes patch embedding and
position embedding for image patches, so this architecture is shown to reduce
the influence of block-wise image transformation. In an experiment, the
proposed method for privacy-preserving image classification is demonstrated to
outperform state-of-the-art methods in terms of classification accuracy and
robustness against various attacks.",None,-1
5e21b7be-d56b-4930-8d6e-d4c84343dbcd,OAK4XAI: Model towards Out-Of-Box eXplainable Artificial Intelligence for Digital Agriculture,0.0723837,"Recent machine learning approaches have been effective in Artificial
Intelligence (AI) applications. They produce robust results with a high level
of accuracy. However, most of these techniques do not provide
human-understandable explanations for supporting their results and decisions.
They usually act as black boxes, and it is not easy to understand how decisions
have been made. Explainable Artificial Intelligence (XAI), which has received
much interest recently, tries to provide human-understandable explanations for
decision-making and trained AI models. For instance, in digital agriculture,
related domains often present peculiar or input features with no link to
background knowledge. The application of the data mining process on
agricultural data leads to results (knowledge), which are difficult to explain.
In this paper, we propose a knowledge map model and an ontology design as an
XAI framework (OAK4XAI) to deal with this issue. The framework does not only
consider the data analysis part of the process, but it takes into account the
semantics aspect of the domain knowledge via an ontology and a knowledge map
model, provided as modules of the framework. Many ongoing XAI studies aim to
provide accurate and verbalizable accounts for how given feature values
contribute to model decisions. The proposed approach, however, focuses on
providing consistent information and definitions of concepts, algorithms, and
values involved in the data mining models. We built an Agriculture Computing
Ontology (AgriComO) to explain the knowledge mined in agriculture. AgriComO has
a well-designed structure and includes a wide range of concepts and
transformations suitable for agriculture and computing domains.",None,-1
77705e9d-78eb-41df-bfc4-1cad5e68dcbf,HealthE: Classifying Entities in Online Textual Health Advice,0.128808,"The processing of entities in natural language is essential to many medical
NLP systems. Unfortunately, existing datasets vastly under-represent the
entities required to model public health relevant texts such as health advice
often found on sites like WebMD. People rely on such information for personal
health management and clinically relevant decision making. In this work, we
release a new annotated dataset, HealthE, consisting of 6,756 health advice.
HealthE has a more granular label space compared to existing medical NER
corpora and contains annotation for diverse health phrases. Additionally, we
introduce a new health entity classification model, EP S-BERT, which leverages
textual context patterns in the classification of entity classes. EP S-BERT
provides a 4-point increase in F1 score over the nearest baseline and a
34-point increase in F1 when compared to off-the-shelf medical NER tools
trained to extract disease and medication mentions from clinical texts. All
code and data are publicly available on Github.",None,-1
cba0eee4-f461-4b0f-a6b6-51f10ebbb0e0,Active Diffusion and VCA-Assisted Image Segmentation of Hyperspectral Images,0.281244,"Hyperspectral images encode rich structure that can be exploited for material
discrimination by machine learning algorithms. This article introduces the
Active Diffusion and VCA-Assisted Image Segmentation (ADVIS) for active
material discrimination. ADVIS selects high-purity, high-density pixels that
are far in diffusion distance (a data-dependent metric) from other high-purity,
high-density pixels in the hyperspectral image. The ground truth labels of
these pixels are queried and propagated to the rest of the image. The ADVIS
active learning algorithm is shown to strongly outperform its fully
unsupervised clustering algorithm counterpart, suggesting that the
incorporation of a very small number of carefully-selected ground truth labels
can result in substantially superior material discrimination in hyperspectral
images.",https://github.com/sampolk/D-VIC,-1
0c12dca2-a2ee-4c87-ac70-1122ca3f0699,Multi-Task Learning for Visual Scene Understanding,0.0686016,"Despite the recent progress in deep learning, most approaches still go for a
silo-like solution, focusing on learning each task in isolation: training a
separate neural network for each individual task. Many real-world problems,
however, call for a multi-modal approach and, therefore, for multi-tasking
models. Multi-task learning (MTL) aims to leverage useful information across
tasks to improve the generalization capability of a model. This thesis is
concerned with multi-task learning in the context of computer vision. First, we
review existing approaches for MTL. Next, we propose several methods that
tackle important aspects of multi-task learning. The proposed methods are
evaluated on various benchmarks. The results show several advances in the
state-of-the-art of multi-task learning. Finally, we discuss several
possibilities for future work.",https://github.com/SimonVandenhende,12
85124db8-ee63-46f0-9772-ea08e2b2ef56,Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks,0.462945,"In the pursuit of explaining implicit regularization in deep learning,
prominent focus was given to matrix and tensor factorizations, which correspond
to simplified neural networks. It was shown that these models exhibit an
implicit tendency towards low matrix and tensor ranks, respectively. Drawing
closer to practical deep learning, the current paper theoretically analyzes the
implicit regularization in hierarchical tensor factorization, a model
equivalent to certain deep convolutional neural networks. Through a dynamical
systems lens, we overcome challenges associated with hierarchy, and establish
implicit regularization towards low hierarchical tensor rank. This translates
to an implicit regularization towards locality for the associated convolutional
networks. Inspired by our theory, we design explicit regularization
discouraging locality, and demonstrate its ability to improve the performance
of modern convolutional networks on non-local tasks, in defiance of
conventional wisdom by which architectural changes are needed. Our work
highlights the potential of enhancing neural networks via theoretical analysis
of their implicit regularization.",https://github.com/asafmaman101/imp_reg_htf,-1
923c42f5-bb19-4951-9448-599b6febba43,DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models,0.997986,"Traditionally, monocular 3D human pose estimation employs a machine learning
model to predict the most likely 3D pose for a given input image. However, a
single image can be highly ambiguous and induces multiple plausible solutions
for the 2D-3D lifting step which results in overly confident 3D pose
predictors. To this end, we propose \emph{DiffPose}, a conditional diffusion
model, that predicts multiple hypotheses for a given input image. In comparison
to similar approaches, our diffusion model is straightforward and avoids
intensive hyperparameter tuning, complex network structures, mode collapse, and
unstable training. Moreover, we tackle a problem of the common two-step
approach that first estimates a distribution of 2D joint locations via
joint-wise heatmaps and consecutively approximates them based on first- or
second-moment statistics. Since such a simplification of the heatmaps removes
valid information about possibly correct, though labeled unlikely, joint
locations, we propose to represent the heatmaps as a set of 2D joint candidate
samples. To extract information about the original distribution from these
samples we introduce our \emph{embedding transformer} that conditions the
diffusion model. Experimentally, we show that DiffPose slightly improves upon
the state of the art for multi-hypothesis pose estimation for simple poses and
outperforms it by a large margin for highly ambiguous poses.",None,-1
6c4774e1-14c1-49a1-a7da-1393a4c218b5,Discrete-Constrained Regression for Local Counting Models,0.484458,"Local counts, or the number of objects in a local area, is a continuous value
by nature. Yet recent state-of-the-art methods show that formulating counting
as a classification task performs better than regression. Through a series of
experiments on carefully controlled synthetic data, we show that this
counter-intuitive result is caused by imprecise ground truth local counts.
Factors such as biased dot annotations and incorrectly matched Gaussian kernels
used to generate ground truth counts introduce deviations from the true local
counts. Standard continuous regression is highly sensitive to these errors,
explaining the performance gap between classification and regression. To
mitigate the sensitivity, we loosen the regression formulation from a
continuous scale to a discrete ordering and propose a novel
discrete-constrained (DC) regression. Applied to crowd counting, DC-regression
is more accurate than both classification and standard regression on three
public benchmarks. A similar advantage also holds for the age estimation task,
verifying the overall effectiveness of DC-regression.",None,-1
42a26473-6c36-4b3b-acd1-c7db7d71ba58,Motion Policy Networks,0.754901,"Collision-free motion generation in unknown environments is a core building
block for robot manipulation. Generating such motions is challenging due to
multiple objectives; not only should the solutions be optimal, the motion
generator itself must be fast enough for real-time performance and reliable
enough for practical deployment. A wide variety of methods have been proposed
ranging from local controllers to global planners, often being combined to
offset their shortcomings. We present an end-to-end neural model called Motion
Policy Networks (M$\pi$Nets) to generate collision-free, smooth motion from
just a single depth camera observation. M$\pi$Nets are trained on over 3
million motion planning problems in over 500,000 environments. Our experiments
show that M$\pi$Nets are significantly faster than global planners while
exhibiting the reactivity needed to deal with dynamic scenes. They are 46%
better than prior neural planners and more robust than local control policies.
Despite being only trained in simulation, M$\pi$Nets transfer well to the real
robot with noisy partial point clouds. Code and data are publicly available at
https://mpinets.github.io.",https://mpinets.github.io,-1
1fca88a7-aea8-4ea8-a41a-1ea39a49eab5,A context-aware knowledge transferring strategy for CTC-based ASR,0.747647,"Non-autoregressive automatic speech recognition (ASR) modeling has received
increasing attention recently because of its fast decoding speed and superior
performance. Among representatives, methods based on the connectionist temporal
classification (CTC) are still a dominating stream. However, the theoretically
inherent flaw, the assumption of independence between tokens, creates a
performance barrier for the school of works. To mitigate the challenge, we
propose a context-aware knowledge transferring strategy, consisting of a
knowledge transferring module and a context-aware training strategy, for
CTC-based ASR. The former is designed to distill linguistic information from a
pre-trained language model, and the latter is framed to modulate the
limitations caused by the conditional independence assumption. As a result, a
knowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is
presented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2
datasets demonstrate the effectiveness of the proposed method.",None,810
ccca76bc-5531-4d33-a69d-ca94ca64f833,Transformation-Equivariant 3D Object Detection for Autonomous Driving,0.880815,"3D object detection received increasing attention in autonomous driving
recently. Objects in 3D scenes are distributed with diverse orientations.
Ordinary detectors do not explicitly model the variations of rotation and
reflection transformations. Consequently, large networks and extensive data
augmentation are required for robust detection. Recent equivariant networks
explicitly model the transformation variations by applying shared networks on
multiple transformed point clouds, showing great potential in object geometry
modeling. However, it is difficult to apply such networks to 3D object
detection in autonomous driving due to its large computation cost and slow
reasoning speed. In this work, we present TED, an efficient
Transformation-Equivariant 3D Detector to overcome the computation cost and
speed issues. TED first applies a sparse convolution backbone to extract
multi-channel transformation-equivariant voxel features; and then aligns and
aggregates these equivariant features into lightweight and compact
representations for high-performance 3D object detection. On the highly
competitive KITTI 3D car detection leaderboard, TED ranked 1st among all
submissions with competitive efficiency.",None,-1
82dd437a-3c53-49b3-a5cd-373fb6bfeae6,Image Search with Text Feedback by Additive Attention Compositional Learning,0.349305,"Effective image retrieval with text feedback stands to impact a range of
real-world applications, such as e-commerce. Given a source image and text
feedback that describes the desired modifications to that image, the goal is to
retrieve the target images that resemble the source yet satisfy the given
modifications by composing a multi-modal (image-text) query. We propose a novel
solution to this problem, Additive Attention Compositional Learning (AACL),
that uses a multi-modal transformer-based architecture and effectively models
the image-text contexts. Specifically, we propose a novel image-text
composition module based on additive attention that can be seamlessly plugged
into deep neural networks. We also introduce a new challenging benchmark
derived from the Shopping100k dataset. AACL is evaluated on three large-scale
datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong
baselines. Extensive experiments show that AACL achieves new state-of-the-art
results on all three datasets.",None,-1
cda5f1f2-8b8c-4035-8ab9-e36caf3b550b,TransLog: A Unified Transformer-based Framework for Log Anomaly Detection,0.483908,"Log anomaly detection is a key component in the field of artificial
intelligence for IT operations (AIOps). Considering log data of variant
domains, retraining the whole network for unknown domains is inefficient in
real industrial scenarios especially for low-resource domains. However,
previous deep models merely focused on extracting the semantics of log sequence
in the same domain, leading to poor generalization on multi-domain logs.
Therefore, we propose a unified Transformer-based framework for log anomaly
detection (\ourmethod{}), which is comprised of the pretraining and
adapter-based tuning stage. Our model is first pretrained on the source domain
to obtain shared semantic knowledge of log data. Then, we transfer the
pretrained model to the target domain via the adapter-based tuning. The
proposed method is evaluated on three public datasets including one source
domain and two target domains. The experimental results demonstrate that our
simple yet efficient approach, with fewer trainable parameters and lower
training costs in the target domain, achieves state-of-the-art performance on
three benchmarks.",None,-1
25f6634f-86bb-47ea-829c-763e2e39ae37,PIC4rl-gym: a ROS2 modular framework for Robots Autonomous Navigation with Deep Reinforcement Learning,0.80804,"Learning agents can optimize standard autonomous navigation improving
flexibility, efficiency, and computational cost of the system by adopting a
wide variety of approaches. This work introduces the \textit{PIC4rl-gym}, a
fundamental modular framework to enhance navigation and learning research by
mixing ROS2 and Gazebo, the standard tools of the robotics community, with Deep
Reinforcement Learning (DRL). The paper describes the whole structure of the
PIC4rl-gym, which fully integrates DRL agent's training and testing in several
indoor and outdoor navigation scenarios and tasks. A modular approach is
adopted to easily customize the simulation by selecting new platforms, sensors,
or models. We demonstrate the potential of our novel gym by benchmarking the
resulting policies, trained for different navigation tasks, with a complete set
of metrics.",https://github.com/PIC4SeR/PIC4rl_gym,3782
df4667a0-4767-489f-918f-3512bddd08c5,"Continual Learning, Fast and Slow",0.568865,"According to the Complementary Learning Systems (CLS)
theory~\cite{mcclelland1995there} in neuroscience, humans do effective
\emph{continual learning} through two complementary systems: a fast learning
system centered on the hippocampus for rapid learning of the specifics,
individual experiences; and a slow learning system located in the neocortex for
the gradual acquisition of structured knowledge about the environment.
Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a
general continual learning framework comprising a fast learning system for
supervised learning of pattern-separated representation from specific tasks and
a slow learning system for representation learning of task-agnostic general
representation via Self-Supervised Learning (SSL). DualNets can seamlessly
incorporate both representation types into a holistic framework to facilitate
better continual learning in deep neural networks. Via extensive experiments,
we demonstrate the promising results of DualNets on a wide range of continual
learning protocols, ranging from the standard offline, task-aware setting to
the challenging online, task-free scenario. Notably, on the
CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly
different visual images, DualNets can achieve competitive performance with
existing state-of-the-art dynamic architecture
strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive
ablation studies to validate DualNets efficacy, robustness, and scalability.
Code will be made available at \url{https://github.com/phquang/DualNet}.",https://github.com/phquang/DualNet,-1
05f24f64-2cfc-45e7-acd3-554293d88b56,Global Convergence of Localized Policy Iteration in Networked Multi-Agent Reinforcement Learning,0.847323,"We study a multi-agent reinforcement learning (MARL) problem where the agents
interact over a given network. The goal of the agents is to cooperatively
maximize the average of their entropy-regularized long-term rewards. To
overcome the curse of dimensionality and to reduce communication, we propose a
Localized Policy Iteration (LPI) algorithm that provably learns a
near-globally-optimal policy using only local information. In particular, we
show that, despite restricting each agent's attention to only its $\kappa$-hop
neighborhood, the agents are able to learn a policy with an optimality gap that
decays polynomially in $\kappa$. In addition, we show the finite-sample
convergence of LPI to the global optimal policy, which explicitly captures the
trade-off between optimality and computational complexity in choosing $\kappa$.
Numerical simulations demonstrate the effectiveness of LPI.",None,-1
5e067623-0d6f-4daf-878e-79496652c3d4,Retrieve-and-Fill for Scenario-based Task-Oriented Semantic Parsing,0.349869,"Task-oriented semantic parsing models have achieved strong results in recent
years, but unfortunately do not strike an appealing balance between model size,
runtime latency, and cross-domain generalizability. We tackle this problem by
introducing scenario-based semantic parsing: a variant of the original task
which first requires disambiguating an utterance's ""scenario"" (an intent-slot
template with variable leaf spans) before generating its frame, complete with
ontology and utterance tokens. This formulation enables us to isolate
coarse-grained and fine-grained aspects of the task, each of which we solve
with off-the-shelf neural modules, also optimizing for the axes outlined above.
Concretely, we create a Retrieve-and-Fill (RAF) architecture comprised of (1) a
retrieval module which ranks the best scenario given an utterance and (2) a
filling module which imputes spans into the scenario to create the frame. Our
model is modular, differentiable, interpretable, and allows us to garner extra
supervision from scenarios. RAF achieves strong results in high-resource,
low-resource, and multilingual settings, outperforming recent approaches by
wide margins despite, using base pre-trained encoders, small sequence lengths,
and parallel decoding.",None,-1
6904dc81-193d-49e0-8d8c-11bf22372a82,MotionSC: Data Set and Network for Real-Time Semantic Mapping in Dynamic Environments,0.484447,"This work addresses a gap in semantic scene completion (SSC) data by creating
a novel outdoor data set with accurate and complete dynamic scenes. Our data
set is formed from randomly sampled views of the world at each time step, which
supervises generalizability to complete scenes without occlusions or traces. We
create SSC baselines from state-of-the-art open source networks and construct a
benchmark real-time dense local semantic mapping algorithm, MotionSC, by
leveraging recent 3D deep learning architectures to enhance SSC with temporal
information. Our network shows that the proposed data set can quantify and
supervise accurate scene completion in the presence of dynamic objects, which
can lead to the development of improved dynamic mapping algorithms. All
software is available at https://github.com/UMich-CURLY/3DMapping.",https://github.com/UMich-CURLY/3DMapping,-1
d233939f-f4fd-4575-b329-6ff8335b24fe,Generalizing Math Word Problem Solvers via Solution Diversification,0.48373,"Current math word problem (MWP) solvers are usually Seq2Seq models trained by
the (one-problem; one-solution) pairs, each of which is made of a problem
description and a solution showing reasoning flow to get the correct answer.
However, one MWP problem naturally has multiple solution equations. The
training of an MWP solver with (one-problem; one-solution) pairs excludes other
correct solutions, and thus limits the generalizability of the MWP solver. One
feasible solution to this limitation is to augment multiple solutions to a
given problem. However, it is difficult to collect diverse and accurate augment
solutions through human efforts. In this paper, we design a new training
framework for an MWP solver by introducing a solution buffer and a solution
discriminator. The buffer includes solutions generated by an MWP solver to
encourage the training data diversity. The discriminator controls the quality
of buffered solutions to participate in training. Our framework is flexibly
applicable to a wide setting of fully, semi-weakly and weakly supervised
training for all Seq2Seq MWP solvers. We conduct extensive experiments on a
benchmark dataset Math23k and a new dataset named Weak12k, and show that our
framework improves the performance of various MWP solvers under different
settings by generating correct and diverse solutions.",https://github.com/LZhenwen/Solution Diversity,-1
601fa20a-feaf-4e00-903c-c079c4dfa074,Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge,0.39421,"We propose a novel open-domain question answering (ODQA) framework for
answering single/multi-hop questions across heterogeneous knowledge sources.
The key novelty of our method is the introduction of the intermediary modules
into the current retriever-reader pipeline. Unlike previous methods that solely
rely on the retriever for gathering all evidence in isolation, our intermediary
performs a chain of reasoning over the retrieved set. Specifically, our method
links the retrieved evidence with its related global context into graphs and
organizes them into a candidate list of evidence chains. Built upon pretrained
language models, our system achieves competitive performance on two ODQA
datasets, OTT-QA and NQ, against tables and passages from Wikipedia. In
particular, our model substantially outperforms the previous state-of-the-art
on OTT-QA with an exact match score of 47.3 (45 % relative gain).",https://github.com/Mayer123/UDT-QA,-1
5a778814-3d10-457e-8417-77a7f3e24e91,Towards Tracing Factual Knowledge in Language Models Back to the Training Data,0.321927,"Language models (LMs) have been shown to memorize a great deal of factual
knowledge contained in their training data. But when an LM generates an
assertion, it is often difficult to determine where it learned this information
and whether it is true. In this paper, we propose the problem of fact tracing:
identifying which training examples taught an LM to generate a particular
factual assertion. Prior work on training data attribution (TDA) may offer
effective tools for identifying such examples, known as ""proponents"". We
present the first quantitative benchmark to evaluate this. We compare two
popular families of TDA methods -- gradient-based and embedding-based -- and
find that much headroom remains. For example, both methods have lower
proponent-retrieval precision than an information retrieval baseline (BM25)
that does not have access to the LM at all. We identify key challenges that may
be necessary for further improvement such as overcoming the problem of gradient
saturation, and also show how several nuanced implementation details of
existing neural TDA methods can significantly improve overall fact tracing
performance.",https://github.com/ekinakyurek/influence,-1
e1768579-fd7b-43dc-b3cb-6c75a5d279e3,Holistic Interaction Transformer Network for Action Detection,0.850023,"Actions are about how we interact with the environment, including other
people, objects, and ourselves. In this paper, we propose a novel multi-modal
Holistic Interaction Transformer Network (HIT) that leverages the largely
ignored, but critical hand and pose information essential to most human
actions. The proposed ""HIT"" network is a comprehensive bi-modal framework that
comprises an RGB stream and a pose stream. Each of them separately models
person, object, and hand interactions. Within each sub-network, an
Intra-Modality Aggregation module (IMA) is introduced that selectively merges
individual interaction units. The resulting features from each modality are
then glued using an Attentive Fusion Mechanism (AFM). Finally, we extract cues
from the temporal context to better classify the occurring actions using cached
memory. Our method significantly outperforms previous approaches on the J-HMDB,
UCF101-24, and MultiSports datasets. We also achieve competitive results on
AVA. The code will be available at https://github.com/joslefaure/HIT.",https://github.com/joslefaure/HIT,-1
6c620f23-2b10-428c-aa52-a278671e4109,A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles,0.868415,"Modern vehicles, including autonomous vehicles and connected vehicles, are
increasingly connected to the external world, which enables various
functionalities and services. However, the improving connectivity also
increases the attack surfaces of the Internet of Vehicles (IoV), causing its
vulnerabilities to cyber-threats. Due to the lack of authentication and
encryption procedures in vehicular networks, Intrusion Detection Systems (IDSs)
are essential approaches to protect modern vehicle systems from network
attacks. In this paper, a transfer learning and ensemble learning-based IDS is
proposed for IoV systems using convolutional neural networks (CNNs) and
hyper-parameter optimization techniques. In the experiments, the proposed IDS
has demonstrated over 99.25% detection rates and F1-scores on two well-known
public benchmark IoV security datasets: the Car-Hacking dataset and the
CICIDS2017 dataset. This shows the effectiveness of the proposed IDS for
cyber-attack detection in both intra-vehicle and external vehicular networks.",https://github.com/Western-OC2-Lab/Intrusion-,-1
ff3dc86f-cad7-47ef-ab63-ae6ad65bf479,Less is More: Task-aware Layer-wise Distillation for Language Model Compression,0.675151,"Layer-wise distillation is a powerful tool to compress large models (i.e.
teacher models) into small ones (i.e., student models). The student distills
knowledge from the teacher by mimicking the hidden representations of the
teacher at every intermediate layer. However, layer-wise distillation is
difficult. Since the student has a smaller model capacity than the teacher, it
is often under-fitted. Furthermore, the hidden representations of the teacher
contain redundant information that the student does not necessarily need for
the target task's learning. To address these challenges, we propose a novel
Task-aware layEr-wise Distillation (TED). TED designs task-aware filters to
align the hidden representations of the student and the teacher at each layer.
The filters select the knowledge that is useful for the target task from the
hidden representations. As such, TED reduces the knowledge gap between the two
models and helps the student to fit better on the target task. We evaluate TED
in two scenarios: continual pre-training and fine-tuning. TED demonstrates
significant and consistent improvements over existing distillation methods in
both scenarios. Code is available at
https://github.com/cliang1453/task-aware-distillation.",https://github.com/cliang1453/task-aware-distillation,-1
24cc29be-2823-4fe7-8a94-1e11e651450e,BioTABQA: Instruction Learning for Biomedical Table Question Answering,0.780816,"Table Question Answering (TQA) is an important but under-explored task. Most
of the existing QA datasets are in unstructured text format and only few of
them use tables as the context. To the best of our knowledge, none of TQA
datasets exist in the biomedical domain where tables are frequently used to
present information. In this paper, we first curate a table question answering
dataset, BioTABQA, using 22 templates and the context from a biomedical
textbook on differential diagnosis. BioTABQA can not only be used to teach a
model how to answer questions from tables but also evaluate how a model
generalizes to unseen questions, an important scenario for biomedical
applications. To achieve the generalization evaluation, we divide the templates
into 17 training and 5 cross-task evaluations. Then, we develop two baselines
using single and multi-tasks learning on BioTABQA. Furthermore, we explore
instructional learning, a recent technique showing impressive generalizing
performance. Experimental results show that our instruction-tuned model
outperforms single and multi-task baselines on an average by ~23% and ~6%
across various evaluation settings, and more importantly, instruction-tuned
model outperforms baselines by ~5% on cross-tasks.",None,14059
601ca73a-aa63-46b7-b27c-e610eedc568b,A new hazard event classification model via deep learning and multifractal,0.390459,"Hazard and operability analysis (HAZOP) is the paradigm of industrial safety
that can reveal the hazards of process from its node deviations, consequences,
causes, measures and suggestions, and such hazards can be considered as hazard
events (HaE). The classification research on HaE has much irreplaceable
pragmatic values. In this paper, we present a novel deep learning model termed
DLF through multifractal to explore HaE classification where the motivation is
that HaE can be naturally regarded as a kind of time series. Specifically,
first HaE is vectorized to get HaE time series by employing BERT. Then, a new
multifractal analysis method termed HmF-DFA is proposed to win HaE fractal
series by analyzing HaE time series. Finally, a new hierarchical gating neural
network (HGNN) is designed to process HaE fractal series to accomplish the
classification of HaE from three aspects: severity, possibility and risk. We
take HAZOP reports of 18 processes as cases, and launch the experiments on this
basis. Results demonstrate that compared with other classifiers, DLF classifier
performs better under metrics of precision, recall and F1-score, especially for
the severity aspect. Also, HmF-DFA and HGNN effectively promote HaE
classification. Our HaE classification system can serve application incentives
to experts, engineers, employees, and other enterprises. We hope our research
can contribute added support to the daily practice in industrial safety.",None,-1
b4de6889-2a51-4f73-a531-4b61ef185cfc,JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional Networks,0.831292,"Graph Convolutional Network (GCN) has exhibited strong empirical performance
in many real-world applications. The vast majority of existing works on GCN
primarily focus on the accuracy while ignoring how confident or uncertain a GCN
is with respect to its predictions. Despite being a cornerstone of trustworthy
graph mining, uncertainty quantification on GCN has not been well studied and
the scarce existing efforts either fail to provide deterministic quantification
or have to change the training procedure of GCN by introducing additional
parameters or architectures. In this paper, we propose the first
frequentist-based approach named JuryGCN in quantifying the uncertainty of GCN,
where the key idea is to quantify the uncertainty of a node as the width of
confidence interval by a jackknife estimator. Moreover, we leverage the
influence functions to estimate the change in GCN parameters without
re-training to scale up the computation. The proposed JuryGCN is capable of
quantifying uncertainty deterministically without modifying the GCN
architecture or introducing additional parameters. We perform extensive
experimental evaluation on real-world datasets in the tasks of both active
learning and semi-supervised node classification, which demonstrate the
efficacy of the proposed method.",None,-1
21ac1db5-f895-4c64-90d9-5d0ccb6ee612,Panoramic Human Activity Recognition,0.687657,"To obtain a more comprehensive activity understanding for a crowded scene, in
this paper, we propose a new problem of panoramic human activity recognition
(PAR), which aims to simultaneous achieve the individual action, social group
activity, and global activity recognition. This is a challenging yet practical
problem in real-world applications. For this problem, we develop a novel
hierarchical graph neural network to progressively represent and model the
multi-granularity human activities and mutual social relations for a crowd of
people. We further build a benchmark to evaluate the proposed method and other
existing related methods. Experimental results verify the rationality of the
proposed PAR problem, the effectiveness of our method and the usefulness of the
benchmark. We will release the source code and benchmark to the public for
promoting the study on this problem.",https://github.com/RuizeHan/PAR,10875
e5bd22e8-57ae-47a2-a71a-488a90ee2c9c,DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization,0.525208,"LiDAR mapping is important yet challenging in self-driving and mobile
robotics. To tackle such a global point cloud registration problem, DeepMapping
converts the complex map estimation into a self-supervised training of simple
deep networks. Despite its broad convergence range on small datasets,
DeepMapping still cannot produce satisfactory results on large-scale datasets
with thousands of frames. This is due to the lack of loop closures and exact
cross-frame point correspondences, and the slow convergence of its global
localization network. We propose DeepMapping2 by adding two novel techniques to
address these issues: (1) organization of training batch based on map topology
from loop closing, and (2) self-supervised local-to-global point consistency
loss leveraging pairwise registration. Our experiments and ablation studies on
public datasets (KITTI, NCLT, and Nebula) demonstrate the effectiveness of our
method.",None,-1
114c1a56-466a-4ec2-9846-b1ee9da70883,Generalized Inter-class Loss for Gait Recognition,0.435356,"Gait recognition is a unique biometric technique that can be performed at a
long distance non-cooperatively and has broad applications in public safety and
intelligent traffic systems. Previous gait works focus more on minimizing the
intra-class variance while ignoring the significance in constraining
inter-class variance. To this end, we propose a generalized inter-class loss
which resolves the inter-class variance from both sample-level feature
distribution and class-level feature distribution. Instead of equal penalty
strength on pair scores, the proposed loss optimizes sample-level inter-class
feature distribution by dynamically adjusting the pairwise weight. Further, in
class-level distribution, generalized inter-class loss adds a constraint on the
uniformity of inter-class feature distribution, which forces the feature
representations to approximate a hypersphere and keep maximal inter-class
variance. In addition, the proposed method automatically adjusts the margin
between classes which enables the inter-class feature distribution to be more
flexible. The proposed method can be generalized to different gait recognition
networks and achieves significant improvements. We conduct a series of
experiments on CASIA-B and OUMVLP, and the experimental results show that the
proposed loss can significantly improve the performance and achieves the
state-of-the-art performances.",https://github.com/ShiqiYu/OpenGait.git,-1
a32e001b-9f09-4c8f-ada4-e59db612c2e6,Clinical outcome prediction under hypothetical interventions -- a representation learning framework for counterfactual reasoning,0.0260456,"Most machine learning (ML) models are developed for prediction only; offering
no option for causal interpretation of their predictions or
parameters/properties. This can hamper the health systems' ability to employ ML
models in clinical decision-making processes, where the need and desire for
predicting outcomes under hypothetical investigations (i.e., counterfactual
reasoning/explanation) is high. In this research, we introduce a new
representation learning framework (i.e., partial concept bottleneck), which
considers the provision of counterfactual explanations as an embedded property
of the risk model. Despite architectural changes necessary for jointly
optimising for prediction accuracy and counterfactual reasoning, the accuracy
of our approach is comparable to prediction-only models. Our results suggest
that our proposed framework has the potential to help researchers and
clinicians improve personalised care (e.g., by investigating the hypothetical
differential effects of interventions)",None,-1
c59b4237-a0ac-4901-bf85-7f8fab0a8c12,Global Contentious Politics Database (GLOCON) Annotation Manuals,0.50496,"The database creation utilized automated text processing tools that detect if
a news article contains a protest event, locate protest information within the
article, and extract pieces of information regarding the detected protest
events. The basis of training and testing the automated tools is the GLOCON
Gold Standard Corpus (GSC), which contains news articles from multiple sources
from each focus country. The articles in the GSC were manually coded by skilled
annotators in both classification and extraction tasks with the utmost accuracy
and consistency that automated tool development demands. In order to assure
these, the annotation manuals in this document lay out the rules according to
which annotators code the news articles. Annotators refer to the manuals at all
times for all annotation tasks and apply the rules that they contain. The
content of the annotation manual is built on the general principles and
standards of linguistic annotation laid out in other prominent annotation
manuals such as ACE, CAMEO, and TimeML. These principles, however, have been
adapted or rather modified heavily to accommodate the social scientific
concepts and variables employed in the EMW project. The manual has been molded
throughout a long trial and error process that accompanied the annotation of
the GSC. It owes much of its current shape to the meticulous work and
invaluable feedback provided by highly specialized teams of annotators, whose
diligence and expertise greatly increased the quality of the corpus.",None,-1
b1f4fd3c-79b2-4c54-9ae3-f2f9635b1cc8,Cross-Lingual Text Classification with Multilingual Distillation and Zero-Shot-Aware Training,0.106865,"Multilingual pre-trained language models (MPLMs) not only can handle tasks in
different languages but also exhibit surprising zero-shot cross-lingual
transferability. However, MPLMs usually are not able to achieve comparable
supervised performance on rich-resource languages compared to the
state-of-the-art monolingual pre-trained models. In this paper, we aim to
improve the multilingual model's supervised and zero-shot performance
simultaneously only with the resources from supervised languages. Our approach
is based on transferring knowledge from high-performance monolingual models
with a teacher-student framework. We let the multilingual model learn from
multiple monolingual models simultaneously. To exploit the model's
cross-lingual transferability, we propose MBLM (multi-branch multilingual
language model), a model built on the MPLMs with multiple language branches.
Each branch is a stack of transformers. MBLM is trained with the
zero-shot-aware training strategy that encourages the model to learn from the
mixture of zero-shot representations from all the branches. The results on two
cross-lingual classification tasks show that, with only the task's supervised
data used, our method improves both the supervised and zero-shot performance of
MPLMs.",None,-1
2d1d0777-370f-4fb6-8280-7d7e1af4ef83,Glass Segmentation with RGB-Thermal Image Pairs,0.34831,"This paper proposes a new glass segmentation method utilizing paired RGB and
thermal images. Due to the large difference between the transmission property
of visible light and that of the thermal energy through the glass where most
glass is transparent to the visible light but opaque to thermal energy, glass
regions of a scene are made more distinguishable with a pair of RGB and thermal
images than solely with an RGB image. To exploit such a unique property, we
propose a neural network architecture that effectively combines an RGB-thermal
image pair with a new multi-modal fusion module based on attention, and
integrate CNN and transformer to extract local features and non-local
dependencies, respectively. As well, we have collected a new dataset containing
5551 RGB-thermal image pairs with ground-truth segmentation annotations. The
qualitative and quantitative evaluations demonstrate the effectiveness of the
proposed approach on fusing RGB and thermal data for glass segmentation. Our
code and data are available at
https://github.com/Dong-Huo/RGB-T-Glass-Segmentation.",https://github.com/Dong-Huo/RGB-T-Glass-Segmentation,-1
542d553d-6b84-4769-ac38-3283f387379b,Progressive Continual Learning for Spoken Keyword Spotting,0.369448,"Catastrophic forgetting is a thorny challenge when updating keyword spotting
(KWS) models after deployment. To tackle such challenges, we propose a
progressive continual learning strategy for small-footprint spoken keyword
spotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a
network instantiator to generate the task-specific sub-networks for remembering
previously learned keywords. As a result, the PCL-KWS approach incrementally
learns new keywords without forgetting prior knowledge. Besides, the
keyword-aware network scaling mechanism of PCL-KWS constrains the growth of
model parameters while achieving high performance. Experimental results show
that after learning five new tasks sequentially, our proposed PCL-KWS approach
archives the new state-of-the-art performance of 92.8% average accuracy for all
the tasks on Google Speech Command dataset compared with other baselines.",None,-1
a26040c9-8c65-410c-b6ee-4d5611fa2816,Hybrid Reinforced Medical Report Generation with M-Linear Attention and Repetition Penalty,0.145095,"To reduce doctors' workload, deep-learning-based automatic medical report
generation has recently attracted more and more research efforts, where deep
convolutional neural networks (CNNs) are employed to encode the input images,
and recurrent neural networks (RNNs) are used to decode the visual features
into medical reports automatically. However, these state-of-the-art methods
mainly suffer from three shortcomings: (i) incomprehensive optimization, (ii)
low-order and unidimensional attention mechanisms, and (iii) repeated
generation. In this article, we propose a hybrid reinforced medical report
generation method with m-linear attention and repetition penalty mechanism
(HReMRG-MR) to overcome these problems. Specifically, a hybrid reward with
different weights is employed to remedy the limitations of single-metric-based
rewards. We also propose a search algorithm with linear complexity to
approximate the best weight combination. Furthermore, we use m-linear attention
modules to explore high-order feature interactions and to achieve multi-modal
reasoning, while a repetition penalty applies penalties to repeated terms
during the model's training process. Extensive experimental studies on two
public datasets show that HReMRG-MR greatly outperforms the state-of-the-art
baselines in terms of all metrics. We also conducted a series of ablation
experiments to prove the effectiveness of all our proposed components. We also
performed a reward search toy experiment to give evidence that our proposed
search approach can significantly reduce the search time while approximating
the best performance.",None,-1
2f3e2ccb-6f53-4f4c-84dc-a7ecdcfd69b3,TurbuGAN: An Adversarial Learning Approach to Spatially-Varying Multiframe Blind Deconvolution with Applications to Imaging Through Turbulence,0.950213,"We present a self-supervised and self-calibrating multi-shot approach to
imaging through atmospheric turbulence, called TurbuGAN. Our approach requires
no paired training data, adapts itself to the distribution of the turbulence,
leverages domain-specific data priors, and can generalize from tens to
thousands of measurements. We achieve such functionality through an adversarial
sensing framework adapted from CryoGAN, which uses a discriminator network to
match the distributions of captured and simulated measurements. Our framework
builds on CryoGAN by (1) generalizing the forward measurement model to
incorporate physically accurate and computationally efficient models for light
propagation through anisoplanatic turbulence, (2) enabling adaptation to
slightly misspecified forward models, and (3) leveraging domain-specific prior
knowledge using pretrained generative networks, when available. We validate
TurbuGAN on both computationally simulated and experimentally captured images
distorted with anisoplanatic turbulence.",None,-1
9a56d4a6-21d5-425c-95aa-76efabd75b7a,Towards Better Document-level Relation Extraction via Iterative Inference,0.609229,"Document-level relation extraction (RE) aims to extract the relations between
entities from the input document that usually containing many
difficultly-predicted entity pairs whose relations can only be predicted
through relational inference. Existing methods usually directly predict the
relations of all entity pairs of input document in a one-pass manner, ignoring
the fact that predictions of some entity pairs heavily depend on the predicted
results of other pairs. To deal with this issue, in this paper, we propose a
novel document-level RE model with iterative inference. Our model is mainly
composed of two modules: 1) a base module expected to provide preliminary
relation predictions on entity pairs; 2) an inference module introduced to
refine these preliminary predictions by iteratively dealing with
difficultly-predicted entity pairs depending on other pairs in an easy-to-hard
manner. Unlike previous methods which only consider feature information of
entity pairs, our inference module is equipped with two Extended Cross
Attention units, allowing it to exploit both feature information and previous
predictions of entity pairs during relational inference. Furthermore, we adopt
a two-stage strategy to train our model. At the first stage, we only train our
base module. During the second stage, we train the whole model, where
contrastive learning is introduced to enhance the training of inference module.
Experimental results on three commonly-used datasets show that our model
consistently outperforms other competitive baselines.",https://github.com/DeepLearnXMU/DocRE-II,-1
34bd4ce3-8438-4d28-981a-68365cfcee9b,D2A-BSP: Distilled Data Association Belief Space Planning with Performance Guarantees Under Budget Constraints,0.539801,"Unresolved data association in ambiguous and perceptually aliased
environments leads to multi-modal hypotheses on both the robot's and the
environment state. To avoid catastrophic results, when operating in such
ambiguous environments, it is crucial to reason about data association within
Belief Space Planning (BSP). However, explicitly considering all possible data
associations, the number of hypotheses grows exponentially with the planning
horizon and determining the optimal action sequence quickly becomes
intractable. Moreover, with hard budget constraints where some non-negligible
hypotheses must be pruned, achieving performance guarantees is crucial. In this
work we present a computationally efficient novel approach that utilizes only a
distilled subset of hypotheses to solve BSP problems while reasoning about data
association. Furthermore, to provide performance guarantees, we derive error
bounds with respect to the optimal solution. We then demonstrate our approach
in an extremely aliased environment, where we manage to significantly reduce
computation time without compromising on the quality of the solution.",None,2485
784c588b-92ff-4454-b4b3-f5391e281627,Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space,0.999997,"Transformer-based language models (LMs) are at the core of modern NLP, but
their internal prediction construction process is opaque and largely not
understood. In this work, we make a substantial step towards unveiling this
underlying prediction process, by reverse-engineering the operation of the
feed-forward network (FFN) layers, one of the building blocks of transformer
models. We view the token representation as a changing distribution over the
vocabulary, and the output from each FFN layer as an additive update to that
distribution. Then, we analyze the FFN updates in the vocabulary space, showing
that each update can be decomposed to sub-updates corresponding to single FFN
parameter vectors, each promoting concepts that are often human-interpretable.
We then leverage these findings for controlling LM predictions, where we reduce
the toxicity of GPT2 by almost 50%, and for improving computation efficiency
with a simple early exit rule, saving 20% of computation on average.",https://github.com/aviclu/ffn-values,-1
058ecc12-f391-4e31-b9c5-be5d66b12dbf,First do not fall: learning to exploit a wall with a damaged humanoid robot,0.20677,"Humanoid robots could replace humans in hazardous situations but most of such
situations are equally dangerous for them, which means that they have a high
chance of being damaged and falling. We hypothesize that humanoid robots would
be mostly used in buildings, which makes them likely to be close to a wall. To
avoid a fall, they can therefore lean on the closest wall, as a human would do,
provided that they find in a few milliseconds where to put the hand(s). This
article introduces a method, called D-Reflex, that learns a neural network that
chooses this contact position given the wall orientation, the wall distance,
and the posture of the robot. This contact position is then used by a
whole-body controller to reach a stable posture. We show that D-Reflex allows a
simulated TALOS robot (1.75m, 100kg, 30 degrees of freedom) to avoid more than
75% of the avoidable falls and can work on the real robot.",https://github.com/resibots/robot-dart,9262
6d28bce1-1f7f-4a14-9866-6b21a4dcbf6d,Translated Skip Connections -- Expanding the Receptive Fields of Fully Convolutional Neural Networks,0.112943,"The effective receptive field of a fully convolutional neural network is an
important consideration when designing an architecture, as it defines the
portion of the input visible to each convolutional kernel. We propose a neural
network module, extending traditional skip connections, called the translated
skip connection. Translated skip connections geometrically increase the
receptive field of an architecture with negligible impact on both the size of
the parameter space and computational complexity. By embedding translated skip
connections into a benchmark architecture, we demonstrate that our module
matches or outperforms four other approaches to expanding the effective
receptive fields of fully convolutional neural networks. We confirm this result
across five contemporary image segmentation datasets from disparate domains,
including the detection of COVID-19 infection, segmentation of aerial imagery,
common object segmentation, and segmentation for self-driving cars.",https://github.com/JoshuaDBruton/TSC,-1
891d023b-d167-4b8f-b8d4-1232c2c05c8b,Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?,0.606776,"Several popular Transformer based language models have been found to be
successful for text-driven brain encoding. However, existing literature
leverages only pretrained text Transformer models and has not explored the
efficacy of task-specific learned Transformer representations. In this work, we
explore transfer learning from representations learned for ten popular natural
language processing tasks (two syntactic and eight semantic) for predicting
brain responses from two diverse datasets: Pereira (subjects reading sentences
from paragraphs) and Narratives (subjects listening to the spoken stories).
Encoding models based on task features are used to predict activity in
different regions across the whole brain. Features from coreference resolution,
NER, and shallow syntax parsing explain greater variance for the reading
activity. On the other hand, for the listening activity, tasks such as
paraphrase generation, summarization, and natural language inference show
better encoding performance. Experiments across all 10 task representations
provide the following cognitive insights: (i) language left hemisphere has
higher predictive brain activity versus language right hemisphere, (ii)
posterior medial cortex, temporo-parieto-occipital junction, dorsal frontal
lobe have higher correlation versus early auditory and auditory association
cortex, (iii) syntactic and semantic tasks display a good predictive
performance across brain regions for reading and listening stimuli resp.",None,-1
2b747305-5b8c-4dbc-93a7-68ee030e0680,Unsupervised Sentence Textual Similarity with Compositional Phrase Semantics,0.507148,"Measuring Sentence Textual Similarity (STS) is a classic task that can be
applied to many downstream NLP applications such as text generation and
retrieval. In this paper, we focus on unsupervised STS that works on various
domains but only requires minimal data and computational resources.
Theoretically, we propose a light-weighted Expectation-Correction (EC)
formulation for STS computation. EC formulation unifies unsupervised STS
approaches including the cosine similarity of Additively Composed (AC) sentence
embeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose
the Recursive Optimal Transport Similarity (ROTS) algorithm to capture the
compositional phrase semantics by composing multiple recursive EC formulations.
ROTS finishes in linear time and is faster than its predecessors. ROTS is
empirically more effective and scalable than previous approaches. Extensive
experiments on 29 STS tasks under various settings show the clear advantage of
ROTS over existing approaches. Detailed ablation studies demonstrate the
effectiveness of our approaches.",https://github.com/zihao-wang/rots,-1
df97a8bd-87fb-47b9-8cc9-72f694052435,Explainable Deep Belief Network based Auto encoder using novel Extended Garson Algorithm,0.0674135,"The most difficult task in machine learning is to interpret trained shallow
neural networks. Deep neural networks (DNNs) provide impressive results on a
larger number of tasks, but it is generally still unclear how decisions are
made by such a trained deep neural network. Providing feature importance is the
most important and popular interpretation technique used in shallow and deep
neural networks. In this paper, we develop an algorithm extending the idea of
Garson Algorithm to explain Deep Belief Network based Auto-encoder (DBNA). It
is used to determine the contribution of each input feature in the DBN. It can
be used for any kind of neural network with many hidden layers. The
effectiveness of this method is tested on both classification and regression
datasets taken from literature. Important features identified by this method
are compared against those obtained by Wald chi square (\c{hi}2). For 2 out of
4 classification datasets and 2 out of 5 regression datasets, our proposed
methodology resulted in the identification of better-quality features leading
to statistically more significant results vis-\`a-vis Wald \c{hi}2.",None,-1
6904ee29-51e1-48fd-9af2-86cda2e49f73,A Visual Navigation Perspective for Category-Level Object Pose Estimation,0.2605,"This paper studies category-level object pose estimation based on a single
monocular image. Recent advances in pose-aware generative models have paved the
way for addressing this challenging task using analysis-by-synthesis. The idea
is to sequentially update a set of latent variables, e.g., pose, shape, and
appearance, of the generative model until the generated image best agrees with
the observation. However, convergence and efficiency are two challenges of this
inference procedure. In this paper, we take a deeper look at the inference of
analysis-by-synthesis from the perspective of visual navigation, and
investigate what is a good navigation policy for this specific task. We
evaluate three different strategies, including gradient descent, reinforcement
learning and imitation learning, via thorough comparisons in terms of
convergence, robustness and efficiency. Moreover, we show that a simple hybrid
approach leads to an effective and efficient solution. We further compare these
strategies to state-of-the-art methods, and demonstrate superior performance on
synthetic and real-world datasets leveraging off-the-shelf pose-aware
generative models.",https://github.com/wrld/visual-navigation-pose-estimation.git,-1
78a2aeda-1c41-46d2-b661-5b1010758d70,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,0.140372,"This paper investigates models of event implications. Specifically, how well
models predict entity state-changes, by targeting their understanding of
physical attributes. Nominally, Large Language models (LLM) have been exposed
to procedural knowledge about how objects interact, yet our benchmarking shows
they fail to reason about the world. Conversely, we also demonstrate that
existing approaches often misrepresent the surprising abilities of LLMs via
improper task encodings and that proper model prompting can dramatically
improve performance of reported baseline results across multiple tasks. In
particular, our results indicate that our prompting technique is especially
useful for unseen attributes (out-of-domain) or when only limited data is
available.",https://github.com/spilioeve/eventsrealm,-1
c17067e1-8d3d-4d75-911b-e37b9f0edb60,Splatting-based Synthesis for Video Frame Interpolation,0.620001,"Frame interpolation is an essential video processing technique that adjusts
the temporal resolution of an image sequence. While deep learning has brought
great improvements to the area of video frame interpolation, techniques that
make use of neural networks can typically not easily be deployed in practical
applications like a video editor since they are either computationally too
demanding or fail at high resolutions. In contrast, we propose a deep learning
approach that solely relies on splatting to synthesize interpolated frames.
This splatting-based synthesis for video frame interpolation is not only much
faster than similar approaches, especially for multi-frame interpolation, but
can also yield new state-of-the-art results at high resolutions.",None,-1
eb6c70b0-083a-4770-8048-d62d6b527cdb,On the Importance of Data Size in Probing Fine-tuned Models,0.23669,"Several studies have investigated the reasons behind the effectiveness of
fine-tuning, usually through the lens of probing. However, these studies often
neglect the role of the size of the dataset on which the model is fine-tuned.
In this paper, we highlight the importance of this factor and its undeniable
role in probing performance. We show that the extent of encoded linguistic
knowledge depends on the number of fine-tuning samples. The analysis also
reveals that larger training data mainly affects higher layers, and that the
extent of this change is a factor of the number of iterations updating the
model during fine-tuning rather than the diversity of the training samples.
Finally, we show through a set of experiments that fine-tuning data size
affects the recoverability of the changes made to the model's linguistic
knowledge.",https://github.com/hmehrafarin/,-1
5cc1e784-8c1c-4053-9aee-78b3029bd3b9,BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics,0.0431561,"The proliferation of automatic faithfulness metrics for summarization has
produced a need for benchmarks to evaluate them. While existing benchmarks
measure the correlation with human judgements of faithfulness on
model-generated summaries, they are insufficient for diagnosing whether metrics
are: 1) consistent, i.e., indicate lower faithfulness as errors are introduced
into a summary, 2) effective on human-written texts, and 3) sensitive to
different error types (as summaries can contain multiple errors). To address
these needs, we present a benchmark of unfaithful minimal pairs (BUMP), a
dataset of 889 human-written, minimally different summary pairs, where a single
error is introduced to a summary from the CNN/DailyMail dataset to produce an
unfaithful summary. We find BUMP complements existing benchmarks in a number of
ways: 1) the summaries in BUMP are harder to discriminate and less probable
under SOTA summarization models, 2) unlike non-pair-based datasets, BUMP can be
used to measure the consistency of metrics, and reveals that the most
discriminative metrics tend not to be the most consistent, and 3) unlike
datasets containing generated summaries with multiple errors, BUMP enables the
measurement of metrics' performance on individual error types.",https://github.com/dataminr-ai/BUMP,-1
ddefa8d8-956d-4421-92cf-907f7e2fec5f,Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection,0.815495,"Nowadays, most methods in end-to-end contextual speech recognition bias the
recognition process towards contextual knowledge. Since all-neural contextual
biasing methods rely on phrase-level contextual modeling and attention-based
relevance modeling, they may encounter confusion between similar
context-specific phrases, which hurts predictions at the token level. In this
work, we focus on mitigating confusion problems with fine-grained contextual
knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge
to reduce the uncertainty of token predictions. Specifically, we first apply
phrase selection to narrow the range of phrase candidates, and then conduct
token attention on the tokens in the selected phrase candidates. Moreover, we
re-normalize the attention weights of most relevant phrases in inference to
obtain more focused phrase-level contextual representations, and inject
position information to better discriminate phrases or tokens. On LibriSpeech
and an in-house 160,000-hour dataset, we explore the proposed methods based on
a controllable all-neural biasing method, collaborative decoding (ColDec). The
proposed methods provide at most 6.1% relative word error rate reduction on
LibriSpeech and 16.4% relative character error rate reduction on the in-house
dataset over ColDec.",https://github.com/MingLunHan/CIF-ColDec,-1
3bf69f76-747c-4343-a83c-0bf911b20b5f,Partially Does It: Towards Scene-Level FG-SBIR with Partial Input,0.924695,"We scrutinise an important observation plaguing scene-level sketch research
-- that a significant portion of scene sketches are ""partial"". A quick pilot
study reveals: (i) a scene sketch does not necessarily contain all objects in
the corresponding photo, due to the subjective holistic interpretation of
scenes, (ii) there exists significant empty (white) regions as a result of
object-level abstraction, and as a result, (iii) existing scene-level
fine-grained sketch-based image retrieval methods collapse as scene sketches
become more partial. To solve this ""partial"" problem, we advocate for a simple
set-based approach using optimal transport (OT) to model cross-modal region
associativity in a partially-aware fashion. Importantly, we improve upon OT to
further account for holistic partialness by comparing intra-modal adjacency
matrices. Our proposed method is not only robust to partial scene-sketches but
also yields state-of-the-art performance on existing datasets.",None,-1
905b9c8d-787e-422c-a34a-c8178b4dd9be,Keys to Better Image Inpainting: Structure and Texture Go Hand in Hand,0.709273,"Deep image inpainting has made impressive progress with recent advances in
image generation and processing algorithms. We claim that the performance of
inpainting algorithms can be better judged by the generated structures and
textures. Structures refer to the generated object boundary or novel geometric
structures within the hole, while texture refers to high-frequency details,
especially man-made repeating patterns filled inside the structural regions. We
believe that better structures are usually obtained from a coarse-to-fine
GAN-based generator network while repeating patterns nowadays can be better
modeled using state-of-the-art high-frequency fast fourier convolutional
layers. In this paper, we propose a novel inpainting network combining the
advantages of the two designs. Therefore, our model achieves a remarkable
visual quality to match state-of-the-art performance in both structure
generation and repeating texture synthesis using a single network. Extensive
experiments demonstrate the effectiveness of the method, and our conclusions
further highlight the two critical factors of image inpainting quality,
structures, and textures, as the future design directions of inpainting
networks.",https://github.com/SHI-Labs/FcF-Inpainting/,-1
71b3877b-1c64-40ef-9ab4-d9e4c0f16075,PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences,0.996914,"Point cloud sequences are irregular and unordered in the spatial dimension
while exhibiting regularities and order in the temporal dimension. Therefore,
existing grid based convolutions for conventional video processing cannot be
directly applied to spatio-temporal modeling of raw point cloud sequences. In
this paper, we propose a point spatio-temporal (PST) convolution to achieve
informative representations of point cloud sequences. The proposed PST
convolution first disentangles space and time in point cloud sequences. Then, a
spatial convolution is employed to capture the local structure of points in the
3D space, and a temporal convolution is used to model the dynamics of the
spatial regions along the time dimension. Furthermore, we incorporate the
proposed PST convolution into a deep network, namely PSTNet, to extract
features of point cloud sequences in a hierarchical manner. Extensive
experiments on widely-used 3D action recognition and 4D semantic segmentation
datasets demonstrate the effectiveness of PSTNet to model point cloud
sequences.",None,-1
0b7ae3a8-cc36-4813-8e7d-e5a701c73614,longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks,0.457757,"Developing methods to adversarially challenge NLP systems is a promising
avenue for improving both model performance and interpretability. Here, we
describe the approach of the team ""longhorns"" on Task 1 of the The First
Workshop on Dynamic Adversarial Data Collection (DADC), which asked teams to
manually fool a model on an Extractive Question Answering task. Our team
finished first, with a model error rate of 62%. We advocate for a systematic,
linguistically informed approach to formulating adversarial questions, and we
describe the results of our pilot experiments, as well as our official
submission.",None,5637
0eee77ee-1937-45f5-a1ed-a23be37569c4,MiniViT: Compressing Vision Transformers with Weight Multiplexing,0.777443,"Vision Transformer (ViT) models have recently drawn much attention in
computer vision due to their high model capability. However, ViT models suffer
from huge number of parameters, restricting their applicability on devices with
limited memory. To alleviate this problem, we propose MiniViT, a new
compression framework, which achieves parameter reduction in vision
transformers while retaining the same performance. The central idea of MiniViT
is to multiplex the weights of consecutive transformer blocks. More
specifically, we make the weights shared across layers, while imposing a
transformation on the weights to increase diversity. Weight distillation over
self-attention is also applied to transfer knowledge from large-scale ViT
models to weight-multiplexed compact models. Comprehensive experiments
demonstrate the efficacy of MiniViT, showing that it can reduce the size of the
pre-trained Swin-B transformer by 48\%, while achieving an increase of 1.0\% in
Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters,
MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters,
without seriously compromising the performance. Finally, we verify the
transferability of MiniViT by reporting its performance on downstream
benchmarks. Code and models are available at here.",None,18607
3662c7cf-a397-4a77-b48f-c0b06f58d6a2,Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing,0.722407,"The explosion of e-commerce has caused the need for processing and analysis
of product titles, like entity typing in product titles. However, the rapid
activity in e-commerce has led to the rapid emergence of new entities, which is
difficult to be solved by general entity typing. Besides, product titles in
e-commerce have very different language styles from text data in general
domain. In order to handle new entities in product titles and address the
special language styles problem of product titles in e-commerce domain, we
propose our textual entailment model with continuous prompt tuning based
hypotheses and fusion embeddings for e-commerce entity typing. First, we
reformulate the entity typing task into a textual entailment problem to handle
new entities that are not present during training. Second, we design a model to
automatically generate textual entailment hypotheses using a continuous prompt
tuning method, which can generate better textual entailment hypotheses without
manual design. Third, we utilize the fusion embeddings of BERT embedding and
CharacterBERT embedding with a two-layer MLP classifier to solve the problem
that the language styles of product titles in e-commerce are different from
that of general domain. To analyze the effect of each contribution, we compare
the performance of entity typing and textual entailment model, and conduct
ablation studies on continuous prompt tuning and fusion embeddings. We also
evaluate the impact of different prompt template initialization for the
continuous prompt tuning. We show our proposed model improves the average F1
score by around 2% compared to the baseline BERT entity typing model.",None,197861
8cbfd227-11b8-4802-bf66-fc6001dcf7e4,Object Manipulation via Visual Target Localization,0.270327,"Object manipulation is a critical skill required for Embodied AI agents
interacting with the world around them. Training agents to manipulate objects,
poses many challenges. These include occlusion of the target object by the
agent's arm, noisy object detection and localization, and the target frequently
going out of view as the agent moves around in the scene. We propose
Manipulation via Visual Object Location Estimation (m-VOLE), an approach that
explores the environment in search for target objects, computes their 3D
coordinates once they are located, and then continues to estimate their 3D
locations even when the objects are not visible, thus robustly aiding the task
of manipulating these objects throughout the episode. Our evaluations show a
massive 3x improvement in success rate over a model that has access to the same
sensory suite but is trained without the object location estimator, and our
analysis shows that our agent is robust to noise in depth perception and agent
localization. Importantly, our proposed approach relaxes several assumptions
about idealized localization and perception that are commonly employed by
recent works in embodied AI -- an important step towards training agents for
object manipulation in the real world.",None,-1
d404aa7d-4d0f-4a57-9b77-4f9ee83bf8d9,A Large Scale Search Dataset for Unbiased Learning to Rank,0.910732,"The unbiased learning to rank (ULTR) problem has been greatly advanced by
recent deep learning techniques and well-designed debias algorithms. However,
promising results on the existing benchmark datasets may not be extended to the
practical scenario due to the following disadvantages observed from those
popular benchmark datasets: (1) outdated semantic feature extraction where
state-of-the-art large scale pre-trained language models like BERT cannot be
exploited due to the missing of the original text;(2) incomplete display
features for in-depth study of ULTR, e.g., missing the displayed abstract of
documents for analyzing the click necessary bias; (3) lacking real-world user
feedback, leading to the prevalence of synthetic datasets in the empirical
study. To overcome the above disadvantages, we introduce the Baidu-ULTR
dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008
expert annotated queries, which is orders of magnitude larger than the existing
ones. Baidu-ULTR provides:(1) the original semantic feature and a pre-trained
language model for easy usage; (2) sufficient display information such as
position, displayed height, and displayed abstract, enabling the comprehensive
study of different biases with advanced techniques such as causal discovery and
meta-learning; and (3) rich user feedback on search result pages (SERPs) like
dwelling time, allowing for user engagement optimization and promoting the
exploration of multi-task learning in ULTR. In this paper, we present the
design principle of Baidu-ULTR and the performance of benchmark ULTR algorithms
on this new data resource, favoring the exploration of ranking for long-tail
queries and pre-training tasks for ranking. The Baidu-ULTR dataset and
corresponding baseline implementation are available at
https://github.com/ChuXiaokai/baidu_ultr_dataset.",https://github.com/ChuXiaokai/baidu_ultr_dataset,-1
06070d40-9d96-470b-ade4-9ac9ff97daed,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,0.616363,"Question Answering (QA) is a longstanding challenge in natural language
processing. Existing QA works mostly focus on specific question types,
knowledge domains, or reasoning skills. The specialty in QA research hinders
systems from modeling commonalities between tasks and generalization for wider
applications. To address this issue, we present ProQA, a unified QA paradigm
that solves various tasks through a single model. ProQA takes a unified
structural prompt as the bridge and improves the QA-centric ability by
structural prompt-based pre-training. Through a structurally designed
prompt-based input schema, ProQA concurrently models the knowledge
generalization for all QA tasks while keeping the knowledge customization for
every specific QA task. Furthermore, ProQA is pre-trained with structural
prompt-formatted large-scale synthesized corpus, which empowers the model with
the commonly-required QA ability. Experimental results on 11 QA benchmarks
demonstrate that ProQA consistently boosts performance on both full data
fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,
ProQA exhibits strong ability in both continual learning and transfer learning
by taking the advantages of the structural prompt.",https://github.com/zhongwanjun/ProQA,-1
8858b84e-b7cf-44bf-baf2-822a8211c631,Exploring Linear Feature Disentanglement For Neural Networks,0.12485,"Non-linear activation functions, e.g., Sigmoid, ReLU, and Tanh, have achieved
great success in neural networks (NNs). Due to the complex non-linear
characteristic of samples, the objective of those activation functions is to
project samples from their original feature space to a linear separable feature
space. This phenomenon ignites our interest in exploring whether all features
need to be transformed by all non-linear functions in current typical NNs,
i.e., whether there exists a part of features arriving at the linear separable
feature space in the intermediate layers, that does not require further
non-linear variation but an affine transformation instead. To validate the
above hypothesis, we explore the problem of linear feature disentanglement for
neural networks in this paper. Specifically, we devise a learnable mask module
to distinguish between linear and non-linear features. Through our designed
experiments we found that some features reach the linearly separable space
earlier than the others and can be detached partly from the NNs. The explored
method also provides a readily feasible pruning strategy which barely affects
the performance of the original model. We conduct our experiments on four
datasets and present promising results.",None,-1
d04ff02f-6e4c-48a1-99c2-202f4db50b56,Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation,0.922059,"The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks.",None,-1
50ac2ecd-9291-4300-b749-9d7be7d537d5,ComMU: Dataset for Combinatorial Music Generation,0.677715,"Commercial adoption of automatic music composition requires the capability of
generating diverse and high-quality music suitable for the desired context
(e.g., music for romantic movies, action games, restaurants, etc.). In this
paper, we introduce combinatorial music generation, a new task to create
varying background music based on given conditions. Combinatorial music
generation creates short samples of music with rich musical metadata, and
combines them to produce a complete music. In addition, we introduce ComMU, the
first symbolic music dataset consisting of short music samples and their
corresponding 12 musical metadata for combinatorial music generation. Notable
properties of ComMU are that (1) dataset is manually constructed by
professional composers with an objective guideline that induces regularity, and
(2) it has 12 musical metadata that embraces composers' intentions. Our results
show that we can generate diverse high-quality music only with metadata, and
that our unique metadata such as track-role and extended chord quality improves
the capacity of the automatic composition. We highly recommend watching our
video before reading the paper (https://pozalabs.github.io/ComMU).",None,-1
03fa7109-2e04-4ffc-9d1e-a12b7f829f43,Prosodic Alignment for off-screen automatic dubbing,0.833214,"The goal of automatic dubbing is to perform speech-to-speech translation
while achieving audiovisual coherence. This entails isochrony, i.e.,
translating the original speech by also matching its prosodic structure into
phrases and pauses, especially when the speaker's mouth is visible. In previous
work, we introduced a prosodic alignment model to address isochrone or
on-screen dubbing. In this work, we extend the prosodic alignment model to also
address off-screen dubbing that requires less stringent synchronization
constraints. We conduct experiments on four dubbing directions - English to
French, Italian, German and Spanish - on a publicly available collection of TED
Talks and on publicly available YouTube videos. Empirical results show that
compared to our previous work the extended prosodic alignment model provides
significantly better subjective viewing experience on videos in which on-screen
and off-screen automatic dubbing is applied for sentences with speakers mouth
visible and not visible, respectively.",None,-1
35f0103e-8db3-4139-b407-c83393a082fc,Bubble identification from images with machine learning methods,0.424227,"An automated and reliable processing of bubbly flow images is highly needed
to analyse large data sets of comprehensive experimental series. A particular
difficulty arises due to overlapping bubble projections in recorded images,
which highly complicates the identification of individual bubbles. Recent
approaches focus on the use of deep learning algorithms for this task and have
already proven the high potential of such techniques. The main difficulties are
the capability to handle different image conditions, higher gas volume
fractions and a proper reconstruction of the hidden segment of a partly
occluded bubble. In the present work, we try to tackle these points by testing
three different methods based on Convolutional Neural Networks (CNNs) for the
two former and two individual approaches that can be used subsequently to
address the latter. To validate our methodology, we created test data sets with
synthetic images that further demonstrate the capabilities as well as
limitations of our combined approach. The generated data, code and trained
models are made accessible to facilitate the use as well as further
developments in the research field of bubble recognition in experimental
images.",https://github.com/ywflow/BubMask,-1
03ea0dae-d434-496a-b527-bd2ce7cb896e,Deep Understanding based Multi-Document Machine Reading Comprehension,0.328947,"Most existing multi-document machine reading comprehension models mainly
focus on understanding the interactions between the input question and
documents, but ignore following two kinds of understandings. First, to
understand the semantic meaning of words in the input question and documents
from the perspective of each other. Second, to understand the supporting cues
for a correct answer from the perspective of intra-document and
inter-documents. Ignoring these two kinds of important understandings would
make the models oversee some important information that may be helpful for
inding correct answers. To overcome this deiciency, we propose a deep
understanding based model for multi-document machine reading comprehension. It
has three cascaded deep understanding modules which are designed to understand
the accurate semantic meaning of words, the interactions between the input
question and documents, and the supporting cues for the correct answer. We
evaluate our model on two large scale benchmark datasets, namely TriviaQA Web
and DuReader. Extensive experiments show that our model achieves
state-of-the-art results on both datasets.",https://github.com/huggingface/transformers/,-1
cc67d048-b15b-4f21-9823-be64f4a3642d,One-Shot Adaptation of GAN in Just One CLIP,0.843755,"There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.",https://github.com/cyclomon/OneshotCLIP,-1
23094c27-887d-467e-baad-37ce52857f0c,Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study,0.30678,"This paper presents an empirical study to build relation extraction systems
in low-resource settings. Based upon recent pre-trained language models, we
comprehensively investigate three schemes to evaluate the performance in
low-resource settings: (i) different types of prompt-based methods with
few-shot labeled data; (ii) diverse balancing methods to address the
long-tailed distribution issue; (iii) data augmentation technologies and
self-training to generate more labeled in-domain data. We create a benchmark
with 8 relation extraction (RE) datasets covering different languages, domains
and contexts and perform extensive comparisons over the proposed schemes with
combinations. Our experiments illustrate: (i) Though prompt-based tuning is
beneficial in low-resource RE, there is still much potential for improvement,
especially in extracting relations from cross-sentence contexts with multiple
relational triples; (ii) Balancing methods are not always helpful for RE with
long-tailed distribution; (iii) Data augmentation complements existing
baselines and can bring much performance gain, while self-training may not
consistently achieve advancement to low-resource RE. Code and datasets are in
https://github.com/zjunlp/LREBench.",https://github.com/zjunlp/LREBench,16796
62fe11e9-ce80-4223-a335-1df0c81dd372,FixMatchSeg: Fixing FixMatch for Semi-Supervised Semantic Segmentation,0.340588,"Supervised deep learning methods for semantic medical image segmentation are
getting increasingly popular in the past few years.However, in resource
constrained settings, getting large number of annotated images is very
difficult as it mostly requires experts, is expensive and
time-consuming.Semi-supervised segmentation can be an attractive solution where
a very few labeled images are used along with a large number of unlabeled ones.
While the gap between supervised and semi-supervised methods have been
dramatically reduced for classification problems in the past couple of years,
there still remains a larger gap in segmentation methods. In this work, we
adapt a state-of-the-art semi-supervised classification method FixMatch to
semantic segmentation task, introducing FixMatchSeg. FixMatchSeg is evaluated
in four different publicly available datasets of different anatomy and
different modality: cardiac ultrasound, chest X-ray, retinal fundus image, and
skin images. When there are few labels, we show that FixMatchSeg performs on
par with strong supervised baselines.",https://github.com/qubvel/segmentation_models.pytorch,-1
69b52b7e-0bad-4d5c-b212-3d5b2eb170e0,"F-coref: Fast, Accurate and Easy to Use Coreference Resolution",0.843522,"We introduce fastcoref, a python package for fast, accurate, and easy-to-use
English coreference resolution. The package is pip-installable, and allows two
modes: an accurate mode based on the LingMess architecture, providing
state-of-the-art coreference accuracy, and a substantially faster model,
F-coref, which is the focus of this work. F-coref allows to process 2.8K
OntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the
LingMess model, and to 12 minutes of the popular AllenNLP coreference model)
with only a modest drop in accuracy. The fast speed is achieved through a
combination of distillation of a compact model from the LingMess model, and an
efficient batching implementation using a technique we call leftover batching.
Our code is available at https://github.com/shon-otmazgin/fastcoref",https://github.com/shon-otmazgin/fastcoref,-1
d126cdc6-06c2-4107-bb85-9aa6842ecf58,Transformer Lesion Tracker,0.0981136,"Evaluating lesion progression and treatment response via longitudinal lesion
tracking plays a critical role in clinical practice. Automated approaches for
this task are motivated by prohibitive labor costs and time consumption when
lesion matching is done manually. Previous methods typically lack the
integration of local and global information. In this work, we propose a
transformer-based approach, termed Transformer Lesion Tracker (TLT).
Specifically, we design a Cross Attention-based Transformer (CAT) to capture
and combine both global and local information to enhance feature extraction. We
also develop a Registration-based Anatomical Attention Module (RAAM) to
introduce anatomical information to CAT so that it can focus on useful feature
knowledge. A Sparse Selection Strategy (SSS) is presented for selecting
features and reducing memory footprint in Transformer training. In addition, we
use a global regression to further improve model performance. We conduct
experiments on a public dataset to show the superiority of our method and find
that our model performance has improved the average Euclidean center error by
at least 14.3% (6mm vs. 7mm) compared with the state-of-the-art (SOTA). Code is
available at https://github.com/TangWen920812/TLT.",https://github.com/TangWen920812/TLT,-1
de83b9ca-3372-439c-874f-9299823ba4dc,Reinforcement Learning with Prior Policy Guidance for Motion Planning of Dual-Arm Free-Floating Space Robot,0.948033,"Reinforcement learning methods as a promising technique have achieved
superior results in the motion planning of free-floating space robots. However,
due to the increase in planning dimension and the intensification of system
dynamics coupling, the motion planning of dual-arm free-floating space robots
remains an open challenge. In particular, the current study cannot handle the
task of capturing a non-cooperative object due to the lack of the pose
constraint of the end-effectors. To address the problem, we propose a novel
algorithm, EfficientLPT, to facilitate RL-based methods to improve planning
accuracy efficiently. Our core contributions are constructing a mixed policy
with prior knowledge guidance and introducing infinite norm to build a more
reasonable reward function. Furthermore, our method successfully captures a
rotating object with different spinning speeds.",None,-1
50d7aae0-fa74-49a7-bf95-373f98e3a3bc,Readability Controllable Biomedical Document Summarization,0.99587,"Different from general documents, it is recognised that the ease with which
people can understand a biomedical text is eminently varied, owing to the
highly technical nature of biomedical documents and the variance of readers'
domain knowledge. However, existing biomedical document summarization systems
have paid little attention to readability control, leaving users with summaries
that are incompatible with their levels of expertise. In recognition of this
urgent demand, we introduce a new task of readability controllable
summarization for biomedical documents, which aims to recognise users'
readability demands and generate summaries that better suit their needs:
technical summaries for experts and plain language summaries (PLS) for laymen.
To establish this task, we construct a corpus consisting of biomedical papers
with technical summaries and PLSs written by the authors, and benchmark
multiple advanced controllable abstractive and extractive summarization models
based on pre-trained language models (PLMs) with prevalent controlling and
generation techniques. Moreover, we propose a novel masked language model (MLM)
based metric and its variant to effectively evaluate the readability
discrepancy between lay and technical summaries. Experimental results from
automated and human evaluations show that though current control techniques
allow for a certain degree of readability adjustment during generation, the
performance of existing controllable summarization methods is far from
desirable in this task.",None,-1
31e1af43-cf9c-413c-8bb6-e586e6a5ea74,What do Models Learn From Training on More Than Text? Measuring Visual Commonsense Knowledge,0.0618474,"There are limitations in learning language from text alone. Therefore, recent
focus has been on developing multimodal models. However, few benchmarks exist
that can measure what language models learn about language from multimodal
training. We hypothesize that training on a visual modality should improve on
the visual commonsense knowledge in language models. Therefore, we introduce
two evaluation tasks for measuring visual commonsense knowledge in language
models and use them to evaluate different multimodal models and unimodal
baselines. Primarily, we find that the visual commonsense knowledge is not
significantly different between the multimodal models and unimodal baseline
models trained on visual text data.",https://github.com/lovhag/measure-visual-commonsense-knowledge,32
9812715c-1e49-4bd7-ab68-a8af5f5e2b5f,MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer for Autonomous Driving,0.222106,"3D object detection is a significant task for autonomous driving. Recently
with the progress of vision transformers, the 2D object detection problem is
being treated with the set-to-set loss. Inspired by these approaches on 2D
object detection and an approach for multi-view 3D object detection DETR3D, we
propose MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer architecture to
fuse image and LiDAR features to improve the detection accuracy. Our end-to-end
single-stage, anchor-free and NMS-free network takes in multi-view images and
LiDAR point clouds and predicts 3D bounding boxes. Firstly, we link the object
queries learnt from data to the image and LiDAR features using a novel
MSF3DDETR cross-attention block. Secondly, the object queries interacts with
each other in multi-head self-attention block. Finally, MSF3DDETR block is
repeated for $L$ number of times to refine the object queries. The MSF3DDETR
network is trained end-to-end on the nuScenes dataset using Hungarian algorithm
based bipartite matching and set-to-set loss inspired by DETR. We present both
quantitative and qualitative results which are competitive to the
state-of-the-art approaches.",None,-1
3b6d1630-3090-430d-b98d-c855fdb2ff2b,Reinforced Imitative Graph Learning for Mobile User Profiling,0.176377,"Mobile user profiling refers to the efforts of extracting users'
characteristics from mobile activities. In order to capture the dynamic varying
of user characteristics for generating effective user profiling, we propose an
imitation-based mobile user profiling framework. Considering the objective of
teaching an autonomous agent to imitate user mobility based on the user's
profile, the user profile is the most accurate when the agent can perfectly
mimic the user behavior patterns. The profiling framework is formulated into a
reinforcement learning task, where an agent is a next-visit planner, an action
is a POI that a user will visit next, and the state of the environment is a
fused representation of a user and spatial entities. An event in which a user
visits a POI will construct a new state, which helps the agent predict users'
mobility more accurately. In the framework, we introduce a spatial Knowledge
Graph (KG) to characterize the semantics of user visits over connected spatial
entities. Additionally, we develop a mutual-updating strategy to quantify the
state that evolves over time. Along these lines, we develop a reinforcement
imitative graph learning framework for mobile user profiling. Finally, we
conduct extensive experiments to demonstrate the superiority of our approach.",https://github.com/maciejkula/spotlight,39478
570bc895-7f7c-4cdb-af67-f9828e0061d4,Controllable Radiance Fields for Dynamic Face Synthesis,0.673508,"Recent work on 3D-aware image synthesis has achieved compelling results using
advances in neural rendering. However, 3D-aware synthesis of face dynamics
hasn't received much attention. Here, we study how to explicitly control
generative model synthesis of face dynamics exhibiting non-rigid motion (e.g.,
facial expression change), while simultaneously ensuring 3D-awareness. For this
we propose a Controllable Radiance Field (CoRF): 1) Motion control is achieved
by embedding motion features within the layered latent motion space of a
style-based generator; 2) To ensure consistency of background, motion features
and subject-specific attributes such as lighting, texture, shapes, albedo, and
identity, a face parsing net, a head regressor and an identity encoder are
incorporated. On head image/video data we show that CoRFs are 3D-aware while
enabling editing of identity, viewing directions, and motion.",https://github.com/YadiraF/DECA,-1
094a051d-2860-412d-999f-4fa5de010f43,Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes,0.214331,"In many sequential tasks, a model needs to remember relevant events from the
distant past to make correct predictions. Unfortunately, a straightforward
application of gradient based training requires intermediate computations to be
stored for every element of a sequence. This requires to store prohibitively
large intermediate data if a sequence consists of thousands or even millions
elements, and as a result, makes learning of very long-term dependencies
infeasible. However, the majority of sequence elements can usually be predicted
by taking into account only temporally local information. On the other hand,
predictions affected by long-term dependencies are sparse and characterized by
high uncertainty given only local information. We propose MemUP, a new training
method that allows to learn long-term dependencies without backpropagating
gradients through the whole sequence at a time. This method can potentially be
applied to any recurrent architecture. LSTM network trained with MemUP performs
better or comparable to baselines while requiring to store less intermediate
data.",https://github.com/griver/memup,1912
cd56b4c9-30de-4c21-a1fc-e8cb7b7529dd,OrthoMAD: Morphing Attack Detection Through Orthogonal Identity Disentanglement,0.388971,"Morphing attacks are one of the many threats that are constantly affecting
deep face recognition systems. It consists of selecting two faces from
different individuals and fusing them into a final image that contains the
identity information of both. In this work, we propose a novel regularisation
term that takes into account the existent identity information in both and
promotes the creation of two orthogonal latent vectors. We evaluate our
proposed method (OrthoMAD) in five different types of morphing in the FRLL
dataset and evaluate the performance of our model when trained on five distinct
datasets. With a small ResNet-18 as the backbone, we achieve state-of-the-art
results in the majority of the experiments, and competitive results in the
others. The code of this paper will be publicly available.",https://github.com/NetoPedro/OrthoMAD,-1
7398cae7-9099-4c87-a061-2c3d11a55d9e,Batch Bayesian optimisation via density-ratio estimation with guarantees,0.278053,"Bayesian optimisation (BO) algorithms have shown remarkable success in
applications involving expensive black-box functions. Traditionally BO has been
set as a sequential decision-making process which estimates the utility of
query points via an acquisition function and a prior over functions, such as a
Gaussian process. Recently, however, a reformulation of BO via density-ratio
estimation (BORE) allowed reinterpreting the acquisition function as a
probabilistic binary classifier, removing the need for an explicit prior over
functions and increasing scalability. In this paper, we present a theoretical
analysis of BORE's regret and an extension of the algorithm with improved
uncertainty estimates. We also show that BORE can be naturally extended to a
batch optimisation setting by recasting the problem as approximate Bayesian
inference. The resulting algorithms come equipped with theoretical performance
guarantees and are assessed against other batch and sequential BO baselines in
a series of experiments.",https://github.com/rafaol/batch-bore-with-guarantees,-1
2f5a297a-e46f-4b88-b60d-c1a5c2f941a3,Universal Domain Adaptive Object Detector,0.539209,"Universal domain adaptive object detection (UniDAOD)is more challenging than
domain adaptive object detection (DAOD) since the label space of the source
domain may not be the same as that of the target and the scale of objects in
the universal scenarios can vary dramatically (i.e, category shift and scale
shift). To this end, we propose US-DAF, namely Universal Scale-Aware Domain
Adaptive Faster RCNN with Multi-Label Learning, to reduce the negative transfer
effect during training while maximizing transferability as well as
discriminability in both domains under a variety of scales. Specifically, our
method is implemented by two modules: 1) We facilitate the feature alignment of
common classes and suppress the interference of private classes by designing a
Filter Mechanism module to overcome the negative transfer caused by category
shift. 2) We fill the blank of scale-aware adaptation in object detection by
introducing a new Multi-Label Scale-Aware Adapter to perform individual
alignment between the corresponding scale for two domains. Experiments show
that US-DAF achieves state-of-the-art results on three scenarios (i.e,
Open-Set, Partial-Set, and Closed-Set) and yields 7.1% and 5.9% relative
improvement on benchmark datasets Clipart1k and Watercolor in particular.",None,-1
62816451-a94c-4650-9766-056a03421a59,Learning to Decompose Visual Features with Latent Textual Prompts,0.783576,"Recent advances in pre-training vision-language models like CLIP have shown
great potential in learning transferable visual representations. Nonetheless,
for downstream inference, CLIP-like models suffer from either 1) degraded
accuracy and robustness in the case of inaccurate text descriptions during
retrieval-based inference (the challenge for zero-shot protocol); or 2)
breaking the well-established vision-language alignment (the challenge for
linear probing). To address them, we propose Decomposed Feature Prompting
(DeFo). DeFo leverages a flexible number of learnable embeddings as textual
input while maintaining the vision-language dual-model architecture, which
enables the model to learn decomposed visual features with the help of
feature-level textual prompts. We further use an additional linear layer to
perform classification, allowing a scalable size of language inputs. Our
empirical study shows DeFo's significance in improving the vision-language
models. For example, DeFo obtains 73.2% test accuracy on ImageNet with a
ResNet-50 backbone without tuning any pretrained weights of both the vision and
language encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and
outperforming state-of-the-art vision-language prompt tuning method by 7.6%.",None,-1
3eab7308-69a3-4063-9be6-86109790f127,Selection Strategies for Commonsense Knowledge,0.148966,"Selection strategies are broadly used in first-order logic theorem proving to
select those parts of a large knowledge base that are necessary to proof a
theorem at hand. Usually, these selection strategies do not take the meaning of
symbol names into account. In knowledge bases with commonsense knowledge,
symbol names are usually chosen to have a meaning and this meaning provides
valuable information for selection strategies. We introduce the vector-based
selection strategy, a purely statistical selection technique for commonsense
knowledge based on word embeddings. We compare different commonsense knowledge
selection techniques for the purpose of theorem proving and demonstrate the
usefulness of vector-based selection with a case study.",None,-1
38d31900-02e2-46a8-8fba-2ab667a75c70,Tackling Background Distraction in Video Object Segmentation,0.51607,"Semi-supervised video object segmentation (VOS) aims to densely track certain
designated objects in videos. One of the main challenges in this task is the
existence of background distractors that appear similar to the target objects.
We propose three novel strategies to suppress such distractors: 1) a
spatio-temporally diversified template construction scheme to obtain
generalized properties of the target objects; 2) a learnable distance-scoring
function to exclude spatially-distant distractors by exploiting the temporal
consistency between two consecutive frames; 3) swap-and-attach augmentation to
force each object to have unique features by providing training samples
containing entangled objects. On all public benchmark datasets, our model
achieves a comparable performance to contemporary state-of-the-art approaches,
even with real-time performance. Qualitative results also demonstrate the
superiority of our approach over existing methods. We believe our approach will
be widely used for future VOS research.",https://github.com/suhwan-cho/TBD,-1
df171a4c-e48f-4cf1-8a9b-f67bbda24ce8,Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification,0.518091,"Semi-supervised learning (SSL) is a common approach to learning predictive
models using not only labeled examples, but also unlabeled examples. While SSL
for the simple tasks of classification and regression has received a lot of
attention from the research community, this is not properly investigated for
complex prediction tasks with structurally dependent variables. This is the
case of multi-label classification and hierarchical multi-label classification
tasks, which may require additional information, possibly coming from the
underlying distribution in the descriptive space provided by unlabeled
examples, to better face the challenging task of predicting simultaneously
multiple class labels.
  In this paper, we investigate this aspect and propose a (hierarchical)
multi-label classification method based on semi-supervised learning of
predictive clustering trees. We also extend the method towards ensemble
learning and propose a method based on the random forest approach. Extensive
experimental evaluation conducted on 23 datasets shows significant advantages
of the proposed method and its extension with respect to their supervised
counterparts. Moreover, the method preserves interpretability and reduces the
time complexity of classical tree-based models.",https://github.com/knowledge-technologies/clus,-1
f52bd570-902c-4470-bfee-d81805a5c7c7,Social Choice Around the Block: On the Computational Social Choice of Blockchain,0.621248,"One of the most innovative aspects of blockchain technology consists in the
introduction of an incentive layer to regulate the behavior of distributed
protocols. The designer of a blockchain system faces therefore issues that are
akin to those relevant for the design of economic mechanisms, and faces them in
a computational setting. From this perspective the present paper argues for the
importance of computational social choice in blockchain research. It identifies
a few challenges at the interface of the two fields that illustrate the strong
potential for cross-fertilization between them.",None,-1
58bdb750-54e5-473e-9f0d-2fa19abc0612,"Dynamic Sparse Network for Time Series Classification: Learning What to ""see''",0.872512,"The receptive field (RF), which determines the region of time series to be
``seen'' and used, is critical to improve the performance for time series
classification (TSC). However, the variation of signal scales across and within
time series data, makes it challenging to decide on proper RF sizes for TSC. In
this paper, we propose a dynamic sparse network (DSN) with sparse connections
for TSC, which can learn to cover various RF without cumbersome
hyper-parameters tuning. The kernels in each sparse layer are sparse and can be
explored under the constraint regions by dynamic sparse training, which makes
it possible to reduce the resource cost. The experimental results show that the
proposed DSN model can achieve state-of-art performance on both univariate and
multivariate TSC datasets with less than 50\% computational cost compared with
recent baseline methods, opening the path towards more accurate resource-aware
methods for time series analyses. Our code is publicly available at:
https://github.com/QiaoXiao7282/DSN.",https://github.com/QiaoXiao7282/DSN,-1
a041a771-e6cb-4e41-a633-84910a8eb5e3,RSCFed: Random Sampling Consensus Federated Semi-supervised Learning,0.835094,"Federated semi-supervised learning (FSSL) aims to derive a global model by
training fully-labeled and fully-unlabeled clients or training partially
labeled clients. The existing approaches work well when local clients have
independent and identically distributed (IID) data but fail to generalize to a
more practical FSSL setting, i.e., Non-IID setting. In this paper, we present a
Random Sampling Consensus Federated learning, namely RSCFed, by considering the
uneven reliability among models from fully-labeled clients, fully-unlabeled
clients or partially labeled clients. Our key motivation is that given models
with large deviations from either labeled clients or unlabeled clients, the
consensus could be reached by performing random sub-sampling over clients. To
achieve it, instead of directly aggregating local models, we first distill
several sub-consensus models by random sub-sampling over clients and then
aggregating the sub-consensus models to the global model. To enhance the
robustness of sub-consensus models, we also develop a novel distance-reweighted
model aggregation method. Experimental results show that our method outperforms
state-of-the-art methods on three benchmarked datasets, including both natural
and medical images. The code is available at
https://github.com/XMed-Lab/RSCFed.",https://github.com/XMed-Lab/RSCFed,-1
3beffa19-61c5-4e35-ba68-8cb97026a57b,Continual Sequence Generation with Adaptive Compositional Modules,0.704817,"Continual learning is essential for real-world deployment when there is a
need to quickly adapt the model to new tasks without forgetting knowledge of
old tasks. Existing work on continual sequence generation either always reuses
existing parameters to learn new tasks, which is vulnerable to catastrophic
forgetting on dissimilar tasks, or blindly adds new parameters for every new
task, which could prevent knowledge sharing between similar tasks. To get the
best of both worlds, in this work, we propose continual sequence generation
with adaptive compositional modules to adaptively add modules in transformer
architectures and compose both old and new modules for new tasks. We also
incorporate pseudo experience replay to facilitate knowledge transfer in those
shared modules. Experiment results on various sequences of generation tasks
show that our framework can adaptively add modules or reuse modules based on
task similarity, outperforming state-of-the-art baselines in terms of both
performance and parameter efficiency. We make our code public at
https://github.com/GT-SALT/Adaptive-Compositional-Modules.",https://github.com/GT-SALT/Adaptive-Compositional-Modules,-1
4047caa7-6e8f-413d-b236-64e9d1a45301,Unsupervised Dense Nuclei Detection and Segmentation with Prior Self-activation Map For Histology Images,0.108277,"The success of supervised deep learning models in medical image segmentation
relies on detailed annotations. However, labor-intensive manual labeling is
costly and inefficient, especially in dense object segmentation. To this end,
we propose a self-supervised learning based approach with a Prior
Self-activation Module (PSM) that generates self-activation maps from the input
images to avoid labeling costs and further produce pseudo masks for the
downstream task. To be specific, we firstly train a neural network using
self-supervised learning and utilize the gradient information in the shallow
layers of the network to generate self-activation maps. Afterwards, a
semantic-guided generator is then introduced as a pipeline to transform visual
representations from PSM to pixel-level semantic pseudo masks for downstream
tasks. Furthermore, a two-stage training module, consisting of a nuclei
detection network and a nuclei segmentation network, is adopted to achieve the
final segmentation. Experimental results show the effectiveness on two public
pathological datasets. Compared with other fully-supervised and
weakly-supervised methods, our method can achieve competitive performance
without any manual annotations.",https://github.com/cpystan/Prior-Self-activation-Map,-1
8804dc3a-163a-439c-a574-dc671cb883c4,Diverse Plausible 360-Degree Image Outpainting for Efficient 3DCG Background Creation,0.76693,"We address the problem of generating a 360-degree image from a single image
with a narrow field of view by estimating its surroundings. Previous methods
suffered from overfitting to the training resolution and deterministic
generation. This paper proposes a completion method using a transformer for
scene modeling and novel methods to improve the properties of a 360-degree
image on the output image. Specifically, we use CompletionNets with a
transformer to perform diverse completions and AdjustmentNet to match color,
stitching, and resolution with an input image, enabling inference at any
resolution. To improve the properties of a 360-degree image on an output image,
we also propose WS-perceptual loss and circular inference. Thorough experiments
show that our method outperforms state-of-the-art (SOTA) methods both
qualitatively and quantitatively. For example, compared to SOTA methods, our
method completes images 16 times larger in resolution and achieves 1.7 times
lower Frechet inception distance (FID). Furthermore, we propose a pipeline that
uses the completion results for lighting and background of 3DCG scenes. Our
plausible background completion enables perceptually natural results in the
application of inserting virtual objects with specular surfaces.",None,-1
48f8afff-2ee2-4044-a3a0-88d9bb41ca71,CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer Learning,0.92756,"Generating artistic portraits is a challenging problem in computer vision.
Existing portrait stylization models that generate good quality results are
based on Image-to-Image Translation and require abundant data from both source
and target domains. However, without enough data, these methods would result in
overfitting. In this work, we propose CtlGAN, a new few-shot artistic portraits
generation model with a novel contrastive transfer learning strategy. We adapt
a pretrained StyleGAN in the source domain to a target artistic domain with no
more than 10 artistic faces. To reduce overfitting to the few training
examples, we introduce a novel Cross-Domain Triplet loss which explicitly
encourages the target instances generated from different latent codes to be
distinguishable. We propose a new encoder which embeds real faces into Z+ space
and proposes a dual-path training strategy to better cope with the adapted
decoder and eliminate the artifacts. Extensive qualitative, quantitative
comparisons and a user study show our method significantly outperforms
state-of-the-arts under 10-shot and 1-shot settings and generates high quality
artistic portraits. The code will be made publicly available.",None,-1
458d909b-e591-4dbd-88ff-ea867c120495,Transformer-based SAR Image Despeckling,0.922288,"Synthetic Aperture Radar (SAR) images are usually degraded by a
multiplicative noise known as speckle which makes processing and interpretation
of SAR images difficult. In this paper, we introduce a transformer-based
network for SAR image despeckling. The proposed despeckling network comprises
of a transformer-based encoder which allows the network to learn global
dependencies between different image regions - aiding in better despeckling.
The network is trained end-to-end with synthetically generated speckled images
using a composite loss function. Experiments show that the proposed method
achieves significant improvements over traditional and convolutional neural
network-based despeckling methods on both synthetic and real SAR images.",https://github.com/malshaV/sar_transformer,-1
4d4624d8-ea6e-443d-afa2-9daf45458bb1,DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition,0.827567,"The task of joint dialog sentiment classification (DSC) and act recognition
(DAR) aims to simultaneously predict the sentiment label and act label for each
utterance in a dialog. In this paper, we put forward a new framework which
models the explicit dependencies via integrating \textit{prediction-level
interactions} other than semantics-level interactions, more consistent with
human intuition. Besides, we propose a speaker-aware temporal graph (SATG) and
a dual-task relational temporal graph (DRTG) to introduce \textit{temporal
relations} into dialog understanding and dual-task reasoning. To implement our
framework, we propose a novel model dubbed DARER, which first generates the
context-, speaker- and temporal-sensitive utterance representations via
modeling SATG, then conducts recurrent dual-task relational reasoning on DRTG,
in which process the estimated label distributions act as key clues in
prediction-level interactions. Experiment results show that DARER outperforms
existing models by large margins while requiring much less computation resource
and costing less training time. Remarkably, on DSC task in Mastodon, DARER
gains a relative improvement of about 25% over previous best model in terms of
F1, with less than 50% parameters and about only 60% required GPU memory.",https://github.com/XingBowen714/DARER,-1
e594feb9-a2b3-4cc4-aee4-4d57a2de1703,Online Decision Transformer,0.999009,"Recent work has shown that offline reinforcement learning (RL) can be
formulated as a sequence modeling problem (Chen et al., 2021; Janner et al.,
2021) and solved via approaches similar to large-scale language modeling.
However, any practical instantiation of RL also involves an online component,
where policies pretrained on passive offline datasets are finetuned via
taskspecific interactions with the environment. We propose Online Decision
Transformers (ODT), an RL algorithm based on sequence modeling that blends
offline pretraining with online finetuning in a unified framework. Our
framework uses sequence-level entropy regularizers in conjunction with
autoregressive modeling objectives for sample-efficient exploration and
finetuning. Empirically, we show that ODT is competitive with the
state-of-the-art in absolute performance on the D4RL benchmark but shows much
more significant gains during the finetuning procedure.",https://github.com/kzl/decision-transformer,-1
bfdd5493-0076-4bb6-a787-e4f081ba6a39,Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models,0.35213,"With many real-world applications of Natural Language Processing (NLP)
comprising of long texts, there has been a rise in NLP benchmarks that measure
the accuracy of models that can handle longer input sequences. However, these
benchmarks do not consider the trade-offs between accuracy, speed, and power
consumption as input sizes or model sizes are varied. In this work, we perform
a systematic study of this accuracy vs. efficiency trade-off on two widely used
long-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during
fine-tuning and inference on four datasets from the SCROLLS benchmark. To study
how this trade-off differs across hyperparameter settings, we compare the
models across four sequence lengths (1024, 2048, 3072, 4096) and two model
sizes (base and large) under a fixed resource budget. We find that LED
consistently achieves better accuracy at lower energy costs than Big Bird. For
summarization, we find that increasing model size is more energy efficient than
increasing sequence length for higher accuracy. However, this comes at the cost
of a large drop in inference speed. For question answering, we find that
smaller models are both more efficient and more accurate due to the larger
training batch sizes possible under a fixed resource budget.",https://github.com/phyllisayk/nlp-efficiency-tradeoff,-1
45c2e06f-4937-4013-9020-f68f34839539,Resolving Semantic Confusions for Improved Zero-Shot Detection,0.0933065,"Zero-shot detection (ZSD) is a challenging task where we aim to recognize and
localize objects simultaneously, even when our model has not been trained with
visual samples of a few target (""unseen"") classes. Recently, methods employing
generative models like GANs have shown some of the best results, where
unseen-class samples are generated based on their semantics by a GAN trained on
seen-class data, enabling vanilla object detectors to recognize unseen objects.
However, the problem of semantic confusion still remains, where the model is
sometimes unable to distinguish between semantically-similar classes. In this
work, we propose to train a generative model incorporating a triplet loss that
acknowledges the degree of dissimilarity between classes and reflects them in
the generated samples. Moreover, a cyclic-consistency loss is also enforced to
ensure that generated visual samples of a class highly correspond to their own
semantics. Extensive experiments on two benchmark ZSD datasets - MSCOCO and
PASCAL-VOC - demonstrate significant gains over the current ZSD methods,
reducing semantic confusion and improving detection for the unseen classes.",https://github.com/sandipan211/ZSD-SC-Resolver,-1
7a2a421b-9662-4a69-a808-c69f0f3c89d3,Vakyansh: ASR Toolkit for Low Resource Indic languages,0.30804,"We present Vakyansh, an end to end toolkit for Speech Recognition in Indic
languages. India is home to almost 121 languages and around 125 crore speakers.
Yet most of the languages are low resource in terms of data and pretrained
models. Through Vakyansh, we introduce automatic data pipelines for data
creation, model training, model evaluation and deployment. We create 14,000
hours of speech data in 23 Indic languages and train wav2vec 2.0 based
pretrained models. These pretrained models are then finetuned to create state
of the art speech recognition models for 18 Indic languages which are followed
by language models and punctuation restoration models. We open source all these
resources with a mission that this will inspire the speech community to develop
speech first applications using our ASR models in Indic languages.",https://github.com/Open-Speech-EkStep/vakyansh-models,-1
851c200c-f738-44e6-99c9-f68dbce71bc0,HyperShot: Few-Shot Learning by Kernel HyperNetworks,0.563876,"Few-shot models aim at making predictions using a minimal number of labeled
examples from a given task. The main challenge in this area is the one-shot
setting where only one element represents each class. We propose HyperShot -
the fusion of kernels and hypernetwork paradigm. Compared to reference
approaches that apply a gradient-based adjustment of the parameters, our model
aims to switch the classification module parameters depending on the task's
embedding. In practice, we utilize a hypernetwork, which takes the aggregated
information from support data and returns the classifier's parameters
handcrafted for the considered problem. Moreover, we introduce the kernel-based
representation of the support examples delivered to hypernetwork to create the
parameters of the classification module. Consequently, we rely on relations
between embeddings of the support examples instead of direct feature values
provided by the backbone models. Thanks to this approach, our model can adapt
to highly different tasks.",None,-1
2b2f979b-9ef7-4668-83e4-c18ef78ebbfb,Learning Sketches for Decomposing Planning Problems into Subproblems of Bounded Width: Extended Version,0.53093,"Recently, sketches have been introduced as a general language for
representing the subgoal structure of instances drawn from the same domain.
Sketches are collections of rules of the form C -> E over a given set of
features where C expresses Boolean conditions and E expresses qualitative
changes. Each sketch rule defines a subproblem: going from a state that
satisfies C to a state that achieves the change expressed by E or a goal state.
Sketches can encode simple goal serializations, general policies, or
decompositions of bounded width that can be solved greedily, in polynomial
time, by the SIW_R variant of the SIW algorithm. Previous work has shown the
computational value of sketches over benchmark domains that, while tractable,
are challenging for domain-independent planners. In this work, we address the
problem of learning sketches automatically given a planning domain, some
instances of the target class of problems, and the desired bound on the sketch
width. We present a logical formulation of the problem, an implementation using
the ASP solver Clingo, and experimental results. The sketch learner and the
SIW_R planner yield a domain-independent planner that learns and exploits
domain structure in a crisp and explicit form.",https://doi.org/10.5281/zenodo.6381592,12924
a5734241-c045-4bf6-8de7-ef4813bf5386,Prototype-Based Layered Federated Cross-Modal Hashing,0.26788,"Recently, deep cross-modal hashing has gained increasing attention. However,
in many practical cases, data are distributed and cannot be collected due to
privacy concerns, which greatly reduces the cross-modal hashing performance on
each client. And due to the problems of statistical heterogeneity, model
heterogeneity, and forcing each client to accept the same parameters, applying
federated learning to cross-modal hash learning becomes very tricky. In this
paper, we propose a novel method called prototype-based layered federated
cross-modal hashing. Specifically, the prototype is introduced to learn the
similarity between instances and classes on server, reducing the impact of
statistical heterogeneity (non-IID) on different clients. And we monitor the
distance between local and global prototypes to further improve the
performance. To realize personalized federated learning, a hypernetwork is
deployed on server to dynamically update different layers' weights of local
model. Experimental results on benchmark datasets show that our method
outperforms state-of-the-art methods.",None,-1
41e8ab30-59cf-4c7b-b2ca-35be90716639,Reliable Face Morphing Attack Detection in On-The-Fly Border Control Scenario with Variation in Image Resolution and Capture Distance,0.618116,"Face Recognition Systems (FRS) are vulnerable to various attacks performed
directly and indirectly. Among these attacks, face morphing attacks are highly
potential in deceiving automatic FRS and human observers and indicate a severe
security threat, especially in the border control scenario. This work presents
a face morphing attack detection, especially in the On-The-Fly (OTF) Automatic
Border Control (ABC) scenario. We present a novel Differential-MAD (D-MAD)
algorithm based on the spherical interpolation and hierarchical fusion of deep
features computed from six different pre-trained deep Convolutional Neural
Networks (CNNs). Extensive experiments are carried out on the newly generated
face morphing dataset (SCFace-Morph) based on the publicly available SCFace
dataset by considering the real-life scenario of Automatic Border Control (ABC)
gates. Experimental protocols are designed to benchmark the proposed and
state-of-the-art (SOTA) D-MAD techniques for different camera resolutions and
capture distances. Obtained results have indicated the superior performance of
the proposed D-MAD method compared to the existing methods.",None,-1
1d2779c1-6768-478e-8c0b-4b6d0448cc99,Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling,0.27894,"To capture the relationship between samples and labels, conditional
generative models often inherit spurious correlations from the training
dataset. This can result in label-conditional distributions that are imbalanced
with respect to another latent attribute. To mitigate this issue, which we call
spurious causality of conditional generation, we propose a general two-step
strategy. (a) Fairness Intervention (FI): emphasize the minority samples that
are hard to generate due to the spurious correlation in the training dataset.
(b) Corrective Sampling (CS): explicitly filter the generated samples and
ensure that they follow the desired latent attribute distribution. We have
designed the fairness intervention to work for various degrees of supervision
on the spurious attribute, including unsupervised, weakly-supervised, and
semi-supervised scenarios. Our experimental results demonstrate that FICS can
effectively resolve spurious causality of conditional generation across various
datasets.",https://github.com/POSTECH-CVLab/PyTorch-StudioGAN,11542
ab262d17-e9ef-4e5f-a544-46352d1bc170,MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph Completion,0.821856,"Multimodal knowledge graph completion (MKGC) aims to predict missing entities
in MKGs. Previous works usually share relation representation across
modalities. This results in mutual interference between modalities during
training, since for a pair of entities, the relation from one modality probably
contradicts that from another modality. Furthermore, making a unified
prediction based on the shared relation representation treats the input in
different modalities equally, while their importance to the MKGC task should be
different. In this paper, we propose MoSE, a Modality Split representation
learning and Ensemble inference framework for MKGC. Specifically, in the
training phase, we learn modality-split relation embeddings for each modality
instead of a single modality-shared one, which alleviates the modality
interference. Based on these embeddings, in the inference phase, we first make
modality-split predictions and then exploit various ensemble methods to combine
the predictions with different weights, which models the modality importance
dynamically. Experimental results on three KG datasets show that MoSE
outperforms state-of-the-art MKGC methods. Codes are available at
https://github.com/OreOZhao/MoSE4MKGC.",https://github.com/OreOZhao/MoSE4MKGC,-1
b5e4c6ba-4bbe-4b08-8079-451ca2efe3f1,Evidential Temporal-aware Graph-based Social Event Detection via Dempster-Shafer Theory,0.458979,"The rising popularity of online social network services has attracted lots of
research on mining social media data, especially on mining social events.
Social event detection, due to its wide applications, has now become a trivial
task. State-of-the-art approaches exploiting Graph Neural Networks (GNNs)
usually follow a two-step strategy: 1) constructing text graphs based on
various views (\textit{co-user}, \textit{co-entities} and
\textit{co-hashtags}); and 2) learning a unified text representation by a
specific GNN model. Generally, the results heavily rely on the quality of the
constructed graphs and the specific message passing scheme. However, existing
methods have deficiencies in both aspects: 1) They fail to recognize the noisy
information induced by unreliable views. 2) Temporal information which works as
a vital indicator of events is neglected in most works. To this end, we propose
ETGNN, a novel Evidential Temporal-aware Graph Neural Network. Specifically, we
construct view-specific graphs whose nodes are the texts and edges are
determined by several types of shared elements respectively. To incorporate
temporal information into the message passing scheme, we introduce a novel
temporal-aware aggregator which assigns weights to neighbours according to an
adaptive time exponential decay formula. Considering the view-specific
uncertainty, the representations of all views are converted into mass functions
through evidential deep learning (EDL) neural networks, and further combined
via Dempster-Shafer theory (DST) to make the final detection. Experimental
results on three real-world datasets demonstrate the effectiveness of ETGNN in
accuracy, reliability and robustness in social event detection.",None,-1
7d426427-2cb8-4534-b890-04ba8e7dcfe6,Dialect-robust Evaluation of Generated Text,0.353535,"Evaluation metrics that are not robust to dialect variation make it
impossible to tell how well systems perform for many groups of users, and can
even penalize systems for producing text in lower-resource dialects. However,
currently, there exists no way to quantify how metrics respond to change in the
dialect of a generated utterance. We thus formalize dialect robustness and
dialect awareness as goals for NLG evaluation metrics. We introduce a suite of
methods and corresponding statistical tests one can use to assess metrics in
light of the two goals. Applying the suite to current state-of-the-art metrics,
we demonstrate that they are not dialect-robust and that semantic perturbations
frequently lead to smaller decreases in a metric than the introduction of
dialect features. As a first step to overcome this limitation, we propose a
training schema, NANO, which introduces regional and language information to
the pretraining process of a metric. We demonstrate that NANO provides a
size-efficient way for models to improve the dialect robustness while
simultaneously improving their performance on the standard metric benchmark.",https://github.com/google-research/bleurt,-1
b36685f3-39ef-42d0-a099-a3bce7167123,Automated Isovist Computation for Minecraft,0.811124,"Procedural content generation for games is a growing trend in both research
and industry, even though there is no consensus of how good content looks, nor
how to automatically evaluate it. A number of metrics have been developed in
the past, usually focused on the artifact as a whole, and mostly lacking
grounding in human experience. In this study we develop a new set of automated
metrics, motivated by ideas from architecture, namely isovists and space
syntax, which have a track record of capturing human experience of space. These
metrics can be computed for a specific game state, from the player's
perspective, and take into account their embodiment in the game world. We show
how to apply those metrics to the 3d blockworld of Minecraft. We use a dataset
of generated settlements from the GDMC Settlement Generation Challenge in
Minecraft and establish several rank-based correlations between the isovist
properties and the rating human judges gave those settelements. We also produce
a range of heat maps that demonstrate the location based applicability of the
approach, which allows for development of those metrics as measures for a game
experience at a specific time and space.",None,-1
5e32f7a2-6e43-4de8-a266-adaa54b43f87,HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification,0.761262,"Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification due to its complex label hierarchy. Recently, the
pretrained language models (PLM)have been widely adopted in HTC through a
fine-tuning paradigm. However, in this paradigm, there exists a huge gap
between the classification tasks with sophisticated label hierarchy and the
masked language model (MLM) pretraining tasks of PLMs and thus the potentials
of PLMs can not be fully tapped. To bridge the gap, in this paper, we propose
HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label
MLM perspective. Specifically, we construct a dynamic virtual template and
label words that take the form of soft prompts to fuse the label hierarchy
knowledge and introduce a zero-bounded multi-label cross entropy loss to
harmonize the objectives of HTC and MLM. Extensive experiments show HPT
achieves state-of-the-art performances on 3 popular HTC datasets and is adept
at handling the imbalance and low resource situations. Our code is available at
https://github.com/wzh9969/HPT.",https://github.com/wzh9969/HPT,-1
746908ee-c33d-46fd-bb68-766e1b3ef0e9,Describing Differences between Text Distributions with Natural Language,0.801211,"How do two distributions of texts differ? Humans are slow at answering this,
since discovering patterns might require tediously reading through hundreds of
samples. We propose to automatically summarize the differences by ""learning a
natural language hypothesis"": given two distributions $D_{0}$ and $D_{1}$, we
search for a description that is more often true for $D_{1}$, e.g., ""is
military-related."" To tackle this problem, we fine-tune GPT-3 to propose
descriptions with the prompt: ""[samples of $D_{0}$] + [samples of $D_{1}$] +
the difference between them is_____."" We then re-rank the descriptions by
checking how often they hold on a larger set of samples with a learned
verifier. On a benchmark of 54 real-world binary classification tasks, while
GPT-3 Curie (13B) only generates a description similar to human annotation 7%
of the time, the performance reaches 61% with fine-tuning and re-ranking, and
our best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to
describe distribution shifts, debug dataset shortcuts, summarize unknown tasks,
and label text clusters, and present analyses based on automatically generated
descriptions.",None,-1
550b9abe-ea5d-41a2-9fd5-7c2c9ffeaca0,PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models,0.986813,"Generalizable 3D part segmentation is important but challenging in vision and
robotics. Training deep models via conventional supervised methods requires
large-scale 3D datasets with fine-grained part annotations, which are costly to
collect. This paper explores an alternative way for low-shot part segmentation
of 3D point clouds by leveraging a pretrained image-language model, GLIP, which
achieves superior performance on open-vocabulary 2D detection. We transfer the
rich knowledge from 2D to 3D through GLIP-based part detection on point cloud
rendering and a novel 2D-to-3D label lifting algorithm. We also utilize
multi-view 3D priors and few-shot prompt tuning to boost performance
significantly. Extensive evaluation on PartNet and PartNet-Mobility datasets
shows that our method enables excellent zero-shot 3D part segmentation. Our
few-shot version not only outperforms existing few-shot approaches by a large
margin but also achieves highly competitive results compared to the fully
supervised counterpart. Furthermore, we demonstrate that our method can be
directly applied to iPhone-scanned point clouds without significant domain
gaps.",None,-1
1ae53a9d-d6b9-4896-b5a5-c2c9c4658336,A case for using rotation invariant features in state of the art feature matchers,0.893859,"The aim of this paper is to demonstrate that a state of the art feature
matcher (LoFTR) can be made more robust to rotations by simply replacing the
backbone CNN with a steerable CNN which is equivariant to translations and
image rotations. It is experimentally shown that this boost is obtained without
reducing performance on ordinary illumination and viewpoint matching sequences.",https://github.com/georg-/se2-loftr,8663
51adfcd5-cd09-4cc6-bd29-79374aa7a529,Explaining Chest X-ray Pathologies in Natural Language,0.431648,"Most deep learning algorithms lack explanations for their predictions, which
limits their deployment in clinical practice. Approaches to improve
explainability, especially in medical imaging, have often been shown to convey
limited information, be overly reassuring, or lack robustness. In this work, we
introduce the task of generating natural language explanations (NLEs) to
justify predictions made on medical images. NLEs are human-friendly and
comprehensive, and enable the training of intrinsically explainable models. To
this goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging
dataset with NLEs. It contains over 38,000 NLEs, which explain the presence of
various thoracic pathologies and chest X-ray findings. We propose a general
approach to solve the task and evaluate several architectures on this dataset,
including via clinician assessment.",https://github.com/maximek3/MIMIC-NLE,-1
b3354071-36a2-49f8-aee5-24243d1886c7,Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification,0.612505,"Multi-grained features extracted from convolutional neural networks (CNNs)
have demonstrated their strong discrimination ability in supervised person
re-identification (Re-ID) tasks. Inspired by them, this work investigates the
way of extracting multi-grained features from a pure transformer network to
address the unsupervised Re-ID problem that is label-free but much more
challenging. To this end, we build a dual-branch network architecture based
upon a modified Vision Transformer (ViT). The local tokens output in each
branch are reshaped and then uniformly partitioned into multiple stripes to
generate part-level features, while the global tokens of two branches are
averaged to produce a global feature. Further, based upon offline-online
associated camera-aware proxies (O2CAP) that is a top-performing unsupervised
Re-ID method, we define offline and online contrastive learning losses with
respect to both global and part-level features to conduct unsupervised
learning. Extensive experiments on three person Re-ID datasets show that the
proposed method outperforms state-of-the-art unsupervised methods by a
considerable margin, greatly mitigating the gap to supervised counterparts.
Code will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.",https://github.com/RikoLi/WACV23-workshop-TMGF,-1
b2d28eb0-8834-45c3-992a-7d9e03a6c4fe,Evaluating and Inducing Personality in Pre-trained Language Models,0.826481,"Standardized and quantified evaluation of machine behaviors is a crux of
understanding LLMs. In this study, we draw inspiration from psychometric
studies by leveraging human personality theory as a tool for studying machine
behaviors. Originating as a philosophical quest for human behaviors, the study
of personality delves into how individuals differ in thinking, feeling, and
behaving. Toward building and understanding human-like social machines, we are
motivated to ask: Can we assess machine behaviors by leveraging human
psychometric tests in a principled and quantitative manner? If so, can we
induce a specific personality in LLMs? To answer these questions, we introduce
the Machine Personality Inventory (MPI) tool for studying machine behaviors;
MPI follows standardized personality tests, built upon the Big Five Personality
Factors (Big Five) theory and personality assessment inventories. By
systematically evaluating LLMs with MPI, we provide the first piece of evidence
demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise
a Personality Prompting (P^2) method to induce LLMs with specific personalities
in a controllable way, capable of producing diverse and verifiable behaviors.
We hope this work sheds light on future studies by adopting personality as the
essential indicator for various downstream tasks, and could further motivate
research into equally intriguing human-like machine behaviors.",None,-1
88a3eed5-2e74-4d46-8f53-f325eb36e1c9,DFGC 2022: The Second DeepFake Game Competition,0.121784,"This paper presents the summary report on our DFGC 2022 competition. The
DeepFake is rapidly evolving, and realistic face-swaps are becoming more
deceptive and difficult to detect. On the contrary, methods for detecting
DeepFakes are also improving. There is a two-party game between DeepFake
creators and defenders. This competition provides a common platform for
benchmarking the game between the current state-of-the-arts in DeepFake
creation and detection methods. The main research question to be answered by
this competition is the current state of the two adversaries when competed with
each other. This is the second edition after the last year's DFGC 2021, with a
new, more diverse video dataset, a more realistic game setting, and more
reasonable evaluation metrics. With this competition, we aim to stimulate
research ideas for building better defenses against the DeepFake threats. We
also release our DFGC 2022 dataset contributed by both our participants and
ourselves to enrich the DeepFake data resources for the research community
(https://github.com/NiCE-X/DFGC-2022).",None,-1
0d15c148-6afc-407c-ac86-564eed8693e9,Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models,0.667477,"This paper presents exploratory work on whether and to what extent biases
against queer and trans people are encoded in large language models (LLMs) such
as BERT. We also propose a method for reducing these biases in downstream
tasks: finetuning the models on data written by and/or about queer people. To
measure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,
modeled after other bias-detection benchmarks but addressing homophobic and
transphobic biases. We found that BERT shows significant homophobic bias, but
this bias can be mostly mitigated by finetuning BERT on a natural language
corpus written by members of the LGBTQ+ community.",https://github.com/katyfelkner/,1371
557e7161-205a-45cd-b3dc-b423c7323e36,Image Segmentation-based Unsupervised Multiple Objects Discovery,0.543427,"Unsupervised object discovery aims to localize objects in images, while
removing the dependence on annotations required by most deep learning-based
methods. To address this problem, we propose a fully unsupervised, bottom-up
approach, for multiple objects discovery. The proposed approach is a two-stage
framework. First, instances of object parts are segmented by using the
intra-image similarity between self-supervised local features. The second step
merges and filters the object parts to form complete object instances. The
latter is performed by two CNN models that capture semantic information on
objects from the entire dataset. We demonstrate that the pseudo-labels
generated by our method provide a better precision-recall trade-off than
existing single and multiple objects discovery methods. In particular, we
provide state-of-the-art results for both unsupervised class-agnostic object
detection and unsupervised image segmentation.",None,-1
7328f234-d6b5-47f4-8ae0-1f7245310fa1,MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition,0.780982,"Unlike the conventional Knowledge Distillation (KD), Self-KD allows a network
to learn knowledge from itself without any guidance from extra networks. This
paper proposes to perform Self-KD from image Mixture (MixSKD), which integrates
these two techniques into a unified framework. MixSKD mutually distills feature
maps and probability distributions between the random pair of original images
and their mixup images in a meaningful way. Therefore, it guides the network to
learn cross-image knowledge by modelling supervisory signals from mixup images.
Moreover, we construct a self-teacher network by aggregating multi-stage
feature maps for providing soft labels to supervise the backbone classifier,
further improving the efficacy of self-boosting. Experiments on image
classification and transfer learning to object detection and semantic
segmentation demonstrate that MixSKD outperforms other state-of-the-art Self-KD
and data augmentation methods. The code is available at
https://github.com/winycg/Self-KD-Lib.",https://github.com/winycg/Self-KD-Lib,-1
73f8e7c0-26b4-4781-8978-37edf879b690,X-PuDu at SemEval-2022 Task 6: Multilingual Learning for English and Arabic Sarcasm Detection,0.444137,"Detecting sarcasm and verbal irony from people's subjective statements is
crucial to understanding their intended meanings and real sentiments and
positions in social scenarios. This paper describes the X-PuDu system that
participated in SemEval-2022 Task 6, iSarcasmEval - Intended Sarcasm Detection
in English and Arabic, which aims at detecting intended sarcasm in various
settings of natural language understanding. Our solution finetunes pre-trained
language models, such as ERNIE-M and DeBERTa, under the multilingual settings
to recognize the irony from Arabic and English texts. Our system ranked second
out of 43, and ninth out of 32 in Task A: one-sentence detection in English and
Arabic; fifth out of 22 in Task B: binary multi-label classification in
English; first out of 16, and fifth out of 13 in Task C: sentence-pair
detection in English and Arabic.",None,-1
767767f9-f7d0-4fed-82fb-35f1a4810eca,Deanthropomorphising NLP: Can a Language Model Be Conscious?,0.85037,"This work is intended as a voice in the discussion over previous claims that
a pretrained large language model (LLM) based on the Transformer model
architecture can be sentient. Such claims have been made concerning the LaMDA
model and also concerning the current wave of LLM-powered chatbots, such as
ChatGPT. This claim, if confirmed, would have serious ramifications in the
Natural Language Processing (NLP) community due to wide-spread use of similar
models. However, here we take the position that such a large language model
cannot be sentient, or conscious, and that LaMDA in particular exhibits no
advances over other similar models that would qualify it. We justify this by
analysing the Transformer architecture through Integrated Information Theory of
consciousness. We see the claims of sentience as part of a wider tendency to
use anthropomorphic language in NLP reporting. Regardless of the veracity of
the claims, we consider this an opportune moment to take stock of progress in
language modelling and consider the ethical implications of the task. In order
to make this work helpful for readers outside the NLP community, we also
present the necessary background in language modelling.",None,-1
6b577ddf-048c-47e6-970d-edc291ea4956,Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach for Rotating Machinery exploiting Augmented Synthetic Data,0.557199,"Artificial Intelligence (AI) is one of the approaches that has been proposed
to analyze the collected data (e.g., vibration signals) providing a diagnosis
of the asset's operating condition. It is known that models trained with
labeled data (supervised) achieve excellent results, but two main problems make
their application in production processes difficult: (i) impossibility or long
time to obtain a sample of all operational conditions (since faults seldom
happen) and (ii) high cost of experts to label all acquired data. Another
limitating factor for the applicability of AI approaches in this context is the
lack of interpretability of the models (black-boxes), which reduces the
confidence of the diagnosis and trust/adoption from users. To overcome these
problems, a new generic and interpretable approach for classifying faults in
rotating machinery based on transfer learning from augmented synthetic data to
real rotating machinery is here proposed, namelly FaultD-XAI (Fault Diagnosis
using eXplainable AI). To provide scalability using transfer learning,
synthetic vibration signals are created mimicking the characteristic behavior
of failures in operation. The application of Gradient-weighted Class Activation
Mapping (Grad-CAM) with 1D Convolutional Neural Network (1D CNN) allows the
interpretation of results, supporting the user in decision making and
increasing diagnostic confidence. The proposed approach not only obtained
promising diagnostic performance, but was also able to learn characteristics
used by experts to identify conditions in a source domain and apply them in
another target domain. The experimental results suggest a promising approach on
exploiting transfer learning, synthetic data and explainable artificial
intelligence for fault diagnosis. Lastly, to guarantee reproducibility and
foster research in the field, the developed dataset is made publicly available.",None,-1
4906adc1-bd14-407a-8fa1-29b728f28cc4,Learning a Grammar Inducer from Massive Uncurated Instructional Videos,0.375781,"Video-aided grammar induction aims to leverage video information for finding
more accurate syntactic grammars for accompanying text. While previous work
focuses on building systems for inducing grammars on text that are well-aligned
with video content, we investigate the scenario, in which text and video are
only in loose correspondence. Such data can be found in abundance online, and
the weak correspondence is similar to the indeterminacy problem studied in
language acquisition. Furthermore, we build a new model that can better learn
video-span correlation without manually designed features adopted by previous
work. Experiments show that our model trained only on large-scale YouTube data
with no text-video alignment reports strong and robust performances across
three unseen datasets, despite domain shift and noisy label issues. Furthermore
our model yields higher F1 scores than the previous state-of-the-art systems
trained on in-domain data.",https://github.com/linjieli222/HERO_Video_Feature_Extractor,-1
a5bdd23a-53d8-44c1-a8f6-3301ef0576e0,4D Unsupervised Object Discovery,0.665042,"Object discovery is a core task in computer vision. While fast progresses
have been made in supervised object detection, its unsupervised counterpart
remains largely unexplored. With the growth of data volume, the expensive cost
of annotations is the major limitation hindering further study. Therefore,
discovering objects without annotations has great significance. However, this
task seems impractical on still-image or point cloud alone due to the lack of
discriminative information. Previous studies underlook the crucial temporal
information and constraints naturally behind multi-modal inputs. In this paper,
we propose 4D unsupervised object discovery, jointly discovering objects from
4D data -- 3D point clouds and 2D RGB images with temporal information. We
present the first practical approach for this task by proposing a ClusterNet on
3D point clouds, which is jointly iteratively optimized with a 2D localization
network. Extensive experiments on the large-scale Waymo Open Dataset suggest
that the localization network and ClusterNet achieve competitive performance on
both class-agnostic 2D object detection and 3D instance segmentation, bridging
the gap between unsupervised methods and full supervised ones. Codes and models
will be made available at https://github.com/Robertwyq/LSMOL.",https://github.com/Robertwyq/LSMOL,-1
e8c60f41-892e-4fe9-bba2-d966750e01cb,Feature Engineering and Classification Models for Partial Discharge in Power Transformers,0.230516,"To ensure reliability, power transformers are monitored for partial discharge
(PD) events, which are symptoms of transformer failure. Since failures can have
catastrophic cascading consequences, it is critical to preempt them as early as
possible. Our goal is to classify PDs as corona, floating, particle, or void,
to gain an understanding of the failure location. Using phase resolved PD
signal data, we create a small set of features, which can be used to classify
PDs with high accuracy. This set of features consists of the total magnitude,
the maximum magnitude, and the length of the longest empty band. These features
represent the entire signal and not just a single phase, so the feature set has
a fixed size and is easily comprehensible. With both Random Forest and SVM
classification methods, we attain a 99% classification accuracy, which is
significantly higher than classification using phase based feature sets such as
phase magnitude. Furthermore, we develop a stacking ensemble to combine several
classification models, resulting in a superior model that outperforms existing
methods in both accuracy and variance.",None,-1
7041b816-5d2e-43fe-973c-6568bbd23abc,OPERA:Operation-Pivoted Discrete Reasoning over Text,0.201351,"Machine reading comprehension (MRC) that requires discrete reasoning
involving symbolic operations, e.g., addition, sorting, and counting, is a
challenging task. According to this nature, semantic parsing-based methods
predict interpretable but complex logical forms. However, logical form
generation is nontrivial and even a little perturbation in a logical form will
lead to wrong answers. To alleviate this issue, multi-predictor -based methods
are proposed to directly predict different types of answers and achieve
improvements. However, they ignore the utilization of symbolic operations and
encounter a lack of reasoning ability and interpretability. To inherit the
advantages of these two types of methods, we propose OPERA, an
operation-pivoted discrete reasoning framework, where lightweight symbolic
operations (compared with logical forms) as neural modules are utilized to
facilitate the reasoning ability and interpretability. Specifically, operations
are first selected and then softly executed to simulate the answer reasoning
procedure. Extensive experiments on both DROP and RACENum datasets show the
reasoning ability of OPERA. Moreover, further analysis verifies its
interpretability.",https://github.com/JD-AI-Research-NLP/OPERA,-1
4d9b346a-8ed5-45e0-90ae-4abd05efb5a9,Parallel Augmentation and Dual Enhancement for Occluded Person Re-identification,0.329595,"Occluded person re-identification (Re-ID), the task of searching for the same
person's images in occluded environments, has attracted lots of attention in
the past decades. Recent approaches concentrate on improving performance on
occluded data by data/feature augmentation or using extra models to predict
occlusions. However, they ignore the imbalance problem in this task and can not
fully utilize the information from the training data. To alleviate these two
issues, we propose a simple yet effective method with Parallel Augmentation and
Dual Enhancement (PADE), which is robust on both occluded and non-occluded data
and does not require any auxiliary clues. First, we design a parallel
augmentation mechanism (PAM) to generate more suitable occluded data to
mitigate the negative effects of unbalanced data. Second, we propose the global
and local dual enhancement strategy (DES) to promote the context information
and details. Experimental results on three widely used occluded datasets and
two non-occluded datasets validate the effectiveness of our method. The code is
available at
https://github.com/littleprince1121/PADE_Parallel_Augmentation_and_Dual_Enhancement_for_Occluded_Person_ReID",None,-1
ef12cb19-3f48-4679-bbe5-acd55e04f6a3,What Do Compressed Multilingual Machine Translation Models Forget?,0.735116,"Recently, very large pre-trained models achieve state-of-the-art results in
various natural language processing (NLP) tasks, but their size makes it more
challenging to apply them in resource-constrained environments. Compression
techniques allow to drastically reduce the size of the models and therefore
their inference time with negligible impact on top-tier metrics. However, the
general performance averaged across multiple tasks and/or languages may hide a
drastic performance drop on under-represented features, which could result in
the amplification of biases encoded by the models. In this work, we assess the
impact of compression methods on Multilingual Neural Machine Translation models
(MNMT) for various language groups, gender, and semantic biases by extensive
analysis of compressed models on different machine translation benchmarks, i.e.
FLORES-101, MT-Gender, and DiBiMT. We show that the performance of
under-represented languages drops significantly, while the average BLEU metric
only slightly decreases. Interestingly, the removal of noisy memorization with
compression leads to a significant improvement for some medium-resource
languages. Finally, we demonstrate that compression amplifies intrinsic gender
and semantic biases, even in high-resource languages. Code:
https://github.com/alirezamshi/bias-compressedMT",https://github.com/alirezamshi/bias-compressedMT,-1
198e2ce5-4741-4574-9cfc-f86884396dde,JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset,0.568936,"JamPatoisNLI provides the first dataset for natural language inference in a
creole language, Jamaican Patois. Many of the most-spoken low-resource
languages are creoles. These languages commonly have a lexicon derived from a
major world language and a distinctive grammar reflecting the languages of the
original speakers and the process of language birth by creolization. This gives
them a distinctive place in exploring the effectiveness of transfer from large
monolingual or multilingual pretrained models. While our work, along with
previous work, shows that transfer from these models to low-resource languages
that are unrelated to languages in their training set is not very effective, we
would expect stronger results from transfer to creoles. Indeed, our experiments
show considerably better results from few-shot learning of JamPatoisNLI than
for such unrelated languages, and help us begin to understand how the unique
relationship between creoles and their high-resource base languages affect
cross-lingual transfer. JamPatoisNLI, which consists of naturally-occurring
premises and expert-written hypotheses, is a step towards steering research
into a traditionally underserved language and a useful benchmark for
understanding cross-lingual NLP.",None,-1
680eb8d5-27e7-4acc-802c-cf2347e81173,Meta Spatio-Temporal Debiasing for Video Scene Graph Generation,0.818597,"Video scene graph generation (VidSGG) aims to parse the video content into
scene graphs, which involves modeling the spatio-temporal contextual
information in the video. However, due to the long-tailed training data in
datasets, the generalization performance of existing VidSGG models can be
affected by the spatio-temporal conditional bias problem. In this work, from
the perspective of meta-learning, we propose a novel Meta Video Scene Graph
Generation (MVSGG) framework to address such a bias problem. Specifically, to
handle various types of spatio-temporal conditional biases, our framework first
constructs a support set and a group of query sets from the training data,
where the data distribution of each query set is different from that of the
support set w.r.t. a type of conditional bias. Then, by performing a novel meta
training and testing process to optimize the model to obtain good testing
performance on these query sets after training on the support set, our
framework can effectively guide the model to learn to well generalize against
biases. Extensive experiments demonstrate the efficacy of our proposed
framework.",None,-1
6851a42f-93e1-4717-93f5-e7c524ce0e09,PanoFormer: Panorama Transformer for Indoor 360 Depth Estimation,0.943347,"Existing panoramic depth estimation methods based on convolutional neural
networks (CNNs) focus on removing panoramic distortions, failing to perceive
panoramic structures efficiently due to the fixed receptive field in CNNs. This
paper proposes the panorama transformer (named PanoFormer) to estimate the
depth in panorama images, with tangent patches from spherical domain, learnable
token flows, and panorama specific metrics. In particular, we divide patches on
the spherical tangent domain into tokens to reduce the negative effect of
panoramic distortions. Since the geometric structures are essential for depth
estimation, a self-attention module is redesigned with an additional learnable
token flow. In addition, considering the characteristic of the spherical
domain, we present two panorama-specific metrics to comprehensively evaluate
the panoramic depth estimation models' performance. Extensive experiments
demonstrate that our approach significantly outperforms the state-of-the-art
(SOTA) methods. Furthermore, the proposed method can be effectively extended to
solve semantic panorama segmentation, a similar pixel2pixel task. Code will be
available.",https://github.com/zhijieshen-bjtu/PanoFormer,-1
918beab3-c176-4332-b44e-c144f6b54f53,Changing the Representation: Examining Language Representation for Neural Sign Language Production,0.636322,"Neural Sign Language Production (SLP) aims to automatically translate from
spoken language sentences to sign language videos. Historically the SLP task
has been broken into two steps; Firstly, translating from a spoken language
sentence to a gloss sequence and secondly, producing a sign language video
given a sequence of glosses. In this paper we apply Natural Language Processing
techniques to the first step of the SLP pipeline. We use language models such
as BERT and Word2Vec to create better sentence level embeddings, and apply
several tokenization techniques, demonstrating how these improve performance on
the low resource translation task of Text to Gloss. We introduce Text to
HamNoSys (T2H) translation, and show the advantages of using a phonetic
representation for sign language translation rather than a sign level gloss
representation. Furthermore, we use HamNoSys to extract the hand shape of a
sign and use this as additional supervision during training, further increasing
the performance on T2H. Assembling best practise, we achieve a BLEU-4 score of
26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art
baselines.",None,-1
5bc4c3d1-8561-4ff3-b20d-2fcbadcde317,Learning crop type mapping from regional label proportions in large-scale SAR and optical imagery,0.223015,"The application of deep learning algorithms to Earth observation (EO) in
recent years has enabled substantial progress in fields that rely on remotely
sensed data. However, given the data scale in EO, creating large datasets with
pixel-level annotations by experts is expensive and highly time-consuming. In
this context, priors are seen as an attractive way to alleviate the burden of
manual labeling when training deep learning methods for EO. For some
applications, those priors are readily available. Motivated by the great
success of contrastive-learning methods for self-supervised feature
representation learning in many computer-vision tasks, this study proposes an
online deep clustering method using crop label proportions as priors to learn a
sample-level classifier based on government crop-proportion data for a whole
agricultural region. We evaluate the method using two large datasets from two
different agricultural regions in Brazil. Extensive experiments demonstrate
that the method is robust to different data types (synthetic-aperture radar and
optical images), reporting higher accuracy values considering the major crop
types in the target regions. Thus, it can alleviate the burden of large-scale
image annotation in EO applications.",None,-1
c204860d-870d-4c05-8834-68a4dfc96ba6,Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances,0.676772,"With the advent of Deep Learning (DL), Super-Resolution (SR) has also become
a thriving research area. However, despite promising results, the field still
faces challenges that require further research e.g., allowing flexible
upsampling, more effective loss functions, and better evaluation metrics. We
review the domain of SR in light of recent advances, and examine
state-of-the-art models such as diffusion (DDPM) and transformer-based SR
models. We present a critical discussion on contemporary strategies used in SR,
and identify promising yet unexplored research directions. We complement
previous surveys by incorporating the latest developments in the field such as
uncertainty-driven losses, wavelet networks, neural architecture search, novel
normalization methods, and the latests evaluation techniques. We also include
several visualizations for the models and methods throughout each chapter in
order to facilitate a global understanding of the trends in the field. This
review is ultimately aimed at helping researchers to push the boundaries of DL
applied to SR.",None,-1
40842168-cba8-4e27-984d-2c26b861722c,"Towards Automated Document Revision: Grammatical Error Correction, Fluency Edits, and Beyond",0.884681,"Natural language processing technology has rapidly improved automated
grammatical error correction tasks, and the community begins to explore
document-level revision as one of the next challenges. To go beyond
sentence-level automated grammatical error correction to NLP-based
document-level revision assistant, there are two major obstacles: (1) there are
few public corpora with document-level revisions being annotated by
professional editors, and (2) it is not feasible to elicit all possible
references and evaluate the quality of revision with such references because
there are infinite possibilities of revision. This paper tackles these
challenges. First, we introduce a new document-revision corpus, TETRA, where
professional editors revised academic papers sampled from the ACL anthology
which contain few trivial grammatical errors that enable us to focus more on
document- and paragraph-level edits such as coherence and consistency. Second,
we explore reference-less and interpretable methods for meta-evaluation that
can detect quality improvements by document revision. We show the uniqueness of
TETRA compared with existing document revision corpora and demonstrate that a
fine-tuned pre-trained language model can discriminate the quality of documents
after revision even when the difference is subtle. This promising result will
encourage the community to further explore automated document revision models
and metrics in future.",https://github.com/chemicaltree/tetra,-1
d3c55138-6b34-4017-a5c1-2bd7a0f61050,Towards Accurate Open-Set Recognition via Background-Class Regularization,0.450777,"In open-set recognition (OSR), classifiers should be able to reject
unknown-class samples while maintaining high closed-set classification
accuracy. To effectively solve the OSR problem, previous studies attempted to
limit latent feature space and reject data located outside the limited space
via offline analyses, e.g., distance-based feature analyses, or complicated
network architectures. To conduct OSR via a simple inference process (without
offline analyses) in standard classifier architectures, we use distance-based
classifiers instead of conventional Softmax classifiers. Afterwards, we design
a background-class regularization strategy, which uses background-class data as
surrogates of unknown-class ones during training phase. Specifically, we
formulate a novel regularization loss suitable for distance-based classifiers,
which reserves sufficiently large class-wise latent feature spaces for known
classes and forces background-class samples to be located far away from the
limited spaces. Through our extensive experiments, we show that the proposed
method provides robust OSR results, while maintaining high closed-set
classification accuracy.",https://github.com/Anjin-Liu/Openset_Learning_AOSR,-1
6643eaa7-e186-4d49-b71f-add0109eb229,MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid,0.77728,"Multi-modal entity alignment (MMEA) aims to discover identical entities
across different knowledge graphs (KGs) whose entities are associated with
relevant images. However, current MMEA algorithms rely on KG-level modality
fusion strategies for multi-modal entity representation, which ignores the
variations of modality preferences of different entities, thus compromising
robustness against noise in modalities such as blurry images and relations.
This paper introduces MEAformer, a multi-modal entity alignment transformer
approach for meta modality hybrid, which dynamically predicts the mutual
correlation coefficients among modalities for more fine-grained entity-level
modality fusion and alignment. Experimental results demonstrate that our model
not only achieves SOTA performance in multiple training scenarios, including
supervised, unsupervised, iterative, and low-resource settings, but also has a
limited number of parameters, efficient runtime, and interpretability. Our code
is available at https://github.com/zjukg/MEAformer.",https://github.com/zjukg/MEAformer,-1
222c72e6-0c12-40ce-9c9f-98ae23c52586,A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification,0.306573,"Many recent deep learning-based solutions have widely adopted the
attention-based mechanism in various tasks of the NLP discipline. However, the
inherent characteristics of deep learning models and the flexibility of the
attention mechanism increase the models' complexity, thus leading to challenges
in model explainability. In this paper, to address this challenge, we propose a
novel practical framework by utilizing a two-tier attention architecture to
decouple the complexity of explanation and the decision-making process. We
apply it in the context of a news article classification task. The experiments
on two large-scaled news corpora demonstrate that the proposed model can
achieve competitive performance with many state-of-the-art alternatives and
illustrate its appropriateness from an explainability perspective.",https://github.com/Ruixinhua/BATM,-1
4d710973-4a39-475a-b894-bdfaf4b5f81d,Towards Grand Unification of Object Tracking,0.996465,"We present a unified method, termed Unicorn, that can simultaneously solve
four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the
same model parameters. Due to the fragmented definitions of the object tracking
problem itself, most existing trackers are developed to address a single or
part of tasks and overspecialize on the characteristics of specific tasks. By
contrast, Unicorn provides a unified solution, adopting the same input,
backbone, embedding, and head across all tracking tasks. For the first time, we
accomplish the great unification of the tracking network architecture and
learning paradigm. Unicorn performs on-par or better than its task-specific
counterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,
BDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will
serve as a solid step towards the general vision model. Code is available at
https://github.com/MasterBin-IIAU/Unicorn.",https://github.com/MasterBin-IIAU/Unicorn,-1
7e7f6ca0-590a-49a7-89b0-4ac0f85799b1,"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish",0.328077,"The availability of compute and data to train larger and larger language
models increases the demand for robust methods of benchmarking the true
progress of LM training. Recent years witnessed significant progress in
standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or
KILT have become de facto standard tools to compare large language models.
Following the trend to replicate GLUE for other languages, the KLEJ benchmark
has been released for Polish. In this paper, we evaluate the progress in
benchmarking for low-resourced languages. We note that only a handful of
languages have such comprehensive benchmarks. We also note the gap in the
number of tasks being evaluated by benchmarks for resource-rich English/Chinese
and the rest of the world. In this paper, we introduce LEPISZCZE (the Polish
word for glew, the Middle English predecessor of glue), a new, comprehensive
benchmark for Polish NLP with a large variety of tasks and high-quality
operationalization of the benchmark. We design LEPISZCZE with flexibility in
mind. Including new models, datasets, and tasks is as simple as possible while
still offering data versioning and model tracking. In the first run of the
benchmark, we test 13 experiments (task and dataset pairs) based on the five
most recent LMs for Polish. We use five datasets from the Polish benchmark and
add eight novel datasets. As the paper's main contribution, apart from
LEPISZCZE, we provide insights and experiences learned while creating the
benchmark for Polish as the blueprint to design similar benchmarks for other
low-resourced languages.",https://github.com/CLARIN-PL/LEPISZCZE,-1
a5d1c956-9b14-4399-8f65-b8c5ec128ffe,DFNet: Enhance Absolute Pose Regression with Direct Feature Matching,0.807341,"We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. By incorporating
exposure-adaptive novel view synthesis, our method successfully addresses
photometric distortions in outdoor environments that existing photometric-based
methods fail to handle. With domain-invariant feature matching, our solution
improves pose regression accuracy using semi-supervised learning on unlabeled
data. In particular, the pipeline consists of two components: Novel View
Synthesizer and DFNet. The former synthesizes novel views compensating for
changes in exposure and the latter regresses camera poses and extracts robust
features that close the domain gap between real images and synthetic ones.
Furthermore, we introduce an online synthetic data generation scheme. We show
that these approaches effectively enhance camera pose estimation both in indoor
and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by
outperforming existing single-image APR methods by as much as 56%, comparable
to 3D structure-based methods.",None,-1
ba15a27d-98df-40ba-a40d-5327d78e6d52,A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces,0.0300174,"Recent advancements in the domain of text-to-image synthesis have culminated
in a multitude of enhancements pertaining to quality, fidelity, and diversity.
Contemporary techniques enable the generation of highly intricate visuals which
rapidly approach near-photorealistic quality. Nevertheless, as progress is
achieved, the complexity of these methodologies increases, consequently
intensifying the comprehension barrier between individuals within the field and
those external to it.
  In an endeavor to mitigate this disparity, we propose a streamlined approach
for text-to-image generation, which encompasses both the training paradigm and
the sampling process. Despite its remarkable simplicity, our method yields
aesthetically pleasing images with few sampling iterations, allows for
intriguing ways for conditioning the model, and imparts advantages absent in
state-of-the-art techniques. To demonstrate the efficacy of this approach in
achieving outcomes comparable to existing works, we have trained a one-billion
parameter text-conditional model, which we refer to as ""Paella"". In the
interest of fostering future exploration in this field, we have made our source
code and models publicly accessible for the research community.",https://github.com/delicious-tasty/Paella,-1
02a2d64d-d1bc-4269-b689-403c5532b402,Motion Guided Deep Dynamic 3D Garments,0.597752,"Realistic dynamic garments on animated characters have many AR/VR
applications. While authoring such dynamic garment geometry is still a
challenging task, data-driven simulation provides an attractive alternative,
especially if it can be controlled simply using the motion of the underlying
character. In this work, we focus on motion guided dynamic 3D garments,
especially for loose garments. In a data-driven setup, we first learn a
generative space of plausible garment geometries. Then, we learn a mapping to
this space to capture the motion dependent dynamic deformations, conditioned on
the previous state of the garment as well as its relative position with respect
to the underlying body. Technically, we model garment dynamics, driven using
the input character motion, by predicting per-frame local displacements in a
canonical state of the garment that is enriched with frame-dependent skinning
weights to bring the garment to the global space. We resolve any remaining
per-frame collisions by predicting residual local displacements. The resultant
garment geometry is used as history to enable iterative rollout prediction. We
demonstrate plausible generalization to unseen body shapes and motion inputs,
and show improvements over multiple state-of-the-art alternatives.",https://geometry.cs.ucl.ac.uk/projects/2022/MotionDeepGarment/,-1
49db8583-794a-4e94-97aa-acee15b3f192,Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI,0.98378,"Recent years have seen a surge of interest in the field of explainable AI
(XAI), with a plethora of algorithms proposed in the literature. However, a
lack of consensus on how to evaluate XAI hinders the advancement of the field.
We highlight that XAI is not a monolithic set of technologies -- researchers
and practitioners have begun to leverage XAI algorithms to build XAI systems
that serve different usage contexts, such as model debugging and
decision-support. Algorithmic research of XAI, however, often does not account
for these diverse downstream usage contexts, resulting in limited effectiveness
or even unintended consequences for actual users, as well as difficulties for
practitioners to make technical choices. We argue that one way to close the gap
is to develop evaluation methods that account for different user requirements
in these usage contexts. Towards this goal, we introduce a perspective of
contextualized XAI evaluation by considering the relative importance of XAI
evaluation criteria for prototypical usage contexts of XAI. To explore the
context dependency of XAI evaluation criteria, we conduct two survey studies,
one with XAI topical experts and another with crowd workers. Our results urge
for responsible AI research with usage-informed evaluation practices, and
provide a nuanced understanding of user requirements for XAI in different usage
contexts.",None,-1
db974a13-9201-4bff-ac73-d528828ea891,Improving Rare Word Recognition with LM-aware MWER Training,0.807174,"Language models (LMs) significantly improve the recognition accuracy of
end-to-end (E2E) models on words rarely seen during training, when used in
either the shallow fusion or the rescoring setups. In this work, we introduce
LMs in the learning of hybrid autoregressive transducer (HAT) models in the
discriminative training framework, to mitigate the training versus inference
gap regarding the use of LMs. For the shallow fusion setup, we use LMs during
both hypotheses generation and loss computation, and the LM-aware MWER-trained
model achieves 10\% relative improvement over the model trained with standard
MWER on voice search test sets containing rare words. For the rescoring setup,
we learn a small neural module to generate per-token fusion weights in a
data-dependent manner. This model achieves the same rescoring WER as regular
MWER-trained model, but without the need for sweeping fusion weights.",None,-1
ccd5e23c-e45c-4771-91fe-0eca44896516,Full Contextual Attention for Multi-resolution Transformers in Semantic Segmentation,0.177902,"Transformers have proved to be very effective for visual recognition tasks.
In particular, vision transformers construct compressed global representations
through self-attention and learnable class tokens. Multi-resolution
transformers have shown recent successes in semantic segmentation but can only
capture local interactions in high-resolution feature maps. This paper extends
the notion of global tokens to build GLobal Attention Multi-resolution (GLAM)
transformers. GLAM is a generic module that can be integrated into most
existing transformer backbones. GLAM includes learnable global tokens, which
unlike previous methods can model interactions between all image regions, and
extracts powerful representations during training. Extensive experiments show
that GLAM-Swin or GLAM-Swin-UNet exhibit substantially better performances than
their vanilla counterparts on ADE20K and Cityscapes. Moreover, GLAM can be used
to segment large 3D medical images, and GLAM-nnFormer achieves new
state-of-the-art performance on the BCV dataset.",https://github.com/open-mmlab/mmsegmentation,-1
50a32756-278a-4c49-9cd4-fce476346a2d,Fair NLP Models with Differentially Private Text Encoders,0.332056,"Encoded text representations often capture sensitive attributes about
individuals (e.g., race or gender), which raise privacy concerns and can make
downstream models unfair to certain groups. In this work, we propose FEDERATE,
an approach that combines ideas from differential privacy and adversarial
training to learn private text representations which also induces fairer
models. We empirically evaluate the trade-off between the privacy of the
representations and the fairness and accuracy of the downstream model on four
NLP datasets. Our results show that FEDERATE consistently improves upon
previous methods, and thus suggest that privacy and fairness can positively
reinforce each other.",https://github.com/saist1993/DPNLP,-1
686726d5-b3ad-412c-be78-cf79de54ba5f,StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,0.65591,"Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance.",None,-1
d7f37da9-714b-494b-aec8-cbfff261a6f3,The Inverse of Exact Renormalization Group Flows as Statistical Inference,0.877106,"We build on the view of the Exact Renormalization Group (ERG) as an
instantiation of Optimal Transport described by a functional
convection-diffusion equation. We provide a new information theoretic
perspective for understanding the ERG through the intermediary of Bayesian
Statistical Inference. This connection is facilitated by the Dynamical Bayesian
Inference scheme, which encodes Bayesian inference in the form of a one
parameter family of probability distributions solving an integro-differential
equation derived from Bayes' law. In this note, we demonstrate how the
Dynamical Bayesian Inference equation is, itself, equivalent to a diffusion
equation which we dub Bayesian Diffusion. Identifying the features that define
Bayesian Diffusion, and mapping them onto the features that define the ERG, we
obtain a dictionary outlining how renormalization can be understood as the
inverse of statistical inference.",None,-1
b068b793-141e-4c0e-98f2-2e013315ad31,Code-DKT: A Code-based Knowledge Tracing Model for Programming Tasks,0.66959,"Knowledge tracing (KT) models are a popular approach for predicting students'
future performance at practice problems using their prior attempts. Though many
innovations have been made in KT, most models including the state-of-the-art
Deep KT (DKT) mainly leverage each student's response either as correct or
incorrect, ignoring its content. In this work, we propose Code-based Deep
Knowledge Tracing (Code-DKT), a model that uses an attention mechanism to
automatically extract and select domain-specific code features to extend DKT.
We compared the effectiveness of Code-DKT against Bayesian and Deep Knowledge
Tracing (BKT and DKT) on a dataset from a class of 50 students attempting to
solve 5 introductory programming assignments. Our results show that Code-DKT
consistently outperforms DKT by 3.07-4.00% AUC across the 5 assignments, a
comparable improvement to other state-of-the-art domain-general KT models over
DKT. Finally, we analyze problem-specific performance through a set of case
studies for one assignment to demonstrate when and how code features improve
Code-DKT's predictions.",https://github.com/YangAzure/Code-DKT,-1
1faf2ecc-de48-426a-90cb-d81577c1e0cd,Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost,0.772238,"State-of-the-art NLP systems represent inputs with word embeddings, but these
are brittle when faced with Out-of-Vocabulary (OOV) words. To address this
issue, we follow the principle of mimick-like models to generate vectors for
unseen words, by learning the behavior of pre-trained embeddings using only the
surface form of words. We present a simple contrastive learning framework,
LOVE, which extends the word representation of an existing pre-trained language
model (such as BERT), and makes it robust to OOV with few additional
parameters. Extensive evaluations demonstrate that our lightweight model
achieves similar or even better performances than prior competitors, both on
original datasets and on corrupted variants. Moreover, it can be used in a
plug-and-play fashion with FastText and BERT, where it significantly improves
their robustness.",https://github.com/tigerchen52/LOVE,129114
945fc2fb-94a5-4cef-9d8c-9d20e58ebd1c,The Theory of Artificial Immutability: Protecting Algorithmic Groups Under Anti-Discrimination Law,0.6479,"Artificial Intelligence (AI) is increasingly used to make important decisions
about people. While issues of AI bias and proxy discrimination are well
explored, less focus has been paid to the harms created by profiling based on
groups that do not map to or correlate with legally protected groups such as
sex or ethnicity. This raises a question: are existing equality laws able to
protect against emergent AI-driven inequality? This article examines the legal
status of algorithmic groups in North American and European non-discrimination
doctrine, law, and jurisprudence and will show that algorithmic groups are not
comparable to traditional protected groups. Nonetheless, these new groups are
worthy of protection. I propose a new theory of harm - ""the theory of
artificial immutability"" - that aims to bring AI groups within the scope of the
law. My theory describes how algorithmic groups act as de facto immutable
characteristics in practice that limit people's autonomy and prevent them from
achieving important goals.",None,-1
494f3a3d-79b7-4959-809d-45a172582e84,Multi-Document Summarization with Centroid-Based Pretraining,0.882681,"In Multi-Document Summarization (MDS), the input can be modeled as a set of
documents, and the output is its summary. In this paper, we focus on
pretraining objectives for MDS. Specifically, we introduce a novel pretraining
objective, which involves selecting the ROUGE-based centroid of each document
cluster as a proxy for its summary. Our objective thus does not require human
written summaries and can be utilized for pretraining on a dataset consisting
solely of document sets. Through zero-shot, few-shot, and fully supervised
experiments on multiple MDS datasets, we show that our model Centrum is better
or comparable to a state-of-the-art model. We make the pretrained and
fine-tuned models freely available to the research community
https://github.com/ratishsp/centrum.",https://github.com/ratishsp/centrum,-1
a8f6b463-26f2-4f14-b88c-9123ee4699e3,Understanding Interpersonal Conflict Types and their Impact on Perception Classification,0.63351,"Studies on interpersonal conflict have a long history and contain many
suggestions for conflict typology. We use this as the basis of a novel
annotation scheme and release a new dataset of situations and conflict aspect
annotations. We then build a classifier to predict whether someone will
perceive the actions of one individual as right or wrong in a given situation.
Our analyses include conflict aspects, but also generated clusters, which are
human validated, and show differences in conflict content based on the
relationship of participants to the author. Our findings have important
implications for understanding conflict and social norms.",https://github.com/caisa-lab/interpersonal-conflict-types,-1
5650267e-d8b2-4cd1-b35c-f1d15e1c9ac0,ER: Equivariance Regularizer for Knowledge Graph Completion,0.471523,"Tensor factorization and distanced based models play important roles in
knowledge graph completion (KGC). However, the relational matrices in KGC
methods often induce a high model complexity, bearing a high risk of
overfitting. As a remedy, researchers propose a variety of different
regularizers such as the tensor nuclear norm regularizer. Our motivation is
based on the observation that the previous work only focuses on the ""size"" of
the parametric space, while leaving the implicit semantic information widely
untouched. To address this issue, we propose a new regularizer, namely,
Equivariance Regularizer (ER), which can suppress overfitting by leveraging the
implicit semantic information. Specifically, ER can enhance the generalization
ability of the model by employing the semantic equivariance between the head
and tail entities. Moreover, it is a generic solution for both distance based
models and tensor factorization based models. The experimental results indicate
a clear and substantial improvement over the state-of-the-art relation
prediction methods.",https://github.com/Lion-ZS/ER,-1
97ce9043-e3c7-472b-a66a-476306cdda15,Synthetic Disinformation Attacks on Automated Fact Verification Systems,0.908585,"Automated fact-checking is a needed technology to curtail the spread of
online misinformation. One current framework for such solutions proposes to
verify claims by retrieving supporting or refuting evidence from related
textual sources. However, the realistic use cases for fact-checkers will
require verifying claims against evidence sources that could be affected by the
same misinformation. Furthermore, the development of modern NLP tools that can
produce coherent, fabricated content would allow malicious actors to
systematically generate adversarial disinformation for fact-checkers.
  In this work, we explore the sensitivity of automated fact-checkers to
synthetic adversarial evidence in two simulated settings: AdversarialAddition,
where we fabricate documents and add them to the evidence repository available
to the fact-checking system, and AdversarialModification, where existing
evidence source documents in the repository are automatically altered. Our
study across multiple models on three benchmarks demonstrates that these
systems suffer significant performance drops against these attacks. Finally, we
discuss the growing threat of modern NLG systems as generators of
disinformation in the context of the challenges they pose to automated
fact-checkers.",https://github.com/Yibing-Du/adversarial-factcheck,-1
31fac8ce-12fb-44d7-bef0-a0236c4d1ee7,Progressive Motion Context Refine Network for Efficient Video Frame Interpolation,0.0816987,"Recently, flow-based frame interpolation methods have achieved great success
by first modeling optical flow between target and input frames, and then
building synthesis network for target frame generation. However, above cascaded
architecture can lead to large model size and inference delay, hindering them
from mobile and real-time applications. To solve this problem, we propose a
novel Progressive Motion Context Refine Network (PMCRNet) to predict motion
fields and image context jointly for higher efficiency. Different from others
that directly synthesize target frame from deep feature, we explore to simplify
frame interpolation task by borrowing existing texture from adjacent input
frames, which means that decoder in each pyramid level of our PMCRNet only
needs to update easier intermediate optical flow, occlusion merge mask and
image residual. Moreover, we introduce a new annealed multi-scale
reconstruction loss to better guide the learning process of this efficient
PMCRNet. Experiments on multiple benchmarks show that proposed approaches not
only achieve favorable quantitative and qualitative results but also reduces
current model size and running time significantly.",None,-1
3720e09f-81c8-43c4-ad41-7391714b23d7,Inharmonious Region Localization via Recurrent Self-Reasoning,0.0898163,"Synthetic images created by image editing operations are prevalent, but the
color or illumination inconsistency between the manipulated region and
background may make it unrealistic. Thus, it is important yet challenging to
localize the inharmonious region to improve the quality of synthetic image.
Inspired by the classic clustering algorithm, we aim to group pixels into two
clusters: inharmonious cluster and background cluster by inserting a novel
Recurrent Self-Reasoning (RSR) module into the bottleneck of UNet structure.
The mask output from RSR module is provided for the decoder as attention
guidance. Finally, we adaptively combine the masks from RSR and the decoder to
form our final mask. Experimental results on the image harmonization dataset
demonstrate that our method achieves competitive performance both
quantitatively and qualitatively.",None,-1
9df34574-2e98-4953-ae15-271b0d74a340,MAiVAR: Multimodal Audio-Image and Video Action Recognizer,0.189424,"Currently, action recognition is predominately performed on video data as
processed by CNNs. We investigate if the representation process of CNNs can
also be leveraged for multimodal action recognition by incorporating
image-based audio representations of actions in a task. To this end, we propose
Multimodal Audio-Image and Video Action Recognizer (MAiVAR), a CNN-based
audio-image to video fusion model that accounts for video and audio modalities
to achieve superior action recognition performance. MAiVAR extracts meaningful
image representations of audio and fuses it with video representation to
achieve better performance as compared to both modalities individually on a
large-scale action recognition dataset.",None,-1
235f4dac-5191-4504-9394-512ed223e16f,Feature Selection Enhancement and Feature Space Visualization for Speech-Based Emotion Recognition,0.368304,"Robust speech emotion recognition relies on the quality of the speech
features. We present speech features enhancement strategy that improves speech
emotion recognition. We used the INTERSPEECH 2010 challenge feature-set. We
identified subsets from the features set and applied Principle Component
Analysis to the subsets. Finally, the features are fused horizontally. The
resulting feature set is analyzed using t-distributed neighbour embeddings
(t-SNE) before the application of features for emotion recognition. The method
is compared with the state-of-the-art methods used in the literature. The
empirical evidence is drawn using two well-known datasets: Emotional Speech
Dataset (EMO-DB) and Ryerson Audio-Visual Database of Emotional Speech and Song
(RAVDESS) for two languages, German and English, respectively. Our method
achieved an average recognition gain of 11.5\% for six out of seven emotions
for the EMO-DB dataset, and 13.8\% for seven out of eight emotions for the
RAVDESS dataset as compared to the baseline study.",None,-1
534b5c13-df34-4ba5-b067-617223c4347a,The Reality of Multi-Lingual Machine Translation,0.180646,"Our book ""The Reality of Multi-Lingual Machine Translation"" discusses the
benefits and perils of using more than two languages in machine translation
systems. While focused on the particular task of sequence-to-sequence
processing and multi-task learning, the book targets somewhat beyond the area
of natural language processing. Machine translation is for us a prime example
of deep learning applications where human skills and learning capabilities are
taken as a benchmark that many try to match and surpass. We document that some
of the gains observed in multi-lingual translation may result from simpler
effects than the assumed cross-lingual transfer of knowledge.
  In the first, rather general part, the book will lead you through the
motivation for multi-linguality, the versatility of deep neural networks
especially in sequence-to-sequence tasks to complications of this learning. We
conclude the general part with warnings against too optimistic and unjustified
explanations of the gains that neural networks demonstrate.
  In the second part, we fully delve into multi-lingual models, with a
particularly careful examination of transfer learning as one of the more
straightforward approaches utilizing additional languages. The recent
multi-lingual techniques, including massive models, are surveyed and practical
aspects of deploying systems for many languages are discussed. The conclusion
highlights the open problem of machine understanding and reminds of two ethical
aspects of building large-scale models: the inclusivity of research and its
ecological trace.",None,-1
5faaccf1-51c0-4a94-91b9-f37777d08cee,AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation,0.665693,"Dense retrievers have made significant strides in text retrieval and
open-domain question answering, even though most achievements were made
possible only with large amounts of human supervision. In this work, we aim to
develop unsupervised methods by proposing two methods that create pseudo
query-document pairs and train dense retrieval models in an annotation-free and
scalable manner: query extraction and transferred query generation. The former
method produces pseudo queries by selecting salient spans from the original
document. The latter utilizes generation models trained for other NLP tasks
(e.g., summarization) to produce pseudo queries. Extensive experiments show
that models trained with the proposed augmentation methods can perform
comparably well (or better) to multiple strong baselines. Combining those
strategies leads to further improvements, achieving the state-of-the-art
performance of unsupervised dense retrieval on both BEIR and ODQA datasets.",None,-1
e81d14ed-0a6b-4c12-ac84-80fb1d417bfd,GMF: General Multimodal Fusion Framework for Correspondence Outlier Rejection,0.817084,"Rejecting correspondence outliers enables to boost the correspondence
quality, which is a critical step in achieving high point cloud registration
accuracy. The current state-of-the-art correspondence outlier rejection methods
only utilize the structure features of the correspondences. However, texture
information is critical to reject the correspondence outliers in our human
vision system. In this paper, we propose General Multimodal Fusion (GMF) to
learn to reject the correspondence outliers by leveraging both the structure
and texture information. Specifically, two cross-attention-based fusion layers
are proposed to fuse the texture information from paired images and structure
information from point correspondences. Moreover, we propose a convolutional
position encoding layer to enhance the difference between Tokens and enable the
encoding feature pay attention to neighbor information. Our position encoding
layer will make the cross-attention operation integrate both local and global
information. Experiments on multiple datasets(3DMatch, 3DLoMatch, KITTI) and
recent state-of-the-art models (3DRegNet, DGR, PointDSC) prove that our GMF
achieves wide generalization ability and consistently improves the point cloud
registration accuracy. Furthermore, several ablation studies demonstrate the
robustness of the proposed GMF on different loss functions, lighting conditions
and noises.The code is available at https://github.com/XiaoshuiHuang/GMF.",https://github.com/XiaoshuiHuang/GMF,-1
256307fa-eff6-44c5-aeda-bc2ae5ce9cf4,Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction,0.594096,"Multimodal named entity recognition and relation extraction (MNER and MRE) is
a fundamental and crucial branch in information extraction. However, existing
approaches for MNER and MRE usually suffer from error sensitivity when
irrelevant object images incorporated in texts. To deal with these issues, we
propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for
visual-enhanced entity and relation extraction, aiming to achieve more
effective and robust performance. Specifically, we regard visual representation
as pluggable visual prefix to guide the textual representation for error
insensitive forecasting decision. We further propose a dynamic gated
aggregation strategy to achieve hierarchical multi-scaled visual features as
visual prefix for fusion. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our method, and achieve state-of-the-art
performance. Code is available in https://github.com/zjunlp/HVPNeT.",https://github.com/zjunlp/HVPNeT,-1
a8cb1d09-e993-4af1-8243-b9e044bc66b3,Formal Mathematics Statement Curriculum Learning,0.999974,"We explore the use of expert iteration in the context of language modeling
applied to formal mathematics. We show that at same compute budget, expert
iteration, by which we mean proof search interleaved with learning,
dramatically outperforms proof search only. We also observe that when applied
to a collection of formal statements of sufficiently varied difficulty, expert
iteration is capable of finding and solving a curriculum of increasingly
difficult problems, without the need for associated ground-truth proofs.
Finally, by applying this expert iteration to a manually curated set of problem
statements, we achieve state-of-the-art on the miniF2F benchmark, automatically
solving multiple challenging problems drawn from high school olympiads.",https://github.com/openai/lean-gym,-1
140ca47d-51a5-4d2b-9db5-e78aaea207a5,Tencent's Multilingual Machine Translation System for WMT22 Large-Scale African Languages,0.590783,"This paper describes Tencent's multilingual machine translation systems for
the WMT22 shared task on Large-Scale Machine Translation Evaluation for African
Languages. We participated in the $\mathbf{constrained}$ translation track in
which only the data and pretrained models provided by the organizer are
allowed. The task is challenging due to three problems, including the absence
of training data for some to-be-evaluated language pairs, the uneven
optimization of language pairs caused by data imbalance, and the curse of
multilinguality. To address these problems, we adopt data augmentation,
distributionally robust optimization, and language family grouping,
respectively, to develop our multilingual neural machine translation (MNMT)
models. Our submissions won the $\mathbf{1st\ place}$ on the blind test sets in
terms of the automatic evaluation metrics. Codes, models, and detailed
competition results are available at
https://github.com/wxjiao/WMT2022-Large-Scale-African.",https://github.com/wxjiao/,-1
7d40a347-4adf-41e3-be6b-e68c301c78f2,MABEL: Attenuating Gender Bias using Textual Entailment Data,0.651926,"Pre-trained language models encode undesirable social biases, which are
further exacerbated in downstream use. To this end, we propose MABEL (a Method
for Attenuating Gender Bias using Entailment Labels), an intermediate
pre-training approach for mitigating gender bias in contextualized
representations. Key to our approach is the use of a contrastive learning
objective on counterfactually augmented, gender-balanced entailment pairs from
natural language inference (NLI) datasets. We also introduce an alignment
regularizer that pulls identical entailment pairs along opposite gender
directions closer. We extensively evaluate our approach on intrinsic and
extrinsic metrics, and show that MABEL outperforms previous task-agnostic
debiasing approaches in terms of fairness. It also preserves task performance
after fine-tuning on downstream tasks. Together, these findings demonstrate the
suitability of NLI data as an effective means of bias mitigation, as opposed to
only using unlabeled sentences in the literature. Finally, we identify that
existing approaches often use evaluation settings that are insufficient or
inconsistent. We make an effort to reproduce and compare previous methods, and
call for unifying the evaluation settings across gender debiasing methods for
better future comparison.",https://github.com/princeton-nlp/MABEL,-1
3a437b23-0b99-444f-9e5a-b72b3fc9e4fa,BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate Speech in Different Social Contexts,0.763829,"Social media platforms and online streaming services have spawned a new breed
of Hate Speech (HS). Due to the massive amount of user-generated content on
these sites, modern machine learning techniques are found to be feasible and
cost-effective to tackle this problem. However, linguistically diverse datasets
covering different social contexts in which offensive language is typically
used are required to train generalizable models. In this paper, we identify the
shortcomings of existing Bangla HS datasets and introduce a large manually
labeled dataset BD-SHS that includes HS in different social contexts. The
labeling criteria were prepared following a hierarchical annotation process,
which is the first of its kind in Bangla HS to the best of our knowledge. The
dataset includes more than 50,200 offensive comments crawled from online social
networking sites and is at least 60% larger than any existing Bangla HS
datasets. We present the benchmark result of our dataset by training different
NLP models resulting in the best one achieving an F1-score of 91.0%. In our
experiments, we found that a word embedding trained exclusively using 1.47
million comments from social media and streaming sites consistently resulted in
better modeling of HS detection in comparison to other pre-trained embeddings.
Our dataset and all accompanying codes is publicly available at
github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media",https://github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media,-1
9fa9cf93-8a77-4710-88dd-b99688c3b4ef,An Extreme-Adaptive Time Series Prediction Model Based on Probability-Enhanced LSTM Neural Networks,0.277467,"Forecasting time series with extreme events has been a challenging and
prevalent research topic, especially when the time series data are affected by
complicated uncertain factors, such as is the case in hydrologic prediction.
Diverse traditional and deep learning models have been applied to discover the
nonlinear relationships and recognize the complex patterns in these types of
data. However, existing methods usually ignore the negative influence of
imbalanced data, or severe events, on model training. Moreover, methods are
usually evaluated on a small number of generally well-behaved time series,
which does not show their ability to generalize. To tackle these issues, we
propose a novel probability-enhanced neural network model, called NEC+, which
concurrently learns extreme and normal prediction functions and a way to choose
among them via selective back propagation. We evaluate the proposed model on
the difficult 3-day ahead hourly water level prediction task applied to 9
reservoirs in California. Experimental results demonstrate that the proposed
model significantly outperforms state-of-the-art baselines and exhibits
superior generalization ability on data with diverse distributions.",https://github.com/davidanastasiu/NECPlus,-1
46079981-7971-4ee2-84e9-9134a30aee74,Self-Supervised Losses for One-Class Textual Anomaly Detection,0.273566,"Current deep learning methods for anomaly detection in text rely on
supervisory signals in inliers that may be unobtainable or bespoke
architectures that are difficult to tune. We study a simpler alternative:
fine-tuning Transformers on the inlier data with self-supervised objectives and
using the losses as an anomaly score. Overall, the self-supervision approach
outperforms other methods under various anomaly detection scenarios, improving
the AUROC score on semantic anomalies by 11.6% and on syntactic anomalies by
22.8% on average. Additionally, the optimal objective and resultant learnt
representation depend on the type of downstream anomaly. The separability of
anomalies and inliers signals that a representation is more effective for
detecting semantic anomalies, whilst the presence of narrow feature directions
signals a representation that is effective for detecting syntactic anomalies.",None,-1
2edb301a-fe85-4e62-950a-4c775dd21fa8,Personalized Game Difficulty Prediction Using Factorization Machines,0.228748,"The accurate and personalized estimation of task difficulty provides many
opportunities for optimizing user experience. However, user diversity makes
such difficulty estimation hard, in that empirical measurements from some user
sample do not necessarily generalize to others. In this paper, we contribute a
new approach for personalized difficulty estimation of game levels, borrowing
methods from content recommendation. Using factorization machines (FM) on a
large dataset from a commercial puzzle game, we are able to predict difficulty
as the number of attempts a player requires to pass future game levels, based
on observed attempt counts from earlier levels and levels played by others. In
addition to performance and scalability, FMs offer the benefit that the learned
latent variable model can be used to study the characteristics of both players
and game levels that contribute to difficulty. We compare the approach to a
simple non-personalized baseline and a personalized prediction using Random
Forests. Our results suggest that FMs are a promising tool enabling game
designers to both optimize player experience and learn more about their players
and the game.",None,3054
e0dd337e-9e83-417c-bbf9-ea0462dba3fc,Does Corpus Quality Really Matter for Low-Resource Languages?,0.326311,"The vast majority of non-English corpora are derived from automatically
filtered versions of CommonCrawl. While prior work has identified major issues
on the quality of these datasets (Kreutzer et al., 2021), it is not clear how
this impacts downstream performance. Taking representation learning in Basque
as a case study, we explore tailored crawling (manually identifying and
scraping websites with high-quality content) as an alternative to filtering
CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque
portion of popular multilingual corpora like CC100 and mC4, yet it has a much
higher quality according to native annotators. For instance, 66% of documents
are rated as high-quality for EusCrawl, in contrast with <33% for both mC4 and
CC100. Nevertheless, we obtain similar results on downstream NLU tasks
regardless of the corpus used for pre-training. Our work suggests that NLU
performance in low-resource languages is not primarily constrained by the
quality of the data, and other factors like corpus size and domain coverage can
play a more important role.",None,-1
9aa799be-7217-47e4-b8a6-c50b18cf1b31,"Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",0.470246,"In order to equip NLP systems with selective prediction capability, several
task-specific approaches have been proposed. However, which approaches work
best across tasks or even if they consistently outperform the simplest baseline
'MaxProb' remains to be explored. To this end, we systematically study
'selective prediction' in a large-scale setup of 17 datasets across several NLP
tasks. Through comprehensive experiments under in-domain (IID), out-of-domain
(OOD), and adversarial (ADV) settings, we show that despite leveraging
additional resources (held-out data/computation), none of the existing
approaches consistently and considerably outperforms MaxProb in all three
settings. Furthermore, their performance does not translate well across tasks.
For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate
Detection datasets but does not fare well on NLI datasets, especially in the
OOD setting. Thus, we recommend that future selective prediction approaches
should be evaluated across tasks and settings for reliable estimation of their
capabilities.",None,-1
83008a05-3ca4-4c63-94b7-384d5bc47094,Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality,0.903804,"Recent visuolinguistic pre-trained models show promising progress on various
end tasks such as image retrieval and video captioning. Yet, they fail
miserably on the recently proposed Winoground dataset, which challenges models
to match paired images and English captions, with items constructed to overlap
lexically but differ in meaning (e.g., ""there is a mug in some grass"" vs.
""there is some grass in a mug""). By annotating the dataset using new
fine-grained tags, we show that solving the Winoground task requires not just
compositional language understanding, but a host of other abilities like
commonsense reasoning or locating small, out-of-focus objects in low-resolution
images. In this paper, we identify the dataset's main challenges through a
suite of experiments on related tasks (probing task, image retrieval task),
data augmentation, and manual inspection of the dataset. Our analysis suggests
that a main challenge in visuolinguistic models may lie in fusing visual and
textual representations, rather than in compositional language understanding.
We release our annotation and code at
https://github.com/ajd12342/why-winoground-hard .",https://github.com/ajd12342/why-winoground-hard,-1
2d65fea0-e537-42c0-81dd-f23ed755c43b,TE2Rules: Explaining Tree Ensembles using Rules,0.216296,"Tree Ensemble (TE) models, such as Gradient Boosted Trees, often achieve
optimal performance on tabular datasets, yet their lack of transparency poses
challenges for comprehending their decision logic. This paper introduces
TE2Rules (Tree Ensemble to Rules), a novel approach for explaining binary
classification tree ensemble models through a list of rules, particularly
focusing on explaining the minority class. Many state-of-the-art explainers
struggle with minority class explanations, making TE2Rules valuable in such
cases. The rules generated by TE2Rules closely approximate the original model,
ensuring high fidelity, providing an accurate and interpretable means to
understand decision-making. Experimental results demonstrate that TE2Rules
scales effectively to tree ensembles with hundreds of trees, achieving higher
fidelity within runtimes comparable to baselines. TE2Rules allows for a
trade-off between runtime and fidelity, enhancing its practical applicability.
The implementation is available here: https://github.com/linkedin/TE2Rules.",https://github.com/linkedin/TE2Rules,-1
feb6af05-4c7b-4402-9d49-bd600664de25,Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation,0.758711,"Neural networks tend to gradually forget the previously learned knowledge
when learning multiple tasks sequentially from dynamic data distributions. This
problem is called \textit{catastrophic forgetting}, which is a fundamental
challenge in the continual learning of neural networks. In this work, we
observe that catastrophic forgetting not only occurs in continual learning but
also affects the traditional static training. Neural networks, especially
neural machine translation models, suffer from catastrophic forgetting even if
they learn from a static training set. To be specific, the final model pays
imbalanced attention to training samples, where recently exposed samples
attract more attention than earlier samples. The underlying cause is that
training samples do not get balanced training in each model update, so we name
this problem \textit{imbalanced training}. To alleviate this problem, we
propose Complementary Online Knowledge Distillation (COKD), which uses
dynamically updated teacher models trained on specific data orders to
iteratively provide complementary knowledge to the student model. Experimental
results on multiple machine translation tasks show that our method successfully
alleviates the problem of imbalanced training and achieves substantial
improvements over strong baseline systems.",https://github.com/ictnlp/COKD,-1
95ab52f8-2163-426d-9e36-cdc910b68aac,Learning Physical Dynamics with Subequivariant Graph Neural Networks,0.575253,"Graph Neural Networks (GNNs) have become a prevailing tool for learning
physical dynamics. However, they still encounter several challenges: 1)
Physical laws abide by symmetry, which is a vital inductive bias accounting for
model generalization and should be incorporated into the model design. Existing
simulators either consider insufficient symmetry, or enforce excessive
equivariance in practice when symmetry is partially broken by gravity. 2)
Objects in the physical world possess diverse shapes, sizes, and properties,
which should be appropriately processed by the model. To tackle these
difficulties, we propose a novel backbone, Subequivariant Graph Neural Network,
which 1) relaxes equivariance to subequivariance by considering external fields
like gravity, where the universal approximation ability holds theoretically; 2)
introduces a new subequivariant object-aware message passing for learning
physical interactions between multiple objects of various shapes in the
particle-based representation; 3) operates in a hierarchical fashion, allowing
for modeling long-range and complex interactions. Our model achieves on average
over 3% enhancement in contact prediction accuracy across 8 scenarios on
Physion and 2X lower rollout MSE on RigidFall compared with state-of-the-art
GNN simulators, while exhibiting strong generalization and data efficiency.",https://hanjq17.github.io/SGNN/,-1
91e2c541-917a-4713-b878-0864f6cf1dd2,Motion Transformer with Global Intention Localization and Local Movement Refinement,0.999956,"Predicting multimodal future behavior of traffic participants is essential
for robotic vehicles to make safe decisions. Existing works explore to directly
predict future trajectories based on latent features or utilize dense goal
candidates to identify agent's destinations, where the former strategy
converges slowly since all motion modes are derived from the same feature while
the latter strategy has efficiency issue since its performance highly relies on
the density of goal candidates. In this paper, we propose Motion TRansformer
(MTR) framework that models motion prediction as the joint optimization of
global intention localization and local movement refinement. Instead of using
goal candidates, MTR incorporates spatial intention priors by adopting a small
set of learnable motion query pairs. Each motion query pair takes charge of
trajectory prediction and refinement for a specific motion mode, which
stabilizes the training process and facilitates better multimodal predictions.
Experiments show that MTR achieves state-of-the-art performance on both the
marginal and joint motion prediction challenges, ranking 1st on the
leaderboards of Waymo Open Motion Dataset. The source code is available at
https://github.com/sshaoshuai/MTR.",https://github.com/sshaoshuai/MTR,-1
1037bc3e-e3d4-4dfd-a0af-e4fdbb691c3d,A Computational Inflection for Scientific Discovery,0.708901,"We stand at the foot of a significant inflection in the trajectory of
scientific discovery. As society continues on its fast-paced digital
transformation, so does humankind's collective scientific knowledge and
discourse. We now read and write papers in digitized form, and a great deal of
the formal and informal processes of science are captured digitally --
including papers, preprints and books, code and datasets, conference
presentations, and interactions in social networks and collaboration and
communication platforms. The transition has led to the creation and growth of a
tremendous amount of information -- much of which is available for public
access -- opening exciting opportunities for computational models and systems
that analyze and harness it. In parallel, exponential growth in data processing
power has fueled remarkable advances in artificial intelligence, including
large neural language models capable of learning powerful representations from
unstructured text. Dramatic changes in scientific communication -- such as the
advent of the first scientific journal in the 17th century -- have historically
catalyzed revolutions in scientific thought. The confluence of societal and
computational trends suggests that computer science is poised to ignite a
revolution in the scientific process itself.",None,101093
b9fc3e32-e66a-49f1-ba5c-46fcfe154dce,Towards Loosely-Coupling Knowledge Graph Embeddings and Ontology-based Reasoning,0.112518,"Knowledge graph completion (a.k.a.~link prediction), i.e.,~the task of
inferring missing information from knowledge graphs, is a widely used task in
many applications, such as product recommendation and question answering. The
state-of-the-art approaches of knowledge graph embeddings and/or rule mining
and reasoning are data-driven and, thus, solely based on the information the
input knowledge graph contains. This leads to unsatisfactory prediction results
which make such solutions inapplicable to crucial domains such as healthcare.
To further enhance the accuracy of knowledge graph completion we propose to
loosely-couple the data-driven power of knowledge graph embeddings with
domain-specific reasoning stemming from experts or entailment regimes (e.g.,
OWL2). In this way, we not only enhance the prediction accuracy with domain
knowledge that may not be included in the input knowledge graph but also allow
users to plugin their own knowledge graph embedding and reasoning method. Our
initial results show that we enhance the MRR accuracy of vanilla knowledge
graph embeddings by up to 3x and outperform hybrid solutions that combine
knowledge graph embeddings with rule mining and reasoning up to 3.5x MRR.",None,-1
279bb0f9-8728-41d0-b4a0-11e130ddaa89,Robust Disentangled Variational Speech Representation Learning for Zero-shot Voice Conversion,0.962868,"Traditional studies on voice conversion (VC) have made progress with parallel
training data and known speakers. Good voice conversion quality is obtained by
exploring better alignment modules or expressive mapping functions. In this
study, we investigate zero-shot VC from a novel perspective of self-supervised
disentangled speech representation learning. Specifically, we achieve the
disentanglement by balancing the information flow between global speaker
representation and time-varying content representation in a sequential
variational autoencoder (VAE). A zero-shot voice conversion is performed by
feeding an arbitrary speaker embedding and content embeddings to the VAE
decoder. Besides that, an on-the-fly data augmentation training strategy is
applied to make the learned representation noise invariant. On TIMIT and VCTK
datasets, we achieve state-of-the-art performance on both objective evaluation,
i.e., speaker verification (SV) on speaker embedding and content embedding, and
subjective evaluation, i.e., voice naturalness and similarity, and remains to
be robust even with noisy source/target utterances.",https://jlian2.github.io/Robust-Voice-Style-Transfer,-1
739876d2-43da-4367-8f5c-7aa88777a90d,Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction,0.74584,"The DocRED dataset is one of the most popular and widely used benchmarks for
document-level relation extraction (RE). It adopts a recommend-revise
annotation scheme so as to have a large-scale annotated dataset. However, we
find that the annotation of DocRED is incomplete, i.e., false negative samples
are prevalent. We analyze the causes and effects of the overwhelming false
negative problem in the DocRED dataset. To address the shortcoming, we
re-annotate 4,053 documents in the DocRED dataset by adding the missed relation
triples back to the original DocRED. We name our revised DocRED dataset
Re-DocRED. We conduct extensive experiments with state-of-the-art neural models
on both datasets, and the experimental results show that the models trained and
evaluated on our Re-DocRED achieve performance improvements of around 13 F1
points. Moreover, we conduct a comprehensive analysis to identify the potential
areas for further improvement. Our dataset is publicly available at
https://github.com/tonytan48/Re-DocRED.",https://github.com/tonytan48/Re-DocRED,-1
bb0a0b8b-81fa-454e-902d-0ffdc9d2caad,Neighboring Backdoor Attacks on Graph Convolutional Network,0.25536,"Backdoor attacks have been widely studied to hide the misclassification rules
in the normal models, which are only activated when the model is aware of the
specific inputs (i.e., the trigger). However, despite their success in the
conventional Euclidean space, there are few studies of backdoor attacks on
graph structured data. In this paper, we propose a new type of backdoor which
is specific to graph data, called neighboring backdoor. Considering the
discreteness of graph data, how to effectively design the triggers while
retaining the model accuracy on the original task is the major challenge. To
address such a challenge, we set the trigger as a single node, and the backdoor
is activated when the trigger node is connected to the target node. To preserve
the model accuracy, the model parameters are not allowed to be modified. Thus,
when the trigger node is not connected, the model performs normally. Under
these settings, in this work, we focus on generating the features of the
trigger node. Two types of backdoors are proposed: (1) Linear Graph Convolution
Backdoor which finds an approximation solution for the feature generation (can
be viewed as an integer programming problem) by looking at the linear part of
GCNs. (2) Variants of existing graph attacks. We extend current gradient-based
attack methods to our backdoor attack scenario. Extensive experiments on two
social networks and two citation networks datasets demonstrate that all
proposed backdoors can achieve an almost 100\% attack success rate while having
no impact on predictive accuracy.",None,-1
60fab58f-ebbc-4219-9a68-d3e61dc3c60e,Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models,0.907968,"Pre-trained vision-language models (e.g., CLIP) have shown promising
zero-shot generalization in many downstream tasks with properly designed text
prompts. Instead of relying on hand-engineered prompts, recent works learn
prompts using the training data from downstream tasks. While effective,
training on domain-specific data reduces a model's generalization capability to
unseen new domains. In this work, we propose test-time prompt tuning (TPT), a
method that can learn adaptive prompts on the fly with a single test sample.
For image classification, TPT optimizes the prompt by minimizing the entropy
with confidence selection so that the model has consistent predictions across
different augmented views of each test sample. In evaluating generalization to
natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP
by 3.6% on average, surpassing previous prompt tuning approaches that require
additional task-specific training data. In evaluating cross-dataset
generalization with unseen categories, TPT performs on par with the
state-of-the-art approaches that use additional training data. Project page:
https://azshue.github.io/TPT.",https://tinyurl.com/yr3zmhma,-1
425e119f-0c92-4c06-8497-fbe49dab48fd,QAScore -- An Unsupervised Unreferenced Metric for the Question Generation Evaluation,0.20757,"Question Generation (QG) aims to automate the task of composing questions for
a passage with a set of chosen answers found within the passage. In recent
years, the introduction of neural generation models has resulted in substantial
improvements of automatically generated questions in terms of quality,
especially compared to traditional approaches that employ manually crafted
heuristics. However, the metrics commonly applied in QG evaluations have been
criticized for their low agreement with human judgement. We therefore propose a
new reference-free evaluation metric that has the potential to provide a better
mechanism for evaluating QG systems, called QAScore. Instead of fine-tuning a
language model to maximize its correlation with human judgements, QAScore
evaluates a question by computing the cross entropy according to the
probability that the language model can correctly generate the masked words in
the answer to that question. Furthermore, we conduct a new crowd-sourcing human
evaluation experiment for the QG evaluation to investigate how QAScore and
other metrics can correlate with human judgements. Experiments show that
QAScore obtains a stronger correlation with the results of our proposed human
evaluation method compared to existing traditional word-overlap-based metrics
such as BLEU and ROUGE, as well as the existing pretrained-model-based metric
BERTScore.",None,-1
172a7406-3fdf-4e3b-8d8b-65f329bd472f,On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,0.810268,"Many recent studies on large-scale language models have reported successful
in-context zero- and few-shot learning ability. However, the in-depth analysis
of when in-context learning occurs is still lacking. For example, it is unknown
how in-context learning performance changes as the training corpus varies.
Here, we investigate the effects of the source and size of the pretraining
corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From
our in-depth investigation, we introduce the following observations: (1)
in-context learning performance heavily depends on the corpus domain source,
and the size of the pretraining corpus does not necessarily determine the
emergence of in-context learning, (2) in-context learning ability can emerge
when a language model is trained on a combination of multiple corpora, even
when each corpus does not result in in-context learning on its own, (3)
pretraining with a corpus related to a downstream task does not always
guarantee the competitive in-context learning performance of the downstream
task, especially in the few-shot setting, and (4) the relationship between
language modeling (measured in perplexity) and in-context learning does not
always correlate: e.g., low perplexity does not always imply high in-context
few-shot learning performance.",None,150035
7f4dc28b-4cba-47d4-a545-15df9b050617,Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning,0.546475,"Our goal is a question-answering (QA) system that can show how its answers
are implied by its own internal beliefs via a systematic chain of reasoning.
Such a capability would allow better understanding of why a model produced the
answer it did. Our approach is to recursively combine a trained
backward-chaining model, capable of generating a set of premises entailing an
answer hypothesis, with a verifier that checks that the model itself believes
those premises (and the entailment itself) through self-querying. To our
knowledge, this is the first system to generate multistep chains that are both
faithful (the answer follows from the reasoning) and truthful (the chain
reflects the system's own internal beliefs). In evaluation using two different
datasets, users judge that a majority (70%+) of generated chains clearly show
how an answer follows from a set of facts - substantially better than a
high-performance baseline - while preserving answer accuracy. By materializing
model beliefs that systematically support an answer, new opportunities arise
for understanding the model's system of belief, and diagnosing and correcting
its misunderstandings when an answer is wrong.",None,-1
9e6d0f1d-f4cb-4d89-ab2d-613de64b7f3a,AiATrack: Attention in Attention for Transformer Visual Tracking,0.984408,"Transformer trackers have achieved impressive advancements recently, where
the attention mechanism plays an important role. However, the independent
correlation computation in the attention mechanism could result in noisy and
ambiguous attention weights, which inhibits further performance improvement. To
address this issue, we propose an attention in attention (AiA) module, which
enhances appropriate correlations and suppresses erroneous ones by seeking
consensus among all correlation vectors. Our AiA module can be readily applied
to both self-attention blocks and cross-attention blocks to facilitate feature
aggregation and information propagation for visual tracking. Moreover, we
propose a streamlined Transformer tracking framework, dubbed AiATrack, by
introducing efficient feature reuse and target-background embeddings to make
full use of temporal references. Experiments show that our tracker achieves
state-of-the-art performance on six tracking benchmarks while running at a
real-time speed.",https://github.com/Little-Podi/AiATrack,-1
145d7752-979a-491d-b717-65455ba12142,RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset for Natural Disaster Damage Assessment,0.063903,"Recent advancements in computer vision and deep learning techniques have
facilitated notable progress in scene understanding, thereby assisting rescue
teams in achieving precise damage assessment. In this paper, we present
RescueNet, a meticulously curated high-resolution post-disaster dataset that
includes detailed classification and semantic segmentation annotations. This
dataset aims to facilitate comprehensive scene understanding in the aftermath
of natural disasters. RescueNet comprises post-disaster images collected after
Hurricane Michael, obtained using Unmanned Aerial Vehicles (UAVs) from multiple
impacted regions. The uniqueness of RescueNet lies in its provision of
high-resolution post-disaster imagery, accompanied by comprehensive annotations
for each image. Unlike existing datasets that offer annotations limited to
specific scene elements such as buildings, RescueNet provides pixel-level
annotations for all classes, including buildings, roads, pools, trees, and
more. Furthermore, we evaluate the utility of the dataset by implementing
state-of-the-art segmentation models on RescueNet, demonstrating its value in
enhancing existing methodologies for natural disaster damage assessment.",https://github.com/BinaLab/RescueNet-A-High-Resolution-Post-Disaster-UAV-Dataset-for-Semantic-Segmentation/tree/main,1500
185abb0e-af35-4e46-9455-dc6f8c1ba0ed,Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models,0.691178,"Vision-Language Pre-training (VLP) models have achieved state-of-the-art
performance in numerous cross-modal tasks. Since they are optimized to capture
the statistical properties of intra- and inter-modality, there remains risk to
learn social biases presented in the data as well. In this work, we (1)
introduce a counterfactual-based bias measurement \emph{CounterBias} to
quantify the social bias in VLP models by comparing the [MASK]ed prediction
probabilities of factual and counterfactual samples; (2) construct a novel
VL-Bias dataset including 24K image-text pairs for measuring gender bias in VLP
models, from which we observed that significant gender bias is prevalent in VLP
models; and (3) propose a VLP debiasing method \emph{FairVLP} to minimize the
difference in the [MASK]ed prediction probabilities between factual and
counterfactual image-text pairs for VLP debiasing. Although CounterBias and
FairVLP focus on social bias, they are generalizable to serve as tools and
provide new insights to probe and regularize more knowledge in VLP models.",https://github.com/VL-Bias/VL-Bias,-1
a1e755d6-59ec-474a-8976-271ecbaccb10,Continuous Examination by Automatic Quiz Assessment Using Spiral Codes and Image Processing,0.135664,"We describe a technical solution implemented at Halmstad University to
automatise assessment and reporting of results of paper-based quiz exams. Paper
quizzes are affordable and within reach of campus education in classrooms.
Offering and taking them is accepted as they cause fewer issues with
reliability and democratic access, e.g. a large number of students can take
them without a trusted mobile device, internet, or battery. By contrast,
correction of the quiz is a considerable obstacle. We suggest mitigating the
issue by a novel image processing technique using harmonic spirals that aligns
answer sheets in sub-pixel accuracy to read student identity and answers and to
email results within minutes, all fully automatically. Using the described
method, we carry out regular weekly examinations in two master courses at the
mentioned centre without a significant workload increase. The employed solution
also enables us to assign a unique identifier to each quiz (e.g. week 1, week
2. . . ) while allowing us to have an individualised quiz for each student.",None,-1
bb16b301-8180-42f9-aeea-8616533f90cd,Making Your First Choice: To Address Cold Start Problem in Vision Active Learning,0.517252,"Active learning promises to improve annotation efficiency by iteratively
selecting the most important data to be annotated first. However, we uncover a
striking contradiction to this promise: active learning fails to select data as
efficiently as random selection at the first few choices. We identify this as
the cold start problem in vision active learning, caused by a biased and
outlier initial query. This paper seeks to address the cold start problem by
exploiting the three advantages of contrastive learning: (1) no annotation is
required; (2) label diversity is ensured by pseudo-labels to mitigate bias; (3)
typical data is determined by contrastive features to reduce outliers.
Experiments are conducted on CIFAR-10-LT and three medical imaging datasets
(i.e. Colon Pathology, Abdominal CT, and Blood Cell Microscope). Our initial
query not only significantly outperforms existing active querying strategies
but also surpasses random selection by a large margin. We foresee our solution
to the cold start problem as a simple yet strong baseline to choose the initial
query for vision active learning. Code is available:
https://github.com/c-liangyu/CSVAL",https://github.com/c-liangyu/CSVAL,-1
2af96e8d-02a9-4bd7-a856-cfe4c08fc47b,Few-shot Learning with Noisy Labels,0.898638,"Few-shot learning (FSL) methods typically assume clean support sets with
accurately labeled samples when training on novel classes. This assumption can
often be unrealistic: support sets, no matter how small, can still include
mislabeled samples. Robustness to label noise is therefore essential for FSL
methods to be practical, but this problem surprisingly remains largely
unexplored. To address mislabeled samples in FSL settings, we make several
technical contributions. (1) We offer simple, yet effective, feature
aggregation methods, improving the prototypes used by ProtoNet, a popular FSL
technique. (2) We describe a novel Transformer model for Noisy Few-Shot
Learning (TraNFS). TraNFS leverages a transformer's attention mechanism to
weigh mislabeled versus correct samples. (3) Finally, we extensively test these
methods on noisy versions of MiniImageNet and TieredImageNet. Our results show
that TraNFS is on-par with leading FSL methods on clean support sets, yet
outperforms them, by far, in the presence of label noise.",None,16626
145ed9c8-c2b5-48cd-9a25-a91bfb6b9bfe,CM3: A Causal Masked Multimodal Model of the Internet,0.989725,"We introduce CM3, a family of causally masked generative models trained over
a large corpus of structured multi-modal documents that can contain both text
and image tokens. Our new causally masked approach generates tokens left to
right while also masking out a small number of long token spans that are
generated at the end of the string, instead of their original positions. The
casual masking object provides a type of hybrid of the more common causal and
masked language models, by enabling full generative modeling while also
providing bidirectional context when generating the masked spans. We train
causally masked language-image models on large-scale web and Wikipedia
articles, where each document contains all of the text, hypertext markup,
hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they
appear in the original HTML source (before masking). The resulting CM3 models
can generate rich structured, multi-modal outputs while conditioning on
arbitrary masked document contexts, and thereby implicitly learn a wide range
of text, image, and cross modal tasks. They can be prompted to recover, in a
zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM.
We set the new state-of-the-art in zero-shot summarization, entity linking, and
entity disambiguation while maintaining competitive performance in the
fine-tuning setting. We can generate images unconditionally, conditioned on
text (like DALL-E) and do captioning all in a zero-shot setting with a single
model.",https://github.com/facebookresearch/GENRE,104111
8d0c9375-3841-4566-8afc-c37363288160,Multi-label Transformer for Action Unit Detection,0.483982,"Action Unit (AU) Detection is the branch of affective computing that aims at
recognizing unitary facial muscular movements. It is key to unlock unbiased
computational face representations and has therefore aroused great interest in
the past few years. One of the main obstacles toward building efficient deep
learning based AU detection system is the lack of wide facial image databases
annotated by AU experts. In that extent the ABAW challenge paves the way toward
better AU detection as it involves a 2M frames AU annotated dataset. In this
paper, we present our submission to the ABAW3 challenge. In a nutshell, we
applied a multi-label detection transformer that leverage multi-head attention
to learn which part of the face image is the most relevant to predict each AU.",None,-1
20c7c65e-ed7b-4a3f-8497-b2e545ec2465,SubER: A Metric for Automatic Evaluation of Subtitle Quality,0.0629825,"This paper addresses the problem of evaluating the quality of automatically
generated subtitles, which includes not only the quality of the
machine-transcribed or translated speech, but also the quality of line
segmentation and subtitle timing. We propose SubER - a single novel metric
based on edit distance with shifts that takes all of these subtitle properties
into account. We compare it to existing metrics for evaluating transcription,
translation, and subtitle quality. A careful human evaluation in a post-editing
scenario shows that the new metric has a high correlation with the post-editing
effort and direct human assessment scores, outperforming baseline metrics
considering only the subtitle text, such as WER and BLEU, and existing methods
to integrate segmentation and timing features.",https://github.com/apptek/SubER,162
7d71e67c-fde9-4e61-a9dc-03f33b601922,On Adversarial Robustness of Deep Image Deblurring,0.534022,"Recent approaches employ deep learning-based solutions for the recovery of a
sharp image from its blurry observation. This paper introduces adversarial
attacks against deep learning-based image deblurring methods and evaluates the
robustness of these neural networks to untargeted and targeted attacks. We
demonstrate that imperceptible distortion can significantly degrade the
performance of state-of-the-art deblurring networks, even producing drastically
different content in the output, indicating the strong need to include
adversarially robust training not only in classification but also for image
recovery.",None,-1
213328de-6f93-4994-aae1-f5f2179e4e4f,Perturbation Augmentation for Fairer NLP,0.892518,"Unwanted and often harmful social biases are becoming ever more salient in
NLP research, affecting both models and datasets. In this work, we ask whether
training on demographically perturbed data leads to fairer language models. We
collect a large dataset of human annotated text perturbations and train a
neural perturbation model, which we show outperforms heuristic alternatives. We
find that (i) language models (LMs) pre-trained on demographically perturbed
corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE
datasets exhibit less demographic bias on downstream tasks, and (iii) fairness
improvements do not come at the expense of performance on downstream tasks.
Lastly, we discuss outstanding questions about how best to evaluate the
(un)fairness of large language models. We hope that this exploration of neural
demographic perturbation will help drive more improvement towards fairer NLP.",https://github.com/facebookresearch/ParlAI,-1
647a9f1d-f494-4025-b2f9-7a6dae25ce84,Data Splits and Metrics for Method Benchmarking on Surgical Action Triplet Datasets,0.846434,"In addition to generating data and annotations, devising sensible data
splitting strategies and evaluation metrics is essential for the creation of a
benchmark dataset. This practice ensures consensus on the usage of the data,
homogeneous assessment, and uniform comparison of research methods on the
dataset. This study focuses on CholecT50, which is a 50 video surgical dataset
that formalizes surgical activities as triplets of <instrument, verb, target>.
In this paper, we introduce the standard splits for the CholecT50 and CholecT45
datasets and show how they compare with existing use of the dataset. CholecT45
is the first public release of 45 videos of CholecT50 dataset. We also develop
a metrics library, ivtmetrics, for model evaluation on surgical triplets.
Furthermore, we conduct a benchmark study by reproducing baseline methods in
the most predominantly used deep learning frameworks (PyTorch and TensorFlow)
to evaluate them using the proposed data splits and metrics and release them
publicly to support future research. The proposed data splits and evaluation
metrics will enable global tracking of research progress on the dataset and
facilitate optimal model selection for further deployment.",https://github.com/CAMMA-public,-1
7d882d11-f093-4558-843c-ad6154619724,DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following,0.856209,"Language-guided Embodied AI benchmarks requiring an agent to navigate an
environment and manipulate objects typically allow one-way communication: the
human user gives a natural language command to the agent, and the agent can
only follow the command passively. We present DialFRED, a dialogue-enabled
embodied instruction following benchmark based on the ALFRED benchmark.
DialFRED allows an agent to actively ask questions to the human user; the
additional information in the user's response is used by the agent to better
complete its task. We release a human-annotated dataset with 53K task-relevant
questions and answers and an oracle to answer questions. To solve DialFRED, we
propose a questioner-performer framework wherein the questioner is pre-trained
with the human-annotated data and fine-tuned with reinforcement learning. We
make DialFRED publicly available and encourage researchers to propose and
evaluate their solutions to building dialog-enabled embodied agents.",https://github.com/xfgao/DialFRED,-1
a138c87b-473e-4d28-91ab-f2af45431f33,Streaming Adaptive Submodular Maximization,0.767535,"Many sequential decision making problems can be formulated as an adaptive
submodular maximization problem. However, most of existing studies in this
field focus on pool-based setting, where one can pick items in any order, and
there have been few studies for the stream-based setting where items arrive in
an arbitrary order and one must immediately decide whether to select an item or
not upon its arrival. In this paper, we introduce a new class of utility
functions, semi-policywise submodular functions. We develop a series of
effective algorithms to maximize a semi-policywise submodular function under
the stream-based setting.",None,-1
82c15f3e-17f4-4b0d-b47c-b46c307e8fe6,ROAD-R: The Autonomous Driving Dataset with Logical Requirements,0.807808,"Neural networks have proven to be very powerful at computer vision tasks.
However, they often exhibit unexpected behaviours, violating known requirements
expressing background knowledge. This calls for models (i) able to learn from
the requirements, and (ii) guaranteed to be compliant with the requirements
themselves. Unfortunately, the development of such models is hampered by the
lack of datasets equipped with formally specified requirements. In this paper,
we introduce the ROad event Awareness Dataset with logical Requirements
(ROAD-R), the first publicly available dataset for autonomous driving with
requirements expressed as logical constraints. Given ROAD-R, we show that
current state-of-the-art models often violate its logical constraints, and that
it is possible to exploit them to create models that (i) have a better
performance, and (ii) are guaranteed to be compliant with the requirements
themselves.",https://github.com/gurkirt/road-dataset,-1
9234bd3b-5c64-4c2c-ac05-e030420274a6,Language-free Training for Zero-shot Video Grounding,0.22102,"Given an untrimmed video and a language query depicting a specific temporal
moment in the video, video grounding aims to localize the time interval by
understanding the text and video simultaneously. One of the most challenging
issues is an extremely time- and cost-consuming annotation collection,
including video captions in a natural language form and their corresponding
temporal regions. In this paper, we present a simple yet novel training
framework for video grounding in the zero-shot setting, which learns a network
with only video data without any annotation. Inspired by the recent
language-free paradigm, i.e. training without language data, we train the
network without compelling the generation of fake (pseudo) text queries into a
natural language form. Specifically, we propose a method for learning a video
grounding model by selecting a temporal interval as a hypothetical correct
answer and considering the visual feature selected by our method in the
interval as a language feature, with the help of the well-aligned
visual-language space of CLIP. Extensive experiments demonstrate the prominence
of our language-free training framework, outperforming the existing zero-shot
video grounding method and even several weakly-supervised approaches with large
margins on two standard datasets.",None,-1
b86818f4-6847-4045-9ba5-2891599b4162,MonoGround: Detecting Monocular 3D Objects from the Ground,0.696527,"Monocular 3D object detection has attracted great attention for its
advantages in simplicity and cost. Due to the ill-posed 2D to 3D mapping
essence from the monocular imaging process, monocular 3D object detection
suffers from inaccurate depth estimation and thus has poor 3D detection
results. To alleviate this problem, we propose to introduce the ground plane as
a prior in the monocular 3d object detection. The ground plane prior serves as
an additional geometric condition to the ill-posed mapping and an extra source
in depth estimation. In this way, we can get a more accurate depth estimation
from the ground. Meanwhile, to take full advantage of the ground plane prior,
we propose a depth-align training strategy and a precise two-stage depth
inference method tailored for the ground plane prior. It is worth noting that
the introduced ground plane prior requires no extra data sources like LiDAR,
stereo images, and depth information. Extensive experiments on the KITTI
benchmark show that our method could achieve state-of-the-art results compared
with other methods while maintaining a very fast speed. Our code and models are
available at https://github.com/cfzd/MonoGround.",https://github.com/cfzd/MonoGround,-1
44ed842d-105f-4c39-9225-5f150e7388ed,Can GAN-induced Attribute Manipulations Impact Face Recognition?,0.0978514,"Impact due to demographic factors such as age, sex, race, etc., has been
studied extensively in automated face recognition systems. However, the impact
of \textit{digitally modified} demographic and facial attributes on face
recognition is relatively under-explored. In this work, we study the effect of
attribute manipulations induced via generative adversarial networks (GANs) on
face recognition performance. We conduct experiments on the CelebA dataset by
intentionally modifying thirteen attributes using AttGAN and STGAN and
evaluating their impact on two deep learning-based face verification methods,
ArcFace and VGGFace. Our findings indicate that some attribute manipulations
involving eyeglasses and digital alteration of sex cues can significantly
impair face recognition by up to 73% and need further analysis.",None,-1
d09d5567-c4df-4020-ab59-5a981dc49d7d,Introduction to Soar,0.0822696,"This paper is the recommended initial reading for a functional overview of
Soar, version 9.6. It includes an abstract overview of the architectural
structure of Soar including its processing, memories, learning modules, their
interfaces, and the representations of knowledge used by those modules. From
there it describes the processing supported by those modules, including
decision making, impasses and substates, procedure learning via chunking,
reinforcement learning, semantic memory, episodic memory, and spatial-visual
reasoning. It then reviews the levels of decision making and variety of
learning in Soar, and analysis of Soar as an architecture supporting general
human-level AI. Following the references is an appendix that contains short
descriptions of recent Soar agents and a glossary of the terminology we use in
describing Soar.",https://soar.eecs.umich.edu/,-1
000dfd3e-6cda-474a-92e9-2e15590098a2,Automatic Evaluation and Analysis of Idioms in Neural Machine Translation,0.192132,"A major open problem in neural machine translation (NMT) is the translation
of idiomatic expressions, such as ""under the weather"". The meaning of these
expressions is not composed by the meaning of their constituent words, and NMT
models tend to translate them literally (i.e., word-by-word), which leads to
confusing and nonsensical translations. Research on idioms in NMT is limited
and obstructed by the absence of automatic methods for quantifying these
errors. In this work, first, we propose a novel metric for automatically
measuring the frequency of literal translation errors without human
involvement. Equipped with this metric, we present controlled translation
experiments with models trained in different conditions (with/without the
test-set idioms) and across a wide range of (global and targeted) metrics and
test sets. We explore the role of monolingual pretraining and find that it
yields substantial targeted improvements, even without observing any
translation examples of the test-set idioms. In our analysis, we probe the role
of idiom context. We find that the randomly initialized models are more local
or ""myopic"" as they are relatively unaffected by variations of the idiom
context, unlike the pretrained ones.",https://github.com/amazon-research/idiom-mt,1321
38594c2e-11d0-4564-8258-3638fb6d1041,Large Language Models are Zero-Shot Reasoners,1.0,"Pretrained large language models (LLMs) are widely used in many sub-fields of
natural language processing (NLP) and generally known as excellent few-shot
learners with task-specific exemplars. Notably, chain of thought (CoT)
prompting, a recent technique for eliciting complex multi-step reasoning
through step-by-step answer examples, achieved the state-of-the-art
performances in arithmetics and symbolic reasoning, difficult system-2 tasks
that do not follow the standard scaling laws for LLMs. While these successes
are often attributed to LLMs' ability for few-shot learning, we show that LLMs
are decent zero-shot reasoners by simply adding ""Let's think step by step""
before each answer. Experimental results demonstrate that our Zero-shot-CoT,
using the same single prompt template, significantly outperforms zero-shot LLM
performances on diverse benchmark reasoning tasks including arithmetics
(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin
Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled
Objects), without any hand-crafted few-shot examples, e.g. increasing the
accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with
large InstructGPT model (text-davinci-002), as well as similar magnitudes of
improvements with another off-the-shelf large model, 540B parameter PaLM. The
versatility of this single prompt across very diverse reasoning tasks hints at
untapped and understudied fundamental zero-shot capabilities of LLMs,
suggesting high-level, multi-task broad cognitive capabilities may be extracted
by simple prompting. We hope our work not only serves as the minimal strongest
zero-shot baseline for the challenging reasoning benchmarks, but also
highlights the importance of carefully exploring and analyzing the enormous
zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or
few-shot exemplars.",None,-1
4e49f842-9a68-4077-8358-aa4fe4f230e5,MGTR: End-to-End Mutual Gaze Detection with Transformer,0.603583,"People's looking at each other or mutual gaze is ubiquitous in our daily
interactions, and detecting mutual gaze is of great significance for
understanding human social scenes. Current mutual gaze detection methods focus
on two-stage methods, whose inference speed is limited by the two-stage
pipeline and the performance in the second stage is affected by the first one.
In this paper, we propose a novel one-stage mutual gaze detection framework
called Mutual Gaze TRansformer or MGTR to perform mutual gaze detection in an
end-to-end manner. By designing mutual gaze instance triples, MGTR can detect
each human head bounding box and simultaneously infer mutual gaze relationship
based on global image information, which streamlines the whole process with
simplicity. Experimental results on two mutual gaze datasets show that our
method is able to accelerate mutual gaze detection process without losing
performance. Ablation study shows that different components of MGTR can capture
different levels of semantic information in images. Code is available at
https://github.com/Gmbition/MGTR",https://github.com/Gmbition/MGTR,-1
a5806ab7-3950-400e-9bec-3d56baf41133,The complexity of unsupervised learning of lexicographic preferences,0.0725044,"This paper considers the task of learning users' preferences on a
combinatorial set of alternatives, as generally used by online configurators,
for example. In many settings, only a set of selected alternatives during past
interactions is available to the learner. Fargier et al. [2018] propose an
approach to learn, in such a setting, a model of the users' preferences that
ranks previously chosen alternatives as high as possible; and an algorithm to
learn, in this setting, a particular model of preferences: lexicographic
preferences trees (LP-trees). In this paper, we study complexity-theoretical
problems related to this approach. We give an upper bound on the sample
complexity of learning an LP-tree, which is logarithmic in the number of
attributes. We also prove that computing the LP tree that minimises the
empirical risk can be done in polynomial time when restricted to the class of
linear LP-trees.",None,-1
d1ba7d5b-0220-4c4b-bf46-ab6747e379b5,Towards ML Methods for Biodiversity: A Novel Wild Bee Dataset and Evaluations of XAI Methods for ML-Assisted Rare Species Annotations,0.0925383,"Insects are a crucial part of our ecosystem. Sadly, in the past few decades,
their numbers have worryingly decreased. In an attempt to gain a better
understanding of this process and monitor the insects populations, Deep
Learning may offer viable solutions. However, given the breadth of their
taxonomy and the typical hurdles of fine grained analysis, such as high
intraclass variability compared to low interclass variability, insect
classification remains a challenging task. There are few benchmark datasets,
which impedes rapid development of better AI models. The annotation of rare
species training data, however, requires expert knowledge. Explainable
Artificial Intelligence (XAI) could assist biologists in these annotation
tasks, but choosing the optimal XAI method is difficult. Our contribution to
these research challenges is threefold: 1) a dataset of thoroughly annotated
images of wild bees sampled from the iNaturalist database, 2) a ResNet model
trained on the wild bee dataset achieving classification scores comparable to
similar state-of-the-art models trained on other fine-grained datasets and 3)
an investigation of XAI methods to support biologists in annotation tasks.",None,-1
dd58f012-c842-41ce-9bf3-9a3cfa741292,Provable Defense against Backdoor Policies in Reinforcement Learning,0.820057,"We propose a provable defense mechanism against backdoor policies in
reinforcement learning under subspace trigger assumption. A backdoor policy is
a security threat where an adversary publishes a seemingly well-behaved policy
which in fact allows hidden triggers. During deployment, the adversary can
modify observed states in a particular way to trigger unexpected actions and
harm the agent. We assume the agent does not have the resources to re-train a
good policy. Instead, our defense mechanism sanitizes the backdoor policy by
projecting observed states to a 'safe subspace', estimated from a small number
of interactions with a clean (non-triggered) environment. Our sanitized policy
achieves $\epsilon$ approximate optimality in the presence of triggers,
provided the number of clean interactions is $O\left(\frac{D}{(1-\gamma)^4
\epsilon^2}\right)$ where $\gamma$ is the discounting factor and $D$ is the
dimension of state space. Empirically, we show that our sanitization defense
performs well on two Atari game environments.",https://github.com/skbharti/Provable-Defense-in-RL,-1
6bcb97d9-eeb6-4ec0-9927-a8dada3f46e8,S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers,0.656669,"The task of converting a natural language question into an executable SQL
query, known as text-to-SQL, is an important branch of semantic parsing. The
state-of-the-art graph-based encoder has been successfully used in this task
but does not model the question syntax well. In this paper, we propose
S$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL
parsers, which effectively leverages the syntactic dependency information of
questions in text-to-SQL to improve the performance. We also employ the
decoupling constraint to induce diverse relational edge embedding, which
further improves the network's performance. Experiments on the Spider and
robustness setting Spider-Syn demonstrate that the proposed approach
outperforms all existing methods when pre-training models are used, resulting
in a performance ranks first on the Spider leaderboard.",None,-1
010c6706-ae0f-46c5-8632-1a55c4d81ddc,Atypical lexical abbreviations identification in Russian medical texts,0.294335,"Abbreviation is a method of word formation that aims to construct the
shortened term from the first letters of the initial phrase. Implicit
abbreviations frequently cause the comprehension difficulties for unprepared
readers. In this paper, we propose an efficient ML-based algorithm which allows
to identify the abbreviations in Russian texts. The method achieves ROC AUC
score 0.926 and F1 score 0.706 which are confirmed as competitive in comparison
with the baselines. Along with the pipeline, we also establish first to our
knowledge Russian dataset that is relevant for the desired task.",https://github.com/aberdichevskaya/abbreviation-identification,-1
a2db1e46-ac2c-4bfb-9ac2-925a9da89b3e,Towards Privacy-Preserving Person Re-identification via Person Identify Shift,0.505474,"Recently privacy concerns of person re-identification (ReID) raise more and
more attention and preserving the privacy of the pedestrian images used by ReID
methods become essential. De-identification (DeID) methods alleviate privacy
issues by removing the identity-related of the ReID data. However, most of the
existing DeID methods tend to remove all personal identity-related information
and compromise the usability of de-identified data on the ReID task. In this
paper, we aim to develop a technique that can achieve a good trade-off between
privacy protection and data usability for person ReID. To achieve this, we
propose a novel de-identification method designed explicitly for person ReID,
named Person Identify Shift (PIS). PIS removes the absolute identity in a
pedestrian image while preserving the identity relationship between image
pairs. By exploiting the interpolation property of variational auto-encoder,
PIS shifts each pedestrian image from the current identity to another with a
new identity, resulting in images still preserving the relative identities.
Experimental results show that our method has a better trade-off between
privacy-preserving and model performance than existing de-identification
methods and can defend against human and model attacks for data privacy.",None,-1
8cb0264b-c445-4997-a88b-4c3d9acbe798,CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR Segmentation,0.92621,"3D LiDAR semantic segmentation is fundamental for autonomous driving. Several
Unsupervised Domain Adaptation (UDA) methods for point cloud data have been
recently proposed to improve model generalization for different sensors and
environments. Researchers working on UDA problems in the image domain have
shown that sample mixing can mitigate domain shift. We propose a new approach
of sample mixing for point cloud UDA, namely Compositional Semantic Mix
(CoSMix), the first UDA approach for point cloud segmentation based on sample
mixing. CoSMix consists of a two-branch symmetric network that can process
labelled synthetic data (source) and real-world unlabelled point clouds
(target) concurrently. Each branch operates on one domain by mixing selected
pieces of data from the other one, and by using the semantic information
derived from source labels and target pseudo-labels. We evaluate CoSMix on two
large-scale datasets, showing that it outperforms state-of-the-art methods by a
large margin. Our code is available at
https://github.com/saltoricristiano/cosmix-uda.",https://github.com/saltoricristiano/cosmix-uda,-1
9474b5e4-0307-44e0-a496-46bf8b154982,General Adversarial Defense Against Black-box Attacks via Pixel Level and Feature Level Distribution Alignments,0.197538,"Deep Neural Networks (DNNs) are vulnerable to the black-box adversarial
attack that is highly transferable. This threat comes from the distribution gap
between adversarial and clean samples in feature space of the target DNNs. In
this paper, we use Deep Generative Networks (DGNs) with a novel training
mechanism to eliminate the distribution gap. The trained DGNs align the
distribution of adversarial samples with clean ones for the target DNNs by
translating pixel values. Different from previous work, we propose a more
effective pixel level training constraint to make this achievable, thus
enhancing robustness on adversarial samples. Further, a class-aware
feature-level constraint is formulated for integrated distribution alignment.
Our approach is general and applicable to multiple tasks, including image
classification, semantic segmentation, and object detection. We conduct
extensive experiments on different datasets. Our strategy demonstrates its
unique effectiveness and generality against black-box attacks.",https://github.com/hszhao/semseg,-1
fca01e29-4788-46ff-b5ba-256b06611a27,Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion,0.334307,"Multilingual speech recognition has drawn significant attention as an
effective way to compensate data scarcity for low-resource languages.
End-to-end (e2e) modelling is preferred over conventional hybrid systems,
mainly because of no lexicon requirement. However, hybrid DNN-HMMs still
outperform e2e models in limited data scenarios. Furthermore, the problem of
manual lexicon creation has been alleviated by publicly available trained
models of grapheme-to-phoneme (G2P) and text to IPA transliteration for a lot
of languages. In this paper, a novel approach of hybrid DNN-HMM acoustic models
fusion is proposed in a multilingual setup for the low-resource languages.
Posterior distributions from different monolingual acoustic models, against a
target language speech signal, are fused together. A separate regression neural
network is trained for each source-target language pair to transform posteriors
from source acoustic model to the target language. These networks require very
limited data as compared to the ASR training. Posterior fusion yields a
relative gain of 14.65% and 6.5% when compared with multilingual and
monolingual baselines respectively. Cross-lingual model fusion shows that the
comparable results can be achieved without using posteriors from the language
dependent ASR.",None,-1
615d42f7-d108-4ad1-841e-8c4b3c612147,Semantic Image Synthesis via Diffusion Models,0.905988,"Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable
success in various image generation tasks compared with Generative Adversarial
Nets (GANs). Recent work on semantic image synthesis mainly follows the
\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality
or diversity of generated images. In this paper, we propose a novel framework
based on DDPM for semantic image synthesis. Unlike previous conditional
diffusion model directly feeds the semantic layout and noisy image as input to
a U-Net structure, which may not fully leverage the information in the input
semantic mask, our framework processes semantic layout and noisy image
differently. It feeds noisy image to the encoder of the U-Net structure while
the semantic layout to the decoder by multi-layer spatially-adaptive
normalization operators. To further improve the generation quality and semantic
interpretability in semantic image synthesis, we introduce the classifier-free
guidance sampling strategy, which acknowledge the scores of an unconditional
model for sampling process. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our proposed method, achieving
state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS).",https://github.com/WeilunWang/semantic-diffusion-model,-1
a16006f7-b70f-47a7-ab55-3a578751be47,Utilizing Class Separation Distance for the Evaluation of Corruption Robustness of Machine Learning Classifiers,0.0594615,"Robustness is a fundamental pillar of Machine Learning (ML) classifiers,
substantially determining their reliability. Methods for assessing classifier
robustness are therefore essential. In this work, we address the challenge of
evaluating corruption robustness in a way that allows comparability and
interpretability on a given dataset. We propose a test data augmentation method
that uses a robustness distance $\epsilon$ derived from the datasets minimal
class separation distance. The resulting MSCR (mean statistical corruption
robustness) metric allows a dataset-specific comparison of different
classifiers with respect to their corruption robustness. The MSCR value is
interpretable, as it represents the classifiers avoidable loss of accuracy due
to statistical corruptions. On 2D and image data, we show that the metric
reflects different levels of classifier robustness. Furthermore, we observe
unexpected optima in classifiers robust accuracy through training and testing
classifiers with different levels of noise. While researchers have frequently
reported on a significant tradeoff on accuracy when training robust models, we
strengthen the view that a tradeoff between accuracy and corruption robustness
is not inherent. Our results indicate that robustness training through simple
data augmentation can already slightly improve accuracy.",https://github.com/georgsiedel/,-1
ba50e50a-0373-4a60-90d3-933903de6c1a,ImLiDAR: Cross-Sensor Dynamic Message Propagation Network for 3D Object Detection,0.248871,"LiDAR and camera, as two different sensors, supply geometric (point clouds)
and semantic (RGB images) information of 3D scenes. However, it is still
challenging for existing methods to fuse data from the two cross sensors,
making them complementary for quality 3D object detection (3OD). We propose
ImLiDAR, a new 3OD paradigm to narrow the cross-sensor discrepancies by
progressively fusing the multi-scale features of camera Images and LiDAR point
clouds. ImLiDAR enables to provide the detection head with cross-sensor yet
robustly fused features. To achieve this, two core designs exist in ImLiDAR.
First, we propose a cross-sensor dynamic message propagation module to combine
the best of the multi-scale image and point features. Second, we raise a direct
set prediction problem that allows designing an effective set-based detector to
tackle the inconsistency of the classification and localization confidences,
and the sensitivity of hand-tuned hyperparameters. Besides, the novel set-based
detector can be detachable and easily integrated into various detection
networks. Comparisons on both the KITTI and SUN-RGBD datasets show clear visual
and numerical improvements of our ImLiDAR over twenty-three state-of-the-art
3OD methods.",None,-1
8bebd1b4-aa9d-485e-9958-1a62e6d4acbd,Physics-Constrained Backdoor Attacks on Power System Fault Localization,0.270787,"The advances in deep learning (DL) techniques have the potential to deliver
transformative technological breakthroughs to numerous complex tasks in modern
power systems that suffer from increasing uncertainty and nonlinearity.
However, the vulnerability of DL has yet to be thoroughly explored in power
system tasks under various physical constraints. This work, for the first time,
proposes a novel physics-constrained backdoor poisoning attack, which embeds
the undetectable attack signal into the learned model and only performs the
attack when it encounters the corresponding signal. The paper illustrates the
proposed attack on the real-time fault line localization application.
Furthermore, the simulation results on the 68-bus power system demonstrate that
DL-based fault line localization methods are not robust to our proposed attack,
indicating that backdoor poisoning attacks pose real threats to DL
implementations in power systems. The proposed attack pipeline can be easily
generalized to other power system tasks.",None,-1
62d223f5-f13f-46d7-a6b4-914f1b5f08a6,Why-So-Deep: Towards Boosting Previously Trained Models for Visual Place Recognition,0.0934448,"Deep learning-based image retrieval techniques for the loop closure detection
demonstrate satisfactory performance. However, it is still challenging to
achieve high-level performance based on previously trained models in different
geographical regions. This paper addresses the problem of their deployment with
simultaneous localization and mapping (SLAM) systems in the new environment.
The general baseline approach uses additional information, such as GPS,
sequential keyframes tracking, and re-training the whole environment to enhance
the recall rate. We propose a novel approach for improving image retrieval
based on previously trained models. We present an intelligent method, MAQBOOL,
to amplify the power of pre-trained models for better image recall and its
application to real-time multiagent SLAM systems. We achieve comparable image
retrieval results at a low descriptor dimension (512-D), compared to the high
descriptor dimension (4096-D) of state-of-the-art methods. We use spatial
information to improve the recall rate in image retrieval on pre-trained
models.",None,-1
a9b1dcf2-7140-4827-a28d-e5691eeeca01,Revisiting Discrete Soft Actor-Critic,0.815925,"We study the adaption of soft actor-critic (SAC) from continuous action space
to discrete action space. We revisit vanilla SAC and provide an in-depth
understanding of its Q value underestimation and performance instability issues
when applied to discrete settings. We thereby propose entropy-penalty and
double average Q-learning with Q-clip to address these issues. Extensive
experiments on typical benchmarks with discrete action space, including Atari
games and a large-scale MOBA game, show the efficacy of our proposed method.
Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.",https://github.com/coldsummerday/Revisiting-Discrete-SAC,-1
46ca2503-0e55-4b34-9a5e-5ffdcf56d59b,A New Approach to Improve Learning-based Deepfake Detection in Realistic Conditions,0.0590308,"Deep convolutional neural networks have achieved exceptional results on
multiple detection and recognition tasks. However, the performance of such
detectors are often evaluated in public benchmarks under constrained and
non-realistic situations. The impact of conventional distortions and processing
operations found in imaging workflows such as compression, noise, and
enhancement are not sufficiently studied. Currently, only a few researches have
been done to improve the detector robustness to unseen perturbations. This
paper proposes a more effective data augmentation scheme based on real-world
image degradation process. This novel technique is deployed for deepfake
detection tasks and has been evaluated by a more realistic assessment
framework. Extensive experiments show that the proposed data augmentation
scheme improves generalization ability to unpredictable data distortions and
unseen datasets.",None,-1
5f810fec-2ad0-4b1f-a41d-9f2600a8f69f,Open-Vocabulary Universal Image Segmentation with MaskCLIP,0.419424,"In this paper, we tackle an emerging computer vision task, open-vocabulary
universal image segmentation, that aims to perform semantic/instance/panoptic
segmentation (background semantic labeling + foreground instance segmentation)
for arbitrary categories of text-based descriptions in inference time. We first
build a baseline method by directly adopting pre-trained CLIP models without
finetuning or distillation. We then develop MaskCLIP, a Transformer-based
approach with a MaskCLIP Visual Encoder, which is an encoder-only module that
seamlessly integrates mask tokens with a pre-trained ViT CLIP model for
semantic/instance segmentation and class prediction. MaskCLIP learns to
efficiently and effectively utilize pre-trained partial/dense CLIP features
within the MaskCLIP Visual Encoder that avoids the time-consuming
student-teacher training process. MaskCLIP outperforms previous methods for
semantic/instance/panoptic segmentation on ADE20K and PASCAL datasets. We show
qualitative illustrations for MaskCLIP with online custom categories. Project
website: https://maskclip.github.io.",https://maskclip.github.io,-1
fb0872d4-d914-4012-82be-da556b0e41a0,py-irt: A Scalable Item Response Theory Library for Python,0.688516,"py-irt is a Python library for fitting Bayesian Item Response Theory (IRT)
models. py-irt estimates latent traits of subjects and items, making it
appropriate for use in IRT tasks as well as ideal-point models. py-irt is built
on top of the Pyro and PyTorch frameworks and uses GPU-accelerated training to
scale to large data sets. Code, documentation, and examples can be found at
https://github.com/nd-ball/py-irt. py-irt can be installed from the GitHub page
or the Python Package Index (PyPI).",https://github.com/nd-ball/py-irt,-1
d1c781a8-bc94-490c-91e4-16d4de7c8c87,A Deep Neural Network for Multiclass Bridge Element Parsing in Inspection Image Analysis,0.4307,"Aerial robots such as drones have been leveraged to perform bridge
inspections. Inspection images with both recognizable structural elements and
apparent surface defects can be collected by onboard cameras to provide
valuable information for the condition assessment. This article aims to
determine a suitable deep neural network (DNN) for parsing multiclass bridge
elements in inspection images. An extensive set of quantitative evaluations
along with qualitative examples show that High-Resolution Net (HRNet) possesses
the desired ability. With data augmentation and a training sample of 130
images, a pre-trained HRNet is efficiently transferred to the task of
structural element parsing and has achieved a 92.67% mean F1-score and 86.33%
mean IoU.",None,-1
8d651294-1ee4-4977-8adf-daf2a9a2df11,Concrete Score Matching: Generalized Score Matching for Discrete Data,0.95781,"Representing probability distributions by the gradient of their density
functions has proven effective in modeling a wide range of continuous data
modalities. However, this representation is not applicable in discrete domains
where the gradient is undefined. To this end, we propose an analogous score
function called the ""Concrete score"", a generalization of the (Stein) score for
discrete settings. Given a predefined neighborhood structure, the Concrete
score of any input is defined by the rate of change of the probabilities with
respect to local directional changes of the input. This formulation allows us
to recover the (Stein) score in continuous domains when measuring such changes
by the Euclidean distance, while using the Manhattan distance leads to our
novel score function in discrete domains. Finally, we introduce a new framework
to learn such scores from samples called Concrete Score Matching (CSM), and
propose an efficient training objective to scale our approach to high
dimensions. Empirically, we demonstrate the efficacy of CSM on density
estimation tasks on a mixture of synthetic, tabular, and high-dimensional image
datasets, and demonstrate that it performs favorably relative to existing
baselines for modeling discrete data.",None,-1
ceb1e6f9-3120-46c1-9b22-6fb53c5804de,Using Ontologies for the Formalization and Recognition of Criticality for Automated Driving,0.792145,"Knowledge representation and reasoning has a long history of examining how
knowledge can be formalized, interpreted, and semantically analyzed by
machines. In the area of automated vehicles, recent advances suggest the
ability to formalize and leverage relevant knowledge as a key enabler in
handling the inherently open and complex context of the traffic world. This
paper demonstrates ontologies to be a powerful tool for a) modeling and
formalization of and b) reasoning about factors associated with criticality in
the environment of automated vehicles. For this, we leverage the well-known
6-Layer Model to create a formal representation of the environmental context.
Within this representation, an ontology models domain knowledge as logical
axioms, enabling deduction on the presence of critical factors within traffic
scenes and scenarios. For executing automated analyses, a joint description
logic and rule reasoner is used in combination with an a-priori predicate
augmentation. We elaborate on the modular approach, present a publicly
available implementation, and evaluate the method by means of a large-scale
drone data set of urban traffic scenarios.",None,-1
5f5a6108-7f14-4735-89f2-8e59d6091370,Co-evolving morphology and control of soft robots using a single genome,0.793808,"When simulating soft robots, both their morphology and their controllers play
important roles in task performance. This paper introduces a new method to
co-evolve these two components in the same process. We do that by using the
hyperNEAT algorithm to generate two separate neural networks in one pass, one
responsible for the design of the robot body structure and the other for the
control of the robot.
  The key difference between our method and most existing approaches is that it
does not treat the development of the morphology and the controller as separate
processes. Similar to nature, our method derives both the ""brain"" and the
""body"" of an agent from a single genome and develops them together. While our
approach is more realistic and doesn't require an arbitrary separation of
processes during evolution, it also makes the problem more complex because the
search space for this single genome becomes larger and any mutation to the
genome affects ""brain"" and the ""body"" at the same time.
  Additionally, we present a new speciation function that takes into
consideration both the genotypic distance, as is the standard for NEAT, and the
similarity between robot bodies. By using this function, agents with very
different bodies are more likely to be in different species, this allows robots
with different morphologies to have more specialized controllers since they
won't crossover with other robots that are too different from them.
  We evaluate the presented methods on four tasks and observe that even if the
search space was larger, having a single genome makes the evolution process
converge faster when compared to having separated genomes for body and control.
The agents in our population also show morphologies with a high degree of
regularity and controllers capable of coordinating the voxels to produce the
necessary movements.",https://github.com/fhtanaka/SGR,-1
26357719-a5f1-4da0-88b9-02a83648951b,On The Robustness of Offensive Language Classifiers,0.354832,"Social media platforms are deploying machine learning based offensive
language classification systems to combat hateful, racist, and other forms of
offensive speech at scale. However, despite their real-world deployment, we do
not yet comprehensively understand the extent to which offensive language
classifiers are robust against adversarial attacks. Prior work in this space is
limited to studying robustness of offensive language classifiers against
primitive attacks such as misspellings and extraneous spaces. To address this
gap, we systematically analyze the robustness of state-of-the-art offensive
language classifiers against more crafty adversarial attacks that leverage
greedy- and attention-based word selection and context-aware embeddings for
word replacement. Our results on multiple datasets show that these crafty
adversarial attacks can degrade the accuracy of offensive language classifiers
by more than 50% while also being able to preserve the readability and meaning
of the modified text.",https://github.com/JonRusert/RobustnessOfOffensiveClassiers,-1
3c237491-8413-4514-a07b-2103e410bd0c,Optimal estimation of Gaussian DAG models,0.761446,"We study the optimal sample complexity of learning a Gaussian directed
acyclic graph (DAG) from observational data. Our main results establish the
minimax optimal sample complexity for learning the structure of a linear
Gaussian DAG model in two settings of interest: 1) Under equal variances
without knowledge of the true ordering, and 2) For general linear models given
knowledge of the ordering. In both cases the sample complexity is $n\asymp
q\log(d/q)$, where $q$ is the maximum number of parents and $d$ is the number
of nodes. We further make comparisons with the classical problem of learning
(undirected) Gaussian graphical models, showing that under the equal variance
assumption, these two problems share the same optimal sample complexity. In
other words, at least for Gaussian models with equal error variances, learning
a directed graphical model is statistically no more difficult than learning an
undirected graphical model. Our results also extend to more general
identification assumptions as well as subgaussian errors.",https://github.com/WY-Chen/EqVarDAG/blob/master/R/EqVarDAG_HD_TD.R,-1
ce2f3324-c579-426b-8757-f3781ba79cce,Learning to See Through with Events,0.590407,"Although synthetic aperture imaging (SAI) can achieve the seeing-through
effect by blurring out off-focus foreground occlusions while recovering
in-focus occluded scenes from multi-view images, its performance is often
deteriorated by dense occlusions and extreme lighting conditions. To address
the problem, this paper presents an Event-based SAI (E-SAI) method by relying
on the asynchronous events with extremely low latency and high dynamic range
acquired by an event camera. Specifically, the collected events are first
refocused by a Refocus-Net module to align in-focus events while scattering out
off-focus ones. Following that, a hybrid network composed of spiking neural
networks (SNNs) and convolutional neural networks (CNNs) is proposed to encode
the spatio-temporal information from the refocused events and reconstruct a
visual image of the occluded targets. Extensive experiments demonstrate that
our proposed E-SAI method can achieve remarkable performance in dealing with
very dense occlusions and extreme lighting conditions and produce high-quality
images from pure events. Codes and datasets are available at
https://dvs-whu.cn/projects/esai/.",https://github.com/YingqianWang/DeOccNet,-1
5240a4e4-5c16-41c8-af49-3abfff13f21d,A Fundamental Probabilistic Fuzzy Logic Framework Suitable for Causal Reasoning,0.111684,"In this paper, we introduce a fundamental framework to create a bridge
between Probability Theory and Fuzzy Logic. Indeed, our theory formulates a
random experiment of selecting crisp elements with the criterion of having a
certain fuzzy attribute. To do so, we associate some specific crisp random
variables to the random experiment. Then, several formulas are presented, which
make it easier to compute different conditional probabilities and expected
values of these random variables. Also, we provide measure theoretical basis
for our probabilistic fuzzy logic framework. Note that in our theory, the
probability density functions of continuous distributions which come from the
aforementioned random variables include the Dirac delta function as a term.
Further, we introduce an application of our theory in Causal Inference.",None,-1
4dfb9458-7f94-4134-842f-31b8dc6a85be,Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation,0.300457,"Non-autoregressive models achieve significant decoding speedup in neural
machine translation but lack the ability to capture sequential dependency.
Directed Acyclic Transformer (DA-Transformer) was recently proposed to model
sequential dependency with a directed acyclic graph. Consequently, it has to
apply a sequential decision process at inference time, which harms the global
translation accuracy. In this paper, we present a Viterbi decoding framework
for DA-Transformer, which guarantees to find the joint optimal solution for the
translation and decoding path under any length constraint. Experimental results
demonstrate that our approach consistently improves the performance of
DA-Transformer while maintaining a similar decoding speedup.",https://github.com/thu-coai/DA-Transformer,-1
75388502-de18-4d96-ad47-686a85ef2a41,Research on Dual Channel News Headline Classification Based on ERNIE Pre-training Model,0.492335,"The classification of news headlines is an important direction in the field
of NLP, and its data has the characteristics of compactness, uniqueness and
various forms. Aiming at the problem that the traditional neural network model
cannot adequately capture the underlying feature information of the data and
cannot jointly extract key global features and deep local features, a
dual-channel network model DC-EBAD based on the ERNIE pre-training model is
proposed. Use ERNIE to extract the lexical, semantic and contextual feature
information at the bottom of the text, generate dynamic word vector
representations fused with context, and then use the BiLSTM-AT network channel
to secondary extract the global features of the data and use the attention
mechanism to give key parts higher The weight of the DPCNN channel is used to
overcome the long-distance text dependence problem and obtain deep local
features. The local and global feature vectors are spliced, and finally passed
to the fully connected layer, and the final classification result is output
through Softmax. The experimental results show that the proposed model improves
the accuracy, precision and F1-score of news headline classification compared
with the traditional neural network model and the single-channel model under
the same conditions. It can be seen that it can perform well in the
multi-classification application of news headline text under large data volume.",None,-1
1d04875a-ea92-4f3f-acfb-4f9b2547fa08,Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation,0.919731,"Continual relation extraction (CRE) aims to continually learn new relations
from a class-incremental data stream. CRE model usually suffers from
catastrophic forgetting problem, i.e., the performance of old relations
seriously degrades when the model learns new relations. Most previous work
attributes catastrophic forgetting to the corruption of the learned
representations as new relations come, with an implicit assumption that the CRE
models have adequately learned the old relations. In this paper, through
empirical studies we argue that this assumption may not hold, and an important
reason for catastrophic forgetting is that the learned representations do not
have good robustness against the appearance of analogous relations in the
subsequent learning process. To address this issue, we encourage the model to
learn more precise and robust representations through a simple yet effective
adversarial class augmentation mechanism (ACA), which is easy to implement and
model-agnostic. Experimental results show that ACA can consistently improve the
performance of state-of-the-art CRE models on two popular benchmarks.",https://github.com/Wangpeiyi9979/ACA,10429
aa583726-0a99-4f3f-a144-a93e99a31f92,The Arabic Ontology -- An Arabic Wordnet with Ontologically Clean Content,0.771269,"We present a formal Arabic wordnet built on the basis of a carefully designed
ontology hereby referred to as the Arabic Ontology. The ontology provides a
formal representation of the concepts that the Arabic terms convey, and its
content was built with ontological analysis in mind, and benchmarked to
scientific advances and rigorous knowledge sources as much as this is possible,
rather than to only speakers' beliefs as lexicons typically are. A
comprehensive evaluation was conducted thereby demonstrating that the current
version of the top-levels of the ontology can top the majority of the Arabic
meanings. The ontology consists currently of about 1,300 well-investigated
concepts in addition to 11,000 concepts that are partially validated. The
ontology is accessible and searchable through a lexicographic search engine
(https://ontology.birzeit.edu) that also includes about 150 Arabic-multilingual
lexicons, and which are being mapped and enriched using the ontology. The
ontology is fully mapped with Princeton WordNet, Wikidata, and other resources.",None,-1
0713a431-9f5e-4206-903a-9e83af92d481,Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition,0.337378,"We address the problem of capturing temporal information for video
classification in 2D networks, without increasing their computational cost.
Existing approaches focus on modifying the architecture of 2D networks (e.g. by
including filters in the temporal dimension to turn them into 3D networks, or
using optical flow, etc.), which increases computation cost. Instead, we
propose a novel sampling strategy, where we re-order the channels of the input
video, to capture short-term frame-to-frame changes. We observe that without
bells and whistles, the proposed sampling strategy improves performance on
multiple architectures (e.g. TSN, TRN, TSM, and MVFNet) and datasets (CATER,
Something-Something-V1 and V2), up to 24% over the baseline of using the
standard video input. In addition, our sampling strategies do not require
training from scratch and do not increase the computational cost of training
and testing. Given the generality of the results and the flexibility of the
approach, we hope this can be widely useful to the video understanding
community. Code is available on our website:
https://github.com/kiyoon/channel_sampling.",https://github.com/kiyoon/channel_sampling,-1
6bad2ebc-7182-4d87-b0b0-33650e1d7892,Swapping Semantic Contents for Mixing Images,0.039898,"Deep architecture have proven capable of solving many tasks provided a
sufficient amount of labeled data. In fact, the amount of available labeled
data has become the principal bottleneck in low label settings such as
Semi-Supervised Learning. Mixing Data Augmentations do not typically yield new
labeled samples, as indiscriminately mixing contents creates between-class
samples. In this work, we introduce the SciMix framework that can learn to
generator to embed a semantic style code into image backgrounds, we obtain new
mixing scheme for data augmentation. We then demonstrate that SciMix yields
novel mixed samples that inherit many characteristics from their non-semantic
parents. Afterwards, we verify those samples can be used to improve the
performance semi-supervised frameworks like Mean Teacher or Fixmatch, and even
fully supervised learning on a small labeled dataset.",None,-1
5e08d467-3b8d-44ce-96b5-2b92d708ba70,D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights,0.321212,"A profound understanding of inter-agent relationships and motion behaviors is
important to achieve high-quality planning when navigating in complex
scenarios, especially at urban traffic intersections. We present a trajectory
prediction approach with respect to traffic lights, D2-TPred, which uses a
spatial dynamic interaction graph (SDG) and a behavior dependency graph (BDG)
to handle the problem of discontinuous dependency in the spatial-temporal
space. Specifically, the SDG is used to capture spatial interactions by
reconstructing sub-graphs for different agents with dynamic and changeable
characteristics during each frame. The BDG is used to infer motion tendency by
modeling the implicit dependency of the current state on priors behaviors,
especially the discontinuous motions corresponding to acceleration,
deceleration, or turning direction. Moreover, we present a new dataset for
vehicle trajectory prediction under traffic lights called VTP-TL. Our
experimental results show that our model achieves more than {20.45% and 20.78%
}improvement in terms of ADE and FDE, respectively, on VTP-TL as compared to
other trajectory prediction algorithms. The dataset and code are available at:
https://github.com/VTP-TL/D2-TPred.",https://github.com/VTP-TL/D2-TPred,-1
bd220877-29fd-471a-ad29-ee1d52799456,A Graph Convolution for Signed Directed Graphs,0.0678065,"A signed directed graph is a graph with sign and direction information on the
edges. Even though signed directed graphs are more informative than unsigned or
undirected graphs, they are more complicated to analyze and have received less
research attention. This paper investigates a spectral graph convolution model
to fully utilize the information embedded in signed directed edges. We propose
a novel complex Hermitian adjacency matrix that encodes graph information via
complex numbers. Compared to a simple connection-based adjacency matrix, the
complex Hermitian can represent edge direction, sign, and connectivity via its
phases and magnitudes. Then, we define a magnetic Laplacian of the proposed
adjacency matrix and prove that it is positive semi-definite (PSD) for the
analyses using spectral graph convolution. We perform extensive experiments on
four real-world datasets. Our experiments show that the proposed scheme
outperforms several state-of-the-art techniques.",https://github.com/huangjunjie-cs/SiGAT,-1
e63c57dd-ce4e-4fa2-82e2-5cbc350eef44,Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer,0.948658,"Text spotting end-to-end methods have recently gained attention in the
literature due to the benefits of jointly optimizing the text detection and
recognition components. Existing methods usually have a distinct separation
between the detection and recognition branches, requiring exact annotations for
the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach
for text spotting and the first text spotting framework which may be trained
with both fully- and weakly-supervised settings. By learning a single latent
representation per word detection, and using a novel loss function based on the
Hungarian loss, our method alleviates the need for expensive localization
annotations. Trained with only text transcription annotations on real data, our
weakly-supervised method achieves competitive performance with previous
state-of-the-art fully-supervised methods. When trained in a fully-supervised
manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.",None,-1
ee779ab3-a9f8-4d2b-8892-2edb2195d8da,Training Debiased Subnetworks with Contrastive Weight Pruning,0.663245,"Neural networks are often biased to spuriously correlated features that
provide misleading statistical evidence that does not generalize. This raises
an interesting question: ``Does an optimal unbiased functional subnetwork exist
in a severely biased network? If so, how to extract such subnetwork?"" While
empirical evidence has been accumulated about the existence of such unbiased
subnetworks, these observations are mainly based on the guidance of
ground-truth unbiased samples. Thus, it is unexplored how to discover the
optimal subnetworks with biased training datasets in practice. To address this,
here we first present our theoretical insight that alerts potential limitations
of existing algorithms in exploring unbiased subnetworks in the presence of
strong spurious correlations. We then further elucidate the importance of
bias-conflicting samples on structure learning. Motivated by these
observations, we propose a Debiased Contrastive Weight Pruning (DCWP)
algorithm, which probes unbiased subnetworks without expensive group
annotations. Experimental results demonstrate that our approach significantly
outperforms state-of-the-art debiasing methods despite its considerable
reduction in the number of parameters.",None,-1
d56d4982-fc0f-404e-a197-c75b1511c86b,CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction,0.466698,"Knowledge graph (KG) link prediction is a fundamental task in artificial
intelligence, with applications in natural language processing, information
retrieval, and biomedicine. Recently, promising results have been achieved by
leveraging cross-modal information in KGs, using ensembles that combine
knowledge graph embeddings (KGEs) and contextual language models (LMs).
However, existing ensembles are either (1) not consistently effective in terms
of ranking accuracy gains or (2) impractically inefficient on larger datasets
due to the combinatorial explosion problem of pairwise ranking with deep
language models. In this paper, we propose a novel tiered ranking architecture
CascadER to maintain the ranking accuracy of full ensembling while improving
efficiency considerably. CascadER uses LMs to rerank the outputs of more
efficient base KGEs, relying on an adaptive subset selection scheme aimed at
invoking the LMs minimally while maximizing accuracy gain over the KGE.
Extensive experiments demonstrate that CascadER improves MRR by up to 9 points
over KGE baselines, setting new state-of-the-art performance on four benchmarks
while improving efficiency by one or more orders of magnitude over competitive
cross-modal baselines. Our empirical analyses reveal that diversity of models
across modalities and preservation of individual models' confidence signals
help explain the effectiveness of CascadER, and suggest promising directions
for cross-modal cascaded architectures. Code and pretrained models are
available at https://github.com/tsafavi/cascader.",None,-1
a32752cd-5962-48df-af87-ccf940e88407,"Do Androids Laugh at Electric Sheep? Humor ""Understanding"" Benchmarks from The New Yorker Caption Contest",0.480779,"Large neural networks can now generate jokes, but do they really ""understand""
humor? We challenge AI models with three tasks derived from the New Yorker
Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning
caption, and explaining why a winning caption is funny. These tasks encapsulate
progressively more sophisticated aspects of ""understanding"" a cartoon; key
elements are the complex, often surprising relationships between images and
captions and the frequent inclusion of indirect and playful allusions to human
experience and culture. We investigate both multimodal and language-only
models: the former are challenged with the cartoon images directly, while the
latter are given multifaceted descriptions of the visual scene to simulate
human-level visual understanding. We find that both types of models struggle at
all three tasks. For example, our best multimodal models fall 30 accuracy
points behind human performance on the matching task, and, even when provided
ground-truth visual scene descriptors, human-authored explanations are
preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in
more than 2/3 of cases. We release models, code, leaderboard, and corpus, which
includes newly-gathered annotations describing the image's locations/entities,
what's unusual in the scene, and an explanation of the joke.",https://capcon.dev/,-1
67a4e95e-8994-4656-a64d-199dbc4943d6,Label-dependent and event-guided interpretable disease risk prediction using EHRs,0.0840032,"Electronic health records (EHRs) contain patients' heterogeneous data that
are collected from medical providers involved in the patient's care, including
medical notes, clinical events, laboratory test results, symptoms, and
diagnoses. In the field of modern healthcare, predicting whether patients would
experience any risks based on their EHRs has emerged as a promising research
area, in which artificial intelligence (AI) plays a key role. To make AI models
practically applicable, it is required that the prediction results should be
both accurate and interpretable. To achieve this goal, this paper proposed a
label-dependent and event-guided risk prediction model (LERP) to predict the
presence of multiple disease risks by mainly extracting information from
unstructured medical notes. Our model is featured in the following aspects.
First, we adopt a label-dependent mechanism that gives greater attention to
words from medical notes that are semantically similar to the names of risk
labels. Secondly, as the clinical events (e.g., treatments and drugs) can also
indicate the health status of patients, our model utilizes the information from
events and uses them to generate an event-guided representation of medical
notes. Thirdly, both label-dependent and event-guided representations are
integrated to make a robust prediction, in which the interpretability is
enabled by the attention weights over words from medical notes. To demonstrate
the applicability of the proposed method, we apply it to the MIMIC-III dataset,
which contains real-world EHRs collected from hospitals. Our method is
evaluated in both quantitative and qualitative ways.",https://github.com/guoyinwang/LEAM,-1
3bfa87b2-49cc-49d0-9a59-f9169d121b42,ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models,0.275331,"We introduce ReservoirComputing.jl, an open source Julia library for
reservoir computing models. The software offers a great number of algorithms
presented in the literature, and allows to expand on them with both internal
and external tools in a simple way. The implementation is highly modular, fast
and comes with a comprehensive documentation, which includes reproduced
experiments from literature. The code and documentation are hosted on Github
under an MIT license https://github.com/SciML/ReservoirComputing.jl.",https://github.com/SciML/ReservoirComputing.jl,-1
959b0f53-dc81-4b0d-a6a1-104a1d2aac4b,Cross-view and Cross-domain Underwater Localization based on Optical Aerial and Acoustic Underwater Images,0.528609,"Cross-view image matches have been widely explored on terrestrial image
localization using aerial images from drones or satellites. This study expands
the cross-view image match idea and proposes a cross-domain and cross-view
localization framework. The method identifies the correlation between color
aerial images and underwater acoustic images to improve the localization of
underwater vehicles that travel in partially structured environments such as
harbors and marinas. The approach is validated on a real dataset acquired by an
underwater vehicle in a marina. The results show an improvement in the
localization when compared to the dead reckoning of the vehicle.",https://github.com/matheusbg8/aracati2017,-1
25c9b681-042d-44c3-aded-d0af11c78695,uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers,0.750648,"The task of Chinese Spelling Check (CSC) is aiming to detect and correct
spelling errors that can be found in the text. While manually annotating a
high-quality dataset is expensive and time-consuming, thus the scale of the
training dataset is usually very small (e.g., SIGHAN15 only contains 2339
samples for training), therefore supervised-learning based models usually
suffer the data sparsity limitation and over-fitting issue, especially in the
era of big language models. In this paper, we are dedicated to investigating
the \textbf{unsupervised} paradigm to address the CSC problem and we propose a
framework named \textbf{uChecker} to conduct unsupervised spelling error
detection and correction. Masked pretrained language models such as BERT are
introduced as the backbone model considering their powerful language diagnosis
capability. Benefiting from the various and flexible MASKing operations, we
propose a Confusionset-guided masking strategy to fine-train the masked
language model to further improve the performance of unsupervised detection and
correction. Experimental results on standard datasets demonstrate the
effectiveness of our proposed model uChecker in terms of character-level and
sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of spelling
error detection and correction respectively.",https://github.com/iqiyi/FASPell,-1
3b7eed1d-0543-4f39-b4f2-a79835379820,Multi-Frames Temporal Abnormal Clues Learning Method for Face Anti-Spoofing,0.198274,"Face anti-spoofing researches are widely used in face recognition and has
received more attention from industry and academics. In this paper, we propose
the EulerNet, a new temporal feature fusion network in which the differential
filter and residual pyramid are used to extract and amplify abnormal clues from
continuous frames, respectively. A lightweight sample labeling method based on
face landmarks is designed to label large-scale samples at a lower cost and has
better results than other methods such as 3D camera. Finally, we collect 30,000
live and spoofing samples using various mobile ends to create a dataset that
replicates various forms of attacks in a real-world setting. Extensive
experiments on public OULU-NPU show that our algorithm is superior to the state
of art and our solution has already been deployed in real-world systems
servicing millions of users.",None,59
874e6997-09b7-495a-b813-17db1344cca6,Occlusion-Aware Cost Constructor for Light Field Depth Estimation,0.991032,"Matching cost construction is a key step in light field (LF) depth
estimation, but was rarely studied in the deep learning era. Recent deep
learning-based LF depth estimation methods construct matching cost by
sequentially shifting each sub-aperture image (SAI) with a series of predefined
offsets, which is complex and time-consuming. In this paper, we propose a
simple and fast cost constructor to construct matching cost for LF depth
estimation. Our cost constructor is composed by a series of convolutions with
specifically designed dilation rates. By applying our cost constructor to SAI
arrays, pixels under predefined disparities can be integrated and matching cost
can be constructed without using any shifting operation. More importantly, the
proposed cost constructor is occlusion-aware and can handle occlusions by
dynamically modulating pixels from different views. Based on the proposed cost
constructor, we develop a deep network for LF depth estimation. Our network
ranks first on the commonly used 4D LF benchmark in terms of the mean square
error (MSE), and achieves a faster running time than other state-of-the-art
methods.",https://github.com/YingqianWang/OACC-Net,-1
2a0a738b-5c97-4b1f-ba25-74dd6aa3169e,Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video,0.707822,"We present HandAvatar, a novel representation for hand animation and
rendering, which can generate smoothly compositional geometry and
self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as
a high-resolution mesh topology to fit personalized hand shapes. Sequentially,
we decompose hand geometry into per-bone rigid parts, and then re-compose
paired geometry encodings to derive an across-part consistent occupancy field.
As for texture modeling, we propose a self-occlusion-aware shading field
(SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record
albedo information under a wide variety of hand poses. Moreover, directed soft
occupancy is designed to describe the ray-to-surface relation, which is
leveraged to generate an illumination field for the disentanglement of
pose-independent albedo and pose-dependent illumination. Trained from monocular
video data, our HandAvatar can perform free-pose hand animation and rendering
while at the same time achieving superior appearance fidelity. We also
demonstrate that HandAvatar provides a route for hand appearance editing.
Project website: https://seanchenxy.github.io/HandAvatarWeb.",None,2845
322656aa-238f-4f54-8385-3cfeac2fb181,Exploiting map information for self-supervised learning in motion forecasting,0.209618,"Inspired by recent developments regarding the application of self-supervised
learning (SSL), we devise an auxiliary task for trajectory prediction that
takes advantage of map-only information such as graph connectivity with the
intent of improving map comprehension and generalization. We apply this
auxiliary task through two frameworks - multitasking and pretraining. In either
framework we observe significant improvement of our baseline in metrics such as
$\mathrm{minFDE}_6$ (as much as 20.3%) and $\mathrm{MissRate}_6$ (as much as
33.3%), as well as a richer comprehension of map features demonstrated by
different training configurations. The results obtained were consistent in all
three data sets used for experiments: Argoverse, Interaction and NuScenes. We
also submit our new pretrained model's results to the Interaction challenge and
achieve $\textit{1st}$ place with respect to $\mathrm{minFDE}_6$ and
$\mathrm{minADE}_6$.",None,-1
84e8aa5c-7482-4a82-9426-05f68d250e2b,Type-aware Embeddings for Multi-Hop Reasoning over Knowledge Graphs,0.695082,"Multi-hop reasoning over real-life knowledge graphs (KGs) is a highly
challenging problem as traditional subgraph matching methods are not capable to
deal with noise and missing information. To address this problem, it has been
recently introduced a promising approach based on jointly embedding logical
queries and KGs into a low-dimensional space to identify answer entities.
However, existing proposals ignore critical semantic knowledge inherently
available in KGs, such as type information. To leverage type information, we
propose a novel TypE-aware Message Passing (TEMP) model, which enhances the
entity and relation representations in queries, and simultaneously improves
generalization, deductive and inductive reasoning. Remarkably, TEMP is a
plug-and-play model that can be easily incorporated into existing
embedding-based models to improve their performance. Extensive experiments on
three real-world datasets demonstrate TEMP's effectiveness.",https://github.com/zhiweihu1103/QE-TEMP,-1
998de700-e8af-4a8e-95f7-b4c7e00c740c,AGReE: A system for generating Automated Grammar Reading Exercises,0.276312,"We describe the AGReE system, which takes user-submitted passages as input
and automatically generates grammar practice exercises that can be completed
while reading. Multiple-choice practice items are generated for a variety of
different grammar constructs: punctuation, articles, conjunctions, pronouns,
prepositions, verbs, and nouns. We also conducted a large-scale human
evaluation with around 4,500 multiple-choice practice items. We notice for 95%
of items, a majority of raters out of five were able to identify the correct
answer and for 85% of cases, raters agree that there is only one correct answer
among the choices. Finally, the error analysis shows that raters made the most
mistakes for punctuation and conjunctions.",None,-1
3c12647a-1fba-4748-b1ac-26426b6300eb,Unit Selection: Learning Benefit Function from Finite Population Data,0.290734,"The unit selection problem is to identify a group of individuals who are most
likely to exhibit a desired mode of behavior, for example, selecting
individuals who would respond one way if incentivized and a different way if
not. The unit selection problem consists of evaluation and search subproblems.
Li and Pearl defined the ""benefit function"" to evaluate the average payoff of
selecting a certain individual with given characteristics. The search
subproblem is then to design an algorithm to identify the characteristics that
maximize the above benefit function. The hardness of the search subproblem
arises due to the large number of characteristics available for each individual
and the sparsity of the data available in each cell of characteristics. In this
paper, we present a machine learning framework that uses the bounds of the
benefit function that are estimable from the finite population data to learn
the bounds of the benefit function for each cell of characteristics. Therefore,
we could easily obtain the characteristics that maximize the benefit function.",None,-1
37694b8b-8985-480e-8e0e-7b3810662312,Policy Optimization over General State and Action Spaces,0.397804,"Reinforcement learning (RL) problems over general state and action spaces are
notoriously challenging. In contrast to the tableau setting, one can not
enumerate all the states and then iteratively update the policies for each
state. This prevents the application of many well-studied RL methods especially
those with provable convergence guarantees. In this paper, we first present a
substantial generalization of the recently developed policy mirror descent
method to deal with general state and action spaces. We introduce new
approaches to incorporate function approximation into this method, so that we
do not need to use explicit policy parameterization at all. Moreover, we
present a novel policy dual averaging method for which possibly simpler
function approximation techniques can be applied. We establish linear
convergence rate to global optimality or sublinear convergence to stationarity
for these methods applied to solve different classes of RL problems under exact
policy evaluation. We then define proper notions of the approximation errors
for policy evaluation and investigate their impact on the convergence of these
methods applied to general-state RL problems with either finite-action or
continuous-action spaces. To the best of our knowledge, the development of
these algorithmic frameworks as well as their convergence analysis appear to be
new in the literature.",None,11470
ed889798-fa59-4a55-90b0-82ef7f94cb30,Depth Perspective-aware Multiple Object Tracking,0.325745,"This paper aims to tackle Multiple Object Tracking (MOT), an important
problem in computer vision but remains challenging due to many practical
issues, especially occlusions. Indeed, we propose a new real-time Depth
Perspective-aware Multiple Object Tracking (DP-MOT) approach to tackle the
occlusion problem in MOT. A simple yet efficient Subject-Ordered Depth
Estimation (SODE) is first proposed to automatically order the depth positions
of detected subjects in a 2D scene in an unsupervised manner. Using the output
from SODE, a new Active pseudo-3D Kalman filter, a simple but effective
extension of Kalman filter with dynamic control variables, is then proposed to
dynamically update the movement of objects. In addition, a new high-order
association approach is presented in the data association step to incorporate
first-order and second-order relationships between the detected objects. The
proposed approach consistently achieves state-of-the-art performance compared
to recent MOT methods on standard MOT benchmarks.",None,-1
ab2e3ad1-ee97-4871-aa01-563a38a3722b,Federated learning for violence incident prediction in a simulated cross-institutional psychiatric setting,0.903522,"Inpatient violence is a common and severe problem within psychiatry. Knowing
who might become violent can influence staffing levels and mitigate severity.
Predictive machine learning models can assess each patient's likelihood of
becoming violent based on clinical notes. Yet, while machine learning models
benefit from having more data, data availability is limited as hospitals
typically do not share their data for privacy preservation. Federated Learning
(FL) can overcome the problem of data limitation by training models in a
decentralised manner, without disclosing data between collaborators. However,
although several FL approaches exist, none of these train Natural Language
Processing models on clinical notes. In this work, we investigate the
application of Federated Learning to clinical Natural Language Processing,
applied to the task of Violence Risk Assessment by simulating a
cross-institutional psychiatric setting. We train and compare four models: two
local models, a federated model and a data-centralised model. Our results
indicate that the federated model outperforms the local models and has similar
performance as the data-centralised model. These findings suggest that
Federated Learning can be used successfully in a cross-institutional setting
and is a step towards new applications of Federated Learning based on clinical
notes",None,-1
0826fe71-6e7a-4d62-a4a8-d475eb2d0d36,Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance,0.567207,"Denoising diffusion probabilistic models (DDPMs) are a recent family of
generative models that achieve state-of-the-art results. In order to obtain
class-conditional generation, it was suggested to guide the diffusion process
by gradients from a time-dependent classifier. While the idea is theoretically
sound, deep learning-based classifiers are infamously susceptible to
gradient-based adversarial attacks. Therefore, while traditional classifiers
may achieve good accuracy scores, their gradients are possibly unreliable and
might hinder the improvement of the generation results. Recent work discovered
that adversarially robust classifiers exhibit gradients that are aligned with
human perception, and these could better guide a generative process towards
semantically meaningful images. We utilize this observation by defining and
training a time-dependent adversarially robust classifier and use it as
guidance for a generative diffusion model. In experiments on the highly
challenging and diverse ImageNet dataset, our scheme introduces significantly
more intelligible intermediate gradients, better alignment with theoretical
findings, as well as improved generation results under several evaluation
metrics. Furthermore, we conduct an opinion survey whose findings indicate that
human raters prefer our method's results.",https://github.com/bahjat-kawar/enhancing-diffusion-robust,82994
5d5bf0ad-142e-4fba-b34a-4cfcc6572a32,Emergent Quantized Communication,0.669995,"The field of emergent communication aims to understand the characteristics of
communication as it emerges from artificial agents solving tasks that require
information exchange. Communication with discrete messages is considered a
desired characteristic, for both scientific and applied reasons. However,
training a multi-agent system with discrete communication is not
straightforward, requiring either reinforcement learning algorithms or relaxing
the discreteness requirement via a continuous approximation such as the
Gumbel-softmax. Both these solutions result in poor performance compared to
fully continuous communication. In this work, we propose an alternative
approach to achieve discrete communication -- quantization of communicated
messages. Using message quantization allows us to train the model end-to-end,
achieving superior performance in multiple setups. Moreover, quantization is a
natural framework that runs the gamut from continuous to discrete
communication. Thus, it sets the ground for a broader view of multi-agent
communication in the deep learning era.",None,-1
3ff32280-a97e-47c3-8526-2a6325dcf297,IDEA: Increasing Text Diversity via Online Multi-Label Recognition for Vision-Language Pre-training,0.367706,"Vision-Language Pre-training (VLP) with large-scale image-text pairs has
demonstrated superior performance in various fields. However, the image-text
pairs co-occurrent on the Internet typically lack explicit alignment
information, which is suboptimal for VLP. Existing methods proposed to adopt an
off-the-shelf object detector to utilize additional image tag information.
However, the object detector is time-consuming and can only identify the
pre-defined object categories, limiting the model capacity. Inspired by the
observation that the texts incorporate incomplete fine-grained image
information, we introduce IDEA, which stands for increasing text diversity via
online multi-label recognition for VLP. IDEA shows that multi-label learning
with image tags extracted from the texts can be jointly optimized during VLP.
Moreover, IDEA can identify valuable image tags online to provide more explicit
textual supervision. Comprehensive experiments demonstrate that IDEA can
significantly boost the performance on multiple downstream datasets with a
small extra computational cost.",https://github.com/xinyu1205/IDEA-pytorch,-1
8fc20250-9d58-4224-b273-e3caa0d7e528,MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image Translation,0.933722,"We present a novel method for exemplar-based image translation, called
matching interleaved diffusion models (MIDMs). Most existing methods for this
task were formulated as GAN-based matching-then-generation framework. However,
in this framework, matching errors induced by the difficulty of semantic
matching across cross-domain, e.g., sketch and photo, can be easily propagated
to the generation step, which in turn leads to degenerated results. Motivated
by the recent success of diffusion models overcoming the shortcomings of GANs,
we incorporate the diffusion models to overcome these limitations.
Specifically, we formulate a diffusion-based matching-and-generation framework
that interleaves cross-domain matching and diffusion steps in the latent space
by iteratively feeding the intermediate warp into the noising process and
denoising it to generate a translated image. In addition, to improve the
reliability of the diffusion process, we design a confidence-aware process
using cycle-consistency to consider only confident regions during translation.
Experimental results show that our MIDMs generate more plausible images than
state-of-the-art methods.",https://ku-cvlab.github.io/MIDMs/,-1
1b1d4eeb-2215-4ce9-8fdb-b7583fada4bb,Tourist Guidance Robot Based on HyperCLOVA,0.335462,"This paper describes our system submitted to Dialogue Robot Competition 2022.
Our proposed system is a combined model of rule-based and generation-based
dialog systems. The system utilizes HyperCLOVA, a Japanese foundation model,
not only to generate responses but also summarization, search information, etc.
We also used our original speech recognition system, which was fine-tuned for
this dialog task. As a result, our system ranked second in the preliminary
round and moved on to the finals.",None,-1
f16b640b-e135-48b7-a93f-0cce5e538aa5,Wav2Vec-Aug: Improved self-supervised training with limited data,0.345182,"Self-supervised learning (SSL) of speech representations has received much
attention over the last few years but most work has focused on languages and
domains with an abundance of unlabeled data. However, for many languages there
is a shortage even in the unlabeled data which limits the effectiveness of SSL.
In this work, we focus on the problem of applying SSL to domains with limited
available data by leveraging data augmentation for Wav2Vec 2.0 pretraining.
Further, we propose improvements to each component of the model which result in
a combined relative word error rate (WER) improvement of up to 13% compared to
Wav2Vec 2.0 on Librispeech test-clean / other.",https://github.com/facebookresearch/WavAugment,-1
811d4774-fc07-4ff3-a6aa-0aac8f3f0bf4,Modeling Emergent Lexicon Formation with a Self-Reinforcing Stochastic Process,0.118322,"We introduce FiLex, a self-reinforcing stochastic process which models finite
lexicons in emergent language experiments. The central property of FiLex is
that it is a self-reinforcing process, parallel to the intuition that the more
a word is used in a language, the more its use will continue. As a theoretical
model, FiLex serves as a way to both explain and predict the behavior of the
emergent language system. We empirically test FiLex's ability to capture the
relationship between the emergent language's hyperparameters and the lexicon's
Shannon entropy.",https://github.com/brendon-boldt/filex-emergent-language,-1
35fab2f5-6d71-4687-bb6c-9a0dfc512530,Structured access: an emerging paradigm for safe AI deployment,0.7871,"Structured access is an emerging paradigm for the safe deployment of
artificial intelligence (AI). Instead of openly disseminating AI systems,
developers facilitate controlled, arm's length interactions with their AI
systems. The aim is to prevent dangerous AI capabilities from being widely
accessible, whilst preserving access to AI capabilities that can be used
safely. The developer must both restrict how the AI system can be used, and
prevent the user from circumventing these restrictions through modification or
reverse engineering of the AI system. Structured access is most effective when
implemented through cloud-based AI services, rather than disseminating AI
software that runs locally on users' hardware. Cloud-based interfaces provide
the AI developer greater scope for controlling how the AI system is used, and
for protecting against unauthorized modifications to the system's design. This
chapter expands the discussion of ""publication norms"" in the AI community,
which to date has focused on the question of how the informational content of
AI research projects should be disseminated (e.g., code and models). Although
this is an important question, there are limits to what can be achieved through
the control of information flows. Structured access views AI software not only
as information that can be shared but also as a tool with which users can have
arm's length interactions. There are early examples of structured access being
practiced by AI developers, but there is much room for further development,
both in the functionality of cloud-based interfaces and in the wider
institutional framework.",None,-1
90263f55-a045-44a1-b82b-5a89c84b67e1,Learning Deep Time-index Models for Time Series Forecasting,0.308243,"Deep learning has been actively applied to time series forecasting, leading
to a deluge of new methods, belonging to the class of historical-value models.
Yet, despite the attractive properties of time-index models, such as being able
to model the continuous nature of underlying time series dynamics, little
attention has been given to them. Indeed, while naive deep time-index models
are far more expressive than the manually predefined function representations
of classical time-index models, they are inadequate for forecasting, being
unable to generalize to unseen time steps due to the lack of inductive bias. In
this paper, we propose DeepTime, a meta-optimization framework to learn deep
time-index models which overcome these limitations, yielding an efficient and
accurate forecasting model. Extensive experiments on real world datasets in the
long sequence time-series forecasting setting demonstrate that our approach
achieves competitive results with state-of-the-art methods, and is highly
efficient. Code is available at https://github.com/salesforce/DeepTime.",https://github.com/salesforce/DeepTime,-1
60610b95-54d7-44d9-bda0-78f77bef208b,Deep Convolutional Learning-Aided Detector for Generalized Frequency Division Multiplexing with Index Modulation,0.485178,"In this paper, a deep convolutional neural network-based symbol detection and
demodulation is proposed for generalized frequency division multiplexing with
index modulation (GFDM-IM) scheme in order to improve the error performance of
the system. The proposed method first pre-processes the received signal by
using a zero-forcing (ZF) detector and then uses a neural network consisting of
a convolutional neural network (CNN) followed by a fully-connected neural
network (FCNN). The FCNN part uses only two fully-connected layers, which can
be adapted to yield a trade-off between complexity and bit error rate (BER)
performance. This two-stage approach prevents the getting stuck of neural
network in a saddle point and enables IM blocks processing independently. It
has been demonstrated that the proposed deep convolutional neural network-based
detection and demodulation scheme provides better BER performance compared to
ZF detector with a reasonable complexity increase. We conclude that
non-orthogonal waveforms combined with IM schemes with the help of deep
learning is a promising physical layer (PHY) scheme for future wireless
networks",None,-1
f1b35206-1d43-4629-aad9-2dbc5d1c9c6d,ExAgt: Expert-guided Augmentation for Representation Learning of Traffic Scenarios,0.0849589,"Representation learning in recent years has been addressed with
self-supervised learning methods. The input data is augmented into two
distorted views and an encoder learns the representations that are invariant to
distortions -- cross-view prediction. Augmentation is one of the key components
in cross-view self-supervised learning frameworks to learn visual
representations. This paper presents ExAgt, a novel method to include expert
knowledge for augmenting traffic scenarios, to improve the learnt
representations without any human annotation. The expert-guided augmentations
are generated in an automated fashion based on the infrastructure, the
interactions between the EGO and the traffic participants and an ideal sensor
model. The ExAgt method is applied in two state-of-the-art cross-view
prediction methods and the representations learnt are tested in downstream
tasks like classification and clustering. Results show that the ExAgt method
improves representation learning compared to using only standard augmentations
and it provides a better representation space stability. The code is available
at https://github.com/lab176344/ExAgt.",https://github.com/lab176344/ExAgt,-1
57ecf52d-0981-4026-996e-1a2d1646b1f5,Confident Adaptive Language Modeling,0.816495,"Recent advances in Transformer-based large language models (LLMs) have led to
significant performance improvements across many tasks. These gains come with a
drastic increase in the models' size, potentially leading to slow and costly
use at inference time. In practice, however, the series of generations made by
LLMs is composed of varying levels of difficulty. While certain predictions
truly benefit from the models' full capacity, other continuations are more
trivial and can be solved with reduced compute. In this work, we introduce
Confident Adaptive Language Modeling (CALM), a framework for dynamically
allocating different amounts of compute per input and generation timestep.
Early exit decoding involves several challenges that we address here, such as:
(1) what confidence measure to use; (2) connecting sequence-level constraints
to local per-token exit decisions; and (3) attending back to missing hidden
representations due to early exits in previous tokens. Through theoretical
analysis and empirical experiments on three diverse text generation tasks, we
demonstrate the efficacy of our framework in reducing compute -- potential
speedup of up to $\times 3$ -- while provably maintaining high performance.",None,-1
1f62d087-f006-4fdf-b9d2-f4a7fc3375b6,SimCURL: Simple Contrastive User Representation Learning from Command Sequences,0.130495,"User modeling is crucial to understanding user behavior and essential for
improving user experience and personalized recommendations. When users interact
with software, vast amounts of command sequences are generated through logging
and analytics systems. These command sequences contain clues to the users'
goals and intents. However, these data modalities are highly unstructured and
unlabeled, making it difficult for standard predictive systems to learn from.
We propose SimCURL, a simple yet effective contrastive self-supervised deep
learning framework that learns user representation from unlabeled command
sequences. Our method introduces a user-session network architecture, as well
as session dropout as a novel way of data augmentation. We train and evaluate
our method on a real-world command sequence dataset of more than half a billion
commands. Our method shows significant improvement over existing methods when
the learned representation is transferred to downstream tasks such as
experience and expertise classification.",None,4963
67ab042f-7542-4ba4-a639-b836e557ba47,SARAS-Net: Scale and Relation Aware Siamese Network for Change Detection,0.563476,"Change detection (CD) aims to find the difference between two images at
different times and outputs a change map to represent whether the region has
changed or not. To achieve a better result in generating the change map, many
State-of-The-Art (SoTA) methods design a deep learning model that has a
powerful discriminative ability. However, these methods still get lower
performance because they ignore spatial information and scaling changes between
objects, giving rise to blurry or wrong boundaries. In addition to these, they
also neglect the interactive information of two different images. To alleviate
these problems, we propose our network, the Scale and Relation-Aware Siamese
Network (SARAS-Net) to deal with this issue. In this paper, three modules are
proposed that include relation-aware, scale-aware, and cross-transformer to
tackle the problem of scene change detection more effectively. To verify our
model, we tested three public datasets, including LEVIR-CD, WHU-CD, and DSFIN,
and obtained SoTA accuracy. Our code is available at
https://github.com/f64051041/SARAS-Net.",https://github.com/f64051041/SARAS-Net,-1
492620cf-7e70-49d8-bea6-fa0a2a802d2e,Towards a Defense Against Federated Backdoor Attacks Under Continuous Training,0.0381327,"Backdoor attacks are dangerous and difficult to prevent in federated learning
(FL), where training data is sourced from untrusted clients over long periods
of time. These difficulties arise because: (a) defenders in FL do not have
access to raw training data, and (b) a new phenomenon we identify called
backdoor leakage causes models trained continuously to eventually suffer from
backdoors due to cumulative errors in defense mechanisms. We propose shadow
learning, a framework for defending against backdoor attacks in the FL setting
under long-range training. Shadow learning trains two models in parallel: a
backbone model and a shadow model. The backbone is trained without any defense
mechanism to obtain good performance on the main task. The shadow model
combines filtering of malicious clients with early-stopping to control the
attack success rate even as the data distribution changes. We theoretically
motivate our design and show experimentally that our framework significantly
improves upon existing defenses against backdoor attacks.",None,-1
a08574af-9f09-4350-88c3-f5f2857cc46c,NeuPhysics: Editable Neural Geometry and Physics from Monocular Videos,0.790482,"We present a method for learning 3D geometry and physics parameters of a
dynamic scene from only a monocular RGB video input. To decouple the learning
of underlying scene geometry from dynamic motion, we represent the scene as a
time-invariant signed distance function (SDF) which serves as a reference
frame, along with a time-conditioned deformation field. We further bridge this
neural geometry representation with a differentiable physics simulator by
designing a two-way conversion between the neural field and its corresponding
hexahedral mesh, enabling us to estimate physics parameters from the source
video by minimizing a cycle consistency loss. Our method also allows a user to
interactively edit 3D objects from the source video by modifying the recovered
hexahedral mesh, and propagating the operation back to the neural field
representation. Experiments show that our method achieves superior mesh and
video reconstruction of dynamic scenes compared to competing Neural Field
approaches, and we provide extensive examples which demonstrate its ability to
extract useful 3D representations from videos captured with consumer-grade
cameras.",https://sites.google.com/view/neuphysics,-1
e08d1e7b-f626-4c9c-804d-3f7cdc03509b,Polysemanticity and Capacity in Neural Networks,0.664065,"Individual neurons in neural networks often represent a mixture of unrelated
features. This phenomenon, called polysemanticity, can make interpreting neural
networks more difficult and so we aim to understand its causes. We propose
doing so through the lens of feature \emph{capacity}, which is the fractional
dimension each feature consumes in the embedding space. We show that in a toy
model the optimal capacity allocation tends to monosemantically represent the
most important features, polysemantically represent less important features (in
proportion to their impact on the loss), and entirely ignore the least
important features. Polysemanticity is more prevalent when the inputs have
higher kurtosis or sparsity and more prevalent in some architectures than
others. Given an optimal allocation of capacity, we go on to study the geometry
of the embedding space. We find a block-semi-orthogonal structure, with
differing block sizes in different models, highlighting the impact of model
architecture on the interpretability of its neurons.",None,-1
a629d851-452f-49dd-a79c-1770161da048,Mosaic Zonotope Shadow Matching for Risk-Aware Autonomous Localization in Harsh Urban Environments,0.038842,"Risk-aware urban localization with the Global Navigation Satellite System
(GNSS) remains an unsolved problem with frequent misdetection of the user's
street or side of the street. Significant advances in 3D map-aided GNSS use
grid-based GNSS shadow matching alongside AI-driven line-of-sight (LOS)
classifiers and server-based processing to improve localization accuracy,
especially in the cross-street direction. Our prior work introduces a new
paradigm for shadow matching that proposes set-valued localization with
computationally efficient zonotope set representations. While existing
literature improved accuracy and efficiency, the current state of shadow
matching theory does not address the needs of risk-aware autonomous systems. We
extend our prior work to propose Mosaic Zonotope Shadow Matching (MZSM) that
employs a classifier-agnostic polytope mosaic architecture to provide
risk-awareness and certifiable guarantees on urban positioning. We formulate a
recursively expanding binary tree that refines an initial location estimate
with set operations into smaller polytopes. Together, the smaller polytopes
form a mosaic. We weight the tree branches with the probability that the user
is in line of sight of the satellite and expand the tree with each new
satellite observation. Our method yields an exact shadow matching distribution
from which we guarantee uncertainty bounds on the user localization. We perform
high-fidelity simulations using a 3D building map of San Francisco to validate
our algorithm's risk-aware improvements. We demonstrate that MZSM provides
certifiable guarantees across varied data-driven LOS classifier accuracies and
yields a more precise understanding of the uncertainty over existing methods.
We validate that our tree-based construction is efficient and tractable,
computing a mosaic from 14 satellites in 0.63 seconds and growing quadratically
in the satellite number.",https://github.com/kristianpaul/SoftGNSS,-1
0f46b066-52a9-45ae-8b3a-e2add85ff397,A Dataset for Medical Instructional Video Classification and Question Answering,0.51062,"This paper introduces a new challenge and datasets to foster research toward
designing systems that can understand medical videos and provide visual answers
to natural language questions. We believe medical videos may provide the best
possible answers to many first aids, medical emergency, and medical education
questions. Toward this, we created the MedVidCL and MedVidQA datasets and
introduce the tasks of Medical Video Classification (MVC) and Medical Visual
Answer Localization (MVAL), two tasks that focus on cross-modal (medical
language and medical video) understanding. The proposed tasks and datasets have
the potential to support the development of sophisticated downstream
applications that can benefit the public and medical practitioners. Our
datasets consist of 6,117 annotated videos for the MVC task and 3,010 annotated
questions and answers timestamps from 899 videos for the MVAL task. These
datasets have been verified and corrected by medical informatics experts. We
have also benchmarked each task with the created MedVidCL and MedVidQA datasets
and proposed the multimodal learning methods that set competitive baselines for
future research.",https://github.com/deepaknlp/MedVidQACL,-1
7e9e3095-30aa-434e-8d4d-f10c99b81868,Face segmentation: A comparison between visible and thermal images,0.158039,"Face segmentation is a first step for face biometric systems. In this paper
we present a face segmentation algorithm for thermographic images. This
algorithm is compared with the classic Viola and Jones algorithm used for
visible images. Experimental results reveal that, when segmenting a
multispectral (visible and thermal) face database, the proposed algorithm is
more than 10 times faster, while the accuracy of face segmentation in thermal
images is higher than in case of Viola-Jones",None,-1
9be675ea-6d0f-4618-a5fe-900d28df1d13,CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers,0.539693,"Context-dependent text-to-SQL is the task of translating multi-turn questions
into database-related SQL queries. Existing methods typically focus on making
full use of history context or previously predicted SQL for currently SQL
parsing, while neglecting to explicitly comprehend the schema and
conversational dependency, such as co-reference, ellipsis and user focus
change. In this paper, we propose CQR-SQL, which uses auxiliary Conversational
Question Reformulation (CQR) learning to explicitly exploit schema and decouple
contextual dependency for SQL parsing. Specifically, we first present a schema
enhanced recursive CQR method to produce domain-relevant self-contained
questions. Secondly, we train CQR-SQL models to map the semantics of multi-turn
questions and auxiliary self-contained questions into the same latent space
through schema grounding consistency task and tree-structured SQL parsing
consistency task, which enhances the abilities of SQL parsing by adequately
contextual understanding. At the time of writing, our CQR-SQL achieves new
state-of-the-art results on two context-dependent text-to-SQL benchmarks SParC
and CoSQL.",None,-1
29b361c0-0529-400c-adf7-f3432e9c2eec,Back to the Roots: Reconstructing Large and Complex Cranial Defects using an Image-based Statistical Shape Model,0.366961,"Designing implants for large and complex cranial defects is a challenging
task, even for professional designers. Current efforts on automating the design
process focused mainly on convolutional neural networks (CNN), which have
produced state-of-the-art results on reconstructing synthetic defects. However,
existing CNN-based methods have been difficult to translate to clinical
practice in cranioplasty, as their performance on complex and irregular cranial
defects remains unsatisfactory. In this paper, a statistical shape model (SSM)
built directly on the segmentation masks of the skulls is presented. We
evaluate the SSM on several cranial implant design tasks, and the results show
that, while the SSM performs suboptimally on synthetic defects compared to
CNN-based approaches, it is capable of reconstructing large and complex defects
with only minor manual corrections. The quality of the resulting implants is
examined and assured by experienced neurosurgeons. In contrast, CNN-based
approaches, even with massive data augmentation, fail or produce
less-than-satisfactory implants for these cases. Codes are publicly available
at https://github.com/Jianningli/ssm",https://github.com/Jianningli/ssm,4894
406b2d89-8358-46fa-9e8d-91bf84ca1920,Visual Recognition by Request,0.381217,"Humans have the ability of recognizing visual semantics in an unlimited
granularity, but existing visual recognition algorithms cannot achieve this
goal. In this paper, we establish a new paradigm named visual recognition by
request (ViRReq) to bridge the gap. The key lies in decomposing visual
recognition into atomic tasks named requests and leveraging a knowledge base, a
hierarchical and text-based dictionary, to assist task definition. ViRReq
allows for (i) learning complicated whole-part hierarchies from highly
incomplete annotations and (ii) inserting new concepts with minimal efforts. We
also establish a solid baseline by integrating language-driven recognition into
recent semantic and instance segmentation methods, and demonstrate its flexible
recognition ability on CPP and ADE20K, two datasets with hierarchical
whole-part annotations.",https://github.com/facebookresearch/detectron2,-1
1742bf57-7625-4c77-9574-61b4bc0565c5,An unambiguous cloudiness index for nonwovens,0.0505527,"Cloudiness or formation is a concept routinely used in industry to address
deviations from homogeneity in nonwovens and papers. Measuring a cloudiness
index based on image data is a common task in industrial quality assurance. The
two most popular ways of quantifying cloudiness are based on power spectrum or
correlation function on the one hand or the Laplacian pyramid on the other
hand. Here, we recall the mathematical basis of the first approach
comprehensively, derive a cloudiness index, and demonstrate its practical
estimation. We prove that the Laplacian pyramid as well as other quantities
characterizing cloudiness like the range of interaction and the intensity of
small-angle scattering are very closely related to the power spectrum. Finally,
we show that the power spectrum is easy to be measured image analytically and
carries more information than the alternatives.",None,-1
182b66c6-e1ac-4ea4-ad2f-a2a5fcf74658,JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning,0.272866,"In online job marketplaces, it is important to establish a well-defined job
title taxonomy for various downstream tasks (e.g., job recommendation, users'
career analysis, and turnover prediction). Job Title Normalization (JTN) is
such a cleaning step to classify user-created non-standard job titles into
normalized ones. However, solving the JTN problem is non-trivial with
challenges: (1) semantic similarity of different job titles, (2) non-normalized
user-created job titles, and (3) large-scale and long-tailed job titles in
real-world applications. To this end, we propose a novel solution, named JAMES,
that constructs three unique embeddings (i.e., graph, contextual, and
syntactic) of a target job title to effectively capture its various traits. We
further propose a multi-aspect co-attention mechanism to attentively combine
these embeddings, and employ neural logical reasoning representations to
collaboratively estimate similarities between messy job titles and normalized
job titles in a reasoning space. To evaluate JAMES, we conduct comprehensive
experiments against ten competing models on a large-scale real-world dataset
with over 350,000 job titles. Our experimental results show that JAMES
significantly outperforms the best baseline by 10.06% in Precision@10 and by
17.52% in NDCG@10, respectively.",None,13066
52ecb704-7e02-4d1e-9538-062dca061b9f,End-to-End Image-Based Fashion Recommendation,0.703351,"In fashion-based recommendation settings, incorporating the item image
features is considered a crucial factor, and it has shown significant
improvements to many traditional models, including but not limited to matrix
factorization, auto-encoders, and nearest neighbor models. While there are
numerous image-based recommender approaches that utilize dedicated deep neural
networks, comparisons to attribute-aware models are often disregarded despite
their ability to be easily extended to leverage items' image features. In this
paper, we propose a simple yet effective attribute-aware model that
incorporates image features for better item representation learning in item
recommendation tasks. The proposed model utilizes items' image features
extracted by a calibrated ResNet50 component. We present an ablation study to
compare incorporating the image features using three different techniques into
the recommender system component that can seamlessly leverage any available
items' attributes. Experiments on two image-based real-world recommender
systems datasets show that the proposed model significantly outperforms all
state-of-the-art image-based models.",https://github.com/Shereen-Elsayed/ImgRec,-1
05ca3619-1b7a-445e-be50-6fa64ba2a698,"SF-PATE: Scalable, Fair, and Private Aggregation of Teacher Ensembles",0.323759,"A critical concern in data-driven processes is to build models whose outcomes
do not discriminate against some demographic groups, including gender,
ethnicity, or age. To ensure non-discrimination in learning tasks, knowledge of
the group attributes is essential. However, in practice, these attributes may
not be available due to legal and ethical requirements. To address this
challenge, this paper studies a model that protects the privacy of the
individuals' sensitive information while also allowing it to learn
non-discriminatory predictors. A key characteristic of the proposed model is to
enable the adoption of off-the-selves and non-private fair models to create a
privacy-preserving and fair model. The paper analyzes the relation between
accuracy, privacy, and fairness, and the experimental evaluation illustrates
the benefits of the proposed models on several prediction tasks. In particular,
this proposal is the first to allow both scalable and accurate training of
private and fair models for very large neural networks.",None,-1
aaad281b-e05e-4790-9f51-fc8607596d90,QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity,0.512302,"The mechanism of existing style transfer algorithms is by minimizing a hybrid
loss function to push the generated image toward high similarities in both
content and style. However, this type of approach cannot guarantee visual
fidelity, i.e., the generated artworks should be indistinguishable from real
ones. In this paper, we devise a new style transfer framework called QuantArt
for high visual-fidelity stylization. QuantArt pushes the latent representation
of the generated artwork toward the centroids of the real artwork distribution
with vector quantization. By fusing the quantized and continuous latent
representations, QuantArt allows flexible control over the generated artworks
in terms of content preservation, style similarity, and visual fidelity.
Experiments on various style transfer settings show that our QuantArt framework
achieves significantly higher visual fidelity compared with the existing style
transfer methods.",https://github.com/siyuhuang/QuantArt,-1
935590db-06d5-4312-87ba-279ebabf4bb7,Pushing the Efficiency Limit Using Structured Sparse Convolutions,0.125902,"Weight pruning is among the most popular approaches for compressing deep
convolutional neural networks. Recent work suggests that in a randomly
initialized deep neural network, there exist sparse subnetworks that achieve
performance comparable to the original network. Unfortunately, finding these
subnetworks involves iterative stages of training and pruning, which can be
computationally expensive. We propose Structured Sparse Convolution (SSC),
which leverages the inherent structure in images to reduce the parameters in
the convolutional filter. This leads to improved efficiency of convolutional
architectures compared to existing methods that perform pruning at
initialization. We show that SSC is a generalization of commonly used layers
(depthwise, groupwise and pointwise convolution) in ``efficient
architectures.'' Extensive experiments on well-known CNN models and datasets
show the effectiveness of the proposed method. Architectures based on SSC
achieve state-of-the-art performance compared to baselines on CIFAR-10,
CIFAR-100, Tiny-ImageNet, and ImageNet classification benchmarks.",https://github.com/vkvermaa/SSC,-1
3330603f-3f5d-403a-b651-6e4604edfc3d,Retrieval augmentation of large language models for lay language generation,0.155465,"Recent lay language generation systems have used Transformer models trained
on a parallel corpus to increase health information accessibility. However, the
applicability of these models is constrained by the limited size and topical
breadth of available corpora. We introduce CELLS, the largest (63k pairs) and
broadest-ranging (12 journals) parallel corpus for lay language generation. The
abstract and the corresponding lay language summary are written by domain
experts, assuring the quality of our dataset. Furthermore, qualitative
evaluation of expert-authored plain language summaries has revealed background
explanation as a key strategy to increase accessibility. Such explanation is
challenging for neural models to generate because it goes beyond simplification
by adding content absent from the source. We derive two specialized paired
corpora from CELLS to address key challenges in lay language generation:
generating background explanations and simplifying the original abstract. We
adopt retrieval-augmented models as an intuitive fit for the task of background
explanation generation, and show improvements in summary quality and simplicity
while maintaining factual correctness. Taken together, this work presents the
first comprehensive study of background explanation for lay language
generation, paving the path for disseminating scientific knowledge to a broader
audience. CELLS is publicly available at:
https://github.com/LinguisticAnomalies/pls_retrieval.",https://github.com/LinguisticAnomalies/pls-retrieval,27504
9cdb7711-b94b-46d3-be36-ba1e2c68df60,PedRecNet: Multi-task deep neural network for full 3D human pose and orientation estimation,0.504285,"We present a multitask network that supports various deep neural network
based pedestrian detection functions. Besides 2D and 3D human pose, it also
supports body and head orientation estimation based on full body bounding box
input. This eliminates the need for explicit face recognition. We show that the
performance of 3D human pose estimation and orientation estimation is
comparable to the state-of-the-art. Since very few data sets exist for 3D human
pose and in particular body and head orientation estimation based on full body
data, we further show the benefit of particular simulation data to train the
network. The network architecture is relatively simple, yet powerful, and
easily adaptable for further research and applications.",https://github.com/noboevbo/PedRec,-1
7135a12e-cf3b-4307-b866-482b2048740d,Typography-MNIST (TMNIST): an MNIST-Style Image Dataset to Categorize Glyphs and Font-Styles,0.112835,"We present Typography-MNIST (TMNIST), a dataset comprising of 565,292
MNIST-style grayscale images representing 1,812 unique glyphs in varied styles
of 1,355 Google-fonts. The glyph-list contains common characters from over 150
of the modern and historical language scripts with symbol sets, and each
font-style represents varying subsets of the total unique glyphs. The dataset
has been developed as part of the CognitiveType project which aims to develop
eye-tracking tools for real-time mapping of type to cognition and to create
computational tools that allow for the easy design of typefaces with cognitive
properties such as readability. The dataset and scripts to generate MNIST-style
images for glyphs in different font styles are freely available at
https://github.com/aiskunks/CognitiveType.",https://github.com/aiskunks/CognitiveType,-1
8f189e56-1f65-4393-83a4-bd09e73aa961,Leveraging Data Recasting to Enhance Tabular Reasoning,0.421483,"Creating challenging tabular inference data is essential for learning complex
reasoning. Prior work has mostly relied on two data generation strategies. The
first is human annotation, which yields linguistically diverse data but is
difficult to scale. The second category for creation is synthetic generation,
which is scalable and cost effective but lacks inventiveness. In this research,
we present a framework for semi-automatically recasting existing tabular data
to make use of the benefits of both approaches. We utilize our framework to
build tabular NLI instances from five datasets that were initially intended for
tasks like table2text creation, tabular Q/A, and semantic parsing. We
demonstrate that recasted data could be used as evaluation benchmarks as well
as augmentation data to enhance performance on tabular NLI tasks. Furthermore,
we investigate the effectiveness of models trained on recasted data in the
zero-shot scenario, and analyse trends in performance across different recasted
datasets types.",https://recasting-to-nli.github.io,-1
54a32a81-4eb6-42dd-8b92-d910b95e4a22,Towards Involving End-users in Interactive Human-in-the-loop AI Fairness,0.745923,"Ensuring fairness in artificial intelligence (AI) is important to counteract
bias and discrimination in far-reaching applications. Recent work has started
to investigate how humans judge fairness and how to support machine learning
(ML) experts in making their AI models fairer. Drawing inspiration from an
Explainable AI (XAI) approach called \emph{explanatory debugging} used in
interactive machine learning, our work explores designing interpretable and
interactive human-in-the-loop interfaces that allow ordinary end-users without
any technical or domain background to identify potential fairness issues and
possibly fix them in the context of loan decisions. Through workshops with
end-users, we co-designed and implemented a prototype system that allowed
end-users to see why predictions were made, and then to change weights on
features to ""debug"" fairness issues. We evaluated the use of this prototype
system through an online study. To investigate the implications of diverse
human values about fairness around the globe, we also explored how cultural
dimensions might play a role in using this prototype. Our results contribute to
the design of interfaces to allow end-users to be involved in judging and
addressing AI fairness through a human-in-the-loop approach.",None,-1
31e266f1-ecdb-4fb1-8d5f-e5ff7fefc4d1,Frontiers and Exact Learning of ELI Queries under DL-Lite Ontologies,0.361861,"We study ELI queries (ELIQs) in the presence of ontologies formulated in the
description logic DL-Lite. For the dialect DL-LiteH, we show that ELIQs have a
frontier (set of least general generalizations) that is of polynomial size and
can be computed in polynomial time. In the dialect DL-LiteF, in contrast,
frontiers may be infinite. We identify a natural syntactic restriction that
enables the same positive results as for DL-LiteH. We use out results on
frontiers to show that ELIQs are learnable in polynomial time in the presence
of a DL-LiteH / restricted DL-LiteF ontology in Angluin's framework of exact
learning with only membership queries.",None,14546
614313a7-2f49-478b-a5ef-bb84ba2cbc72,DistSPECTRL: Distributing Specifications in Multi-Agent Reinforcement Learning Systems,0.121108,"While notable progress has been made in specifying and learning objectives
for general cyber-physical systems, applying these methods to distributed
multi-agent systems still pose significant challenges. Among these are the need
to (a) craft specification primitives that allow expression and interplay of
both local and global objectives, (b) tame explosion in the state and action
spaces to enable effective learning, and (c) minimize coordination frequency
and the set of engaged participants for global objectives. To address these
challenges, we propose a novel specification framework that allows natural
composition of local and global objectives used to guide training of a
multi-agent system. Our technique enables learning expressive policies that
allow agents to operate in a coordination-free manner for local objectives,
while using a decentralized communication protocol for enforcing global ones.
Experimental results support our claim that sophisticated multi-agent
distributed planning problems can be effectively realized using
specification-guided learning.",https://github.com/yokian/distspectrl,-1
0d021d29-89d5-406c-9640-8b191fdc4b08,The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents,0.221329,"Learned communication between agents is a powerful tool when approaching
decision-making problems that are hard to overcome by any single agent in
isolation. However, continual coordination and communication learning between
machine agents or human-machine partnerships remains a challenging open
problem. As a stepping stone toward solving the continual communication
learning problem, in this paper we contribute a multi-faceted study into what
we term Pavlovian signalling -- a process by which learned, temporally extended
predictions made by one agent inform decision-making by another agent with
different perceptual access to their shared environment. We seek to establish
how different temporal processes and representational choices impact Pavlovian
signalling between learning agents. To do so, we introduce a partially
observable decision-making domain we call the Frost Hollow. In this domain a
prediction learning agent and a reinforcement learning agent are coupled into a
two-part decision-making system that seeks to acquire sparse reward while
avoiding time-conditional hazards. We evaluate two domain variations: 1)
machine prediction and control learning in a linear walk, and 2) a prediction
learning machine interacting with a human participant in a virtual reality
environment. Our results showcase the speed of learning for Pavlovian
signalling, the impact that different temporal representations do (and do not)
have on agent-agent coordination, and how temporal aliasing impacts agent-agent
and human-agent interactions differently. As a main contribution, we establish
Pavlovian signalling as a natural bridge between fixed signalling paradigms and
fully adaptive communication learning. Our results therefore point to an
actionable, constructivist path towards continual communication learning
between reinforcement learning agents, with potential impact in a range of
real-world settings.",None,84402
c349f159-1e53-49c7-a8d4-3bfc5909e1a4,Label-enhanced Prototypical Network with Contrastive Learning for Multi-label Few-shot Aspect Category Detection,0.826386,"Multi-label aspect category detection allows a given review sentence to
contain multiple aspect categories, which is shown to be more practical in
sentiment analysis and attracting increasing attention. As annotating large
amounts of data is time-consuming and labor-intensive, data scarcity occurs
frequently in real-world scenarios, which motivates multi-label few-shot aspect
category detection. However, research on this problem is still in infancy and
few methods are available. In this paper, we propose a novel label-enhanced
prototypical network (LPN) for multi-label few-shot aspect category detection.
The highlights of LPN can be summarized as follows. First, it leverages label
description as auxiliary knowledge to learn more discriminative prototypes,
which can retain aspect-relevant information while eliminating the harmful
effect caused by irrelevant aspects. Second, it integrates with contrastive
learning, which encourages that the sentences with the same aspect label are
pulled together in embedding space while simultaneously pushing apart the
sentences with different aspect labels. In addition, it introduces an adaptive
multi-label inference module to predict the aspect count in the sentence, which
is simple yet effective. Extensive experimental results on three datasets
demonstrate that our proposed model LPN can consistently achieve
state-of-the-art performance.",None,-1
ac833b4f-895e-4572-b734-ad7f97743f55,CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation,0.757249,"We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based
framework for panoptic segmentation designed around clustering. It rethinks the
existing transformer architectures used in segmentation and detection;
CMT-DeepLab considers the object queries as cluster centers, which fill the
role of grouping the pixels when applied to segmentation. The clustering is
computed with an alternating procedure, by first assigning pixels to the
clusters by their feature affinity, and then updating the cluster centers and
pixel features. Together, these operations comprise the Clustering Mask
Transformer (CMT) layer, which produces cross-attention that is denser and more
consistent with the final segmentation task. CMT-DeepLab improves the
performance over prior art significantly by 4.4% PQ, achieving a new
state-of-the-art of 55.7% PQ on the COCO test-dev set.",None,-1
9a32b310-30a7-4320-90d7-20e355bef6c3,High-Resolution Image Editing via Multi-Stage Blended Diffusion,0.407403,"Diffusion models have shown great results in image generation and in image
editing. However, current approaches are limited to low resolutions due to the
computational cost of training diffusion models for high-resolution generation.
We propose an approach that uses a pre-trained low-resolution diffusion model
to edit images in the megapixel range. We first use Blended Diffusion to edit
the image at a low resolution, and then upscale it in multiple stages, using a
super-resolution model and Blended Diffusion. Using our approach, we achieve
higher visual fidelity than by only applying off the shelf super-resolution
methods to the output of the diffusion model. We also obtain better global
consistency than directly using the diffusion model at a higher resolution.",https://github.com/pfnet-research/multi-stage-blended-diffusion,-1
c782c258-f3ad-467b-8732-804a8756c6e4,A Statistical Model for Predicting Generalization in Few-Shot Classification,0.113257,"The estimation of the generalization error of classifiers often relies on a
validation set. Such a set is hardly available in few-shot learning scenarios,
a highly disregarded shortcoming in the field. In these scenarios, it is common
to rely on features extracted from pre-trained neural networks combined with
distance-based classifiers such as nearest class mean. In this work, we
introduce a Gaussian model of the feature distribution. By estimating the
parameters of this model, we are able to predict the generalization error on
new classification tasks with few samples. We observe that accurate distance
estimates between class-conditional densities are the key to accurate estimates
of the generalization performance. Therefore, we propose an unbiased estimator
for these distances and integrate it in our numerical analysis. We empirically
show that our approach outperforms alternatives such as the leave-one-out
cross-validation strategy.",https://github.com/ybendou/fs-generalization,-1
a8640fa4-fbf8-4c5a-8b11-de052b8be3d1,Single Stage Virtual Try-on via Deformable Attention Flows,0.999997,"Virtual try-on aims to generate a photo-realistic fitting result given an
in-shop garment and a reference person image. Existing methods usually build up
multi-stage frameworks to deal with clothes warping and body blending
respectively, or rely heavily on intermediate parser-based labels which may be
noisy or even inaccurate. To solve the above challenges, we propose a
single-stage try-on framework by developing a novel Deformable Attention Flow
(DAFlow), which applies the deformable attention scheme to multi-flow
estimation. With pose keypoints as the guidance only, the self- and
cross-deformable attention flows are estimated for the reference person and the
garment images, respectively. By sampling multiple flow fields, the
feature-level and pixel-level information from different semantic areas are
simultaneously extracted and merged through the attention mechanism. It enables
clothes warping and body synthesizing at the same time which leads to
photo-realistic results in an end-to-end manner. Extensive experiments on two
try-on datasets demonstrate that our proposed method achieves state-of-the-art
performance both qualitatively and quantitatively. Furthermore, additional
experiments on the other two image editing tasks illustrate the versatility of
our method for multi-view synthesis and image animation.",None,-1
2eedc60c-493a-4517-a60b-de7c267f8eec,Super-resolution 3D Human Shape from a Single Low-Resolution Image,0.350102,"We propose a novel framework to reconstruct super-resolution human shape from
a single low-resolution input image. The approach overcomes limitations of
existing approaches that reconstruct 3D human shape from a single image, which
require high-resolution images together with auxiliary data such as surface
normal or a parametric model to reconstruct high-detail shape. The proposed
framework represents the reconstructed shape with a high-detail implicit
function. Analogous to the objective of 2D image super-resolution, the approach
learns the mapping from a low-resolution shape to its high-resolution
counterpart and it is applied to reconstruct 3D shape detail from
low-resolution images. The approach is trained end-to-end employing a novel
loss function which estimates the information lost between a low and
high-resolution representation of the same 3D surface shape. Evaluation for
single image reconstruction of clothed people demonstrates that our method
achieves high-detail surface reconstruction from low-resolution images without
auxiliary data. Extensive experiments show that the proposed approach can
estimate super-resolution human geometries with a significantly higher level of
detail than that obtained with previous approaches when applied to
low-resolution images.",None,13308
3bc519cf-2ef9-4969-9c17-eba31097cac1,Semantic Similarity Computing Model Based on Multi Model Fine-Grained Nonlinear Fusion,0.764845,"Natural language processing (NLP) task has achieved excellent performance in
many fields, including semantic understanding, automatic summarization, image
recognition and so on. However, most of the neural network models for NLP
extract the text in a fine-grained way, which is not conducive to grasp the
meaning of the text from a global perspective. To alleviate the problem, the
combination of the traditional statistical method and deep learning model as
well as a novel model based on multi model nonlinear fusion are proposed in
this paper. The model uses the Jaccard coefficient based on part of speech,
Term Frequency-Inverse Document Frequency (TF-IDF) and word2vec-CNN algorithm
to measure the similarity of sentences respectively. According to the
calculation accuracy of each model, the normalized weight coefficient is
obtained and the calculation results are compared. The weighted vector is input
into the fully connected neural network to give the final classification
results. As a result, the statistical sentence similarity evaluation algorithm
reduces the granularity of feature extraction, so it can grasp the sentence
features globally. Experimental results show that the matching of sentence
similarity calculation method based on multi model nonlinear fusion is 84%, and
the F1 value of the model is 75%.",None,18646
be56591b-d054-4b71-bc94-26754ed25ab7,Empathic Conversations: A Multi-level Dataset of Contextualized Conversations,0.942892,"Empathy is a cognitive and emotional reaction to an observed situation of
others. Empathy has recently attracted interest because it has numerous
applications in psychology and AI, but it is unclear how different forms of
empathy (e.g., self-report vs counterpart other-report, concern vs. distress)
interact with other affective phenomena or demographics like gender and age. To
better understand this, we created the {\it Empathic Conversations} dataset of
annotated negative, empathy-eliciting dialogues in which pairs of participants
converse about news articles. People differ in their perception of the empathy
of others. These differences are associated with certain characteristics such
as personality and demographics. Hence, we collected detailed characterization
of the participants' traits, their self-reported empathetic response to news
articles, their conversational partner other-report, and turn-by-turn
third-party assessments of the level of self-disclosure, emotion, and empathy
expressed. This dataset is the first to present empathy in multiple forms along
with personal distress, emotion, personality characteristics, and person-level
demographic information. We present baseline models for predicting some of
these features from conversations.",https://github.com/wwbp/empathic_reactions,-1
7e7ec353-42d8-437e-8351-976ba08c1252,Self-Supervised Leaf Segmentation under Complex Lighting Conditions,0.389368,"As an essential prerequisite task in image-based plant phenotyping, leaf
segmentation has garnered increasing attention in recent years. While
self-supervised learning is emerging as an effective alternative to various
computer vision tasks, its adaptation for image-based plant phenotyping remains
rather unexplored. In this work, we present a self-supervised leaf segmentation
framework consisting of a self-supervised semantic segmentation model, a
color-based leaf segmentation algorithm, and a self-supervised color correction
model. The self-supervised semantic segmentation model groups the semantically
similar pixels by iteratively referring to the self-contained information,
allowing the pixels of the same semantic object to be jointly considered by the
color-based leaf segmentation algorithm for identifying the leaf regions.
Additionally, we propose to use a self-supervised color correction model for
images taken under complex illumination conditions. Experimental results on
datasets of different plant species demonstrate the potential of the proposed
self-supervised framework in achieving effective and generalizable leaf
segmentation.",https://github.com/lxfhfut/Self-Supervised-Leaf-Segmentation,-1
46f68e10-22f5-47e2-8168-cb1f4039f4b8,LineCap: Line Charts for Data Visualization Captioning Models,0.924862,"Data visualization captions help readers understand the purpose of a
visualization and are crucial for individuals with visual impairments. The
prevalence of poor figure captions and the successful application of deep
learning approaches to image captioning motivate the use of similar techniques
for automated figure captioning. However, research in this field has been
stunted by the lack of suitable datasets. We introduce LineCap, a novel figure
captioning dataset of 3,528 figures, and we provide insights from curating this
dataset and using end-to-end deep learning models for automated figure
captioning.",https://github.com/anita76/LineCapDataset,-1
2251b670-6f57-480b-ac4a-757997ee233b,SPICEprop: Backpropagating Errors Through Memristive Spiking Neural Networks,0.244261,"We present a fully memristive spiking neural network (MSNN) consisting of
novel memristive neurons trained using the backpropagation through time (BPTT)
learning rule. Gradient descent is applied directly to the memristive
integrated-and-fire (MIF) neuron designed using analog SPICE circuit models,
which generates distinct depolarization, hyperpolarization, and repolarization
voltage waveforms. Synaptic weights are trained by BPTT using the membrane
potential of the MIF neuron model and can be processed on memristive crossbars.
The natural spiking dynamics of the MIF neuron model are fully differentiable,
eliminating the need for gradient approximations that are prevalent in the
spiking neural network literature. Despite the added complexity of training
directly on SPICE circuit models, we achieve 97.58% accuracy on the MNIST
testing dataset and 75.26% on the Fashion-MNIST testing dataset, the highest
accuracies among all fully MSNNs.",None,-1
920e2aff-984b-4708-b506-3fedd5ad223a,Question rewriting? Assessing its importance for conversational question answering,0.311409,"In conversational question answering, systems must correctly interpret the
interconnected interactions and generate knowledgeable answers, which may
require the retrieval of relevant information from a background repository.
Recent approaches to this problem leverage neural language models, although
different alternatives can be considered in terms of modules for (a)
representing user questions in context, (b) retrieving the relevant background
information, and (c) generating the answer. This work presents a conversational
question answering system designed specifically for the Search-Oriented
Conversational AI (SCAI) shared task, and reports on a detailed analysis of its
question rewriting module. In particular, we considered different variations of
the question rewriting module to evaluate the influence on the subsequent
components, and performed a careful analysis of the results obtained with the
best system configuration. Our system achieved the best performance in the
shared task and our analysis emphasizes the importance of the conversation
context representation for the overall system performance.",None,-1
80ff701b-bc14-4cbd-b835-bde928341c30,"DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis",0.971302,"Discriminative learning, restorative learning, and adversarial learning have
proven beneficial for self-supervised learning schemes in computer vision and
medical imaging. Existing efforts, however, omit their synergistic effects on
each other in a ternary setup, which, we envision, can significantly benefit
deep semantic representation learning. To realize this vision, we have
developed DiRA, the first framework that unites discriminative, restorative,
and adversarial learning in a unified manner to collaboratively glean
complementary visual information from unlabeled medical images for fine-grained
semantic representation learning. Our extensive experiments demonstrate that
DiRA (1) encourages collaborative learning among three learning ingredients,
resulting in more generalizable representation across organs, diseases, and
modalities; (2) outperforms fully supervised ImageNet models and increases
robustness in small data regimes, reducing annotation cost across multiple
medical imaging applications; (3) learns fine-grained semantic representation,
facilitating accurate lesion localization with only image-level annotation; and
(4) enhances state-of-the-art restorative approaches, revealing that DiRA is a
general mechanism for united representation learning. All code and pre-trained
models are available at https: //github.com/JLiangLab/DiRA.",https://github.com/JLiangLab/DiRA,-1
1aac90af-e73c-47bb-9228-1e0bdfbb9f01,HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crisis Response,0.42079,"Timely and effective response to humanitarian crises requires quick and
accurate analysis of large amounts of text data - a process that can highly
benefit from expert-assisted NLP systems trained on validated and annotated
data in the humanitarian response domain. To enable creation of such NLP
systems, we introduce and release HumSet, a novel and rich multilingual dataset
of humanitarian response documents annotated by experts in the humanitarian
response community. The dataset provides documents in three languages (English,
French, Spanish) and covers a variety of humanitarian crises from 2018 to 2021
across the globe. For each document, HUMSET provides selected snippets
(entries) as well as assigned classes to each entry annotated using common
humanitarian information analysis frameworks. HUMSET also provides novel and
challenging entry extraction and multi-label entry classification tasks. In
this paper, we take a first step towards approaching these tasks and conduct a
set of experiments on Pre-trained Language Models (PLM) to establish strong
baselines for future research in this domain. The dataset is available at
https://blog.thedeep.io/humset/.",https://github.com/the-deep/humset,-1
36e55999-1663-424a-b9cf-0298c3298505,"Less Data, More Knowledge: Building Next Generation Semantic Communication Networks",0.999941,"Semantic communication is viewed as a revolutionary paradigm that can
potentially transform how we design and operate wireless communication systems.
However, despite a recent surge of research activities in this area, the
research landscape remains limited. In this tutorial, we present the first
rigorous vision of a scalable end-to-end semantic communication network that is
founded on novel concepts from artificial intelligence (AI), causal reasoning,
and communication theory. We first discuss how the design of semantic
communication networks requires a move from data-driven networks towards
knowledge-driven ones. Subsequently, we highlight the necessity of creating
semantic representations of data that satisfy the key properties of minimalism,
generalizability, and efficiency so as to do more with less. We then explain
how those representations can form the basis a so-called semantic language. By
using semantic representation and languages, we show that the traditional
transmitter and receiver now become a teacher and apprentice. Then, we define
the concept of reasoning by investigating the fundamentals of causal
representation learning and their role in designing semantic communication
networks. We demonstrate that reasoning faculties are majorly characterized by
the ability to capture causal and associational relationships in datastreams.
For such reasoning-driven networks, we propose novel and essential semantic
communication metrics that include new ""reasoning capacity"" measures that could
go beyond Shannon's bound to capture the convergence of computing and
communication. Finally, we explain how semantic communications can be scaled to
large-scale networks (6G and beyond). In a nutshell, we expect this tutorial to
provide a comprehensive reference on how to properly build, analyze, and deploy
future semantic communication networks.",None,70013
336e0c8f-81d5-4ead-83e6-61ca07550d6c,In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models,0.349714,"Given the success with in-context learning of large pre-trained language
models, we introduce in-context learning distillation to transfer in-context
few-shot learning ability from large models to smaller models. We propose to
combine in-context learning objectives with language modeling objectives to
distill both the ability to read in-context examples and task knowledge to the
smaller models. We perform in-context learning distillation under two different
few-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask
In-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask
few-shot learning but also requires more computation than Meta-ICT. Our method
shows consistent improvements for both Meta-ICT and Multitask-ICT on two
benchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal
that in-context learning objectives and language modeling objectives are
complementary under the Multitask-ICT paradigm. In-context learning objectives
achieve the best performance when combined with language modeling objectives.",None,30354
193590bc-21e9-403d-9b80-e1b649ea3a7f,Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks,0.637302,"Early diagnosis and intervention are clinically considered the paramount part
of treating cerebral palsy (CP), so it is essential to design an efficient and
interpretable automatic prediction system for CP. We highlight a significant
difference between CP infants' frequency of human movement and that of the
healthy group, which improves prediction performance. However, the existing
deep learning-based methods did not use the frequency information of infants'
movement for CP prediction. This paper proposes a frequency attention informed
graph convolutional network and validates it on two consumer-grade RGB video
datasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency
attention module aids in improving both classification performance and system
interpretability. In addition, we design a frequency-binning method that
retains the critical frequency of the human joint position data while filtering
the noise. Our prediction performance achieves state-of-the-art research on
both datasets. Our work demonstrates the effectiveness of frequency information
in supporting the prediction of CP non-intrusively and provides a way for
supporting the early diagnosis of CP in the resource-limited regions where the
clinical resources are not abundant.",https://github.com/zhz95/FAIGCN,3498
6ca9c61d-a806-4204-b771-b6792b6560cb,SC-wLS: Towards Interpretable Feed-forward Camera Re-localization,0.859523,"Visual re-localization aims to recover camera poses in a known environment,
which is vital for applications like robotics or augmented reality.
Feed-forward absolute camera pose regression methods directly output poses by a
network, but suffer from low accuracy. Meanwhile, scene coordinate based
methods are accurate, but need iterative RANSAC post-processing, which brings
challenges to efficient end-to-end training and inference. In order to have the
best of both worlds, we propose a feed-forward method termed SC-wLS that
exploits all scene coordinate estimates for weighted least squares pose
regression. This differentiable formulation exploits a weight network imposed
on 2D-3D correspondences, and requires pose supervision only. Qualitative
results demonstrate the interpretability of learned weights. Evaluations on
7Scenes and Cambridge datasets show significantly promoted performance when
compared with former feed-forward counterparts. Moreover, our SC-wLS method
enables a new capability: self-supervised test-time adaptation on the weight
network. Codes and models are publicly available.",https://github.com/XinWu98/SC-wLSAbstract.Visualre-localizationaimstorecovercameraposesinaknownenvironment,-1
637c37ff-13ec-4749-bb2b-616b3de94409,Deep Reinforcement Learning for Stabilization of Large-scale Probabilistic Boolean Networks,0.35901,"The ability to direct a Probabilistic Boolean Network (PBN) to a desired
state is important to applications such as targeted therapeutics in cancer
biology. Reinforcement Learning (RL) has been proposed as a framework that
solves a discrete-time optimal control problem cast as a Markov Decision
Process. We focus on an integrative framework powered by a model-free deep RL
method that can address different flavours of the control problem (e.g., with
or without control inputs; attractor state or a subset of the state space as
the target domain). The method is agnostic to the distribution of probabilities
for the next state, hence it does not use the probability transition matrix.
The time complexity is linear on the time steps, or interactions between the
agent (deep RL) and the environment (PBN), during training. Indeed, we explore
the scalability of the deep RL approach to (set) stabilization of large-scale
PBNs and demonstrate successful control on large networks, including a
metastatic melanoma PBN with 200 nodes.",https://github.com/UoS-PLCCN/pbn-rl/,-1
2a87a284-2a97-4c6c-a825-fc566ac0e319,Boilerplate Detection via Semantic Classification of TextBlocks,0.0243924,"We present a hierarchical neural network model called SemText to detect HTML
boilerplate based on a novel semantic representation of HTML tags, class names,
and text blocks. We train SemText on three published datasets of news webpages
and fine-tune it using a small number of development data in CleanEval and
GoogleTrends-2017. We show that SemText achieves the state-of-the-art accuracy
on these datasets. We then demonstrate the robustness of SemText by showing
that it also detects boilerplate effectively on out-of-domain community-based
question-answer webpages.",https://github.com/dreamlegends/Semtext,-1
51cabbe3-d806-44be-a6b2-d1c6805ac2bf,Generative Cooperative Learning for Unsupervised Video Anomaly Detection,0.980096,"Video anomaly detection is well investigated in weakly-supervised and
one-class classification (OCC) settings. However, unsupervised video anomaly
detection methods are quite sparse, likely because anomalies are less frequent
in occurrence and usually not well-defined, which when coupled with the absence
of ground truth supervision, could adversely affect the performance of the
learning algorithms. This problem is challenging yet rewarding as it can
completely eradicate the costs of obtaining laborious annotations and enable
such systems to be deployed without human intervention. To this end, we propose
a novel unsupervised Generative Cooperative Learning (GCL) approach for video
anomaly detection that exploits the low frequency of anomalies towards building
a cross-supervision between a generator and a discriminator. In essence, both
networks get trained in a cooperative fashion, thereby allowing unsupervised
learning. We conduct extensive experiments on two large-scale video anomaly
detection datasets, UCF crime, and ShanghaiTech. Consistent improvement over
the existing state-of-the-art unsupervised and OCC methods corroborate the
effectiveness of our approach.",None,-1
506a7d46-88dd-433c-8698-eee9521cab9b,Self-Configurable Stabilized Real-Time Detection Learning for Autonomous Driving Applications,0.0828189,"Guaranteeing real-time and accurate object detection simultaneously is
paramount in autonomous driving environments. However, the existing object
detection neural network systems are characterized by a tradeoff between
computation time and accuracy, making it essential to optimize such a tradeoff.
Fortunately, in many autonomous driving environments, images come in a
continuous form, providing an opportunity to use optical flow. In this paper,
we improve the performance of an object detection neural network utilizing
optical flow estimation. In addition, we propose a Lyapunov optimization
framework for time-average performance maximization subject to stability. It
adaptively determines whether to use optical flow to suit the dynamic vehicle
environment, thereby ensuring the vehicle's queue stability and the
time-average maximum performance simultaneously. To verify the key ideas, we
conduct numerical experiments with various object detection neural networks and
optical flow estimation networks. In addition, we demonstrate the
self-configurable stabilized detection with YOLOv3-tiny and FlowNet2-S, which
are the real-time object detection network and an optical flow estimation
network, respectively. In the demonstration, our proposed framework improves
the accuracy by 3.02%, the number of detected objects by 59.6%, and the queue
stability for computing capabilities.",None,-1
7954be40-1d7e-4bbb-91b7-17e0c8b83003,Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors,0.656093,"Robustness of machine learning models on ever-changing real-world data is
critical, especially for applications affecting human well-being such as
content moderation. New kinds of abusive language continually emerge in online
discussions in response to current events (e.g., COVID-19), and the deployed
abuse detection systems should be updated regularly to remain accurate. In this
paper, we show that general abusive language classifiers tend to be fairly
reliable in detecting out-of-domain explicitly abusive utterances but fail to
detect new types of more subtle, implicit abuse. Next, we propose an
interpretability technique, based on the Testing Concept Activation Vector
(TCAV) method from computer vision, to quantify the sensitivity of a trained
model to the human-defined concepts of explicit and implicit abusive language,
and use that to explain the generalizability of the model on new data, in this
case, COVID-related anti-Asian hate speech. Extending this technique, we
introduce a novel metric, Degree of Explicitness, for a single instance and
show that the new metric is beneficial in suggesting out-of-domain unlabeled
examples to effectively enrich the training data with informative, implicitly
abusive texts.",https://github.com/IsarNejad/TCAV-for-Text-Classifiers,-1
dad3f858-1e4a-4f22-8aa5-28f898450ea9,gBuilder: A Scalable Knowledge Graph Construction System for Unstructured Corpus,0.35939,"We design a user-friendly and scalable knowledge graph construction (KGC)
system for extracting structured knowledge from the unstructured corpus.
Different from existing KGC systems, gBuilder provides a flexible and
user-defined pipeline to embrace the rapid development of IE models. More
built-in template-based or heuristic operators and programmable operators are
available for adapting to data from different domains. Furthermore, we also
design a cloud-based self-adaptive task scheduling for gBuilder to ensure its
scalability on large-scale knowledge graph construction. Experimental
evaluation demonstrates the ability of gBuilder to organize multiple
information extraction models for knowledge graph construction in a uniform
platform, and confirms its high scalability on large-scale KGC tasks.",None,-1
5f3ef8ff-dd06-4c16-90b8-545d0de0c37f,Semeval-2022 Task 1: CODWOE -- Comparing Dictionaries and Word Embeddings,0.908236,"Word embeddings have advanced the state of the art in NLP across numerous
tasks. Understanding the contents of dense neural representations is of utmost
interest to the computational semantics community. We propose to focus on
relating these opaque word vectors with human-readable definitions, as found in
dictionaries. This problem naturally divides into two subtasks: converting
definitions into embeddings, and converting embeddings into definitions. This
task was conducted in a multilingual setting, using comparable sets of
embeddings trained homogeneously.",https://github.com/pi,-1
9bebb991-d8d5-48dc-a611-e1a84631d721,Open-Ended Diverse Solution Discovery with Regulated Behavior Patterns for Cross-Domain Adaptation,0.136014,"While Reinforcement Learning can achieve impressive results for complex
tasks, the learned policies are generally prone to fail in downstream tasks
with even minor model mismatch or unexpected perturbations. Recent works have
demonstrated that a policy population with diverse behavior characteristics can
generalize to downstream environments with various discrepancies. However, such
policies might result in catastrophic damage during the deployment in practical
scenarios like real-world systems due to the unrestricted behaviors of trained
policies. Furthermore, training diverse policies without regulation of the
behavior can result in inadequate feasible policies for extrapolating to a wide
range of test conditions with dynamics shifts. In this work, we aim to train
diverse policies under the regularization of the behavior patterns. We motivate
our paradigm by observing the inverse dynamics in the environment with partial
state information and propose Diversity in Regulation (DiR) training diverse
policies with regulated behaviors to discover desired patterns that benefit the
generalization. Considerable empirical results on various variations of
different environments indicate that our method attains improvements over other
diversity-driven counterparts.",None,-1
f3ea3b0c-b346-45cc-9b64-57492aad986f,Attentive Task Interaction Network for Multi-Task Learning,0.0207436,"Multitask learning (MTL) has recently gained a lot of popularity as a
learning paradigm that can lead to improved per-task performance while also
using fewer per-task model parameters compared to single task learning. One of
the biggest challenges regarding MTL networks involves how to share features
across tasks. To address this challenge, we propose the Attentive Task
Interaction Network (ATI-Net). ATI-Net employs knowledge distillation of the
latent features for each task, then combines the feature maps to provide
improved contextualized information to the decoder. This novel approach to
introducing knowledge distillation into an attention based multitask network
outperforms state of the art MTL baselines such as the standalone MTAN and
PAD-Net, with roughly the same number of model parameters.",https://github.com/Armanfard-Lab/ATI-Net,-1
36767f8c-a120-4a4d-bd3b-9103da170780,Roadmap for Cybersecurity in Autonomous Vehicles,0.824327,"Autonomous vehicles are on the horizon and will be transforming
transportation safety and comfort. These vehicles will be connected to various
external systems and utilize advanced embedded systems to perceive their
environment and make intelligent decisions. However, this increased
connectivity makes these vehicles vulnerable to various cyber-attacks that can
have catastrophic effects. Attacks on automotive systems are already on the
rise in today's vehicles and are expected to become more commonplace in future
autonomous vehicles. Thus, there is a need to strengthen cybersecurity in
future autonomous vehicles. In this article, we discuss major automotive
cyber-attacks over the past decade and present state-of-the-art solutions that
leverage artificial intelligence (AI). We propose a roadmap towards building
secure autonomous vehicles and highlight key open challenges that need to be
addressed.",None,-1
3b8f3a22-9832-4dad-a42c-cbb6cfeaeaa9,GBSVM: Granular-ball Support Vector Machine,0.890357,"GBSVM (Granular-ball Support Vector Machine) is a significant attempt to
construct a classifier using the coarse-to-fine granularity of a granular-ball
as input, rather than a single data point. It is the first classifier whose
input contains no points. However, the existing model has some errors, and its
dual model has not been derived. As a result, the current algorithm cannot be
implemented or applied. To address these problems, this paper has fixed the
errors of the original model of the existing GBSVM, and derived its dual model.
Furthermore, a particle swarm optimization algorithm is designed to solve the
dual model. The sequential minimal optimization algorithm is also carefully
designed to solve the dual model. The solution is faster and more stable than
the particle swarm optimization based version. The experimental results on the
UCI benchmark datasets demonstrate that GBSVM has good robustness and
efficiency. All codes have been released in the open source library at
http://www.cquptshuyinxia.com/GBSVM.html or https://github.com/syxiaa/GBSVM.",https://github.com/syxiaa/GBSVM,-1
734c517d-3f54-44b5-a378-b3e38450bd36,Policy Distillation with Selective Input Gradient Regularization for Efficient Interpretability,0.150804,"Although deep Reinforcement Learning (RL) has proven successful in a wide
range of tasks, one challenge it faces is interpretability when applied to
real-world problems. Saliency maps are frequently used to provide
interpretability for deep neural networks. However, in the RL domain, existing
saliency map approaches are either computationally expensive and thus cannot
satisfy the real-time requirement of real-world scenarios or cannot produce
interpretable saliency maps for RL policies. In this work, we propose an
approach of Distillation with selective Input Gradient Regularization (DIGR)
which uses policy distillation and input gradient regularization to produce new
policies that achieve both high interpretability and computation efficiency in
generating saliency maps. Our approach is also found to improve the robustness
of RL policies to multiple adversarial attacks. We conduct experiments on three
tasks, MiniGrid (Fetch Object), Atari (Breakout) and CARLA Autonomous Driving,
to demonstrate the importance and effectiveness of our approach.",None,-1
7eed49f1-8fba-4ebb-8388-0d262b5b4c83,ROMA: Run-Time Object Detection To Maximize Real-Time Accuracy,0.126995,"This paper analyzes the effects of dynamically varying video contents and
detection latency on the real-time detection accuracy of a detector and
proposes a new run-time accuracy variation model, ROMA, based on the findings
from the analysis. ROMA is designed to select an optimal detector out of a set
of detectors in real time without label information to maximize real-time
object detection accuracy. ROMA utilizing four YOLOv4 detectors on an NVIDIA
Jetson Nano shows real-time accuracy improvements by 4 to 37% for a scenario of
dynamically varying video contents and detection latency consisting of MOT17Det
and MOT20Det datasets, compared to individual YOLOv4 detectors and two
state-of-the-art runtime techniques.",None,-1
6cf91090-9e3a-4929-bc78-1ee0b53955d1,Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual Understanding With Multilingual Language Models,0.188062,"Pre-trained multilingual language models show significant performance gains
for zero-shot cross-lingual model transfer on a wide range of natural language
understanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation,
pre-trained models are only fine-tuned on English data and tested on a variety
of target languages. In this paper, we do cross-lingual evaluation on various
NLU tasks (sentence classification, sequence labeling, question answering)
using prompt-tuning and compare it with fine-tuning. The results show that
prompt tuning achieves much better cross-lingual transfer than fine-tuning
across datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we
demonstrate through the analysis that prompt tuning can have better
cross-lingual transferability of representations on downstream tasks with
better aligned decision boundaries.",https://github.com/salesforce/MPT,-1
8a59b0ee-a9ff-4a16-bca6-73995dacb41a,Attention-Aware Anime Line Drawing Colorization,0.474212,"Automatic colorization of anime line drawing has attracted much attention in
recent years since it can substantially benefit the animation industry.
User-hint based methods are the mainstream approach for line drawing
colorization, while reference-based methods offer a more intuitive approach.
Nevertheless, although reference-based methods can improve feature aggregation
of the reference image and the line drawing, the colorization results are not
compelling in terms of color consistency or semantic correspondence. In this
paper, we introduce an attention-based model for anime line drawing
colorization, in which a channel-wise and spatial-wise Convolutional Attention
module is used to improve the ability of the encoder for feature extraction and
key area perception, and a Stop-Gradient Attention module with cross-attention
and self-attention is used to tackle the cross-domain long-range dependency
problem. Extensive experiments show that our method outperforms other SOTA
methods, with more accurate line structure and semantic color information.",None,-1
0f362834-1f48-45dc-b48e-5c1af6e0677a,"The Conversational Short-phrase Speaker Diarization (CSSD) Task: Dataset, Evaluation Metric and Baselines",0.986236,"The conversation scenario is one of the most important and most challenging
scenarios for speech processing technologies because people in conversation
respond to each other in a casual style. Detecting the speech activities of
each person in a conversation is vital to downstream tasks, like natural
language processing, machine translation, etc. People refer to the detection
technology of ""who speak when"" as speaker diarization (SD). Traditionally,
diarization error rate (DER) has been used as the standard evaluation metric of
SD systems for a long time. However, DER fails to give enough importance to
short conversational phrases, which are short but important on the semantic
level. Also, a carefully and accurately manually-annotated testing dataset
suitable for evaluating the conversational SD technologies is still unavailable
in the speech community. In this paper, we design and describe the
Conversational Short-phrases Speaker Diarization (CSSD) task, which consists of
training and testing datasets, evaluation metric and baselines. In the dataset
aspect, despite the previously open-sourced 180-hour conversational
MagicData-RAMC dataset, we prepare an individual 20-hour conversational speech
test dataset with carefully and artificially verified speakers timestamps
annotations for the CSSD task. In the metric aspect, we design the new
conversational DER (CDER) evaluation metric, which calculates the SD accuracy
at the utterance level. In the baseline aspect, we adopt a commonly used
method: Variational Bayes HMM x-vector system, as the baseline of the CSSD
task. Our evaluation metric is publicly available at
https://github.com/SpeechClub/CDER_Metric.",https://github.com/MagicHub-io/MagicData-RAMC,-1
3354607e-0fc9-4f51-9d37-23d526154a56,The ReturnZero System for VoxCeleb Speaker Recognition Challenge 2022,0.158419,"In this paper, we describe the top-scoring submissions for team RTZR VoxCeleb
Speaker Recognition Challenge 2022 (VoxSRC-22) in the closed dataset, speaker
verification Track 1. The top performed system is a fusion of 7 models, which
contains 3 different types of model architectures. We focus on training models
to learn extra-temporal information. Therefore, all models were trained with
4-6 second frames for each utterance. Also, we apply the Large Margin
Fine-tuning strategy which has shown good performance on the previous
challenges for some of our fusion models. While the evaluation process, we
apply the scoring methods with adaptive symmetric normalization (AS-Norm) and
matrix score average (MSA). Finally, we mix up models with logistic regression
to fuse all the trained models. The final submission achieves 0.165 DCF and
2.912% EER on the VoxSRC22 test set.",None,-1
10b7bd11-e93b-4ac9-bc99-838deb34a0c0,Generalizing to New Physical Systems via Context-Informed Dynamics Model,0.904573,"Data-driven approaches to modeling physical systems fail to generalize to
unseen systems that share the same general dynamics with the learning domain,
but correspond to different physical contexts. We propose a new framework for
this key problem, context-informed dynamics adaptation (CoDA), which takes into
account the distributional shift across systems for fast and efficient
adaptation to new dynamics. CoDA leverages multiple environments, each
associated to a different dynamic, and learns to condition the dynamics model
on contextual parameters, specific to each environment. The conditioning is
performed via a hypernetwork, learned jointly with a context vector from
observed data. The proposed formulation constrains the search hypothesis space
to foster fast adaptation and better generalization across environments. We
theoretically motivate our approach and show state-of-the-art generalization
results on a set of nonlinear dynamics, representative of a variety of
application domains. We also show, on these systems, that new system parameters
can be inferred from context vectors with minimal supervision. Code is
available at https://github.com/yuan-yin/CoDA .",None,-1
1fe82d85-cda2-442d-a4cb-24d96afc21dd,Hypergraph Convolutional Networks for Weakly-Supervised Semantic Segmentation,0.313599,"Semantic segmentation is a fundamental topic in computer vision. Several deep
learning methods have been proposed for semantic segmentation with outstanding
results. However, these models require a lot of densely annotated images. To
address this problem, we propose a new algorithm that uses HyperGraph
Convolutional Networks for Weakly-supervised Semantic Segmentation
(HyperGCN-WSS). Our algorithm constructs spatial and k-Nearest Neighbor (k-NN)
graphs from the images in the dataset to generate the hypergraphs. Then, we
train a specialized HyperGraph Convolutional Network (HyperGCN) architecture
using some weak signals. The outputs of the HyperGCN are denominated
pseudo-labels, which are later used to train a DeepLab model for semantic
segmentation. HyperGCN-WSS is evaluated on the PASCAL VOC 2012 dataset for
semantic segmentation, using scribbles or clicks as weak signals. Our algorithm
shows competitive performance against previous methods.",None,9488
b677dce4-eace-4538-a325-ec2c972d4c02,Direct parsing to sentiment graphs,0.389379,"This paper demonstrates how a graph-based semantic parser can be applied to
the task of structured sentiment analysis, directly predicting sentiment graphs
from text. We advance the state of the art on 4 out of 5 standard benchmark
sets. We release the source code, models and predictions.",https://github.com/jerbarnes/direct_parsing_to_sent_graph,-1
066d7de0-afac-46db-aa67-20b361be152f,Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities,0.639195,"As for other forms of AI, speech recognition has recently been examined with
respect to performance disparities across different user cohorts. One approach
to achieve fairness in speech recognition is to (1) identify speaker cohorts
that suffer from subpar performance and (2) apply fairness mitigation measures
targeting the cohorts discovered. In this paper, we report on initial findings
with both discovery and mitigation of performance disparities using data from a
product-scale AI assistant speech recognition system. We compare cohort
discovery based on geographic and demographic information to a more scalable
method that groups speakers without human labels, using speaker embedding
technology. For fairness mitigation, we find that oversampling of
underrepresented cohorts, as well as modeling speaker cohort membership by
additional input variables, reduces the gap between top- and bottom-performing
cohorts, without deteriorating overall recognition accuracy.",None,31243
ee2567a0-77a0-4445-92d1-b84ebe523b3d,Learning with Style: Continual Semantic Segmentation Across Tasks and Domains,0.167203,"Deep learning models dealing with image understanding in real-world settings
must be able to adapt to a wide variety of tasks across different domains.
Domain adaptation and class incremental learning deal with domain and task
variability separately, whereas their unified solution is still an open
problem. We tackle both facets of the problem together, taking into account the
semantic shift within both input and label spaces. We start by formally
introducing continual learning under task and domain shift. Then, we address
the proposed setup by using style transfer techniques to extend knowledge
across domains when learning incremental tasks and a robust distillation
framework to effectively recollect task knowledge under incremental domain
shift. The devised framework (LwS, Learning with Style) is able to generalize
incrementally acquired task knowledge across all the domains encountered,
proving to be robust against catastrophic forgetting. Extensive experimental
evaluation on multiple autonomous driving datasets shows how the proposed
method outperforms existing approaches, which prove to be ill-equipped to deal
with continual semantic segmentation under both task and domain shift.",None,-1
a7cc4f16-1ac4-47cc-a7d4-c76de47f3100,Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification,0.311788,"Most methods tackle zero-shot video classification by aligning
visual-semantic representations within seen classes, which limits
generalization to unseen classes. To enhance model generalizability, this paper
presents an end-to-end framework that preserves alignment and uniformity
properties for representations on both seen and unseen classes. Specifically,
we formulate a supervised contrastive loss to simultaneously align
visual-semantic features (i.e., alignment) and encourage the learned features
to distribute uniformly (i.e., uniformity). Unlike existing methods that only
consider the alignment, we propose uniformity to preserve maximal-info of
existing features, which improves the probability that unobserved features fall
around observed data. Further, we synthesize features of unseen classes by
proposing a class generator that interpolates and extrapolates the features of
seen classes. Besides, we introduce two metrics, closeness and dispersion, to
quantify the two properties and serve as new measurements of model
generalizability. Experiments show that our method significantly outperforms
SoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51. Code is
available.",https://github.com/ShipuLoveMili/CVPR2022-AURL,-1
24ca46c3-7071-42f3-8c7f-d2c08d01760c,Box Supervised Video Segmentation Proposal Network,0.123671,"Video Object Segmentation (VOS) has been targeted by various fully-supervised
and self-supervised approaches. While fully-supervised methods demonstrate
excellent results, self-supervised ones, which do not use pixel-level ground
truth, attract much attention. However, self-supervised approaches pose a
significant performance gap. Box-level annotations provide a balanced
compromise between labeling effort and result quality for image segmentation
but have not been exploited for the video domain. In this work, we propose a
box-supervised video object segmentation proposal network, which takes
advantage of intrinsic video properties. Our method incorporates object motion
in the following way: first, motion is computed using a bidirectional temporal
difference and a novel bounding box-guided motion compensation. Second, we
introduce a novel motion-aware affinity loss that encourages the network to
predict positive pixel pairs if they share similar motion and color. The
proposed method outperforms the state-of-the-art self-supervised benchmark by
16.4% and 6.9% $\mathcal{J}$ &$\mathcal{F}$ score and the majority of fully
supervised methods on the DAVIS and Youtube-VOS dataset without imposing
network architectural specifications. We provide extensive tests and ablations
on the datasets, demonstrating the robustness of our method.",https://github.com/Tanveer81/BoxVOS.git,4189
796fb83e-94d2-4bfd-8b83-04232ec932a9,A Study on Transformer Configuration and Training Objective,0.0337247,"Transformer-based models have delivered impressive results on many tasks,
particularly vision and language tasks. In many model training situations,
conventional configurations are typically adopted. For example, we often set
the base model with hidden dimensions (i.e. model width) to be 768 and the
number of transformer layers (i.e. model depth) to be 12. In this paper, we
revisit these conventional configurations. Through theoretical analysis and
experimental evaluation, we show that the masked autoencoder is effective in
alleviating the over-smoothing issue in deep transformer training. Based on
this finding, we propose Bamboo, an idea of using deeper and narrower
transformer configurations, for masked autoencoder training. On ImageNet, with
such a simple change in configuration, re-designed model achieves 87.1% top-1
accuracy and outperforms SoTA models like MAE and BEiT. On language tasks,
re-designed model outperforms BERT with default setting by 1.1 points on
average, on GLUE datasets.",None,-1
9a74cccf-1e5d-4f6c-97a2-7b7438c333e9,Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries,0.805202,"Knowledge graph (KG) embeddings have been a mainstream approach for reasoning
over incomplete KGs. However, limited by their inherently shallow and static
architectures, they can hardly deal with the rising focus on complex logical
queries, which comprise logical operators, imputed edges, multiple source
entities, and unknown intermediate entities. In this work, we present the
Knowledge Graph Transformer (kgTransformer) with masked pre-training and
fine-tuning strategies. We design a KG triple transformation method to enable
Transformer to handle KGs, which is further strengthened by the
Mixture-of-Experts (MoE) sparse activation. We then formulate the complex
logical queries as masked prediction and introduce a two-stage masked
pre-training strategy to improve transferability and generalizability.
Extensive experiments on two benchmarks demonstrate that kgTransformer can
consistently outperform both KG embedding-based baselines and advanced encoders
on nine in-domain and out-of-domain reasoning tasks. Additionally,
kgTransformer can reason with explainability via providing the full reasoning
paths to interpret given answers.",https://github.com/THUDM/kgTransformer,-1
4953dd6a-adc1-43d8-b712-1350fcf34ef1,SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models,0.787109,"Recent research showed promising results on combining pretrained language
models (LMs) with canonical utterance for few-shot semantic parsing. The
canonical utterance is often lengthy and complex due to the compositional
structure of formal languages. Learning to generate such canonical utterance
requires significant amount of data to reach high performance. Fine-tuning with
only few-shot samples, the LMs can easily forget pretrained knowledge, overfit
spurious biases, and suffer from compositionally out-of-distribution
generalization errors. To tackle these issues, we propose a novel few-shot
semantic parsing method -- SeqZero. SeqZero decomposes the problem into a
sequence of sub-problems, which correspond to the sub-clauses of the formal
language. Based on the decomposition, the LMs only need to generate short
answers using prompts for predicting sub-clauses. Thus, SeqZero avoids
generating a long canonical utterance at once. Moreover, SeqZero employs not
only a few-shot model but also a zero-shot model to alleviate the overfitting.
In particular, SeqZero brings out the merits from both models via ensemble
equipped with our proposed constrained rescaling. SeqZero achieves SOTA
performance of BART-based models on GeoQuery and EcommerceQuery, which are two
few-shot datasets with compositional data split.",https://github.com/amzn/SeqZero,-1
91295552-1030-447d-994a-06fe5130432b,AutoField: Automating Feature Selection in Deep Recommender Systems,0.835652,"Feature quality has an impactful effect on recommendation performance.
Thereby, feature selection is a critical process in developing deep
learning-based recommender systems. Most existing deep recommender systems,
however, focus on designing sophisticated neural networks, while neglecting the
feature selection process. Typically, they just feed all possible features into
their proposed deep architectures, or select important features manually by
human experts. The former leads to non-trivial embedding parameters and extra
inference time, while the latter requires plenty of expert knowledge and human
labor effort. In this work, we propose an AutoML framework that can adaptively
select the essential feature fields in an automatic manner. Specifically, we
first design a differentiable controller network, which is capable of
automatically adjusting the probability of selecting a particular feature
field; then, only selected feature fields are utilized to retrain the deep
recommendation model. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our framework. We conduct further experiments
to investigate its properties, including the transferability, key components,
and parameter sensitivity.",https://github.com/rixwew/pytorch-fm,-1
735b4e3b-eba9-40b2-9917-f8866a69eee9,Target-aware Molecular Graph Generation,0.680885,"Generating molecules with desired biological activities has attracted growing
attention in drug discovery. Previous molecular generation models are designed
as chemocentric methods that hardly consider the drug-target interaction,
limiting their practical applications. In this paper, we aim to generate
molecular drugs in a target-aware manner that bridges biological activity and
molecular design. To solve this problem, we compile a benchmark dataset from
several publicly available datasets and build baselines in a unified framework.
Building on the recent advantages of flow-based molecular generation models, we
propose SiamFlow, which forces the flow to fit the distribution of target
sequence embeddings in latent space. Specifically, we employ an alignment loss
and a uniform loss to bring target sequence embeddings and drug graph
embeddings into agreements while avoiding collapse. Furthermore, we formulate
the alignment into a one-to-many problem by learning spaces of target sequence
embeddings. Experiments quantitatively show that our proposed method learns
meaningful representations in the latent space toward the target-aware
molecular graph generation and provides an alternative approach to bridge
biology and chemistry in drug discovery.",None,-1
56358f29-650c-402a-ad9a-c27a7185695b,Towards Summary Candidates Fusion,0.848198,"Sequence-to-sequence deep neural models fine-tuned for abstractive
summarization can achieve great performance on datasets with enough human
annotations. Yet, it has been shown that they have not reached their full
potential, with a wide gap between the top beam search output and the oracle
beam. Recently, re-ranking methods have been proposed, to learn to select a
better summary candidate. However, such methods are limited by the summary
quality aspects captured by the first-stage candidates. To bypass this
limitation, we propose a new paradigm in second-stage abstractive summarization
called SummaFusion that fuses several summary candidates to produce a novel
abstractive second-stage summary. Our method works well on several
summarization datasets, improving both the ROUGE scores and qualitative
properties of fused summaries. It is especially good when the candidates to
fuse are worse, such as in the few-shot setup where we set a new
state-of-the-art. We will make our code and checkpoints available at
https://github.com/ntunlp/SummaFusion/.",https://github.com/ntunlp/SummaFusion/,-1
86230e6d-df16-4015-bfe2-dc32e20c35e9,ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields,0.47641,"While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline
methods with an emphasis on visual fidelity, our paper addresses the online use
case that prioritises real-time adaptability. We present ParticleNeRF, a new
approach that dynamically adapts to changes in the scene geometry by learning
an up-to-date representation online, every 200ms. ParticleNeRF achieves this
using a novel particle-based parametric encoding. We couple features to
particles in space and backpropagate the photometric reconstruction loss into
the particles' position gradients, which are then interpreted as velocity
vectors. Governed by a lightweight physics system to handle collisions, this
lets the features move freely with the changing scene geometry. We demonstrate
ParticleNeRF on various dynamic scenes containing translating, rotating,
articulated, and deformable objects. ParticleNeRF is the first online dynamic
NeRF and achieves fast adaptability with better visual fidelity than
brute-force online InstantNGP and other baseline approaches on dynamic scenes
with online constraints. Videos of our system can be found at our project
website https://sites.google.com/view/particlenerf.",None,-1
fe15aa11-21c6-43b6-8c12-e1b5c9169295,VIDM: Video Implicit Diffusion Models,0.927319,"Diffusion models have emerged as a powerful generative method for
synthesizing high-quality and diverse set of images. In this paper, we propose
a video generation method based on diffusion models, where the effects of
motion are modeled in an implicit condition manner, i.e. one can sample
plausible video motions according to the latent feature of frames. We improve
the quality of the generated videos by proposing multiple strategies such as
sampling space truncation, robustness penalty, and positional group
normalization. Various experiments are conducted on datasets consisting of
videos with different resolutions and different number of frames. Results show
that the proposed method outperforms the state-of-the-art generative
adversarial network-based methods by a significant margin in terms of FVD
scores as well as perceptible visual quality.",None,-1
4f4069c2-724e-43bf-bce6-40b575240534,Recovering Sign Bits of DCT Coefficients in Digital Images as an Optimization Problem,0.413621,"Recovering unknown, missing, damaged, distorted, or lost information in DCT
coefficients is a common task in multiple applications of digital image
processing, including image compression, selective image encryption, and image
communication. This paper investigates the recovery of sign bits in DCT
coefficients of digital images, by proposing two different approximation
methods to solve a mixed integer linear programming (MILP) problem, which is
NP-hard in general. One method is a relaxation of the MILP problem to a linear
programming (LP) problem, and the other splits the original MILP problem into
some smaller MILP problems and an LP problem. We considered how the proposed
methods can be applied to JPEG-encoded images and conducted extensive
experiments to validate their performances. The experimental results showed
that the proposed methods outperformed other existing methods by a substantial
margin, both according to objective quality metrics and our subjective
evaluation.",https://github.com/ChengqingLi/DCT_SBR,-1
b0c58dfa-3881-49d3-b8b8-56d0cd9f3388,NTULM: Enriching Social Media Text Representations with Non-Textual Units,0.408569,"On social media, additional context is often present in the form of
annotations and meta-data such as the post's author, mentions, Hashtags, and
hyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit
that NTUs provide social context beyond their textual semantics and leveraging
these units can enrich social media text representations. In this work we
construct an NTU-centric social heterogeneous network to co-embed NTUs. We then
principally integrate these NTU embeddings into a large pretrained language
model by fine-tuning with these additional units. This adds context to noisy
short-text social media. Experiments show that utilizing NTU-augmented text
representations significantly outperforms existing text-only baselines by 2-5\%
relative points on many downstream tasks highlighting the importance of context
to social media NLP. We also highlight that including NTU context into the
initial layers of language model alongside text is better than using it after
the text embedding is generated. Our work leads to the generation of holistic
general purpose social media content embedding.",None,-1
babd8a96-5252-49f8-94b1-b6b79257889a,Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal,0.708596,"Transformer-based large language models are trained to make predictions about
the next word by aggregating representations of previous tokens through their
self-attention mechanism. In the field of cognitive modeling, such attention
patterns have recently been interpreted as embodying the process of cue-based
retrieval, in which attention over multiple targets is taken to generate
interference and latency during retrieval. Under this framework, this work
first defines an entropy-based predictor that quantifies the diffuseness of
self-attention, as well as distance-based predictors that capture the
incremental change in attention patterns across timesteps. Moreover, following
recent studies that question the informativeness of attention weights, we also
experiment with alternative methods for incorporating vector norms into
attention weights. Regression experiments using predictors calculated from the
GPT-2 language model show that these predictors deliver a substantially better
fit to held-out self-paced reading and eye-tracking data over a rigorous
baseline including GPT-2 surprisal. Additionally, the distance-based predictors
generally demonstrated higher predictive power, with effect sizes of up to 6.59
ms per standard deviation on self-paced reading times (compared to 2.82 ms for
surprisal) and 1.05 ms per standard deviation on eye-gaze durations (compared
to 3.81 ms for surprisal).",https://github.com/byungdoh/attn_dist,-1
902fd11f-d835-432b-b1d5-e977a7de9fdb,SinTra: Learning an inspiration model from a single multi-track music segment,0.0239083,"In this paper, we propose SinTra, an auto-regressive sequential generative
model that can learn from a single multi-track music segment, to generate
coherent, aesthetic, and variable polyphonic music of multi-instruments with an
arbitrary length of bar. For this task, to ensure the relevance of generated
samples and training music, we present a novel pitch-group representation.
SinTra, consisting of a pyramid of Transformer-XL with a multi-scale training
strategy, can learn both the musical structure and the relative positional
relationship between notes of the single training music segment. Additionally,
for maintaining the inter-track correlation, we use the convolution operation
to process multi-track music, and when decoding, the tracks are independent to
each other to prevent interference. We evaluate SinTra with both subjective
study and objective metrics. The comparison results show that our framework can
learn information from a single music segment more sufficiently than Music
Transformer. Also the comparison between SinTra and its variant, i.e., the
single-stage SinTra with the first stage only, shows that the pyramid structure
can effectively suppress overly-fragmented notes.",https://github.com/qingweisong/SinTra,-1
1babafc4-a1c1-4868-a7cb-c27f87b17720,Heterogeneous Feature Distillation Network for SAR Image Semantic Segmentation,0.101298,"Semantic segmentation for SAR (Synthetic Aperture Radar) images has attracted
increasing attention in the remote sensing community recently, due to SAR's
all-time and all-weather imaging capability. However, SAR images are generally
more difficult to be segmented than their EO (Electro-Optical) counterparts,
since speckle noises and layovers are inevitably involved in SAR images. To
address this problem, we investigate how to introduce EO features to assist the
training of a SAR-segmentation model, and propose a heterogeneous feature
distillation network for segmenting SAR images, called HFD-Net, where a
SAR-segmentation student model gains knowledge from a pre-trained
EO-segmentation teacher model. In the proposed HFD-Net, both the student and
teacher models employ an identical architecture but different parameter
configurations, and a heterogeneous feature distillation model is explored for
transferring latent EO features from the teacher model to the student model and
then enhancing the ability of the student model for SAR image segmentation. In
addition, a heterogeneous feature alignment module is explored to aggregate
multi-scale features for segmentation in each of the student model and teacher
model. Extensive experimental results on two public datasets demonstrate that
the proposed HFD-Net outperforms seven state-of-the-art SAR image semantic
segmentation methods.",None,-1
0ae64fa5-f293-430c-96e0-bb2b4e33c997,LAMNER: Code Comment Generation Using Character Language Model and Named Entity Recognition,0.0289566,"Code comment generation is the task of generating a high-level natural
language description for a given code method or function. Although researchers
have been studying multiple ways to generate code comments automatically,
previous work mainly considers representing a code token in its entirety
semantics form only (e.g., a language model is used to learn the semantics of a
code token), and additional code properties such as the tree structure of a
code are included as an auxiliary input to the model. There are two
limitations: 1) Learning the code token in its entirety form may not be able to
capture information succinctly in source code, and 2) The code token does not
contain additional syntactic information, inherently important in programming
languages.
  In this paper, we present LAnguage Model and Named Entity Recognition
(LAMNER), a code comment generator capable of encoding code constructs
effectively and capturing the structural property of a code token. A
character-level language model is used to learn the semantic representation to
encode a code token. For the structural property of a token, a Named Entity
Recognition model is trained to learn the different types of code tokens. These
representations are then fed into an encoder-decoder architecture to generate
code comments. We evaluate the generated comments from LAMNER and other
baselines on a popular Java dataset with four commonly used metrics. Our
results show that LAMNER is effective and improves over the best baseline model
in BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, and CIDEr by 14.34%,
18.98%, 21.55%, 23.00%, 10.52%, 1.44%, and 25.86%, respectively. Additionally,
we fused LAMNER's code representation with the baseline models, and the fused
models consistently showed improvement over the non-fused models. The human
evaluation further shows that LAMNER produces high-quality code comments.",https://github.com/fardfh-lab/LAMNER,-1
1b47823c-8c5a-4249-bd13-178d0b74113c,Zero-shot Blind Image Denoising via Implicit Neural Representations,0.361606,"Recent denoising algorithms based on the ""blind-spot"" strategy show
impressive blind image denoising performances, without utilizing any external
dataset. While the methods excel in recovering highly contaminated images, we
observe that such algorithms are often less effective under a low-noise or real
noise regime. To address this gap, we propose an alternative denoising strategy
that leverages the architectural inductive bias of implicit neural
representations (INRs), based on our two findings: (1) INR tends to fit the
low-frequency clean image signal faster than the high-frequency noise, and (2)
INR layers that are closer to the output play more critical roles in fitting
higher-frequency parts. Building on these observations, we propose a denoising
algorithm that maximizes the innate denoising capability of INRs by penalizing
the growth of deeper layer weights. We show that our method outperforms
existing zero-shot denoising methods under an extensive set of low-noise or
real-noise scenarios.",None,11542
238737e5-eaf8-4922-a9a8-93f04ba71a6b,Sufficient Statistic Memory Approximate Message Passing,0.838186,"Approximate message passing (AMP) type algorithms have been widely used in
the signal reconstruction of certain large random linear systems. A key feature
of the AMP-type algorithms is that their dynamics can be correctly described by
state evolution. However, state evolution does not necessarily guarantee the
convergence of iterative algorithms. To solve the convergence problem of
AMP-type algorithms in principle, this paper proposes a memory AMP (MAMP) under
a sufficient statistic condition, named sufficient statistic MAMP (SS-MAMP). We
show that the covariance matrices of SS-MAMP are L-banded and convergent. Given
an arbitrary MAMP, we can construct the SS-MAMP by damping, which not only
ensures the convergence, but also preserves the orthogonality, i.e., its
dynamics can be correctly described by state evolution.",None,-1
32a5fa80-a61c-4f0e-b060-bdad75bc0b26,On the Influence of Explainable AI on Automation Bias,0.570767,"Artificial intelligence (AI) is gaining momentum, and its importance for the
future of work in many areas, such as medicine and banking, is continuously
rising. However, insights on the effective collaboration of humans and AI are
still rare. Typically, AI supports humans in decision-making by addressing
human limitations. However, it may also evoke human bias, especially in the
form of automation bias as an over-reliance on AI advice. We aim to shed light
on the potential to influence automation bias by explainable AI (XAI). In this
pre-test, we derive a research model and describe our study design.
Subsequentially, we conduct an online experiment with regard to hotel review
classifications and discuss first results. We expect our research to contribute
to the design and development of safe hybrid intelligence systems.",None,-1
408e6570-9c6d-40d6-91e0-ea679c307967,Mirror modular cloning and fast quantum associative retrieval,0.102354,"We show that a quantum state can be perfectly cloned up to global mirroring
with a unitary transformation that depends on one single parameter. We then
show that this is equivalent to ""perfect"" cloning for quantum associative
memories which, as a consequence efficiently hold exponentially more
information than their classical counterparts. Finally, we present a quantum
associative retrieval algorithm which can correct corrupted inputs and is
exponentially faster than the Grover algorithm.",None,-1
3a74d742-d8e0-4502-a07e-6b38b6015efe,Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation,0.264034,"Research on Automatic Story Generation (ASG) relies heavily on human and
automatic evaluation. However, there is no consensus on which human evaluation
criteria to use, and no analysis of how well automatic criteria correlate with
them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a
set of 6 orthogonal and comprehensive human criteria, carefully motivated by
the social sciences literature. We also present HANNA, an annotated dataset of
1,056 stories produced by 10 different ASG systems. HANNA allows us to
quantitatively evaluate the correlations of 72 automatic metrics with human
criteria. Our analysis highlights the weaknesses of current metrics for ASG and
allows us to formulate practical recommendations for ASG evaluation.",https://github.com/dig-team/hanna-benchmark-asg,3215
6c6e062a-e040-4832-93eb-c0b82ceb6450,HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders,0.217363,"In this paper, HeadPosr is proposed to predict the head poses using a single
RGB image. \textit{HeadPosr} uses a novel architecture which includes a
transformer encoder. In concrete, it consists of: (1) backbone; (2) connector;
(3) transformer encoder; (4) prediction head. The significance of using a
transformer encoder for HPE is studied. An extensive ablation study is
performed on varying the (1) number of encoders; (2) number of heads; (3)
different position embeddings; (4) different activations; (5) input channel
size, in a transformer used in HeadPosr. Further studies on using: (1)
different backbones, (2) using different learning rates are also shown. The
elaborated experiments and ablations studies are conducted using three
different open-source widely used datasets for HPE, i.e., 300W-LP, AFLW2000,
and BIWI datasets. Experiments illustrate that \textit{HeadPosr} outperforms
all the state-of-art methods including both the landmark-free and the others
based on using landmark or depth estimation on the AFLW2000 dataset and BIWI
datasets when trained with 300W-LP. It also outperforms when averaging the
results from the compared datasets, hence setting a benchmark for the problem
of HPE, also demonstrating the effectiveness of using transformers over the
state-of-the-art.",None,-1
cc1739ae-b485-415d-b9c8-b44b0dccd46b,PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection,0.796125,"Recent years have witnessed a trend of applying context frames to boost the
performance of object detection as video object detection. Existing methods
usually aggregate features at one stroke to enhance the feature. These methods,
however, usually lack spatial information from neighboring frames and suffer
from insufficient feature aggregation. To address the issues, we perform a
progressive way to introduce both temporal information and spatial information
for an integrated enhancement. The temporal information is introduced by the
temporal feature aggregation model (TFAM), by conducting an attention mechanism
between the context frames and the target frame (i.e., the frame to be
detected). Meanwhile, we employ a Spatial Transition Awareness Model (STAM) to
convey the location transition information between each context frame and
target frame. Built upon a transformer-based detector DETR, our PTSEFormer also
follows an end-to-end fashion to avoid heavy post-processing procedures while
achieving 88.1% mAP on the ImageNet VID dataset. Codes are available at
https://github.com/Hon-Wong/PTSEFormer.",https://github.com/Hon-Wong/PTSEFormer,-1
4c3dc8c2-bb75-4891-9a01-79f2a27f6ad1,Variable Bitrate Neural Fields,0.999999,"Neural approximations of scalar and vector fields, such as signed distance
functions and radiance fields, have emerged as accurate, high-quality
representations. State-of-the-art results are obtained by conditioning a neural
approximation with a lookup from trainable feature grids that take on part of
the learning task and allow for smaller, more efficient neural networks.
Unfortunately, these feature grids usually come at the cost of significantly
increased memory consumption compared to stand-alone neural network models. We
present a dictionary method for compressing such feature grids, reducing their
memory consumption by up to 100x and permitting a multiresolution
representation which can be useful for out-of-core streaming. We formulate the
dictionary optimization as a vector-quantized auto-decoder problem which lets
us learn end-to-end discrete neural representations in a space where no direct
supervision is available and with dynamic topology and structure. Our source
code will be available at https://github.com/nv-tlabs/vqad.",https://github.com/nv-tlabs/nglod,-1
978f0187-d03b-42c3-a378-c58ab0d8556e,Learning Control Admissibility Models with Graph Neural Networks for Multi-Agent Navigation,0.862066,"Deep reinforcement learning in continuous domains focuses on learning control
policies that map states to distributions over actions that ideally concentrate
on the optimal choices in each step. In multi-agent navigation problems, the
optimal actions depend heavily on the agents' density. Their interaction
patterns grow exponentially with respect to such density, making it hard for
learning-based methods to generalize. We propose to switch the learning
objectives from predicting the optimal actions to predicting sets of admissible
actions, which we call control admissibility models (CAMs), such that they can
be easily composed and used for online inference for an arbitrary number of
agents. We design CAMs using graph neural networks and develop training methods
that optimize the CAMs in the standard model-free setting, with the additional
benefit of eliminating the need for reward engineering typically required to
balance collision avoidance and goal-reaching requirements. We evaluate the
proposed approach in multi-agent navigation environments. We show that the CAM
models can be trained in environments with only a few agents and be easily
composed for deployment in dense environments with hundreds of agents,
achieving better performance than state-of-the-art methods.",None,-1
77bc1e1a-63ba-4411-8aa7-de42e5bf599f,"MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction",0.979835,"The diverse relationships among real-world events, including coreference,
temporal, causal, and subevent relations, are fundamental to understanding
natural languages. However, two drawbacks of existing datasets limit event
relation extraction (ERE) tasks: (1) Small scale. Due to the annotation
complexity, the data scale of existing datasets is limited, which cannot well
train and evaluate data-hungry models. (2) Absence of unified annotation.
Different types of event relations naturally interact with each other, but
existing datasets only cover limited relation types at once, which prevents
models from taking full advantage of relation interactions. To address these
issues, we construct a unified large-scale human-annotated ERE dataset
MAVEN-ERE with improved annotation schemes. It contains 103,193 event
coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and
15,841 subevent relations, which is larger than existing datasets of all the
ERE tasks by at least an order of magnitude. Experiments show that ERE on
MAVEN-ERE is quite challenging, and considering relation interactions with
joint learning can improve performances. The dataset and source codes can be
obtained from https://github.com/THU-KEG/MAVEN-ERE.",https://github.com/THU-KEG/MAVEN-ERE,-1
8741a635-6805-478a-9750-ddd8c7d7ec6e,AnyFace: Free-style Text-to-Face Synthesis and Manipulation,0.994248,"Existing text-to-image synthesis methods generally are only applicable to
words in the training dataset. However, human faces are so variable to be
described with limited words. So this paper proposes the first free-style
text-to-face method namely AnyFace enabling much wider open world applications
such as metaverse, social media, cosmetics, forensics, etc. AnyFace has a novel
two-stream framework for face image synthesis and manipulation given arbitrary
descriptions of the human face. Specifically, one stream performs text-to-face
generation and the other conducts face image reconstruction. Facial text and
image features are extracted using the CLIP (Contrastive Language-Image
Pre-training) encoders. And a collaborative Cross Modal Distillation (CMD)
module is designed to align the linguistic and visual features across these two
streams. Furthermore, a Diverse Triplet Loss (DT loss) is developed to model
fine-grained features and improve facial diversity. Extensive experiments on
Multi-modal CelebA-HQ and CelebAText-HQ demonstrate significant advantages of
AnyFace over state-of-the-art methods. AnyFace can achieve high-quality,
high-resolution, and high-diversity face synthesis and manipulation results
without any constraints on the number and content of input captions.",None,-1
75e00f30-ae78-40d0-9080-163e52f6c27f,Near Perfect GAN Inversion,0.430688,"To edit a real photo using Generative Adversarial Networks (GANs), we need a
GAN inversion algorithm to identify the latent vector that perfectly reproduces
it. Unfortunately, whereas existing inversion algorithms can synthesize images
similar to real photos, they cannot generate the identical clones needed in
most applications. Here, we derive an algorithm that achieves near perfect
reconstructions of photos. Rather than relying on encoder- or
optimization-based methods to find an inverse mapping on a fixed generator
$G(\cdot)$, we derive an approach to locally adjust $G(\cdot)$ to more
optimally represent the photos we wish to synthesize. This is done by locally
tweaking the learned mapping $G(\cdot)$ s.t. $\| {\bf x} - G({\bf z})
\|<\epsilon$, with ${\bf x}$ the photo we wish to reproduce, ${\bf z}$ the
latent vector, $\|\cdot\|$ an appropriate metric, and $\epsilon > 0$ a small
scalar. We show that this approach can not only produce synthetic images that
are indistinguishable from the real photos we wish to replicate, but that these
images are readily editable. We demonstrate the effectiveness of the derived
algorithm on a variety of datasets including human faces, animals, and cars,
and discuss its importance for diversity and inclusion.",https://github.com/davisking/dlibofStyleGAN2,-1
1da933f4-e005-4840-8825-98678f25d14b,SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller,0.698318,"In this paper, we propose a new task of sub-event generation for an unseen
process to evaluate the understanding of the coherence of sub-event actions and
objects. To solve the problem, we design SubeventWriter, a sub-event sequence
generation framework with a coherence controller. Given an unseen process, the
framework can iteratively construct the sub-event sequence by generating one
sub-event at each iteration. We also design a very effective coherence
controller to decode more coherent sub-events. As our extensive experiments and
analysis indicate, SubeventWriter can generate more reliable and meaningful
sub-event sequences for unseen processes.",https://github.com/HKUST-KnowComp/SubeventWriter,-1
d92f02be-1c11-431d-af78-91f6646cf2bc,Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment,0.478116,"Despite recent progress towards scaling up multimodal vision-language models,
these models are still known to struggle on compositional generalization
benchmarks such as Winoground. We find that a critical component lacking from
current vision-language models is relation-level alignment: the ability to
match directional semantic relations in text (e.g., ""mug in grass"") with
spatial relationships in the image (e.g., the position of the mug relative to
the grass). To tackle this problem, we show that relation alignment can be
enforced by encouraging the directed language attention from 'mug' to 'grass'
(capturing the semantic relation 'in') to match the directed visual attention
from the mug to the grass. Tokens and their corresponding objects are softly
identified using the cross-modal attention. We prove that this notion of soft
relation alignment is equivalent to enforcing congruence between vision and
language attention matrices under a 'change of basis' provided by the
cross-modal attention matrix. Intuitively, our approach projects visual
attention into the language attention space to calculate its divergence from
the actual language attention, and vice versa. We apply our Cross-modal
Attention Congruence Regularization (CACR) loss to UNITER and improve on the
state-of-the-art approach to Winoground.",None,-1
a2e54acc-afb9-46cb-9910-4123f04f19f9,Personalized Federated Learning for Multi-task Fault Diagnosis of Rotating Machinery,0.42607,"Intelligent fault diagnosis is essential to safe operation of machinery.
However, due to scarce fault samples and data heterogeneity in field machinery,
deep learning based diagnosis methods are prone to over-fitting with poor
generalization ability. To solve the problem, this paper proposes a
personalized federated learning framework, enabling multi-task fault diagnosis
method across multiple factories in a privacypreserving manner. Firstly,
rotating machines from different factories with similar vibration feature data
are categorized into machine groups using a federated clustering method. Then,
a multi-task deep learning model based on convolutional neural network is
constructed to diagnose the multiple faults of machinery with heterogeneous
information fusion. Finally, a personalized federated learning framework is
proposed to solve data heterogeneity across different machines using adaptive
hierarchical aggregation strategy. The case study on collected data from real
machines verifies the effectiveness of the proposed framework. The result shows
that the diagnosis accuracy could be improved significantly using the proposed
personalized federated learning, especially for those machines with scarce
fault samples.",None,-1
216e6838-dc41-4f6f-a3a7-abc6f0618e45,Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity,0.817573,"This paper analyzes three formal models of Transformer encoders that differ
in the form of their self-attention mechanism: unique hard attention (UHAT);
generalized unique hard attention (GUHAT), which generalizes UHAT; and
averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers,
viewed as string acceptors, can only recognize formal languages in the
complexity class AC$^0$, the class of languages recognizable by families of
Boolean circuits of constant depth and polynomial size. This upper bound
subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages
or the PARITY language, since those languages are outside AC$^0$ (Furst et al.,
1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are
recognizable by AHAT networks, implying that AHAT can recognize languages that
UHAT and GUHAT cannot.",None,20886
d4a930e3-f1af-4d09-91a0-81803b97887a,Rhino: Deep Causal Temporal Relationship Learning With History-dependent Noise,0.949108,"Discovering causal relationships between different variables from time series
data has been a long-standing challenge for many domains such as climate
science, finance, and healthcare. Given the complexity of real-world
relationships and the nature of observations in discrete time, causal discovery
methods need to consider non-linear relations between variables, instantaneous
effects and history-dependent noise (the change of noise distribution due to
past actions). However, previous works do not offer a solution addressing all
these problems together. In this paper, we propose a novel causal relationship
learning framework for time-series data, called Rhino, which combines vector
auto-regression, deep learning and variational inference to model non-linear
relationships with instantaneous effects while allowing the noise distribution
to be modulated by historical observations. Theoretically, we prove the
structural identifiability of Rhino. Our empirical results from extensive
synthetic experiments and two real-world benchmarks demonstrate better
discovery performance compared to relevant baselines, with ablation studies
revealing its robustness under model misspecification.",https://github.com/sakhanna/SRU_for_GCI/tree/master/data,-1
a28395d2-596c-463b-b9e0-19da140d64e7,Scalable Joint Learning of Wireless Multiple-Access Policies and their Signaling,0.448471,"In this paper, we apply an multi-agent reinforcement learning (MARL)
framework allowing the base station (BS) and the user equipments (UEs) to
jointly learn a channel access policy and its signaling in a wireless multiple
access scenario. In this framework, the BS and UEs are reinforcement learning
(RL) agents that need to cooperate in order to deliver data. The comparison
with a contention-free and a contention-based baselines shows that our
framework achieves a superior performance in terms of goodput even in high
traffic situations while maintaining a low collision rate. The scalability of
the proposed method is studied, since it is a major problem in MARL and this
paper provides the first results in order to address it.",None,-1
24225dc5-6660-44d7-a42c-afcae3463987,UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection Using Generative-based and Mutation-based Data Augmentation,0.0532168,"Sarcasm is a term that refers to the use of words to mock, irritate, or amuse
someone. It is commonly used on social media. The metaphorical and creative
nature of sarcasm presents a significant difficulty for sentiment analysis
systems based on affective computing. The methodology and results of our team,
UTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in
this paper. We put different models, and data augmentation approaches to the
test and report on which one works best. The tests begin with traditional
machine learning models and progress to transformer-based and attention-based
models. We employed data augmentation based on data mutation and data
generation. Using RoBERTa and mutation-based data augmentation, our best
approach achieved an F1-sarcastic of 0.38 in the competition's evaluation
phase. After the competition, we fixed our model's flaws and achieved an
F1-sarcastic of 0.414.",https://github.com/AmirAbaskohi/SemEval2022-Task6-Sarcasm-Detection,-1
d55ddeca-3c7b-4685-b2b9-e0ff1148d624,Content-Based Search for Deep Generative Models,0.0766128,"The growing proliferation of customized and pretrained generative models has
made it infeasible for a user to be fully cognizant of every model in
existence. To address this need, we introduce the task of content-based model
search: given a query and a large set of generative models, finding the models
that best match the query. As each generative model produces a distribution of
images, we formulate the search task as an optimization problem to select the
model with the highest probability of generating similar content as the query.
We introduce a formulation to approximate this probability given the query from
different modalities, e.g., image, sketch, and text. Furthermore, we propose a
contrastive learning framework for model retrieval, which learns to adapt
features for various query modalities. We demonstrate that our method
outperforms several baselines on Generative Model Zoo, a new benchmark we
create for the model retrieval task.",None,-1
487d6955-37ea-4866-aa74-1b47a37b26a6,PLM-ICD: Automatic ICD Coding with Pretrained Language Models,0.523438,"Automatically classifying electronic health records (EHRs) into diagnostic
codes has been challenging to the NLP community. State-of-the-art methods
treated this problem as a multilabel classification problem and proposed
various architectures to model this problem. However, these systems did not
leverage the superb performance of pretrained language models, which achieved
superb performance on natural language understanding tasks. Prior work has
shown that pretrained language models underperformed on this task with the
regular finetuning scheme. Therefore, this paper aims at analyzing the causes
of the underperformance and developing a framework for automatic ICD coding
with pretrained language models. We spotted three main issues through the
experiments: 1) large label space, 2) long input sequences, and 3) domain
mismatch between pretraining and fine-tuning. We propose PLMICD, a framework
that tackles the challenges with various strategies. The experimental results
show that our proposed framework can overcome the challenges and achieves
state-of-the-art performance in terms of multiple metrics on the benchmark
MIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD",https://github.com/MiuLab/PLM-ICD,-1
29fa583b-e65e-4435-8793-9d5623bb29d4,Pessimistic Off-Policy Optimization for Learning to Rank,0.165876,"Off-policy learning is a framework for optimizing policies without deploying
them, using data collected by another policy. In recommender systems, this is
especially challenging due to the imbalance in logged data: some items are
recommended and thus logged more frequently than others. This is further
perpetuated when recommending a list of items, as the action space is
combinatorial. To address this challenge, we study pessimistic off-policy
optimization for learning to rank. The key idea is to compute lower confidence
bounds on parameters of click models and then return the list with the highest
pessimistic estimate of its value. This approach is computationally efficient
and we analyze it. We study its Bayesian and frequentist variants, and overcome
the limitation of unknown prior by incorporating empirical Bayes. To show the
empirical effectiveness of our approach, we compare it to off-policy optimizers
that use inverse propensity scores or neglect uncertainty. Our approach
outperforms all baselines, is robust, and is also general.",None,5108
a6d78ef1-2547-40f1-b819-162b566cf5f9,Reinforcing Generated Images via Meta-learning for One-Shot Fine-Grained Visual Recognition,0.273975,"One-shot fine-grained visual recognition often suffers from the problem of
having few training examples for new fine-grained classes. To alleviate this
problem, off-the-shelf image generation techniques based on Generative
Adversarial Networks (GANs) can potentially create additional training images.
However, these GAN-generated images are often not helpful for actually
improving the accuracy of one-shot fine-grained recognition. In this paper, we
propose a meta-learning framework to combine generated images with original
images, so that the resulting ""hybrid"" training images improve one-shot
learning. Specifically, the generic image generator is updated by a few
training instances of novel classes, and a Meta Image Reinforcing Network
(MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as
image reinforcement. Our experiments demonstrate consistent improvement over
baselines on one-shot fine-grained image classification benchmarks.
Furthermore, our analysis shows that the reinforced images have more diversity
compared to the original and GAN-generated images.",http://vision.soic.indiana.edu/metairnet/,-1
88d3bca3-18fd-4653-a6cc-cbf4878f88f6,Unknown-Aware Domain Adversarial Learning for Open-Set Domain Adaptation,0.896969,"Open-Set Domain Adaptation (OSDA) assumes that a target domain contains
unknown classes, which are not discovered in a source domain. Existing domain
adversarial learning methods are not suitable for OSDA because distribution
matching with $\textit{unknown}$ classes leads to negative transfer. Previous
OSDA methods have focused on matching the source and the target distribution by
only utilizing $\textit{known}$ classes. However, this $\textit{known}$-only
matching may fail to learn the target-$\textit{unknown}$ feature space.
Therefore, we propose Unknown-Aware Domain Adversarial Learning (UADAL), which
$\textit{aligns}$ the source and the target-$\textit{known}$ distribution while
simultaneously $\textit{segregating}$ the target-$\textit{unknown}$
distribution in the feature alignment procedure. We provide theoretical
analyses on the optimized state of the proposed $\textit{unknown-aware}$
feature alignment, so we can guarantee both $\textit{alignment}$ and
$\textit{segregation}$ theoretically. Empirically, we evaluate UADAL on the
benchmark datasets, which shows that UADAL outperforms other methods with
better feature alignments by reporting state-of-the-art performances.",https://github.com/JoonHo-Jang/UADAL,-1
8668718b-3ac4-4f36-8b7d-1f4e1dd51325,A Privacy-Preserving Subgraph-Level Federated Graph Neural Network via Differential Privacy,0.276738,"Currently, the federated graph neural network (GNN) has attracted a lot of
attention due to its wide applications in reality without violating the privacy
regulations. Among all the privacy-preserving technologies, the differential
privacy (DP) is the most promising one due to its effectiveness and light
computational overhead. However, the DP-based federated GNN has not been well
investigated, especially in the sub-graph-level setting, such as the scenario
of recommendation system. The biggest challenge is how to guarantee the privacy
and solve the non independent and identically distributed (non-IID) data in
federated GNN simultaneously. In this paper, we propose DP-FedRec, a DP-based
federated GNN to fill the gap. Private Set Intersection (PSI) is leveraged to
extend the local graph for each client, and thus solve the non-IID problem.
Most importantly, DP is applied not only on the weights but also on the edges
of the intersection graph from PSI to fully protect the privacy of clients. The
evaluation demonstrates DP-FedRec achieves better performance with the graph
extension and DP only introduces little computations overhead.",None,-1
00d9c24c-581f-40bc-95d8-ab3c18adaac9,IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty,0.546644,"Single image surface normal estimation and depth estimation are closely
related problems as the former can be calculated from the latter. However, the
surface normals computed from the output of depth estimation methods are
significantly less accurate than the surface normals directly estimated by
networks. To reduce such discrepancy, we introduce a novel framework that uses
surface normal and its uncertainty to recurrently refine the predicted
depth-map. The depth of each pixel can be propagated to a query pixel, using
the predicted surface normal as guidance. We thus formulate depth refinement as
a classification of choosing the neighboring pixel to propagate from. Then, by
propagating to sub-pixel points, we upsample the refined, low-resolution
output. The proposed method shows state-of-the-art performance on NYUv2 and
iBims-1 - both in terms of depth and normal. Our refinement module can also be
attached to the existing depth estimation methods to improve their accuracy. We
also show that our framework, only trained for depth estimation, can also be
used for depth completion. The code is available at
https://github.com/baegwangbin/IronDepth.",https://github.com/baegwangbin/IronDepth,68183
482e1c70-f2c1-44a6-8720-e7d808229474,Prototypical Verbalizer for Prompt-based Few-shot Tuning,0.683766,"Prompt-based tuning for pre-trained language models (PLMs) has shown its
effectiveness in few-shot learning. Typically, prompt-based tuning wraps the
input text into a cloze question. To make predictions, the model maps the
output words to labels via a verbalizer, which is either manually designed or
automatically built. However, manual verbalizers heavily depend on
domain-specific prior knowledge and human efforts, while finding appropriate
label words automatically still remains challenging.In this work, we propose
the prototypical verbalizer (ProtoVerb) which is built directly from training
data. Specifically, ProtoVerb learns prototype vectors as verbalizers by
contrastive learning. In this way, the prototypes summarize training instances
and are able to enclose rich class-level semantics. We conduct experiments on
both topic classification and entity typing tasks, and the results demonstrate
that ProtoVerb significantly outperforms current automatic verbalizers,
especially when training data is extremely scarce. More surprisingly, ProtoVerb
consistently boosts prompt-based tuning even on untuned PLMs, indicating an
elegant non-tuning way to utilize PLMs. Our codes are avaliable at
https://github.com/thunlp/OpenPrompt.",https://github.com/thunlp/OpenPrompt,-1
9e443305-26d2-444b-a16d-048b379a1ad3,"Two ways to make your robot proactive: reasoning about human intentions, or reasoning about possible futures",0.441924,"Robots sharing their space with humans need to be proactive in order to be
helpful. Proactive robots are able to act on their own initiative in an
anticipatory way to benefit humans. In this work, we investigate two ways to
make robots proactive. One way is to recognize humans' intentions and to act to
fulfill them, like opening the door that you are about to cross. The other way
is to reason about possible future threats or opportunities and to act to
prevent or to foster them, like recommending you to take an umbrella since rain
has been forecasted. In this paper, we present approaches to realize these two
types of proactive behavior. We then present an integrated system that can
generate proactive robot behavior by reasoning on both factors: intentions and
predictions. We illustrate our system on a sample use case including a domestic
robot and a human. We first run this use case with the two separate proactive
systems, intention-based and prediction-based, and then run it with our
integrated system. The results show that the integrated system is able to take
into account a broader variety of aspects that are needed for proactivity.",None,-1
1d193ead-8453-47e6-8aeb-bafec4e58d7c,Zero-Shot Retrieval with Search Agents and Hybrid Environments,0.667085,"Learning to search is the task of building artificial agents that learn to
autonomously use a search box to find information. So far, it has been shown
that current language models can learn symbolic query reformulation policies,
in combination with traditional term-based retrieval, but fall short of
outperforming neural retrievers. We extend the previous learning to search
setup to a hybrid environment, which accepts discrete query refinement
operations, after a first-pass retrieval step via a dual encoder. Experiments
on the BEIR task show that search agents, trained via behavioral cloning,
outperform the underlying search system based on a combined dual encoder
retriever and cross encoder reranker. Furthermore, we find that simple
heuristic Hybrid Retrieval Environments (HRE) can improve baseline performance
by several nDCG points. The search agent based on HRE (HARE) matches
state-of-the-art performance, balanced in both zero-shot and in-domain
evaluations, via interpretable actions, and at twice the speed.",None,2397
15b3fbb6-3bc0-464f-b19a-585c0dca1786,A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking,0.512686,"Large-scale graph training is a notoriously challenging problem for graph
neural networks (GNNs). Due to the nature of evolving graph structures into the
training process, vanilla GNNs usually fail to scale up, limited by the GPU
memory space. Up to now, though numerous scalable GNN architectures have been
proposed, we still lack a comprehensive survey and fair benchmark of this
reservoir to find the rationale for designing scalable GNNs. To this end, we
first systematically formulate the representative methods of large-scale graph
training into several branches and further establish a fair and consistent
benchmark for them by a greedy hyperparameter searching. In addition, regarding
efficiency, we theoretically evaluate the time and space complexity of various
branches and empirically compare them w.r.t GPU memory usage, throughput, and
convergence. Furthermore, We analyze the pros and cons for various branches of
scalable GNNs and then present a new ensembling training manner, named EnGCN,
to address the existing issues. Our code is available at
https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking.",https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking,-1
7ac3d517-44da-4f2a-8085-d3f348db80b5,Face Relighting with Geometrically Consistent Shadows,0.653585,"Most face relighting methods are able to handle diffuse shadows, but struggle
to handle hard shadows, such as those cast by the nose. Methods that propose
techniques for handling hard shadows often do not produce geometrically
consistent shadows since they do not directly leverage the estimated face
geometry while synthesizing them. We propose a novel differentiable algorithm
for synthesizing hard shadows based on ray tracing, which we incorporate into
training our face relighting model. Our proposed algorithm directly utilizes
the estimated face geometry to synthesize geometrically consistent hard
shadows. We demonstrate through quantitative and qualitative experiments on
Multi-PIE and FFHQ that our method produces more geometrically consistent
shadows than previous face relighting methods while also achieving
state-of-the-art face relighting performance under directional lighting. In
addition, we demonstrate that our differentiable hard shadow modeling improves
the quality of the estimated face geometry over diffuse shading models.",https://github.com/andrewhou1/GeomConsistentFR,-1
1c46b6ca-4c4c-48ed-a8b9-271590eca9a8,AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning,0.806545,"Biomedical named entity recognition (BioNER) seeks to automatically recognize
biomedical entities in natural language text, serving as a necessary foundation
for downstream text mining tasks and applications such as information
extraction and question answering. Manually labeling training data for the
BioNER task is costly, however, due to the significant domain expertise
required for accurate annotation. The resulting data scarcity causes current
BioNER approaches to be prone to overfitting, to suffer from limited
generalizability, and to address a single entity type at a time (e.g., gene or
disease). We therefore propose a novel all-in-one (AIO) scheme that uses
external data from existing annotated resources to enhance the accuracy and
stability of BioNER models. We further present AIONER, a general-purpose BioNER
tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER
on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and
compares favorably to other state-of-the-art approaches such as multi-task
learning. We further demonstrate the practical utility of AIONER in three
independent tasks to recognize entity types not previously seen in training
data, as well as the advantages of AIONER over existing methods for processing
biomedical text at a large scale (e.g., the entire PubMed data).",None,-1
5d8b0570-dc65-4d11-8c6f-caf7b1391c71,A Close Look into the Calibration of Pre-trained Language Models,0.846799,"Pre-trained language models (PLMs) may fail in giving reliable estimates of
their predictive uncertainty. We take a close look into this problem, aiming to
answer two questions: (1) Do PLMs learn to become calibrated in the training
process? (2) How effective are existing calibration methods? For the first
question, we conduct fine-grained control experiments to study the dynamic
change in PLMs' calibration performance in training. We consider six factors as
control variables, including dataset difficulty, available training samples,
training steps, the number of tunable parameters, model scale, and pretraining.
We observe a consistent change in calibration performance across six factors.
We find that PLMs don't learn to become calibrated in training, evidenced by
the continual increase in confidence, no matter whether the predictions are
correct or not. We highlight that our finding somewhat contradicts two
established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining
improves model calibration. Next, we study the effectiveness of existing
calibration methods in mitigating the overconfidence issue. Besides unlearnable
calibration methods (e.g., label smoothing), we adapt and extend two recently
proposed learnable methods that directly collect data to train models to have
reasonable confidence estimations. Experimental results show that learnable
methods significantly reduce PLMs' confidence in wrong predictions. The code is
available at \url{https://github.com/lifan-yuan/PLMCalibration}.",https://github.com/lifan-yuan/PLMCalibration,-1
b478efb5-e0d3-4dfa-9721-fc6c236b11f9,Unsupervised Discovery and Composition of Object Light Fields,0.982555,"Neural scene representations, both continuous and discrete, have recently
emerged as a powerful new paradigm for 3D scene understanding. Recent efforts
have tackled unsupervised discovery of object-centric neural scene
representations. However, the high cost of ray-marching, exacerbated by the
fact that each object representation has to be ray-marched separately, leads to
insufficiently sampled radiance fields and thus, noisy renderings, poor
framerates, and high memory and time complexity during training and rendering.
Here, we propose to represent objects in an object-centric, compositional scene
representation as light fields. We propose a novel light field compositor
module that enables reconstructing the global light field from a set of
object-centric light fields. Dubbed Compositional Object Light Fields (COLF),
our method enables unsupervised learning of object-centric neural scene
representations, state-of-the-art reconstruction and novel view synthesis
performance on standard datasets, and rendering and training speeds at orders
of magnitude faster than existing 3D approaches.",https://github.com/cameronosmith/colf,-1
0cc6043e-6f93-44ef-b0ec-e42f6862a593,Flow-Adapter Architecture for Unsupervised Machine Translation,0.453305,"In this work, we propose a flow-adapter architecture for unsupervised NMT. It
leverages normalizing flows to explicitly model the distributions of
sentence-level latent representations, which are subsequently used in
conjunction with the attention mechanism for the translation task. The primary
novelties of our model are: (a) capturing language-specific sentence
representations separately for each language using normalizing flows and (b)
using a simple transformation of these latent representations for translating
from one language to another. This architecture allows for unsupervised
training of each language independently. While there is prior work on latent
variables for supervised MT, to the best of our knowledge, this is the first
work that uses latent variables and normalizing flows for unsupervised MT. We
obtain competitive results on several unsupervised MT benchmarks.",https://github.com/facebookresearch/XLM/blob/main/get-data-nmt.sh,-1
826fb492-6329-4a5b-ada8-62dca8293734,Zero-shot Domain Adaptation for Neural Machine Translation with Retrieved Phrase-level Prompts,0.310904,"Domain adaptation is an important challenge for neural machine translation.
However, the traditional fine-tuning solution requires multiple extra training
and yields a high cost. In this paper, we propose a non-tuning paradigm,
resolving domain adaptation with a prompt-based method. Specifically, we
construct a bilingual phrase-level database and retrieve relevant pairs from it
as a prompt for the input sentences. By utilizing Retrieved Phrase-level
Prompts (RePP), we effectively boost the translation quality. Experiments show
that our method improves domain-specific machine translation for 6.2 BLEU
scores and improves translation constraints for 11.5% accuracy without
additional training.",https://github.com/roeeaharoni/unsupervised-domain-clusters,-1
6219b719-c23a-4f1b-9813-baa46419af54,Model-Free Opponent Shaping,0.585135,"In general-sum games, the interaction of self-interested learning agents
commonly leads to collectively worst-case outcomes, such as defect-defect in
the iterated prisoner's dilemma (IPD). To overcome this, some methods, such as
Learning with Opponent-Learning Awareness (LOLA), shape their opponents'
learning process. However, these methods are myopic since only a small number
of steps can be anticipated, are asymmetric since they treat other agents as
naive learners, and require the use of higher-order derivatives, which are
calculated through white-box access to an opponent's differentiable learning
algorithm. To address these issues, we propose Model-Free Opponent Shaping
(M-FOS). M-FOS learns in a meta-game in which each meta-step is an episode of
the underlying inner game. The meta-state consists of the inner policies, and
the meta-policy produces a new inner policy to be used in the next episode.
M-FOS then uses generic model-free optimisation methods to learn meta-policies
that accomplish long-horizon opponent shaping. Empirically, M-FOS
near-optimally exploits naive learners and other, more sophisticated algorithms
from the literature. For example, to the best of our knowledge, it is the first
method to learn the well-known Zero-Determinant (ZD) extortion strategy in the
IPD. In the same settings, M-FOS leads to socially optimal outcomes under
meta-self-play. Finally, we show that M-FOS can be scaled to high-dimensional
settings.",https://github.com/luchris429/Model-Free-Opponent-Shaping,-1
d6366757-6b64-41a7-8304-5a9abbc9bd3b,UWC: Unit-wise Calibration Towards Rapid Network Compression,0.0889593,"This paper introduces a post-training quantization~(PTQ) method achieving
highly efficient Convolutional Neural Network~ (CNN) quantization with high
performance. Previous PTQ methods usually reduce compression error via
performing layer-by-layer parameters calibration. However, with lower
representational ability of extremely compressed parameters (e.g., the
bit-width goes less than 4), it is hard to eliminate all the layer-wise errors.
This work addresses this issue via proposing a unit-wise feature reconstruction
algorithm based on an observation of second order Taylor series expansion of
the unit-wise error. It indicates that leveraging the interaction between
adjacent layers' parameters could compensate layer-wise errors better. In this
paper, we define several adjacent layers as a Basic-Unit, and present a
unit-wise post-training algorithm which can minimize quantization error. This
method achieves near-original accuracy on ImageNet and COCO when quantizing
FP32 models to INT4 and INT3.",None,-1
8970f13a-3a0f-46e3-badb-07640a18d1ea,Interactive Segmentation of Radiance Fields,0.837679,"Radiance Fields (RF) are popular to represent casually-captured scenes for
new view synthesis and several applications beyond it. Mixed reality on
personal spaces needs understanding and manipulating scenes represented as RFs,
with semantic segmentation of objects as an important step. Prior segmentation
efforts show promise but don't scale to complex objects with diverse
appearance. We present the ISRF method to interactively segment objects with
fine structure and appearance. Nearest neighbor feature matching using
distilled semantic features identifies high-confidence seed regions. Bilateral
search in a joint spatio-semantic space grows the region to recover accurate
segmentation. We show state-of-the-art results of segmenting objects from RFs
and compositing them to another scene, changing appearance, etc., and an
interactive segmentation tool that others can use.
  Project Page: https://rahul-goel.github.io/isrf/",https://rahul-goel.github.io/isrf/,-1
dceb2252-d2cf-4611-b80d-796c20681594,E^2VTS: Energy-Efficient Video Text Spotting from Unmanned Aerial Vehicles,0.0581971,"Unmanned Aerial Vehicles (UAVs) based video text spotting has been
extensively used in civil and military domains. UAV's limited battery capacity
motivates us to develop an energy-efficient video text spotting solution. In
this paper, we first revisit RCNN's crop & resize training strategy and
empirically find that it outperforms aligned RoI sampling on a real-world video
text dataset captured by UAV. To reduce energy consumption, we further propose
a multi-stage image processor that takes videos' redundancy, continuity, and
mixed degradation into account. Lastly, the model is pruned and quantized
before deployed on Raspberry Pi. Our proposed energy-efficient video text
spotting solution, dubbed as E^2VTS, outperforms all previous methods by
achieving a competitive tradeoff between energy efficiency and performance. All
our codes and pre-trained models are available at
https://github.com/wuzhenyusjtu/LPCVC20-VideoTextSpotting.",https://github.com/wuzhenyusjtu/LPCVC20-VideoTextSpotting,-1
84b85ff1-5bbf-4109-ac96-ee95bd576000,Reinforcement Learning with Brain-Inspired Modulation can Improve Adaptation to Environmental Changes,0.426493,"Developments in reinforcement learning (RL) have allowed algorithms to
achieve impressive performance in highly complex, but largely static problems.
In contrast, biological learning seems to value efficiency of adaptation to a
constantly-changing world. Here we build on a recently-proposed neuronal
learning rule that assumes each neuron can optimize its energy balance by
predicting its own future activity. That assumption leads to a neuronal
learning rule that uses presynaptic input to modulate prediction error. We
argue that an analogous RL rule would use action probability to modulate reward
prediction error. This modulation makes the agent more sensitive to negative
experiences, and more careful in forming preferences. We embed the proposed
rule in both tabular and deep-Q-network RL algorithms, and find that it
outperforms conventional algorithms in simple, but highly-dynamic tasks. We
suggest that the new rule encapsulates a core principle of biological
intelligence; an important component for allowing algorithms to adapt to change
in a human-like way.",https://github.com/echalmers/modulated_td_error,-1
1734ac26-8790-4215-9801-9f7504dbad73,Pixel is All You Need: Adversarial Trajectory-Ensemble Active Learning for Salient Object Detection,0.068232,"Although weakly-supervised techniques can reduce the labeling effort, it is
unclear whether a saliency model trained with weakly-supervised data (e.g.,
point annotation) can achieve the equivalent performance of its
fully-supervised version. This paper attempts to answer this unexplored
question by proving a hypothesis: there is a point-labeled dataset where
saliency models trained on it can achieve equivalent performance when trained
on the densely annotated dataset. To prove this conjecture, we proposed a novel
yet effective adversarial trajectory-ensemble active learning (ATAL). Our
contributions are three-fold: 1) Our proposed adversarial attack triggering
uncertainty can conquer the overconfidence of existing active learning methods
and accurately locate these uncertain pixels. {2)} Our proposed
trajectory-ensemble uncertainty estimation method maintains the advantages of
the ensemble networks while significantly reducing the computational cost. {3)}
Our proposed relationship-aware diversity sampling algorithm can conquer
oversampling while boosting performance. Experimental results show that our
ATAL can find such a point-labeled dataset, where a saliency model trained on
it obtained $97\%$ -- $99\%$ performance of its fully-supervised version with
only ten annotated points per image.",None,14647
75bed6c5-1d42-4095-9d8c-56eed875c7ba,Generating Negative Samples for Sequential Recommendation,0.0544524,"To make Sequential Recommendation (SR) successful, recent works focus on
designing effective sequential encoders, fusing side information, and mining
extra positive self-supervision signals. The strategy of sampling negative
items at each time step is less explored. Due to the dynamics of users'
interests and model updates during training, considering randomly sampled items
from a user's non-interacted item set as negatives can be uninformative. As a
result, the model will inaccurately learn user preferences toward items.
Identifying informative negatives is challenging because informative negative
items are tied with both dynamically changed interests and model parameters
(and sampling process should also be efficient). To this end, we propose to
Generate Negative Samples (items) for SR (GenNi). A negative item is sampled at
each time step based on the current SR model's learned user preferences toward
items. An efficient implementation is proposed to further accelerate the
generation process, making it scalable to large-scale recommendation tasks.
Extensive experiments on four public datasets verify the importance of
providing high-quality negative samples for SR and demonstrate the
effectiveness and efficiency of GenNi.",https://github.com/graytowne/caser_pytorch,-1
4d53e077-f887-4400-8add-6c576c963278,ALTO: A Large-Scale Dataset for UAV Visual Place Recognition and Localization,0.812782,"We present the ALTO dataset, a vision-focused dataset for the development and
benchmarking of Visual Place Recognition and Localization methods for Unmanned
Aerial Vehicles. The dataset is composed of two long (approximately 150km and
260km) trajectories flown by a helicopter over Ohio and Pennsylvania, and it
includes high precision GPS-INS ground truth location data, high precision
accelerometer readings, laser altimeter readings, and RGB downward facing
camera imagery. In addition, we provide reference imagery over the flight
paths, which makes this dataset suitable for VPR benchmarking and other tasks
common in Localization, such as image registration and visual odometry. To the
author's knowledge, this is the largest real-world aerial-vehicle dataset of
this kind. Our dataset is available at https://github.com/MetaSLAM/ALTO.",https://github.com/MetaSLAM/ALTO,-1
f4025d4d-3720-4742-8a8c-a72fc543d8c5,Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks,0.465506,"Randomization is currently a widely used approach in Sim2Real transfer for
data-driven learning algorithms in robotics. Still, most Sim2Real studies
report results for a specific randomization technique and often on a highly
customized robotic system, making it difficult to evaluate different
randomization approaches systematically. To address this problem, we define an
easy-to-reproduce experimental setup for a robotic reach-and-balance
manipulator task, which can serve as a benchmark for comparison. We compare
four randomization strategies with three randomized parameters both in
simulation and on a real robot. Our results show that more randomization helps
in Sim2Real transfer, yet it can also harm the ability of the algorithm to find
a good policy in simulation. Fully randomized simulations and fine-tuning show
differentiated results and translate better to the real robot than the other
approaches tested.",https://josifovski.github.io/sim2real-randomization-effects/,-1
c488b20e-a974-4078-a4c2-2a48915067f4,Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization,0.415939,"Current 3D scene stylization methods transfer textures and colors as styles
using arbitrary style references, lacking meaningful semantic correspondences.
We introduce Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR) to
address this limitation. This controllable method stylizes a 3D scene using
radiance fields with a single stylized 2D view as a reference. We propose a ray
registration process based on the stylized reference view to obtain pseudo-ray
supervision in novel views. Then we exploit semantic correspondences in content
images to fill occluded regions with perceptually similar styles, resulting in
non-photorealistic and continuous novel view sequences. Our experimental
results demonstrate that Ref-NPR outperforms existing scene and video
stylization methods regarding visual quality and semantic correspondence. The
code and data are publicly available on the project page at
https://ref-npr.github.io.",https://ref-npr.github.io,-1
aa75b6ce-13bd-46a5-a662-abab30cb770b,Domain Adaptation via Bidirectional Cross-Attention Transformer,0.492483,"Domain Adaptation (DA) aims to leverage the knowledge learned from a source
domain with ample labeled data to a target domain with unlabeled data only.
Most existing studies on DA contribute to learning domain-invariant feature
representations for both domains by minimizing the domain gap based on
convolution-based neural networks. Recently, vision transformers significantly
improved performance in multiple vision tasks. Built on vision transformers, in
this paper we propose a Bidirectional Cross-Attention Transformer (BCAT) for DA
with the aim to improve the performance. In the proposed BCAT, the attention
mechanism can extract implicit source and target mixup feature representations
to narrow the domain discrepancy. Specifically, in BCAT, we design a
weight-sharing quadruple-branch transformer with a bidirectional
cross-attention mechanism to learn domain-invariant feature representations.
Extensive experiments demonstrate that the proposed BCAT model achieves
superior performance on four benchmark datasets over existing state-of-the-art
DA methods that are based on convolutions or transformers.",None,-1
e99b83b8-8a76-4a83-abc7-0475a9544ec0,BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach,0.99964,"Bilevel optimization (BO) is useful for solving a variety of important
machine learning problems including but not limited to hyperparameter
optimization, meta-learning, continual learning, and reinforcement learning.
Conventional BO methods need to differentiate through the low-level
optimization process with implicit differentiation, which requires expensive
calculations related to the Hessian matrix. There has been a recent quest for
first-order methods for BO, but the methods proposed to date tend to be
complicated and impractical for large-scale deep learning applications. In this
work, we propose a simple first-order BO algorithm that depends only on
first-order gradient information, requires no implicit differentiation, and is
practical and efficient for large-scale non-convex functions in deep learning.
We provide non-asymptotic convergence analysis of the proposed method to
stationary points for non-convex objectives and present empirical results that
show its superior practical performance.",https://github.com/JunjieYang97/stocBiO,-1
30e11d9f-17fe-48dc-ada0-5399f935862a,A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss,0.226583,"For readability assessment, traditional methods mainly employ machine
learning classifiers with hundreds of linguistic features. Although the deep
learning model has become the prominent approach for almost all NLP tasks, it
is less explored for readability assessment. In this paper, we propose a
BERT-based model with feature projection and length-balanced loss (BERT-FP-LBL)
for readability assessment. Specially, we present a new difficulty knowledge
guided semi-supervised method to extract topic features to complement the
traditional linguistic features. From the linguistic features, we employ
projection filtering to extract orthogonal features to supplement BERT
representations. Furthermore, we design a new length-balanced loss to handle
the greatly varying length distribution of data. Our model achieves
state-of-the-art performances on two English benchmark datasets and one dataset
of Chinese textbooks, and also achieves the near-perfect accuracy of 99\% on
one English dataset. Moreover, our proposed model obtains comparable results
with human experts in consistency test.",https://github.com/liwb1219/zhfeat,1295
378e4ced-2043-4f39-9b54-2fb5711318e6,Look to the Right: Mitigating Relative Position Bias in Extractive Question Answering,0.19095,"Extractive question answering (QA) models tend to exploit spurious
correlations to make predictions when a training set has unintended biases.
This tendency results in models not being generalizable to examples where the
correlations do not hold. Determining the spurious correlations QA models can
exploit is crucial in building generalizable QA models in real-world
applications; moreover, a method needs to be developed that prevents these
models from learning the spurious correlations even when a training set is
biased. In this study, we discovered that the relative position of an answer,
which is defined as the relative distance from an answer span to the closest
question-context overlap word, can be exploited by QA models as superficial
cues for making predictions. Specifically, we find that when the relative
positions in a training set are biased, the performance on examples with
relative positions unseen during training is significantly degraded. To
mitigate the performance degradation for unseen relative positions, we propose
an ensemble-based debiasing method that does not require prior knowledge about
the distribution of relative positions. We demonstrate that the proposed method
mitigates the models' reliance on relative positions using the biased and full
SQuAD dataset. We hope that this study can help enhance the generalization
ability of QA models in real-world applications.",https://github.com/KazutoshiShinoda/RelativePositionBias,-1
d4d36e37-3fac-4970-9a64-9bbec6d92ca5,Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection,0.887628,"In some scenarios, classifier requires detecting out-of-distribution samples
far from its training data. With desirable characteristics, reconstruction
autoencoder-based methods deal with this problem by using input reconstruction
error as a metric of novelty vs. normality. We formulate the essence of such
approach as a quadruplet domain translation with an intrinsic bias to only
query for a proxy of conditional data uncertainty. Accordingly, an improvement
direction is formalized as maximumly compressing the autoencoder's latent space
while ensuring its reconstructive power for acting as a described domain
translator. From it, strategies are introduced including semantic
reconstruction, data certainty decomposition and normalized L2 distance to
substantially improve original methods, which together establish
state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of
CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method
works without any additional data, hard-to-implement structure, time-consuming
pipeline, and even harming the classification accuracy of known classes.",https://github.com/xxx,-1
21f14ac2-2a49-42f1-95c9-ad3bb54a9c6a,textless-lib: a Library for Textless Spoken Language Processing,0.976723,"Textless spoken language processing research aims to extend the applicability
of standard NLP toolset onto spoken language and languages with few or no
textual resources. In this paper, we introduce textless-lib, a PyTorch-based
library aimed to facilitate research in this research area. We describe the
building blocks that the library provides and demonstrate its usability by
discuss three different use-case examples: (i) speaker probing, (ii) speech
resynthesis and compression, and (iii) speech continuation. We believe that
textless-lib substantially simplifies research the textless setting and will be
handful not only for speech researchers but also for the NLP community at
large. The code, documentation, and pre-trained models are available at
https://github.com/facebookresearch/textlesslib/ .",https://github.com/facebookresearch/textlesslib/,-1
0224d54a-8acd-4fce-a008-a989b2483e6a,Qualia as physical measurements: a mathematical model of qualia and pure concepts,0.664089,"A space of qualia is defined to be a sober topological space whose points are
the qualia and whose open sets are the pure concepts in the sense of Lewis,
carrying additional algebraic structure that conveys the conscious experience
of subjective time and logical abstraction. This structure is analogous to that
of a space of physical measurements. It is conjectured that qualia and
measurements have the same nature, corresponding to fundamental processes via
which classical information is produced and physically stored, and that
therefore the hard problem of consciousness and the measurement problem are two
facets of the same problem. The space of qualia is independent from any
preexisting notions of spacetime and conscious agent, but its structure caters
for a derived geometric model of observer. Intersubjectivity is based on
relating different observers in a way that leads to a logical version of
quantum superposition.",None,-1
f5f159b5-0688-428c-a7a5-cb87bc9440cd,RandoMix: A mixed sample data augmentation method with multiple mixed modes,0.337674,"Data augmentation plays a crucial role in enhancing the robustness and
performance of machine learning models across various domains. In this study,
we introduce a novel mixed-sample data augmentation method called RandoMix.
RandoMix is specifically designed to simultaneously address robustness and
diversity challenges. It leverages a combination of linear and mask-mixed
modes, introducing flexibility in candidate selection and weight adjustments.
We evaluate the effectiveness of RandoMix on diverse datasets, including
CIFAR-10/100, Tiny-ImageNet, ImageNet, and Google Speech Commands. Our results
demonstrate its superior performance compared to existing techniques such as
Mixup, CutMix, Fmix, and ResizeMix. Notably, RandoMix excels in enhancing model
robustness against adversarial noise, natural noise, and sample occlusion. The
comprehensive experimental results and insights into parameter tuning
underscore the potential of RandoMix as a versatile and effective data
augmentation method. Moreover, it seamlessly integrates into the training
pipeline.",None,-1
2193f18e-f940-49be-97ec-c79f46cd9269,Read Top News First: A Document Reordering Approach for Multi-Document News Summarization,0.456774,"A common method for extractive multi-document news summarization is to
re-formulate it as a single-document summarization problem by concatenating all
documents as a single meta-document. However, this method neglects the relative
importance of documents. We propose a simple approach to reorder the documents
according to their relative importance before concatenating and summarizing
them. The reordering makes the salient content easier to learn by the
summarization model. Experiments show that our approach outperforms previous
state-of-the-art methods with more complex architectures.",https://github.com/zhaochaocs/MDS-DR,30354
8c36d9f8-2198-4168-94ed-afce8cb2ebc5,On the link between conscious function and general intelligence in humans and machines,0.235021,"In popular media, there is often a connection drawn between the advent of
awareness in artificial agents and those same agents simultaneously achieving
human or superhuman level intelligence. In this work, we explore the validity
and potential application of this seemingly intuitive link between
consciousness and intelligence. We do so by examining the cognitive abilities
associated with three contemporary theories of conscious function: Global
Workspace Theory (GWT), Information Generation Theory (IGT), and Attention
Schema Theory (AST). We find that all three theories specifically relate
conscious function to some aspect of domain-general intelligence in humans.
With this insight, we turn to the field of Artificial Intelligence (AI) and
find that, while still far from demonstrating general intelligence, many
state-of-the-art deep learning methods have begun to incorporate key aspects of
each of the three functional theories. Having identified this trend, we use the
motivating example of mental time travel in humans to propose ways in which
insights from each of the three theories may be combined into a single unified
and implementable model. Given that it is made possible by cognitive abilities
underlying each of the three functional theories, artificial agents capable of
mental time travel would not only possess greater general intelligence than
current approaches, but also be more consistent with our current understanding
of the functional role of consciousness in humans, thus making it a promising
near-term goal for AI research.",None,-1
75f4325a-9815-4640-b35c-69b92172cb7b,Clues Before Answers: Generation-Enhanced Multiple-Choice QA,0.982885,"A trending paradigm for multiple-choice question answering (MCQA) is using a
text-to-text framework. By unifying data in different tasks into a single
text-to-text format, it trains a generative encoder-decoder model which is both
powerful and universal. However, a side effect of twisting a generation target
to fit the classification nature of MCQA is the under-utilization of the
decoder and the knowledge that can be decoded. To exploit the generation
capability and underlying knowledge of a pre-trained encoder-decoder model, in
this paper, we propose a generation-enhanced MCQA model named GenMC. It
generates a clue from the question and then leverages the clue to enhance a
reader for MCQA. It outperforms text-to-text models on multiple MCQA datasets.",https://github.com/nju-websoft/GenMC,-1
f77e582d-93fe-40f4-8ed1-50abf4772655,Watermarking Pre-trained Encoders in Contrastive Learning,0.473558,"Contrastive learning has become a popular technique to pre-train image
encoders, which could be used to build various downstream classification models
in an efficient way. This process requires a large amount of data and
computation resources. Hence, the pre-trained encoders are an important
intellectual property that needs to be carefully protected. It is challenging
to migrate existing watermarking techniques from the classification tasks to
the contrastive learning scenario, as the owner of the encoder lacks the
knowledge of the downstream tasks which will be developed from the encoder in
the future. We propose the \textit{first} watermarking methodology for the
pre-trained encoders. We introduce a task-agnostic loss function to effectively
embed into the encoder a backdoor as the watermark. This backdoor can still
exist in any downstream models transferred from the encoder. Extensive
evaluations over different contrastive learning algorithms, datasets, and
downstream tasks indicate our watermarks exhibit high effectiveness and
robustness against different adversarial operations.",https://github.com/facebookresearch/moco,-1
290a5329-d3f7-4472-b839-3c7683d3d00b,Alibaba-Translate China's Submission for WMT 2022 Metrics Shared Task,0.48775,"In this report, we present our submission to the WMT 2022 Metrics Shared
Task. We build our system based on the core idea of UNITE (Unified Translation
Evaluation), which unifies source-only, reference-only, and
source-reference-combined evaluation scenarios into one single model.
Specifically, during the model pre-training phase, we first apply the
pseudo-labeled data examples to continuously pre-train UNITE. Notably, to
reduce the gap between pre-training and fine-tuning, we use data cropping and a
ranking-based score normalization strategy. During the fine-tuning phase, we
use both Direct Assessment (DA) and Multidimensional Quality Metrics (MQM) data
from past years' WMT competitions. Specially, we collect the results from
models with different pre-trained language model backbones, and use different
ensembling strategies for involved translation directions.",https://github.com/wanyu2018umac/,-1
484007fa-443a-44b0-a23c-c1f56ad35172,Leveraging Log Instructions in Log-based Anomaly Detection,0.0798259,"Artificial Intelligence for IT Operations (AIOps) describes the process of
maintaining and operating large IT systems using diverse AI-enabled methods and
tools for, e.g., anomaly detection and root cause analysis, to support the
remediation, optimization, and automatic initiation of self-stabilizing IT
activities. The core step of any AIOps workflow is anomaly detection, typically
performed on high-volume heterogeneous data such as log messages (logs),
metrics (e.g., CPU utilization), and distributed traces. In this paper, we
propose a method for reliable and practical anomaly detection from system logs.
It overcomes the common disadvantage of related works, i.e., the need for a
large amount of manually labeled training data, by building an anomaly
detection model with log instructions from the source code of 1000+ GitHub
projects. The instructions from diverse systems contain rich and heterogenous
information about many different normal and abnormal IT events and serve as a
foundation for anomaly detection. The proposed method, named ADLILog, combines
the log instructions and the data from the system of interest (target system)
to learn a deep neural network model through a two-phase learning procedure.
The experimental results show that ADLILog outperforms the related approaches
by up to 60% on the F1 score while satisfying core non-functional requirements
for industrial deployments such as unsupervised design, efficient model
updates, and small model sizes.",https://github.com/ADLILog/ADLILog,-1
ae6da2a9-f7cf-4b3f-8fa7-d02ef589be02,NaturalProver: Grounded Mathematical Proof Generation with Language Models,0.997389,"Theorem proving in natural mathematical language - the mixture of symbolic
and natural language used by humans - plays a central role in mathematical
advances and education, and tests aspects of reasoning that are core to
intelligence. Yet it has remained underexplored with modern generative models.
We study large-scale language models on two new generation tasks: suggesting
the next step in a mathematical proof, and full proof generation. We develop
NaturalProver, a language model that generates proofs by conditioning on
background references (e.g. theorems and definitions that are either retrieved
or human-provided), and optionally enforces their presence with constrained
decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves
the quality of next-step suggestions and generated proofs over fine-tuned
GPT-3, according to human evaluations from university-level mathematics
students. NaturalProver is capable of proving some theorems that require short
(2-6 step) proofs, and providing next-step suggestions that are rated as
correct and useful over 40% of the time, which is to our knowledge the first
demonstration of these capabilities using neural language models.",https://github.com/wellecks/naturalprover,-1
273b4935-46f0-4515-a2cc-9e497b78024f,Mingling Foresight with Imagination: Model-Based Cooperative Multi-Agent Reinforcement Learning,0.142556,"Recently, model-based agents have achieved better performance than model-free
ones using the same computational budget and training time in single-agent
environments. However, due to the complexity of multi-agent systems, it is
tough to learn the model of the environment. The significant compounding error
may hinder the learning process when model-based methods are applied to
multi-agent tasks. This paper proposes an implicit model-based multi-agent
reinforcement learning method based on value decomposition methods. Under this
method, agents can interact with the learned virtual environment and evaluate
the current state value according to imagined future states in the latent
space, making agents have the foresight. Our approach can be applied to any
multi-agent value decomposition method. The experimental results show that our
method improves the sample efficiency in different partially observable Markov
decision process domains.",None,-1
abf13567-9500-417d-be71-d4a5f214e15c,KnowGL: Knowledge Generation and Linking from Text,0.635611,"We propose KnowGL, a tool that allows converting text into structured
relational data represented as a set of ABox assertions compliant with the TBox
of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a
sequence generation task by leveraging pre-trained sequence-to-sequence
language models, e.g. BART. Given a sentence, we fine-tune such models to
detect pairs of entity mentions and jointly generate a set of facts consisting
of the full set of semantic annotations for a KG, such as entity labels, entity
types, and their relationships. To showcase the capabilities of our tool, we
build a web application consisting of a set of UI widgets that help users to
navigate through the semantic data extracted from a given input text. We make
the KnowGL model available at https://huggingface.co/ibm/knowgl-large.",None,-1
8c711137-50fa-4e4d-ba02-5e6b66af0cf9,Soft-Labeled Contrastive Pre-training for Function-level Code Representation,0.315772,"Code contrastive pre-training has recently achieved significant progress on
code-related tasks. In this paper, we present \textbf{SCodeR}, a
\textbf{S}oft-labeled contrastive pre-training framework with two positive
sample construction methods to learn functional-level \textbf{Code}
\textbf{R}epresentation. Considering the relevance between codes in a
large-scale code corpus, the soft-labeled contrastive pre-training can obtain
fine-grained soft-labels through an iterative adversarial manner and use them
to learn better code representation. The positive sample construction is
another key for contrastive pre-training. Previous works use
transformation-based methods like variable renaming to generate semantically
equal positive codes. However, they usually result in the generated code with a
highly similar surface form, and thus mislead the model to focus on superficial
code structure instead of code semantics. To encourage SCodeR to capture
semantic information from the code, we utilize code comments and abstract
syntax sub-trees of the code to build positive samples. We conduct experiments
on four code-related tasks over seven datasets. Extensive experimental results
show that SCodeR achieves new state-of-the-art performance on all of them,
which illustrates the effectiveness of the proposed pre-training method.",https://github.com/microsoft/AR2/tree/main/SCodeR,-1
0dc5dbf1-31b2-46c1-b308-413354f9707f,Learning Program Synthesis for Integer Sequences from Scratch,0.203029,"We present a self-learning approach for synthesizing programs from integer
sequences. Our method relies on a tree search guided by a learned policy. Our
system is tested on the On-Line Encyclopedia of Integer Sequences. There, it
discovers, on its own, solutions for 27987 sequences starting from basic
operators and without human-written training examples.",https://github.com/barakeel/oeis-synthesis,6083
51b933ff-33c9-4633-bda9-e6ab756ddf34,Dialogs Re-enacted Across Languages,0.291986,"To support machine learning of cross-language prosodic mappings and other
ways to improve speech-to-speech translation, we present a protocol for
collecting closely matched pairs of utterances across languages, a description
of the resulting data collection and its public release, and some observations
and musings. This report is intended for: people using this corpus, people
extending this corpus, and people designing similar collections of bilingual
dialog data.",https://github.com/joneavila/DRAL,-1
66d3a10a-9d8c-40e0-b2d5-63673996763f,Global Matching with Overlapping Attention for Optical Flow Estimation,0.980582,"Optical flow estimation is a fundamental task in computer vision. Recent
direct-regression methods using deep neural networks achieve remarkable
performance improvement. However, they do not explicitly capture long-term
motion correspondences and thus cannot handle large motions effectively. In
this paper, inspired by the traditional matching-optimization methods where
matching is introduced to handle large displacements before energy-based
optimizations, we introduce a simple but effective global matching step before
the direct regression and develop a learning-based matching-optimization
framework, namely GMFlowNet. In GMFlowNet, global matching is efficiently
calculated by applying argmax on 4D cost volumes. Additionally, to improve the
matching quality, we propose patch-based overlapping attention to extract large
context features. Extensive experiments demonstrate that GMFlowNet outperforms
RAFT, the most popular optimization-only method, by a large margin and achieves
state-of-the-art performance on standard benchmarks. Thanks to the matching and
overlapping attention, GMFlowNet obtains major improvements on the predictions
for textureless regions and large motions. Our code is made publicly available
at https://github.com/xiaofeng94/GMFlowNet",https://github.com/xiaofeng94/GMFlowNet,-1
9ed3a352-f689-4f50-9362-bde5bc2406a4,Combined CNN Transformer Encoder for Enhanced Fine-grained Human Action Recognition,0.313813,"Fine-grained action recognition is a challenging task in computer vision. As
fine-grained datasets have small inter-class variations in spatial and temporal
space, fine-grained action recognition model requires good temporal reasoning
and discrimination of attribute action semantics. Leveraging on CNN's ability
in capturing high level spatial-temporal feature representations and
Transformer's modeling efficiency in capturing latent semantics and global
dependencies, we investigate two frameworks that combine CNN vision backbone
and Transformer Encoder to enhance fine-grained action recognition: 1) a
vision-based encoder to learn latent temporal semantics, and 2) a multi-modal
video-text cross encoder to exploit additional text input and learn cross
association between visual and text semantics. Our experimental results show
that both our Transformer encoder frameworks effectively learn latent temporal
semantics and cross-modality association, with improved recognition performance
over CNN vision model. We achieve new state-of-the-art performance on the
FineGym benchmark dataset for both proposed architectures.",None,-1
f21531d8-1fdf-46dc-8e21-3a8e420a8723,Interactive Sketching of Mannequin Poses,0.108368,"It can be easy and even fun to sketch humans in different poses. In contrast,
creating those same poses on a 3D graphics ""mannequin"" is comparatively
tedious. Yet 3D body poses are necessary for various downstream applications.
We seek to preserve the convenience of 2D sketching while giving users of
different skill levels the flexibility to accurately and more quickly
pose\slash refine a 3D mannequin.
  At the core of the interactive system, we propose a machine-learning model
for inferring the 3D pose of a CG mannequin from sketches of humans drawn in a
cylinder-person style. Training such a model is challenging because of artist
variability, a lack of sketch training data with corresponding ground truth 3D
poses, and the high dimensionality of human pose-space. Our unique approach to
synthesizing vector graphics training data underpins our integrated
ML-and-kinematics system. We validate the system by tightly coupling it with a
user interface, and by performing a user study, in addition to quantitative
comparisons.",None,-1
53a960c3-b469-484b-bb52-4a14a1794de9,DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation,0.987203,"Dialog response generation in open domain is an important research topic
where the main challenge is to generate relevant and diverse responses. In this
paper, we propose a new dialog pre-training framework called DialogVED, which
introduces continuous latent variables into the enhanced encoder-decoder
pre-training framework to increase the relevance and diversity of responses.
With the help of a large dialog corpus (Reddit), we pre-train the model using
the following 4 tasks adopted in language models (LMs) and variational
autoencoders (VAEs): 1) masked language model; 2) response generation; 3)
bag-of-words prediction; and 4) KL divergence reduction. We also add additional
parameters to model the turn structure in dialogs to improve the performance of
the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and
DSTC7-AVSD benchmarks for response generation. Experimental results show that
our model achieves the new state-of-the-art results on all these datasets.",None,-1
4a5d162c-439c-47bf-982b-507d2051e6d9,DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships,0.117042,"In this paper, we propose DimonGen, which aims to generate diverse sentences
describing concept relationships in various everyday scenarios. To support
this, we first create a benchmark dataset for this task by adapting the
existing CommonGen dataset. We then propose a two-stage model called MoREE to
generate the target sentences. MoREE consists of a mixture of retrievers model
that retrieves diverse context sentences related to the given concepts, and a
mixture of generators model that generates diverse sentences based on the
retrieved contexts. We conduct experiments on the DimonGen task and show that
MoREE outperforms strong baselines in terms of both the quality and diversity
of the generated sentences. Our results demonstrate that MoREE is able to
generate diverse sentences that reflect different relationships between
concepts, leading to a comprehensive understanding of concept relationships.",https://github.com/liuchenzhengyi/DimonGen,-1
77b8e087-48c6-4c97-be06-dde74b35e212,Translation Word-Level Auto-Completion: What can we achieve out of the box?,0.313823,"Research on Machine Translation (MT) has achieved important breakthroughs in
several areas. While there is much more to be done in order to build on this
success, we believe that the language industry needs better ways to take full
advantage of current achievements. Due to a combination of factors, including
time, resources, and skills, businesses tend to apply pragmatism into their AI
workflows. Hence, they concentrate more on outcomes, e.g. delivery, shipping,
releases, and features, and adopt high-level working production solutions,
where possible. Among the features thought to be helpful for translators are
sentence-level and word-level translation auto-suggestion and auto-completion.
Suggesting alternatives can inspire translators and limit their need to refer
to external resources, which hopefully boosts their productivity. This work
describes our submissions to WMT's shared task on word-level auto-completion,
for the Chinese-to-English, English-to-Chinese, German-to-English, and
English-to-German language directions. We investigate the possibility of using
pre-trained models and out-of-the-box features from available libraries. We
employ random sampling to generate diverse alternatives, which reveals good
results. Furthermore, we introduce our open-source API, based on CTranslate2,
to serve translations, auto-suggestions, and auto-completions.",https://github.com/ymoslem/WLAC,-1
1319c8ed-e922-402d-9906-0f8fc2b63857,A Self-Guided Framework for Radiology Report Generation,0.715289,"Automatic radiology report generation is essential to computer-aided
diagnosis. Through the success of image captioning, medical report generation
has been achievable. However, the lack of annotated disease labels is still the
bottleneck of this area. In addition, the image-text data bias problem and
complex sentences make it more difficult to generate accurate reports. To
address these gaps, we pre-sent a self-guided framework (SGF), a suite of
unsupervised and supervised deep learning methods to mimic the process of human
learning and writing. In detail, our framework obtains the domain knowledge
from medical reports with-out extra disease labels and guides itself to extract
fined-grain visual features as-sociated with the text. Moreover, SGF
successfully improves the accuracy and length of medical report generation by
incorporating a similarity comparison mechanism that imitates the process of
human self-improvement through compar-ative practice. Extensive experiments
demonstrate the utility of our SGF in the majority of cases, showing its
superior performance over state-of-the-art meth-ods. Our results highlight the
capacity of the proposed framework to distinguish fined-grained visual details
between words and verify its advantage in generating medical reports.",None,2284
4def2c1c-b6dd-49d4-8bef-cac9d651ceb1,Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus,0.521587,"In this research, we propose a complete set of approaches for identifying and
extracting emotions from Bangla texts. We provide a Bangla emotion classifier
for six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla
words using transformer-based models, which exhibit phenomenal results in
recent days, especially for high-resource languages. The Unified Bangla
Multi-class Emotion Corpus (UBMEC) is used to assess the performance of our
models. UBMEC is created by combining two previously released manually labeled
datasets of Bangla comments on six emotion classes with fresh manually labeled
Bangla comments created by us. The corpus dataset and code we used in this work
are publicly available.",https://github.com/Sakibsourav019/UBMEC-Unified-Bangla-Multi-class-Emotion-Corpus-,-1
3a28ab66-a454-42f6-8f0e-2a79c275df6e,SumREN: Summarizing Reported Speech about Events in News,0.558095,"A primary objective of news articles is to establish the factual record for
an event, frequently achieved by conveying both the details of the specified
event (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and
how people reacted to it (i.e., reported statements). However, existing work on
news summarization almost exclusively focuses on the event details. In this
work, we propose the novel task of summarizing the reactions of different
speakers, as expressed by their reported statements, to a given event. To this
end, we create a new multi-document summarization benchmark, SUMREN, comprising
745 summaries of reported statements from various public figures obtained from
633 news articles discussing 132 events. We propose an automatic silver
training data generation approach for our task, which helps smaller models like
BART achieve GPT-3 level performance on this task. Finally, we introduce a
pipeline-based framework for summarizing reported speech, which we empirically
show to generate summaries that are more abstractive and factual than baseline
query-focused summarization approaches.",https://github.com/amazon-science/SumREN,-1
7eeb36f6-4f16-4a4a-8dd7-a67994973bc4,Evaluating Long-Term Memory in 3D Mazes,0.536736,"Intelligent agents need to remember salient information to reason in
partially-observed environments. For example, agents with a first-person view
should remember the positions of relevant objects even if they go out of view.
Similarly, to effectively navigate through rooms agents need to remember the
floor plan of how rooms are connected. However, most benchmark tasks in
reinforcement learning do not test long-term memory in agents, slowing down
progress in this important research direction. In this paper, we introduce the
Memory Maze, a 3D domain of randomized mazes specifically designed for
evaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze
measures long-term memory separate from confounding agent abilities and
requires the agent to localize itself by integrating information over time.
With Memory Maze, we propose an online reinforcement learning benchmark, a
diverse offline dataset, and an offline probing evaluation. Recording a human
player establishes a strong baseline and verifies the need to build up and
retain memories, which is reflected in their gradually increasing rewards
within each episode. We find that current algorithms benefit from training with
truncated backpropagation through time and succeed on small mazes, but fall
short of human performance on the large mazes, leaving room for future
algorithmic designs to be evaluated on the Memory Maze.",https://github.com/jurgisp/memory-maze,-1
f574c529-6b0b-4fb0-9e78-fd4c062e5532,LaSQuE: Improved Zero-Shot Classification from Explanations Through Quantifier Modeling and Curriculum Learning,0.0201099,"A hallmark of human intelligence is the ability to learn new concepts purely
from language. Several recent approaches have explored training machine
learning models via natural language supervision. However, these approaches
fall short in leveraging linguistic quantifiers (such as 'always' or 'rarely')
and mimicking humans in compositionally learning complex tasks. Here, we
present LaSQuE, a method that can learn zero-shot classifiers from language
explanations by using three new strategies - (1) modeling the semantics of
linguistic quantifiers in explanations (including exploiting ordinal strength
relationships, such as 'always' > 'likely'), (2) aggregating information from
multiple explanations using an attention-based mechanism, and (3) model
training via curriculum learning. With these strategies, LaSQuE outperforms
prior work, showing an absolute gain of up to 7% in generalizing to unseen
real-world classification tasks.",None,-1
9ee301c8-bc0e-4254-a849-c6996df8b540,Improving abstractive summarization with energy-based re-ranking,0.261271,"Current abstractive summarization systems present important weaknesses which
prevent their deployment in real-world applications, such as the omission of
relevant information and the generation of factual inconsistencies (also known
as hallucinations). At the same time, automatic evaluation metrics such as CTC
scores have been recently proposed that exhibit a higher correlation with human
judgments than traditional lexical-overlap metrics such as ROUGE. In this work,
we intend to close the loop by leveraging the recent advances in summarization
metrics to create quality-aware abstractive summarizers. Namely, we propose an
energy-based model that learns to re-rank summaries according to one or a
combination of these metrics. We experiment using several metrics to train our
energy-based re-ranker and show that it consistently improves the scores
achieved by the predicted summaries. Nonetheless, human evaluation results show
that the re-ranking approach should be used with care for highly abstractive
summaries, as the available metrics are not yet sufficiently reliable for this
purpose.",https://github.com/Priberam/SummEBR,-1
98540505-41d9-4122-b976-6e2c623aa238,Is Surprisal in Issue Trackers Actionable?,0.157774,"Background. From information theory, surprisal is a measurement of how
unexpected an event is. Statistical language models provide a probabilistic
approximation of natural languages, and because surprisal is constructed with
the probability of an event occuring, it is therefore possible to determine the
surprisal associated with English sentences. The issues and pull requests of
software repository issue trackers give insight into the development process
and likely contain the surprising events of this process.
  Objective. Prior works have identified that unusual events in software
repositories are of interest to developers, and use simple code metrics-based
methods for detecting them. In this study we will propose a new method for
unusual event detection in software repositories using surprisal. With the
ability to find surprising issues and pull requests, we intend to further
analyse them to determine if they actually hold importance in a repository, or
if they pose a significant challenge to address. If it is possible to find bad
surprises early, or before they cause additional troubles, it is plausible that
effort, cost and time will be saved as a result.
  Method. After extracting the issues and pull requests from 5000 of the most
popular software repositories on GitHub, we will train a language model to
represent these issues. We will measure their perceived importance in the
repository, measure their resolution difficulty using several analogues,
measure the surprisal of each, and finally generate inferential statistics to
describe any correlations.",None,-1
444f38dd-1957-41ed-a35f-5b2ead66c534,When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture,0.809908,"Vision Transformers (ViTs) have recently achieved competitive performance in
broad vision tasks. Unfortunately, on popular threat models, naturally trained
ViTs are shown to provide no more adversarial robustness than convolutional
neural networks (CNNs). Adversarial training is still required for ViTs to
defend against such adversarial attacks. In this paper, we provide the first
and comprehensive study on the adversarial training recipe of ViTs via
extensive evaluation of various training techniques across benchmark datasets.
We find that pre-training and SGD optimizer are necessary for ViTs' adversarial
training. Further considering ViT as a new type of model architecture, we
investigate its adversarial robustness from the perspective of its unique
architectural components. We find, when randomly masking gradients from some
attention blocks or masking perturbations on some patches during adversarial
training, the adversarial robustness of ViTs can be remarkably improved, which
may potentially open up a line of work to explore the architectural information
inside the newly designed models like ViTs. Our code is available at
https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers.",https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers,-1
4698279d-eb26-4d7e-af7b-fd07b5e665fa,A Screen-Shooting Resilient Document Image Watermarking Scheme using Deep Neural Network,0.649139,"With the advent of the screen-reading era, the confidential documents
displayed on the screen can be easily captured by a camera without leaving any
traces. Thus, this paper proposes a novel screen-shooting resilient
watermarking scheme for document image using deep neural network. By applying
this scheme, when the watermarked image is displayed on the screen and captured
by a camera, the watermark can be still extracted from the captured
photographs. Specifically, our scheme is an end-to-end neural network with an
encoder to embed watermark and a decoder to extract watermark. During the
training process, a distortion layer between encoder and decoder is added to
simulate the distortions introduced by screen-shooting process in real scenes,
such as camera distortion, shooting distortion, light source distortion.
Besides, an embedding strength adjustment strategy is designed to improve the
visual quality of the watermarked image with little loss of extraction
accuracy. The experimental results show that the scheme has higher robustness
and visual quality than other three recent state-of-the-arts. Specially, even
if the shooting distances and angles are in extreme, our scheme can also obtain
high extraction accuracy.",https://github.com/gslxr/Screen-Shooting-Resilient-Document-Image-Watermarking,-1
2abd93f0-0e93-4503-a3d9-b4b0b1229a8e,Efficient Long Sequence Modeling via State Space Augmented Transformer,0.809395,"Transformer models have achieved superior performance in various natural
language processing tasks. However, the quadratic computational cost of the
attention mechanism limits its practicality for long sequences. There are
existing attention variants that improve the computational efficiency, but they
have limited ability to effectively compute global information. In parallel to
Transformer models, state space models (SSMs) are tailored for long sequences,
but they are not flexible enough to capture complicated local information. We
propose SPADE, short for $\underline{\textbf{S}}$tate
s$\underline{\textbf{P}}$ace
$\underline{\textbf{A}}$ugmente$\underline{\textbf{D}}$
Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the
bottom layer of SPADE, and we employ efficient local attention methods for the
other layers. The SSM augments global information, which complements the lack
of long-range dependency issue in local attention methods. Experimental results
on the Long Range Arena benchmark and language modeling tasks demonstrate the
effectiveness of the proposed method. To further demonstrate the scalability of
SPADE, we pre-train large encoder-decoder models and present fine-tuning
results on natural language understanding and natural language generation
tasks.",https://github.com/microsoft/EfficientLongSequenceModeling,82331
1870a56a-401f-471d-9c43-41915e7d4bf2,Lightweight Monocular Depth Estimation with an Edge Guided Network,0.254361,"Monocular depth estimation is an important task that can be applied to many
robotic applications. Existing methods focus on improving depth estimation
accuracy via training increasingly deeper and wider networks, however these
suffer from large computational complexity. Recent studies found that edge
information are important cues for convolutional neural networks (CNNs) to
estimate depth. Inspired by the above observations, we present a novel
lightweight Edge Guided Depth Estimation Network (EGD-Net) in this study. In
particular, we start out with a lightweight encoder-decoder architecture and
embed an edge guidance branch which takes as input image gradients and
multi-scale feature maps from the backbone to learn the edge attention
features. In order to aggregate the context information and edge attention
features, we design a transformer-based feature aggregation module (TRFA). TRFA
captures the long-range dependencies between the context information and edge
attention features through cross-attention mechanism. We perform extensive
experiments on the NYU depth v2 dataset. Experimental results show that the
proposed method runs about 96 fps on a Nvidia GTX 1080 GPU whilst achieving the
state-of-the-art performance in terms of accuracy.",None,10318
7f97240f-93fe-4eaf-9fb3-4d9725094444,Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models,0.826226,"Neural network models have achieved state-of-the-art performance on
grapheme-to-phoneme (G2P) conversion. However, their performance relies on
large-scale pronunciation dictionaries, which may not be available for a lot of
languages. Inspired by the success of the pre-trained language model BERT, this
paper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which
is built by self-supervised training on a large, language-specific word list
with only grapheme information. Furthermore, two approaches are developed to
incorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e.,
fine-tuning GBERT or fusing GBERT into the Transformer model by attention.
Experimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean
datasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our
GBERT-based G2P models under both medium-resource and low-resource data
conditions.",https://github.com/ldong1111/GraphemeBERT,-1
713274a2-f282-4f12-b22d-f1a3e7f9428d,EfficientNeRF: Efficient Neural Radiance Fields,0.894623,"Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications.",https://github.com/dvlab-research/EfcientNeRF,-1
e454ae5e-43d2-4491-b26a-d099becab162,Lexical semantics enhanced neural word embeddings,0.133078,"Current breakthroughs in natural language processing have benefited
dramatically from neural language models, through which distributional
semantics can leverage neural data representations to facilitate downstream
applications. Since neural embeddings use context prediction on word
co-occurrences to yield dense vectors, they are inevitably prone to capture
more semantic association than semantic similarity. To improve vector space
models in deriving semantic similarity, we post-process neural word embeddings
through deep metric learning, through which we can inject lexical-semantic
relations, including syn/antonymy and hypo/hypernymy, into a distributional
space. We introduce hierarchy-fitting, a novel semantic specialization approach
to modelling semantic similarity nuances inherently stored in the IS-A
hierarchies. Hierarchy-fitting attains state-of-the-art results on the common-
and rare-word benchmark datasets for deriving semantic similarity from neural
word embeddings. It also incorporates an asymmetric distance function to
specialize hypernymy's directionality explicitly, through which it
significantly improves vanilla embeddings in multiple evaluation tasks of
detecting hypernymy and directionality without negative impacts on semantic
similarity judgement. The results demonstrate the efficacy of hierarchy-fitting
in specializing neural embeddings with semantic relations in late fusion,
potentially expanding its applicability to aggregating heterogeneous data and
various knowledge resources for learning multimodal semantic spaces.",None,-1
0237c292-4b2d-46af-91bc-0af6cb206aef,Language-independence of DisCoCirc's Text Circuits: English and Urdu,0.154703,"DisCoCirc is a newly proposed framework for representing the grammar and
semantics of texts using compositional, generative circuits. While it
constitutes a development of the Categorical Distributional Compositional
(DisCoCat) framework, it exposes radically new features. In particular, [14]
suggested that DisCoCirc goes some way toward eliminating grammatical
differences between languages. In this paper we provide a sketch that this is
indeed the case for restricted fragments of English and Urdu. We first develop
DisCoCirc for a fragment of Urdu, as it was done for English in [14]. There is
a simple translation from English grammar to Urdu grammar, and vice versa. We
then show that differences in grammatical structure between English and Urdu -
primarily relating to the ordering of words and phrases - vanish when passing
to DisCoCirc circuits.",None,-1
b0baaf0e-1845-403e-979c-09580454cd9e,Modeling the Lighting in Scenes as Style for Auto White-Balance Correction,0.3732,"Style may refer to different concepts (e.g. painting style, hairstyle,
texture, color, filter, etc.) depending on how the feature space is formed. In
this work, we propose a novel idea of interpreting the lighting in the single-
and multi-illuminant scenes as the concept of style. To verify this idea, we
introduce an enhanced auto white-balance (AWB) method that models the lighting
in single- and mixed-illuminant scenes as the style factor. Our AWB method does
not require any illumination estimation step, yet contains a network learning
to generate the weighting maps of the images with different WB settings.
Proposed network utilizes the style information, extracted from the scene by a
multi-head style extraction module. AWB correction is completed after blending
these weighting maps and the scene. Experiments on single- and mixed-illuminant
datasets demonstrate that our proposed method achieves promising correction
results when compared to the recent works. This shows that the lighting in the
scenes with multiple illuminations can be modeled by the concept of style.
Source code and trained models are available on
https://github.com/birdortyedi/lighting-as-style-awb-correction.",https://github.com/birdortyedi/lighting-as-style-awb-correction,-1
e2bb01d6-a2ee-4164-8c29-9cdfc9447dc6,Finding the optimal human strategy for Wordle using maximum correct letter probabilities and reinforcement learning,0.297163,"Wordle is an online word puzzle game that gained viral popularity in January
2022. The goal is to guess a hidden five letter word. After each guess, the
player gains information about whether the letters they guessed are present in
the word, and whether they are in the correct position. Numerous blogs have
suggested guessing strategies and starting word lists that improve the chance
of winning. Optimized algorithms can win 100% of games within five of the six
allowed trials. However, it is infeasible for human players to use these
algorithms due to an inability to perfectly recall all known 5-letter words and
perform complex calculations that optimize information gain. Here, we present
two different methods for choosing starting words along with a framework for
discovering the optimal human strategy based on reinforcement learning. Human
Wordle players can use the rules we discover to optimize their chance of
winning.",https://github.com/benton-anderson/wordle-opt,-1
265fc7d5-8b3e-4526-8b07-4578135fd712,Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment,0.872701,"Training a generative adversarial network (GAN) with limited data has been a
challenging task. A feasible solution is to start with a GAN well-trained on a
large scale source domain and adapt it to the target domain with a few samples,
termed as few shot generative model adaption. However, existing methods are
prone to model overfitting and collapse in extremely few shot setting (less
than 10). To solve this problem, we propose a relaxed spatial structural
alignment method to calibrate the target generative models during the adaption.
We design a cross-domain spatial structural consistency loss comprising the
self-correlation and disturbance correlation consistency loss. It helps align
the spatial structural information between the synthesis image pairs of the
source and target domains. To relax the cross-domain alignment, we compress the
original latent space of generative models to a subspace. Image pairs generated
from the subspace are pulled closer. Qualitative and quantitative experiments
show that our method consistently surpasses the state-of-the-art methods in few
shot setting.",https://github.com/StevenShaw1999/RSSA,18126
8a788169-2014-417d-b5a8-2c965145dfc8,CORE: Consistent Representation Learning for Face Forgery Detection,0.694656,"Face manipulation techniques develop rapidly and arouse widespread public
concerns. Despite that vanilla convolutional neural networks achieve acceptable
performance, they suffer from the overfitting issue. To relieve this issue,
there is a trend to introduce some erasing-based augmentations. We find that
these methods indeed attempt to implicitly induce more consistent
representations for different augmentations via assigning the same label for
different augmented images. However, due to the lack of explicit
regularization, the consistency between different representations is less
satisfactory. Therefore, we constrain the consistency of different
representations explicitly and propose a simple yet effective framework,
COnsistent REpresentation Learning (CORE). Specifically, we first capture the
different representations with different augmentations, then regularize the
cosine distance of the representations to enhance the consistency. Extensive
experiments (in-dataset and cross-dataset) demonstrate that CORE performs
favorably against state-of-the-art face forgery detection methods.",https://github.com/niyunsheng/CORE,-1
3c307ff7-e00e-4dd4-b51c-d234ae566be8,Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation,0.960023,"Scene flow estimation, which extracts point-wise motion between scenes, is
becoming a crucial task in many computer vision tasks. However, all of the
existing estimation methods utilize only the unidirectional features,
restricting the accuracy and generality. This paper presents a novel scene flow
estimation architecture using bidirectional flow embedding layers. The proposed
bidirectional layer learns features along both forward and backward directions,
enhancing the estimation performance. In addition, hierarchical feature
extraction and warping improve the performance and reduce computational
overhead. Experimental results show that the proposed architecture achieved a
new state-of-the-art record by outperforming other approaches with large margin
in both FlyingThings3D and KITTI benchmarks. Codes are available at
https://github.com/cwc1260/BiFlow.",https://github.com/cwc1260/BiFlow,1348
ff4ad3b0-30a0-4656-aef8-1144345ff616,Data-driven Feature Tracking for Event Cameras,0.957945,"Because of their high temporal resolution, increased resilience to motion
blur, and very sparse output, event cameras have been shown to be ideal for
low-latency and low-bandwidth feature tracking, even in challenging scenarios.
Existing feature tracking methods for event cameras are either handcrafted or
derived from first principles but require extensive parameter tuning, are
sensitive to noise, and do not generalize to different scenarios due to
unmodeled effects. To tackle these deficiencies, we introduce the first
data-driven feature tracker for event cameras, which leverages low-latency
events to track features detected in a grayscale frame. We achieve robust
performance via a novel frame attention module, which shares information across
feature tracks. By directly transferring zero-shot from synthetic to real data,
our data-driven tracker outperforms existing approaches in relative feature age
by up to 120% while also achieving the lowest latency. This performance gap is
further increased to 130% by adapting our tracker to real data with a novel
self-supervision strategy.",https://github.com/uzh-rpg/deep_ev_tracker,-1
96859d0e-d217-46f4-aa54-36b95ac4f206,An Assessment of the Impact of OCR Noise on Language Models,0.0926918,"Neural language models are the backbone of modern-day natural language
processing applications. Their use on textual heritage collections which have
undergone Optical Character Recognition (OCR) is therefore also increasing.
Nevertheless, our understanding of the impact OCR noise could have on language
models is still limited. We perform an assessment of the impact OCR noise has
on a variety of language models, using data in Dutch, English, French and
German. We find that OCR noise poses a significant obstacle to language
modelling, with language models increasingly diverging from their noiseless
targets as OCR quality lowers. In the presence of small corpora, simpler models
including PPMI and Word2Vec consistently outperform transformer-based models in
this respect.",https://doi.org/10.5281/zenodo.5799211,-1
44db87e4-2666-475e-92d1-a587e581c4df,User-Controllable Latent Transformer for StyleGAN Image Layout Editing,0.568531,"Latent space exploration is a technique that discovers interpretable latent
directions and manipulates latent codes to edit various attributes in images
generated by generative adversarial networks (GANs). However, in previous work,
spatial control is limited to simple transformations (e.g., translation and
rotation), and it is laborious to identify appropriate latent directions and
adjust their parameters. In this paper, we tackle the problem of editing the
StyleGAN image layout by annotating the image directly. To do so, we propose an
interactive framework for manipulating latent codes in accordance with the user
inputs. In our framework, the user annotates a StyleGAN image with locations
they want to move or not and specifies a movement direction by mouse dragging.
From these user inputs and initial latent codes, our latent transformer based
on a transformer encoder-decoder architecture estimates the output latent
codes, which are fed to the StyleGAN generator to obtain a result image. To
train our latent transformer, we utilize synthetic data and pseudo-user inputs
generated by off-the-shelf StyleGAN and optical flow models, without manual
supervision. Quantitative and qualitative evaluations demonstrate the
effectiveness of our method over existing methods.",https://github.com/justinpinkney/awesome-pretrained-stylegan2,-1
4bd3a7b6-abff-44d6-856c-eb982ebf95a5,"Artificial Intelligence for Cybersecurity: Threats, Attacks and Mitigation",0.835668,"With the advent of the digital era, every day-to-day task is automated due to
technological advances. However, technology has yet to provide people with
enough tools and safeguards. As the internet connects more-and-more devices
around the globe, the question of securing the connected devices grows at an
even spiral rate. Data thefts, identity thefts, fraudulent transactions,
password compromises, and system breaches are becoming regular everyday news.
The surging menace of cyber-attacks got a jolt from the recent advancements in
Artificial Intelligence. AI is being applied in almost every field of different
sciences and engineering. The intervention of AI not only automates a
particular task but also improves efficiency by many folds. So it is evident
that such a scrumptious spread would be very appetizing to cybercriminals. Thus
the conventional cyber threats and attacks are now ``intelligent"" threats. This
article discusses cybersecurity and cyber threats along with both conventional
and intelligent ways of defense against cyber-attacks. Furthermore finally, end
the discussion with the potential prospects of the future of AI in
cybersecurity.",None,-1
251f156a-2106-4b7a-a722-ea0ba61e86ca,Adaptive Local-Component-aware Graph Convolutional Network for One-shot Skeleton-based Action Recognition,0.721547,"Skeleton-based action recognition receives increasing attention because the
skeleton representations reduce the amount of training data by eliminating
visual information irrelevant to actions. To further improve the sample
efficiency, meta-learning-based one-shot learning solutions were developed for
skeleton-based action recognition. These methods find the nearest neighbor
according to the similarity between instance-level global average embedding.
However, such measurement holds unstable representativity due to inadequate
generalized learning on local invariant and noisy features, while intuitively,
more fine-grained recognition usually relies on determining key local body
movements. To address this limitation, we present the Adaptive
Local-Component-aware Graph Convolutional Network, which replaces the
comparison metric with a focused sum of similarity measurements on aligned
local embedding of action-critical spatial/temporal segments. Comprehensive
one-shot experiments on the public benchmark of NTU-RGB+D 120 indicate that our
method provides a stronger representation than the global embedding and helps
our model reach state-of-the-art.",None,-1
f9e44d80-e4e3-4581-a750-1401bfe4808e,Contingency-constrained economic dispatch with safe reinforcement learning,0.0689895,"Future power systems will rely heavily on micro grids with a high share of
decentralised renewable energy sources and energy storage systems. The high
complexity and uncertainty in this context might make conventional power
dispatch strategies infeasible. Reinforcement-learning based (RL) controllers
can address this challenge, however, cannot themselves provide safety
guarantees, preventing their deployment in practice. To overcome this
limitation, we propose a formally validated RL controller for economic
dispatch. We extend conventional constraints by a time-dependent constraint
encoding the islanding contingency. The contingency constraint is computed
using set-based backwards reachability analysis and actions of the RL agent are
verified through a safety layer. Unsafe actions are projected into the safe
action space while leveraging constrained zonotope set representations for
computational efficiency. The developed approach is demonstrated on a
residential use case using real-world measurements.",None,-1
8e9e562d-ca9c-4810-9766-fe9e64c16ac0,Efficient Algorithms for Planning with Participation Constraints,0.501251,"We consider the problem of planning with participation constraints introduced
in [Zhang et al., 2022]. In this problem, a principal chooses actions in a
Markov decision process, resulting in separate utilities for the principal and
the agent. However, the agent can and will choose to end the process whenever
his expected onward utility becomes negative. The principal seeks to compute
and commit to a policy that maximizes her expected utility, under the
constraint that the agent should always want to continue participating. We
provide the first polynomial-time exact algorithm for this problem for
finite-horizon settings, where previously only an additive
$\varepsilon$-approximation algorithm was known. Our approach can also be
extended to the (discounted) infinite-horizon case, for which we give an
algorithm that runs in time polynomial in the size of the input and
$\log(1/\varepsilon)$, and returns a policy that is optimal up to an additive
error of $\varepsilon$.",None,-1
3cc42bb7-f165-4427-865e-3cdc77bac527,3D Dual-Fusion: Dual-Domain Dual-Query Camera-LiDAR Fusion for 3D Object Detection,0.635653,"Fusing data from cameras and LiDAR sensors is an essential technique to
achieve robust 3D object detection. One key challenge in camera-LiDAR fusion
involves mitigating the large domain gap between the two sensors in terms of
coordinates and data distribution when fusing their features. In this paper, we
propose a novel camera-LiDAR fusion architecture called, 3D Dual-Fusion, which
is designed to mitigate the gap between the feature representations of camera
and LiDAR data. The proposed method fuses the features of the camera-view and
3D voxel-view domain and models their interactions through deformable
attention. We redesign the transformer fusion encoder to aggregate the
information from the two domains. Two major changes include 1) dual query-based
deformable attention to fuse the dual-domain features interactively and 2) 3D
local self-attention to encode the voxel-domain queries prior to dual-query
decoding. The results of an experimental evaluation show that the proposed
camera-LiDAR fusion architecture achieved competitive performance on the KITTI
and nuScenes datasets, with state-of-the-art performances in some 3D object
detection benchmarks categories.",None,3955
f0db1084-6000-4b2e-8d35-602b103da021,Large Language Models Struggle to Learn Long-Tail Knowledge,0.999999,"The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail.",https://github.com/kingoflolz/mesh-transformer-jax,-1
84671fe5-e948-49fc-b58e-9c9925872971,3D Pose Based Feedback for Physical Exercises,0.901406,"Unsupervised self-rehabilitation exercises and physical training can cause
serious injuries if performed incorrectly. We introduce a learning-based
framework that identifies the mistakes made by a user and proposes corrective
measures for easier and safer individual training. Our framework does not rely
on hard-coded, heuristic rules. Instead, it learns them from data, which
facilitates its adaptation to specific user needs. To this end, we use a Graph
Convolutional Network (GCN) architecture acting on the user's pose sequence to
model the relationship between the body joints trajectories. To evaluate our
approach, we introduce a dataset with 3 different physical exercises. Our
approach yields 90.9% mistake identification accuracy and successfully corrects
94.2% of the mistakes.",None,-1
5436fe13-e37f-436b-b5c3-0dfe6dea4f7c,NAFSSR: Stereo Image Super-Resolution Using NAFNet,0.938019,"Stereo image super-resolution aims at enhancing the quality of
super-resolution results by utilizing the complementary information provided by
binocular systems. To obtain reasonable performance, most methods focus on
finely designing modules, loss functions, and etc. to exploit information from
another viewpoint. This has the side effect of increasing system complexity,
making it difficult for researchers to evaluate new ideas and compare methods.
This paper inherits a strong and simple image restoration model, NAFNet, for
single-view feature extraction and extends it by adding cross attention modules
to fuse features between views to adapt to binocular scenarios. The proposed
baseline for stereo image super-resolution is noted as NAFSSR. Furthermore,
training/testing strategies are proposed to fully exploit the performance of
NAFSSR. Extensive experiments demonstrate the effectiveness of our method. In
particular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012,
KITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place
in the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models
will be released at https://github.com/megvii-research/NAFNet.",https://github.com/megvii-research/NAFNet,-1
70971da0-53af-4c91-93f5-c5a9e760a03b,Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning,0.191265,"Prompt tuning is a parameter-efficient approach to adapting pre-trained
language models to downstream tasks. Although prompt tuning has been shown to
match the performance of full model tuning when training data is sufficient, it
tends to struggle in few-shot learning settings. In this paper, we present
Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot
learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks.
On downstream tasks, the pre-trained prompts are selectively activated and
combined, leading to strong compositional generalization to unseen tasks. To
bridge the gap between pre-training and fine-tuning, we formulate upstream and
downstream tasks into a unified machine reading comprehension task. Extensive
experiments under two learning paradigms, i.e., gradient descent and black-box
tuning, show that MP2 significantly outperforms prompt tuning, full model
tuning, and prior prompt pre-training methods in few-shot settings. In
addition, we demonstrate that MP2 can achieve surprisingly fast and strong
adaptation to downstream tasks by merely learning 8 parameters to combine the
pre-trained modular prompts.",https://github.com/Hzfinfdu/MPMP,-1
ec08b187-ea28-4686-807b-60e77cb4f5ea,Breaking BERT: Evaluating and Optimizing Sparsified Attention,0.026288,"Transformers allow attention between all pairs of tokens, but there is reason
to believe that most of these connections - and their quadratic time and memory
- may not be necessary. But which ones? We evaluate the impact of
sparsification patterns with a series of ablation experiments. First, we
compare masks based on syntax, lexical similarity, and token position to random
connections, and measure which patterns reduce performance the least. We find
that on three common finetuning tasks even using attention that is at least 78%
sparse can have little effect on performance if applied at later transformer
layers, but that applying sparsity throughout the network reduces performance
significantly. Second, we vary the degree of sparsity for three patterns
supported by previous work, and find that connections to neighbouring tokens
are the most significant. Finally, we treat sparsity as an optimizable
parameter, and present an algorithm to learn degrees of neighboring connections
that gives a fine-grained control over the accuracy-sparsity trade-off while
approaching the performance of existing methods.",None,-1
da003ede-a671-41b0-a64d-e647f2c8157c,NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models,0.783808,"Neural image caption generation (NICG) models have received massive attention
from the research community due to their excellent performance in visual
understanding. Existing work focuses on improving NICG model accuracy while
efficiency is less explored. However, many real-world applications require
real-time feedback, which highly relies on the efficiency of NICG models.
Recent research observed that the efficiency of NICG models could vary for
different inputs. This observation brings in a new attack surface of NICG
models, i.e., An adversary might be able to slightly change inputs to cause the
NICG models to consume more computational resources. To further understand such
efficiency-oriented threats, we propose a new attack approach, NICGSlowDown, to
evaluate the efficiency robustness of NICG models. Our experimental results
show that NICGSlowDown can generate images with human-unnoticeable
perturbations that will increase the NICG model latency up to 483.86%. We hope
this research could raise the community's concern about the efficiency
robustness of NICG models.",https://github.com/NICGSlowDown,-1
b7afc19c-9f61-46d5-b633-6aff4f0cd243,Guess What Moves: Unsupervised Video and Image Segmentation by Anticipating Motion,0.610219,"Motion, measured via optical flow, provides a powerful cue to discover and
learn objects in images and videos. However, compared to using appearance, it
has some blind spots, such as the fact that objects become invisible if they do
not move. In this work, we propose an approach that combines the strengths of
motion-based and appearance-based segmentation. We propose to supervise an
image segmentation network with the pretext task of predicting regions that are
likely to contain simple motion patterns, and thus likely to correspond to
objects. As the model only uses a single image as input, we can apply it in two
settings: unsupervised video segmentation, and unsupervised image segmentation.
We achieve state-of-the-art results for videos, and demonstrate the viability
of our approach on still images containing novel objects. Additionally we
experiment with different motion models and optical flow backbones and find the
method to be robust to these change. Project page and code available at
https://www.robots.ox.ac.uk/~vgg/research/gwm.",https://www.robots.ox.ac.uk/~vgg/research/gwm,-1
7843c206-f8c9-45f4-bcd8-94c8615ab980,Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation,0.468744,"Past works on multimodal machine translation (MMT) elevate bilingual setup by
incorporating additional aligned vision information. However, an image-must
requirement of the multimodal dataset largely hinders MMT's development --
namely that it demands an aligned form of [image, source text, target text].
This limitation is generally troublesome during the inference phase especially
when the aligned image is not provided as in the normal NMT setup. Thus, in
this work, we introduce IKD-MMT, a novel MMT framework to support the
image-free inference phase via an inversion knowledge distillation scheme. In
particular, a multimodal feature generator is executed with a knowledge
distillation module, which directly generates the multimodal feature from
(only) source texts as the input. While there have been a few prior works
entertaining the possibility to support image-free inference for machine
translation, their performances have yet to rival the image-must translation.
In our experiments, we identify our method as the first image-free approach to
comprehensively rival or even surpass (almost) all image-must frameworks, and
achieved the state-of-the-art result on the often-used Multi30k benchmark. Our
code and data are available at: https://github.com/pengr/IKD-mmt/tree/master..",https://github.com/pengr/IKD-mmt/tree/master,-1
90e1c798-05af-4841-aece-b3368e8a5ce7,A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing,0.317593,"A key algorithm for understanding the world is material segmentation, which
assigns a label (metal, glass, etc.) to each pixel. We find that a model
trained on existing data underperforms in some settings and propose to address
this with a large-scale dataset of 3.2 million dense segments on 44,560 indoor
and outdoor images, which is 23x more segments than existing data. Our data
covers a more diverse set of scenes, objects, viewpoints and materials, and
contains a more fair distribution of skin types. We show that a model trained
on our data outperforms a state-of-the-art model across datasets and
viewpoints. We propose a large-scale scene parsing benchmark and baseline of
0.729 per-pixel accuracy, 0.585 mean class accuracy and 0.420 mean IoU across
46 materials.",https://github.com/apple/ml-dms-dataset,-1
8282a19d-b809-4c5d-bc22-13c8c21492cc,CamoFormer: Masked Separable Attention for Camouflaged Object Detection,0.623977,"How to identify and segment camouflaged objects from the background is
challenging. Inspired by the multi-head self-attention in Transformers, we
present a simple masked separable attention (MSA) for camouflaged object
detection. We first separate the multi-head self-attention into three parts,
which are responsible for distinguishing the camouflaged objects from the
background using different mask strategies. Furthermore, we propose to capture
high-resolution semantic representations progressively based on a simple
top-down decoder with the proposed MSA to attain precise segmentation results.
These structures plus a backbone encoder form a new model, dubbed CamoFormer.
Extensive experiments show that CamoFormer surpasses all existing
state-of-the-art methods on three widely-used camouflaged object detection
benchmarks. There are on average around 5% relative improvements over previous
methods in terms of S-measure and weighted F-measure.",https://github.com/HVision-NKU/CamoFormer,-1
c8fa2e93-3856-4b0e-a5b0-28814cc51153,TAS-NIR: A VIS+NIR Dataset for Fine-grained Semantic Segmentation in Unstructured Outdoor Environments,0.376858,"Vegetation Indices based on paired images of the visible color spectrum (VIS)
and near infrared spectrum (NIR) have been widely used in remote sensing
applications. These vegetation indices are extended for their application in
autonomous driving in unstructured outdoor environments. In this domain we can
combine traditional vegetation indices like the Normalized Difference
Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI) with Convolutional
Neural Networks (CNNs) pre-trained on available VIS datasets. By laying a focus
on learning calibrated CNN outputs, we can provide an approach to fuse known
hand-crafted image features with CNN predictions for different domains as well.
The method is evaluated on a VIS+NIR dataset of semantically annotated images
in unstructured outdoor environments. The dataset is available at
mucar3.de/iros2022-ppniv-tas-nir.",None,-1
380cc7d7-f50d-4451-b60b-d15bc25a2160,LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs,0.806713,"Recent advance in 2D CNNs has revealed that large kernels are important.
However, when directly applying large convolutional kernels in 3D CNNs, severe
difficulties are met, where those successful module designs in 2D become
surprisingly ineffective on 3D networks, including the popular depth-wise
convolution. To address this vital challenge, we instead propose the
spatial-wise partition convolution and its large-kernel module. As a result, it
avoids the optimization and efficiency issues of naive 3D large kernels. Our
large-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D
tasks of semantic segmentation and object detection. It achieves 73.9% mIoU on
the ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection
benchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance
further boosts to 74.2% NDS with a simple multi-modal fusion. In addition,
LargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object
detection. For the first time, we show that large kernels are feasible and
essential for 3D visual tasks.",https://github.com/dvlab-research/LargeKernel3D,306651
9e35788e-ff51-4ca1-8ea7-3100f479d923,Realistic Blur Synthesis for Learning Image Deblurring,0.737305,"Training learning-based deblurring methods demands a tremendous amount of
blurred and sharp image pairs. Unfortunately, existing synthetic datasets are
not realistic enough, and deblurring models trained on them cannot handle real
blurred images effectively. While real datasets have recently been proposed,
they provide limited diversity of scenes and camera settings, and capturing
real datasets for diverse settings is still challenging. To resolve this, this
paper analyzes various factors that introduce differences between real and
synthetic blurred images. To this end, we present RSBlur, a novel dataset with
real blurred images and the corresponding sharp image sequences to enable a
detailed analysis of the difference between real and synthetic blur. With the
dataset, we reveal the effects of different factors in the blur generation
process. Based on the analysis, we also present a novel blur synthesis pipeline
to synthesize more realistic blur. We show that our synthesis pipeline can
improve the deblurring performance on real blurred images.",None,-1
fead796c-918b-4911-9bda-759c651cb7eb,Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning,0.589504,"The spread of rumors along with breaking events seriously hinders the truth
in the era of social media. Previous studies reveal that due to the lack of
annotated resources, rumors presented in minority languages are hard to be
detected. Furthermore, the unforeseen breaking events not involved in
yesterday's news exacerbate the scarcity of data resources. In this work, we
propose a novel zero-shot framework based on prompt learning to detect rumors
falling in different domains or presented in different languages. More
specifically, we firstly represent rumor circulated on social media as diverse
propagation threads, then design a hierarchical prompt encoding mechanism to
learn language-agnostic contextual representations for both prompts and rumor
data. To further enhance domain adaptation, we model the domain-invariant
structural features from the propagation threads, to incorporate structural
position representations of influential community response. In addition, a new
virtual response augmentation method is used to improve model training.
Extensive experiments conducted on three real-world datasets demonstrate that
our proposed model achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages.",https://github.com/PengyaoYi/zeroRumor,-1
6f3b4ae3-6688-4205-a681-495d14fbc8aa,TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition,0.531379,"Creation of 3D content by stylization is a promising yet challenging problem
in computer vision and graphics research. In this work, we focus on stylizing
photorealistic appearance renderings of a given surface mesh of arbitrary
topology. Motivated by the recent surge of cross-modal supervision of the
Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which
transfers the appearance style of a given 3D shape according to a text prompt
in a photorealistic manner. Technically, we propose to disentangle the
appearance style as the spatially varying bidirectional reflectance
distribution function, the local geometric variation, and the lighting
condition, which are jointly optimized, via supervision of the CLIP loss, by a
spherical Gaussians based differentiable renderer. As such, TANGO enables
photorealistic 3D style transfer by automatically predicting reflectance
effects even for bare, low-quality meshes, without training on a task-specific
dataset. Extensive experiments show that TANGO outperforms existing methods of
text-driven 3D style transfer in terms of photorealistic quality, consistency
of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and
results are available at our project webpage https://cyw-3d.github.io/tango/.",https://cyw-3d.github.io/tango,-1
4c3193a2-ae0e-48f9-b192-218c06a90e6e,"Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)",0.452609,"We study the average robustness notion in deep neural networks in (selected)
wide and narrow, deep and shallow, as well as lazy and non-lazy training
settings. We prove that in the under-parameterized setting, width has a
negative effect while it improves robustness in the over-parameterized setting.
The effect of depth closely depends on the initialization and the training
mode. In particular, when initialized with LeCun initialization, depth helps
robustness with the lazy training regime. In contrast, when initialized with
Neural Tangent Kernel (NTK) and He-initialization, depth hurts the robustness.
Moreover, under the non-lazy training regime, we demonstrate how the width of a
two-layer ReLU network benefits robustness. Our theoretical developments
improve the results by [Huang et al. NeurIPS21; Wu et al. NeurIPS21] and are
consistent with [Bubeck and Sellke NeurIPS21; Bubeck et al. COLT21].",https://github.com/bearpaw/pytorch-classification,-1
a6f05c1e-96c4-4418-8b67-e28f1dde41fb,Defending Compositionality in Emergent Languages,0.0449135,"Compositionality has traditionally been understood as a major factor in
productivity of language and, more broadly, human cognition. Yet, recently,
some research started to question its status, showing that artificial neural
networks are good at generalization even without noticeable compositional
behavior. We argue that some of these conclusions are too strong and/or
incomplete. In the context of a two-agent communication game, we show that
compositionality indeed seems essential for successful generalization when the
evaluation is done on a proper dataset.",https://github.com/michal-au/emlang-compos.git,-1
aa5fd226-99d1-40bf-a7df-0b28e3b0a9b4,On the Road to Online Adaptation for Semantic Image Segmentation,0.936674,"We propose a new problem formulation and a corresponding evaluation framework
to advance research on unsupervised domain adaptation for semantic image
segmentation. The overall goal is fostering the development of adaptive
learning systems that will continuously learn, without supervision, in
ever-changing environments. Typical protocols that study adaptation algorithms
for segmentation models are limited to few domains, adaptation happens offline,
and human intervention is generally required, at least to annotate data for
hyper-parameter tuning. We argue that such constraints are incompatible with
algorithms that can continuously adapt to different real-world situations. To
address this, we propose a protocol where models need to learn online, from
sequences of temporally correlated images, requiring continuous, frame-by-frame
adaptation. We accompany this new protocol with a variety of baselines to
tackle the proposed formulation, as well as an extensive analysis of their
behaviors, which can serve as a starting point for future research.",https://github.com/naver/oasis,-1
1b2b3667-18e9-49e8-8bc8-782718fe277e,Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection,0.222062,"Video anomaly detection is a core problem in vision. Correctly detecting and
identifying anomalous behaviors in pedestrians from video data will enable
safety-critical applications such as surveillance, activity monitoring, and
human-robot interaction. In this paper, we propose to leverage trajectory
localization and prediction for unsupervised pedestrian anomaly event
detection. Different than previous reconstruction-based approaches, our
proposed framework rely on the prediction errors of normal and abnormal
pedestrian trajectories to detect anomalies spatially and temporally. We
present experimental results on real-world benchmark datasets on varying
timescales and show that our proposed trajectory-predictor-based anomaly
detection pipeline is effective and efficient at identifying anomalous
activities of pedestrians in videos. Code will be made available at
https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.",https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection,604
982f5d93-eaa2-44e7-b0b7-8bcf4b07671d,Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models,0.283111,"Zero-shot cross-lingual transfer learning has been shown to be highly
challenging for tasks involving a lot of linguistic specificities or when a
cultural gap is present between languages, such as in hate speech detection. In
this paper, we highlight this limitation for hate speech detection in several
domains and languages using strict experimental settings. Then, we propose to
train on multilingual auxiliary tasks -- sentiment analysis, named entity
recognition, and tasks relying on syntactic information -- to improve zero-shot
transfer of hate speech detection models across languages. We show how hate
speech detection models benefit from a cross-lingual knowledge proxy brought by
auxiliary tasks fine-tuning and highlight these tasks' positive impact on
bridging the hate speech linguistic and cultural gap between languages.",https://github.com/ArijRB/Multilingual-Auxiliary-Tasks-Training-Bridging-the-Gap-between-Languages-for-Zero-Shot-Transfer-of-/,-1
9c164c6e-9e05-4ba2-beb9-e3a90dbdb7a9,Improving Predictive Performance and Calibration by Weight Fusion in Semantic Segmentation,0.109199,"Averaging predictions of a deep ensemble of networks is apopular and
effective method to improve predictive performance andcalibration in various
benchmarks and Kaggle competitions. However, theruntime and training cost of
deep ensembles grow linearly with the size ofthe ensemble, making them
unsuitable for many applications. Averagingensemble weights instead of
predictions circumvents this disadvantageduring inference and is typically
applied to intermediate checkpoints ofa model to reduce training cost. Albeit
effective, only few works haveimproved the understanding and the performance of
weight averaging.Here, we revisit this approach and show that a simple weight
fusion (WF)strategy can lead to a significantly improved predictive performance
andcalibration. We describe what prerequisites the weights must meet interms of
weight space, functional space and loss. Furthermore, we presenta new test
method (called oracle test) to measure the functional spacebetween weights. We
demonstrate the versatility of our WF strategy acrossstate of the art
segmentation CNNs and Transformers as well as real worlddatasets such as
BDD100K and Cityscapes. We compare WF with similarapproaches and show our
superiority for in- and out-of-distribution datain terms of predictive
performance and calibration.",None,-1
87c4977f-b744-4d07-8b9f-7c153868fbf3,Nonlinear desirability theory,0.414903,"Desirability can be understood as an extension of Anscombe and Aumann's
Bayesian decision theory to sets of expected utilities. At the core of
desirability lies an assumption of linearity of the scale in which rewards are
measured. It is a traditional assumption used to derive the expected utility
model, which clashes with a general representation of rational decision making,
though. Allais has, in particular, pointed this out in 1953 with his famous
paradox. We note that the utility scale plays the role of a closure operator
when we regard desirability as a logical theory. This observation enables us to
extend desirability to the nonlinear case by letting the utility scale be
represented via a general closure operator. The new theory directly expresses
rewards in actual nonlinear currency (money), much in Savage's spirit, while
arguably weakening the founding assumptions to a minimum. We characterise the
main properties of the new theory both from the perspective of sets of gambles
and of their lower and upper prices (previsions). We show how Allais paradox
finds a solution in the new theory, and discuss the role of sets of
probabilities in the theory.",None,-1
a8eca41d-1f58-4a77-9a1f-a4a2969c0896,Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs,0.443671,"The extraction of aspect terms is a critical step in fine-grained sentiment
analysis of text. Existing approaches for this task have yielded impressive
results when the training and testing data are from the same domain. However,
these methods show a drastic decrease in performance when applied to
cross-domain settings where the domain of the testing data differs from that of
the training data. To address this lack of extensibility and robustness, we
propose a novel approach for automatically constructing domain-specific
knowledge graphs that contain information relevant to the identification of
aspect terms. We introduce a methodology for injecting information from these
knowledge graphs into Transformer models, including two alternative mechanisms
for knowledge insertion: via query enrichment and via manipulation of attention
patterns. We demonstrate state-of-the-art performance on benchmark datasets for
cross-domain aspect term extraction using our approach and investigate how the
amount of external knowledge available to the Transformer impacts model
performance.",None,-1
1f49c93d-9785-41b3-b51d-4d5489966cb7,Summarizing Patients Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models,0.692584,"Automatically summarizing patients' main problems from daily progress notes
using natural language processing methods helps to battle against information
and cognitive overload in hospital settings and potentially assists providers
with computerized diagnostic decision support. Problem list summarization
requires a model to understand, abstract, and generate clinical documentation.
In this work, we propose a new NLP task that aims to generate a list of
problems in a patient's daily care plan using input from the provider's
progress notes during hospitalization. We investigate the performance of T5 and
BART, two state-of-the-art seq2seq transformer architectures, in solving this
problem. We provide a corpus built on top of progress notes from publicly
available electronic health record progress notes in the Medical Information
Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain
text, and we experiment with a data augmentation method and a domain adaptation
pre-training method to increase exposure to medical vocabulary and knowledge.
Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence
embedding, and F-score on medical concepts. Results show that T5 with domain
adaptive pre-training achieves significant performance gains compared to a
rule-based system and general domain pre-trained language models, indicating a
promising direction for tackling the problem summarization task.",//git.doit.wisc.edu/smph/dom/UW-ICU-Data-Science-Lab/drbench,9607
4ff88da4-03b8-413e-884c-ffc44e69a82c,Decoupling Makes Weakly Supervised Local Feature Better,0.90894,"Weakly supervised learning can help local feature methods to overcome the
obstacle of acquiring a large-scale dataset with densely labeled
correspondences. However, since weak supervision cannot distinguish the losses
caused by the detection and description steps, directly conducting weakly
supervised learning within a joint describe-then-detect pipeline suffers
limited performance. In this paper, we propose a decoupled describe-then-detect
pipeline tailored for weakly supervised local feature learning. Within our
pipeline, the detection step is decoupled from the description step and
postponed until discriminative and robust descriptors are learned. In addition,
we introduce a line-to-window search strategy to explicitly use the camera pose
information for better descriptor learning. Extensive experiments show that our
method, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous
fully and weakly supervised methods and achieves state-of-the-art performance
on a wide range of downstream tasks.",https://github.com/The-Learning-And-Vision-Atelier-LAVA/PoSFeat,-1
421997e4-c97f-46ac-96f8-394d12ca739f,ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,0.571662,"This work introduces a new multi-task, parameter-efficient language model
(LM) tuning method that learns to transfer knowledge across different tasks via
a mixture of soft prompts-small prefix embedding vectors pre-trained for
different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt
Tuning), obtains source prompts as encodings of large-scale source tasks into a
small number of parameters and trains an attention module to interpolate the
source prompts and a newly initialized target prompt for every instance in the
target task. During training, only the target task prompt and the attention
weights, which are shared between tasks in multi-task training, are updated,
while the original LM and source prompts are intact. ATTEMPT is highly
parameter-efficient (e.g., updates 2,300 times fewer parameters than full
fine-tuning) while achieving high task performance using knowledge from
high-resource tasks. Moreover, it is modular using pre-trained soft prompts,
and can flexibly add or remove source prompts for effective knowledge transfer.
Our experimental results across 21 diverse NLP datasets show that ATTEMPT
significantly outperforms prompt tuning and outperforms or matches fully
fine-tuned or other parameter-efficient tuning approaches that use over ten
times more parameters. Finally, ATTEMPT outperforms previous work in few-shot
learning settings.",https://github.com/AkariAsai/ATTEMPT,-1
ef161bf8-b155-42fe-9791-b0e47bdb4021,Learning from Drivers to Tackle the Amazon Last Mile Routing Research Challenge,0.332901,"The goal of the Amazon Last Mile Routing Research Challenge is to integrate
the real-life experience of Amazon drivers into the solution of optimal route
planning and optimization. This paper presents our method that tackles this
challenge by hierarchically combining machine learning and conventional
Traveling Salesperson Problem (TSP) solvers. Our method reaps the benefits from
both worlds. On the one hand, our method encodes driver know-how by learning a
sequential probability model from historical routes at the zone level, where
each zone contains a few parcel stops. It then uses a single step policy
iteration method, known as the Rollout algorithm, to generate plausible zone
sequences sampled from the learned probability model. On the other hand, our
method utilizes proven methods developed in the rich TSP literature to sequence
stops within each zone efficiently. The outcome of such a combination appeared
to be promising. Our method obtained an evaluation score of $0.0374$, which is
comparable to what the top three teams have achieved on the official Challenge
leaderboard. Moreover, our learning-based method is applicable to driving
routes that may exhibit distinct sequential patterns beyond the scope of this
Challenge. The source code of our method is publicly available at
https://github.com/aws-samples/amazon-sagemaker-amazon-routing-challenge-sol",https://github.com/aws-samples/amazon-sagemaker-amazon-routing-challenge-sol,-1
bb95d676-a78c-43a1-b6cb-2d635aee43f0,Walk this Way! Entity Walks and Property Walks for RDF2vec,0.650062,"RDF2vec is a knowledge graph embedding mechanism which first extracts
sequences from knowledge graphs by performing random walks, then feeds those
into the word embedding algorithm word2vec for computing vector representations
for entities. In this poster, we introduce two new flavors of walk extraction
coined e-walks and p-walks, which put an emphasis on the structure or the
neighborhood of an entity respectively, and thereby allow for creating
embeddings which focus on similarity or relatedness. By combining the walk
strategies with order-aware and classic RDF2vec, as well as CBOW and skip-gram
word2vec embeddings, we conduct a preliminary evaluation with a total of 12
RDF2vec variants.",None,-1
31985d71-0c2a-4e61-833a-cc4ece7e68a2,Omnivore: A Single Model for Many Visual Modalities,0.926519,"Prior work has studied different visual modalities in isolation and developed
separate architectures for recognition of images, videos, and 3D data. Instead,
in this paper, we propose a single model which excels at classifying images,
videos, and single-view 3D data using exactly the same model parameters. Our
'Omnivore' model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities.
Omnivore is simple to train, uses off-the-shelf standard datasets, and performs
at-par or better than modality-specific models of the same size. A single
Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN
RGB-D. After finetuning, our models outperform prior work on a variety of
vision tasks and generalize across modalities. Omnivore's shared visual
representation naturally enables cross-modal recognition without access to
correspondences between modalities. We hope our results motivate researchers to
model visual modalities together.",None,111734
987268c4-6a93-4569-aa88-fd6e2786d413,BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining,0.999338,"Pre-trained language models have attracted increasing attention in the
biomedical domain, inspired by their great success in the general natural
language domain. Among the two main branches of pre-trained language models in
the general language domain, i.e., BERT (and its variants) and GPT (and its
variants), the first one has been extensively studied in the biomedical domain,
such as BioBERT and PubMedBERT. While they have achieved great success on a
variety of discriminative downstream biomedical tasks, the lack of generation
ability constrains their application scope. In this paper, we propose BioGPT, a
domain-specific generative Transformer language model pre-trained on large
scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and
demonstrate that our model outperforms previous models on most tasks.
Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI
end-to-end relation extraction tasks respectively, and 78.2% accuracy on
PubMedQA, creating a new record. Our case study on text generation further
demonstrates the advantage of BioGPT on biomedical literature to generate
fluent descriptions for biomedical terms. Code is available at
https://github.com/microsoft/BioGPT.",https://github.com/microsoft/BioGPT,-1
6cbce68b-8272-4998-817d-1f86be34a1dc,SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding,0.311969,"In this paper, we investigate how to achieve better visual grounding with
modern vision-language transformers, and propose a simple yet powerful
Selective Retraining (SiRi) mechanism for this challenging task. Particularly,
SiRi conveys a significant principle to the research of visual grounding, i.e.,
a better initialized vision-language encoder would help the model converge to a
better local minimum, advancing the performance accordingly. In specific, we
continually update the parameters of the encoder as the training goes on, while
periodically re-initialize rest of the parameters to compel the model to be
better optimized based on an enhanced encoder. SiRi can significantly
outperform previous approaches on three popular benchmarks. Specifically, our
method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the
state-of-the-art approaches (training from scratch) by more than 10.21%.
Additionally, we reveal that SiRi performs surprisingly superior even with
limited training data. We also extend it to transformer-based visual grounding
models and other vision-language tasks to verify the validity.",https://github.com/qumengxue/siri-vg.git,-1
adaaf973-73f0-4e36-be09-a730dd167f88,mm-Wave Radar Hand Shape Classification Using Deformable Transformers,0.274568,"A novel, real-time, mm-Wave radar-based static hand shape classification
algorithm and implementation are proposed. The method finds several
applications in low cost and privacy sensitive touchless control technology
using 60 Ghz radar as the sensor input. As opposed to prior Range-Doppler image
based 2D classification solutions, our method converts raw radar data to 3D
sparse cartesian point clouds.The demonstrated 3D radar neural network model
using deformable transformers significantly surpasses the performance results
set by prior methods which either utilize custom signal processing or apply
generic convolutional techniques on Range-Doppler FFT images. Experiments are
performed on an internally collected dataset using an off-the-shelf radar
sensor.",None,-1
77b001aa-251e-4729-8822-b1f61f49c077,Privacy-Preserving Personalized Fitness Recommender System (P3FitRec): A Multi-level Deep Learning Approach,0.305858,"Recommender systems have been successfully used in many domains with the help
of machine learning algorithms. However, such applications tend to use
multi-dimensional user data, which has raised widespread concerns about the
breach of users privacy. Meanwhile, wearable technologies have enabled users to
collect fitness-related data through embedded sensors to monitor their
conditions or achieve personalized fitness goals. In this paper, we propose a
novel privacy-aware personalized fitness recommender system. We introduce a
multi-level deep learning framework that learns important features from a
large-scale real fitness dataset that is collected from wearable IoT devices to
derive intelligent fitness recommendations. Unlike most existing approaches,
our approach achieves personalization by inferring the fitness characteristics
of users from sensory data and thus minimizing the need for explicitly
collecting user identity or biometric information, such as name, age, height,
weight. In particular, our proposed models and algorithms predict (a)
personalized exercise distance recommendations to help users to achieve target
calories, (b) personalized speed sequence recommendations to adjust exercise
speed given the nature of the exercise and the chosen route, and (c)
personalized heart rate sequence to guide the user of the potential health
status for future exercises. Our experimental evaluation on a real-world Fitbit
dataset demonstrated high accuracy in predicting exercise distance, speed
sequence, and heart rate sequence compared to similar studies. Furthermore, our
approach is novel compared to existing studies as it does not require
collecting and using users sensitive information, and thus it preserves the
users privacy.",None,-1
973973d9-3b0c-450e-acd6-ec34d613e3ce,FaceDancer: Pose- and Occlusion-Aware High Fidelity Face Swapping,0.546002,"In this work, we present a new single-stage method for subject agnostic face
swapping and identity transfer, named FaceDancer. We have two major
contributions: Adaptive Feature Fusion Attention (AFFA) and Interpreted Feature
Similarity Regularization (IFSR). The AFFA module is embedded in the decoder
and adaptively learns to fuse attribute features and features conditioned on
identity information without requiring any additional facial segmentation
process. In IFSR, we leverage the intermediate features in an identity encoder
to preserve important attributes such as head pose, facial expression,
lighting, and occlusion in the target face, while still transferring the
identity of the source face with high fidelity. We conduct extensive
quantitative and qualitative experiments on various datasets and show that the
proposed FaceDancer outperforms other state-of-the-art networks in terms of
identityn transfer, while having significantly better pose preservation than
most of the previous methods.",https://github.com/felixrosberg/FaceDancer,-1
c14b7db7-78f1-4b05-b351-5d5662bfaab4,Learned k-NN Distance Estimation,0.594149,"Big data mining is well known to be an important task for data science,
because it can provide useful observations and new knowledge hidden in given
large datasets. Proximity-based data analysis is particularly utilized in many
real-life applications. In such analysis, the distances to k nearest neighbors
are usually employed, thus its main bottleneck is derived from data retrieval.
Much efforts have been made to improve the efficiency of these analyses.
However, they still incur large costs, because they essentially need many data
accesses. To avoid this issue, we propose a machine-learning technique that
quickly and accurately estimates the k-NN distances (i.e., distances to the k
nearest neighbors) of a given query. We train a fully connected neural network
model and utilize pivots to achieve accurate estimation. Our model is designed
to have useful advantages: it infers distances to the k-NNs at a time, its
inference time is O(1) (no data accesses are incurred), but it keeps high
accuracy. Our experimental results and case studies on real datasets
demonstrate the efficiency and effectiveness of our solution.",https://github.com/arailly/pivnet,-1
786c4fef-4ca7-470a-a650-5125acea57ea,Construction and Evaluation of a Self-Attention Model for Semantic Understanding of Sentence-Final Particles,0.0440456,"Sentence-final particles serve an essential role in spoken Japanese because
they express the speaker's mental attitudes toward a proposition and/or an
interlocutor. They are acquired at early ages and occur very frequently in
everyday conversation. However, there has been little proposal for a
computational model of acquiring sentence-final particles. This paper proposes
Subjective BERT, a self-attention model that takes various subjective senses in
addition to language and images as input and learns the relationship between
words and subjective senses. An evaluation experiment revealed that the model
understands the usage of ""yo"", which expresses the speaker's intention to
communicate new information, and that of ""ne"", which denotes the speaker's
desire to confirm that some information is shared.",None,-1
0080b0ee-1573-420c-b5a8-ffe14c049127,A Simple Strategy to Provable Invariance via Orbit Mapping,0.208256,"Many applications require robustness, or ideally invariance, of neural
networks to certain transformations of input data. Most commonly, this
requirement is addressed by training data augmentation, using adversarial
training, or defining network architectures that include the desired invariance
by design. In this work, we propose a method to make network architectures
provably invariant with respect to group actions by choosing one element from a
(possibly continuous) orbit based on a fixed criterion. In a nutshell, we
intend to 'undo' any possible transformation before feeding the data into the
actual network. Further, we empirically analyze the properties of different
approaches which incorporate invariance via training or architecture, and
demonstrate the advantages of our method in terms of robustness and
computational efficiency. In particular, we investigate the robustness with
respect to rotations of images (which can hold up to discretization artifacts)
as well as the provable orientation and scaling invariance of 3D point cloud
classification.",https://github.com/QUVA-Lab/e2cnn_experiments,3895
80c53587-41f2-4d59-af9e-05712ed08504,HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations,0.847999,"Recently, various response generation models for two-party conversations have
achieved impressive improvements, but less effort has been paid to multi-party
conversations (MPCs) which are more practical and complicated. Compared with a
two-party conversation where a dialogue context is a sequence of utterances,
building a response generation model for MPCs is more challenging, since there
exist complicated context structures and the generated responses heavily rely
on both interlocutors (i.e., speaker and addressee) and history utterances. To
address these challenges, we present HeterMPC, a heterogeneous graph-based
neural network for response generation in MPCs which models the semantics of
utterances and interlocutors simultaneously with two types of nodes in a graph.
Besides, we also design six types of meta relations with
node-edge-type-dependent parameters to characterize the heterogeneous
interactions within the graph. Through multi-hop updating, HeterMPC can
adequately utilize the structural knowledge of conversations for response
generation. Experimental results on the Ubuntu Internet Relay Chat (IRC)
channel benchmark show that HeterMPC outperforms various baseline models for
response generation in MPCs.",https://github.com/lxchtan/HeterMPC,-1
b1858f6b-161a-4aa5-9d1f-56fff05221cc,A Moral- and Event- Centric Inspection of Gender Bias in Fairy Tales at A Large Scale,0.283024,"Fairy tales are a common resource for young children to learn a language or
understand how a society works. However, gender bias, e.g., stereotypical
gender roles, in this literature may cause harm and skew children's world view.
Instead of decades of qualitative and manual analysis of gender bias in fairy
tales, we computationally analyze gender bias in a fairy tale dataset
containing 624 fairy tales from 7 different cultures. We specifically examine
gender difference in terms of moral foundations, which are measures of human
morality, and events, which reveal human activities associated with each
character. We find that the number of male characters is two times that of
female characters, showing a disproportionate gender representation. Our
analysis further reveal stereotypical portrayals of both male and female
characters in terms of moral foundations and events. Female characters turn out
more associated with care-, loyalty- and sanctity- related moral words, while
male characters are more associated with fairness- and authority- related moral
words. Female characters' events are often about emotion (e.g., weep),
appearance (e.g., comb), household (e.g., bake), etc.; while male characters'
events are more about profession (e.g., hunt), violence (e.g., destroy),
justice (e.g., judge), etc. Gender bias in terms of moral foundations shows an
obvious difference across cultures. For example, female characters are more
associated with care and sanctity in high uncertainty-avoidance cultures which
are less open to changes and unpredictability. Based on the results, we propose
implications for children's literature and early literacy research.",https://github.com/medianeuroscience/emfdscore,-1
a94dee7c-30ec-4c7f-9e6d-db6dbcf5aee6,CDialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware Dialog Generation,0.433811,"The development of conversational agents to interact with patients and
deliver clinical advice has attracted the interest of many researchers,
particularly in light of the COVID-19 pandemic. The training of an end-to-end
neural based dialog system, on the other hand, is hampered by a lack of
multi-turn medical dialog corpus. We make the very first attempt to release a
high-quality multi-turn Medical Dialog dataset relating to Covid-19 disease
named CDialog, with over 1K conversations collected from the online medical
counselling websites. We annotate each utterance of the conversation with seven
different categories of medical entities, including diseases, symptoms, medical
tests, medical history, remedies, medications and other aspects as additional
labels. Finally, we propose a novel neural medical dialog system based on the
CDialog dataset to advance future research on developing automated medical
dialog systems. We use pre-trained language models for dialogue generation,
incorporating annotated medical entities, to generate a virtual doctor's
response that addresses the patient's query. Experimental results show that the
proposed dialog models perform comparably better when supplemented with entity
information and hence can improve the response quality.",https://github.com/deekshaVarshney/CDialog,-1
1e39df4a-1f77-46ed-8076-17392fb7df47,Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics,0.635434,"Recent work incorporates pre-trained word embeddings such as BERT embeddings
into Neural Topic Models (NTMs), generating highly coherent topics. However,
with high-quality contextualized document representations, do we really need
sophisticated neural models to obtain coherent and interpretable topics? In
this paper, we conduct thorough experiments showing that directly clustering
high-quality sentence embeddings with an appropriate word selecting method can
generate more coherent and diverse topics than NTMs, achieving also higher
efficiency and simplicity.",https://github.com/hyintell/topicx,-1
1cd27eca-fa43-4b7d-b7ba-2119add61d63,Collecting Interactive Multi-modal Datasets for Grounded Language Understanding,0.191716,"Human intelligence can remarkably adapt quickly to new tasks and
environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research which
can enable similar capabilities in machines, we made the following
contributions (1) formalized the collaborative embodied agent using natural
language task; (2) developed a tool for extensive and scalable data collection;
and (3) collected the first dataset for interactive grounded language
understanding.",https://github.com/aliannejadi/ClariQ,-1
e5e23871-16e5-4d41-95f3-54c0137d6376,DetIE: Multilingual Open Information Extraction Inspired by Object Detection,0.45841,"State of the art neural methods for open information extraction (OpenIE)
usually extract triplets (or tuples) iteratively in an autoregressive or
predicate-based manner in order not to produce duplicates. In this work, we
propose a different approach to the problem that can be equally or more
successful. Namely, we present a novel single-pass method for OpenIE inspired
by object detection algorithms from computer vision. We use an order-agnostic
loss based on bipartite matching that forces unique predictions and a
Transformer-based encoder-only architecture for sequence labeling. The proposed
approach is faster and shows superior or similar performance in comparison with
state of the art models on standard benchmarks in terms of both quality metrics
and inference time. Our model sets the new state of the art performance of
67.7% F1 on CaRB evaluated as OIE2016 while being 3.35x faster at inference
than previous state of the art. We also evaluate the multilingual version of
our model in the zero-shot setting for two languages and introduce a strategy
for generating synthetic multilingual data to fine-tune the model for each
specific language. In this setting, we show performance improvement 15% on
multilingual Re-OIE2016, reaching 75% F1 for both Portuguese and Spanish
languages. Code and models are available at
https://github.com/sberbank-ai/DetIE.",https://github.com/sberbank-ai/DetIE,-1
01b6bf6e-6d19-475b-b65f-7bca4225e696,Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions,0.643779,"We describe an augmented intelligence system for simplifying and enhancing
the modeling experience for operations research. Using this system, the user
receives a suggested formulation of an optimization problem based on its
description. To facilitate this process, we build an intuitive user interface
system that enables the users to validate and edit the suggestions. We
investigate controlled generation techniques to obtain an automatic suggestion
of formulation. Then, we evaluate their effectiveness with a newly created
dataset of linear programming problems drawn from various application domains.",None,-1
17579b19-810d-4f89-898a-606b9ae7af06,Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian,0.505969,"We present the shared task on artificial text detection in Russian, which is
organized as a part of the Dialogue Evaluation initiative, held in 2022. The
shared task dataset includes texts from 14 text generators, i.e., one human
writer and 13 text generative models fine-tuned for one or more of the
following generation tasks: machine translation, paraphrase generation, text
summarization, text simplification. We also consider back-translation and
zero-shot generation approaches. The human-written texts are collected from
publicly available resources across multiple domains. The shared task consists
of two sub-tasks: (i) to determine if a given text is automatically generated
or written by a human; (ii) to identify the author of a given text. The first
task is framed as a binary classification problem. The second task is a
multi-class classification problem. We provide count-based and BERT-based
baselines, along with the human evaluation on the first sub-task. A total of 30
and 8 systems have been submitted to the binary and multi-class sub-tasks,
correspondingly. Most teams outperform the baselines by a wide margin. We
publicly release our codebase, human evaluation results, and other materials in
our GitHub repository (https://github.com/dialogue-evaluation/RuATD).",https://github.com/dialogue-evaluation/RuATD,2091
b68be24c-fb3b-4435-b3bc-7a97b350e595,Object-based active inference,0.324714,"The world consists of objects: distinct entities possessing independent
properties and dynamics. For agents to interact with the world intelligently,
they must translate sensory inputs into the bound-together features that
describe each object. These object-based representations form a natural basis
for planning behavior. Active inference (AIF) is an influential unifying
account of perception and action, but existing AIF models have not leveraged
this important inductive bias. To remedy this, we introduce 'object-based
active inference' (OBAI), marrying AIF with recent deep object-based neural
networks. OBAI represents distinct objects with separate variational beliefs,
and uses selective attention to route inputs to their corresponding object
slots. Object representations are endowed with independent action-based
dynamics. The dynamics and generative model are learned from experience with a
simple environment (active multi-dSprites). We show that OBAI learns to
correctly segment the action-perturbed objects from video input, and to
manipulate these objects towards arbitrary goals.",None,-1
2a64f471-3b02-4629-a679-e5ff575ab80a,Doctor Recommendation in Online Health Forums via Expertise Learning,0.114371,"Huge volumes of patient queries are daily generated on online health forums,
rendering manual doctor allocation a labor-intensive task. To better help
patients, this paper studies a novel task of doctor recommendation to enable
automatic pairing of a patient to a doctor with relevant expertise. While most
prior work in recommendation focuses on modeling target users from their past
behavior, we can only rely on the limited words in a query to infer a patient's
needs for privacy reasons. For doctor modeling, we study the joint effects of
their profiles and previous dialogues with other patients and explore their
interactions via self-learning. The learned doctor embeddings are further
employed to estimate their capabilities of handling a patient query with a
multi-head attention mechanism. For experiments, a large-scale dataset is
collected from Chunyu Yisheng, a Chinese online health forum, where our model
exhibits the state-of-the-art results, outperforming baselines only consider
profiles and past dialogues to characterize a doctor.",https://github.com/polyusmart/Doctor-Recommendation,-1
58f59309-74ca-4d85-a196-100ffd60493d,TAD: Transfer Learning-based Multi-Adversarial Detection of Evasion Attacks against Network Intrusion Detection Systems,0.772581,"Nowadays, intrusion detection systems based on deep learning deliver
state-of-the-art performance. However, recent research has shown that specially
crafted perturbations, called adversarial examples, are capable of
significantly reducing the performance of these intrusion detection systems.
The objective of this paper is to design an efficient transfer learning-based
adversarial detector and then to assess the effectiveness of using multiple
strategically placed adversarial detectors compared to a single adversarial
detector for intrusion detection systems. In our experiments, we implement
existing state-of-the-art models for intrusion detection. We then attack those
models with a set of chosen evasion attacks. In an attempt to detect those
adversarial attacks, we design and implement multiple transfer learning-based
adversarial detectors, each receiving a subset of the information passed
through the IDS. By combining their respective decisions, we illustrate that
combining multiple detectors can further improve the detectability of
adversarial traffic compared to a single detector in the case of a parallel IDS
design.",None,-1
6295f603-5b9f-43ac-a627-cad8a359c249,A Flexible Schema-Guided Dialogue Management Framework: From Friendly Peer to Virtual Standardized Cancer Patient,0.184296,"A schema-guided approach to dialogue management has been shown in recent work
to be effective in creating robust customizable virtual agents capable of
acting as friendly peers or task assistants. However, successful applications
of these methods in open-ended, mixed-initiative domains remain elusive --
particularly within medical domains such as virtual standardized patients,
where such complex interactions are commonplace -- and require more extensive
and flexible dialogue management capabilities than previous systems provide. In
this paper, we describe a general-purpose schema-guided dialogue management
framework used to develop SOPHIE, a virtual standardized cancer patient that
allows a doctor to conveniently practice for interactions with patients. We
conduct a crowdsourced evaluation of conversations between medical students and
SOPHIE. Our agent is judged to produce responses that are natural, emotionally
appropriate, and consistent with her role as a cancer patient. Furthermore, it
significantly outperforms an end-to-end neural model fine-tuned on a human
standardized patient corpus, attesting to the advantages of a schema-guided
approach.",None,-1
3e31bc07-738d-4da1-bed7-ce2c8672dd98,Linear Leaky-Integrate-and-Fire Neuron Model Based Spiking Neural Networks and Its Mapping Relationship to Deep Neural Networks,0.782246,"Spiking neural networks (SNNs) are brain-inspired machine learning algorithms
with merits such as biological plausibility and unsupervised learning
capability. Previous works have shown that converting Artificial Neural
Networks (ANNs) into SNNs is a practical and efficient approach for
implementing an SNN. However, the basic principle and theoretical groundwork
are lacking for training a non-accuracy-loss SNN. This paper establishes a
precise mathematical mapping between the biological parameters of the Linear
Leaky-Integrate-and-Fire model (LIF)/SNNs and the parameters of ReLU-AN/Deep
Neural Networks (DNNs). Such mapping relationship is analytically proven under
certain conditions and demonstrated by simulation and real data experiments. It
can serve as the theoretical basis for the potential combination of the
respective merits of the two categories of neural networks.",None,-1
891cd8b2-a9a2-4950-ae8f-400ac2ab8ba3,AutoSNN: Towards Energy-Efficient Spiking Neural Networks,0.938501,"Spiking neural networks (SNNs) that mimic information transmission in the
brain can energy-efficiently process spatio-temporal information through
discrete and sparse spikes, thereby receiving considerable attention. To
improve accuracy and energy efficiency of SNNs, most previous studies have
focused solely on training methods, and the effect of architecture has rarely
been studied. We investigate the design choices used in the previous studies in
terms of the accuracy and number of spikes and figure out that they are not
best-suited for SNNs. To further improve the accuracy and reduce the spikes
generated by SNNs, we propose a spike-aware neural architecture search
framework called AutoSNN. We define a search space consisting of architectures
without undesirable design choices. To enable the spike-aware architecture
search, we introduce a fitness that considers both the accuracy and number of
spikes. AutoSNN successfully searches for SNN architectures that outperform
hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate
the effectiveness of AutoSNN on various datasets including neuromorphic
datasets.",https://github.com/nabk89/AutoSNN,-1
21a6cf2a-3583-4d39-9fd6-60af3b826126,Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction,0.868504,"Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and
corresponding cause clauses, which have recently received growing attention.
Previous methods sequentially encode features with a specified order. They
first encode the emotion and cause features for clause extraction and then
combine them for pair extraction. This lead to an imbalance in inter-task
feature interaction where features extracted later have no direct contact with
the former. To address this issue, we propose a novel Pair-Based Joint Encoding
(PBJE) network, which generates pairs and clauses features simultaneously in a
joint feature encoding manner to model the causal relationship in clauses. PBJE
can balance the information flow among emotion clauses, cause clauses and
pairs. From a multi-relational perspective, we construct a heterogeneous
undirected graph and apply the Relational Graph Convolutional Network (RGCN) to
capture the various relationship between clauses and the relationship between
pairs and clauses. Experimental results show that PBJE achieves
state-of-the-art performance on the Chinese benchmark corpus.",https://github.com/tutuDoki/PBJE-ECPE,-1
3f556675-60b9-4248-bf72-6f3fa03fbb59,TourBERT: A pretrained language model for the tourism industry,0.206508,"The Bidirectional Encoder Representations from Transformers (BERT) is
currently one of the most important and state-of-the-art models for natural
language. However, it has also been shown that for domain-specific tasks it is
helpful to pretrain BERT on a domain-specific corpus. In this paper, we present
TourBERT, a pretrained language model for tourism. We describe how TourBERT was
developed and evaluated. The evaluations show that TourBERT is outperforming
BERT in all tourism-specific tasks.",None,149
c1c8550d-f07f-4ea3-a942-5c8f305c53d1,Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations,0.674969,"Neural networks have achieved tremendous success in a large variety of
applications. However, their memory footprint and computational demand can
render them impractical in application settings with limited hardware or energy
resources. In this work, we propose a novel algorithm to find efficient
low-rank subnetworks. Remarkably, these subnetworks are determined and adapted
already during the training phase and the overall time and memory resources
required by both training and evaluating them are significantly reduced. The
main idea is to restrict the weight matrices to a low-rank manifold and to
update the low-rank factors rather than the full matrix during training. To
derive training updates that are restricted to the prescribed manifold, we
employ techniques from dynamic model order reduction for matrix differential
equations. This allows us to provide approximation, stability, and descent
guarantees. Moreover, our method automatically and dynamically adapts the ranks
during training to achieve the desired approximation accuracy. The efficiency
of the proposed method is demonstrated through a variety of numerical
experiments on fully-connected and convolutional networks.",https://github.com/COMPiLELab/DLRT,-1
8133a55f-d0d3-4a5f-8922-862bea89770d,"Deep learning and machine learning for Malaria detection: overview, challenges and future directions",0.244828,"To have the greatest impact, public health initiatives must be made using
evidence-based decision-making. Machine learning Algorithms are created to
gather, store, process, and analyse data to provide knowledge and guide
decisions. A crucial part of any surveillance system is image analysis. The
communities of computer vision and machine learning has ended up curious about
it as of late. This study uses a variety of machine learning and image
processing approaches to detect and forecast the malarial illness. In our
research, we discovered the potential of deep learning techniques as smart
tools with broader applicability for malaria detection, which benefits
physicians by assisting in the diagnosis of the condition. We examine the
common confinements of deep learning for computer frameworks and organising,
counting need of preparing data, preparing overhead, realtime execution, and
explain ability, and uncover future inquire about bearings focusing on these
restrictions.",None,-1
63a77933-700f-4b62-9d2a-a0c9da5e3513,EMA-VIO: Deep Visual-Inertial Odometry with External Memory Attention,0.913878,"Accurate and robust localization is a fundamental need for mobile agents.
Visual-inertial odometry (VIO) algorithms exploit the information from camera
and inertial sensors to estimate position and translation. Recent deep learning
based VIO models attract attentions as they provide pose information in a
data-driven way, without the need of designing hand-crafted algorithms.
Existing learning based VIO models rely on recurrent models to fuse multimodal
data and process sensor signal, which are hard to train and not efficient
enough. We propose a novel learning based VIO framework with external memory
attention that effectively and efficiently combines visual and inertial
features for states estimation. Our proposed model is able to estimate pose
accurately and robustly, even in challenging scenarios, e.g., on overcast days
and water-filled ground , which are difficult for traditional VIO algorithms to
extract visual features. Experiments validate that it outperforms both
traditional and learning based VIO baselines in different scenes.",None,-1
96dbf247-7367-4a3b-a7c9-ae42d71033e8,Spatiotemporal Costmap Inference for MPC via Deep Inverse Reinforcement Learning,0.777634,"It can be difficult to autonomously produce driver behavior so that it
appears natural to other traffic participants. Through Inverse Reinforcement
Learning (IRL), we can automate this process by learning the underlying reward
function from human demonstrations. We propose a new IRL algorithm that learns
a goal-conditioned spatiotemporal reward function. The resulting costmap is
used by Model Predictive Controllers (MPCs) to perform a task without any
hand-designing or hand-tuning of the cost function. We evaluate our proposed
Goal-conditioned SpatioTemporal Zeroing Maximum Entropy Deep IRL (GSTZ)-MEDIRL
framework together with MPC in the CARLA simulator for autonomous driving, lane
keeping, and lane changing tasks in a challenging dense traffic highway
scenario. Our proposed methods show higher success rates compared to other
baseline methods including behavior cloning, state-of-the-art RL policies, and
MPC with a learning-based behavior prediction model.",None,-1
2cef28ec-d750-4874-82b5-3cc33957a719,Streaming Radiance Fields for 3D Video Synthesis,0.592729,"We present an explicit-grid based method for efficiently reconstructing
streaming radiance fields for novel view synthesis of real world dynamic
scenes. Instead of training a single model that combines all the frames, we
formulate the dynamic modeling problem with an incremental learning paradigm in
which per-frame model difference is trained to complement the adaption of a
base model on the current frame. By exploiting the simple yet effective tuning
strategy with narrow bands, the proposed method realizes a feasible framework
for handling video sequences on-the-fly with high training efficiency. The
storage overhead induced by using explicit grid representations can be
significantly reduced through the use of model difference based compression. We
also introduce an efficient strategy to further accelerate model optimization
for each frame. Experiments on challenging video sequences demonstrate that our
approach is capable of achieving a training speed of 15 seconds per-frame with
competitive rendering quality, which attains $1000 \times$ speedup over the
state-of-the-art implicit methods. Code is available at
https://github.com/AlgoHunt/StreamRF.",https://github.com/AlgoHunt/StreamRF,-1
00a03633-45bc-442e-a640-e0891c8b3b4c,Personalized Diagnostic Tool for Thyroid Cancer Classification using Multi-view Ultrasound,0.240478,"Over the past decades, the incidence of thyroid cancer has been increasing
globally. Accurate and early diagnosis allows timely treatment and helps to
avoid over-diagnosis. Clinically, a nodule is commonly evaluated from both
transverse and longitudinal views using thyroid ultrasound. However, the
appearance of the thyroid gland and lesions can vary dramatically across
individuals. Identifying key diagnostic information from both views requires
specialized expertise. Furthermore, finding an optimal way to integrate
multi-view information also relies on the experience of clinicians and adds
further difficulty to accurate diagnosis. To address these, we propose a
personalized diagnostic tool that can customize its decision-making process for
different patients. It consists of a multi-view classification module for
feature extraction and a personalized weighting allocation network that
generates optimal weighting for different views. It is also equipped with a
self-supervised view-aware contrastive loss to further improve the model
robustness towards different patient groups. Experimental results show that the
proposed framework can better utilize multi-view information and outperform the
competing methods.",None,-1
73a25412-b60a-4691-a1d3-821baca8eb92,Dataset Condensation with Latent Space Knowledge Factorization and Sharing,0.299334,"In this paper, we introduce a novel approach for systematically solving
dataset condensation problem in an efficient manner by exploiting the
regularity in a given dataset. Instead of condensing the dataset directly in
the original input space, we assume a generative process of the dataset with a
set of learnable codes defined in a compact latent space followed by a set of
tiny decoders which maps them differently to the original input space. By
combining different codes and decoders interchangeably, we can dramatically
increase the number of synthetic examples with essentially the same parameter
count, because the latent space is much lower dimensional and since we can
assume as many decoders as necessary to capture different styles represented in
the dataset with negligible cost. Such knowledge factorization allows efficient
sharing of information between synthetic examples in a systematic way,
providing far better trade-off between compression ratio and quality of the
generated examples. We experimentally show that our method achieves new
state-of-the-art records by significant margins on various benchmark datasets
such as SVHN, CIFAR10, CIFAR100, and TinyImageNet.",None,-1
43609747-0743-40a7-81d1-f88441468d3c,Optical tracking in team sports,0.798104,"Sports analysis has gained paramount importance for coaches, scouts, and
fans. Recently, computer vision researchers have taken on the challenge of
collecting the necessary data by proposing several methods of automatic player
and ball tracking. Building on the gathered tracking data, data miners are able
to perform quantitative analysis on the performance of players and teams. With
this survey, our goal is to provide a basic understanding for quantitative data
analysts about the process of creating the input data and the characteristics
thereof. Thus, we summarize the recent methods of optical tracking by providing
a comprehensive taxonomy of conventional and deep learning methods, separately.
Moreover, we discuss the preprocessing steps of tracking, the most common
challenges in this domain, and the application of tracking data to sports
teams. Finally, we compare the methods by their cost and limitations, and
conclude the work by highlighting potential future research directions.",None,-1
ef58860a-2995-48cc-940a-b6ac55cbae36,Predicting tacrolimus exposure in kidney transplanted patients using machine learning,0.113455,"Tacrolimus is one of the cornerstone immunosuppressive drugs in most
transplantation centers worldwide following solid organ transplantation.
Therapeutic drug monitoring of tacrolimus is necessary in order to avoid
rejection of the transplanted organ or severe side effects. However, finding
the right dose for a given patient is challenging, even for experienced
clinicians. Consequently, a tool that can accurately estimate the drug exposure
for individual dose adaptions would be of high clinical value. In this work, we
propose a new technique using machine learning to estimate the tacrolimus
exposure in kidney transplant recipients. Our models achieve predictive errors
that are at the same level as an established population pharmacokinetic model,
but are faster to develop and require less knowledge about the pharmacokinetic
properties of the drug.",None,-1
4f56af9c-8ec2-42a0-bee2-e5e6bec5fbfc,"Choose, not Hoard: Information-to-Model Matching for Artificial Intelligence in O-RAN",0.266786,"Open Radio Access Network (O-RAN) is an emerging paradigm, whereby
virtualized network infrastructure elements from different vendors communicate
via open, standardized interfaces. A key element therein is the RAN Intelligent
Controller (RIC), an Artificial Intelligence (AI)-based controller.
Traditionally, all data available in the network has been used to train a
single AI model to be used at the RIC. This paper introduces, discusses, and
evaluates the creation of multiple AI model instances at different RICs,
leveraging information from some (or all) locations for their training. This
brings about a flexible relationship between gNBs, the AI models used to
control them, and the data such models are trained with. Experiments with
real-world traces show how using multiple AI model instances that choose
training data from specific locations improve the performance of traditional
approaches following the hoarding strategy.",None,-1
4185b679-6de5-4a5a-b16f-b40629d80d33,Defect detection and segmentation in X-Ray images of magnesium alloy castings using the Detectron2 framework,0.516926,"New production techniques have emerged that have made it possible to produce
metal parts with more complex shapes, making the quality control process more
difficult. This implies that the visual and superficial analysis has become
even more inefficient. On top of that, it is also not possible to detect
internal defects that these parts could have. The use of X-Ray images has made
this process much easier, allowing not only to detect superficial defects in a
much simpler way, but also to detect welding or casting defects that could
represent a serious hazard for the physical integrity of the metal parts. On
the other hand, the use of an automatic segmentation approach for detecting
defects would help diminish the dependence of defect detection on the
subjectivity of the factory operators and their time dependence variability.
The aim of this paper is to apply a deep learning system based on Detectron2, a
state-of-the-art library applied to object detection and segmentation in
images, for the identification and segmentation of these defects on X-Ray
images obtained mainly from automotive parts",None,-1
d6237d87-93c7-4e90-99f2-950687c7dfc7,A Two-Stage Efficient 3-D CNN Framework for EEG Based Emotion Recognition,0.345428,"This paper proposes a novel two-stage framework for emotion recognition using
EEG data that outperforms state-of-the-art models while keeping the model size
small and computationally efficient. The framework consists of two stages; the
first stage involves constructing efficient models named EEGNet, which is
inspired by the state-of-the-art efficient architecture and employs
inverted-residual blocks that contain depthwise separable convolutional layers.
The EEGNet models on both valence and arousal labels achieve the average
classification accuracy of 90%, 96.6%, and 99.5% with only 6.4k, 14k, and 25k
parameters, respectively. In terms of accuracy and storage cost, these models
outperform the previous state-of-the-art result by up to 9%. In the second
stage, we binarize these models to further compress them and deploy them easily
on edge devices. Binary Neural Networks (BNNs) typically degrade model
accuracy. We improve the EEGNet binarized models in this paper by introducing
three novel methods and achieving a 20\% improvement over the baseline binary
models. The proposed binarized EEGNet models achieve accuracies of 81%, 95%,
and 99% with storage costs of 0.11Mbits, 0.28Mbits, and 0.46Mbits,
respectively. Those models help deploy a precise human emotion recognition
system on the edge environment.",None,-1
723dd5d4-003c-4e07-bd90-82e5872b7ce1,End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting,0.234305,"Spoken Language Understanding (SLU) is a core task in most human-machine
interaction systems. With the emergence of smart homes, smart phones and smart
speakers, SLU has become a key technology for the industry. In a classical SLU
approach, an Automatic Speech Recognition (ASR) module transcribes the speech
signal into a textual representation from which a Natural Language
Understanding (NLU) module extracts semantic information. Recently End-to-End
SLU (E2E SLU) based on Deep Neural Networks has gained momentum since it
benefits from the joint optimization of the ASR and the NLU parts, hence
limiting the cascade of error effect of the pipeline architecture. However,
little is known about the actual linguistic properties used by E2E models to
predict concepts and intents from speech input. In this paper, we present a
study identifying the signal features and other linguistic properties used by
an E2E model to perform the SLU task. The study is carried out in the
application domain of a smart home that has to handle non-English (here French)
voice commands. The results show that a good E2E SLU performance does not
always require a perfect ASR capability. Furthermore, the results show the
superior capabilities of the E2E model in handling background noise and
syntactic variation compared to the pipeline model. Finally, a finer-grained
analysis suggests that the E2E model uses the pitch information of the input
signal to identify voice command concepts. The results and methodology outlined
in this paper provide a springboard for further analyses of E2E models in
speech processing.",None,-1
bbc5b15f-9b3e-46a4-8d83-214c14b5226a,Learning from One and Only One Shot,0.0619988,"Humans can generalize from only a few examples and from little pre-training
on similar tasks. Yet, machine learning (ML) typically requires large data to
learn or pre-learn to transfer. Inspired by nativism, we directly model basic
human-innate priors in abstract visual tasks e.g., character/doodle
recognition. This yields a white-box model that learns general-appearance
similarity -- how any two images look in general -- by mimicking how humans
naturally ""distort"" an object at first sight. Using simply the nearest-neighbor
classifier on this similarity space, we achieve human-level character
recognition using only 1--10 examples per class and nothing else (no
pre-training). This differs from few-shot learning (FSL) using significant
pre-training. On standard benchmarks MNIST/EMNIST and the Omniglot challenge,
we outperform both neural-network-based and classical ML in the ""tiny-data""
regime, including FSL pre-trained on large data. Our model enables unsupervised
learning too: by learning the non-Euclidean, general-appearance similarity
space in a k-means style, we can generate human-intuitive archetypes as cluster
``centroids''.",None,-1
